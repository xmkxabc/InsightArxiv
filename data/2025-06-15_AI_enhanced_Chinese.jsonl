{"id": "2506.10020", "title": "From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment", "authors": ["Kyubyung Chae", "Hyunbin Jin", "Taesup Kim"], "summary": "Safely aligning large language models (LLMs) often demands extensive\nhuman-labeled preference data, a process that's both costly and time-consuming.\nWhile synthetic data offers a promising alternative, current methods frequently\nrely on complex iterative prompting or auxiliary models. To address this, we\nintroduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,\ntraining-free, and model-agnostic framework that repurposes LLM attack\ntechniques. RAAI works by detecting internal refusal signals and adaptively\ninjecting predefined phrases to elicit harmful, yet fluent, completions. Our\nexperiments show RAAI effectively jailbreaks LLMs, increasing the harmful\nresponse rate from a baseline of 2.15% to up to 61.04% on average across four\nbenchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by\nRAAI improves model robustness against harmful prompts while preserving general\ncapabilities on standard tasks like MMLU and ARC. This work highlights how LLM\nattack methodologies can be reframed as practical tools for scalable and\ncontrollable safety alignment.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10020v1", "AI": {"title_translation": "从威胁到工具：利用拒绝感知注入攻击进行安全对齐", "tldr": "本文提出了一种名为RAAI的框架，通过将LLM攻击技术转化为工具，生成合成数据来提高LLM的安全对齐，同时保持其通用能力。", "motivation": "安全对齐大型语言模型（LLMs）需要大量昂贵且耗时的人工标注偏好数据。虽然合成数据是一个有前景的替代方案，但现有方法通常依赖于复杂的迭代提示或辅助模型，这促使研究者寻找更直接、无需训练且模型无关的方法。", "method": "本文引入了拒绝感知自适应注入（RAAI），这是一个直接、无需训练且模型无关的框架，它重新利用了LLM攻击技术。RAAI通过检测LLM内部的拒绝信号并自适应地注入预定义短语，以引出有害但流畅的完成。", "result": "实验表明，RAAI能有效越狱LLMs，在四个基准测试中，有害响应率从基线的2.15%平均提高到61.04%。更重要的是，使用RAAI生成的合成数据对LLMs进行微调，可以提高模型对有害提示的鲁棒性，同时在MMLU和ARC等标准任务上保持通用能力。", "conclusion": "这项工作强调了LLM攻击方法可以被重新定义为可扩展和可控安全对齐的实用工具。", "translation": "安全对齐大型语言模型（LLMs）通常需要大量人工标注的偏好数据，这个过程既昂贵又耗时。虽然合成数据提供了一个有前景的替代方案，但现有方法经常依赖于复杂的迭代提示或辅助模型。为了解决这个问题，我们引入了拒绝感知自适应注入（RAAI），这是一个直接、无需训练且模型无关的框架，它重新利用了LLM攻击技术。RAAI通过检测内部拒绝信号并自适应地注入预定义短语来引出有害但流畅的完成。我们的实验表明，RAAI能有效越狱LLMs，在四个基准测试中，有害响应率从基线的2.15%平均提高到高达61.04%。至关重要的是，使用RAAI生成的合成数据对LLMs进行微调，可以提高模型对有害提示的鲁棒性，同时在MMLU和ARC等标准任务上保持通用能力。这项工作强调了LLM攻击方法可以被重新定义为可扩展和可控安全对齐的实用工具。", "summary": "本文提出了一种名为拒绝感知自适应注入（RAAI）的新框架，旨在通过将LLM攻击技术转化为工具来解决LLM安全对齐中对昂贵人工标注数据的依赖。RAAI通过检测模型内部的拒绝信号并注入特定短语来生成有害但流畅的合成数据。实验证明，RAAI能有效提高LLM的有害响应率，并且利用这些合成数据进行微调可以增强模型对有害提示的鲁棒性，同时不影响其在通用任务上的性能。这项工作展示了攻击方法在LLM安全对齐中的潜在应用，提供了一种可扩展且可控的解决方案。", "keywords": "LLM安全对齐, 注入攻击, 合成数据, 拒绝感知, 越狱", "comments": "本文的创新之处在于将通常用于攻击LLMs的技术重新定义为促进安全对齐的工具，这提供了一个新颖的视角。RAAI框架的“无需训练”和“模型无关”特性是其重要优势，意味着它具有广泛的适用性和较低的实施成本。通过生成合成数据来提升模型鲁棒性，同时保持通用能力，解决了LLM对齐中的一个关键挑战，即如何高效且不牺牲性能地进行安全对齐。这为LLM安全研究开辟了新的途径。"}}
{"id": "2506.10022", "title": "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges", "authors": ["Haoyang Li", "Huan Gao", "Zhiyuan Zhao", "Zhiyu Lin", "Junyu Gao", "Xuelong Li"], "summary": "The widespread adoption of Large Language Models (LLMs) has heightened\nconcerns about their security, particularly their vulnerability to jailbreak\nattacks that leverage crafted prompts to generate malicious outputs. While\nprior research has been conducted on general security capabilities of LLMs,\ntheir specific susceptibility to jailbreak attacks in code generation remains\nlargely unexplored. To fill this gap, we propose MalwareBench, a benchmark\ndataset containing 3,520 jailbreaking prompts for malicious code-generation,\ndesigned to evaluate LLM robustness against such threats. MalwareBench is based\non 320 manually crafted malicious code generation requirements, covering 11\njailbreak methods and 29 code functionality categories. Experiments show that\nmainstream LLMs exhibit limited ability to reject malicious code-generation\nrequirements, and the combination of multiple jailbreak methods further reduces\nthe model's security capabilities: specifically, the average rejection rate for\nmalicious content is 60.93%, dropping to 39.92% when combined with jailbreak\nattack algorithms. Our work highlights that the code security capabilities of\nLLMs still pose significant challenges.", "comment": "Accepted as ACL 2025 main conference", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10022v1", "AI": {"title_translation": "LLM陷入交火：恶意软件请求与越狱挑战", "tldr": "本文提出了MalwareBench数据集，用于评估大型语言模型（LLMs）在恶意代码生成方面的越狱攻击漏洞。实验结果表明，主流LLMs对恶意代码的拒绝能力有限，尤其是在结合多种越狱方法时，其安全能力显著下降。", "motivation": "尽管之前对LLM的通用安全能力有所研究，但其在代码生成方面对越狱攻击的特定脆弱性仍未得到充分探索。本文旨在弥补这一空白，评估LLM抵御此类威胁的鲁棒性。", "method": "本文提出了MalwareBench，这是一个包含3,520个用于恶意代码生成的越狱提示的基准数据集。该数据集基于320个手工制作的恶意代码生成需求，涵盖了11种越狱方法和29种代码功能类别。", "result": "实验表明，主流LLMs拒绝恶意代码生成请求的能力有限。结合多种越狱方法会进一步降低模型的安全能力：恶意内容的平均拒绝率为60.93%，而当与越狱攻击算法结合时，该比率降至39.92%。", "conclusion": "LLMs的代码安全能力仍面临重大挑战。", "translation": "大型语言模型（LLM）的广泛采用加剧了对其安全性的担忧，特别是它们容易受到利用精心设计的提示生成恶意输出的越狱攻击。尽管此前已经对LLM的通用安全能力进行了研究，但它们在代码生成方面对越狱攻击的特定脆弱性在很大程度上仍未被探索。为了弥补这一空白，我们提出了MalwareBench，一个包含3,520个用于恶意代码生成的越狱提示的基准数据集，旨在评估LLM抵御此类威胁的鲁棒性。MalwareBench基于320个手工制作的恶意代码生成需求，涵盖了11种越狱方法和29种代码功能类别。实验表明，主流LLM拒绝恶意代码生成要求的能力有限，并且多种越狱方法的结合进一步降低了模型的安全能力：具体来说，恶意内容的平均拒绝率为60.93%，而当与越狱攻击算法结合时，该比率降至39.92%。我们的工作强调，LLM的代码安全能力仍然面临重大挑战。", "summary": "本文针对大型语言模型（LLMs）在恶意代码生成方面对越狱攻击的脆弱性进行了研究。研究人员提出了MalwareBench数据集，包含3,520个恶意代码生成越狱提示，以评估LLMs的安全性。实验结果显示，主流LLMs在拒绝恶意代码方面的能力有限，特别是当多种越狱方法结合使用时，其安全能力会显著下降，表明LLMs的代码安全仍面临严峻挑战。", "keywords": "大型语言模型, 越狱攻击, 恶意代码生成, 安全性, MalwareBench", "comments": "该论文通过构建MalwareBench数据集，填补了LLMs在恶意代码生成越狱攻击领域研究的空白，具有重要的实践意义。其创新之处在于系统性地评估了多种越狱方法对LLMs代码生成安全性的影响，并量化了其脆弱性。研究结果揭示了当前LLMs在代码安全方面的不足，为未来LLMs的安全加固提供了明确的方向。"}}
{"id": "2506.10024", "title": "Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models", "authors": ["Elena Sofia Ruzzetti", "Giancarlo A. Xompero", "Davide Venditti", "Fabio Massimo Zanzotto"], "summary": "Large Language Models (LLMs) memorize, and thus, among huge amounts of\nuncontrolled data, may memorize Personally Identifiable Information (PII),\nwhich should not be stored and, consequently, not leaked. In this paper, we\nintroduce Private Memorization Editing (PME), an approach for preventing\nprivate data leakage that turns an apparent limitation, that is, the LLMs'\nmemorization ability, into a powerful privacy defense strategy. While attacks\nagainst LLMs have been performed exploiting previous knowledge regarding their\ntraining data, our approach aims to exploit the same kind of knowledge in order\nto make a model more robust. We detect a memorized PII and then mitigate the\nmemorization of PII by editing a model knowledge of its training data. We\nverify that our procedure does not affect the underlying language model while\nmaking it more robust against privacy Training Data Extraction attacks. We\ndemonstrate that PME can effectively reduce the number of leaked PII in a\nnumber of configurations, in some cases even reducing the accuracy of the\nprivacy attacks to zero.", "comment": "To be published at ACL 2025 (Main)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10024v1", "AI": {"title_translation": "私有记忆编辑：将记忆转化为防御以增强大型语言模型中的数据隐私", "tldr": "提出PME方法，通过编辑模型对训练数据的记忆来防止LLM泄露个人身份信息，同时不影响模型性能。", "motivation": "大型语言模型（LLMs）会记忆训练数据中的个人身份信息（PII），导致隐私泄露风险。需要一种方法来防止这种泄露。", "method": "引入“私有记忆编辑（PME）”方法。该方法检测被记忆的PII，然后通过编辑模型对其训练数据的知识来减轻PII的记忆，从而防止隐私数据泄露。", "result": "PME过程不影响底层语言模型。使模型对隐私训练数据提取攻击更具鲁棒性。PME能有效减少泄露的PII数量，在某些情况下甚至将隐私攻击的准确性降至零。", "conclusion": "PME是一种有效且不损害模型性能的策略，能够将LLM的记忆能力转化为强大的隐私防御机制，显著增强数据隐私并抵御训练数据提取攻击。", "translation": "大型语言模型（LLMs）具有记忆能力，因此在大量不受控的数据中，可能会记忆个人身份信息（PII），而这些信息不应被存储，从而也不应被泄露。在本文中，我们引入了私有记忆编辑（PME），这是一种防止私人数据泄露的方法，它将一个明显的局限性，即LLM的记忆能力，转化为一种强大的隐私防御策略。虽然针对LLMs的攻击已经利用了关于其训练数据的先前知识，但我们的方法旨在利用相同类型的知识来使模型更具鲁棒性。我们检测到被记忆的PII，然后通过编辑模型对其训练数据的知识来减轻PII的记忆。我们验证了我们的程序在使其更能抵抗隐私训练数据提取攻击的同时，不影响底层语言模型。我们证明了PME可以在多种配置下有效减少泄露的PII数量，在某些情况下甚至将隐私攻击的准确性降至零。", "summary": "本文提出了私有记忆编辑（PME）方法，旨在解决大型语言模型（LLMs）记忆并可能泄露个人身份信息（PII）的问题。PME通过检测和编辑模型对训练数据中PII的记忆来防止数据泄露。研究表明，该方法在不损害LLM性能的前提下，显著增强了模型对隐私训练数据提取攻击的鲁棒性，有效减少了PII泄露，甚至能将攻击准确性降至零，从而将LLM的记忆能力转化为一种有效的隐私防御机制。", "keywords": "大型语言模型, 隐私保护, 记忆编辑, 个人身份信息, 数据安全", "comments": "该研究创新性地将LLM的“记忆”这一潜在弱点转化为隐私防御的优势，提出PME方法。这种“以彼之道还施彼身”的思路非常巧妙，即利用对训练数据知识的掌握来反制隐私攻击。其重要性在于提供了一种在模型训练后进行隐私保护的有效手段，对于解决LLM在实际应用中面临的隐私合规性挑战具有重要意义。"}}
{"id": "2506.10025", "title": "Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers", "authors": ["Yuanhaur Chang", "Oren Heller", "Yaniv Shlomo", "Iddo Bar-Noy", "Ella Bokobza", "Michal Grinstein-Weiss", "Ning Zhang"], "summary": "Key decision-makers in small and medium businesses (SMBs) often lack the\nawareness and knowledge to implement cybersecurity measures effectively. To\ngain a deeper understanding of how SMB executives navigate cybersecurity\ndecision-making, we deployed a mixed-method approach, conducting\nsemi-structured interviews (n=21) and online surveys (n=322) with SMB key\ndecision-makers. Using thematic analysis, we revealed SMB decision-makers'\nperceived risks in terms of the digital assets they valued, and found reasons\nfor their choice of defense measures and factors impacting security perception.\nWe employed the situational awareness model to characterize decision-makers\nbased on cybersecurity awareness, identifying those who have comparatively low\nawareness in the fight against adversaries. We further explored the\nrelationship between awareness and business attributes, and constructed a\nholistic structural equation model to understand how awareness can be improved.\nFinally, we proposed interventions to help SMBs overcome potential challenges.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10025v1", "AI": {"title_translation": "关注差距：通过中小企业关键决策者的态势感知揭示安全障碍", "tldr": "本研究通过混合方法调查中小企业关键决策者，揭示了他们在网络安全方面的认知差距和挑战，并提出了改进意识和干预措施。", "motivation": "中小企业（SMBs）的关键决策者通常缺乏有效实施网络安全措施的意识和知识。本研究旨在深入了解中小企业高管如何进行网络安全决策。", "method": "研究采用了混合方法，对中小企业关键决策者进行了半结构化访谈（n=21）和在线调查（n=322）。数据分析使用了主题分析法来揭示决策者对数字资产的感知风险，并利用态势感知模型来描述决策者的网络安全意识。此外，还构建了一个整体结构方程模型来理解如何提高意识。", "result": "研究揭示了中小企业决策者对其所重视的数字资产的感知风险，找到了他们选择防御措施的原因以及影响安全感知的因素。研究根据网络安全意识对决策者进行了分类，识别出在对抗威胁方面意识相对较低的人群。此外，还探讨了意识与业务属性之间的关系。", "conclusion": "本研究提出了帮助中小企业克服潜在挑战的干预措施。", "translation": "中小企业（SMBs）的关键决策者通常缺乏有效实施网络安全措施的意识和知识。为了更深入地了解中小企业高管如何进行网络安全决策，我们采用了混合方法，对中小企业关键决策者进行了半结构化访谈（n=21）和在线调查（n=322）。通过主题分析，我们揭示了中小企业决策者对其所重视的数字资产的感知风险，并找到了他们选择防御措施的原因以及影响安全感知的因素。我们采用态势感知模型根据网络安全意识对决策者进行了特征描述，识别出在对抗威胁方面意识相对较低的人群。我们进一步探讨了意识与业务属性之间的关系，并构建了一个整体结构方程模型来理解如何提高意识。最后，我们提出了帮助中小企业克服潜在挑战的干预措施。", "summary": "本研究通过对21名中小企业关键决策者进行半结构化访谈和对322名决策者进行在线调查，采用混合方法深入探讨了中小企业高管在网络安全决策中的现状。研究运用主题分析揭示了决策者对数字资产的风险感知、防御措施选择原因及安全感知影响因素。通过态势感知模型识别出网络安全意识较低的决策者，并构建结构方程模型分析意识提升途径。最终，研究提出了帮助中小企业克服网络安全障碍的干预措施。", "keywords": "中小企业, 网络安全, 决策者, 态势感知, 结构方程模型", "comments": "这项研究通过结合定性和定量方法，深入分析了中小企业在网络安全决策中面临的挑战和认知差距，具有重要的实践意义。它不仅揭示了问题所在，还提出了具体的干预措施，有助于提升中小企业的整体网络安全水平。"}}
{"id": "2506.10540", "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation", "authors": ["Haoyuan Shi", "Yunxin Li", "Xinyu Chen", "Longyue Wang", "Baotian Hu", "Min Zhang"], "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.10540v1", "AI": {"title_translation": "AniMaker：MCTS驱动片段生成的自动化多智能体动画故事创作", "tldr": "AniMaker是一个多智能体框架，利用MCTS驱动的片段生成和故事感知的片段选择，从文本输入自动创建连贯的多场景动画故事，解决了现有视频生成模型在故事连贯性和视觉连续性方面的挑战。", "motivation": "尽管视频生成模型快速发展，但生成跨多个场景和角色的连贯叙事视频仍然具有挑战性。现有方法常将预生成关键帧僵化地转换为固定长度片段，导致叙事脱节和节奏问题。此外，视频生成模型固有的不稳定性意味着即使单个低质量片段也可能显著降低整个输出动画的逻辑连贯性和视觉连续性。", "method": "我们引入了AniMaker，一个多智能体框架，能够实现高效的多候选片段生成和故事感知片段选择，仅从文本输入创建全局一致且故事连贯的动画。该框架围绕专门的智能体构建，包括用于故事板生成的导演智能体、用于视频片段生成的摄影智能体、用于评估的评审智能体以及用于编辑和画外音的后期制作智能体。AniMaker方法的核心是两个关键技术组件：摄影智能体中的MCTS-Gen，一种受蒙特卡洛树搜索（MCTS）启发的高效策略，可智能地探索候选空间以生成高潜力片段并优化资源使用；以及评审智能体中的AniEval，第一个专门为多镜头动画评估设计的框架，通过考虑每个片段在其前置和后置片段的上下文来评估故事层级一致性、动作完成度和动画特定特征等关键方面。", "result": "实验表明，AniMaker在VBench和我们提出的AniEval框架等流行指标下取得了卓越的质量，同时显著提高了多候选生成的效率。", "conclusion": "AniMaker通过其多智能体框架和创新的MCTS-Gen及AniEval组件，有效地克服了当前视频生成在故事连贯性和视觉连续性上的挑战，使AI生成的故事动画更接近生产标准。", "translation": "尽管视频生成模型取得了快速进展，但生成跨多个场景和角色的连贯叙事视频仍然具有挑战性。当前方法通常将预生成的关键帧僵化地转换为固定长度片段，导致叙事脱节和节奏问题。此外，视频生成模型固有的不稳定性意味着即使单个低质量片段也可能显著降低整个输出动画的逻辑连贯性和视觉连续性。为了克服这些障碍，我们引入了AniMaker，一个多智能体框架，能够实现高效的多候选片段生成和故事感知片段选择，从而仅从文本输入创建全局一致且故事连贯的动画。该框架围绕专门的智能体构建，包括用于故事板生成的导演智能体、用于视频片段生成的摄影智能体、用于评估的评审智能体以及用于编辑和画外音的后期制作智能体。AniMaker方法的核心是两个关键技术组件：摄影智能体中的MCTS-Gen，一种受蒙特卡洛树搜索（MCTS）启发的高效策略，可智能地探索候选空间以生成高潜力片段并优化资源使用；以及评审智能体中的AniEval，第一个专门为多镜头动画评估设计的框架，通过考虑每个片段在其前置和后置片段的上下文来评估故事层级一致性、动作完成度和动画特定特征等关键方面。实验表明，AniMaker在VBench和我们提出的AniEval框架等流行指标下取得了卓越的质量，同时显著提高了多候选生成的效率，推动AI生成的故事动画更接近生产标准。", "summary": "AniMaker是一个创新的多智能体框架，旨在解决现有视频生成模型在创建连贯多场景动画故事时面临的挑战。它通过引入专门的智能体（导演、摄影、评审、后期制作）来管理从故事板到最终编辑的整个流程。核心技术包括MCTS-Gen，用于高效生成高质量视频片段，以及AniEval，一个专门用于评估多镜头动画故事连贯性和视觉连续性的框架。实验证明AniMaker在质量和效率上均表现出色，使AI生成的动画故事更接近专业标准。", "keywords": "多智能体, 动画故事创作, MCTS, 视频生成, 故事连贯性", "comments": "AniMaker的创新之处在于其多智能体协作框架和引入MCTS-Gen进行智能片段生成，以及AniEval作为首个多镜头动画评估框架。这解决了现有方法在叙事连贯性和视觉连续性上的痛点，显著提升了AI生成动画的质量和实用性，推动了该领域向生产级应用迈进。"}}
{"id": "2506.10004", "title": "Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming", "authors": ["Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "summary": "Extended reality (XR) is rapidly advancing, and poised to revolutionize\ncontent creation and consumption. In XR, users integrate various sensory inputs\nto form a cohesive perception of the virtual environment. This survey reviews\nthe state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,\nwe define XR and introduce various XR headsets along with their multimodal\ninteraction methods to provide a foundational understanding. We then analyze XR\ntraffic characteristics to highlight the unique data transmission requirements.\nWe also explore factors that influence the quality of experience in XR systems,\naiming to identify key elements for enhancing user satisfaction. Following\nthis, we present visual attention-based optimization methods for XR streaming\nto improve efficiency and performance. Finally, we examine current applications\nand highlight challenges to provide insights into ongoing and future\ndevelopments of XR.", "comment": "accepted by ACM Transactions on Multimedia Computing, Communications,\n  and Applications", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10004v1", "AI": {"title_translation": "沉浸式多媒体通信：扩展现实流媒体的最新技术", "tldr": "这篇综述探讨了扩展现实（XR）流媒体的最新技术，涵盖了XR定义、头显、流量特性、体验质量、优化方法以及应用和挑战。", "motivation": "扩展现实（XR）正在迅速发展，并有望彻底改变内容的创作和消费方式。这篇综述旨在审查XR流媒体的最新技术，以提供对XR的基础理解、其独特的数据传输需求、提升用户满意度的关键因素、效率和性能优化方法，并为XR的当前和未来发展提供见解。", "method": "这篇综述通过以下步骤审查了XR流媒体的最新技术：首先，定义了XR并介绍了各种XR头显及其多模态交互方法；其次，分析了XR流量特性以突出其独特的数据传输要求；接着，探讨了影响XR系统体验质量的因素；然后，介绍了基于视觉注意力的XR流媒体优化方法；最后，审视了当前的应用并强调了挑战。", "result": "这篇综述提供了一个对XR的基础理解，揭示了XR独特的流量特性和数据传输要求，识别了提升用户满意度的关键体验质量要素，介绍了基于视觉注意力的优化方法，并对XR的当前应用、持续发展和未来挑战提供了见解。", "conclusion": "这篇综述通过审视当前应用并突出挑战，为XR的持续和未来发展提供了深入的见解。", "translation": "扩展现实（XR）正在迅速发展，并有望彻底改变内容的创作和消费方式。在XR中，用户整合各种感官输入，形成对虚拟环境的连贯感知。本综述回顾了XR流媒体的最新技术，重点关注多种范式。首先，我们定义了XR并介绍了各种XR头显及其多模态交互方法，以提供基础理解。然后，我们分析了XR流量特性，以突出其独特的数据传输要求。我们还探讨了影响XR系统体验质量的因素，旨在识别提升用户满意度的关键要素。在此之后，我们介绍了基于视觉注意力的XR流媒体优化方法，以提高效率和性能。最后，我们审查了当前的应用并强调了挑战，旨在为XR的持续和未来发展提供见解。", "summary": "本综述全面回顾了扩展现实（XR）流媒体的最新技术。文章首先定义了XR并介绍了相关硬件和交互方式，接着分析了XR流量特性和影响用户体验质量的因素。随后，论文探讨了基于视觉注意力的优化方法以提升流媒体效率和性能。最后，文章考察了XR的现有应用并指出了面临的挑战，为XR的当前和未来发展提供了深入的见解。", "keywords": "扩展现实, XR, 流媒体, 沉浸式通信, 体验质量", "comments": "作为一篇综述性论文，其创新性在于系统性地梳理了XR流媒体领域的最新进展，提供了一个全面的概览。论文结构清晰，从基础概念到技术细节，再到应用和挑战，为研究人员和工程师提供了宝贵的参考。其重要性在于，在XR技术快速发展的背景下，它为理解该领域的核心问题、现有解决方案和未来方向提供了一个坚实的基础。"}}
{"id": "2506.10043", "title": "TrioXpert: An automated incident management framework for microservice system", "authors": ["Yongqian Sun", "Yu Luo", "Xidao Wen", "Yuan Yuan", "Xiaohui Nie", "Shenglin Zhang", "Tong Liu", "Xi Luo"], "summary": "Automated incident management plays a pivotal role in large-scale\nmicroservice systems. However, many existing methods rely solely on\nsingle-modal data (e.g., metrics, logs, and traces) and struggle to\nsimultaneously address multiple downstream tasks, including anomaly detection\n(AD), failure triage (FT), and root cause localization (RCL). Moreover, the\nlack of clear reasoning evidence in current techniques often leads to\ninsufficient interpretability. To address these limitations, we propose\nTrioXpert, an end-to-end incident management framework capable of fully\nleveraging multimodal data. TrioXpert designs three independent data processing\npipelines based on the inherent characteristics of different modalities,\ncomprehensively characterizing the operational status of microservice systems\nfrom both numerical and textual dimensions. It employs a collaborative\nreasoning mechanism using large language models (LLMs) to simultaneously handle\nmultiple tasks while providing clear reasoning evidence to ensure strong\ninterpretability. We conducted extensive evaluations on two popular\nmicroservice system datasets, and the experimental results demonstrate that\nTrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),\nFT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10043v1", "AI": {"title_translation": "TrioXpert：一个用于微服务系统的自动化事件管理框架", "tldr": "TrioXpert是一个端到端的自动化事件管理框架，它利用多模态数据和LLM的协同推理机制，显著提高了微服务系统在异常检测、故障分类和根因定位方面的性能，并提供了可解释的推理证据。", "motivation": "现有微服务系统事件管理方法主要依赖单一模态数据，难以同时处理异常检测、故障分类和根因定位等多个下游任务，且缺乏清晰的推理证据，导致可解释性不足。", "method": "TrioXpert提出一个端到端的事件管理框架，充分利用多模态数据。它设计了三个独立的异构数据处理管道，从数值和文本维度全面表征微服务系统的运行状态。该框架采用大型语言模型（LLMs）的协同推理机制，同时处理多任务并提供清晰的推理证据以确保强可解释性。", "result": "在两个流行的微服务系统数据集上进行了广泛评估，实验结果表明TrioXpert在异常检测（AD）方面性能提升4.7%至57.7%，故障分类（FT）方面提升2.1%至40.6%，根因定位（RCL）方面提升1.6%至163.1%。", "conclusion": "TrioXpert通过充分利用多模态数据和LLM的协同推理机制，显著提升了微服务系统自动化事件管理在多个任务上的性能，并解决了可解释性不足的问题。", "translation": "自动化事件管理在大型微服务系统中发挥着关键作用。然而，许多现有方法仅依赖单一模态数据（例如，指标、日志和跟踪），并且难以同时处理多个下游任务，包括异常检测（AD）、故障分类（FT）和根因定位（RCL）。此外，当前技术中缺乏清晰的推理证据常常导致可解释性不足。为了解决这些限制，我们提出了TrioXpert，一个能够充分利用多模态数据的端到端事件管理框架。TrioXpert根据不同模态的内在特性设计了三个独立的数据处理管道，从数值和文本维度全面表征微服务系统的运行状态。它采用大型语言模型（LLMs）的协同推理机制，同时处理多个任务，并提供清晰的推理证据以确保强可解释性。我们在两个流行的微服务系统数据集上进行了广泛评估，实验结果表明TrioXpert在AD（提升4.7%至57.7%）、FT（提升2.1%至40.6%）和RCL（提升1.6%至163.1%）任务中取得了出色的性能。", "summary": "TrioXpert是一个针对微服务系统的自动化事件管理框架，旨在解决现有方法在处理多模态数据、同时执行多任务以及提供可解释性方面的不足。该框架通过设计独立的异构数据处理管道，并利用大型语言模型（LLMs）的协同推理机制，能够全面分析微服务系统的运行状态，并同时处理异常检测、故障分类和根因定位等任务，同时提供清晰的推理证据。实验结果表明，TrioXpert在这些任务上均取得了显著的性能提升。", "keywords": "微服务系统, 事件管理, 多模态数据, 大型语言模型, 可解释性", "comments": "该论文的创新点在于提出了一个端到端的框架TrioXpert，它能够充分利用多模态数据进行微服务系统的事件管理。特别值得关注的是，它引入了大型语言模型（LLMs）进行协同推理，这不仅提高了多任务处理能力，还显著增强了结果的可解释性，解决了现有方法的一大痛点。其在AD、FT和RCL任务上显著的性能提升也证明了其有效性和实用性。"}}
{"id": "2506.10093", "title": "Leveraging LLMs for Mission Planning in Precision Agriculture", "authors": ["Marcos Abel Zuzuárregui", "Stefano Carpin"], "summary": "Robotics and artificial intelligence hold significant potential for advancing\nprecision agriculture. While robotic systems have been successfully deployed\nfor various tasks, adapting them to perform diverse missions remains\nchallenging, particularly because end users often lack technical expertise. In\nthis paper, we present an end-to-end system that leverages large language\nmodels (LLMs), specifically ChatGPT, to enable users to assign complex data\ncollection tasks to autonomous robots using natural language instructions. To\nenhance reusability, mission plans are encoded using an existing IEEE task\nspecification standard, and are executed on robots via ROS2 nodes that bridge\nhigh-level mission descriptions with existing ROS libraries. Through extensive\nexperiments, we highlight the strengths and limitations of LLMs in this\ncontext, particularly regarding spatial reasoning and solving complex routing\nchallenges, and show how our proposed implementation overcomes them.", "comment": "Published in Proceedings of 2025 International Conference on Robotics\n  and Automation (ICRA)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10093v1", "AI": {"title_translation": "利用大型语言模型进行精准农业任务规划", "tldr": "本文提出一个端到端系统，利用大型语言模型（LLMs，特别是ChatGPT）使非技术用户能够通过自然语言指令为精准农业中的自主机器人分配复杂的任务。该系统将任务计划编码为IEEE标准，并通过ROS2执行，并探讨了LLMs在此背景下的优缺点。", "motivation": "精准农业中的终端用户通常缺乏技术专长，这使得机器人系统难以适应执行多样化的任务。本研究的动机是使非技术用户能够使用自然语言指令向自主机器人分配复杂的数据收集任务。", "method": "该论文提出了一个端到端系统，该系统利用大型语言模型（LLMs），特别是ChatGPT，将自然语言指令转换为自主机器人的复杂数据收集任务。为了提高可重用性，任务计划使用现有的IEEE任务规范标准进行编码，并通过ROS2节点在机器人上执行，这些节点将高级任务描述与现有ROS库连接起来。", "result": "通过广泛的实验，该研究强调了LLMs在此背景下的优点和局限性，特别是在空间推理和解决复杂路径规划挑战方面，并展示了所提出的实现如何克服这些局限性。", "conclusion": "本文成功展示了一个利用LLMs的系统，该系统使非技术用户能够通过自然语言控制农业机器人，并有效解决了空间推理和路径规划方面的挑战。", "translation": "机器人技术和人工智能在推动精准农业方面具有巨大潜力。虽然机器人系统已成功部署用于各种任务，但使其适应执行多样化任务仍然具有挑战性，特别是由于最终用户通常缺乏技术专长。在本文中，我们提出了一个端到端系统，该系统利用大型语言模型（LLM），特别是ChatGPT，使用户能够通过自然语言指令将复杂的数据收集任务分配给自主机器人。为了提高可重用性，任务计划使用现有的IEEE任务规范标准进行编码，并通过ROS2节点在机器人上执行，这些节点将高级任务描述与现有ROS库连接起来。通过广泛的实验，我们强调了LLM在此背景下的优点和局限性，特别是在空间推理和解决复杂路径规划挑战方面，并展示了我们提出的实现如何克服这些问题。", "summary": "本文介绍了一个端到端系统，该系统利用大型语言模型（LLMs），特别是ChatGPT，简化了精准农业中自主机器人的任务规划。它允许非技术用户使用自然语言分配复杂的数据收集任务。该系统使用IEEE标准编码任务计划以提高可重用性，并通过ROS2执行。实验证明了该系统利用LLMs的能力，同时解决了它们在空间推理和路径规划方面的局限性。", "keywords": "LLMs, 精准农业, 机器人技术, 任务规划, 自然语言", "comments": "该论文提出了一种创新方法，通过利用LLMs弥合了精准农业中复杂机器人系统与非技术终端用户之间的鸿沟。使用自然语言接口和遵循IEEE标准以实现可重用性是其主要优势。它还批判性地评估了LLMs在空间推理和路径规划方面的局限性，并提供了解决方案。"}}
{"id": "2506.10079", "title": "Cybernetic Marionette: Channeling Collective Agency Through a Wearable Robot in a Live Dancer-Robot Duet", "authors": ["Anup Sathya", "Jiasheng Li", "Zeyu Yan", "Adriane Fang", "Bill Kules", "Jonathan David Martin", "Huaishu Peng"], "summary": "We describe DANCE^2, an interactive dance performance in which audience\nmembers channel their collective agency into a dancer-robot duet by voting on\nthe behavior of a wearable robot affixed to the dancer's body. At key moments\nduring the performance, the audience is invited to either continue the\nchoreography or override it, shaping the unfolding interaction through\nreal-time collective input. While post-performance surveys revealed that\nparticipants felt their choices meaningfully influenced the performance, voting\ndata across four public performances exhibited strikingly consistent patterns.\nThis tension between what audience members do, what they feel, and what\nactually changes highlights a complex interplay between agentive behavior, the\nexperience of agency, and power. We reflect on how choreography, interaction\ndesign, and the structure of the performance mediate this relationship,\noffering a live analogy for algorithmically curated digital systems where\nagency is felt, but not exercised.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10079v1", "AI": {"title_translation": "赛博提线木偶：通过可穿戴机器人在现场舞者-机器人二重奏中引导集体能动性", "tldr": "观众通过投票控制可穿戴机器人影响舞者-机器人表演，但投票数据一致性与观众感受的能动性存在差异，揭示了能动性体验与实际影响之间的复杂关系。", "motivation": "探讨集体能动性、能动性体验与权力之间的复杂关系，并通过现场表演作为算法策展数字系统的类比，其中能动性被感知但未被真正行使。", "method": "描述了名为DANCE^2的互动舞蹈表演，观众通过投票控制附着在舞者身上的可穿戴机器人的行为，实时影响表演。通过演后调查和投票数据分析观众的体验和实际影响。", "result": "演后调查显示参与者认为他们的选择有意义地影响了表演，但四场公开表演的投票数据却呈现出惊人的一致性模式。这揭示了观众的行动、感受和实际变化之间的紧张关系。", "conclusion": "编舞、互动设计和表演结构共同调节了能动性行为、能动性体验和权力之间的关系，为算法策展的数字系统提供了一个现场类比，其中能动性被感知但未被真正行使。", "translation": "我们描述了DANCE^2，这是一场互动舞蹈表演，观众通过投票决定附着在舞者身上的可穿戴机器人的行为，从而将他们的集体能动性引导到一个舞者-机器人二重奏中。在表演的关键时刻，观众被邀请选择继续编舞或推翻它，通过实时集体输入塑造不断展开的互动。虽然表演后的调查显示参与者认为他们的选择有意义地影响了表演，但四场公开表演的投票数据却呈现出惊人的一致性模式。观众的行动、感受和实际变化之间的这种张力凸显了能动性行为、能动性体验和权力之间复杂的相互作用。我们反思了编舞、互动设计和表演结构如何调节这种关系，为算法策展的数字系统提供了一个现场类比，在这些系统中，能动性被感知但并未被行使。", "summary": "本文介绍了DANCE^2互动舞蹈表演，观众通过投票控制舞者佩戴的机器人，从而引导集体能动性。尽管观众普遍认为他们的选择影响了表演，但实际投票数据显示出高度一致性，揭示了能动性体验与实际影响之间的张力。研究反思了编舞、互动设计和表演结构如何调节这种关系，并将其作为算法策展数字系统中“能动性被感知但未被行使”现象的现场类比。", "keywords": "集体能动性, 可穿戴机器人, 互动表演, 能动性体验, 权力", "comments": "这项研究通过一个具体的艺术表演案例，深入探讨了“能动性”这一复杂概念，尤其是在集体参与和技术介导情境下的体现。其创新之处在于将艺术实践作为社会科学研究的实验平台，揭示了用户在算法驱动系统中可能面临的“被赋予能动性假象”的困境。研究结果对于理解人机交互、集体行为以及数字平台设计中的权力动态具有重要意义。"}}
{"id": "2506.10017", "title": "Design of A* based heuristic algorithm for efficient interdiction in multi-Layer networks", "authors": ["Sukanya Samanta"], "summary": "Intercepting a criminal using limited police resources presents a significant\nchallenge in dynamic crime environments, where the criminal's location\ncontinuously changes over time. The complexity is further heightened by the\nvastness of the transportation network. To tackle this problem, we propose a\nlayered graph representation, in which each time step is associated with a\nduplicate of the transportation network. For any given set of attacker\nstrategies, a near-optimal defender strategy is computed using the A-Star\nheuristic algorithm applied to the layered graph. The defender's goal is to\nmaximize the probability of successful interdiction. We evaluate the\nperformance of the proposed method by comparing it with a Mixed-Integer Linear\nProgramming (MILP) approach used for the defender. The comparison considers\nboth computational efficiency and solution quality. The results demonstrate\nthat our approach effectively addresses the complexity of the problem and\ndelivers high-quality solutions within a short computation time.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.10017v1", "AI": {"title_translation": "基于A*启发式算法的多层网络高效拦截设计", "tldr": "本文提出一种基于A*启发式算法的方法，利用分层图表示来高效拦截动态犯罪环境中的目标，并在计算效率和解决方案质量方面表现出色。", "motivation": "在动态犯罪环境中，犯罪分子位置不断变化，且交通网络庞大，使用有限警力拦截犯罪分子是一个重大挑战。", "method": "作者提出一种分层图表示方法，其中每个时间步都与交通网络的副本相关联。对于任何给定的攻击者策略，使用应用于分层图的A*启发式算法计算近最优的防御者策略，以最大化成功拦截的概率。", "result": "研究结果表明，所提出的方法有效解决了问题的复杂性，并在短时间内提供了高质量的解决方案。与混合整数线性规划（MILP）方法相比，该方法在计算效率和解决方案质量方面均表现良好。", "conclusion": "该研究提出的基于A*启发式算法的多层网络拦截方法能够高效且高质量地解决动态犯罪环境中的拦截问题。", "translation": "使用有限的警力在动态犯罪环境中拦截罪犯是一个重大挑战，因为罪犯的位置会随时间不断变化。交通网络的广阔性进一步增加了复杂性。为了解决这个问题，我们提出了一种分层图表示，其中每个时间步都与交通网络的副本相关联。对于任何给定的攻击者策略，使用应用于分层图的A*启发式算法计算出近乎最优的防御者策略。防御者的目标是最大化成功拦截的概率。我们通过将所提出的方法与用于防御者的混合整数线性规划（MILP）方法进行比较来评估其性能。比较考虑了计算效率和解决方案质量。结果表明，我们的方法有效地解决了问题的复杂性，并在短时间内提供了高质量的解决方案。", "summary": "本文针对动态犯罪环境中有限警力拦截不断变化的犯罪分子这一挑战，提出了一种基于A*启发式算法的新方法。该方法通过将交通网络表示为分层图，其中每个时间步对应一个网络副本，从而计算出近最优的防御者策略以最大化拦截成功率。实验结果表明，该方法在处理问题复杂性、计算效率和解决方案质量方面均优于传统的MILP方法。", "keywords": "A*算法, 启发式算法, 多层网络, 拦截, 动态犯罪", "comments": "该论文的创新点在于将动态拦截问题转化为分层图上的路径搜索问题，并巧妙地运用A*启发式算法进行求解，有效平衡了计算效率和解的质量。这对于实际的警务资源调度和犯罪拦截具有重要的指导意义。"}}
{"id": "2506.10111", "title": "AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components", "authors": ["Abiodun Ganiyu", "Pranshav Gajjar", "Vijay K Shah"], "summary": "The advent of Open Radio Access Networks (O-RAN) has transformed the\ntelecommunications industry by promoting interoperability, vendor diversity,\nand rapid innovation. However, its disaggregated architecture introduces\ncomplex testing challenges, particularly in validating multi-vendor components\nagainst O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as\nthose provided by Open Testing and Integration Centres (OTICs), rely heavily on\nmanual processes, are fragmented and prone to human error, leading to\ninconsistency and scalability issues. To address these limitations, we present\nAI5GTest -- an AI-powered, specification-aware testing framework designed to\nautomate the validation of O-RAN components. AI5GTest leverages a cooperative\nLarge Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and\nDebug-LLM. Gen-LLM automatically generates expected procedural flows for test\ncases based on 3GPP and O-RAN specifications, while Val-LLM cross-references\nsignaling messages against these flows to validate compliance and detect\ndeviations. If anomalies arise, Debug-LLM performs root cause analysis,\nproviding insight to the failure cause. To enhance transparency and\ntrustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the\nGen-LLM presents top-k relevant official specifications to the tester for\napproval before proceeding with validation. Evaluated using a range of test\ncases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest\ndemonstrates a significant reduction in overall test execution time compared to\ntraditional manual methods, while maintaining high validation accuracy.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10111v1", "AI": {"title_translation": "AI5GTest：AI驱动的5G O-RAN组件规范感知自动化测试与验证", "tldr": "AI5GTest是一个AI驱动的框架，利用协作式大型语言模型（LLM）自动化验证5G O-RAN组件，显著减少测试时间并保持高准确性。", "motivation": "O-RAN的分解架构引入了复杂的测试挑战，现有框架（如OTICs）严重依赖手动过程，碎片化且易于人为错误，导致不一致和可伸缩性问题。", "method": "本文提出了AI5GTest，一个AI驱动的规范感知测试框架，旨在自动化验证O-RAN组件。该框架利用一个协作式大型语言模型（LLM）框架，包括Gen-LLM、Val-LLM和Debug-LLM。Gen-LLM负责根据3GPP和O-RAN规范自动生成测试用例的预期流程；Val-LLM负责交叉引用信令消息以验证合规性并检测偏差；Debug-LLM在出现异常时执行根本原因分析。此外，AI5GTest还结合了人机协作机制，以增强透明度和可信度。", "result": "经O-RAN TIFG和WG5-IOT测试规范的测试用例评估，AI5GTest与传统手动方法相比，显著减少了总体测试执行时间，同时保持了高验证准确性。", "conclusion": "AI5GTest成功地解决了O-RAN组件测试的自动化和效率问题，通过AI驱动和规范感知的方法，提高了验证的准确性和可靠性，并有效应对了多供应商环境下的复杂性挑战。", "translation": "开放式无线接入网络 (O-RAN) 的出现通过促进互操作性、供应商多样性和快速创新，改变了电信行业。然而，其分解架构带来了复杂的测试挑战，特别是在根据 O-RAN 联盟和 3GPP 规范验证多供应商组件方面。现有框架，例如开放测试和集成中心 (OTIC) 提供的框架，严重依赖手动流程，碎片化且容易出现人为错误，导致不一致性和可扩展性问题。为了解决这些限制，我们提出了 AI5GTest——一个由 AI 驱动的、规范感知的测试框架，旨在自动化 O-RAN 组件的验证。AI5GTest 利用一个协作式大型语言模型 (LLM) 框架，该框架由 Gen-LLM、Val-LLM 和 Debug-LLM 组成。Gen-LLM 根据 3GPP 和 O-RAN 规范自动生成测试用例的预期过程流，而 Val-LLM 则根据这些流交叉引用信令消息以验证合规性并检测偏差。如果出现异常，Debug-LLM 会执行根本原因分析，提供故障原因的见解。为了提高透明度和可信度，AI5GTest 结合了人机协作机制，其中 Gen-LLM 在进行验证之前向测试人员提供前 k 个相关的官方规范以供批准。通过使用从 O-RAN TIFG 和 WG5-IOT 测试规范获得的一系列测试用例进行评估，AI5GTest 与传统手动方法相比，显著减少了总体测试执行时间，同时保持了高验证准确性。", "summary": "本文介绍了AI5GTest，一个AI驱动的规范感知测试框架，旨在自动化5G O-RAN组件的验证。该框架利用Gen-LLM、Val-LLM和Debug-LLM组成的协作式大型语言模型，分别负责生成测试流程、验证合规性及进行故障分析。AI5GTest通过人机协作机制增强透明度，并在评估中显示出相比传统手动方法显著减少测试时间并保持高准确性的优势，有效解决了O-RAN测试中的效率和准确性问题。", "keywords": "O-RAN, 5G, 自动化测试, LLM, 规范验证", "comments": "AI5GTest的创新在于将大型语言模型应用于复杂的O-RAN组件自动化测试，特别是通过Gen-LLM、Val-LLM和Debug-LLM的协同工作，实现了从测试用例生成到故障诊断的全流程自动化。引入人机协作机制提升了系统的可信度和透明度。该方法对于解决O-RAN多供应商环境下的测试复杂性和效率问题具有重要意义，是电信行业测试自动化领域的一个重要进展。"}}
{"id": "2506.10373", "title": "CarbonSet: A Dataset to Analyze Trends and Benchmark the Sustainability of CPUs and GPUs", "authors": ["Jiajun Hu", "Chetan Choppali Sudarshan", "Vidya A. Chhabria", "Aman Arora"], "summary": "Over the years, the chip industry has consistently developed high-performance\nprocessors to address the increasing demands across diverse applications.\nHowever, the rapid expansion of chip production has significantly increased\ncarbon emissions, raising critical concerns about environmental sustainability.\nWhile researchers have previously modeled the carbon footprint (CFP) at both\nsystem and processor levels, a holistic analysis of sustainability trends\nencompassing the entire chip lifecycle remains lacking. This paper presents\nCarbonSet, a comprehensive dataset integrating sustainability and performance\nmetrics for CPUs and GPUs over the past decade. CarbonSet aims to benchmark and\nassess the design of next-generation processors. Leveraging this dataset, we\nconducted detailed analysis of flagship processors' sustainability trends over\nthe last decade. This paper further highlights that modern processors are not\nyet sustainably designed, with total carbon emissions increasing more than\n50$\\times$ in the past three years due to the surging demand driven by the AI\nboom. Power efficiency remains a significant concern, while advanced process\nnodes pose new challenges requiring to effectively amortize the dramatically\nincreased manufacturing carbon emissions.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10373v1", "AI": {"title_translation": "CarbonSet：一个用于分析CPU和GPU可持续性趋势并进行基准测试的数据集", "tldr": "该论文推出了CarbonSet数据集，用于分析CPU和GPU的可持续性，并指出由于AI需求激增，现代处理器的碳排放量大幅增加，设计仍不具可持续性。", "motivation": "芯片生产迅速扩张导致碳排放显著增加，对环境可持续性构成严重关切。现有研究缺乏对涵盖整个芯片生命周期的可持续性趋势的整体分析。", "method": "本文提出了CarbonSet数据集，这是一个综合性数据集，整合了过去十年CPU和GPU的可持续性与性能指标。利用该数据集，对旗舰处理器的可持续性趋势进行了详细分析。", "result": "现代处理器设计尚不具备可持续性，过去三年由于AI需求的激增，总碳排放量增加了50多倍。功耗效率仍然是一个重大问题，先进工艺节点也带来了有效摊销大幅增加的制造碳排放的新挑战。", "conclusion": "当前处理器设计在可持续性方面存在显著不足，尤其是面对AI驱动的需求激增，碳排放量急剧上升，迫切需要改进设计以实现更好的可持续性，并有效摊销制造碳排放。", "translation": "多年来，芯片行业不断开发高性能处理器，以满足各种应用日益增长的需求。然而，芯片生产的迅速扩张显著增加了碳排放，引发了对环境可持续性的严重担忧。尽管研究人员此前已在系统和处理器层面建立了碳足迹（CFP）模型，但仍缺乏对涵盖整个芯片生命周期的可持续性趋势的整体分析。本文提出了CarbonSet，这是一个综合性数据集，整合了过去十年CPU和GPU的可持续性和性能指标。CarbonSet旨在为下一代处理器的设计提供基准和评估。利用该数据集，我们对过去十年旗舰处理器的可持续性趋势进行了详细分析。本文进一步强调，现代处理器尚未实现可持续设计，由于人工智能热潮驱动的需求激增，过去三年总碳排放量增加了50多倍。功耗效率仍然是一个重大问题，而先进工艺节点带来了新的挑战，需要有效摊销大幅增加的制造碳排放。", "summary": "本文介绍了CarbonSet数据集，该数据集整合了过去十年CPU和GPU的可持续性和性能指标，旨在弥补芯片生命周期可持续性分析的不足。利用该数据集，研究发现现代处理器设计尚不具备可持续性，尤其在AI需求驱动下，近三年总碳排放量增长超过50倍。论文强调功耗效率和先进工艺制造碳排放是未来可持续芯片设计的关键挑战。", "keywords": "CarbonSet, 可持续性, 碳排放, CPU, GPU, 人工智能", "comments": "这项研究通过构建CarbonSet数据集，首次全面整合了过去十年CPU和GPU的可持续性和性能指标，填补了芯片生命周期可持续性分析的空白，具有显著的创新性。在AI热潮导致芯片碳排放急剧增加的背景下，该研究提供了一个关键的基准工具，对推动行业可持续发展具有重要意义。"}}
{"id": "2506.10248", "title": "Resilience through Automated Adaptive Configuration for Distribution and Replication", "authors": ["Scott D. Stoller", "Balaji Jayasankar", "Yanhong A. Liu"], "summary": "This paper presents a powerful automated framework for making complex systems\nresilient under failures, by optimized adaptive distribution and replication of\ninterdependent software components across heterogeneous hardware components\nwith widely varying capabilities. A configuration specifies how software is\ndistributed and replicated: which software components to run on each computer,\nwhich software components to replicate, which replication protocols to use,\netc. We present an algorithm that, given a system model and resilience\nrequirements, (1) determines initial configurations of the system that are\nresilient, and (2) generates a reconfiguration policy that determines\nreconfiguration actions to execute in response to failures and recoveries. This\nmodel-finding algorithm is based on state-space exploration and incorporates\npowerful optimizations, including a quotient reduction based on a novel\nequivalence relation between states. We present experimental results from\nsuccessfully applying a prototype implementation of our framework to a model of\nan autonomous driving system.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10248v1", "AI": {"title_translation": "通过自动化自适应配置实现分布式和复制系统的弹性", "tldr": "本文提出一个自动化框架，通过优化软件组件在异构硬件上的自适应分布和复制，增强复杂系统在故障下的弹性。", "motivation": "为了使复杂系统在面对故障时仍能保持弹性，需要一个强大的自动化框架来优化互依赖软件组件在异构硬件组件上的自适应分布和复制。", "method": "本文提出了一个基于状态空间探索的模型查找算法，该算法能够根据系统模型和弹性要求，确定初始的弹性配置并生成应对故障和恢复的重配置策略。该算法包含了基于状态之间新颖等价关系的商约简等优化技术。", "result": "研究人员成功地将该框架的原型实现在一个自动驾驶系统模型上，并获得了实验结果。", "conclusion": "该自动化框架通过智能地确定初始弹性配置和动态生成重配置策略，有效提升了复杂系统在面对故障时的弹性。", "translation": "本文提出了一个强大的自动化框架，通过优化互依赖软件组件在具有广泛不同能力的异构硬件组件上的自适应分布和复制，使复杂系统在故障下具有弹性。一个配置指定了软件如何分布和复制：哪些软件组件在每台计算机上运行，哪些软件组件需要复制，使用哪种复制协议等。我们提出了一种算法，给定一个系统模型和弹性要求，该算法（1）确定系统初始的弹性配置，以及（2）生成一个重配置策略，该策略确定响应故障和恢复时要执行的重配置操作。这种模型查找算法基于状态空间探索，并结合了强大的优化，包括基于状态之间新颖等价关系的商约简。我们展示了将我们框架的原型实现成功应用于自动驾驶系统模型的实验结果。", "summary": "本文介绍了一个自动化框架，旨在通过对互依赖软件组件在异构硬件上的优化自适应分布和复制，提升复杂系统在故障下的弹性。该框架包含一个模型查找算法，该算法能够确定系统的初始弹性配置，并生成应对故障和恢复的重配置策略。此算法利用状态空间探索和如商约简等优化技术。研究通过将原型应用于自动驾驶系统模型验证了其有效性。", "keywords": "弹性, 自动化配置, 分布式系统, 复制, 故障恢复", "comments": "这篇论文的创新点在于其自动化框架，特别是模型查找算法，它不仅能确定初始的弹性配置，还能动态生成重配置策略以应对故障，这对于复杂分布式系统的可靠性至关重要。其引入的基于新颖等价关系的商约简优化，有望显著提高算法的效率。"}}
{"id": "2506.10170", "title": "Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot", "authors": ["Anh T. Nguyen", "Ajay Anand", "Michelle J. Johnson"], "summary": "Action observation (AO) therapy is a promising rehabilitative treatment for\nmotor and language function in individuals recovering from neurological\nconditions, such as stroke. This pilot study aimed to investigate the potential\nof humanoid robots to support AO therapy in rehabilitation settings. The brain\nactivity of three healthy right-handed participants was monitored with\nelectroencephalography (EEG) while they observed eight different actions\nperformed by two agents, a human actor and a robot, using their left and right\narms. Their event-related spectral perturbations (ERSPs, changes in the\nspectral power of neural oscillations in response to an event or stimulus,\ncompared to baseline) in sensorimotor regions were analyzed. The single-subject\nanalysis showed variability in ERSP patterns among all participants, including\npower suppression in sensorimotor mu and beta rhythms. One participant showed\nstronger responses to \"robot\" AO conditions than to \"human\" conditions. Strong\nand positive correlations in ERSP across all conditions were observed for\nalmost all participants and channels, implying common cognitive processes or\nneural networks at play in the mirror neuron system during AO. The results\nsupport the feasibility of using EEG to explore differences in neural responses\nto observation of robot- and human-induced actions.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10170v1", "AI": {"title_translation": "探索观察人类演员和类人机器人执行动作时的脑电图反应", "tldr": "本初步研究旨在评估类人机器人在动作观察（AO）疗法中的应用潜力，通过EEG监测健康参与者观察人类和机器人动作时的脑活动。结果显示个体间ERSP模式存在差异，但大多数人对两种观察条件下的ERSP存在强相关性，支持EEG在区分对机器人和人类动作观察的神经反应方面的可行性。", "motivation": "动作观察（AO）疗法对神经系统疾病（如中风）患者的运动和语言功能康复有前景。本研究旨在探讨类人机器人在康复环境中支持AO疗法的潜力。", "method": "招募了三名健康的右撇子参与者。使用脑电图（EEG）监测他们在观察人类演员和机器人使用左右手臂执行的八种不同动作时的脑活动。分析了感觉运动区域的事件相关谱扰动（ERSPs）。", "result": "单受试者分析显示所有参与者的ERSP模式存在变异性，包括感觉运动mu和beta节律的功率抑制。一名参与者对“机器人”AO条件的反应比“人类”条件更强。几乎所有参与者和通道在所有条件下都观察到ERSP的强正相关，暗示了在动作观察过程中镜像神经元系统中存在共同的认知过程或神经网络。", "conclusion": "结果支持使用EEG探索观察机器人和人类诱导动作之间神经反应差异的可行性。", "translation": "动作观察（AO）疗法是一种有前景的康复治疗方法，用于改善中风等神经系统疾病患者的运动和语言功能。这项初步研究旨在调查类人机器人在康复环境中支持AO疗法的潜力。研究通过脑电图（EEG）监测了三名健康的右撇子参与者的大脑活动，他们观察了人类演员和机器人使用左右手臂执行的八种不同动作。分析了他们感觉运动区域的事件相关谱扰动（ERSPs，即神经振荡光谱功率相对于基线的变化，以响应事件或刺激）。单受试者分析显示，所有参与者的ERSP模式存在变异性，包括感觉运动mu和beta节律的功率抑制。其中一名参与者对“机器人”AO条件的反应比“人类”条件更强。几乎所有参与者和通道在所有条件下都观察到ERSP的强正相关，这暗示了在动作观察过程中镜像神经元系统中存在共同的认知过程或神经网络。结果支持使用脑电图探索观察机器人和人类诱导动作之间神经反应差异的可行性。", "summary": "本初步研究旨在评估类人机器人在动作观察（AO）疗法中的应用潜力。研究使用EEG记录了三名健康参与者在观察人类和机器人执行动作时的脑活动，并分析了感觉运动区域的事件相关谱扰动（ERSPs）。结果显示参与者间ERSP模式存在个体差异，但多数参与者对两种观察条件下的ERSP表现出强相关性，表明镜像神经元系统存在共同的神经过程。研究支持EEG在区分对机器人和人类动作观察的神经反应方面的可行性。", "keywords": "动作观察疗法, 类人机器人, 脑电图, 事件相关谱扰动, 镜像神经元系统", "comments": "这是一项初步研究，样本量较小（三名参与者），因此结果的普遍性可能有限。然而，它为将类人机器人纳入动作观察疗法提供了初步证据，并验证了使用EEG作为评估工具的可行性。未来需要更大规模的研究来证实这些发现并深入探索机器人作为治疗辅助工具的有效性。"}}
{"id": "2506.10377", "title": "Chance and Mass Interpretations of Probabilities in Markov Decision Processes (Extended Version)", "authors": ["Yun Chen Tsai", "Kittiphon Phalakarn", "S. Akshay", "Ichiro Hasuo"], "summary": "Markov decision processes (MDPs) are a popular model for decision-making in\nthe presence of uncertainty. The conventional view of MDPs in verification\ntreats them as state transformers with probabilities defined over sequences of\nstates and with schedulers making random choices. An alternative view,\nespecially well-suited for modeling dynamical systems, defines MDPs as\ndistribution transformers with schedulers distributing probability masses. Our\nmain contribution is a unified semantical framework that accommodates these two\nviews and two new ones. These four semantics of MDPs arise naturally through\nidentifying different sources of randomness in an MDP (namely schedulers,\nconfigurations, and transitions) and providing different ways of interpreting\nthese probabilities (called the chance and mass interpretations). These\nsemantics are systematically unified through a mathematical construct called\nchance-mass (CM) classifier. As another main contribution, we study a\nreachability problem in each of the two new semantics, demonstrating their\nhardness and providing two algorithms for solving them.", "comment": null, "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.10377v1", "AI": {"title_translation": "马尔可夫决策过程中概率的几率和质量解释（扩展版）", "tldr": "本文提出了一个统一的语义框架，整合了马尔可夫决策过程（MDPs）中概率的两种现有解释以及两种新解释，并通过研究可达性问题来验证新语义。", "motivation": "传统的马尔可夫决策过程（MDPs）在验证中被视为状态转换器，其概率定义在状态序列上，调度器进行随机选择。另一种观点将MDPs定义为分布转换器，调度器分配概率质量。本文的动机是提供一个统一的语义框架来容纳这两种现有观点以及两种新的观点。", "method": "本文通过识别MDP中不同的随机性来源（调度器、配置和转换）并提供不同的概率解释方式（几率和质量解释），自然地产生了四种MDP语义。这些语义通过一个称为几率-质量（CM）分类器的数学构造系统地统一起来。此外，本文研究了两种新语义中的可达性问题，并提供了两种解决算法。", "result": "本文统一了马尔可夫决策过程（MDPs）的四种语义，并通过几率-质量（CM）分类器实现了系统性统一。对于两种新语义中的可达性问题，本文证明了其困难性并提出了两种相应的解决算法。", "conclusion": "本文成功提出了一个统一的语义框架，整合了马尔可夫决策过程（MDPs）中概率的多种解释方式，并通过CM分类器提供了系统的数学统一。此外，本文还为新语义中的可达性问题提供了解决方案。", "translation": "马尔可夫决策过程（MDPs）是存在不确定性时进行决策的流行模型。验证中MDPs的传统观点将其视为状态转换器，其概率定义在状态序列上，调度器进行随机选择。另一种观点，特别适合建模动态系统，将MDPs定义为分布转换器，调度器分配概率质量。我们的主要贡献是一个统一的语义框架，该框架容纳了这两种观点以及两种新的观点。MDPs的这四种语义通过识别MDP中不同的随机性来源（即调度器、配置和转换）并提供不同的概率解释方式（称为几率和质量解释）自然产生。这些语义通过一个称为几率-质量（CM）分类器的数学构造系统地统一起来。作为另一个主要贡献，我们研究了两种新语义中的可达性问题，证明了它们的困难性并提供了两种解决算法。", "summary": "本文提出了一种统一的语义框架，整合了马尔可夫决策过程（MDPs）中概率的四种解释，包括传统的基于状态转换的观点、基于分布转换的观点以及两种新的观点。这些解释源于对随机性来源（调度器、配置、转换）和概率解释方式（几率、质量）的不同识别，并通过一个名为几率-质量（CM）分类器的数学构造实现系统统一。此外，论文还研究了在新语义中的可达性问题，证明了其困难性并提出了相应的解决算法。", "keywords": "马尔可夫决策过程, 概率解释, 统一语义框架, 几率-质量分类器, 可达性问题", "comments": "本文的创新之处在于提出了一个统一的语义框架，系统地整合了马尔可夫决策过程（MDPs）中概率的多种解释，并通过引入“几率-质量（CM）分类器”这一数学构造，为MDPs的语义分析提供了新的视角和工具。这对于深化对MDPs的理解，尤其是在处理不同随机性来源和概率解释场景下具有重要意义。此外，对新语义中可达性问题的研究及其算法的提出，也体现了理论与实践相结合的价值。"}}
{"id": "2506.10057", "title": "Inverted Classroom in der Einführungsveranstaltung Programmierung", "authors": ["Ulrich von Zadow", "Natalie Kiesler"], "summary": "Traditionally, the introductory programming course for computer science\nstudents at Nuremberg Tech had been implemented as a combination of lectures\nand exercise sessions. Due to high failure rates in the winter semester\n2023/24, an experimental teaching concept based on the inverted classroom was\nimplemented for one cohort in the winter semester 2024/25. Students had to\nprepare themselves through literature work and activating teaching and learning\nmethods. The course was accompanied by a series of data collections (i.e., a\nTeaching Analysis Poll, two surveys, and a teaching diary) to gain insights\ninto students' learning methods and behaviors. The concept was evaluated\npositively overall, although many detailed opportunities for improvement were\nidentified. In this article, we document the results of the surveys and discuss\nthe implications", "comment": "10 pages, in German language, 4 figures, MINT Symposium 2025", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10057v1", "AI": {"title_translation": "编程入门课程中的翻转课堂", "tldr": "由于高挂科率，纽伦堡理工学院在2024/25冬季学期对计算机科学入门编程课程试行了翻转课堂教学模式，通过数据收集评估发现总体效果积极但仍有改进空间。", "motivation": "由于纽伦堡理工学院计算机科学专业学生在2023/24冬季学期的入门编程课程中挂科率高，因此需要探索新的教学概念来改善学习效果。", "method": "研究团队在2024/25冬季学期对一个学生群体实施了基于翻转课堂的实验性教学概念。学生需通过文献阅读和激活式教学方法进行预习。课程期间通过教学分析调查、两次问卷调查和教学日记等方式收集数据，以了解学生的学习方法和行为。", "result": "该教学概念总体上获得了积极评价，但同时也发现许多具体的改进机会。文章记录了问卷调查结果。", "conclusion": "翻转课堂概念在编程入门课程中显示出积极效果，但仍有详细的改进空间，文章将讨论其影响。", "translation": "传统上，纽伦堡理工学院计算机科学学生的入门编程课程采用讲座和练习相结合的形式。由于2023/24冬季学期的挂科率较高，2024/25冬季学期对一个班级实施了基于翻转课堂的实验性教学概念。学生需要通过文献工作和激活式教学方法进行自我准备。课程期间通过一系列数据收集（即教学分析调查、两次问卷调查和教学日记）来深入了解学生的学习方法和行为。该概念总体上获得了积极评价，尽管也发现了许多详细的改进机会。本文记录了问卷调查的结果并讨论了其影响。", "summary": "本研究针对纽伦堡理工学院计算机科学入门编程课程高挂科率的问题，在2024/25冬季学期试行了翻转课堂教学模式。该模式要求学生课前通过文献和主动学习方式进行准备，并通过多种数据收集方法（教学分析调查、问卷、教学日记）评估其效果。结果显示该概念总体积极，但仍有提升空间，文章将详细记录问卷调查结果并讨论其启示。", "keywords": "翻转课堂, 编程教育, 教学评估, 高等教育, 挂科率", "comments": "该研究通过实际教学案例验证了翻转课堂在解决传统教学模式高挂科率问题上的潜力，其创新在于将翻转课堂引入编程入门课程，并辅以多维度的数据收集进行评估。研究的价值在于为高等教育编程教学改革提供了实践经验和数据支持。然而，抽象中未详细说明具体的改进机会，也未提及是否与传统教学模式进行对比，这可能是其局限性。"}}
{"id": "2506.10000", "title": "A Survey of Data Compression Algorithms and their Applications", "authors": ["Mohammad Hosseini"], "summary": "Today, with the growing demands of information storage and data transfer,\ndata compression is becoming increasingly important. Data Compression is a\ntechnique which is used to decrease the size of data. This is very useful when\nsome huge files have to be transferred over networks or being stored on a data\nstorage device and the size is more than the capacity of the data storage or\nwould consume so much bandwidth for transmission in a network. With the advent\nof the Internet and mobile devices with limited resources, data compression has\ngained even more importance. It can be effectively used to save both storage\nand bandwidth, thus to decrease download duration. Data compression can be\nachieved by a host of techniques. During this survey, I'm going to thoroughly\ndiscuss some of important data compression algorithms, their performance\nevaluation, and their major applications along with today's issues and recent\nresearch approaches.", "comment": "Network Systems Lab, Simon Fraser University, 2012", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10000v1", "AI": {"title_translation": "数据压缩算法及其应用综述", "tldr": "本文综述了数据压缩算法、其性能评估、主要应用以及当前问题和最新研究方法。", "motivation": "随着信息存储和数据传输需求的增长，以及互联网和移动设备资源有限的出现，数据压缩在节省存储和带宽方面变得越来越重要。", "method": "本文对重要的数据压缩算法、其性能评估、主要应用以及当前面临的问题和最近的研究方法进行了全面讨论和综述。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "如今，随着信息存储和数据传输需求的不断增长，数据压缩变得越来越重要。数据压缩是一种用于减小数据大小的技术。当一些巨大的文件必须通过网络传输或存储在数据存储设备上，并且其大小超出数据存储容量或在网络传输中会消耗大量带宽时，这项技术非常有用。随着互联网和资源有限的移动设备的出现，数据压缩变得更加重要。它可以有效地节省存储和带宽，从而缩短下载时间。数据压缩可以通过多种技术实现。在本次调查中，我将彻底讨论一些重要的数据压缩算法、它们的性能评估、主要应用以及当今的问题和最新的研究方法。", "summary": "本文旨在综述数据压缩算法及其应用。鉴于当前信息存储和数据传输需求的增长，以及互联网和移动设备资源限制的背景，数据压缩在节省存储和带宽方面的重要性日益凸显。该综述将深入探讨重要的数据压缩算法、它们的性能评估、主要应用，并讨论当前面临的问题和最新的研究方法。", "keywords": "数据压缩, 算法, 应用, 性能评估, 综述", "comments": "这是一篇综述性论文，旨在全面梳理数据压缩领域的重要算法、应用、性能及最新研究进展，对于理解该领域的基础和前沿具有参考价值。"}}
{"id": "2506.10203", "title": "Formalizing Neuromorphic Control Systems: A General Proposal and A Rhythmic Case Study", "authors": ["Taisia Medvedeva", "Alessio Franci", "Fernando Castaños"], "summary": "Neuromorphic control is receiving growing attention due to the multifaceted\nadvantages it brings over more classical control approaches, including: sparse\nand on-demand sensing, information transmission, and actuation;\nenergy-efficient designs and realizations in neuromorphic hardware; event-based\nsignal processing and control signal computation. However, a general\ncontrol-theoretical formalization of what \"neuromorphic control systems\" are\nand how we can rigorously analyze, design, and control them is still largely\nmissing. In this note, we suggest a possible path toward formalizing\nneuromorphic control systems. We apply the proposed framework to a rhythmic\ncontrol case study and rigorously show how it has the potential to make\nneuromorphic control systems analysis and design amenable to mature control\ntheoretical approaches like describing function analysis and harmonic balance,\nfast-slow analysis, discrete and hybrid systems, and robust optimization.", "comment": "Submitted to the 64th IEEE Conference on Decision and Control", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10203v1", "AI": {"title_translation": "形式化神经形态控制系统：一种通用提案与一个节律案例研究", "tldr": "本文提出了一种形式化神经形态控制系统的方法，并通过一个节律控制案例研究验证了其潜力，使其能够采用成熟的控制理论进行分析和设计。", "motivation": "神经形态控制系统虽然具有稀疏传感、高效能、事件驱动等优点，但目前缺乏对其的通用控制理论形式化，导致难以严格分析、设计和控制。", "method": "论文提出了一种可能的形式化神经形态控制系统的通用框架，并将其应用于一个节律控制案例研究。", "result": "通过将所提出的框架应用于节律控制案例研究，论文严格展示了它如何能够使神经形态控制系统的分析和设计适用于描述函数分析、谐波平衡、快慢分析、离散和混合系统以及鲁棒优化等成熟的控制理论方法。", "conclusion": "所提出的形式化方法有望弥补神经形态控制系统在严格分析和设计方面的不足，使其能够利用现有成熟的控制理论工具，从而促进该领域的发展。", "translation": "神经形态控制因其相对于经典控制方法的多方面优势而受到越来越多的关注，这些优势包括：稀疏和按需传感、信息传输和执行；神经形态硬件中的节能设计和实现；以及基于事件的信号处理和控制信号计算。然而，关于“神经形态控制系统”是什么以及如何严格分析、设计和控制它们的通用控制理论形式化仍然 largely 缺失。在本说明中，我们提出了一种可能的形式化神经形态控制系统的路径。我们将所提出的框架应用于一个节律控制案例研究，并严格展示了它如何有可能使神经形态控制系统的分析和设计适用于描述函数分析和谐波平衡、快慢分析、离散和混合系统以及鲁棒优化等成熟的控制理论方法。", "summary": "本文提出了一种通用框架，旨在形式化神经形态控制系统，以解决当前缺乏严格分析和设计方法的挑战。通过一个节律控制案例研究，论文证明了该框架能够将神经形态控制系统的分析与设计与描述函数分析、谐波平衡、快慢分析、离散和混合系统以及鲁棒优化等成熟的控制理论方法相结合。", "keywords": "神经形态控制, 形式化, 节律控制, 控制理论, 系统分析", "comments": "本文的创新点在于提出了一种通用框架来形式化神经形态控制系统，填补了该领域在理论严谨性方面的空白。其重要性在于，通过将神经形态控制与成熟的控制理论相结合，有望加速该领域的发展，使其设计和分析更加系统化和可靠。"}}
{"id": "2506.10107", "title": "Deep Semantic Segmentation for Multi-Source Localization Using Angle of Arrival Measurements", "authors": ["Mustafa Atahan Nuhoglu", "Hakan Ali Cirpan"], "summary": "This paper presents a solution for multi source localization using only angle\nof arrival measurements. The receiver platform is in motion, while the sources\nare assumed to be stationary. Although numerous methods exist for single source\nlocalization, many relying on pseudo-linear formulations or non convex\noptimization techniques, there remains a significant gap in research addressing\nmulti source localization in dynamic environments. To bridge this gap, we\npropose a deep learning-based framework that leverages semantic segmentation\nmodels for multi source localization. Specifically, we employ UNet and UNetPP\nas backbone models, processing input images that encode the platform's\npositions along with the corresponding direction finding lines at each\nposition. By analyzing the intersections of these lines, the models effectively\nidentify and localize multiple sources. Through simulations, we evaluate both\nsingle- and multi-source localization scenarios. Our results demonstrate that\nwhile the proposed approach performs comparably to traditional methods in\nsingle source localization, it achieves accurate source localization even in\nchallenging conditions with high noise levels and an increased number of\nsources.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10107v1", "AI": {"title_translation": "深度语义分割在基于到达角测量的多源定位中的应用", "tldr": "本文提出了一种基于深度学习语义分割（使用UNet和UNetPP）的方法，用于在接收平台移动、信源静止的动态环境下，仅通过到达角测量进行多源定位。", "motivation": "现有单源定位方法众多，但针对动态环境下多源定位的研究存在显著空白，且传统方法可能不适用于高噪声和多源复杂条件。", "method": "提出一个基于深度学习的框架，利用语义分割模型（具体为UNet和UNetPP）进行多源定位。输入图像编码了平台位置和对应的测向线，模型通过分析这些线的交点来识别和定位多个信源。", "result": "在单源定位方面与传统方法表现相当；在多源定位方面，即使在高噪声和信源数量增加的挑战性条件下，也能实现准确的信源定位。", "conclusion": "提出的深度学习方法能有效解决动态环境下的多源定位问题，尤其在高噪声和多源场景下表现出色。", "translation": "本文提出了一种仅使用到达角测量进行多源定位的解决方案。接收平台处于运动状态，而信源假定为静止。尽管存在许多用于单源定位的方法，其中许多依赖伪线性公式或非凸优化技术，但在动态环境下解决多源定位的研究仍存在显著空白。为了弥补这一空白，我们提出了一种基于深度学习的框架，该框架利用语义分割模型进行多源定位。具体而言，我们采用UNet和UNetPP作为骨干模型，处理编码了平台位置以及每个位置相应测向线的输入图像。通过分析这些线的交点，模型有效地识别并定位多个信源。通过仿真，我们评估了单源和多源定位场景。我们的结果表明，虽然所提出的方法在单源定位中与传统方法表现相当，但即使在高噪声水平和信源数量增加的挑战性条件下，它也能实现准确的信源定位。", "summary": "本文针对动态环境下仅利用到达角测量进行多源定位的挑战，提出了一种创新的深度学习框架。该框架采用UNet和UNetPP等语义分割模型，将平台位置和测向线编码为输入图像，并通过分析这些线的交点来精确识别和定位多个静止信源。仿真结果表明，该方法在单源定位上与传统方法相当，但在高噪声和多源复杂场景下展现出更优越的定位精度。", "keywords": "多源定位, 到达角测量, 深度学习, 语义分割, UNetPP", "comments": "这篇论文的创新点在于将深度学习中的语义分割技术引入多源定位问题，有效地解决了传统方法在高噪声和多源动态环境下定位精度不足的痛点。利用图像编码和交点分析的思路新颖，为复杂环境下的定位提供了新的解决方案，具有重要的研究价值和应用潜力。"}}
{"id": "2506.10289", "title": "RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding", "authors": ["Yisi Liu", "Chenyang Wang", "Hanjo Kim", "Raniya Khan", "Gopala Anumanchipalli"], "summary": "Voice conversion has emerged as a pivotal technology in numerous applications\nranging from assistive communication to entertainment. In this paper, we\npresent RT-VC, a zero-shot real-time voice conversion system that delivers\nultra-low latency and high-quality performance. Our approach leverages an\narticulatory feature space to naturally disentangle content and speaker\ncharacteristics, facilitating more robust and interpretable voice\ntransformations. Additionally, the integration of differentiable digital signal\nprocessing (DDSP) enables efficient vocoding directly from articulatory\nfeatures, significantly reducing conversion latency. Experimental evaluations\ndemonstrate that, while maintaining synthesis quality comparable to the current\nstate-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms,\nrepresenting a 13.3\\% reduction in latency.", "comment": "ACL Demo Track 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10289v1", "AI": {"title_translation": "RT-VC：基于语音发音编码的实时零样本语音转换", "tldr": "RT-VC是一个实时零样本语音转换系统，利用发音特征和可微分数字信号处理实现低延迟和高质量的语音转换，并在保持与SOTA相当的合成质量下，将CPU延迟降低了13.3%。", "motivation": "语音转换已成为辅助通信和娱乐等众多应用中的关键技术。当前研究旨在实现超低延迟和高质量的实时零样本语音转换。", "method": "RT-VC系统利用发音特征空间自然地解耦内容和说话人特征，以实现更鲁棒和可解释的语音转换。此外，它集成了可微分数字信号处理（DDSP），可以直接从发音特征高效地进行声码，显著减少了转换延迟。", "result": "实验评估表明，RT-VC在保持与当前最先进（SOTA）方法相当的合成质量的同时，实现了61.4毫秒的CPU延迟，延迟降低了13.3%。", "conclusion": "RT-VC成功地实现了一个实时零样本语音转换系统，该系统具有超低延迟和高质量性能，通过利用发音特征和可微分数字信号处理，显著提升了语音转换的效率和鲁棒性。", "translation": "语音转换已成为辅助通信到娱乐等众多应用中的关键技术。在本文中，我们提出了RT-VC，一个零样本实时语音转换系统，它提供了超低延迟和高质量的性能。我们的方法利用发音特征空间来自然地解耦内容和说话人特性，从而促进更鲁棒和可解释的语音转换。此外，可微分数字信号处理（DDSP）的集成使得可以直接从发音特征高效地进行声码，显著减少了转换延迟。实验评估表明，在保持与当前最先进（SOTA）方法相当的合成质量的同时，RT-VC实现了61.4毫秒的CPU延迟，代表了13.3%的延迟降低。", "summary": "RT-VC是一种新型的实时零样本语音转换系统，旨在实现超低延迟和高质量的语音转换。该系统通过利用发音特征空间来有效解耦语音内容和说话人特性，并结合可微分数字信号处理（DDSP）进行高效声码，从而显著降低了转换延迟。实验结果显示，RT-VC在保持与现有先进技术相当的合成质量的同时，将CPU延迟降低了13.3%。", "keywords": "语音转换, 实时, 零样本, 发音编码, 低延迟", "comments": "本文提出了一种创新的实时零样本语音转换方法，通过利用发音特征空间和集成可微分数字信号处理，有效地解决了语音转换中的延迟和质量问题。其关键创新在于将内容和说话人特征在发音空间中解耦，以及高效的声码技术，这对于实时应用具有重要意义。性能提升的数据（13.3%的延迟降低）也具体而有说服力。"}}
{"id": "2506.10130", "title": "A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI", "authors": ["Luciano Floridi"], "summary": "This article introduces a conjecture that formalises a fundamental trade-off\nbetween provable correctness and broad data-mapping capacity in Artificial\nIntelligence (AI) systems. When an AI system is engineered for deductively\nwatertight guarantees (demonstrable certainty about the error-free nature of\nits outputs) -- as in classical symbolic AI -- its operational domain must be\nnarrowly circumscribed and pre-structured. Conversely, a system that can input\nhigh-dimensional data to produce rich information outputs -- as in contemporary\ngenerative models -- necessarily relinquishes the possibility of zero-error\nperformance, incurring an irreducible risk of errors or misclassification. By\nmaking this previously implicit trade-off explicit and open to rigorous\nverification, the conjecture significantly reframes both engineering ambitions\nand philosophical expectations for AI. After reviewing the historical\nmotivations for this tension, the article states the conjecture in\ninformation-theoretic form and contextualises it within broader debates in\nepistemology, formal verification, and the philosophy of technology. It then\noffers an analysis of its implications and consequences, drawing on notions of\nunderdetermination, prudent epistemic risk, and moral responsibility. The\ndiscussion clarifies how, if correct, the conjecture would help reshape\nevaluation standards, governance frameworks, and hybrid system design. The\nconclusion underscores the importance of eventually proving or refuting the\ninequality for the future of trustworthy AI.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10130v1", "AI": {"title_translation": "符号AI和生成式AI中确定性与范围之间基本权衡的猜想", "tldr": "本文提出了一个猜想，即AI系统中可证明的正确性与广泛数据映射能力之间存在根本性权衡：符号AI追求确定性但范围受限，而生成式AI范围广但无法保证零错误。", "motivation": "明确并形式化人工智能系统中可证明的正确性与广泛数据映射能力之间存在的此前隐性的基本权衡，以重塑AI的工程目标和哲学期望。", "method": "通过引入并形式化一个关于确定性与范围之间权衡的猜想，将其置于信息论框架内，并结合认识论、形式验证和技术哲学等领域进行探讨。文章还分析了其影响和后果，并讨论了其对评估标准、治理框架和混合系统设计的影响。", "result": "提出了一个关于AI系统中可证明的正确性与广泛数据映射能力之间基本权衡的猜想，并阐明了如果该猜想成立，将如何重塑AI的评估标准、治理框架和混合系统设计。", "conclusion": "强调了最终证明或驳斥该不等式对于未来可信赖AI发展的重要性。", "translation": "本文提出了一个猜想，旨在形式化人工智能（AI）系统中可证明的正确性与广泛数据映射能力之间存在的根本性权衡。当一个AI系统被设计为具有演绎上严密的保证（对其输出无错误的证明确定性）——如在经典符号AI中——其操作领域必须被狭窄地限定和预先结构化。相反，一个能够输入高维数据以产生丰富信息输出的系统——如当代生成模型——必然放弃零错误性能的可能性，从而承担不可避免的错误或误分类风险。通过使这种先前隐含的权衡变得明确并可进行严格验证，该猜想显著地重塑了AI的工程抱负和哲学期望。在回顾了这种张力的历史动机之后，文章以信息论形式阐述了该猜想，并将其置于认识论、形式验证和技术哲学的更广泛辩论中。随后，文章利用欠确定性、审慎的认知风险和道德责任等概念，分析了其影响和后果。讨论阐明了如果该猜想正确，它将如何帮助重塑评估标准、治理框架和混合系统设计。结论强调了最终证明或驳斥该不等式对于未来可信赖AI的重要性。", "summary": "本文提出了一个关于符号AI和生成式AI之间核心权衡的猜想，即AI系统在追求可证明的零错误（确定性）时，其操作范围必然受限，如同经典符号AI；而当系统能够处理高维数据并产生丰富输出（范围广）时，如当代生成模型，则不可避免地存在错误风险。这一猜想旨在明确并形式化这一基本权衡，从而影响AI的工程设计、评估标准、治理和哲学期望。文章从信息论角度阐述了该猜想，并探讨了其在认识论、形式验证和技术哲学中的深远影响，强调了对其进行验证的重要性，以指导未来可信赖AI的发展。", "keywords": "确定性, 范围, 符号AI, 生成式AI, 权衡, 可信赖AI", "comments": "这篇论文提出了一个具有深远哲学和工程意义的猜想，它明确了AI设计中长期存在的确定性与范围之间的内在矛盾。如果这一猜想得到验证，将对AI的评估标准、治理策略以及混合AI系统的设计产生革命性影响，对于构建可信赖的AI至关重要。其创新之处在于将这一隐性权衡形式化，并将其置于信息论和哲学框架下进行探讨。"}}
{"id": "2506.10339", "title": "New Approximation Guarantees for The Inventory Staggering Problem", "authors": ["Noga Alon", "Danny Segev"], "summary": "Since its inception in the mid-60s, the inventory staggering problem has been\nexplored and exploited in a wide range of application domains, such as\nproduction planning, stock control systems, warehousing, and aerospace/defense\nlogistics. However, even with a rich history of academic focus, we are still\nvery much in the dark when it comes to cornerstone computational questions\naround inventory staggering and to related structural characterizations, with\nour methodological toolbox being severely under-stocked.\n  The central contribution of this paper consists in devising a host of\nalgorithmic techniques and analytical ideas -- some being entirely novel and\nsome leveraging well-studied concepts in combinatorics and number theory -- for\nsurpassing essentially all known approximation guarantees for the inventory\nstaggering problem. In particular, our work demonstrates that numerous\nstructural properties open the door for designing polynomial-time approximation\nschemes, including polynomially-bounded cycle lengths, constantly-many distinct\ntime intervals, so-called nested instances, and pairwise coprime settings.\nThese findings offer substantial improvements over currently available\nconstant-factor approximations and resolve outstanding open questions in their\nrespective contexts. In parallel, we develop new theory around a number of\nyet-uncharted questions, related to the sampling complexity of peak inventory\nestimation as well as to the plausibility of groupwise synchronization.\nInterestingly, we establish the global nature of inventory staggering, proving\nthat there are $n$-item instances where, for every subset of roughly $\\sqrt{n}$\nitems, no policy improves on the worst-possible one by a factor greater than\n$1+\\epsilon$, whereas for the entire instance, there exists a policy that\noutperforms the worst-possible one by a factor of nearly $2$, which is optimal.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.10339v1", "AI": {"title_translation": "库存交错问题的新近似保证", "tldr": "本文通过新颖的算法技术和分析思想，显著改进了库存交错问题的近似保证，解决了悬而未决的问题，并揭示了该问题的全局特性。", "motivation": "库存交错问题自20世纪60年代中期以来在生产计划、库存控制、仓储和航空航天/国防物流等多个应用领域得到了广泛探索和利用。然而，尽管有丰富的学术关注历史，但在库存交错问题的核心计算问题及相关结构特征方面，我们仍然知之甚少，现有方法论工具严重不足。", "method": "本文的核心贡献在于设计了一系列算法技术和分析思想，其中一些是全新的，另一些则利用了组合学和数论中已深入研究的概念，以超越库存交错问题几乎所有已知的近似保证。具体而言，工作展示了许多结构特性为设计多项式时间近似方案提供了可能性，包括多项式有界循环长度、恒定数量的不同时间间隔、所谓的嵌套实例以及成对互质设置。同时，本文还围绕一些尚未探索的问题开发了新理论，涉及峰值库存估计的采样复杂性以及群组同步的可行性。", "result": "本文的工作超越了库存交错问题几乎所有已知的近似保证。研究表明，许多结构特性（如多项式有界循环长度、恒定数量的不同时间间隔、嵌套实例和成对互质设置）为设计多项式时间近似方案提供了可能性。这些发现对现有常数因子近似值提供了实质性改进，并解决了各自领域中悬而未决的开放问题。此外，本文建立了库存交错问题的全局性质，证明了存在n个项目的实例，其中对于大约√n个项目的每个子集，没有策略能比最差策略改进超过1+ε的因子，而对于整个实例，存在一个策略能比最差策略改进近2倍，这是最优的。", "conclusion": "本文通过提供改进的近似保证和解决长期存在的开放问题，在库存交错问题的计算理解方面取得了重大突破，并揭示了该问题的全局特性。", "translation": "自20世纪60年代中期诞生以来，库存交错问题已在生产计划、库存控制系统、仓储以及航空航天/国防物流等广泛应用领域得到了探索和利用。然而，即使有着丰富的学术关注历史，在库存交错问题及其相关结构特征的核心计算问题上，我们仍然知之甚少，我们的方法论工具箱严重不足。本文的核心贡献在于设计了一系列算法技术和分析思想——其中一些是全新的，另一些则利用了组合学和数论中已深入研究的概念——以超越库存交错问题几乎所有已知的近似保证。特别是，我们的工作表明，许多结构特性为设计多项式时间近似方案打开了大门，包括多项式有界循环长度、恒定数量的不同时间间隔、所谓的嵌套实例以及成对互质设置。这些发现对目前可用的常数因子近似值提供了实质性改进，并解决了各自背景下悬而未决的开放问题。同时，我们围绕一些尚未探索的问题开发了新理论，这些问题与峰值库存估计的采样复杂性以及群组同步的可行性有关。有趣的是，我们确立了库存交错问题的全局性质，证明了存在n个项目的实例，其中对于大约√n个项目的每个子集，没有策略能比最差策略改进超过1+ε的因子，而对于整个实例，存在一个策略能比最差策略改进近2倍，这是最优的。", "summary": "本文解决了库存交错问题，这是一个被广泛研究但计算上仍未充分探索的领域。它引入了新颖的算法和分析技术，显著改善了现有近似保证，使得针对各种结构特性能够设计多项式时间近似方案。这项研究解决了若干开放问题，并揭示了库存交错问题的全局性质，即最优策略可能需要全局视角而非局部视角。", "keywords": "库存交错, 近似保证, 算法技术, 组合学, 数论", "comments": "本文通过显著提高库存交错问题的近似保证，做出了重要贡献，这对实际应用至关重要。其创新性地利用组合学和数论概念，以及发现问题特有的结构特性，都值得关注。关于问题全局性质的发现为理解该问题增添了新的维度。"}}
{"id": "2506.10009", "title": "The Iris File Extension", "authors": ["Ryan Erik Landvater", "Michael David Olp", "Mustafa Yousif", "Ulysses Balis"], "summary": "A modern digital pathology vendor-agnostic binary slide format specifically\ntargeting the unmet need of efficient real-time transfer and display has not\nyet been established. Growing adoption of digital pathology only intensifies\nthe need for an intermediary digital slide format with an emphasis on\nperformance for use between slide servers and image management software or for\ninter-institutional transmission of cases. Although the DICOM standard is a\nwell-established format widely used for long-term storage of both images and\ncritically associated metadata, its inherent limitations on maximum image\ndimensions can impact retrieval speed, particularly when accessing whole slide\nimages using a pyramidal structure of slide viewer applications. Here, we\nintroduce the Iris file extension, a binary file container specification\nexplicitly designed for whole slide image systems that can abstract the file\nstructure outline into memory for immediate tile access. The Iris file\nextension adds modern compression support, a dynamic structure with optional\nfile features, computationally trivial deep file validation and corruption\nrecovery capabilities, and slide annotation support. In addition to the file\nspecification document, we provide source code to allow for (de)serialization\nand validation of a binary stream against the standard and corresponding binary\nbuilds with C++, Python, and JavaScript language bindings. We further provide\nfull encoder and decoder implementation source code, as well as binary builds\n(as part of the separate Iris Codec Community module) with language bindings\nfor C++ and Python to allow for easy integration with existing WSI solutions.\nWe provide the Iris File Extension specification openly to the community in the\nform of a Creative Commons Attribution-No Derivative 4.0 international license.", "comment": "8 pages, 6 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10009v1", "AI": {"title_translation": "Iris文件扩展名", "tldr": "本文介绍了Iris，一种用于数字病理全切片图像的新型二进制文件格式，旨在实现高效的实时传输和显示，克服了DICOM等现有格式的局限性。", "motivation": "现有数字病理学缺乏高效的实时传输和显示供应商无关的二进制切片格式，且DICOM标准在处理全切片图像时存在检索速度限制，因此需要一种新的高性能中间数字切片格式。", "method": "论文引入了Iris文件扩展名，这是一种专为全切片图像系统设计的二进制文件容器规范，支持现代压缩、动态结构、文件验证和损坏恢复、以及切片注释。作者还提供了反/序列化、验证的源代码和二进制构建（支持C++、Python、JavaScript），以及完整的编码器和解码器实现。该规范以知识共享许可形式开放。", "result": "引入了Iris文件扩展名，一种新的二进制全切片图像格式，旨在解决数字病理学中高效实时传输和显示的需求，并提供了用于（解）序列化、验证、编码和解码的开源代码和多语言绑定二进制构建。", "conclusion": "Iris文件扩展名作为一种开放、供应商无关的二进制全切片图像格式，解决了当前数字病理图像在高效实时传输和显示方面的不足，并通过提供开源工具促进了其应用。", "translation": "目前尚未建立一种现代的、与供应商无关的数字病理二进制切片格式，以满足高效实时传输和显示的需求。数字病理学的日益普及，只会加剧对一种中间数字切片格式的需求，该格式应侧重于在切片服务器和图像管理软件之间使用，或用于机构间病例传输的性能。尽管DICOM标准是一种成熟的格式，广泛用于图像和关键相关元数据的长期存储，但其固有的最大图像尺寸限制可能会影响检索速度，尤其是在使用金字塔结构切片查看器应用程序访问全切片图像时。在此，我们介绍了Iris文件扩展名，这是一种专门为全切片图像系统设计的二进制文件容器规范，可以将文件结构大纲抽象到内存中以实现即时切片访问。Iris文件扩展名增加了现代压缩支持、具有可选文件功能的动态结构、计算上微不足道的深度文件验证和损坏恢复功能，以及切片注释支持。除了文件规范文档，我们还提供了源代码，以允许对二进制流进行（解）序列化和根据标准进行验证，以及带有C++、Python和JavaScript语言绑定的相应二进制构建。我们还提供了完整的编码器和解码器实现源代码，以及带有C++和Python语言绑定的二进制构建（作为独立的Iris编解码器社区模块的一部分），以便于与现有WSI解决方案轻松集成。我们以知识共享署名-禁止演绎4.0国际许可的形式，向社区公开Iris文件扩展名规范。", "summary": "本文介绍了Iris文件扩展名，一种专为数字病理学中全切片图像设计的新型开源二进制文件格式。它旨在解决高效实时传输和显示的迫切需求，弥补了DICOM等现有格式在处理大尺寸图像时的不足。Iris支持现代压缩、动态结构、强大的验证和恢复功能以及注释支持，通过将文件结构抽象到内存中实现即时切片访问。作者发布了该规范，并提供了多语言源代码和二进制构建，以便于集成到现有全切片成像解决方案中。", "keywords": "数字病理学, 全切片图像, 文件格式, Iris, 实时传输", "comments": "本文通过提出一种开放、供应商无关的全切片图像存储和传输标准，解决了数字病理学领域一个重要的未满足需求。其对实时性能的关注，以及腐败恢复和开源实现等强大功能，使Iris成为一项潜在的重大贡献。其开放许可模式进一步鼓励了广泛采用和协作，这对于建立新的行业标准至关重要。"}}
{"id": "2506.10027", "title": "Learning-based density-equalizing map", "authors": ["Yanwen Huang", "Lok Ming Lui", "Gary P. T. Choi"], "summary": "Density-equalizing map (DEM) serves as a powerful technique for creating\nshape deformations with the area changes reflecting an underlying density\nfunction. In recent decades, DEM has found widespread applications in fields\nsuch as data visualization, geometry processing, and medical imaging.\nTraditional approaches to DEM primarily rely on iterative numerical solvers for\ndiffusion equations or optimization-based methods that minimize handcrafted\nenergy functionals. However, these conventional techniques often face several\nchallenges: they may suffer from limited accuracy, produce overlapping\nartifacts in extreme cases, and require substantial algorithmic redesign when\nextended from 2D to 3D, due to the derivative-dependent nature of their energy\nformulations. In this work, we propose a novel learning-based\ndensity-equalizing mapping framework (LDEM) using deep neural networks.\nSpecifically, we introduce a loss function that enforces density uniformity and\ngeometric regularity, and utilize a hierarchical approach to predict the\ntransformations at both the coarse and dense levels. Our method demonstrates\nsuperior density-equalizing and bijectivity properties compared to prior\nmethods for a wide range of simple and complex density distributions, and can\nbe easily applied to surface remeshing with different effects. Also, it\ngeneralizes seamlessly from 2D to 3D domains without structural changes to the\nmodel architecture or loss formulation. Altogether, our work opens up new\npossibilities for scalable and robust computation of density-equalizing maps\nfor practical applications.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10027v1", "AI": {"title_translation": "基于学习的密度均衡映射", "tldr": "本文提出了一种新颖的基于深度学习的密度均衡映射框架（LDEM），解决了传统方法在准确性、重叠伪影以及从2D到3D扩展方面的局限性，并展示了优越的性能和泛化能力。", "motivation": "传统的密度均衡映射（DEM）方法主要依赖迭代数值求解器或基于优化的方法，但存在准确性有限、在极端情况下产生重叠伪影以及由于能量公式的导数依赖性导致从2D到3D扩展需要大量算法重新设计等挑战。", "method": "本文提出了一种新颖的基于学习的密度均衡映射框架（LDEM），利用深度神经网络实现。具体来说，引入了一个强制密度均匀性和几何规律性的损失函数，并采用分层方法预测粗略和密集级别的变换。", "result": "与现有方法相比，LDEM在广泛的简单和复杂密度分布下，展示了卓越的密度均衡和双射特性。它可以轻松应用于具有不同效果的表面网格重构，并且无需模型架构或损失公式的结构性改变即可从2D域无缝推广到3D域。", "conclusion": "本文的工作为实际应用中密度均衡映射的可扩展和鲁棒计算开辟了新的可能性。", "translation": "密度均衡映射（DEM）作为一种强大的技术，能够创建形状变形，其面积变化反映了底层的密度函数。近几十年来，DEM已广泛应用于数据可视化、几何处理和医学成像等领域。传统的DEM方法主要依赖于扩散方程的迭代数值求解器或最小化手工设计能量泛函的基于优化方法。然而，这些传统技术常常面临一些挑战：它们可能精度有限，在极端情况下产生重叠伪影，并且由于其能量公式的导数依赖性，从2D扩展到3D时需要大量的算法重新设计。在这项工作中，我们提出了一种新颖的基于学习的密度均衡映射框架（LDEM），使用深度神经网络。具体来说，我们引入了一个强制密度均匀性和几何规律性的损失函数，并利用分层方法预测粗略和密集级别的变换。与现有方法相比，我们的方法在广泛的简单和复杂密度分布下，展示了卓越的密度均衡和双射特性，并且可以轻松应用于具有不同效果的表面网格重构。此外，它无需模型架构或损失公式的结构性改变即可从2D域无缝推广到3D域。总而言之，我们的工作为实际应用中密度均衡映射的可扩展和鲁棒计算开辟了新的可能性。", "summary": "本文提出了一种名为LDEM的新型基于学习的密度均衡映射（DEM）框架，旨在克服传统DEM方法在准确性、重叠问题以及2D到3D泛化方面的局限性。LDEM利用深度神经网络，并引入了一个结合密度均匀性和几何规律性的损失函数，同时采用分层方法进行变换预测。实验结果表明，LDEM在各种密度分布下均表现出优于现有方法的密度均衡和双射特性，并能无缝地从2D推广到3D，为DEM的实际应用提供了可扩展且鲁棒的计算方法。", "keywords": "密度均衡映射, 深度学习, 神经网络, 几何处理, 数据可视化", "comments": "本文的创新之处在于将深度学习应用于密度均衡映射，成功解决了传统方法长期存在的精度限制、重叠伪影以及2D到3D扩展困难等问题。特别是其从2D到3D的无缝泛化能力，以及在精度和双射性方面的显著提升，极大地扩展了密度均衡映射在实际应用中的潜力和鲁棒性。"}}
{"id": "2506.10658", "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation", "authors": ["Narges Nemati", "Mostafa Haghir Chehreghani"], "summary": "Matrix completion is a widely adopted framework in recommender systems, as\npredicting the missing entries in the user-item rating matrix enables a\ncomprehensive understanding of user preferences. However, current graph neural\nnetwork (GNN)-based approaches are highly sensitive to noisy or irrelevant\nedges--due to their inherent message-passing mechanisms--and are prone to\noverfitting, which limits their generalizability. To overcome these challenges,\nwe propose a novel method called Matrix Completion using Contrastive Learning\n(MCCL). Our approach begins by extracting local neighborhood subgraphs for each\ninteraction and subsequently generates two distinct graph representations. The\nfirst representation emphasizes denoising by integrating GNN layers with an\nattention mechanism, while the second is obtained via a graph variational\nautoencoder that aligns the feature distribution with a standard prior. A\nmutual learning loss function is employed during training to gradually\nharmonize these representations, enabling the model to capture common patterns\nand significantly enhance its generalizability. Extensive experiments on\nseveral real-world datasets demonstrate that our approach not only improves the\nnumerical accuracy of the predicted scores--achieving up to a 0.8% improvement\nin RMSE--but also produces superior rankings with improvements of up to 36% in\nranking metrics.", "comment": "30 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10658v1", "AI": {"title_translation": "鲁棒推荐的去噪与增强图视图对比矩阵补全", "tldr": "本文提出了一种名为MCCL的新方法，通过对比学习和双重图表示（一个去噪，一个通过GVAE）来解决基于GNN的推荐系统中矩阵补全对噪声的敏感性和过拟合问题，显著提高了预测准确性和排名效果。", "motivation": "现有的基于图神经网络（GNN）的矩阵补全方法在推荐系统中对噪声或不相关边高度敏感，并且容易过拟合，这限制了它们的泛化能力。", "method": "本文提出了一种名为对比学习矩阵补全（MCCL）的新方法。该方法首先为每个交互提取局部邻域子图，然后生成两种不同的图表示：第一种通过集成GNN层与注意力机制来强调去噪；第二种通过图变分自编码器（GVAE）获得，用于将特征分布与标准先验对齐。训练过程中采用互学习损失函数逐步协调这些表示，以捕捉共同模式并显著增强模型的泛化能力。", "result": "在多个真实世界数据集上的广泛实验表明，该方法不仅提高了预测分数的数值准确性（RMSE提高了0.8%），而且在排名指标上表现更优异（排名指标提高了36%）。", "conclusion": "本文提出的MCCL方法通过结合去噪和增强图视图的对比学习，有效解决了传统GNN方法在推荐系统矩阵补全中对噪声的敏感性和泛化能力受限的问题，显著提升了预测精度和排名效果。", "translation": "矩阵补全是推荐系统中广泛采用的框架，因为它能够预测用户-物品评分矩阵中的缺失条目，从而全面理解用户偏好。然而，当前基于图神经网络（GNN）的方法由于其固有的消息传递机制，对噪声或不相关边高度敏感，并且容易过拟合，这限制了它们的泛化能力。为了克服这些挑战，我们提出了一种名为对比学习矩阵补全（MCCL）的新方法。我们的方法首先为每个交互提取局部邻域子图，然后生成两种不同的图表示。第一种表示通过集成GNN层与注意力机制来强调去噪，而第二种通过图变分自编码器获得，用于将特征分布与标准先验对齐。训练过程中采用互学习损失函数逐步协调这些表示，使模型能够捕捉共同模式并显著增强其泛化能力。在多个真实世界数据集上的广泛实验表明，我们的方法不仅提高了预测分数的数值准确性（RMSE提高了0.8%），而且在排名指标上表现更优异（排名指标提高了36%）。", "summary": "本文提出了一种名为对比学习矩阵补全（MCCL）的新型推荐系统方法，旨在解决现有GNN基矩阵补全模型对噪声敏感和过拟合的问题。MCCL通过为每个交互生成两种不同的图表示——一种侧重去噪，另一种通过图变分自编码器进行特征对齐——并利用互学习损失函数来协调这些表示。实验证明，MCCL显著提升了预测分数的准确性（RMSE提高0.8%）和排名效果（排名指标提高36%），从而增强了模型的泛化能力和鲁棒性。", "keywords": "矩阵补全, 对比学习, 图神经网络, 推荐系统, 去噪", "comments": "该论文提出了一种创新的对比学习框架MCCL，通过双重图视图（去噪与增强）和互学习策略，有效解决了GNN在推荐系统中面临的噪声敏感性和泛化能力差的问题。其亮点在于结合了注意力机制进行去噪和GVAE进行特征对齐，并通过对比学习思想提升模型对噪声的鲁棒性和泛化性，对鲁棒推荐系统研究具有重要意义。"}}
{"id": "2506.09999", "title": "Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion", "authors": ["Yukun Chen", "Zihuan Qiu", "Fanman Meng", "Hongliang Li", "Linfeng Xu", "Qingbo Wu"], "summary": "Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that\nfocus only on vision and text, this paper explores MCIL across vision, audio\nand text modalities, addressing challenges in integrating complementary\ninformation and mitigating catastrophic forgetting. To tackle these issues, we\npropose an MCIL method based on multimodal pre-trained models. Firstly, a\nMultimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts\n(MoE) structure is introduced to achieve effective incremental fine-tuning for\nAudioCLIP. Secondly, to enhance feature discriminability and generalization, we\npropose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking\nthreshold mechanism and a dynamic feature fusion mechanism, along with a\nstrategy to enhance text diversity. Thirdly, a novel multimodal\nclass-incremental contrastive training loss is proposed to optimize cross-modal\nalignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced\nfor comprehensive assessment. Extensive experiments on three multimodal\ndatasets validate the effectiveness of our method.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09999v1", "AI": {"title_translation": "在自适应融合下利用预训练模型进行多模态类别增量学习", "tldr": "本文提出了一种基于多模态预训练模型的MCIL方法，用于处理视觉、音频和文本模态，通过MIFE、AAVFM、对比损失和新评估指标来解决信息整合和灾难性遗忘问题。", "motivation": "解决传统多模态类别增量学习（MCIL）方法仅关注视觉和文本的问题，并处理视觉、音频和文本模态在整合互补信息和缓解灾难性遗忘方面的挑战。", "method": "提出了一种基于多模态预训练模型的MCIL方法。具体包括：1. 引入基于专家混合（MoE）结构的多模态增量特征提取器（MIFE），实现AudioCLIP的有效增量微调。2. 提出自适应音视频融合模块（AAVFM），包含掩蔽阈值机制和动态特征融合机制，并增强文本多样性策略，以增强特征判别性和泛化性。3. 提出新颖的多模态类别增量对比训练损失，优化MCIL中的跨模态对齐。4. 引入两个MCIL专用评估指标进行综合评估。", "result": "在三个多模态数据集上的大量实验验证了该方法的有效性。", "conclusion": "该方法能有效解决多模态类别增量学习中信息整合和灾难性遗忘的挑战，并在视觉、音频和文本模态上表现出色。", "translation": "与传统仅关注视觉和文本的多模态类别增量学习（MCIL）方法不同，本文探索了视觉、音频和文本模态之间的MCIL，解决了整合互补信息和缓解灾难性遗忘的挑战。为了解决这些问题，我们提出了一种基于多模态预训练模型的MCIL方法。首先，引入了基于专家混合（MoE）结构的多模态增量特征提取器（MIFE），以实现AudioCLIP的有效增量微调。其次，为了增强特征判别性和泛化性，我们提出了一种自适应音视频融合模块（AAVFM），该模块包含掩蔽阈值机制和动态特征融合机制，以及一种增强文本多样性的策略。第三，提出了一种新颖的多模态类别增量对比训练损失，以优化MCIL中的跨模态对齐。最后，引入了两个MCIL专用评估指标进行综合评估。在三个多模态数据集上的大量实验验证了我们方法的有效性。", "summary": "本文提出了一种新颖的多模态类别增量学习（MCIL）方法，旨在解决视觉、音频和文本模态的整合挑战及灾难性遗忘问题。该方法利用多模态预训练模型，通过引入基于MoE的增量特征提取器（MIFE）进行AudioCLIP微调，设计自适应音视频融合模块（AAVFM）以增强特征，并提出新的对比训练损失以优化跨模态对齐。此外，还引入了专门的评估指标。实验结果验证了其在多模态数据集上的有效性。", "keywords": "多模态类别增量学习, 预训练模型, 自适应融合, 灾难性遗忘, 对比学习", "comments": "该论文的创新点在于将MCIL扩展到视觉、音频和文本三种模态，而非传统的双模态。通过引入MIFE、AAVFM和新的对比损失，有效解决了多模态信息融合和灾难性遗忘的核心问题。同时，提出专用的评估指标也提升了研究的严谨性。"}}
{"id": "2506.10301", "title": "Towards Understanding Bias in Synthetic Data for Evaluation", "authors": ["Hossein A. Rahmani", "Varsha Ramineni", "Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz"], "summary": "Test collections are crucial for evaluating Information Retrieval (IR)\nsystems. Creating a diverse set of user queries for these collections can be\nchallenging, and obtaining relevance judgments, which indicate how well\nretrieved documents match a query, is often costly and resource-intensive.\nRecently, generating synthetic datasets using Large Language Models (LLMs) has\ngained attention in various applications. While previous work has used LLMs to\ngenerate synthetic queries or documents to improve ranking models, using LLMs\nto create synthetic test collections is still relatively unexplored. Previous\nwork~\\cite{rahmani2024synthetic} showed that synthetic test collections have\nthe potential to be used for system evaluation, however, more analysis is\nneeded to validate this claim. In this paper, we thoroughly investigate the\nreliability of synthetic test collections constructed using LLMs, where LLMs\nare used to generate synthetic queries, labels, or both. In particular, we\nexamine the potential biases that might occur when such test collections are\nused for evaluation. We first empirically show the presence of such bias in\nevaluation results and analyse the effects it might have on system evaluation.\nWe further validate the presence of such bias using a linear mixed-effects\nmodel. Our analysis shows that while the effect of bias present in evaluation\nresults obtained using synthetic test collections could be significant, for\ne.g.~computing absolute system performance, its effect may not be as\nsignificant in comparing relative system performance. Codes and data are\navailable at: https://github.com/rahmanidashti/BiasSyntheticData.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10301v1", "AI": {"title_translation": "探究合成数据在评估中的偏差", "tldr": "论文研究了LLM生成的合成测试集在信息检索系统评估中的偏差，发现对绝对性能影响显著，但对相对性能影响较小。", "motivation": "构建信息检索系统的测试集成本高昂且资源密集。尽管LLM生成合成数据已受关注，但利用LLM创建合成测试集的研究相对较少，且其潜在偏差需要深入分析。", "method": "论文深入探究了使用LLM构建的合成测试集的可靠性，其中LLM用于生成合成查询、标签或两者。通过实证分析展示了评估结果中偏差的存在及其影响，并使用线性混合效应模型进一步验证了偏差。", "result": "使用合成测试集获得的评估结果存在偏差。这种偏差在计算绝对系统性能时可能很显著，但在比较相对系统性能时可能不那么显著。", "conclusion": "尽管合成测试集在评估绝对系统性能时可能引入显著偏差，但对于比较相对系统性能而言，其影响可能不那么显著，这表明合成测试集在某些评估场景中仍具潜在价值。", "translation": "信息检索 (IR) 系统的评估离不开测试集。为这些测试集创建多样化的用户查询可能具有挑战性，而获取相关性判断（指示检索到的文档与查询的匹配程度）通常成本高昂且资源密集。最近，使用大型语言模型 (LLM) 生成合成数据集在各种应用中受到了关注。虽然之前的工作已使用LLM生成合成查询或文档来改进排名模型，但利用LLM创建合成测试集仍相对未被充分探索。之前的工作~\\[rahmani2024synthetic\\] 表明合成测试集有潜力用于系统评估，然而，需要更多的分析来验证这一说法。在本文中，我们彻底调查了使用LLM构建的合成测试集的可靠性，其中LLM用于生成合成查询、标签或两者。特别是，我们检查了当此类测试集用于评估时可能出现的潜在偏差。我们首先通过实证证明了评估结果中存在此类偏差，并分析了它可能对系统评估产生的影响。我们还使用线性混合效应模型进一步验证了此类偏差的存在。我们的分析表明，尽管使用合成测试集获得的评估结果中存在的偏差效应可能很显著，例如计算绝对系统性能，但其对比较相对系统性能的影响可能不那么显著。代码和数据可在以下网址获取：https://github.com/rahmanidashti/BiasSyntheticData。", "summary": "本文研究了大型语言模型（LLM）生成的合成测试集在信息检索（IR）系统评估中的可靠性和潜在偏差。研究通过实证和线性混合效应模型验证了，尽管在计算绝对系统性能时存在显著偏差，但这种偏差在比较相对系统性能时影响较小。这表明LLM生成的合成数据在IR评估中具有细微的适用性。", "keywords": "合成数据, 大型语言模型, 信息检索, 偏差, 测试集", "comments": "这篇论文探讨了利用LLM进行信息检索评估这一新兴且关键领域，并着重分析了合成数据中常被忽视的偏差问题。其发现偏差对相对性能比较影响较小，是一个有价值的洞察，可能指导未来的研究和实际应用。通过实证和模型验证，论文的论点得到了加强。"}}
{"id": "2506.10019", "title": "A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations", "authors": ["Tian Lan", "Yang-Hao Zhou", "Zi-Ao Ma", "Fanshu Sun", "Rui-Qing Sun", "Junyu Luo", "Rong-Cheng Tu", "Heyan Huang", "Chen Xu", "Zhijing Wu", "Xian-Ling Mao"], "summary": "Recent advances in deep learning have significantly enhanced generative AI\ncapabilities across text, images, and audio. However, automatically evaluating\nthe quality of these generated outputs presents ongoing challenges. Although\nnumerous automatic evaluation methods exist, current research lacks a\nsystematic framework that comprehensively organizes these methods across text,\nvisual, and audio modalities. To address this issue, we present a comprehensive\nreview and a unified taxonomy of automatic evaluation methods for generated\ncontent across all three modalities; We identify five fundamental paradigms\nthat characterize existing evaluation approaches across these domains. Our\nanalysis begins by examining evaluation methods for text generation, where\ntechniques are most mature. We then extend this framework to image and audio\ngeneration, demonstrating its broad applicability. Finally, we discuss\npromising directions for future research in cross-modal evaluation\nmethodologies.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10019v1", "AI": {"title_translation": "文本、视觉和语音生成自动评估方法综述", "tldr": "这篇综述提供了一个统一的分类框架，系统地组织了文本、图像和音频生成内容的自动评估方法，并识别了五种基本范式。", "motivation": "尽管深度学习显著提升了生成式AI的能力，但自动评估生成内容的质量仍具挑战。现有研究缺乏一个系统性的框架来全面组织文本、视觉和音频模态的自动评估方法。", "method": "作者提出了一个全面的综述和统一的分类法，用于跨文本、视觉和音频三种模态的生成内容自动评估方法。他们识别了表征现有评估方法的五种基本范式，并首先考察了文本生成评估方法，然后将框架扩展到图像和音频生成。", "result": "论文提出了一个统一的自动评估方法分类法，识别了五种适用于文本、图像和音频生成的基本评估范式，并展示了该框架的广泛适用性。", "conclusion": "论文讨论了跨模态评估方法未来研究的有前景方向。", "translation": "深度学习的最新进展显著增强了文本、图像和音频领域的生成式AI能力。然而，自动评估这些生成内容的质量仍然面临持续的挑战。尽管存在大量的自动评估方法，但当前研究缺乏一个系统性的框架来全面组织跨文本、视觉和音频模态的这些方法。为了解决这个问题，我们提出了一个全面的综述和统一的分类法，用于跨所有三种模态的生成内容自动评估方法；我们识别了表征这些领域现有评估方法的五种基本范式。我们的分析首先考察了文本生成评估方法，其中技术最为成熟。然后，我们将该框架扩展到图像和音频生成，展示了其广泛适用性。最后，我们讨论了跨模态评估方法未来研究的有前景方向。", "summary": "这篇综述论文旨在解决当前研究中缺乏系统性框架来组织文本、图像和音频生成内容自动评估方法的问题。作者提出了一个全面的综述和一个统一的分类法，识别了五种基本评估范式，并首先详细分析了文本生成评估方法，随后将该框架扩展到图像和音频生成，强调了其普适性。文章最后展望了跨模态评估方法未来研究的方向。", "keywords": "自动评估, 生成式AI, 文本生成, 图像生成, 音频生成", "comments": "这篇综述论文通过提供一个统一的分类框架，系统地梳理了跨多种模态（文本、视觉、语音）的自动评估方法，填补了现有研究的空白。其创新之处在于提出并识别了五种基本评估范式，为理解和开发生成式AI的评估方法提供了清晰的结构。这对于促进生成式AI领域的发展及其评估标准的统一具有重要意义。"}}
{"id": "2506.10005", "title": "Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models", "authors": ["Sridhar S", "Nithin A", "Shakeel Rifath", "Vasantha Raj"], "summary": "Advances in generative artificial intelligence have altered multimedia\ncreation, allowing for automatic cinematic video synthesis from text inputs.\nThis work describes a method for creating 60-second cinematic movies\nincorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for\nnarrative structuring, and a hybrid audio pipeline using gTTS and\nYouTube-sourced music. It uses a five-scene framework, which is augmented by\nlinear frame interpolation, cinematic post-processing (e.g., sharpening), and\naudio-video synchronization to provide professional-quality results. It was\ncreated in a GPU-accelerated Google Colab environment using Python 3.11. It has\na dual-mode Gradio interface (Simple and Advanced), which supports resolutions\nof up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA\nmemory management and error handling ensure reliability. The experiments\ndemonstrate outstanding visual quality, narrative coherence, and efficiency,\nfurthering text-to-video synthesis for creative, educational, and industrial\napplications.", "comment": "10 pages, seven figures about Multimodal Cinematic Video Synthesis\n  Using Text-to-Image and Audio Generation Models", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10005v1", "AI": {"title_translation": "多模态电影视频合成：基于文本到图像和音频生成模型", "tldr": "该研究提出了一种使用文本到图像和音频生成模型自动合成60秒电影视频的方法，结合了Stable Diffusion、GPT-2和混合音频管线，实现了高质量、连贯的电影视频。", "motivation": "生成式人工智能的进步改变了多媒体创作，使得从文本输入自动合成电影视频成为可能。", "method": "该方法结合了Stable Diffusion进行高保真图像合成，GPT-2进行叙事结构，以及使用gTTS和YouTube来源音乐的混合音频管线。它采用五场景框架，并通过线性帧插值、电影后期处理（如锐化）和音视频同步进行增强。该系统在GPU加速的Google Colab环境中使用Python 3.11开发，并提供双模式Gradio界面，支持高达1024x768的分辨率和15-30 FPS的帧率，同时包含CUDA内存管理和错误处理等优化。", "result": "实验证明了该方法在视觉质量、叙事连贯性和效率方面的出色表现。", "conclusion": "该方法进一步推动了文本到视频合成技术，适用于创意、教育和工业应用。", "translation": "生成式人工智能的进步改变了多媒体创作，使得从文本输入自动合成电影视频成为可能。这项工作描述了一种创建60秒电影的方法，该方法结合了Stable Diffusion用于高保真图像合成、GPT-2用于叙事结构，以及使用gTTS和YouTube来源音乐的混合音频管线。它采用五场景框架，并通过线性帧插值、电影后期处理（例如锐化）和音视频同步进行增强，以提供专业质量的结果。它是在GPU加速的Google Colab环境中使用Python 3.11创建的。它具有双模式Gradio界面（简单和高级），支持高达1024x768的分辨率和15-30 FPS的帧率。CUDA内存管理和错误处理等优化确保了可靠性。实验展示了出色的视觉质量、叙事连贯性和效率，进一步推动了文本到视频合成在创意、教育和工业应用中的发展。", "summary": "这篇论文介绍了一种多模态电影视频合成方法，能够根据文本输入自动生成60秒的电影视频。该系统整合了Stable Diffusion进行图像生成、GPT-2进行叙事构建，并结合了gTTS和YouTube音乐的混合音频方案。通过五场景框架、帧插值、后期处理和音视频同步，该方法在Google Colab环境下实现了专业级的视觉质量和叙事连贯性，为文本到视频合成在多种应用中提供了高效且高质量的解决方案。", "keywords": "电影视频合成, 文本到视频, Stable Diffusion, GPT-2, 多模态生成", "comments": "这项工作通过集成多种先进的生成模型（如Stable Diffusion和GPT-2）并结合专业的电影制作技术（如帧插值、后期处理），在文本到视频合成领域取得了显著进展。其创新点在于构建了一个端到端的多模态合成管线，能够生成具有叙事结构和高质量视听效果的电影视频。其局限性可能包括对特定GPU环境的依赖以及在生成更长或更复杂叙事视频时的挑战。"}}
{"id": "2506.10097", "title": "Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring", "authors": ["Tomoya Nishida", "Noboru Harada", "Daisuke Niizumi", "Davide Albertini", "Roberto Sannino", "Simone Pradolini", "Filippo Augusti", "Keisuke Imoto", "Kota Dohi", "Harsh Purohit", "Takashi Endo", "Yohei Kawaguchi"], "summary": "This paper introduces the task description for the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2025 Challenge Task 2,\ntitled \"First-shot unsupervised anomalous sound detection (ASD) for machine\ncondition monitoring.\" Building on the DCASE 2024 Challenge Task 2, this task\nis structured as a first-shot problem within a domain generalization framework.\nThe primary objective of the first-shot approach is to facilitate the rapid\ndeployment of ASD systems for new machine types without requiring\nmachine-specific hyperparameter tunings. For DCASE 2025 Challenge Task 2,\nsounds from previously unseen machine types have been collected and provided as\nthe evaluation dataset. Results and analysis of the challenge submissions will\nbe added following the challenge's submission deadline.", "comment": "this article draws heavily from arXiv:2406.07250v1", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10097v1", "AI": {"title_translation": "DCASE 2025挑战赛任务2：机器状态监测的首次无监督异常声音检测的描述与讨论", "tldr": "介绍了DCASE 2025挑战赛任务2，该任务旨在通过首次无监督异常声音检测，实现新机器类型ASD系统的快速部署，无需特定机器的超参数调整。", "motivation": "该任务的主要目标是促进异常声音检测（ASD）系统在新机器类型上的快速部署，而无需针对特定机器进行超参数调整，从而解决实际应用中的部署效率问题。", "method": "该任务是DCASE 2025挑战赛任务2，构建在DCASE 2024挑战赛任务2之上，被构建为一个在域泛化框架内的“首次”问题。评估数据集包含了来自先前未见过的机器类型的声音。", "result": "未提及摘要。", "conclusion": "未提及摘要。", "translation": "本文介绍了检测与分类声学场景与事件（DCASE）2025挑战赛任务2的任务描述，该任务题为“用于机器状态监测的首次无监督异常声音检测（ASD）”。该任务建立在DCASE 2024挑战赛任务2的基础上，被构建为一个域泛化框架内的“首次”问题。首次方法的主要目标是促进异常声音检测系统在新机器类型上的快速部署，而无需机器特定的超参数调整。对于DCASE 2025挑战赛任务2，已收集并提供了来自先前未见过的机器类型声音作为评估数据集。挑战赛提交截止日期后，将补充挑战赛提交结果和分析。", "summary": "本文详细阐述了DCASE 2025挑战赛任务2，即“机器状态监测的首次无监督异常声音检测”的具体内容。该任务旨在通过“首次”方法，在域泛化框架下，实现异常声音检测系统对新型机器的快速部署，且无需进行繁琐的机器特定超参数调整。为此，挑战赛提供了包含未见机器类型声音的评估数据集。论文指出，挑战赛结果和分析将在提交截止日期后公布。", "keywords": "异常声音检测, 机器状态监测, DCASE 挑战赛, 首次学习, 域泛化", "comments": "这篇论文介绍了一个重要的挑战任务，旨在推动异常声音检测技术在工业应用中的实际部署。其创新点在于采用了“首次无监督”和“域泛化”框架，旨在解决现有ASD系统在新机器类型上部署时需要大量标记数据和手动调优的问题。这对于实现更高效、更通用的机器状态监测具有重要意义。"}}
{"id": "2506.10028", "title": "Secure Data Access in Cloud Environments Using Quantum Cryptography", "authors": ["S. Vasavi Venkata Lakshmi", "Ziaul Haque Choudhury"], "summary": "Cloud computing has made storing and accessing data easier but keeping it\nsecure is a big challenge nowadays. Traditional methods of ensuring data may\nnot be strong enough in the future when powerful quantum computers become\navailable. To solve this problem, this study uses quantum cryptography to\nprotect data in the cloud environment. Quantum Key Distribution (QKD) creates\nsecure keys by sending information using quantum particles like photons.\nSpecifically, we use the BB84 protocol, a simple and reliable way to make\nsecure keys that cannot be stolen without detection. To protect the data, we\nuse the Quantum One Time pad (QOTP) for encryption and decryption, ensuring the\ndata stays completely private. This study shows how these Quantum methods can\nbe applied in cloud systems to provide a strong defense against hackers, even\nif they have access to quantum computers. The combination of QKD, BB84, and\nQOTP creates a safe and reliable way to keep data secure when it is stored or\nshared in the cloud. Using quantum cryptography, this paper provides a way to\nensure data security now and in the future, making cloud computing safer for\neveryone to store their data securely and safely.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10028v1", "AI": {"title_translation": "在云环境中利用量子密码学实现安全数据访问", "tldr": "本研究提出并演示了如何使用量子密码学（结合量子密钥分发BB84协议和量子一次性密码QOTP）来保护云数据，以抵御当前和未来的量子计算机威胁。", "motivation": "随着云计算的普及，数据存储和访问变得便捷，但数据安全面临巨大挑战。传统的数据加密方法在面对未来强大的量子计算机时可能不再安全。因此，需要一种更强大的方法来确保存储在云中的数据安全。", "method": "本研究采用量子密码学来保护云环境中的数据。具体方法包括：1. 使用量子密钥分发（QKD），通过量子粒子（如光子）发送信息来创建安全密钥。2. 密钥生成采用BB84协议，这是一种简单可靠且能检测窃听的密钥生成方式。3. 数据加密和解密使用量子一次性密码（QOTP），以确保数据的完全隐私。", "result": "本研究展示了如何将量子密钥分发（QKD）和量子一次性密码（QOTP）等量子方法应用于云系统，以提供强大的防御，抵御包括拥有量子计算机的黑客在内的攻击。QKD、BB84和QOTP的结合为云中存储或共享数据提供了一种安全可靠的保护方式。", "conclusion": "通过结合量子密钥分发（QKD）的BB84协议和量子一次性密码（QOTP），本研究提供了一种确保云数据在当前和未来安全的方法，使云计算对所有人来说都更安全可靠。", "translation": "云计算使得数据存储和访问变得更加便捷，但确保其安全性如今是一个巨大的挑战。当强大的量子计算机问世时，传统的确保数据安全的方法在未来可能不够强大。为了解决这个问题，本研究利用量子密码学来保护云环境中的数据。量子密钥分发（QKD）通过使用量子粒子（如光子）发送信息来创建安全密钥。具体来说，我们使用BB84协议，这是一种简单可靠的生成安全密钥的方法，且在不被发现的情况下无法被窃取。为了保护数据，我们使用量子一次性密码（QOTP）进行加密和解密，确保数据完全私密。本研究展示了这些量子方法如何应用于云系统，以提供强大的防御，抵御黑客攻击，即使他们可以使用量子计算机。QKD、BB84和QOTP的结合创造了一种安全可靠的方式，可以在数据存储或共享在云中时保持其安全。通过使用量子密码学，本文提供了一种确保数据当前和未来安全的方法，使云计算对每个人来说都更安全地存储他们的数据。", "summary": "本研究旨在解决云计算中日益增长的数据安全挑战，特别是面对未来量子计算机的威胁。论文提出并演示了一种基于量子密码学的数据保护方案，该方案结合了量子密钥分发（QKD）的BB84协议用于安全密钥生成，以及量子一次性密码（QOTP）用于数据加密和解密。研究结果表明，这种整合方法能够为云数据提供强大的、量子安全的保护，确保数据在当前和未来的隐私性和安全性。", "keywords": "量子密码学, 云计算安全, 量子密钥分发, BB84协议, 量子一次性密码", "comments": "本文提出了一种前瞻性的解决方案，通过引入量子密码学来应对未来量子计算对传统加密方法的威胁。其创新之处在于将量子密钥分发（QKD）与量子一次性密码（QOTP）相结合，形成一个端到端的量子安全数据保护框架。这种方法理论上提供了无条件的安全保障，对于长期数据安全至关重要。然而，实际部署的复杂性、成本以及量子设备的成熟度是其潜在的局限性。"}}
{"id": "2506.10874", "title": "Higher-Order Uncoupled Learning Dynamics and Nash Equilibrium", "authors": ["Sarah A. Toonsi", "Jeff S. Shamma"], "summary": "We study learnability of mixed-strategy Nash Equilibrium (NE) in general\nfinite games using higher-order replicator dynamics as well as classes of\nhigher-order uncoupled heterogeneous dynamics. In higher-order uncoupled\nlearning dynamics, players have no access to utilities of opponents (uncoupled)\nbut are allowed to use auxiliary states to further process information\n(higher-order). We establish a link between uncoupled learning and feedback\nstabilization with decentralized control. Using this association, we show that\nfor any finite game with an isolated completely mixed-strategy NE, there exist\nhigher-order uncoupled learning dynamics that lead (locally) to that NE. We\nfurther establish the lack of universality of learning dynamics by linking\nlearning to the control theoretic concept of simultaneous stabilization. We\nconstruct two games such that any higher-order dynamics that learn the\ncompletely mixed-strategy NE of one of these games can never learn the\ncompletely mixed-strategy NE of the other. Next, motivated by imposing natural\nrestrictions on allowable learning dynamics, we introduce the Asymptotic Best\nResponse (ABR) property. Dynamics with the ABR property asymptotically learn a\nbest response in environments that are asymptotically stationary. We show that\nthe ABR property relates to an internal stability condition on higher-order\nlearning dynamics. We provide conditions under which NE are compatible with the\nABR property. Finally, we address learnability of mixed-strategy NE in the\nbandit setting using a bandit version of higher-order replicator dynamics.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.10874v1", "AI": {"title_translation": "高阶非耦合学习动力学与纳什均衡", "tldr": "研究了在高阶非耦合学习动力学下有限博弈中混合策略纳什均衡的可学习性，并揭示了学习动力学的普适性限制。", "motivation": "研究在一般有限博弈中混合策略纳什均衡（NE）的可学习性，尤其是在高阶非耦合异构动力学和高阶复制器动力学下。此外，还引入了渐近最优响应（ABR）特性，以对允许的学习动力学施加自然限制。", "method": "使用高阶复制器动力学和高阶非耦合异构动力学。建立非耦合学习与分散控制下的反馈稳定之间的联系。利用控制理论中的同步稳定概念来探讨学习动力学的普适性。引入渐近最优响应（ABR）特性。在强盗设置中使用高阶复制器动力学的强盗版本研究可学习性。", "result": "对于任何具有孤立完全混合策略NE的有限博弈，存在导致该NE的高阶非耦合学习动力学（局部）。通过与同步稳定的联系，揭示了学习动力学缺乏普适性。构建了两个博弈，证明任何学习其中一个博弈NE的高阶动力学无法学习另一个的NE。ABR特性与高阶学习动力学的内部稳定性条件相关。提供了NE与ABR特性兼容的条件。在强盗设置中讨论了混合策略NE的可学习性。", "conclusion": "论文研究了高阶非耦合学习动力学下混合策略纳什均衡的可学习性，证明了其存在性但缺乏普适性，并引入了渐近最优响应特性来分析学习动力学的内部稳定性。", "translation": "我们研究了在一般有限博弈中，使用高阶复制器动力学以及高阶非耦合异构动力学类别下混合策略纳什均衡（NE）的可学习性。在高阶非耦合学习动力学中，玩家无法访问对手的效用（非耦合），但被允许使用辅助状态来进一步处理信息（高阶）。我们建立了非耦合学习与分散控制下的反馈稳定之间的联系。利用这种关联，我们表明对于任何具有孤立完全混合策略NE的有限博弈，存在导致（局部）该NE的高阶非耦合学习动力学。我们通过将学习与控制理论中的同步稳定概念联系起来，进一步确立了学习动力学缺乏普适性。我们构建了两个博弈，使得任何学习其中一个博弈的完全混合策略NE的高阶动力学永远无法学习另一个博弈的完全混合策略NE。接下来，为了对允许的学习动力学施加自然限制，我们引入了渐近最优响应（ABR）特性。具有ABR特性的动力学在渐近静止的环境中渐近地学习最优响应。我们表明ABR特性与高阶学习动力学的内部稳定性条件相关。我们提供了NE与ABR特性兼容的条件。最后，我们使用高阶复制器动力学的强盗版本，解决了强盗设置中混合策略NE的可学习性问题。", "summary": "本文探讨了在一般有限博弈中，高阶非耦合学习动力学下混合策略纳什均衡（NE）的可学习性。研究建立了非耦合学习与分散控制中反馈稳定的联系，证明了对于孤立完全混合策略NE，存在相应的局部收敛高阶非耦合学习动力学。同时，论文指出学习动力学缺乏普适性，并引入了渐近最优响应（ABR）特性来分析学习动力学的内部稳定性，最后在强盗设置中探讨了NE的可学习性。", "keywords": "纳什均衡, 高阶学习动力学, 非耦合学习, 反馈稳定, 渐近最优响应", "comments": "这篇论文的创新点在于引入了“高阶非耦合学习动力学”的概念，允许玩家在不直接了解对手效用的情况下使用辅助状态处理信息。它通过建立与控制理论的联系，不仅证明了某些NE的可学习性，也揭示了学习动力学在普适性上的局限性，这对于理解复杂博弈中的学习行为具有重要意义。引入ABR特性提供了一种分析学习动力学内部稳定性的新视角。"}}
{"id": "2506.10052", "title": "Quantum resources in resource management systems", "authors": ["Iskandar Sitdikov", "M. Emre Sahin", "Utz Bacher", "Aleksander Wennersteen", "Andrew Damin", "Mark Birmingham", "Philippa Rubin", "Stefano Mensa", "Matthieu Moreau", "Aurelien Nober", "Hitomi Takahashi", "Munetaka Ohtani"], "summary": "Quantum computers are beginning to operate in high-performance computing\n(HPC) environments. Quantum can complement classical resources for specific\nworkloads, but their adoption depends on integration into existing HPC\ninfrastructure. Treating quantum devices as first-class resources allows for\nunified scheduling, improved usability, and support for hybrid\nquantum-classical applications. This paper presents the design architecture and\nreference implementation for quantum resources control using existing workload\nmanagement systems. We introduce a suite of plugins for Slurm that enable\nintegration of on-prem and cloud quantum computing resources into existing\nhigh-performance computing centers. The paper details the interface design,\nplugin concept and implementation, operational aspects for heterogeneous\ncompute clusters, as well as considerations for other resource management\nsystems.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10052v1", "AI": {"title_translation": "资源管理系统中的量子资源", "tldr": "该论文提出了一种将量子计算资源集成到现有高性能计算（HPC）工作负载管理系统（如Slurm）中的方法，以实现统一调度和支持混合量子-经典应用。", "motivation": "量子计算机开始在HPC环境中运行，它们可以补充经典资源，但其广泛采用取决于能否集成到现有HPC基础设施中。将量子设备作为一流资源处理可以实现统一调度、提高可用性并支持混合量子-经典应用。", "method": "本文提出了一种利用现有工作负载管理系统（特别是Slurm）控制量子资源的设计架构和参考实现。具体方法是引入一套Slurm插件，以将本地和云端量子计算资源集成到现有HPC中心。论文详细介绍了接口设计、插件概念与实现、异构计算集群的操作方面以及对其他资源管理系统的考虑。", "result": "论文提出了将量子资源集成到现有高性能计算（HPC）环境中的设计架构和参考实现，并通过为Slurm开发插件成功实现了量子计算资源（包括本地和云端）的集成。", "conclusion": "通过将量子设备作为一流资源集成到现有工作负载管理系统（如Slurm）中，可以实现量子资源与经典资源的统一调度，提高可用性，并有效支持混合量子-经典应用，从而促进量子计算在HPC环境中的广泛采用。", "translation": "量子计算机正开始在高性能计算（HPC）环境中运行。量子计算可以补充特定工作负载的经典资源，但其采用取决于与现有HPC基础设施的集成。将量子设备视为一流资源可以实现统一调度、提高可用性并支持混合量子-经典应用程序。本文介绍了使用现有工作负载管理系统控制量子资源的设计架构和参考实现。我们为Slurm引入了一套插件，使本地和云端量子计算资源能够集成到现有高性能计算中心。本文详细介绍了接口设计、插件概念与实现、异构计算集群的操作方面，以及对其他资源管理系统的考虑。", "summary": "本文提出了一种将量子计算资源无缝集成到现有高性能计算（HPC）基础设施中的方法。通过将量子设备视为一流资源，并利用现有工作负载管理系统（如Slurm），论文设计并实现了一套插件，旨在实现量子与经典资源的统一调度、提高可用性，并支持混合量子-经典应用程序。这项工作为量子计算在HPC环境中的广泛采用提供了实用的解决方案。", "keywords": "量子计算, 资源管理, 高性能计算, Slurm, 混合量子-经典", "comments": "这篇论文通过提出将量子资源集成到现有高性能计算（HPC）基础设施中的实用方法，解决了量子计算实际应用中的一个关键挑战。其创新之处在于利用现有成熟的工作负载管理系统（如Slurm）并通过插件机制实现集成，而非从头构建新系统。这大大降低了量子技术在HPC环境中落地的门槛，对于促进混合量子-经典应用的发展具有重要意义。"}}
{"id": "2506.10049", "title": "Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)", "authors": ["Francesco Vinci", "Gyunam Park", "Wil van der Aalst", "Massimiliano de Leoni"], "summary": "Business Process Simulation (BPS) refers to techniques designed to replicate\nthe dynamic behavior of a business process. Many approaches have been proposed\nto automatically discover simulation models from historical event logs,\nreducing the cost and time to manually design them. However, in dynamic\nbusiness environments, organizations continuously refine their processes to\nenhance efficiency, reduce costs, and improve customer satisfaction. Existing\ntechniques to process simulation discovery lack adaptability to real-time\noperational changes. In this paper, we propose a streaming process simulation\ndiscovery technique that integrates Incremental Process Discovery with Online\nMachine Learning methods. This technique prioritizes recent data while\npreserving historical information, ensuring adaptation to evolving process\ndynamics. Experiments conducted on four different event logs demonstrate the\nimportance in simulation of giving more weight to recent data while retaining\nhistorical knowledge. Our technique not only produces more stable simulations\nbut also exhibits robustness in handling concept drift, as highlighted in one\nof the use cases.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10049v1", "AI": {"title_translation": "演化业务流程仿真模型的在线发现（扩展版）", "tldr": "提出一种流式过程仿真发现技术，结合增量过程发现和在线机器学习，以适应不断变化的业务流程并优先考虑最新数据。", "motivation": "现有业务流程仿真模型发现技术缺乏对实时操作变化的适应性，无法处理动态业务环境中不断演化的流程。", "method": "提出一种流式过程仿真发现技术，该技术将增量过程发现与在线机器学习方法相结合，在保留历史信息的同时优先处理最新数据。", "result": "在四种不同事件日志上的实验表明，在仿真中赋予最新数据更高权重同时保留历史知识的重要性。该技术不仅能产生更稳定的仿真，而且在处理概念漂移方面也表现出鲁棒性。", "conclusion": "该提出的流式过程仿真发现技术通过集成增量过程发现和在线机器学习，有效解决了动态业务流程中仿真模型发现的适应性问题，实现了更稳定、更鲁棒的仿真，尤其是在处理概念漂移方面。", "translation": "业务流程仿真 (BPS) 是指旨在复制业务流程动态行为的技术。许多方法已被提出用于从历史事件日志中自动发现仿真模型，从而降低手动设计它们的成本和时间。然而，在动态业务环境中，组织不断改进其流程以提高效率、降低成本和改善客户满意度。现有流程仿真发现技术缺乏对实时操作变化的适应性。在本文中，我们提出了一种流式过程仿真发现技术，该技术将增量过程发现与在线机器学习方法相结合。该技术在保留历史信息的同时优先处理最新数据，确保适应不断演进的流程动态。在四种不同事件日志上进行的实验证明了在仿真中赋予最新数据更高权重同时保留历史知识的重要性。我们的技术不仅能产生更稳定的仿真，而且在处理概念漂移方面也表现出鲁棒性，正如其中一个用例所强调的那样。", "summary": "本文提出了一种流式过程仿真发现技术，旨在解决现有方法在动态业务环境中适应性不足的问题。该技术结合了增量过程发现和在线机器学习，能够优先处理最新数据同时保留历史信息，从而适应不断演进的业务流程。实验证明，该方法能产生更稳定的仿真结果，并有效处理概念漂移。", "keywords": "业务流程仿真, 在线发现, 增量过程发现, 在线机器学习, 概念漂移", "comments": "这篇论文的创新点在于将增量过程发现与在线机器学习相结合，以实现对不断演化的业务流程的实时仿真模型发现。它解决了传统方法在动态环境下的适应性问题，通过优先考虑最新数据同时保留历史信息，提高了仿真的稳定性与鲁棒性，特别是在处理概念漂移方面的表现是其亮点。这对于需要持续优化流程的企业具有重要意义。"}}
{"id": "2506.10098", "title": "Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models", "authors": ["Christian Reichenbächer", "Philipp Rank", "Jochen Hipp", "Oliver Bringmann"], "summary": "This paper presents the first application of Gaussian Mixture Copula Models\nto the statistical modeling of driving scenarios for the safety validation of\nautomated driving systems. Knowledge of the joint probability distribution of\nscenario parameters is essential for scenario-based safety assessment, where\nrisk quantification depends on the likelihood of concrete parameter\ncombinations. Gaussian Mixture Copula Models bring together the multimodal\nexpressivity of Gaussian Mixture Models and the flexibility of copulas,\nenabling separate modeling of marginal distributions and dependencies. We\nbenchmark Gaussian Mixture Copula Models against previously proposed approaches\n- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving\ndata drawn from scenarios defined in United Nations Regulation No. 157. Our\nevaluation across 18 million scenario instances demonstrates that Gaussian\nMixture Copula Models provide a better fit to the data in terms of both\nlikelihood and Sinkhorn distance. These results suggest that Gaussian Mixture\nCopula Models are a compelling foundation for future scenario-based validation\nframeworks.", "comment": "8 pages, 4 figures; This work has been submitted to the IEEE for\n  possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10098v1", "AI": {"title_translation": "使用高斯混合Copula模型估计场景参数的联合概率", "tldr": "本文首次将高斯混合Copula模型应用于自动驾驶系统安全验证中的驾驶场景统计建模，并证明其在数据拟合方面优于现有方法。", "motivation": "自动驾驶系统安全验证中，场景参数的联合概率分布知识对于基于场景的安全评估至关重要，因为风险量化取决于具体参数组合的可能性。", "method": "本文首次将高斯混合Copula模型应用于自动驾驶系统的驾驶场景统计建模。该模型结合了高斯混合模型的多模态表达能力和Copula的灵活性，能够分别建模边际分布和依赖关系。通过使用来自联合国法规No. 157中定义的真实驾驶数据，将高斯混合Copula模型与先前提出的方法（高斯混合模型和高斯Copula模型）进行了基准测试。", "result": "在对1800万个场景实例的评估中，高斯混合Copula模型在似然性和Sinkhorn距离方面都提供了更好的数据拟合。", "conclusion": "这些结果表明，高斯混合Copula模型是未来基于场景的验证框架的有力基础。", "translation": "本文首次将高斯混合Copula模型应用于自动驾驶系统安全验证中的驾驶场景统计建模。场景参数联合概率分布的知识对于基于场景的安全评估至关重要，其中风险量化取决于具体参数组合的可能性。高斯混合Copula模型结合了高斯混合模型的多模态表达能力和Copula的灵活性，能够分别建模边际分布和依赖关系。我们使用来自联合国法规No. 157中定义的真实驾驶数据，将高斯混合Copula模型与先前提出的方法——高斯混合模型和高斯Copula模型——进行了基准测试。我们对1800万个场景实例的评估表明，高斯混合Copula模型在似然性和Sinkhorn距离方面都提供了更好的数据拟合。这些结果表明，高斯混合Copula模型是未来基于场景的验证框架的有力基础。", "summary": "本研究首次将高斯混合Copula模型应用于自动驾驶系统驾驶场景的统计建模，旨在解决场景参数联合概率分布估计的关键问题。该模型结合了高斯混合模型的表达能力和Copula的灵活性，实现了边际分布与依赖关系的独立建模。通过对1800万个真实驾驶场景实例的评估，结果显示高斯混合Copula模型在数据拟合方面优于传统的高斯混合模型和高斯Copula模型，为未来的场景验证框架提供了坚实基础。", "keywords": "高斯混合Copula模型, 场景参数, 联合概率, 自动驾驶, 安全验证", "comments": "本文首次将高斯混合Copula模型应用于自动驾驶场景的统计建模，这一创新性应用结合了高斯混合模型处理多模态数据的优势和Copula模型分离边际分布与依赖结构的灵活性，有效提升了对复杂驾驶场景参数联合概率的估计精度。其在大量真实数据上的优异表现，对于基于场景的自动驾驶系统安全评估具有重要意义，有望成为未来验证框架的核心工具。"}}
{"id": "2506.10164", "title": "Mastery Learning Improves Performance on Complex Tasks on PCP Literacy Test", "authors": ["Chandana Srinivas", "Elif E. Firat", "Robert S. Laramee", "Alark Joshi"], "summary": "Developing literacy with unfamiliar data visualization techniques such as\nParallel Coordinate Plots (PCPs) can be a significant challenge for students.\nWe adopted the Revised Bloom's taxonomy to instruct students on Parallel\nCoordinate Plots (PCPs) using Mastery Learning in the classroom. To evaluate\nMastery Learning's impact, we conducted an intervention in a Data Visualization\ncourse to teach students about PCPs using the Revised Bloom's taxonomy with and\nwithout Mastery Learning. Based on our intervention, we found that while\nstudents in both groups performed similarly on the first two (Remember,\nUnderstand) modules, the students in the Mastery Learning group performed\nbetter on modules that required more advanced thinking (Analyze, Evaluate) and\ndemonstrated a better comprehension of PCPs. We provide all the materials\ndeveloped including the six-module Bloom's Taxonomy PCP literacy (BTPL) test\nfor full reproducibility on our website at\nhttps://vis-graphics.github.io/PCP-Literacy-Test/.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10164v1", "AI": {"title_translation": "精熟学习提高PCP素养测试中复杂任务的表现", "tldr": "本研究发现，精熟学习能显著提高学生在并行坐标图（PCPs）素养测试中对需要高级思维的复杂任务的表现，尤其是在分析和评估模块。", "motivation": "学生在学习并行坐标图（PCPs）等不熟悉的数据可视化技术时面临重大挑战，本研究旨在探索精熟学习如何帮助学生克服这一挑战。", "method": "研究采用修订版布鲁姆分类法，在数据可视化课程中对学生进行PCP教学，并设置了有精熟学习和无精熟学习的对照组干预实验来评估精熟学习的影响。", "result": "干预结果显示，在记忆和理解模块上，两组学生表现相似；但在需要更高级思维（分析、评估）的模块上，精熟学习组的学生表现更好，对PCPs的理解也更深入。", "conclusion": "精熟学习能够有效提高学生在学习数据可视化技术时对复杂任务的表现和深层理解。", "translation": "掌握学习能提高PCP素养测试中复杂任务的表现\n\n开发对并行坐标图（PCPs）等不熟悉的数据可视化技术的素养对学生来说可能是一个重大挑战。我们采用修订版布鲁姆分类法，在课堂上使用精熟学习教授学生并行坐标图（PCPs）。为了评估精熟学习的影响，我们在数据可视化课程中进行了一项干预，使用修订版布鲁姆分类法，在有精熟学习和无精熟学习的情况下教授学生PCPs。根据我们的干预，我们发现，虽然两组学生在前两个模块（记忆、理解）上的表现相似，但精熟学习组的学生在需要更高级思维（分析、评估）的模块上表现更好，并展示了对PCPs更好的理解。我们提供了所有开发材料，包括六模块的布鲁姆分类法PCP素养（BTPL）测试，以便在我们的网站https://vis-graphics.github.io/PCP-Literacy-Test/上完全重现。", "summary": "本研究探讨了精熟学习在教授学生并行坐标图（PCPs）方面的有效性。通过在数据可视化课程中进行干预，并结合修订版布鲁姆分类法，研究发现精熟学习组的学生在需要高级思维（如分析和评估）的PCP素养测试模块上表现优于非精熟学习组，表明精熟学习能促进对复杂概念的更深层次理解。研究还提供了所有实验材料以供复现。", "keywords": "精熟学习, 并行坐标图, 数据可视化, 布鲁姆分类法, 素养测试", "comments": "该研究的重要性在于其为教授复杂数据可视化技术提供了一种有效的教学方法——精熟学习，并结合了布鲁姆分类法，为教育实践提供了实证支持。其创新之处在于将精熟学习应用于特定的数据可视化素养培养，并提供了可复现的实验材料，这对于后续研究和教学实践具有很高的价值。"}}
{"id": "2506.10135", "title": "Inference of Hierarchical Core-Periphery Structure in Temporal Network", "authors": ["Theodore Y. Faust", "Mason A. Porter"], "summary": "Networks can have various types of mesoscale structures. One type of\nmesoscale structure in networks is core-periphery structure, which consists of\ndensely-connected core nodes and sparsely-connected peripheral nodes. The core\nnodes are connected densely to each other and can be connected to the\nperipheral nodes, which are connected sparsely to other nodes. There has been\nmuch research on core-periphery structure in time-independent networks, but few\ncore-periphery detection methods have been developed for time-dependent (i.e.,\n``temporal\") networks. Using a multilayer-network representation of temporal\nnetworks and an inference approach that employs stochastic block models, we\ngeneralize a recent method for detecting hierarchical core-periphery structure\n\\cite{Polanco23} from time-independent networks to temporal networks. In\ncontrast to ``onion-like'' nested core-periphery structures (where each node is\nassigned to a group according to how deeply it is nested in a network's core),\nhierarchical core-periphery structures encompass networks with nested\nstructures, tree-like structures (where any two groups must either be disjoint\nor have one as a strict subset of the other), and general non-nested mesoscale\nstructures (where the group assignments of nodes do not have to be nested in\nany way). To perform statistical inference and thereby identify core-periphery\nstructure, we use a Markov-chain Monte Carlo (MCMC) approach. We illustrate our\nmethod for detecting hierarchical core-periphery structure in two real-world\ntemporal networks, and we briefly discuss the structures that we identify in\nthese networks.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.10135v1", "AI": {"title_translation": "临时网络中层次核心-边缘结构的推断", "tldr": "本文提出了一种基于多层网络表示和随机块模型的推断方法，用于在时间网络中检测层次核心-边缘结构，并使用马尔可夫链蒙特卡罗方法进行统计推断。", "motivation": "现有的核心-边缘结构检测方法主要针对时间无关网络，而针对时间依赖（即“临时”）网络的检测方法很少。", "method": "该方法利用时间网络的多层网络表示和采用随机块模型的推断方法，将最近用于检测时间无关网络中层次核心-边缘结构的方法推广到时间网络。它使用马尔可夫链蒙特卡罗（MCMC）方法进行统计推断。", "result": "该方法在两个真实世界的时间网络中展示了其检测层次核心-边缘结构的能力，并简要讨论了识别出的结构。", "conclusion": "本文成功地将层次核心-边缘结构检测方法推广到时间网络，并提供了一种通过统计推断识别此类结构的新工具。", "translation": "网络可以有各种类型的介观结构。网络中的一种介观结构是核心-边缘结构，它由连接紧密的核心节点和连接稀疏的边缘节点组成。核心节点之间连接紧密，并且可以与边缘节点连接，而边缘节点与其他节点的连接稀疏。关于时间无关网络中核心-边缘结构的研究很多，但针对时间依赖（即“临时”）网络的核-边缘检测方法却很少。利用时间网络的多层网络表示和采用随机块模型的推断方法，我们将最近用于检测时间无关网络中层次核心-边缘结构的方法[Polanco23]推广到时间网络。与“洋葱状”嵌套核心-边缘结构（其中每个节点根据其在网络核心中的嵌套深度被分配到一个组）相反，层次核心-边缘结构包含具有嵌套结构、树状结构（其中任意两个组必须要么不相交，要么一个严格包含另一个）和一般非嵌套介观结构（其中节点的组分配不一定以任何方式嵌套）的网络。为了执行统计推断并因此识别核心-边缘结构，我们使用马尔可夫链蒙特卡罗（MCMC）方法。我们用两个真实世界的时间网络说明了我们检测层次核心-边缘结构的方法，并简要讨论了我们在这些网络中识别出的结构。", "summary": "本文针对时间依赖网络中核心-边缘结构检测方法不足的问题，提出了一种新的推断方法。该方法将时间网络表示为多层网络，并结合随机块模型和马尔可夫链蒙特卡罗（MCMC）方法，将现有针对时间无关网络的层次核心-边缘结构检测方法推广到时间网络。作者通过在两个真实世界的时间网络上应用该方法，展示了其有效性。", "keywords": "核心-边缘结构, 时间网络, 层次结构, 随机块模型, 马尔可夫链蒙特卡罗", "comments": "本文的创新之处在于首次将层次核心-边缘结构的概念及其检测方法扩展到时间网络领域，填补了该领域研究的空白。通过采用多层网络表示和统计推断方法，为理解复杂时间网络的动态介观结构提供了新的工具。"}}
{"id": "2506.10651", "title": "Large Language Models-Empowered Wireless Networks: Fundamentals, Architecture, and Challenges", "authors": ["Latif U. Khan", "Maher Guizani", "Sami Muhaidat", "Choong Seon Hong"], "summary": "The rapid advancement of wireless networks has resulted in numerous\nchallenges stemming from their extensive demands for quality of service towards\ninnovative quality of experience metrics (e.g., user-defined metrics in terms\nof sense of physical experience for haptics applications). In the meantime,\nlarge language models (LLMs) emerged as promising solutions for many difficult\nand complex applications/tasks. These lead to a notion of the integration of\nLLMs and wireless networks. However, this integration is challenging and needs\ncareful attention in design. Therefore, in this article, we present a notion of\nrational wireless networks powered by \\emph{telecom LLMs}, namely,\n\\emph{LLM-native wireless systems}. We provide fundamentals, vision, and a case\nstudy of the distributed implementation of LLM-native wireless systems. In the\ncase study, we propose a solution based on double deep Q-learning (DDQN) that\noutperforms existing DDQN solutions. Finally, we provide open challenges.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10651v1", "AI": {"title_translation": "大型语言模型赋能的无线网络：基础、架构与挑战", "tldr": "本文探讨了大型语言模型（LLMs）与无线网络的融合，提出了“电信LLMs”和“LLM原生无线系统”的概念，并提供了一个分布式实现的案例研究，其中基于DDQN的解决方案表现优于现有方案，最后讨论了开放性挑战。", "motivation": "无线网络在满足服务质量（QoS）和体验质量（QoE）方面面临巨大挑战。与此同时，大型语言模型（LLMs）已成为许多复杂应用和任务的有效解决方案。因此，将LLMs与无线网络集成以应对这些挑战成为一个有前景的方向。", "method": "本文提出了由“电信LLMs”驱动的“LLM原生无线系统”的概念。文章提供了其基本原理、愿景，并进行了一个分布式实现的案例研究。在案例研究中，作者提出了一种基于双深度Q学习（DDQN）的解决方案。", "result": "在案例研究中，所提出的基于双深度Q学习（DDQN）的解决方案优于现有的DDQN解决方案。", "conclusion": "本文提出了LLM原生无线系统的概念，并探讨了将大型语言模型与无线网络集成的基本原理、架构和挑战。研究结果表明，这种集成具有潜力，但仍面临许多开放性挑战。", "translation": "无线网络的快速发展带来了诸多挑战，这些挑战源于对服务质量的广泛需求，以及对创新体验质量指标（例如，触觉应用中用户定义的物理体验感指标）的需求。与此同时，大型语言模型（LLMs）作为许多困难和复杂应用/任务的有前景的解决方案而出现。这些都导致了LLMs与无线网络集成概念的提出。然而，这种集成具有挑战性，在设计中需要仔细关注。因此，在本文中，我们提出了由“电信LLMs”驱动的理性无线网络的概念，即“LLM原生无线系统”。我们提供了LLM原生无线系统分布式实现的基本原理、愿景和案例研究。在案例研究中，我们提出了一种基于双深度Q学习（DDQN）的解决方案，该解决方案优于现有的DDQN解决方案。最后，我们提出了开放性挑战。", "summary": "本文探讨了大型语言模型（LLMs）与无线网络的融合，以应对当前无线网络在服务质量（QoS）和体验质量（QoE）方面的挑战。文章提出了“电信LLM”驱动的“LLM原生无线系统”概念，并阐述了其基本原理、愿景，并通过一个分布式实现的案例研究展示了其潜力。案例研究中，作者提出了一种基于双深度Q学习（DDQN）的解决方案，该方案表现优于现有DDQN方案。最后，文章还指出了该领域面临的开放性挑战。", "keywords": "大型语言模型, 无线网络, 电信LLMs, LLM原生系统, 双深度Q学习", "comments": "这篇论文具有创新性，因为它提出了将大型语言模型整合到无线网络中的新颖方法，并创造了“电信LLM”和“LLM原生无线系统”的术语。这种跨学科的方法利用了LLM的先进能力来管理和优化网络，从而应对现代无线通信的复杂需求。性能优于现有DDQN的案例研究为他们的理论框架增加了实际验证，为未来智能无线网络的设计奠定了基础。"}}
{"id": "2506.10441", "title": "EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques", "authors": ["Oğuzhan Canpolat", "Ataberk Olgun", "David Novo", "Oğuz Ergin", "Onur Mutlu"], "summary": "DRAM is a critical component of modern computing systems. Recent works\npropose numerous techniques (that we call DRAM techniques) to enhance\nDRAM-based computing systems' throughput, reliability, and computing\ncapabilities (e.g., in-DRAM bulk data copy). Evaluating the system-wide\nbenefits of DRAM techniques is challenging as they often require modifications\nacross multiple layers of the computing stack. Prior works propose FPGA-based\nplatforms for rapid end-to-end evaluation of DRAM techniques on real DRAM\nchips. Unfortunately, existing platforms fall short in two major aspects: (1)\nthey require deep expertise in hardware description languages, limiting\naccessibility; and (2) they are not designed to accurately model modern\ncomputing systems.\n  We introduce EasyDRAM, an FPGA-based framework for rapid and accurate\nend-to-end evaluation of DRAM techniques on real DRAM chips. EasyDRAM overcomes\nthe main drawbacks of prior FPGA-based platforms with two key ideas. First,\nEasyDRAM removes the need for hardware description language expertise by\nenabling developers to implement DRAM techniques using a high-level language\n(C++). At runtime, EasyDRAM executes the software-defined memory system design\nin a programmable memory controller. Second, EasyDRAM tackles a fundamental\nchallenge in accurately modeling modern systems: real processors typically\noperate at higher clock frequencies than DRAM, a disparity that is difficult to\nreplicate on FPGA platforms. EasyDRAM addresses this challenge by decoupling\nthe processor-DRAM interface and advancing the system state using a novel\ntechnique we call time scaling, which faithfully captures the timing behavior\nof the modeled system.\n  We believe and hope that EasyDRAM will enable innovative ideas in memory\nsystem design to rapidly come to fruition. To aid future research EasyDRAM\nimplementation is open sourced at https://github.com/CMU-SAFARI/EasyDRAM.", "comment": "Extended version of our publication at DSN 2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10441v1", "AI": {"title_translation": "EasyDRAM：一种用于快速准确评估新兴DRAM技术的基于FPGA的基础设施", "tldr": "EasyDRAM是一个基于FPGA的框架，用于快速准确地评估新兴DRAM技术，它通过允许C++编程和创新的时间尺度技术解决了现有平台对硬件专业知识的要求以及建模准确性问题。", "motivation": "评估DRAM技术具有挑战性，因为它们需要跨计算堆栈多层修改。现有的基于FPGA的平台存在两个主要缺点：1) 需要深厚的硬件描述语言专业知识，限制了可访问性；2) 未能准确建模现代计算系统。", "method": "EasyDRAM通过两个关键思想克服了现有平台的缺点：1) 允许开发者使用高级语言（C++）实现DRAM技术，无需硬件描述语言专业知识，并在可编程内存控制器中执行软件定义的内存系统设计。2) 通过解耦处理器-DRAM接口并使用名为“时间尺度”的新颖技术推进系统状态，忠实地捕捉建模系统的时序行为，解决了准确建模现代系统的挑战。", "result": "EasyDRAM能够对真实DRAM芯片上的DRAM技术进行快速准确的端到端评估，克服了现有FPGA平台的局限性，并实现了对现代系统准确的建模。", "conclusion": "作者相信并希望EasyDRAM将使内存系统设计中的创新理念迅速实现。EasyDRAM的实现已开源以帮助未来的研究。", "translation": "DRAM是现代计算系统的关键组成部分。最近的工作提出了许多技术（我们称之为DRAM技术）来增强基于DRAM的系统的吞吐量、可靠性和计算能力（例如，DRAM内批量数据复制）。评估DRAM技术的系统范围效益具有挑战性，因为它们通常需要对计算堆栈的多个层进行修改。先前的工作提出了基于FPGA的平台，用于在真实DRAM芯片上快速进行DRAM技术的端到端评估。不幸的是，现有平台在两个主要方面存在不足：(1) 它们需要深厚的硬件描述语言专业知识，限制了可访问性；(2) 它们并非旨在准确建模现代计算系统。\n我们引入了EasyDRAM，一个基于FPGA的框架，用于在真实DRAM芯片上快速准确地对DRAM技术进行端到端评估。EasyDRAM通过两个关键思想克服了现有基于FPGA平台的主要缺点。首先，EasyDRAM通过允许开发人员使用高级语言（C++）实现DRAM技术，消除了对硬件描述语言专业知识的需求。在运行时，EasyDRAM在可编程内存控制器中执行软件定义的内存系统设计。其次，EasyDRAM解决了准确建模现代系统的一个基本挑战：真实处理器通常以比DRAM更高的时钟频率运行，这种差异在FPGA平台上很难复制。EasyDRAM通过解耦处理器-DRAM接口并使用我们称为时间尺度的新颖技术推进系统状态来解决这一挑战，该技术忠实地捕获了建模系统的时序行为。\n我们相信并希望EasyDRAM将使内存系统设计中的创新理念迅速实现。为了帮助未来的研究，EasyDRAM的实现已在https://github.com/CMU-SAFARI/EasyDRAM 开源。", "summary": "本文介绍了EasyDRAM，一个基于FPGA的框架，旨在解决现有DRAM技术评估平台在易用性和准确性方面的不足。EasyDRAM通过支持C++编程简化了DRAM技术实现，并利用“时间尺度”技术解决了处理器与DRAM频率差异造成的建模挑战，从而实现了对真实DRAM芯片上新兴DRAM技术的快速、准确的端到端评估。该框架的开源旨在促进内存系统设计的创新。", "keywords": "FPGA, DRAM, 内存系统, 评估, 时间尺度", "comments": "EasyDRAM的创新之处在于它显著降低了DRAM技术评估的门槛，通过允许使用高级语言（C++）进行编程，使得非硬件专家也能参与到DRAM系统设计中。其引入的“时间尺度”技术巧妙地解决了FPGA平台在模拟真实处理器-DRAM频率差异方面的难题，提高了评估的准确性。作为一个开源项目，它有望加速内存系统领域的研发进程。"}}
{"id": "2506.10356", "title": "Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector Multiplication?", "authors": ["Omid Asudeh", "Sina Mahdipour Saravani", "Gerald Sabin", "Fabrice Rastello", "P Sadayappan"], "summary": "This work evaluates the impact of sparse matrix reordering on the performance\nof sparse matrix-vector multiplication across different multicore CPU\nplatforms. Reordering can significantly enhance performance by optimizing the\nnon-zero element patterns to reduce total data movement and improve the\nload-balancing. We examine how these gains vary over different CPUs for\ndifferent reordering strategies, focusing on both sequential and parallel\nexecution. We address multiple aspects, including appropriate measurement\nmethodology, comparison across different kinds of reordering strategies,\nconsistency across machines, and impact of load imbalance.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10356v1", "AI": {"title_translation": "稀疏矩阵重排序对稀疏矩阵-向量乘法有效吗？", "tldr": "本研究评估了稀疏矩阵重排序在不同多核CPU平台上对稀疏矩阵-向量乘法性能的影响。", "motivation": "稀疏矩阵重排序可以通过优化非零元素模式来减少总数据移动并改善负载平衡，从而显著提升性能。本研究的动机是评估这种性能提升在不同CPU上、针对不同重排序策略（包括顺序和并行执行）的变化情况。", "method": "本研究通过在不同多核CPU平台上，针对不同的重排序策略（包括顺序和并行执行），评估了稀疏矩阵重排序对稀疏矩阵-向量乘法性能的影响。研究方法涵盖了适当的测量方法、不同重排序策略之间的比较、机器之间的一致性以及负载不平衡的影响。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "这项工作评估了稀疏矩阵重排序对不同多核CPU平台上稀疏矩阵-向量乘法性能的影响。重排序可以通过优化非零元素模式来减少总数据移动并改善负载平衡，从而显著提升性能。我们研究了这些增益在不同CPU上、针对不同重排序策略（重点关注顺序和并行执行）的变化情况。我们解决了多个方面的问题，包括适当的测量方法、不同类型重排序策略之间的比较、机器之间的一致性以及负载不平衡的影响。", "summary": "本研究评估了稀疏矩阵重排序对多核CPU上稀疏矩阵-向量乘法性能的影响。重排序能通过优化数据模式来减少数据移动和改善负载平衡，从而提升性能。论文探讨了这些性能增益在不同CPU和不同重排序策略（包括顺序和并行）下的变化，并讨论了测量方法、策略比较、跨机器一致性及负载不平衡等问题。", "keywords": "稀疏矩阵, 矩阵重排序, 稀疏矩阵-向量乘法, 多核CPU, 性能优化", "comments": "该论文着重于评估稀疏矩阵重排序这一经典优化技术在现代多核CPU架构上的实际效果。其重要性在于，尽管重排序技术已存在，但在不同硬件平台和并行执行环境下其具体性能表现和影响因素仍需深入研究。论文关注了测量方法和负载不平衡等实践问题，这对于理解和应用此类优化至关重要。"}}
{"id": "2506.10224", "title": "Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces", "authors": ["Alejandro N Diaz", "Shane A McQuarrie", "John T Tencer", "Patrick J Blonigan"], "summary": "This paper develops an interpretable, non-intrusive reduced-order modeling\ntechnique using regularized kernel interpolation. Existing non-intrusive\napproaches approximate the dynamics of a reduced-order model (ROM) by solving a\ndata-driven least-squares regression problem for low-dimensional matrix\noperators. Our approach instead leverages regularized kernel interpolation,\nwhich yields an optimal approximation of the ROM dynamics from a user-defined\nreproducing kernel Hilbert space. We show that our kernel-based approach can\nproduce interpretable ROMs whose structure mirrors full-order model structure\nby embedding judiciously chosen feature maps into the kernel. The approach is\nflexible and allows a combination of informed structure through feature maps\nand closure terms via more general nonlinear terms in the kernel. We also\nderive a computable a posteriori error bound that combines standard error\nestimates for intrusive projection-based ROMs and kernel interpolants. The\napproach is demonstrated in several numerical experiments that include\ncomparisons to operator inference using both proper orthogonal decomposition\nand quadratic manifold dimension reduction.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10224v1", "AI": {"title_translation": "可解释且灵活的基于再生核希尔伯特空间的非侵入式降阶模型", "tldr": "一种新的基于再生核希尔伯特空间的非侵入式降阶模型，具有可解释性和灵活性，并提供误差界。", "motivation": "现有非侵入式降阶模型（ROM）通过数据驱动的最小二乘回归来近似动力学，本研究旨在提出一种更可解释、更灵活的方法。", "method": "本文开发了一种使用正则化核插值技术的可解释、非侵入式降阶建模方法。该方法利用用户定义的再生核希尔伯特空间，通过在核中嵌入特征映射来使ROM结构与全阶模型结构保持一致，并允许通过特征映射和更一般的非线性核项结合结构和闭合项。此外，还推导了一个可计算的后验误差界。", "result": "该核方法能够生成结构与全阶模型相似的可解释降阶模型，并且具有灵活性，可结合通过特征映射获得的结构信息以及通过更一般的非线性核项获得的闭合项。该方法在多个数值实验中得到了验证，并与使用POD和二次流形降维的算子推断进行了比较。", "conclusion": "本文提出的基于再生核希尔伯特空间的非侵入式降阶建模方法是可解释且灵活的，并通过数值实验证明了其有效性和优越性。", "translation": "本文开发了一种使用正则化核插值的可解释、非侵入式降阶建模技术。现有的非侵入式方法通过求解低维矩阵算子的数据驱动最小二乘回归问题来近似降阶模型（ROM）的动力学。我们的方法则利用正则化核插值，从用户定义的再生核希尔伯特空间中获得ROM动力学的最优近似。我们展示了我们的基于核的方法可以通过在核中嵌入精心选择的特征映射来生成结构与全阶模型结构相似的可解释ROM。该方法具有灵活性，允许通过特征映射组合信息结构，并通过核中更一般的非线性项组合闭合项。我们还推导了一个可计算的后验误差界，该误差界结合了侵入式基于投影的ROM和核插值器的标准误差估计。该方法在多个数值实验中得到了验证，其中包括与使用本征正交分解和二次流形降维的算子推断的比较。", "summary": "本文提出了一种基于正则化核插值的可解释、非侵入式降阶建模技术。该方法利用再生核希尔伯特空间提供对降阶模型动力学的最优近似，并通过嵌入特征映射实现模型结构的可解释性，同时通过结合特征映射和非线性核项实现灵活性。研究还推导了一个可计算的后验误差界，并通过数值实验验证了该方法的有效性。", "keywords": "降阶模型, 再生核希尔伯特空间, 非侵入式, 核插值, 可解释性", "comments": "本文的创新点在于将正则化核插值应用于非侵入式降阶模型，实现了模型的可解释性和灵活性，并提供了理论上的误差界。"}}
{"id": "2506.10610", "title": "Minimality and computability of languages of G-shifts", "authors": ["Djamel Eddine Amir", "Benjamin Hellouin de Menibus"], "summary": "Motivated by the notion of strong computable type for sets in computable\nanalysis, we define the notion of strong computable type for $G$-shifts, where\n$G$ is a finitely generated group with decidable word problem. A $G$-shift has\nstrong computable type if one can compute its language from the complement of\nits language. We obtain a characterization of $G$-shifts with strong computable\ntype in terms of a notion of minimality with respect to properties with a\nbounded computational complexity. We provide a self-contained direct proof, and\nalso explain how this characterization can be obtained from an existing similar\ncharacterization for sets by Amir and Hoyrup, and discuss its connexions with\nresults by Jeandel on closure spaces. We apply this characterization to several\nclasses of shifts that are minimal with respect to specific properties. This\nprovides a unifying approach that not only generalizes many existing results\nbut also has the potential to yield new findings effortlessly. In contrast to\nthe case of sets, we prove that strong computable type for G-shifts is\npreserved under products. We conclude by discussing some generalizations and\nfuture directions.", "comment": "Accepted to ICALP 2025", "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.10610v1", "AI": {"title_translation": "G-移位语言的最小性和可计算性", "tldr": "本文定义了G-移位的强可计算类型，并将其特性化为最小性概念，提供了一个统一的方法，概括了现有结果，并证明了其在乘积下的保持性。", "motivation": "受可计算分析中集合的强可计算类型概念的启发。", "method": "定义了G-移位的强可计算类型，即可以从其语言的补集计算其语言。通过与有界计算复杂度的最小性概念，获得了G-移位强可计算类型的特性化。提供了一个独立的直接证明，并解释了如何从Amir和Hoyrup对集合的现有相似特性化中获得，并讨论了其与Jeandel关于闭合空间结果的联系。将此特性化应用于几类在特定属性下是最小的移位。", "result": "获得了G-移位强可计算类型的特性化，这提供了一种统一的方法，不仅概括了许多现有结果，而且有望轻松产生新的发现。与集合的情况不同，证明了G-移位的强可计算类型在乘积下是保持的。", "conclusion": "讨论了一些泛化和未来的方向。", "translation": "受可计算分析中集合的强可计算类型概念的启发，我们定义了G-移位的强可计算类型概念，其中G是具有可判定词问题的有限生成群。如果可以从其语言的补集计算出其语言，则G-移位具有强可计算类型。我们通过一个关于有界计算复杂度的最小性概念，获得了G-移位强可计算类型的特性化。我们提供了一个独立的直接证明，并解释了如何从Amir和Hoyrup对集合的现有相似特性化中获得这种特性化，并讨论了其与Jeandel关于闭合空间结果的联系。我们将此特性化应用于几类在特定属性下是最小的移位。这提供了一种统一的方法，不仅概括了许多现有结果，而且有望轻松产生新的发现。与集合的情况不同，我们证明了G-移位的强可计算类型在乘积下是保持的。最后，我们讨论了一些泛化和未来的方向。", "summary": "本文在可计算分析中集合的强可计算类型概念的启发下，为G-移位定义了强可计算类型，即其语言可从其补集计算。研究通过一个关于有界计算复杂度的最小性概念，对具有强可计算类型的G-移位进行了特性化，并提供了直接证明，同时探讨了其与现有集合特性化及闭合空间结果的关联。该特性化被应用于多类最小移位，展示了其泛化现有成果并促进新发现的潜力。此外，研究还证明了G-移位的强可计算类型在乘积下得以保持，这与集合的情况不同。", "keywords": "G-移位, 强可计算类型, 最小性, 语言, 可计算性", "comments": "本文的创新之处在于将强可计算类型的概念从集合推广到G-移位，并提出了一个基于最小性的统一特性化方法。其重要性体现在它不仅能够概括现有结果，还具有产生新发现的潜力。此外，发现G-移位的强可计算类型在乘积下保持，这与集合的情况形成对比，是一个重要的理论贡献。"}}
{"id": "2506.10217", "title": "Data-Centric Safety and Ethical Measures for Data and AI Governance", "authors": ["Srija Chakraborty"], "summary": "Datasets play a key role in imparting advanced capabilities to artificial\nintelligence (AI) foundation models that can be adapted to various downstream\ntasks. These downstream applications can introduce both beneficial and harmful\ncapabilities -- resulting in dual use AI foundation models, with various\ntechnical and regulatory approaches to monitor and manage these risks. However,\ndespite the crucial role of datasets, responsible dataset design and ensuring\ndata-centric safety and ethical practices have received less attention. In this\nstudy, we pro-pose responsible dataset design framework that encompasses\nvarious stages in the AI and dataset lifecycle to enhance safety measures and\nreduce the risk of AI misuse due to low quality, unsafe and unethical data\ncontent. This framework is domain agnostic, suitable for adoption for various\napplications and can promote responsible practices in dataset creation, use,\nand sharing to facilitate red teaming, minimize risks, and increase trust in AI\nmodels.", "comment": "Paper accepted and presented at the AAAI 2025 Workshop on Datasets\n  and Evaluators of AI Safety https://sites.google.com/view/datasafe25/home", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10217v1", "AI": {"title_translation": "以数据为中心的数据和人工智能治理安全与伦理措施", "tldr": "本研究提出了一个负责任的数据集设计框架，旨在增强人工智能模型的安全性，并通过解决数据质量、不安全和不道德的数据内容问题来降低人工智能滥用的风险。", "motivation": "尽管数据集在赋予AI基础模型高级能力方面发挥着关键作用，但负责任的数据集设计以及确保以数据为中心的安全和伦理实践却受到了较少关注，这可能导致AI滥用风险。", "method": "本研究提出了一个负责任的数据集设计框架，该框架涵盖了AI和数据集生命周期的各个阶段。", "result": "该框架旨在增强安全措施，并减少因低质量、不安全和不道德数据内容导致AI滥用的风险。它具有领域无关性，适用于各种应用，并能促进数据集创建、使用和共享中的负责任实践，从而有助于红队测试、最小化风险并增加对AI模型的信任。", "conclusion": "所提出的负责任的数据集设计框架能够促进数据集创建、使用和共享中的负责任实践，从而减少AI滥用风险并增加对AI模型的信任。", "translation": "数据集在赋予人工智能（AI）基础模型高级能力方面发挥着关键作用，这些模型可以适应各种下游任务。这些下游应用可能带来有益和有害的能力——导致双重用途的AI基础模型，并有各种技术和监管方法来监控和管理这些风险。然而，尽管数据集起着至关重要的作用，但负责任的数据集设计和确保以数据为中心的安全和伦理实践却受到了较少关注。在本研究中，我们提出了一个负责任的数据集设计框架，该框架涵盖了AI和数据集生命周期的各个阶段，以增强安全措施并减少因低质量、不安全和不道德数据内容导致AI滥用的风险。该框架是领域无关的，适用于各种应用，并能促进数据集创建、使用和共享中的负责任实践，以促进红队测试，最小化风险，并增加对AI模型的信任。", "summary": "本研究提出一个以数据为中心的负责任数据集设计框架，旨在解决当前AI发展中数据集安全与伦理实践受关注不足的问题。该框架涵盖AI和数据集生命周期的多个阶段，旨在通过解决低质量、不安全和不道德的数据内容，增强AI模型的安全性并降低其滥用风险。该框架具有领域无关性，适用于多种应用，并能促进负责任的数据集创建、使用和共享，从而有助于风险最小化和提升AI信任度。", "keywords": "数据安全, AI治理, 伦理措施, 数据集设计, 风险管理", "comments": "本文的创新之处在于其明确提出并专注于“以数据为中心”的安全与伦理措施，并为此设计了一个涵盖AI和数据集整个生命周期的框架。这对于解决AI模型潜在的“双重用途”问题以及提升AI的可信赖性具有重要意义。该框架的领域无关性使其具有广泛的应用潜力。"}}
{"id": "2506.10121", "title": "HiKO: A Hierarchical Framework for Beyond-Second-Order KO Codes", "authors": ["Shubham Srivastava", "Adrish Banerjee"], "summary": "This paper introduces HiKO (Hierarchical Kronecker Operation), a novel\nframework for training high-rate neural error-correcting codes that enables KO\ncodes to outperform Reed-Muller codes beyond second order. To our knowledge,\nthis is the first attempt to extend KO codes beyond second order. While\nconventional KO codes show promising results for low-rate regimes ($r < 2$),\nthey degrade at higher rates -- a critical limitation for practical deployment.\nOur framework incorporates three key innovations: (1) a hierarchical training\nmethodology that decomposes complex high-rate codes into simpler constituent\ncodes for efficient knowledge transfer, (2) enhanced neural architectures with\ndropout regularization and learnable skip connections tailored for the Plotkin\nstructure, and (3) a progressive unfreezing strategy that systematically\ntransitions from pre-trained components to fully optimized integrated codes.\nOur experiments show that HiKO codes consistently outperform traditional\nReed-Muller codes across various configurations, achieving notable performance\nimprovements for third-order ($r = 3$) and fourth-order ($r = 4$) codes.\nAnalysis reveals that HiKO codes successfully approximate Shannon-optimal\nGaussian codebooks while preserving efficient decoding properties. This\nrepresents the first successful extension of KO codes beyond second order,\nopening new possibilities for neural code deployment in high-throughput\ncommunication systems.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10121v1", "AI": {"title_translation": "HiKO：一种超越二阶KO码的层次化框架", "tldr": "HiKO是一种新的分层框架，首次将KO码扩展到二阶以上，解决了高码率下的性能下降问题，并超越了Reed-Muller码。", "motivation": "传统KO码在低码率下表现良好，但在高码率下性能下降，这限制了其实际部署。本文旨在将KO码扩展到二阶以上，以解决这一局限性。", "method": "本文引入了HiKO（Hierarchical Kronecker Operation）框架，包含三项关键创新：1. 分层训练方法，将复杂的高码率码分解为更简单的组成码以实现高效知识迁移；2. 针对Plotkin结构增强的神经网络架构，采用Dropout正则化和可学习跳跃连接；3. 渐进式解冻策略，系统地从预训练组件过渡到完全优化的集成码。", "result": "HiKO码在各种配置下始终优于传统的Reed-Muller码，在三阶（r=3）和四阶（r=4）码上实现了显著的性能提升。分析表明，HiKO码成功地近似了香农最优高斯码本，同时保留了高效的解码特性。", "conclusion": "HiKO是首次成功将KO码扩展到二阶以上，为高吞吐量通信系统中的神经码部署开辟了新的可能性。", "translation": "本文介绍了HiKO（分层克罗内克运算），一种用于训练高码率神经纠错码的新颖框架，它使KO码能够超越二阶Reed-Muller码。据我们所知，这是首次尝试将KO码扩展到二阶以上。虽然传统的KO码在低码率（r < 2）下表现出有希望的结果，但它们在高码率下性能下降——这是实际部署的关键限制。我们的框架包含了三项关键创新：(1) 一种分层训练方法，将复杂的高码率码分解为更简单的组成码，以实现高效的知识迁移；(2) 针对Plotkin结构增强的神经网络架构，带有Dropout正则化和可学习跳跃连接；(3) 一种渐进式解冻策略，系统地从预训练组件过渡到完全优化的集成码。我们的实验表明，HiKO码在各种配置下始终优于传统的Reed-Muller码，在三阶（r = 3）和四阶（r = 4）码上实现了显著的性能改进。分析表明，HiKO码成功地近似了香农最优高斯码本，同时保留了高效的解码特性。这代表了KO码首次成功扩展到二阶以上，为高吞吐量通信系统中的神经码部署开辟了新的可能性。", "summary": "本文提出HiKO（分层克罗内克运算）框架，首次将神经纠错KO码扩展到二阶以上，解决了传统KO码在高码率下的性能瓶颈。HiKO通过分层训练、增强神经网络架构和渐进式解冻策略等创新，在高阶码上显著超越了传统的Reed-Muller码，并能近似香农最优高斯码本，为高吞吐量通信系统中的神经码应用提供了新途径。", "keywords": "神经纠错码, KO码, 高阶码, 分层训练, Reed-Muller码", "comments": "HiKO框架的创新之处在于其分层训练方法和针对高阶码的架构优化，成功将KO码的应用范围扩展到传统限制之外，对于提升高吞吐量通信系统的编码效率具有重要意义。这是神经纠错码领域的一个重要进展。"}}
{"id": "2506.10221", "title": "Model Predictive Control-Based Optimal Energy Management of Autonomous Electric Vehicles Under Cold Temperatures", "authors": ["Shanthan Kumar Padisala", "Satadru Dey"], "summary": "In autonomous electric vehicles (AEVs), battery energy must be judiciously\nallocated to satisfy primary propulsion demands and secondary auxiliary\ndemands, particularly the Heating, Ventilation, and Air Conditioning (HVAC)\nsystem. This becomes especially critical when the battery is in a low state of\ncharge under cold ambient conditions, and cabin heating and battery\npreconditioning (prior to actual charging) can consume a significant percentage\nof available energy, directly impacting the driving range. In such cases, one\nusually prioritizes propulsion or applies heuristic rules for thermal\nmanagement, often resulting in suboptimal energy utilization. There is a\npressing need for a principled approach that can dynamically allocate battery\npower in a way that balances thermal comfort, battery health and\npreconditioning, along with range preservation. This paper attempts to address\nthis issue using real-time Model Predictive Control to optimize the power\nconsumption between the propulsion, HVAC, and battery temperature preparation\nso that it can be charged immediately once the destination is reached.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10221v1", "AI": {"title_translation": "模型预测控制在低温环境下自动电动汽车最优能量管理中的应用", "tldr": "本文提出了一种基于模型预测控制的方法，用于在低温环境下优化自动电动汽车的电池能量分配，以平衡热舒适、电池健康和续航里程，尤其是在低电量时。", "motivation": "在低温环境下，自动电动汽车（AEV）的电池能量分配面临挑战，尤其是当电池电量低时，车厢加热和电池预处理会消耗大量能量，影响续航里程。传统方法通常优先考虑推进或采用启发式规则，导致能量利用次优。因此，迫切需要一种有原则的方法来动态分配电池功率，以平衡热舒适、电池健康、预处理和续航里程。", "method": "使用实时模型预测控制（Model Predictive Control, MPC）来优化推进系统、HVAC系统和电池温度准备之间的功耗分配。", "result": "Not mentioned in abstract", "conclusion": "本文通过实时模型预测控制，旨在解决低温环境下自动电动汽车的能量管理问题，实现热舒适、电池健康和续航里程之间的平衡，并确保到达目的地后电池能立即充电。", "translation": "在自动电动汽车（AEV）中，电池能量必须审慎分配以满足主要推进需求和次要辅助需求，特别是供暖、通风和空调（HVAC）系统。当电池在低温环境条件下降至低电量时，这一点变得尤为关键，因为车厢加热和电池预处理（实际充电前）会消耗可用能量的很大一部分，直接影响续航里程。在这种情况下，通常会优先考虑推进或应用启发式规则进行热管理，这往往导致能量利用次优。迫切需要一种有原则的方法，能够动态分配电池功率，以平衡热舒适、电池健康、预处理以及续航里程的保持。本文试图通过使用实时模型预测控制来解决这个问题，以优化推进、HVAC和电池温度准备之间的功耗，从而确保一旦到达目的地即可立即充电。", "summary": "本文提出了一种基于实时模型预测控制（MPC）的策略，旨在解决自动电动汽车（AEV）在低温环境下电池能量管理的问题。在低温且电池电量低时，HVAC和电池预处理会显著消耗能量并影响续航里程。针对传统启发式方法的不足，该研究通过MPC优化推进、HVAC和电池预处理之间的功率分配，以平衡乘员舒适度、电池健康、预处理需求和续航里程，确保能量高效利用并支持快速充电。", "keywords": "模型预测控制, 能量管理, 自动电动汽车, 低温, 电池预处理", "comments": "本文的创新点在于将模型预测控制应用于低温环境下电动汽车的综合能量管理，特别是考虑了电池预处理和HVAC与推进的协同优化，这对于提升电动汽车在冬季的实用性和用户体验具有重要意义。它试图解决一个实际且关键的能量分配挑战，避免了次优的启发式方法。"}}
{"id": "2506.10156", "title": "Quantifying Data Requirements for EEG Independent Component Analysis Using AMICA", "authors": ["Gwenevere Frank", "Seyed Yahya Shirazi", "Jason Palmer", "Gert Cauwenberghs", "Scott Makeig", "Arnaud Delorme"], "summary": "Independent Component Analysis (ICA) is an important step in EEG processing\nfor a wide-ranging set of applications. However, ICA requires well-designed\nstudies and data collection practices to yield optimal results. Past studies\nhave focused on quantitative evaluation of the differences in quality produced\nby different ICA algorithms as well as different configurations of parameters\nfor AMICA, a multimodal ICA algorithm that is considered the benchmark against\nwhich other algorithms are measured. Here, the effect of the data quantity\nversus the number of channels on decomposition quality is explored. AMICA\ndecompositions were run on a 71 channel dataset with 13 subjects while randomly\nsubsampling data to correspond to specific ratios of the number of frames in a\ndataset to the channel count. Decomposition quality was evaluated for the\nvarying quantities of data using measures of mutual information reduction (MIR)\nand the near dipolarity of components. We also note that an asymptotic trend\ncan be seen in the increase of MIR and a general increasing trend in near\ndipolarity with increasing data, but no definitive plateau in these metrics was\nobserved, suggesting that the benefits of collecting additional EEG data may\nextend beyond common heuristic thresholds and continue to enhance decomposition\nquality.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10156v1", "AI": {"title_translation": "使用AMICA量化EEG独立成分分析的数据需求", "tldr": "研究了AMICA在EEG独立成分分析中数据量与通道数对分解质量的影响，发现数据量越多，分解质量越好，且没有明确的上限。", "motivation": "过去的ICA研究主要关注算法和参数配置，而数据量对EEG独立成分分析（ICA）分解质量的影响尚未被充分探索，鉴于ICA对高质量数据的需求，有必要量化数据量与通道数对分解质量的影响。", "method": "在一个包含13个受试者的71通道EEG数据集上运行AMICA分解，通过随机抽样数据以对应数据集中帧数与通道数的特定比例来改变数据量。分解质量通过互信息减少量（MIR）和成分的近偶极性进行评估。", "result": "随着数据量的增加，互信息减少量（MIR）呈渐近趋势增加，成分的近偶极性也普遍增加。然而，这些指标没有观察到明确的平台期。", "conclusion": "收集额外EEG数据的好处可能超出常见的经验阈值，并继续提高独立成分分析的分解质量。", "translation": "独立成分分析（ICA）是脑电图（EEG）处理中一个重要的步骤，广泛应用于各种应用。然而，ICA需要精心设计的研究和数据收集实践才能产生最佳结果。过去的研究主要集中于定量评估不同ICA算法以及AMICA（一种被认为是衡量其他算法基准的多模态ICA算法）不同参数配置产生的质量差异。本文探讨了数据量与通道数对分解质量的影响。AMICA分解在一个包含13个受试者的71通道数据集上运行，同时随机抽样数据以对应数据集中帧数与通道数的特定比例。使用互信息减少量（MIR）和成分的近偶极性指标评估了不同数据量下的分解质量。我们还注意到，随着数据量的增加，MIR呈渐近趋势增加，近偶极性也普遍呈增加趋势，但未观察到这些指标的明确平台期，这表明收集额外EEG数据的好处可能超出常见的经验阈值，并继续提高分解质量。", "summary": "本文旨在量化在使用AMICA进行EEG独立成分分析时，数据量与通道数对分解质量的影响。研究通过在一个71通道、13受试者的数据集上，随机抽样不同比例的数据帧与通道数，并以互信息减少量和近偶极性作为评估指标。结果显示，随着数据量的增加，分解质量持续提升，且未观察到明确的饱和平台期，这表明收集更多的EEG数据可能持续优化ICA分解效果，超越了传统的经验阈值。", "keywords": "EEG, 独立成分分析, AMICA, 数据量, 分解质量", "comments": "这项研究深入探讨了EEG独立成分分析中一个关键但常被忽视的因素——数据量。其发现挑战了传统观念，即数据量达到一定阈值后收益递减，为EEG实验设计和数据收集提供了新的视角和指导，鼓励研究者收集更多数据以期获得更优的ICA分解结果，具有重要的实践意义。"}}
{"id": "2506.10312", "title": "AC/DC: LLM-based Audio Comprehension via Dialogue Continuation", "authors": ["Yusuke Fujita", "Tomoya Mizumoto", "Atsushi Kojima", "Lianbo Liu", "Yui Sudo"], "summary": "We propose an instruction-following audio comprehension model that leverages\nthe dialogue continuation ability of large language models (LLMs). Instead of\ndirectly generating target captions in training data, the proposed method\ntrains a model to produce responses as if the input caption triggered a\ndialogue. This dialogue continuation training mitigates the caption variation\nproblem. Learning to continue a dialogue effectively captures the caption's\nmeaning beyond its surface-level words. As a result, our model enables\nzero-shot instruction-following capability without multitask instruction\ntuning, even trained solely on audio captioning datasets. Experiments on\nAudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene\nquestion-answering tests demonstrate our model's ability to follow various\nunseen instructions.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10312v1", "AI": {"title_translation": "AC/DC: 基于LLM的通过对话延续实现的音频理解", "tldr": "提出了一种基于LLM的音频理解模型AC/DC，通过对话延续训练解决字幕变异问题，实现零样本指令遵循能力。", "motivation": "传统方法在训练数据中直接生成目标字幕存在字幕变异问题，且难以捕捉字幕的深层含义。", "method": "本文提出了AC/DC模型，一个指令遵循音频理解模型，它利用大型语言模型（LLM）的对话延续能力。该模型不直接生成目标字幕，而是训练模型生成仿佛输入字幕触发对话的响应，从而有效捕捉字幕的深层含义。", "result": "即使仅在音频字幕数据集上训练，AC/DC模型也能实现零样本指令遵循能力，无需多任务指令微调。在AudioCaps、WavCaps和Clotho数据集上通过AudioBench音频场景问答测试，证明了模型能够遵循各种未见指令。", "conclusion": "通过将音频理解任务转化为对话延续，AC/DC模型成功缓解了字幕变异问题，并展现出强大的零样本指令遵循能力。", "translation": "我们提出了一种指令遵循音频理解模型，该模型利用大型语言模型（LLM）的对话延续能力。该方法不直接在训练数据中生成目标字幕，而是训练模型生成仿佛输入字幕触发了对话的响应。这种对话延续训练缓解了字幕变异问题。学习延续对话能有效地捕捉字幕超越表面词汇的含义。因此，我们的模型即使仅在音频字幕数据集上训练，也能实现零样本指令遵循能力，无需多任务指令微调。在AudioCaps、WavCaps和Clotho数据集上进行的AudioBench音频场景问答测试实验表明，我们的模型能够遵循各种未见指令。", "summary": "本文提出了AC/DC，一个基于LLM的音频理解模型，通过创新的对话延续训练方法，而非直接生成字幕，来解决音频字幕的变异问题并捕捉深层含义。该方法使模型即使仅在音频字幕数据集上训练，也能展现出卓越的零样本指令遵循能力，并在多个标准数据集上得到了验证。", "keywords": "音频理解, 大型语言模型, 对话延续, 零样本学习, 指令遵循", "comments": "该论文通过将音频理解任务转化为对话延续任务，巧妙地利用了LLM的强大能力，有效解决了传统音频字幕生成中的字幕变异问题，并实现了零样本指令遵循，这对于音频理解领域是一个重要的创新。"}}
{"id": "2506.10157", "title": "One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence", "authors": ["Michelle M. Li", "Ben Y. Reis", "Adam Rodman", "Tianxi Cai", "Noa Dagan", "Ran D. Balicer", "Joseph Loscalzo", "Isaac S. Kohane", "Marinka Zitnik"], "summary": "Medical foundation models, including language models trained on clinical\nnotes, vision-language models on medical images, and multimodal models on\nelectronic health records, can summarize clinical notes, answer medical\nquestions, and assist in decision-making. Adapting these models to new\npopulations, specialties, or settings typically requires fine-tuning, careful\nprompting, or retrieval from knowledge bases. This can be impractical, and\nlimits their ability to interpret unfamiliar inputs and adjust to clinical\nsituations not represented during training. As a result, models are prone to\ncontextual errors, where predictions appear reasonable but fail to account for\ncritical patient-specific or contextual information. These errors stem from a\nfundamental limitation that current models struggle with: dynamically adjusting\ntheir behavior across evolving contexts of medical care. In this Perspective,\nwe outline a vision for context-switching in medical AI: models that\ndynamically adapt their reasoning without retraining to new specialties,\npopulations, workflows, and clinical roles. We envision context-switching AI to\ndiagnose, manage, and treat a wide range of diseases across specialties and\nregions, and expand access to medical care.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10157v1", "AI": {"title_translation": "一个病人，多种情境：通过情境智能扩展医疗AI", "tldr": "本文提出了一种愿景，即医疗AI模型应能动态适应不同的医疗情境，而无需重新训练，以解决现有模型在处理未训练情境时容易出现情境错误的问题。", "motivation": "当前的医疗基础模型在适应新的患者群体、专业领域或设置时，通常需要微调、精心提示或从知识库中检索，这既不切实际也限制了它们解释不熟悉输入和适应训练中未涵盖的临床情况的能力。这导致模型容易出现情境错误，即预测看似合理但未能考虑关键的患者特定或情境信息。这些错误源于现有模型难以动态调整其行为以适应不断变化的医疗护理情境这一基本限制。", "method": "本文提出了一种在医疗AI中实现情境切换的愿景：即模型能够无需重新训练就能动态调整其推理，以适应新的专业、人群、工作流程和临床角色。", "result": "Not mentioned in abstract", "conclusion": "本文设想情境切换AI将能够诊断、管理和治疗跨专业和地区的各种疾病，并扩大医疗服务的可及性。", "translation": "医疗基础模型，包括在临床笔记上训练的语言模型、在医学图像上训练的视觉-语言模型以及在电子健康记录上训练的多模态模型，能够总结临床笔记、回答医学问题并协助决策。将这些模型适应新的患者群体、专业领域或设置通常需要微调、精心提示或从知识库中检索。这既不切实际，也限制了它们解释不熟悉输入和适应训练中未涵盖的临床情况的能力。因此，模型容易出现情境错误，即预测看似合理但未能考虑关键的患者特定或情境信息。这些错误源于现有模型难以动态调整其行为以适应不断变化的医疗护理情境这一基本限制。在此视角中，我们概述了医疗AI中情境切换的愿景：模型能够无需重新训练就能动态调整其推理，以适应新的专业、人群、工作流程和临床角色。我们设想情境切换AI将能够诊断、管理和治疗跨专业和地区的各种疾病，并扩大医疗服务的可及性。", "summary": "本文探讨了当前医疗AI模型在适应不同医疗情境时面临的挑战，即需要耗时的微调或提示才能避免情境错误。作者提出了一种“情境切换”的愿景，旨在开发能够动态适应新专业、人群和工作流程而无需重新训练的医疗AI模型。这种能力有望使AI更准确地理解和应对患者的复杂性，从而扩大医疗服务的可及性并提升诊断和治疗能力。", "keywords": "医疗AI, 情境智能, 基础模型, 动态适应, 情境切换", "comments": "本文提出了一种前瞻性的视角，强调了医疗AI领域中一个关键但被忽视的问题：情境适应性。其创新之处在于提出了“情境切换”的概念，旨在克服现有模型在处理未见情境时的局限性，减少情境错误。如果这一愿景得以实现，将极大地提高医疗AI的鲁棒性和实用性，使其能够更广泛、更安全地应用于临床实践，尤其是在资源受限或专业领域多样化的环境中。"}}
{"id": "2506.10717", "title": "Structural Parameterizations of $k$-Planarity", "authors": ["Tatsuya Gima", "Yasuaki Kobayashi", "Yuto Okada"], "summary": "The concept of $k$-planarity is extensively studied in the context of Beyond\nPlanarity. A graph is $k$-planar if it admits a drawing in the plane in which\neach edge is crossed at most $k$ times. The local crossing number of a graph is\nthe minimum integer $k$ such that it is $k$-planar. The problem of determining\nwhether an input graph is $1$-planar is known to be NP-complete even for\nnear-planar graphs [Cabello and Mohar, SIAM J. Comput. 2013], that is, the\ngraphs obtained from planar graphs by adding a single edge. Moreover, the local\ncrossing number is hard to approximate within a factor $2 - \\varepsilon$ for\nany $\\varepsilon > 0$ [Urschel and Wellens, IPL 2021]. To address this\ncomputational intractability, Bannister, Cabello, and Eppstein [JGAA 2018]\ninvestigated the parameterized complexity of the case of $k = 1$, particularly\nfocusing on structural parameterizations on input graphs, such as treedepth,\nvertex cover number, and feedback edge number. In this paper, we extend their\napproach by considering the general case $k \\ge 1$ and give (tight)\nparameterized upper and lower bound results. In particular, we strengthen the\naforementioned lower bound results to subclasses of constant-treewidth graphs:\nwe show that testing $1$-planarity is NP-complete even for near-planar graphs\nwith feedback vertex set number at most $3$ and pathwidth at most $4$, and the\nlocal crossing number is hard to approximate within any constant factor for\ngraphs with feedback vertex set number at most $2$.", "comment": "20 pages, 9 figures", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.10717v1", "AI": {"title_translation": "k-平面性的结构参数化", "tldr": "本文研究了 $k$-平面性（$k \text{ ≥ } 1$）的参数化复杂性，提供了紧密的参数化上下界结果，并针对特定图类强化了先前的难解性结论。", "motivation": "确定图是否为 $k$-平面以及近似局部交叉数的问题在计算上是难解的（NP-完全且难以近似）。为了解决这种计算上的难处理性，先前的研究针对 $k=1$ 的情况探讨了基于输入图结构参数化的方法。本文旨在将这种方法推广到一般的 $k \text{ ≥ } 1$。", "method": "本文将Bannister、Cabello和Eppstein（2018）针对 $k=1$ 的结构参数化方法推广到一般情况 $k \text{ ≥ } 1$。通过考虑不同的图结构参数，作者给出了（紧密的）参数化上界和下界结果。此外，他们还将先前关于难解性的下界结果推广并强化到常数树宽图的子类上。", "result": "本文给出了针对 $k \text{ ≥ } 1$ 的 $k$-平面性的（紧密的）参数化上界和下界结果。具体而言，他们强化了先前的下界结果：即使对于反馈顶点集数至多为3且路径宽度至多为4的近平面图，测试1-平面性仍然是NP-完全的；对于反馈顶点集数至多为2的图，局部交叉数在任意常数因子内都难以近似。", "conclusion": "本文为 $k$-平面性（针对一般 $k \text{ ≥ } 1$）建立了新的（且紧密的）参数化复杂性界限，进一步揭示了即使对于高度结构化的图类，该问题在计算上仍然是困难的。", "translation": "k-平面性的结构参数化\n\nk-平面性的概念在“超越平面性”的背景下得到了广泛研究。一个图是k-平面的，如果它允许在平面上绘制，使得每条边最多被交叉k次。图的局部交叉数是使其成为k-平面的最小整数k。即使对于近平面图（即通过添加一条边从平面图获得的图），确定输入图是否为1-平面图的问题也已知是NP-完全的[Cabello和Mohar，SIAM J. Comput. 2013]。此外，对于任何 ε > 0，局部交叉数在2 - ε因子内都难以近似[Urschel和Wellens，IPL 2021]。为了解决这种计算上的难处理性，Bannister、Cabello和Eppstein[JGAA 2018]研究了k=1情况的参数化复杂性，特别关注输入图的结构参数化，例如树深、顶点覆盖数和反馈边数。在本文中，我们通过考虑一般情况k ≥ 1来扩展他们的方法，并给出（紧密的）参数化上界和下界结果。特别是，我们将上述下界结果强化到常数树宽图的子类：我们表明，即使对于反馈顶点集数至多为3且路径宽度至多为4的近平面图，测试1-平面性仍然是NP-完全的；对于反馈顶点集数至多为2的图，局部交叉数在任意常数因子内都难以近似。", "summary": "本文研究了图的 $k$-平面性（即每条边最多被交叉 $k$ 次的平面绘制）的参数化复杂性，该问题因其NP-完全性和难以近似性而具有计算挑战性。在扩展先前针对 $k=1$ 的结构参数化工作的基础上，本研究为一般 $k \text{ ≥ } 1$ 的情况提供了（紧密的）参数化上界和下界结果。值得注意的是，研究强化了现有下界，证明了即使对于具有特定结构约束（如小的反馈顶点集数和路径宽度）的近平面图，测试 $1$-平面性仍然是NP-完全的，并且局部交叉数难以近似。", "keywords": "k-平面性, 参数化复杂性, 局部交叉数, 树深, 反馈顶点集", "comments": "本文的创新之处在于将 $k$-平面性的结构参数化研究推广到了一般 $k \text{ ≥ } 1$ 的情况，并提供了紧密的参数化上下界。其重要性在于，通过强化现有下界，它揭示了即使在图具有强结构约束（例如常数树宽）的情况下， $k$-平面性问题在计算上仍然具有固有难度。这为理解图绘制问题的计算边界提供了更深入的见解，并可能指导未来算法设计中对参数选择的考量。"}}
{"id": "2506.10142", "title": "Rethinking Brain Tumor Segmentation from the Frequency Domain Perspective", "authors": ["Minye Shao", "Zeyu Wang", "Haoran Duan", "Yawen Huang", "Bing Zhai", "Shizheng Wang", "Yang Long", "Yefeng Zheng"], "summary": "Precise segmentation of brain tumors, particularly contrast-enhancing regions\nvisible in post-contrast MRI (areas highlighted by contrast agent injection),\nis crucial for accurate clinical diagnosis and treatment planning but remains\nchallenging. However, current methods exhibit notable performance degradation\nin segmenting these enhancing brain tumor areas, largely due to insufficient\nconsideration of MRI-specific tumor features such as complex textures and\ndirectional variations. To address this, we propose the Harmonized Frequency\nFusion Network (HFF-Net), which rethinks brain tumor segmentation from a\nfrequency-domain perspective. To comprehensively characterize tumor regions, we\ndevelop a Frequency Domain Decomposition (FDD) module that separates MRI images\ninto low-frequency components, capturing smooth tumor contours and\nhigh-frequency components, highlighting detailed textures and directional\nedges. To further enhance sensitivity to tumor boundaries, we introduce an\nAdaptive Laplacian Convolution (ALC) module that adaptively emphasizes critical\nhigh-frequency details using dynamically updated convolution kernels. To\neffectively fuse tumor features across multiple scales, we design a Frequency\nDomain Cross-Attention (FDCA) integrating semantic, positional, and\nslice-specific information. We further validate and interpret frequency-domain\nimprovements through visualization, theoretical reasoning, and experimental\nanalyses. Extensive experiments on four public datasets demonstrate that\nHFF-Net achieves an average relative improvement of 4.48\\% (ranging from 2.39\\%\nto 7.72\\%) in the mean Dice scores across the three major subregions, and an\naverage relative improvement of 7.33% (ranging from 5.96% to 8.64%) in the\nsegmentation of contrast-enhancing tumor regions, while maintaining favorable\ncomputational efficiency and clinical applicability. Code:\nhttps://github.com/VinyehShaw/HFF.", "comment": "Accepted by IEEE Transactions on Medical Imaging", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10142v1", "AI": {"title_translation": "从频域角度重新思考脑肿瘤分割", "tldr": "HFF-Net通过在频域处理MRI图像，显著提高了脑肿瘤（特别是对比增强区域）的分割精度，解决了现有方法对复杂纹理和方向变化考虑不足的问题。", "motivation": "精确分割脑肿瘤，特别是MRI中可见的对比增强区域，对临床诊断和治疗规划至关重要，但现有方法在分割这些区域时表现不佳，主要原因是未能充分考虑MRI特有的肿瘤特征，如复杂纹理和方向变化。", "method": "本文提出了谐波频率融合网络（HFF-Net），该网络从频域角度重新思考脑肿瘤分割。HFF-Net包含：1. 频率域分解（FDD）模块，将MRI图像分解为低频（捕获平滑轮廓）和高频（突出细节纹理和方向边缘）分量。2. 自适应拉普拉斯卷积（ALC）模块，使用动态更新的卷积核自适应地强调关键高频细节，增强对肿瘤边界的敏感性。3. 频率域交叉注意力（FDCA）模块，融合语义、位置和切片特异性信息，有效融合多尺度肿瘤特征。该方法还通过可视化、理论推理和实验分析验证了频域改进。", "result": "在四个公共数据集上的广泛实验表明，HFF-Net在三个主要子区域的平均Dice分数上实现了4.48%（2.39%至7.72%）的平均相对改进，在对比增强肿瘤区域的分割上实现了7.33%（5.96%至8.64%）的平均相对改进，同时保持了良好的计算效率和临床适用性。", "conclusion": "HFF-Net通过从频域视角处理脑肿瘤分割，并引入FDD、ALC和FDCA模块，显著提高了脑肿瘤（尤其是对比增强区域）的分割精度，同时保持了计算效率和临床适用性。", "translation": "精确分割脑肿瘤，特别是对比增强MRI中可见的区域（通过对比剂注射突出显示的区域），对于准确的临床诊断和治疗规划至关重要，但仍然具有挑战性。然而，当前方法在分割这些增强型脑肿瘤区域时表现出显著的性能下降，这主要是由于对MRI特有的肿瘤特征（如复杂纹理和方向变化）考虑不足。为了解决这个问题，我们提出了谐波频率融合网络（HFF-Net），该网络从频域角度重新思考脑肿瘤分割。为了全面表征肿瘤区域，我们开发了一个频率域分解（FDD）模块，将MRI图像分离成低频分量（捕获平滑的肿瘤轮廓）和高频分量（突出详细的纹理和方向边缘）。为了进一步增强对肿瘤边界的敏感性，我们引入了一个自适应拉普拉斯卷积（ALC）模块，该模块使用动态更新的卷积核自适应地强调关键的高频细节。为了有效融合多尺度肿瘤特征，我们设计了一个频率域交叉注意力（FDCA），它整合了语义、位置和切片特异性信息。我们通过可视化、理论推理和实验分析进一步验证和解释了频域改进。在四个公共数据集上的广泛实验表明，HFF-Net在三个主要子区域的平均Dice分数上实现了4.48%（范围从2.39%到7.72%）的平均相对改进，在对比增强肿瘤区域的分割上实现了7.33%（范围从5.96%到8.64%）的平均相对改进，同时保持了良好的计算效率和临床适用性。代码：https://github.com/VinyehShaw/HFF。", "summary": "本文提出了一种名为谐波频率融合网络（HFF-Net）的新方法，旨在通过从频域角度处理MRI图像来提高脑肿瘤分割的精度。针对现有方法在处理MRI特有肿瘤特征（如复杂纹理和方向变化）方面的不足，HFF-Net引入了频率域分解（FDD）模块来分离图像的低频和高频分量，自适应拉普拉斯卷积（ALC）模块以增强边界敏感性，以及频率域交叉注意力（FDCA）模块来有效融合多尺度特征。实验结果表明，HFF-Net在多个公共数据集上显著提升了脑肿瘤（特别是对比增强区域）的分割性能，并具有良好的计算效率和临床适用性。", "keywords": "脑肿瘤分割, 频域, HFF-Net, MRI, 对比增强区域", "comments": "该论文的创新点在于将脑肿瘤分割问题从传统的空间域转换到频域视角进行处理，通过分解图像的频率成分来更精细地捕获肿瘤特征。HFF-Net中FDD、ALC和FDCA模块的设计，特别是ALC的自适应核和FDCA的多信息融合能力，有效地解决了现有方法在处理复杂MRI肿瘤特征时的局限性。其在对比增强区域分割上的显著提升，对临床诊断和治疗规划具有重要意义。"}}
{"id": "2506.10035", "title": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training", "authors": ["Fuhan Cai", "Yong Guo", "Jie Li", "Wenbo Li", "Xiangzhong Fang", "Jian Chen"], "summary": "Recent advancements in text-to-image (T2I) generation have led to the\nemergence of highly expressive models such as diffusion transformers (DiTs),\nexemplified by FLUX. However, their massive parameter sizes lead to slow\ninference, high memory usage, and poor deployability. Existing acceleration\nmethods (e.g., single-step distillation and attention pruning) often suffer\nfrom significant performance degradation and incur substantial training costs.\nTo address these limitations, we propose FastFLUX, an architecture-level\npruning framework designed to enhance the inference efficiency of FLUX. At its\ncore is the Block-wise Replacement with Linear Layers (BRLL) method, which\nreplaces structurally complex residual branches in ResBlocks with lightweight\nlinear layers while preserving the original shortcut connections for stability.\nFurthermore, we introduce Sandwich Training (ST), a localized fine-tuning\nstrategy that leverages LoRA to supervise neighboring blocks, mitigating\nperformance drops caused by structural replacement. Experiments show that our\nFastFLUX maintains high image quality under both qualitative and quantitative\nevaluations, while significantly improving inference speed, even with 20\\% of\nthe hierarchy pruned. Our code will be available soon.", "comment": "14 pages", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10035v1", "AI": {"title_translation": "FastFLUX：通过块级替换和三明治训练剪枝FLUX", "tldr": "提出FastFLUX，一个通过块级替换和三明治训练来加速FLUX扩散模型推理的剪枝框架，同时保持图像质量。", "motivation": "现有的文本到图像生成模型（如FLUX）参数量大，导致推理慢、内存占用高、部署困难。现有加速方法（如单步蒸馏和注意力剪枝）常导致性能显著下降且训练成本高。", "method": "提出FastFLUX，一个架构级剪枝框架。核心是“块级线性层替换 (BRLL)”方法，用轻量级线性层替换残差块中结构复杂的残差分支，同时保留原始捷径连接以保持稳定性。此外，引入“三明治训练 (ST)”，一种利用LoRA监督相邻块的局部微调策略，以减轻结构替换导致的性能下降。", "result": "实验表明，FastFLUX在定性和定量评估下均保持高图像质量，即使剪枝了20%的层级，也能显著提高推理速度。", "conclusion": "FastFLUX通过其创新的剪枝和训练策略，有效解决了大型扩散模型推理效率低的问题，实现了速度提升而无显著性能损失。", "translation": "近年来，文本到图像（T2I）生成技术的进步催生了像扩散Transformer（DiTs）这样表现力极强的模型，其中FLUX就是典型代表。然而，它们庞大的参数量导致推理缓慢、内存占用高以及部署困难。现有的加速方法（例如单步蒸馏和注意力剪枝）通常会遭受显著的性能下降并产生高昂的训练成本。为了解决这些限制，我们提出了FastFLUX，一个旨在提高FLUX推理效率的架构级剪枝框架。其核心是“块级线性层替换（BRLL）”方法，该方法用轻量级线性层替换残差块中结构复杂的残差分支，同时保留原始的快捷连接以保持稳定性。此外，我们引入了“三明治训练（ST）”，这是一种利用LoRA监督相邻块的局部微调策略，以减轻结构替换引起的性能下降。实验表明，我们的FastFLUX在定性和定量评估下均保持了高图像质量，即使在剪枝了20%的层级后，也能显著提高推理速度。我们的代码即将可用。", "summary": "本文提出了FastFLUX，一个针对大型文本到图像扩散模型FLUX的架构级剪枝框架，旨在解决其推理速度慢和资源消耗大的问题。FastFLUX引入了块级线性层替换（BRLL）方法，用轻量级线性层替代复杂的残差分支，并结合三明治训练（ST）策略，利用LoRA进行局部微调以缓解性能下降。实验结果表明，即使在20%的层级被剪枝的情况下，FastFLUX也能在显著提升推理速度的同时保持高图像质量。", "keywords": "文本到图像生成, 扩散模型, 模型剪枝, 模型加速, FLUX", "comments": "FastFLUX的创新之处在于其结合了架构级剪枝（BRLL）和精细化训练策略（ST），特别是三明治训练利用LoRA进行局部监督，有效解决了结构性剪枝可能带来的性能下降问题。这种方法为大型T2I模型的部署提供了新的思路，具有重要的实际应用价值。"}}
{"id": "2506.10014", "title": "NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing", "authors": ["Wei Li", "Mengcheng Lan", "Jiaxing Xu", "Yiping Ke"], "summary": "Graphs are essential for modeling complex interactions across domains such as\nsocial networks, biology, and recommendation systems. Traditional Graph Neural\nNetworks, particularly Message Passing Neural Networks (MPNNs), rely heavily on\nsupervised learning, limiting their generalization and applicability in\nlabel-scarce scenarios. Recent self-supervised approaches still require labeled\nfine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,\nLarge Language Models (LLMs) excel in natural language tasks but face\nsignificant challenges when applied to graphs, including preserving reasoning\nabilities, managing extensive token lengths from rich node attributes, and\nbeing limited to textual-attributed graphs (TAGs) and a single level task. To\novercome these limitations, we propose the Node-Oriented Conceptualization LLM\n(NOCL), a novel framework that leverages two core techniques: 1) node\ndescription, which converts heterogeneous node attributes into structured\nnatural language, extending LLM from TAGs to non-TAGs; 2) node concept, which\nencodes node descriptions into compact semantic embeddings using pretrained\nlanguage models, significantly reducing token lengths by up to 93.9% compared\nto directly using node descriptions. Additionally, our NOCL employs graph\nrepresentation descriptors to unify graph tasks at various levels into a\nshared, language-based query format, paving a new direction for Graph\nFoundation Models. Experimental results validate NOCL's competitive supervised\nperformance relative to traditional MPNNs and hybrid LLM-MPNN methods and\ndemonstrate superior generalization in zero-shot settings.", "comment": "10 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1703.00552, arXiv:1403.2844 by other authors", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10014v1", "AI": {"title_translation": "NOCL：用于图任务的面向节点概念化大型语言模型，无需消息传递", "tldr": "NOCL提出了一种无需消息传递的面向节点概念化大型语言模型，通过节点描述和节点概念技术，将异构图数据转化为LLM可处理的格式，并在图任务中展现出优越的泛化能力和竞争力。", "motivation": "传统的图神经网络（特别是消息传递神经网络MPNNs）过度依赖监督学习，限制了其在标签稀缺场景的泛化和适用性。近期自监督方法仍需标签微调，限制了零样本场景的有效性。大型语言模型（LLMs）在自然语言任务中表现出色，但在应用于图时面临挑战，包括保持推理能力、管理富节点属性的冗长令牌长度、以及受限于文本属性图（TAGs）和单一级别任务。", "method": "本文提出了NOCL（Node-Oriented Conceptualization LLM），一个新颖的框架，利用两个核心技术：1) 节点描述：将异构节点属性转换为结构化自然语言，将LLM从文本属性图（TAGs）扩展到非TAGs；2) 节点概念：使用预训练语言模型将节点描述编码为紧凑的语义嵌入，与直接使用节点描述相比，令牌长度显著减少高达93.9%。此外，NOCL采用图表示描述符，将各种级别的图任务统一为共享的、基于语言的查询格式。", "result": "实验结果验证了NOCL相对于传统MPNNs和混合LLM-MPNN方法具有竞争性的监督性能，并在零样本设置中表现出卓越的泛化能力。", "conclusion": "NOCL通过其节点描述和节点概念技术，成功地将大型语言模型应用于图任务，解决了传统GNN和LLM在图领域应用的局限性，特别是在零样本泛化方面表现出色，并为图基础模型开辟了新方向。", "translation": "图对于建模社交网络、生物学和推荐系统等领域的复杂交互至关重要。传统的图神经网络，特别是消息传递神经网络（MPNNs），严重依赖监督学习，限制了它们在标签稀缺场景中的泛化和适用性。最近的自监督方法仍然需要带标签的微调，限制了它们在零样本场景中的有效性。同时，大型语言模型（LLMs）在自然语言任务中表现出色，但在应用于图时面临重大挑战，包括保持推理能力、管理来自丰富节点属性的冗长令牌长度，以及受限于文本属性图（TAGs）和单一级别任务。为了克服这些限制，我们提出了面向节点概念化的大型语言模型（NOCL），这是一个新颖的框架，利用两个核心技术：1）节点描述，将异构节点属性转换为结构化自然语言，将LLM从TAGs扩展到非TAGs；2）节点概念，使用预训练语言模型将节点描述编码为紧凑的语义嵌入，与直接使用节点描述相比，令牌长度显著减少高达93.9%。此外，我们的NOCL采用图表示描述符，将各种级别的图任务统一为共享的、基于语言的查询格式，为图基础模型开辟了新方向。实验结果验证了NOCL相对于传统MPNNs和混合LLM-MPNN方法具有竞争性的监督性能，并在零样本设置中表现出卓越的泛化能力。", "summary": "本文提出了一种名为NOCL（Node-Oriented Conceptualization LLM）的新型框架，旨在解决传统图神经网络在标签稀缺场景中的泛化限制以及大型语言模型在处理图数据时的挑战。NOCL通过“节点描述”将异构节点属性转化为自然语言，并将LLM的应用范围从文本属性图扩展到非文本属性图。同时，通过“节点概念”技术将节点描述编码为紧凑的语义嵌入，显著减少了令牌长度。该框架还统一了不同级别的图任务为语言查询格式，为图基础模型奠定了基础。实验证明，NOCL在监督学习性能上与现有方法相当，并在零样本设置中展现出卓越的泛化能力。", "keywords": "图神经网络, 大型语言模型, 节点概念化, 零样本学习, 图基础模型", "comments": "NOCL的创新之处在于其将LLM应用于图任务的独特视角，通过“节点描述”和“节点概念”有效地桥接了异构图数据与LLM处理能力的鸿沟。它避免了传统GNN的消息传递机制，同时解决了LLM在处理图数据时面临的令牌长度和非文本属性图的限制。这项工作为构建“图基础模型”提供了一个新颖且有潜力的方向，特别是在零样本泛化能力方面具有重要意义。"}}
{"id": "2506.10329", "title": "Context-Adaptive Graph Neural Networks for Next POI Recommendation", "authors": ["Yu Lei", "Limin Shen", "Zhu Sun", "Tiantian He", "Yew-Soon Ong"], "summary": "Next Point-of-Interest (POI) recommendation is a critical task in\nlocation-based services, aiming to predict users' next visits based on their\ncheck-in histories. While many existing methods leverage Graph Neural Networks\n(GNNs) to incorporate collaborative information and improve recommendation\naccuracy, most of them model each type of context using separate graphs,\ntreating different factors in isolation. This limits their ability to model the\nco-influence of multiple contextual factors on user transitions during message\npropagation, resulting in suboptimal attention weights and recommendation\nperformance. Furthermore, they often prioritize sequential components as the\nprimary predictor, potentially undermining the semantic and structural\ninformation encoded in the POI embeddings learned by GNNs. To address these\nlimitations, we propose a Context-Adaptive Graph Neural Networks (CAGNN) for\nnext POI recommendation, which dynamically adjusts attention weights using\nedge-specific contextual factors and enables mutual enhancement between\ngraph-based and sequential components. Specifically, CAGNN introduces (1) a\ncontext-adaptive attention mechanism that jointly incorporates different types\nof contextual factors into the attention computation during graph propagation,\nenabling the model to dynamically capture collaborative and context-dependent\ntransition patterns; (2) a graph-sequential mutual enhancement module, which\naligns the outputs of the graph- and sequential-based modules via the KL\ndivergence, enabling mutual enhancement of both components. Experimental\nresults on three real-world datasets demonstrate that CAGNN consistently\noutperforms state-of-the-art methods. Meanwhile, theoretical guarantees are\nprovided that our context-adaptive attention mechanism improves the\nexpressiveness of POI representations.", "comment": "12 pages, 6 figures", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10329v1", "AI": {"title_translation": "上下文自适应图神经网络用于下一兴趣点推荐", "tldr": "本文提出了一种上下文自适应图神经网络（CAGNN），通过动态调整注意力权重和图序列互增强机制，显著提升了下一兴趣点推荐的准确性。", "motivation": "现有方法在下一兴趣点推荐中存在局限性：1) 大多数GNN方法独立建模不同类型的上下文，限制了多上下文因素协同影响的建模能力，导致注意力权重和推荐性能不佳。2) 它们常将序列组件作为主要预测器，可能削弱GNN学习到的POI嵌入中编码的语义和结构信息。", "method": "本文提出上下文自适应图神经网络（CAGNN）以解决上述局限性。CAGNN引入了：1) 上下文自适应注意力机制：在图传播过程中联合纳入不同类型的上下文因素进行注意力计算，动态捕获协作和上下文依赖的转换模式。2) 图序列互增强模块：通过KL散度对齐基于图和基于序列模块的输出，实现两组件的相互增强。", "result": "在三个真实世界数据集上的实验结果表明，CAGNN持续优于最先进的方法。同时，理论上保证了上下文自适应注意力机制提高了POI表示的表达能力。", "conclusion": "CAGNN通过其上下文自适应注意力机制和图序列互增强模块，有效解决了现有下一兴趣点推荐方法中上下文建模不足和图序列信息融合不佳的问题，从而显著提升了推荐性能和POI表示的表达能力。", "translation": "下一兴趣点（POI）推荐是基于位置服务中的一项关键任务，旨在根据用户的签到历史预测他们接下来的访问。尽管许多现有方法利用图神经网络（GNN）来整合协作信息并提高推荐准确性，但它们大多使用单独的图来建模每种类型的上下文，将不同因素孤立处理。这限制了它们在消息传播过程中建模多个上下文因素协同影响的能力，导致次优的注意力权重和推荐性能。此外，它们通常将序列组件作为主要预测器，这可能会削弱GNN学习到的POI嵌入中编码的语义和结构信息。为了解决这些局限性，我们提出了一种用于下一POI推荐的上下文自适应图神经网络（CAGNN），它使用特定于边缘的上下文因素动态调整注意力权重，并实现了基于图和序列组件之间的相互增强。具体来说，CAGNN引入了：（1）一种上下文自适应注意力机制，在图传播过程中将不同类型的上下文因素联合纳入注意力计算，使模型能够动态捕获协作和上下文相关的转换模式；（2）一个图序列互增强模块，通过KL散度对齐基于图和基于序列模块的输出，实现两个组件的相互增强。在三个真实世界数据集上的实验结果表明，CAGNN持续优于最先进的方法。同时，理论上保证了我们的上下文自适应注意力机制提高了POI表示的表达能力。", "summary": "本文提出了一种名为CAGNN的上下文自适应图神经网络，用于下一兴趣点（POI）推荐。该模型通过引入上下文自适应注意力机制，在图传播中联合考虑多种上下文因素以动态捕获转换模式；并设计了图序列互增强模块，通过KL散度对齐图和序列模块的输出，实现两者的相互增强。实验结果表明，CAGNN在真实世界数据集上持续超越现有最先进方法，并提升了POI表示的表达能力。", "keywords": "下一兴趣点推荐, 图神经网络, 上下文自适应, 注意力机制, 序列建模", "comments": "这篇论文的创新点在于提出了上下文自适应注意力机制和图序列互增强模块，有效解决了现有GNN在POI推荐中独立处理上下文和图序列信息融合不足的问题。通过动态调整注意力权重和互增强机制，模型能够更全面地捕获用户行为模式，提升推荐准确性，具有重要的实际应用价值。"}}
{"id": "2506.10055", "title": "TaskCraft: Automated Generation of Agentic Tasks", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10055v1", "AI": {"title_translation": "TaskCraft：代理任务的自动化生成", "tldr": "TaskCraft是一个自动化工作流程，用于生成可扩展、多工具且可验证的代理任务，以解决现有指令数据缺乏工具交互和代理基准测试成本高昂的问题。", "motivation": "代理任务在NLP和AI领域日益重要，但现有指令数据缺乏工具交互，且当前的代理基准测试依赖昂贵的人工标注，限制了其可扩展性。", "method": "我们引入了TaskCraft，这是一个自动化工作流程，通过深度扩展和宽度扩展来创建结构上和层次上复杂的挑战，从而扩展原子任务，以生成难度可扩展、多工具且可验证的代理任务及执行轨迹。", "result": "实证结果表明，这些任务改进了生成工作流中的提示优化，并增强了代理基础模型的监督微调。我们提供了一个包含约36,000个不同难度任务的大规模合成数据集。", "conclusion": "TaskCraft提供了一个自动化、可扩展的代理任务生成方法，解决了现有数据和基准的局限性，并为代理模型的研究和评估提供了大规模数据集。", "translation": "代理任务需要多步骤的问题解决能力，具备自主性、工具使用和自适应推理，它们在自然语言处理和人工智能的进步中变得越来越核心。然而，现有指令数据缺乏工具交互，当前的代理基准测试依赖昂贵的人工标注，这限制了它们的可扩展性。我们引入了\\textsc{TaskCraft}，一个自动化工作流程，用于生成难度可扩展、多工具且可验证的代理任务及其执行轨迹。TaskCraft通过基于深度和基于宽度的扩展来扩展原子任务，以创建结构上和层次上复杂的挑战。实证结果表明，这些任务改进了生成工作流中的提示优化，并增强了代理基础模型的监督微调。我们提出了一个包含约36,000个不同难度任务的大规模合成数据集，以支持未来关于代理调整和评估的研究。", "summary": "TaskCraft是一个自动化系统，旨在生成可扩展、多工具且可验证的代理任务，以解决现有代理任务数据和基准的局限性。它通过深度和宽度扩展创建复杂的任务，并生成了包含约36,000个任务的大规模合成数据集。实验证明，这些任务能够优化提示并改进代理基础模型的微调。", "keywords": "代理任务, 自动化生成, TaskCraft, 数据集, 任务扩展", "comments": "TaskCraft的创新之处在于其自动化生成复杂代理任务的能力，解决了当前人工标注成本高昂和数据稀缺的问题。其深度和宽度扩展机制使得任务难度可控且多样化，对于推动代理AI的发展和评估具有重要意义。提供大规模合成数据集也显著降低了研究门槛。"}}
{"id": "2506.10082", "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning", "authors": ["Chenjian Gao", "Lihe Ding", "Xin Cai", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "summary": "Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our approach preserves background\nregions while enabling controllable edits propagation. This solution offers\nefficient and adaptable video editing without altering the model architecture.\nTo better steer this process, we incorporate additional references, such as\nalternate viewpoints or representative scene states, which serve as visual\nanchors for how content should unfold. We address the control challenge using a\nmask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model\nto the editing context. The model must learn from two distinct sources: the\ninput video provides spatial structure and motion cues, while reference images\noffer appearance guidance. A spatial mask enables region-specific learning by\ndynamically modulating what the model attends to, ensuring that each area draws\nfrom the appropriate source. Experimental results show our method achieves\nsuperior video editing performance compared to state-of-the-art methods.", "comment": "12 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10082v1", "AI": {"title_translation": "LoRA-Edit：通过掩码感知LoRA微调实现可控的首帧引导视频编辑", "tldr": "LoRA-Edit提出了一种基于掩码的LoRA微调方法，用于可控的首帧引导视频编辑，解决了现有方法灵活性不足的问题，并在保持背景的同时实现可控编辑传播。", "motivation": "现有扩散模型视频编辑方法依赖大规模预训练，限制了特定编辑的灵活性；首帧引导编辑对后续帧缺乏灵活性。", "method": "提出LoRA-Edit，一种基于掩码的LoRA微调方法，用于适应预训练的图像到视频（I2V）模型。该方法通过空间掩码实现区域特定学习，动态调制模型关注点，确保每个区域从输入视频（空间结构和运动）和参考图像（外观指导）中获取信息，同时保持背景区域并实现可控编辑传播。", "result": "实验结果表明，该方法在视频编辑性能上优于现有SOTA方法。", "conclusion": "LoRA-Edit通过其掩码感知LoRA微调策略，提供了一种高效、灵活且可控的视频编辑解决方案，解决了现有方法的局限性，并实现了卓越的性能。", "translation": "扩散模型在视频编辑方面取得了显著成果，能够生成高质量的视频编辑。然而，当前方法通常依赖于大规模预训练，限制了特定编辑的灵活性。首帧引导编辑提供了对第一帧的控制，但对后续帧缺乏灵活性。为了解决这个问题，我们提出了一种基于掩码的LoRA（低秩适应）微调方法，该方法调整预训练的图像到视频（I2V）模型以实现灵活的视频编辑。我们的方法在保留背景区域的同时，能够实现可控的编辑传播。该解决方案提供了高效且适应性强的视频编辑，而无需改变模型架构。为了更好地引导这一过程，我们引入了额外的参考，例如替代视角或代表性场景状态，它们作为内容应如何展开的视觉锚点。我们通过掩码驱动的LoRA微调策略解决了控制挑战，该策略将预训练的图像到视频模型适应到编辑上下文中。模型必须从两个不同来源学习：输入视频提供空间结构和运动线索，而参考图像提供外观指导。空间掩码通过动态调制模型关注的内容来实现区域特定学习，确保每个区域都从适当的来源获取信息。实验结果表明，我们的方法比最先进的方法实现了更优异的视频编辑性能。", "summary": "本文提出LoRA-Edit，一种基于掩码的LoRA微调方法，旨在解决现有扩散模型视频编辑在灵活性和后续帧控制上的不足。通过将预训练的图像到视频模型适应于编辑任务，LoRA-Edit利用空间掩码实现区域特定学习，并结合输入视频和参考图像提供的信息，在保持背景的同时实现可控的编辑传播，且无需修改模型架构。实验证明其性能优于现有SOTA方法。", "keywords": "视频编辑, 扩散模型, LoRA, 掩码, 首帧引导", "comments": "该论文的创新点在于结合了LoRA微调与掩码感知机制，为视频编辑提供了更精细的区域控制和更高的灵活性，尤其是在保持背景的同时进行局部编辑的能力。这种方法无需大规模预训练或改变模型架构，使得视频编辑更加高效和可控。"}}
{"id": "2506.10207", "title": "FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification", "authors": ["Jun Bai", "Rajib Rana", "Di Wu", "Youyang Qu", "Xiaohui Tao", "Ji Zhang"], "summary": "Federated Learning (FL) provides a privacy-preserving paradigm for training\naudio classification (AC) models across distributed clients without sharing raw\ndata. However, Federated Audio Classification (FedAC) faces three critical\nchallenges that substantially hinder performance: data heterogeneity, model\nheterogeneity, and data poisoning. While prior works have attempted to address\nthese issues, they are typically treated independently, lacking a unified and\nrobust solution suited to real-world federated audio scenarios. To bridge this\ngap, we propose FedMLAC, a unified mutual learning framework designed to\nsimultaneously tackle these challenges in FedAC. Specifically, FedMLAC\nintroduces a dual-model architecture on each client, comprising a personalized\nlocal AC model and a lightweight, globally shared Plug-in model. Through\nbidirectional knowledge distillation, the Plug-in model enables global\nknowledge transfer while adapting to client-specific data distributions, thus\nsupporting both generalization and personalization. To further enhance\nrobustness against corrupted audio data, we develop a Layer-wise Pruning\nAggregation (LPA) strategy that filters unreliable Plug-in model updates based\non parameter deviations during server-side aggregation. Extensive experiments\non four diverse audio classification benchmarks, spanning both speech and\nnon-speech tasks, demonstrate that FedMLAC consistently outperforms existing\nstate-of-the-art methods in terms of classification accuracy and robustness to\nnoisy data.", "comment": "initial version", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10207v1", "AI": {"title_translation": "FedMLAC：互学习驱动的异构联邦音频分类", "tldr": "FedMLAC是一个统一的互学习框架，通过双模型架构和层级剪枝聚合策略，解决了联邦音频分类中的数据异构性、模型异构性和数据投毒问题，并显著提高了性能和鲁棒性。", "motivation": "联邦音频分类（FedAC）面临数据异构性、模型异构性和数据投毒三大挑战，现有方法通常独立解决这些问题，缺乏适用于真实世界联邦音频场景的统一且鲁棒的解决方案。", "method": "本文提出了FedMLAC，一个统一的互学习框架，旨在同时解决联邦音频分类中的数据异构性、模型异构性和数据投毒问题。具体而言，FedMLAC在每个客户端引入了双模型架构，包括一个个性化本地AC模型和一个轻量级、全局共享的Plug-in模型。通过双向知识蒸馏，Plug-in模型能够实现全局知识迁移并适应客户端特定数据分布，从而支持泛化和个性化。此外，为增强对损坏音频数据的鲁棒性，开发了一种层级剪枝聚合（LPA）策略，该策略在服务器端聚合期间根据参数偏差过滤不可靠的Plug-in模型更新。", "result": "在涵盖语音和非语音任务的四个不同音频分类基准测试上进行的广泛实验表明，FedMLAC在分类准确性和对噪声数据的鲁棒性方面始终优于现有最先进的方法。", "conclusion": "FedMLAC通过其统一的互学习框架和鲁棒的聚合策略，成功解决了联邦音频分类中的多重挑战，显著提升了性能和鲁棒性。", "translation": "联邦学习（FL）提供了一种隐私保护范式，可以在不共享原始数据的情况下，在分布式客户端上训练音频分类（AC）模型。然而，联邦音频分类（FedAC）面临三个严重阻碍性能的关键挑战：数据异构性、模型异构性和数据投毒。尽管先前的研究试图解决这些问题，但它们通常是独立处理的，缺乏适用于真实世界联邦音频场景的统一且鲁棒的解决方案。为了弥补这一空白，我们提出了FedMLAC，一个统一的互学习框架，旨在同时解决FedAC中的这些挑战。具体来说，FedMLAC在每个客户端引入了一个双模型架构，包括一个个性化本地AC模型和一个轻量级、全局共享的Plug-in模型。通过双向知识蒸馏，Plug-in模型能够实现全局知识迁移，同时适应客户端特定数据分布，从而支持泛化和个性化。为了进一步增强对损坏音频数据的鲁棒性，我们开发了一种层级剪枝聚合（LPA）策略，该策略在服务器端聚合期间根据参数偏差过滤不可靠的Plug-in模型更新。在涵盖语音和非语音任务的四个不同音频分类基准测试上进行的广泛实验表明，FedMLAC在分类准确性和对噪声数据的鲁棒性方面始终优于现有最先进的方法。", "summary": "本文提出了FedMLAC，一个统一的互学习框架，旨在解决联邦音频分类（FedAC）中数据异构性、模型异构性和数据投毒等核心挑战。FedMLAC在客户端采用双模型架构（个性化本地模型和全局Plug-in模型），并通过双向知识蒸馏实现知识共享与个性化。此外，引入层级剪枝聚合策略增强对抗恶意数据的鲁棒性。实验证明，FedMLAC在多个音频分类任务上显著优于现有SOTA方法，提升了准确性和抗噪能力。", "keywords": "联邦学习, 音频分类, 互学习, 异构性, 数据投毒", "comments": "FedMLAC的创新点在于提出了一个统一的框架来同时解决联邦学习中的多个异构性和鲁棒性挑战，而不是分别处理。双模型架构结合知识蒸馏，以及针对数据投毒的LPA策略，为实际联邦音频应用提供了更全面的解决方案，展示了其在复杂联邦环境下的实用性和有效性。"}}
{"id": "2506.10029", "title": "Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks", "authors": ["Rafaël Nouailles"], "summary": "Large Language models (LLMs) are transforming digital usage, particularly in\ntext generation, image creation, information retrieval and code development.\nChatGPT, launched by OpenAI in November 2022, quickly became a reference,\nprompting the emergence of competitors such as Google's Gemini. However, these\ntechnological advances raise new cybersecurity challenges, including prompt\ninjection attacks, the circumvention of regulatory measures (jailbreaking), the\nspread of misinformation (hallucinations) and risks associated with deep fakes.\nThis paper presents a comparative analysis of the security and alignment levels\nof ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated\nwith experiments.", "comment": "in French language", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10029v1", "AI": {"title_translation": "ChatGPT和Gemini安全与对齐的实证评估：通过越狱实验进行的漏洞比较分析", "tldr": "本文通过越狱实验对ChatGPT和Gemini的安全性和对齐水平进行了比较分析，并提出了越狱技术的分类。", "motivation": "大型语言模型（LLMs）正在改变数字使用，但随之带来了新的网络安全挑战，包括提示注入攻击、规避监管措施（越狱）、错误信息传播和深度伪造风险。因此，有必要对ChatGPT和Gemini等主流LLMs的安全性及对齐水平进行评估。", "method": "本文对ChatGPT和Gemini的安全性和对齐水平进行了比较分析，并通过越狱实验来评估漏洞，同时提出了越狱技术的分类。", "result": "本文提出了与实验相关的越狱技术分类，并对ChatGPT和Gemini的安全和对齐水平进行了比较分析。具体的比较分析结果未在摘要中详细说明。", "conclusion": "Not mentioned in abstract", "translation": "大型语言模型（LLM）正在改变数字使用，特别是在文本生成、图像创建、信息检索和代码开发方面。OpenAI于2022年11月推出的ChatGPT迅速成为一个标杆，促使了谷歌Gemini等竞争对手的出现。然而，这些技术进步带来了新的网络安全挑战，包括提示注入攻击、规避监管措施（越狱）、错误信息传播（幻觉）以及与深度伪造相关的风险。本文对ChatGPT和Gemini的安全性和对齐水平进行了比较分析，并提出了与实验相关的越狱技术分类。", "summary": "大型语言模型（LLMs）如ChatGPT和Gemini正在重塑数字应用，但同时也引发了严重的网络安全问题，例如提示注入攻击和越狱。本文旨在通过一系列越狱实验，对ChatGPT和Gemini这两种主流LLMs的安全性与对齐水平进行实证比较分析，并在此基础上提出越狱技术的分类体系。", "keywords": "ChatGPT, Gemini, LLMs, 安全性, 越狱, 漏洞分析", "comments": "本研究通过对ChatGPT和Gemini进行越狱实验，对LLMs的安全性和对齐问题进行了实证分析，尤其关注了越狱漏洞。其创新点在于提出了越狱技术的分类，这对于理解和缓解LLMs的安全风险具有重要意义，为未来的模型加固和安全研究提供了基础。"}}
{"id": "2506.10117", "title": "A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild", "authors": ["Klim Kireev", "Ana-Maria Creţu", "Raphael Meier", "Sarah Adel Bargal", "Elissa Redmiles", "Carmela Troncoso"], "summary": "Platforms and the law regulate digital content depicting minors (defined as\nindividuals under 18 years of age) differently from other types of content.\nGiven the sheer amount of content that needs to be assessed, machine\nlearning-based automation tools are commonly used to detect content depicting\nminors. To our knowledge, no dataset or benchmark currently exists for\ndetecting these identification methods in a multi-modal environment. To fill\nthis gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an\nimage-caption dataset aimed at benchmarking tools that detect depictions of\nminors. Our dataset is richer than previous child image datasets, containing\nimages of children in a variety of contexts, including fictional depictions and\npartially visible bodies. ICCWD contains 10,000 image-caption pairs manually\nlabeled to indicate the presence or absence of a child in the image. To\ndemonstrate the possible utility of our dataset, we use it to benchmark three\ndifferent detectors, including a commercial age estimation system applied to\nimages. Our results suggest that child detection is a challenging task, with\nthe best method achieving a 75.3% true positive rate. We hope the release of\nour dataset will aid in the design of better minor detection methods in a wide\nrange of scenarios.", "comment": "14 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10117v1", "AI": {"title_translation": "一个用于检测野外儿童的手动标注图像-字幕数据集", "tldr": "发布了一个新的手动标注图像-字幕数据集（ICCWD），用于检测未成年人，结果显示儿童检测是一项具有挑战性的任务，现有工具仍有局限性。", "motivation": "平台和法律对描绘未成年人的数字内容有特殊规定，现有机器学习自动化工具需要更好的基准测试，但目前缺乏用于多模态环境下检测未成年人的数据集。", "method": "发布了图像-字幕野外儿童数据集（ICCWD），包含10,000对手动标注的图像-字幕对，涵盖虚构描绘和部分可见身体等多种情境。使用ICCWD对包括商业年龄估计系统在内的三种不同检测器进行了基准测试。", "result": "儿童检测是一项具有挑战性的任务，最好的方法达到了75.3%的真阳性率。", "conclusion": "ICCWD数据集的发布有望帮助在各种场景下设计出更好的未成年人检测方法。", "translation": "平台和法律对描绘未成年人（定义为18岁以下个体）的数字内容有不同于其他类型内容的规定。鉴于需要评估的内容量巨大，机器学习自动化工具常用于检测描绘未成年人的内容。据我们所知，目前在多模态环境下还没有用于检测这些识别方法的现有数据集或基准。为了填补这一空白，我们发布了图像-字幕野外儿童数据集（ICCWD），这是一个旨在为检测未成年人描绘的工具提供基准的图像-字幕数据集。我们的数据集比以前的儿童图像数据集更丰富，包含各种情境下的儿童图像，包括虚构描绘和部分可见的身体。ICCWD包含10,000对手动标注的图像-字幕对，用于指示图像中是否存在儿童。为了展示我们数据集的潜在效用，我们使用它对三种不同的检测器进行了基准测试，包括应用于图像的商业年龄估计系统。我们的结果表明，儿童检测是一项具有挑战性的任务，最好的方法达到了75.3%的真阳性率。我们希望我们数据集的发布将有助于在各种场景下设计出更好的未成年人检测方法。", "summary": "本文介绍了野外儿童图像-字幕数据集（ICCWD），这是一个新的手动标注图像-字幕数据集，包含10,000对图像-字幕，旨在为检测数字内容中的未成年人工具提供基准。鉴于该受监管领域缺乏多模态数据集，ICCWD包含了虚构和部分可见描绘等多种情境。使用现有检测器进行的基准测试表明，儿童检测仍然是一项具有挑战性的任务，最佳性能真阳性率为75.3%。该数据集旨在促进改进未成年人检测方法的发展。", "keywords": "儿童检测, 图像-字幕数据集, 未成年人内容, 数据集, 内容审核", "comments": "本文通过提供一个急需的多模态数据集，解决了内容审核中一个关键且受法律约束的领域。手动标注和多样化的内容（虚构、部分可见）是其显著优势，使其比以前的数据集更具鲁棒性。基准测试结果突出了现有检测工具的局限性，强调了该数据集对未来在该挑战性领域进行研究和开发的重要性。"}}
{"id": "2506.10051", "title": "The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks", "authors": ["Md Istiak Hossain Shihab", "Christopher Hundhausen", "Ahsun Tariq", "Summit Haque", "Yunhan Qiao", "Brian Mulanda"], "summary": "When graduates of computing degree programs enter the software industry, they\nwill most likely join teams working on legacy code bases developed by people\nother than themselves. In these so-called brownfield software development\nsettings, generative artificial intelligence (GenAI) coding assistants like\nGitHub Copilot are rapidly transforming software development practices, yet the\nimpact of GenAI on student programmers performing brownfield development tasks\nremains underexplored. This paper investigates how GitHub Copilot influences\nundergraduate students' programming performance, behaviors, and understanding\nwhen completing brownfield programming tasks in which they add new code to an\nunfamiliar code base. We conducted a controlled experiment in which 10\nundergraduate computer science students completed highly similar brownfield\ndevelopment tasks with and without Copilot in a legacy web application. Using a\nmixed-methods approach combining performance analysis, behavioral analysis, and\nexit interviews, we found that students completed tasks 35% faster (p < 0.05)\nand made 50% more solution progress p (< 0.05) when using Copilot. Moreover,\nour analysis revealed that, when using Copilot, students spent 11% less time\nmanually writing code (p < 0.05), and 12% less time conducting web searches (p\n< 0.05), providing evidence of a fundamental shift in how they engaged in\nprogramming. In exit interviews, students reported concerns about not\nunderstanding how or why Copilot suggestions work. This research suggests the\nneed for computing educators to develop new pedagogical approaches that\nleverage GenAI assistants' benefits while fostering reflection on how and why\nGenAI suggestions address brownfield programming tasks. Complete study results\nand analysis are presented at https://ghcopilot-icer.github.io/.", "comment": "14 pages, 5 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10051v1", "AI": {"title_translation": "GitHub Copilot 对计算机专业学生在棕地编程任务中编程有效性、效率和过程的影响", "tldr": "研究发现 GitHub Copilot 能显著提高学生在棕地编程任务中的效率和进度，但学生对其建议的理解存在顾虑。", "motivation": "毕业生进入软件行业常处理遗留代码，而生成式AI编码助手（如GitHub Copilot）正在改变开发实践，但其对学生程序员在棕地开发任务中的影响尚不明确。", "method": "进行了一项对照实验，10名本科计算机科学学生在遗留Web应用程序中，分别使用和不使用Copilot完成高度相似的棕地开发任务。采用混合方法，结合性能分析、行为分析和离职访谈。", "result": "使用Copilot时，学生完成任务速度快35%，解决方案进展多50%。学生手动编写代码时间减少11%，网络搜索时间减少12%。学生在访谈中表示担忧不理解Copilot建议的工作原理或原因。", "conclusion": "研究表明计算机教育工作者需要开发新的教学方法，以利用生成式AI助手的优势，同时培养学生对生成式AI建议如何以及为何解决棕地编程任务的反思。", "translation": "当计算机学位课程的毕业生进入软件行业时，他们很可能会加入处理由他人开发的遗留代码库的团队。在这些所谓的“棕地”软件开发环境中，像GitHub Copilot这样的生成式人工智能（GenAI）编码助手正在迅速改变软件开发实践，但GenAI对执行棕地开发任务的学生程序员的影响仍未得到充分探索。本文调查了GitHub Copilot如何影响本科学生在向不熟悉的现有代码库添加新代码的棕地编程任务中表现、行为和理解。我们进行了一项对照实验，其中10名本科计算机科学学生在遗留Web应用程序中，分别使用和不使用Copilot完成了高度相似的棕地开发任务。采用结合性能分析、行为分析和离职访谈的混合方法，我们发现学生在使用Copilot时完成任务速度快35%（p < 0.05），解决方案进展多50%（p < 0.05）。此外，我们的分析显示，在使用Copilot时，学生手动编写代码的时间减少了11%（p < 0.05），进行网络搜索的时间减少了12%（p < 0.05），这表明他们的编程方式发生了根本性转变。在离职访谈中，学生表达了对不理解Copilot建议如何或为何起作用的担忧。这项研究表明，计算机教育工作者需要开发新的教学方法，以利用GenAI助手的优势，同时培养对GenAI建议如何以及为何解决棕地编程任务的反思。完整的学习结果和分析可在https://ghcopilot-icer.github.io/上查阅。", "summary": "本文研究了GitHub Copilot对计算机专业学生在棕地编程任务中表现、行为和理解的影响。通过一项有10名本科生参与的对照实验，结果显示使用Copilot显著提高了任务完成速度和解决方案进展，并改变了学生的编程方式，减少了手动编码和网络搜索时间。然而，学生对Copilot建议的理解存在顾虑。研究强调教育者需要开发新的教学策略，以有效利用GenAI工具并促进学生对其工作原理的深入理解。", "keywords": "GitHub Copilot, 棕地编程, 学生编程, 生成式AI, 编程教育", "comments": "这项研究创新性地关注了生成式AI编码助手对学生在“棕地”环境下的影响，填补了该领域的研究空白。其混合方法提供了量化效率提升和质化行为转变的全面视角。研究结果对计算机教育具有重要指导意义，提示教育者在推广AI工具的同时，需注重培养学生对AI建议的批判性思维和深层理解，以避免过度依赖和知识空缺。"}}
{"id": "2506.10106", "title": "One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture", "authors": ["Marcos Abel Zuzuárregui", "Mustafa Melih Toslak", "Stefano Carpin"], "summary": "Artificial intelligence is transforming precision agriculture, offering\nfarmers new tools to streamline their daily operations. While these\ntechnological advances promise increased efficiency, they often introduce\nadditional complexity and steep learning curves that are particularly\nchallenging for non-technical users who must balance tech adoption with\nexisting workloads. In this paper, we present a natural language (NL) robotic\nmission planner that enables non-specialists to control heterogeneous robots\nthrough a common interface. By leveraging large language models (LLMs) and\npredefined primitives, our architecture seamlessly translates human language\ninto intermediate descriptions that can be executed by different robotic\nplatforms. With this system, users can formulate complex agricultural missions\nwithout writing any code. In the work presented in this paper, we extend our\nprevious system tailored for wheeled robot mission planning through a new class\nof experiments involving robotic manipulation and computer vision tasks. Our\nresults demonstrate that the architecture is both general enough to support a\ndiverse set of robots and powerful enough to execute complex mission requests.\nThis work represents a significant step toward making robotic automation in\nprecision agriculture more accessible to non-technical users.", "comment": "Accepted to International Federation of Automatic Control (IFAC)\n  Sensing, Control and Automation Technologies for Agriculture - 8th\n  AGRICONTROL 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10106v1", "AI": {"title_translation": "一劳永逸：基于大型语言模型的精准农业异构任务规划", "tldr": "本文提出了一种基于大型语言模型（LLM）的自然语言机器人任务规划器，使非专业用户能够通过通用接口控制异构机器人，从而简化精准农业中的机器人自动化，并通过机器人操作和计算机视觉任务的实验证明了其通用性和强大性。", "motivation": "精准农业中的人工智能技术虽然提高了效率，但其引入的复杂性和陡峭的学习曲线对非技术用户构成了挑战，他们需要在技术采纳与现有工作量之间取得平衡。现有系统可能针对特定机器人类型（如轮式机器人）定制，缺乏对异构机器人和更复杂任务的通用支持。", "method": "本文提出了一种基于大型语言模型（LLM）和预定义原语的自然语言（NL）机器人任务规划器。该架构能将人类语言无缝转换为可由不同机器人平台执行的中间描述。该系统允许用户无需编写代码即可制定复杂的农业任务。作者扩展了之前针对轮式机器人任务规划的系统，通过引入机器人操作和计算机视觉任务的新类实验来验证其通用性。", "result": "该架构被证明足够通用，可以支持多样化的机器人集合，并且足够强大，可以执行复杂的任务请求。实验扩展到包括机器人操作和计算机视觉任务，验证了系统的能力。", "conclusion": "这项工作代表着使精准农业中的机器人自动化对非技术用户更易于访问的重要一步，通过一个通用的、基于LLM的自然语言接口实现了异构机器人的任务规划。", "translation": "人工智能正在改变精准农业，为农民提供了简化日常操作的新工具。虽然这些技术进步有望提高效率，但它们常常引入额外的复杂性和陡峭的学习曲线，这对非技术用户来说尤其具有挑战性，他们必须在技术采纳与现有工作量之间取得平衡。在本文中，我们提出了一种自然语言（NL）机器人任务规划器，使非专业人员能够通过通用接口控制异构机器人。通过利用大型语言模型（LLM）和预定义原语，我们的架构无缝地将人类语言翻译成可由不同机器人平台执行的中间描述。有了这个系统，用户无需编写任何代码即可制定复杂的农业任务。在本文中，我们通过涉及机器人操作和计算机视觉任务的新类实验，扩展了我们之前为轮式机器人任务规划量身定制的系统。我们的结果表明，该架构既足够通用以支持多样化的机器人集合，又足够强大以执行复杂的任务请求。这项工作代表着使精准农业中的机器人自动化对非技术用户更易于访问的重要一步。", "summary": "本文提出了一种基于大型语言模型（LLM）的自然语言机器人任务规划器，旨在解决精准农业中机器人技术对非技术用户造成的复杂性问题。该系统通过将人类语言转换为可执行的机器人指令，使得非专业人员能够通过单一接口控制异构机器人，无需编写代码即可规划复杂的农业任务。通过对机器人操作和计算机视觉任务的实验，研究证明了该架构的通用性和执行复杂任务的能力，从而显著提高了精准农业中机器人自动化的可访问性。", "keywords": "精准农业, 异构机器人, 任务规划, 大型语言模型, 自然语言接口", "comments": "本文的创新之处在于利用大型语言模型（LLM）作为核心，实现了自然语言到异构机器人任务规划的无缝转换，极大地降低了非技术用户使用机器人技术的门槛。其重要性体现在推动精准农业的智能化和自动化普及，使得农民能够更便捷地利用先进技术。该方法通过统一接口和LLM的强大理解能力，有效解决了异构机器人控制的复杂性问题，对未来农业机器人的人机交互和系统集成具有重要启示。"}}
{"id": "2506.10197", "title": "Intergenerational AI Literacy in Korean Immigrant Families: Interpretive Gatekeeping Meets Convenient Critical Deferment", "authors": ["Jeongone Seo", "Ryan Womack", "Tawfiq Ammari"], "summary": "As artificial intelligence (AI) becomes deeply integrated into family life,\nimmigrant families must navigate unique intergenerational, linguistic, and\ncultural challenges. This study examines how Korean immigrant families in the\nUnited States negotiate the use of AI tools such as ChatGPT and smart\nassistants in their homes. Through 20 semi-structured interviews with parents\nand teens, we identify two key practices that shape their engagement:\ninterpretive gatekeeping, where parents mediate their children's AI use through\na lens of cultural and ethical values, and convenient critical deferment, where\nteens strategically postpone critical evaluation of AI for immediate academic\nand social utility. These intertwined practices challenge conventional,\nskills-based models of AI literacy, revealing it instead as a dynamic and\nrelational practice co-constructed through ongoing family negotiation. We\ncontribute to information science and HCI by offering a new conceptual\nextension for intergenerational AI literacy and providing design implications\nfor more equitable, culturally attuned, and family-centered AI systems.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10197v1", "AI": {"title_translation": "韩裔移民家庭中的代际AI素养：解释性把关与便捷式批判推迟", "tldr": "本研究探讨了韩裔移民家庭如何应对AI工具的使用，发现父母通过“解释性把关”中介子女的AI使用，而青少年则通过“便捷式批判推迟”优先考虑即时效用，挑战了传统的AI素养模型，揭示其为一种动态的家庭协商实践。", "motivation": "随着人工智能（AI）深度融入家庭生活，移民家庭在代际、语言和文化上面临独特挑战，因此需要研究韩裔移民家庭如何协商AI工具的使用。", "method": "通过对20名韩裔移民家庭的家长和青少年进行半结构化访谈。", "result": "识别出两种关键实践：父母的“解释性把关”（通过文化和道德价值观中介子女的AI使用）和青少年的“便捷式批判推迟”（为即时学术和社交效用而策略性推迟批判性评估）。这些实践表明AI素养是一种通过持续家庭协商共同构建的动态关系实践，而非传统的基于技能的模型。", "conclusion": "AI素养是家庭协商共同构建的动态关系实践，而非传统的基于技能的模型。研究为代际AI素养提供了新的概念延伸，并为更公平、文化适应和以家庭为中心的AI系统提供了设计启示。", "translation": "随着人工智能（AI）深度融入家庭生活，移民家庭必须应对独特的代际、语言和文化挑战。本研究考察了在美国的韩裔移民家庭如何协商在家庭中使用ChatGPT和智能助手等AI工具。通过对20名家长和青少年进行半结构化访谈，我们识别出两种塑造他们参与的关键实践：解释性把关，即父母通过文化和道德价值观的视角中介子女的AI使用；以及便捷式批判推迟，即青少年为了即时的学术和社交效用而策略性地推迟对AI的批判性评估。这些相互交织的实践挑战了传统的、基于技能的AI素养模型，揭示出AI素养是一种通过持续的家庭协商共同构建的动态关系实践。我们通过为代际AI素养提供新的概念延伸，并为更公平、文化适应和以家庭为中心的AI系统提供设计启示，从而对信息科学和人机交互领域做出贡献。", "summary": "本研究深入分析了AI在韩裔移民家庭中的应用和协商过程。通过访谈，论文揭示了父母的“解释性把关”和青少年的“便捷式批判推迟”这两种关键实践如何共同塑造了家庭内部的AI素养。研究挑战了传统上将AI素养视为技能习得的观念，提出它是一种动态且通过家庭持续协商共同构建的关系性实践，并为设计更具文化敏感性和以家庭为中心的AI系统提供了重要的设计启示。", "keywords": "AI素养, 移民家庭, 代际, 解释性把关, 批判推迟", "comments": "该研究创新性地将AI素养置于家庭互动和文化背景中进行考察，超越了传统的技能导向模型。它揭示了移民家庭在AI融入过程中面临的独特挑战和适应策略，特别是“解释性把关”和“便捷式批判推迟”这两个概念的提出，为理解代际间AI使用和素养的形成提供了新的视角。这对于未来设计更具包容性和文化适应性的AI系统具有重要指导意义。"}}
{"id": "2506.10120", "title": "GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments", "authors": ["Maryam Khalid", "Akane Sano"], "summary": "Graph-based Active Learning (AL) leverages the structure of graphs to\nefficiently prioritize label queries, reducing labeling costs and user burden\nin applications like health monitoring, human behavior analysis, and sensor\nnetworks. By identifying strategically positioned nodes, graph AL minimizes\ndata collection demands while maintaining model performance, making it a\nvaluable tool for dynamic environments. Despite its potential, existing graph\nAL methods are often evaluated on static graph datasets and primarily focus on\nprediction accuracy, neglecting user-centric considerations such as sampling\ndiversity, query fairness, and adaptability to dynamic settings. To bridge this\ngap, we introduce GRAIL, a novel benchmarking framework designed to evaluate\ngraph AL strategies in dynamic, real-world environments. GRAIL introduces novel\nmetrics to assess sustained effectiveness, diversity, and user burden, enabling\na comprehensive evaluation of AL methods under varying conditions. Extensive\nexperiments on datasets featuring dynamic, real-life human sensor data reveal\ntrade-offs between prediction performance and user burden, highlighting\nlimitations in existing AL strategies. GRAIL demonstrates the importance of\nbalancing node importance, query diversity, and network topology, providing an\nevaluation mechanism for graph AL solutions in dynamic environments.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10120v1", "AI": {"title_translation": "GRAIL：动态传感环境中图主动学习的基准", "tldr": "GRAIL是一个新的基准框架，用于在动态真实环境中评估图主动学习策略，并引入新指标来衡量持续有效性、多样性和用户负担。", "motivation": "现有图主动学习方法多在静态数据集上评估，且主要关注预测精度，忽视了用户中心考量（如采样多样性、查询公平性、动态适应性）。", "method": "引入GRAIL，一个新颖的基准框架，旨在评估动态、真实环境中的图主动学习策略。GRAIL引入了新的指标来评估持续有效性、多样性和用户负担。", "result": "对动态、真实人体传感器数据集进行的广泛实验揭示了预测性能和用户负担之间的权衡，突出了现有主动学习策略的局限性。", "conclusion": "GRAIL证明了平衡节点重要性、查询多样性和网络拓扑的重要性，为动态环境中的图主动学习解决方案提供了一个评估机制。", "translation": "基于图的主动学习（AL）利用图的结构有效优先化标签查询，从而在健康监测、人类行为分析和传感器网络等应用中降低标注成本和用户负担。通过识别战略性定位的节点，图AL在保持模型性能的同时最大限度地减少了数据收集需求，使其成为动态环境中的宝贵工具。尽管其潜力巨大，但现有的图AL方法通常在静态图数据集上进行评估，并且主要关注预测精度，忽视了以用户为中心的考虑因素，如采样多样性、查询公平性和对动态设置的适应性。为了弥补这一空白，我们引入了GRAIL，一个新颖的基准测试框架，旨在评估动态、真实世界环境中的图AL策略。GRAIL引入了新颖的指标来评估持续有效性、多样性和用户负担，从而能够在不同条件下对AL方法进行全面评估。对包含动态、真实人体传感器数据的数据集进行的广泛实验揭示了预测性能和用户负担之间的权衡，突出了现有AL策略的局限性。GRAIL展示了平衡节点重要性、查询多样性和网络拓扑的重要性，为动态环境中的图AL解决方案提供了一个评估机制。", "summary": "本文介绍了GRAIL，一个用于评估动态传感环境中图主动学习（AL）策略的新型基准框架。现有图AL方法在静态数据集上评估且仅关注预测精度，忽略了采样多样性、查询公平性和动态适应性等用户中心考量。GRAIL通过引入新的指标来评估持续有效性、多样性和用户负担，弥补了这一空白。实验结果揭示了预测性能和用户负担之间的权衡，强调了平衡节点重要性、查询多样性和网络拓扑对动态环境中图AL解决方案的重要性。", "keywords": "图主动学习, 动态环境, 基准测试, 用户负担, 传感器网络", "comments": "GRAIL的创新之处在于它首次提出了一个针对动态真实环境的图主动学习评估框架，并引入了用户中心的新指标，这对于推动图主动学习在实际应用中的发展具有重要意义。它揭示了现有方法的局限性，并强调了实际应用中性能与用户负担之间的权衡。"}}
{"id": "2506.10851", "title": "Energy-Efficient Deep Learning for Traffic Classification on Microcontrollers", "authors": ["Adel Chehade", "Edoardo Ragusa", "Paolo Gastaldo", "Rodolfo Zunino"], "summary": "In this paper, we present a practical deep learning (DL) approach for\nenergy-efficient traffic classification (TC) on resource-limited\nmicrocontrollers, which are widely used in IoT-based smart systems and\ncommunication networks. Our objective is to balance accuracy, computational\nefficiency, and real-world deployability. To that end, we develop a lightweight\n1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which\nachieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K\nparameters, a 20.12K maximum tensor size, and 10.08M floating-point operations\n(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies\nranging from 94% to 99%. To enable deployment, the model is quantized to INT8,\nsuffering only a marginal 1-2% accuracy drop relative to its Float32\ncounterpart. We evaluate real-world inference performance on two\nmicrocontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive\nNucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and\n115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,\nrespectively. These results demonstrate the feasibility of on-device encrypted\ntraffic analysis, paving the way for scalable, low-power IoT security\nsolutions.", "comment": "Accepted at IEEE ISCC 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10851v1", "AI": {"title_translation": "微控制器上能量高效的深度学习流量分类", "tldr": "该研究提出了一种能量高效的深度学习方法，用于在资源受限的微控制器上进行流量分类，实现了高精度和低功耗。", "motivation": "在物联网智能系统和通信网络中广泛使用的资源受限微控制器上，需要实现能量高效的流量分类，并平衡精度、计算效率和实际部署能力。", "method": "开发了一种轻量级1D-CNN模型，通过硬件感知神经架构搜索（HW-NAS）进行优化。为实现部署，模型被量化为INT8。研究在ISCX VPN-NonVPN数据集上进行训练和评估，并在STM32F746G-DISCO和Nucleo-F401RE两种微控制器上评估了实际推理性能。", "result": "在ISCX VPN-NonVPN数据集上实现了96.59%的精度，模型参数量为88.26K，最大张量大小为20.12K，浮点运算量为10.08M FLOPs。模型在各种流量分类任务中泛化能力强，精度范围为94%至99%。INT8量化后精度仅下降1-2%。在STM32F746G-DISCO上推理延迟为31.43ms，能耗为7.86 mJ；在Nucleo-F401RE上推理延迟为115.40ms，能耗为29.10 mJ。", "conclusion": "这些结果证明了在设备上进行加密流量分析的可行性，为可扩展、低功耗的物联网安全解决方案铺平了道路。", "translation": "在本文中，我们提出了一种实用的深度学习（DL）方法，用于在资源受限的微控制器上进行能量高效的流量分类（TC），这些微控制器广泛应用于基于物联网的智能系统和通信网络。我们的目标是平衡精度、计算效率和实际部署能力。为此，我们开发了一种轻量级1D-CNN，通过硬件感知神经架构搜索（HW-NAS）进行优化，在ISCX VPN-NonVPN数据集上实现了96.59%的精度，仅有88.26K个参数、20.12K的最大张量大小和10.08M的浮点运算（FLOPs）。此外，它在各种TC任务中表现出泛化能力，精度范围为94%至99%。为了实现部署，模型被量化为INT8，相对于其Float32对应版本，精度仅略微下降1-2%。我们在两种微控制器上评估了实际推理性能：高性能STM32F746G-DISCO和成本敏感型Nucleo-F401RE。部署的模型分别实现了31.43毫秒和115.40毫秒的推理延迟，每次推理的能耗分别为7.86毫焦和29.10毫焦。这些结果证明了设备上加密流量分析的可行性，为可扩展、低功耗的物联网安全解决方案铺平了道路。", "summary": "本文提出了一种在资源受限微控制器上实现能量高效流量分类的深度学习方法。研究人员开发了一个轻量级1D-CNN模型，通过硬件感知神经架构搜索进行优化，并在ISCX VPN-NonVPN数据集上实现了96.59%的精度。该模型参数量小，泛化能力强，且经过INT8量化后精度损失微小。在实际微控制器上的测试表明，该方法实现了低延迟和低能耗的推理，为物联网安全提供了可行的设备端加密流量分析方案。", "keywords": "流量分类, 深度学习, 微控制器, 能量高效, 物联网安全", "comments": "这篇论文的创新之处在于结合了硬件感知神经架构搜索和模型量化，以实现在资源受限的微控制器上进行高效的深度学习推理。其重要性在于为物联网设备提供了实用的、低功耗的流量分类解决方案，特别是在设备端进行加密流量分析，这对于提升物联网安全具有重要意义。"}}
{"id": "2506.10921", "title": "Towards Zero-Stall Matrix Multiplication on Energy-Efficient RISC-V Clusters for Machine Learning Acceleration", "authors": ["Luca Colagrande", "Lorenzo Leone", "Maximilian Coco", "Andrei Deaconeasa", "Luca Benini"], "summary": "The growing computational demands of machine learning (ML) workloads have\ndriven the design of ML accelerators aiming at an optimal tradeoff between\nefficiency and flexibility. A widely explored architecture for flexible ML\naccelerators is based on clusters of lightweight instruction processors sharing\nmulti-banked L1 memory, augmented with specialized instruction extensions for\nkey ML-related computations, such as matrix multiplication (matmul). However,\ninstruction extensions should be coupled with microarchitectural optimizations\nthat remove inefficiencies due to control flow (loop handling) and memory\naccess, without drastically increasing processor complexity. Moving from a\nstate-of-the-art (SoA) ML accelerator cluster based on RISC-V processors, we\npropose a low-overhead optimized microarchitecture that eliminates these\ninefficiencies almost entirely while retaining programmability. We introduce\n\"zero-overhead loop nests\" to remove control overheads, and a \"zero-conflict\nmemory subsystem\", leveraging a novel double-buffering-aware interconnect, to\neliminate bank conflicts in L1 memory. With these enhancements, we attain\nnear-ideal utilizations between 96.1% and 99.4%, achieving 11% performance and\n8% energy efficiency improvements over the baseline SoA RISC-V cluster. We\ndemonstrate comparable utilizations and performance to a specialized SoA\naccelerator, with only 12% difference in energy efficiency, while providing a\nfully-programmable general-purpose solution supporting a significantly wider\nrange of workloads.", "comment": "7 pages, 5 figures, 2 tables. Accepted at ISLPED 2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10921v1", "AI": {"title_translation": "面向机器学习加速的节能RISC-V集群上的零停顿矩阵乘法", "tldr": "本文提出了一种优化的微架构，通过“零开销循环嵌套”和“零冲突内存子系统”消除了RISC-V机器学习加速器集群中的控制流和内存访问低效问题，实现了接近理想的利用率和显著的性能及能效提升，同时保持了完全可编程性。", "motivation": "机器学习工作负载日益增长的计算需求推动了机器学习加速器的设计，旨在实现效率和灵活性的最佳权衡。现有的基于RISC-V处理器的机器学习加速器集群在控制流（循环处理）和内存访问方面存在效率低下问题，需要进行微架构优化以消除这些低效，同时避免大幅增加处理器复杂性。", "method": "本文提出了一种低开销的优化微架构，该架构几乎完全消除了控制流和内存访问的低效，同时保留了可编程性。具体方法包括引入“零开销循环嵌套”以消除控制开销，以及利用新型双缓冲感知互连的“零冲突内存子系统”以消除L1内存中的bank冲突。", "result": "通过这些增强，实现了96.1%到99.4%的接近理想的利用率，相比基线SoA RISC-V集群，性能提升了11%，能效提升了8%。与专门的SoA加速器相比，利用率和性能相当，能效仅有12%的差异，同时提供了完全可编程的通用解决方案，支持更广泛的工作负载。", "conclusion": "本文提出的微架构显著提高了RISC-V集群上机器学习加速的效率和性能，提供了一个高度可编程且多功能的解决方案，其表现可与专用加速器相媲美，同时支持更广泛的机器学习工作负载。", "translation": "机器学习（ML）工作负载日益增长的计算需求推动了ML加速器的设计，旨在实现效率和灵活性之间的最佳权衡。一种广泛探索的灵活ML加速器架构基于轻量级指令处理器集群，这些处理器共享多bank的L1内存，并通过专门的指令扩展增强，用于关键的ML相关计算，例如矩阵乘法（matmul）。然而，指令扩展应与微架构优化相结合，以消除由于控制流（循环处理）和内存访问引起的低效，同时不大幅增加处理器复杂性。在最先进（SoA）的基于RISC-V处理器的ML加速器集群的基础上，我们提出了一种低开销的优化微架构，该架构几乎完全消除了这些低效，同时保留了可编程性。我们引入了“零开销循环嵌套”以消除控制开销，以及利用新型双缓冲感知互连的“零冲突内存子系统”以消除L1内存中的bank冲突。通过这些增强，我们实现了96.1%至99.4%的接近理想的利用率，相比基线SoA RISC-V集群，性能提升了11%，能效提升了8%。我们展示了与专用SoA加速器相当的利用率和性能，能效仅有12%的差异，同时提供了完全可编程的通用解决方案，支持显著更广泛的工作负载。", "summary": "本文提出了一种针对RISC-V集群上机器学习加速的优化微架构，旨在解决现有方案中控制流和内存访问导致的效率低下问题。通过引入“零开销循环嵌套”和“零冲突内存子系统”（利用双缓冲感知互连），该设计显著提升了矩阵乘法的效率。实验结果表明，该方案实现了接近理想的利用率（96.1%-99.4%），性能和能效分别比基线RISC-V集群提升了11%和8%，并且在保持完全可编程性的同时，性能与专用加速器相当，能效差距仅为12%，支持更广泛的机器学习任务。", "keywords": "机器学习, RISC-V, 矩阵乘法, 微架构, 能效", "comments": "本文的创新之处在于提出了两种具体的微架构优化技术：“零开销循环嵌套”和“零冲突内存子系统”，有效解决了RISC-V集群在机器学习加速中常见的控制流和内存访问瓶颈。其重要性在于，在不显著增加复杂性的前提下，显著提升了通用可编程RISC-V平台的性能和能效，使其能够与专用加速器相媲美，同时提供了更广泛的负载支持，这对于边缘AI和灵活的ML部署具有重要意义。"}}
{"id": "2506.10401", "title": "HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration", "authors": ["Jiaqi Lv", "Xufeng He", "Yanchen Liu", "Xu Dai", "Yang Hu", "Shouyi Yin"], "summary": "The rapid growth of deep learning has driven exponential increases in model\nparameters and computational demands. NVIDIA GPUs and their CUDA-based software\necosystem provide robust support for parallel computing, significantly\nalleviating computational bottlenecks. Meanwhile, due to the cultivation of\nuser programming habits and the high performance of GPUs, the CUDA ecosystem\nhas established a dominant position in the field of parallel software. This\ndominance requires other hardware platforms to support CUDA-based software with\nperformance portability. However, translating CUDA code to other platforms\nposes significant challenges due to differences in parallel programming\nparadigms and hardware architectures. Existing approaches rely on language\nextensions, domain-specific languages (DSLs), or compilers but face limitations\nin workload coverage and generalizability. Moreover, these methods often incur\nsubstantial development costs. Recently, LLMs have demonstrated extraordinary\npotential in various vertical domains, especially in code-related tasks.\nHowever, the performance of existing LLMs in CUDA transpilation, particularly\nfor high-performance code, remains suboptimal. The main reason for this\nlimitation lies in the lack of high-quality training datasets. To address these\nchallenges, we propose a novel framework for generating high-performance CUDA\nand corresponding platform code pairs, leveraging AI compiler and automatic\noptimization technology. We further enhance the framework with a graph-based\ndata augmentation method and introduce HPCTransEval, a benchmark for evaluating\nLLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU\ntranspilation as a case study on leading LLMs. The result demonstrates that our\nframework significantly improves CUDA transpilation, highlighting the potential\nof LLMs to address compatibility challenges within the CUDA ecosystem.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10401v1", "AI": {"title_translation": "HPCTransCompile：一个用于高性能CUDA转译和LLM初步探索的AI编译器生成数据集", "tldr": "该研究提出了HPCTransCompile框架，通过AI编译器和自动优化技术生成高质量的CUDA代码转译数据集，以解决现有LLM在高性能CUDA转译方面性能不佳的问题，并改善了LLM在CUDA转译上的表现。", "motivation": "深度学习的快速发展导致模型参数和计算需求呈指数级增长，NVIDIA GPU和CUDA生态系统在并行计算领域占据主导地位。然而，将CUDA代码转译到其他平台面临挑战，现有方法存在工作负载覆盖率和通用性限制，且开发成本高。尽管LLM在代码任务中显示出潜力，但由于缺乏高质量的训练数据集，其在高性能CUDA转译方面的表现不佳。", "method": "提出了一种新颖的框架，利用AI编译器和自动优化技术生成高性能CUDA及其对应平台代码对。该框架通过基于图的数据增强方法进一步增强，并引入了HPCTransEval基准来评估LLM在CUDA转译上的性能。", "result": "以CUDA到CPU转译为例，对主流LLM进行的实验表明，该框架显著改善了CUDA转译，突出了LLM解决CUDA生态系统内兼容性挑战的潜力。", "conclusion": "通过提出HPCTransCompile框架并生成高质量数据集，本研究显著提升了LLM在高性能CUDA转译方面的表现，验证了LLM解决CUDA生态系统兼容性问题的巨大潜力。", "translation": "深度学习的快速发展推动了模型参数和计算需求的指数级增长。NVIDIA GPU及其基于CUDA的软件生态系统为并行计算提供了强大的支持，显著缓解了计算瓶颈。同时，由于用户编程习惯的培养和GPU的高性能，CUDA生态系统在并行软件领域建立了主导地位。这种主导地位要求其他硬件平台能够支持CUDA软件的性能可移植性。然而，由于并行编程范式和硬件架构的差异，将CUDA代码转译到其他平台带来了重大挑战。现有方法依赖于语言扩展、领域特定语言（DSL）或编译器，但在工作负载覆盖率和通用性方面面临限制。此外，这些方法通常会产生高昂的开发成本。最近，大型语言模型（LLM）在各种垂直领域，尤其是在代码相关任务中，展现出非凡的潜力。然而，现有LLM在CUDA转译方面的性能，特别是对于高性能代码，仍然不尽如人意。造成这一限制的主要原因在于缺乏高质量的训练数据集。为了应对这些挑战，我们提出了一种新颖的框架，利用AI编译器和自动优化技术生成高性能CUDA及其对应平台代码对。我们通过基于图的数据增强方法进一步增强了该框架，并引入了HPCTransEval，一个用于评估LLM在CUDA转译上性能的基准。我们以CUDA到CPU转译作为案例研究，对主流LLM进行了实验。结果表明，我们的框架显著改善了CUDA转译，突出了LLM解决CUDA生态系统内兼容性挑战的潜力。", "summary": "本论文提出了HPCTransCompile框架，旨在解决大型语言模型（LLM）在高性能CUDA代码转译中因缺乏高质量训练数据而表现不佳的问题。该框架利用AI编译器和自动优化技术，生成高性能CUDA及其对应平台的代码对，并通过图基数据增强方法进行强化。此外，论文还引入了HPCTransEval基准来评估LLM的转译性能。实验结果表明，该框架显著提升了LLM在CUDA转译上的能力，展示了LLM在解决CUDA生态系统兼容性挑战方面的巨大潜力。", "keywords": "CUDA转译, 大型语言模型, AI编译器, 数据集生成, 性能可移植性", "comments": "该论文的创新点在于提出了一个结合AI编译器和自动优化技术来生成高质量CUDA转译数据集的框架，有效解决了LLM训练数据稀缺的问题。通过引入图基数据增强和专门的评估基准HPCTransEval，该研究为高性能代码转译领域LLM的应用开辟了新的道路，对于促进CUDA生态系统内的跨平台兼容性具有重要意义。"}}
{"id": "2506.10711", "title": "PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment", "authors": ["Li Luo", "Shangsong Liang"], "summary": "Generating accurate and stable long rollouts is a notorious challenge for\ntime-dependent PDEs (Partial Differential Equations). Recently, motivated by\nthe importance of high-frequency accuracy, a refiner model called PDERefiner\nutilizes diffusion models to refine outputs for every time step, since the\ndenoising process could increase the correctness of modeling high frequency\npart. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the\namplitude of high frequency part better than not doing refinement process.\nHowever, for some other cases, the spectrum might be more complicated. For\nexample, for a harder PDE like Navior-Stokes equation, diffusion models could\nover-degrade the higher frequency part. This motivates us to release the\nconstraint that each frequency weighs the same. We enhance our refiner model\nwith doing adjustments on spectral space, which recovers Blurring diffusion\nmodels. We developed a new v-prediction technique for Blurring diffusion\nmodels, recovering the MSE training objective on the first refinement step. We\nshow that in this case, for different model backbones, such as U-Net and neural\noperators, the outputs of PDE-SpectralRefiner are more accurate for both\none-step MSE loss and rollout loss.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10711v1", "AI": {"title_translation": "PDESpectralRefiner：通过频谱调整实现更准确的长期模拟", "tldr": "PDESpectralRefiner通过在扩散模型中引入频谱调整，解决了时间依赖性偏微分方程（PDEs）长期模拟中高频部分过度衰减的问题，从而提高了模拟的准确性。", "motivation": "时间依赖性偏微分方程（PDEs）在生成准确稳定的长期模拟方面面临挑战。现有的基于扩散模型的细化器（如PDERefiner）虽然能提高高频部分的正确性，但对于一些复杂的PDEs（如Navier-Stokes方程），可能会过度衰减高频部分，这促使研究者需要一种能对不同频率进行加权调整的方法。", "method": "作者通过在频谱空间进行调整来增强其细化模型，从而恢复了模糊扩散模型。此外，他们为模糊扩散模型开发了一种新的v-预测技术，该技术在第一个细化步骤中恢复了MSE训练目标。", "result": "PDESpectralRefiner的输出在一步MSE损失和模拟损失方面都更准确，并且适用于U-Net和神经算子等不同的模型骨干。", "conclusion": "通过在频谱空间进行调整并引入新的v-预测技术，PDESpectralRefiner有效地解决了扩散模型在PDEs长期模拟中高频部分过度衰减的问题，显著提高了模拟的准确性。", "translation": "时间依赖性偏微分方程（PDEs）生成准确稳定的长期模拟是一个臭名昭著的挑战。最近，受高频精度重要性的启发，一个名为PDERefiner的细化模型利用扩散模型来细化每个时间步的输出，因为去噪过程可以增加高频部分建模的正确性。对于一维Kuramoto-Sivashinsky方程，细化模型比不进行细化过程能更好地降低高频部分的振幅。然而，对于其他一些情况，频谱可能更复杂。例如，对于像Navier-Stokes方程这样更难的PDE，扩散模型可能会过度降低高频部分。这促使我们放开每个频率权重相同的约束。我们通过在频谱空间进行调整来增强我们的细化模型，这恢复了模糊扩散模型。我们为模糊扩散模型开发了一种新的v-预测技术，在第一个细化步骤中恢复了MSE训练目标。我们表明，在这种情况下，对于不同的模型骨干，例如U-Net和神经算子，PDESpectralRefiner的输出在一步MSE损失和模拟损失方面都更准确。", "summary": "本文介绍了PDESpectralRefiner，一个增强的细化模型，旨在提高时间依赖性偏微分方程长期模拟的准确性和稳定性。为了解决先前基于扩散的细化器可能过度衰减高频的限制，PDESpectralRefiner引入了频谱空间调整和一种新的模糊扩散模型v-预测技术。实验表明，这种方法在一步MSE损失和模拟损失方面都能产生更准确的结果，并且适用于各种模型架构。", "keywords": "PDESpectralRefiner, 偏微分方程, 扩散模型, 频谱调整, 长期模拟", "comments": "这项工作的创新之处在于通过引入频谱调整，解决了基于扩散的PDE求解器在高频部分过度衰减的特定问题。这对于实现准确的长期模拟非常重要，特别是对于复杂的PDEs。新开发的v-预测技术也是一个显著的贡献。"}}
{"id": "2506.10876", "title": "Landauer Principle and Thermodynamics of Computation", "authors": ["Pritam Chattopadhyay", "Avijit Misra", "Tanmoy Pandit", "Goutam Paul"], "summary": "According to the Landauer principle, any logically irreversible process\naccompanies entropy production, which results in heat dissipation in the\nenvironment. Erasing of information, one of the primary logically irreversible\nprocesses, has a lower bound on heat dissipated into the environment, called\nthe Landauer bound (LB). However, the practical erasure processes dissipate\nmuch more heat than the LB. Recently, there have been a few experimental\ninvestigations to reach this bound both in the classical and quantum domains.\nThere has also been a spate of activities to enquire about this LB in finite\ntime, with finite-size heat baths, non-Markovian and nonequilibrium environment\nin the quantum regime where the effects of fluctuations and correlation of the\nsystems with the bath can no longer be ignored. This article provides a\ncomprehensive review of the recent progress on the Landauer bound, which serves\nas a fundamental principle in the thermodynamics of computation. We also\nprovide a perspective for future endeavors in these directions. Furthermore, we\nreview the recent exploration toward establishing energetic bounds of a\ncomputational process. We also review the thermodynamic aspects of error\ncorrection, which is an indispensable part of information processing and\ncomputations. In doing so, we briefly discuss the basics of these fields to\nprovide a complete picture.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10876v1", "AI": {"title_translation": "朗道尔原理与计算热力学", "tldr": "本文综述了朗道尔原理的最新进展，包括实验验证、有限时间/非平衡环境下的研究，以及计算过程的能量界限和纠错的热力学方面。", "motivation": "本文旨在全面回顾朗道尔原理的最新进展，该原理是计算热力学中的一个基本原理。同时，也为未来的研究方向提供展望，并回顾了计算过程的能量界限以及纠错的热力学方面。", "method": "本文采用综述（review）的方法，对朗道尔原理、计算热力学中的能量界限以及纠错的热力学方面进行了全面回顾。", "result": "文章回顾了朗道尔原理的最新进展，包括在经典和量子领域达到朗道尔极限的实验探索，以及在有限时间、有限热浴、非马尔可夫和非平衡量子环境下的朗道尔极限研究。此外，还回顾了计算过程的能量界限和纠错的热力学方面。", "conclusion": "本文对朗道尔原理的最新进展进行了全面回顾，该原理是计算热力学中的基本原理，并为未来的研究方向提供了展望。同时，也探讨了计算过程的能量界限和纠错的热力学方面。", "translation": "根据朗道尔原理，任何逻辑上不可逆的过程都伴随着熵的产生，这导致了环境中热量的耗散。信息擦除作为主要的逻辑不可逆过程之一，其耗散到环境中的热量有一个下限，称为朗道尔极限（LB）。然而，实际的擦除过程耗散的热量远高于朗道尔极限。最近，在经典和量子领域都有一些实验研究试图达到这个极限。同时，也有大量活动探讨在有限时间、有限尺寸热浴、非马尔可夫和非平衡量子体系中朗道尔极限的问题，在这些情况下，系统与热浴的涨落和关联效应不再能被忽略。本文全面回顾了朗道尔极限的最新进展，它作为计算热力学中的一个基本原理。我们还为这些方向的未来努力提供了展望。此外，我们回顾了最近在建立计算过程能量界限方面的探索。我们还回顾了纠错的热力学方面，这是信息处理和计算中不可或缺的一部分。在此过程中，我们简要讨论了这些领域的基础知识，以提供一个完整的图景。", "summary": "本文全面综述了朗道尔原理在计算热力学中的最新进展，该原理指出逻辑不可逆过程伴随熵产生和热耗散。文章回顾了达到朗道尔极限的实验探索、有限时间与非平衡环境下的理论研究，并探讨了计算过程的能量界限以及信息处理中纠错的热力学特性，同时为未来研究提供了展望。", "keywords": "朗道尔原理, 计算热力学, 信息擦除, 能量界限, 纠错", "comments": "本文作为一篇综述性文章，全面梳理了朗道尔原理在计算热力学领域的最新进展，涵盖了实验和理论研究，并扩展到能量界限和纠错的热力学方面。其重要性在于为该领域的研究人员提供了系统的知识框架和未来研究方向的指引。"}}
{"id": "2506.10272", "title": "Collective Bargaining in the Information Economy Can Address AI-Driven Power Concentration", "authors": ["Nicholas Vincent", "Matthew Prewitt", "Hanlin Li"], "summary": "This position paper argues that there is an urgent need to restructure\nmarkets for the information that goes into AI systems. Specifically, producers\nof information goods (such as journalists, researchers, and creative\nprofessionals) need to be able to collectively bargain with AI product builders\nin order to receive reasonable terms and a sustainable return on the\ninformational value they contribute. We argue that without increased market\ncoordination or collective bargaining on the side of these primary information\nproducers, AI will exacerbate a large-scale \"information market failure\" that\nwill lead not only to undesirable concentration of capital, but also to a\npotential \"ecological collapse\" in the informational commons. On the other\nhand, collective bargaining in the information economy can create market\nfrictions and aligned incentives necessary for a pro-social, sustainable AI\nfuture. We provide concrete actions that can be taken to support a\ncoalition-based approach to achieve this goal. For example, researchers and\ndevelopers can establish technical mechanisms such as federated data management\ntools and explainable data value estimations, to inform and facilitate\ncollective bargaining in the information economy. Additionally, regulatory and\npolicy interventions may be introduced to support trusted data intermediary\norganizations representing guilds or syndicates of information producers.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10272v1", "AI": {"title_translation": "信息经济中的集体谈判可以解决人工智能驱动的权力集中问题", "tldr": "AI可能导致信息市场失灵和权力集中，集体谈判是解决之道。", "motivation": "解决AI系统对信息生产者造成的不公平条款和不可持续回报，以及AI加剧的信息市场失灵和资本集中，甚至信息公域的“生态崩溃”。", "method": "提出通过信息生产者的集体谈判来重构信息市场；具体行动包括研究人员和开发者建立技术机制（如联邦数据管理工具、可解释的数据价值评估），以及引入监管和政策干预来支持可信的数据中介组织。", "result": "通过集体谈判，可以创造市场摩擦和一致的激励机制，实现一个亲社会、可持续的AI未来，避免信息市场失灵和权力集中。", "conclusion": "信息经济中的集体谈判是实现亲社会、可持续AI未来的关键，需要技术和政策干预来支持信息生产者的联盟化方法。", "translation": "这篇立场文件认为，迫切需要重构用于人工智能系统的信息市场。具体而言，信息商品生产者（如记者、研究人员和创意专业人士）需要能够与人工智能产品开发者进行集体谈判，以获得合理的条款和对其贡献的信息价值的可持续回报。我们认为，如果这些主要信息生产者缺乏市场协调或集体谈判，人工智能将加剧大规模的“信息市场失灵”，这不仅会导致资本的不良集中，还会导致信息公域的潜在“生态崩溃”。另一方面，信息经济中的集体谈判可以创造市场摩擦和一致的激励机制，这对于一个亲社会、可持续的人工智能未来是必要的。我们提供了可以采取的具体行动来支持实现这一目标的联盟式方法。例如，研究人员和开发者可以建立技术机制，如联邦数据管理工具和可解释的数据价值评估，以告知和促进信息经济中的集体谈判。此外，可以引入监管和政策干预，以支持代表信息生产者行会或辛迪加的可信数据中介组织。", "summary": "本立场文件主张，为应对人工智能可能加剧的信息市场失灵、资本集中及信息公域的“生态崩溃”，迫切需要重构人工智能系统的信息市场。核心解决方案是信息生产者（如记者、研究人员、创意专业人士）与人工智能产品开发者进行集体谈判，以确保合理回报和可持续发展。文章提出，集体谈判能创造促进亲社会、可持续AI未来的市场激励，并通过技术机制（如联邦数据管理、数据价值评估）和政策干预（如支持数据中介组织）来支持这种联盟化的方法。", "keywords": "集体谈判, 信息经济, 人工智能, 权力集中, 市场失灵", "comments": "本文创新性地将传统劳工领域的“集体谈判”概念引入到信息经济和AI领域，以应对AI发展带来的新型权力集中和市场失灵问题。其重要性在于提出了一个具体的、多维度的解决方案，结合了技术和政策层面的干预，旨在保护信息生产者权益并促进AI的健康可持续发展。"}}
{"id": "2506.10166", "title": "DeepPolar+: Breaking the BER-BLER Trade-off with Self-Attention and SMART (SNR-MAtched Redundancy Technique) decoding", "authors": ["Shubham Srivastava", "Adrish Banerjee"], "summary": "DeepPolar codes have recently emerged as a promising approach for channel\ncoding, demonstrating superior bit error rate (BER) performance compared to\nconventional polar codes. Despite their excellent BER characteristics, these\ncodes exhibit suboptimal block error rate (BLER) performance, creating a\nfundamental BER-BLER trade-off that severely limits their practical deployment\nin communication systems. This paper introduces DeepPolar+, an enhanced neural\npolar coding framework that systematically eliminates this BER-BLER trade-off\nby simultaneously improving BLER performance while maintaining the superior BER\ncharacteristics of DeepPolar codes. Our approach achieves this breakthrough\nthrough three key innovations: (1) an attention-enhanced decoder architecture\nthat leverages multi-head self-attention mechanisms to capture complex\ndependencies between bit positions, (2) a structured loss function that jointly\noptimizes for both bit-level accuracy and block-level reliability, and (3) an\nadaptive SNR-Matched Redundancy Technique (SMART) for decoding DeepPolar+ code\n(DP+SMART decoder) that combines specialized models with CRC verification for\nrobust performance across diverse channel conditions. For a (256,37) code\nconfiguration, DeepPolar+ demonstrates notable improvements in both BER and\nBLER performance compared to conventional successive cancellation decoding and\nDeepPolar, while achieving remarkably faster convergence through improved\narchitecture and optimization strategies. The DeepPolar+SMART variant further\namplifies these dual improvements, delivering significant gains in both error\nrate metrics over existing approaches. DeepPolar+ effectively bridges the gap\nbetween theoretical potential and practical implementation of neural polar\ncodes, offering a viable path forward for next-generation error correction\nsystems.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10166v1", "AI": {"title_translation": "DeepPolar+：通过自注意力机制和SMART（信噪比匹配冗余技术）解码打破BER-BLER权衡", "tldr": "DeepPolar+通过引入自注意力机制和SMART解码技术，显著提升了DeepPolar码的误块率（BLER）性能，同时保持了优异的误码率（BER），成功打破了BER-BLER权衡，为神经极化码的实际应用提供了新的解决方案。", "motivation": "现有的DeepPolar码虽然具有卓越的误码率（BER）性能，但其误块率（BLER）性能次优，导致了基本的BER-BLER权衡，严重限制了它们在通信系统中的实际部署和应用。", "method": "本文提出了DeepPolar+，一个增强的神经极化编码框架，通过以下三项关键创新来消除BER-BLER权衡：1. 注意力增强的解码器架构：利用多头自注意力机制捕获比特位置间的复杂依赖关系。2. 结构化损失函数：联合优化比特级精度和块级可靠性。3. 自适应SNR-匹配冗余技术（SMART）用于DeepPolar+码解码（DP+SMART解码器）：结合专用模型与CRC验证，以在不同信道条件下实现鲁棒性能。", "result": "对于(256,37)码配置，DeepPolar+在BER和BLER性能上均显著优于传统逐次抵消解码和DeepPolar。通过改进架构和优化策略，DeepPolar+实现了显著更快的收敛速度。DeepPolar+SMART变体进一步放大了这些双重改进，在错误率指标上实现了比现有方法更大的提升。", "conclusion": "DeepPolar+有效弥合了神经极化码理论潜力与实际实现之间的差距，为下一代错误纠正系统提供了可行的发展路径。", "translation": "DeepPolar码最近作为一种有前途的信道编码方法出现，与传统极化码相比，表现出卓越的误码率（BER）性能。尽管其BER特性优异，但这些码的误块率（BLER）性能次优，造成了基本的BER-BLER权衡，严重限制了它们在通信系统中的实际部署。本文介绍了DeepPolar+，一个增强的神经极化编码框架，通过同时提高BLER性能并保持DeepPolar码卓越的BER特性，系统地消除了这种BER-BLER权衡。我们的方法通过三项关键创新实现了这一突破：(1) 一种注意力增强的解码器架构，利用多头自注意力机制捕获比特位置之间的复杂依赖关系，(2) 一个结构化损失函数，联合优化比特级精度和块级可靠性，以及 (3) 一种用于DeepPolar+码解码的自适应SNR匹配冗余技术（SMART）（DP+SMART解码器），结合专用模型和CRC验证，在不同信道条件下实现鲁棒性能。对于(256,37)码配置，与传统逐次抵消解码和DeepPolar相比，DeepPolar+在BER和BLER性能上均表现出显著改进，同时通过改进的架构和优化策略实现了显著更快的收敛。DeepPolar+SMART变体进一步放大了这些双重改进，在现有方法上实现了错误率指标的显著提升。DeepPolar+有效弥合了神经极化码理论潜力与实际实现之间的差距，为下一代错误纠正系统提供了可行的途径。", "summary": "本文提出了DeepPolar+，一个增强的神经极化编码框架，旨在解决现有DeepPolar码在误码率（BER）和误块率（BLER）之间存在的权衡问题。通过引入注意力增强的解码器架构（利用多头自注意力）、结构化损失函数（联合优化比特级精度和块级可靠性）以及自适应SNR匹配冗余技术（SMART）解码器，DeepPolar+成功地同时提升了BLER性能并保持了优异的BER特性。实验结果表明，DeepPolar+在BER和BLER上均优于传统方法和DeepPolar，并实现了更快的收敛，有效推动了神经极化码的实际应用。", "keywords": "DeepPolar+, 神经极化码, BER-BLER权衡, 自注意力, SMART解码", "comments": "这篇论文的核心创新在于通过集成自注意力机制和SNR匹配冗余技术，成功解决了DeepPolar码在实际应用中面临的BER-BLER权衡这一关键挑战。通过对解码器架构、损失函数和解码策略的全面优化，该工作显著提升了神经极化码的实用性和鲁棒性，为下一代通信系统中的高性能错误纠正提供了重要的技术突破。"}}
{"id": "2506.10251", "title": "Energy Aware Camera Location Search Algorithm for Increasing Precision of Observation in Automated Manufacturing", "authors": ["Rongfei Li", "Francis Assadian"], "summary": "Visual servoing technology has been well developed and applied in many\nautomated manufacturing tasks, especially in tools' pose alignment. To access a\nfull global view of tools, most applications adopt eye-to-hand configuration or\neye-to-hand/eye-in-hand cooperation configuration in an automated manufacturing\nenvironment. Most research papers mainly put efforts into developing control\nand observation architectures in various scenarios, but few of them have\ndiscussed the importance of the camera's location in eye-to-hand configuration.\nIn a manufacturing environment, the quality of camera estimations may vary\nsignificantly from one observation location to another, as the combined effects\nof environmental conditions result in different noise levels of a single image\nshot at different locations. In this paper, we propose an algorithm for the\ncamera's moving policy so that it explores the camera workspace and searches\nfor the optimal location where the images' noise level is minimized. Also, this\nalgorithm ensures the camera ends up at a suboptimal (if the optimal one is\nunreachable) location among the locations already searched, with limited energy\navailable for moving the camera. Unlike a simple brute force approach, the\nalgorithm enables the camera to explore space more efficiently by adapting the\nsearch policy from learning the environment. With the aid of an image averaging\ntechnique, this algorithm, in use of a solo camera, achieves the observation\naccuracy in eye-to-hand configurations to a desirable extent without filtering\nout high-frequency information in the original image. An automated\nmanufacturing application has been simulated and the results show the success\nof this algorithm's improvement of observation precision with limited energy.", "comment": "35 pages, 24 figures, Journal, Published in: Applied Sciences, 2024,\n  vol. 14, article 9140. For published version, see this http URL:\n  https://doi.org/10.3390/app14199140", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10251v1", "AI": {"title_translation": "自动化制造中提高观测精度的节能相机位置搜索算法", "tldr": "本文提出了一种节能的相机位置搜索算法，用于在自动化制造环境中找到最佳相机位置以最小化图像噪声并提高观测精度。", "motivation": "在自动化制造中，相机位置对图像估计质量有显著影响，因为不同位置的环境条件会导致不同的图像噪声水平。现有研究很少讨论相机位置的重要性。", "method": "本文提出了一种相机移动策略算法，使其探索相机工作空间并搜索图像噪声水平最小的最佳位置。该算法在有限的相机移动能量下，确保相机停留在已搜索位置中的次优（如果最优不可达）位置。与简单的暴力方法不同，该算法通过学习环境来调整搜索策略，从而更有效地探索空间。借助图像平均技术，该算法使用单个相机在不滤除原始图像中高频信息的情况下，将手眼配置中的观测精度提高到理想程度。", "result": "通过模拟一个自动化制造应用，结果表明该算法在有限能量下成功提高了观测精度。", "conclusion": "该算法利用单个相机和图像平均技术，在不滤除原始图像中高频信息的情况下，将手眼配置中的观测精度提高到理想程度。", "translation": "视觉伺服技术已经得到了很好的发展，并应用于许多自动化制造任务中，特别是在工具姿态对齐方面。为了获取工具的完整全局视图，大多数应用在自动化制造环境中采用手眼（eye-to-hand）配置或手眼/眼内（eye-to-hand/eye-in-hand）协作配置。大多数研究论文主要致力于在各种场景中开发控制和观测架构，但很少有论文讨论手眼配置中相机位置的重要性。在制造环境中，相机估计的质量可能因不同的观测位置而显著不同，因为环境条件的综合影响导致在不同位置拍摄的单张图像的噪声水平不同。在本文中，我们提出了一种相机移动策略算法，使其探索相机工作空间并搜索图像噪声水平最小的最佳位置。此外，该算法确保相机在已搜索位置中停留在次优（如果最优不可达）位置，且相机移动的可用能量有限。与简单的暴力方法不同，该算法通过学习环境来调整搜索策略，从而更有效地探索空间。借助图像平均技术，该算法使用单个相机，在不滤除原始图像中高频信息的情况下，将手眼配置中的观测精度提高到理想程度。一个自动化制造应用已经进行了模拟，结果表明该算法在有限能量下成功提高了观测精度。", "summary": "本文针对自动化制造中相机位置对观测精度的影响，提出了一种节能的相机位置搜索算法。该算法通过学习环境，智能地探索相机工作空间，旨在寻找图像噪声最小的最佳位置，并在能量有限的情况下确保相机停留在次优位置。通过图像平均技术，该算法使用单个相机即可有效提高手眼配置下的观测精度，且不损失图像高频信息。仿真结果验证了其在有限能量下提高观测精度的有效性。", "keywords": "视觉伺服, 相机位置, 噪声抑制, 自动化制造, 节能", "comments": "该论文的创新点在于提出了一个节能且自适应的相机位置搜索算法，解决了传统手眼配置中相机位置对观测精度影响的忽视。它通过智能探索和图像平均技术，避免了暴力搜索的低效，并在能量受限的实际工业环境中实现了高精度观测，具有重要的实际应用价值。其能够在不牺牲高频信息的前提下提高精度，是其显著优点。"}}
{"id": "2506.10237", "title": "Intelligent Travel Activity Monitoring: Generalized Distributed Acoustic Sensing Approaches", "authors": ["Ruikang Zhong", "Chia-Yen Chiang", "Mona Jaber", "Rupert De Wilde", "Peter Hayward"], "summary": "Obtaining data on active travel activities such as walking, jogging, and\ncycling is important for refining sustainable transportation systems (STS).\nEffectively monitoring these activities not only requires sensing solutions to\nhave a joint feature of being accurate, economical, and privacy-preserving, but\nalso enough generalizability to adapt to different climate environments and\ndeployment conditions. In order to provide a generalized sensing solution, a\ndeep learning (DL)-enhanced distributed acoustic sensing (DAS) system for\nmonitoring active travel activities is proposed. By leveraging the ambient\nvibrations captured by DAS, this scheme infers motion patterns without relying\non image-based or wearable devices, thereby addressing privacy concerns. We\nconduct real-world experiments in two geographically distinct locations and\ncollect comprehensive datasets to evaluate the performance of the proposed\nsystem. To address the generalization challenges posed by heterogeneous\ndeployment environments, we propose two solutions according to network\navailability: 1) an Internet-of-Things (IoT) scheme based on federated learning\n(FL) is proposed, and it enables geographically different DAS nodes to be\ntrained collaboratively to improve generalizability; 2) an off-line\ninitialization approach enabled by meta-learning is proposed to develop\nhigh-generality initialization for DL models and to enable rapid model\nfine-tuning with limited data samples, facilitating generalization at newly\nestablished or isolated DAS nodes. Experimental results of the walking and\ncycling classification problem demonstrate the performance and generalizability\nof the proposed DL-enhanced DAS system, paving the way for practical,\nlarge-scale DAS monitoring of active travel.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10237v1", "AI": {"title_translation": "智能出行活动监测：通用分布式声学传感方法", "tldr": "提出了一种结合深度学习的分布式声学传感（DAS）系统，用于监测步行和骑行等出行活动，通过联邦学习和元学习解决了系统泛化性问题，实现了准确、经济和保护隐私的出行监测。", "motivation": "为了完善可持续交通系统，获取步行、慢跑和骑行等主动出行活动数据至关重要。有效的监测方案需要同时具备准确性、经济性和隐私保护性，并能适应不同气候环境和部署条件的泛化能力。", "method": "本文提出了一种深度学习（DL）增强的分布式声学传感（DAS）系统，通过利用DAS捕获的环境振动来推断运动模式，避免了对基于图像或可穿戴设备的依赖，从而保护隐私。为了解决异构部署环境带来的泛化挑战，根据网络可用性提出了两种方案：1）基于联邦学习（FL）的物联网（IoT）方案，使地理位置不同的DAS节点能够协作训练以提高泛化能力；2）基于元学习的离线初始化方法，用于为DL模型开发高泛化性初始化，并支持使用有限数据样本进行快速模型微调，以促进在新建立或隔离的DAS节点上的泛化。", "result": "在步行和骑行分类问题上的实验结果表明，所提出的深度学习增强DAS系统具有良好的性能和泛化能力。", "conclusion": "所提出的深度学习增强DAS系统为主动出行的实用、大规模DAS监测铺平了道路。", "translation": "获取步行、慢跑和骑行等主动出行活动数据对于完善可持续交通系统（STS）至关重要。有效监测这些活动不仅要求传感解决方案同时具备准确性、经济性和隐私保护性，还需要足够的泛化能力以适应不同的气候环境和部署条件。为了提供一种通用的传感解决方案，本文提出了一种深度学习（DL）增强的分布式声学传感（DAS）系统，用于监测主动出行活动。通过利用DAS捕获的环境振动，该方案无需依赖基于图像或可穿戴设备即可推断运动模式，从而解决了隐私问题。我们在两个地理位置不同的地方进行了真实世界实验，并收集了全面的数据集来评估所提出系统的性能。为了解决异构部署环境带来的泛化挑战，我们根据网络可用性提出了两种解决方案：1）提出了一种基于联邦学习（FL）的物联网（IoT）方案，它使地理位置不同的DAS节点能够协作训练以提高泛化能力；2）提出了一种由元学习实现的离线初始化方法，用于为DL模型开发高泛化性初始化，并支持使用有限数据样本进行快速模型微调，从而促进在新建立或隔离的DAS节点上的泛化。步行和骑行分类问题的实验结果证明了所提出的DL增强DAS系统的性能和泛化能力，为主动出行的实用、大规模DAS监测铺平了道路。", "summary": "本文提出了一种深度学习（DL）增强的分布式声学传感（DAS）系统，旨在智能监测步行、慢跑和骑行等主动出行活动。该系统利用环境振动进行运动模式推断，避免了图像或可穿戴设备的使用，从而有效保护用户隐私。为解决系统在不同部署环境下的泛化难题，研究提出了两种创新方案：一是基于联邦学习的物联网（IoT）协同训练机制，增强不同DAS节点间的协作泛化能力；二是基于元学习的离线初始化方法，支持DL模型在数据受限的新节点上快速适应和微调。实验结果验证了该DL-增强DAS系统在步行和骑行分类任务上的优异性能和泛化能力，为未来大规模主动出行监测奠定了基础。", "keywords": "分布式声学传感, 深度学习, 联邦学习, 元学习, 出行监测", "comments": "这篇论文通过结合深度学习和分布式声学传感技术，提出了一种新颖且具有隐私保护特性的出行活动监测方案。其创新点在于利用环境振动而非传统图像或可穿戴设备，有效解决了隐私顾虑。此外，针对异构部署环境下的泛化性挑战，引入联邦学习和元学习两种策略，显著提升了系统在实际应用中的适应性。这对于推动可持续交通系统的发展和大规模智能城市基础设施建设具有重要意义。"}}
{"id": "2506.10349", "title": "Joint ASR and Speaker Role Tagging with Serialized Output Training", "authors": ["Anfeng Xu", "Tiantian Feng", "Shrikanth Narayanan"], "summary": "Automatic Speech Recognition systems have made significant progress with\nlarge-scale pre-trained models. However, most current systems focus solely on\ntranscribing the speech without identifying speaker roles, a function that is\ncritical for conversational AI. In this work, we investigate the use of\nserialized output training (SOT) for joint ASR and speaker role tagging. By\naugmenting Whisper with role-specific tokens and fine-tuning it with SOT, we\nenable the model to generate role-aware transcriptions in a single decoding\npass. We compare the SOT approach against a self-supervised previous baseline\nmethod on two real-world conversational datasets. Our findings show that this\napproach achieves more than 10% reduction in multi-talker WER, demonstrating\nits feasibility as a unified model for speaker-role aware speech transcription.", "comment": "Under review", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10349v1", "AI": {"title_translation": "联合ASR和说话人角色标注的序列化输出训练", "tldr": "本文提出使用序列化输出训练（SOT）方法，通过增强Whisper模型，实现联合ASR和说话人角色标注，显著降低了多说话人词错率（WER）。", "motivation": "当前大多数自动语音识别（ASR）系统仅专注于语音转录，而未能识别说话人角色，这对于对话式AI至关重要。", "method": "研究并采用序列化输出训练（SOT）方法，用于联合ASR和说话人角色标注。具体做法是，通过添加角色特定标记来增强Whisper模型，并使用SOT进行微调，使模型能够在单次解码过程中生成角色感知的转录。将此SOT方法与一种自监督的先前基线方法在两个真实世界的对话数据集上进行了比较。", "result": "该方法在多说话人词错率（WER）上实现了超过10%的降低。", "conclusion": "该方法证明了其作为统一模型实现说话人角色感知语音转录的可行性。", "translation": "自动语音识别系统在大规模预训练模型的帮助下取得了显著进展。然而，当前大多数系统仅专注于转录语音，而没有识别说话人角色，这对于对话式AI至关重要。在这项工作中，我们研究了使用序列化输出训练（SOT）进行联合ASR和说话人角色标注的方法。通过使用角色特定标记增强Whisper模型，并使用SOT进行微调，我们使模型能够在单次解码过程中生成角色感知的转录。我们将SOT方法与一种自监督的先前基线方法在两个真实世界的对话数据集上进行了比较。我们的研究结果表明，这种方法在多说话人词错率（WER）上实现了超过10%的降低，证明了其作为统一模型实现说话人角色感知语音转录的可行性。", "summary": "本文旨在解决当前ASR系统无法识别说话人角色的局限性，这对于对话式AI至关重要。研究提出了一种新颖的方法，即利用序列化输出训练（SOT）来联合执行ASR和说话人角色标注。通过使用角色特定标记增强并微调Whisper模型，该系统能够单次解码生成角色感知的转录。在两个真实世界对话数据集上的实验结果表明，该方法能将多说话人词错率（WER）降低10%以上，证明了其作为说话人角色感知语音转录统一模型的有效性。", "keywords": "ASR, 说话人角色标注, 序列化输出训练, Whisper, 对话式AI", "comments": "这项工作的创新之处在于将说话人角色标注直接集成到ASR过程中，通过对预训练模型（如Whisper）应用序列化输出训练（SOT），实现了单次解码生成角色感知转录。这对于开发更复杂的对话式AI和提高ASR在多说话人环境中的实用性至关重要。多说话人词错率（WER）的显著降低有力地证明了其实用价值。"}}
{"id": "2506.10179", "title": "Correlation vs causation in Alzheimer's disease: an interpretability-driven study", "authors": ["Hamzah Dabool", "Raghad Mustafa"], "summary": "Understanding the distinction between causation and correlation is critical\nin Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and\nthe identification of true disease drivers. This experiment investigates the\nrelationships among clinical, cognitive, genetic, and biomarker features using\na combination of correlation analysis, machine learning classification, and\nmodel interpretability techniques. Employing the XGBoost algorithm, we\nidentified key features influencing AD classification, including cognitive\nscores and genetic risk factors. Correlation matrices revealed clusters of\ninterrelated variables, while SHAP (SHapley Additive exPlanations) values\nprovided detailed insights into feature contributions across disease stages.\nOur results highlight that strong correlations do not necessarily imply\ncausation, emphasizing the need for careful interpretation of associative data.\nBy integrating feature importance and interpretability with classical\nstatistical analysis, this work lays groundwork for future causal inference\nstudies aimed at uncovering true pathological mechanisms. Ultimately,\ndistinguishing causal factors from correlated markers can lead to improved\nearly diagnosis and targeted interventions for Alzheimer's disease.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10179v1", "AI": {"title_translation": "阿尔茨海默病中的相关性与因果关系：一项可解释性驱动的研究", "tldr": "本研究利用相关性分析、机器学习分类和模型可解释性技术，区分阿尔茨海默病中的因果关系与相关性，强调强相关性不必然意味着因果关系，并为未来的因果推断研究奠定基础。", "motivation": "理解阿尔茨海默病 (AD) 中因果关系与相关性之间的区别至关重要，因为它影响诊断、治疗以及识别真正的疾病驱动因素。", "method": "本研究结合了相关性分析、机器学习分类（采用 XGBoost 算法）和模型可解释性技术（如 SHAP 值），以调查临床、认知、遗传和生物标志物特征之间的关系。", "result": "研究识别了影响 AD 分类的关键特征，包括认知评分和遗传风险因素。相关性矩阵揭示了相互关联变量的簇，而 SHAP 值提供了特征在疾病各个阶段贡献的详细见解。结果强调强相关性不一定意味着因果关系。", "conclusion": "区分因果因素与相关标记可以改善阿尔茨海默病的早期诊断和靶向干预。这项工作通过整合特征重要性和可解释性与经典统计分析，为未来旨在揭示真正病理机制的因果推断研究奠定了基础。", "translation": "理解阿尔茨海默病 (AD) 研究中因果关系与相关性之间的区别至关重要，因为它影响诊断、治疗以及识别真正的疾病驱动因素。本实验通过结合相关性分析、机器学习分类和模型可解释性技术，调查了临床、认知、遗传和生物标志物特征之间的关系。我们采用 XGBoost 算法，识别了影响 AD 分类的关键特征，包括认知评分和遗传风险因素。相关性矩阵揭示了相互关联变量的簇，而 SHAP (SHapley Additive exPlanations) 值则提供了特征在疾病各个阶段贡献的详细见解。我们的结果强调，强相关性不一定意味着因果关系，强调需要仔细解释关联数据。通过将特征重要性和可解释性与经典统计分析相结合，这项工作为未来旨在揭示真正病理机制的因果推断研究奠定了基础。最终，区分因果因素与相关标记可以改善阿尔茨海默病的早期诊断和靶向干预。", "summary": "本研究旨在区分阿尔茨海默病 (AD) 中的因果关系与相关性，这对于疾病诊断、治疗和驱动因素识别至关重要。研究结合了相关性分析、XGBoost 机器学习分类和 SHAP 可解释性技术，调查了临床、认知、遗传和生物标志物特征间的关系。结果识别了认知评分和遗传风险因素等关键特征，并强调强相关性不必然意味着因果关系。该工作为未来旨在揭示真正病理机制的因果推断研究奠定了基础，以期通过区分因果因素和相关标记来改善 AD 的早期诊断和干预。", "keywords": "阿尔茨海默病, 因果关系, 相关性, 机器学习, 可解释性", "comments": "这项研究通过整合机器学习和可解释性技术来探讨阿尔茨海默病中的因果关系与相关性，具有创新性。其重要性在于强调了在复杂疾病研究中仔细区分因果关系的重要性，为未来的因果推断研究提供了新的视角和方法论基础，有望改进 AD 的诊断和治疗策略。"}}
{"id": "2506.10845", "title": "Faster CONGEST Approximation Algorithms for Maximum Weighted Independent Set in Sparse Graphs", "authors": ["Salwa Faour", "Fabian Kuhn"], "summary": "The maximum independent set problem is a classic optimization problem that\nhas also been studied quite intensively in the distributed setting. While the\nproblem is hard to approximate in general, there are good approximation\nalgorithms known for several sparse graph families. In this paper, we consider\ndeterministic distributed CONGEST algorithms for the weighted version of the\nproblem in trees and graphs of bounded arboricity.\n  For trees, we prove that the task of deterministically computing a\n$(1-\\epsilon)$-approximate solution to the maximum weight independent set\n(MWIS) problem has a tight $\\Theta(\\log^*(n) / \\epsilon)$ complexity. The lower\nbound already holds on unweighted oriented paths. On the upper bound side, we\nshow that the bound can be achieved even in unrooted trees.\n  For graphs $G=(V,E)$ of arboricity $\\beta>1$, we give two algorithms. If the\nsum of all node weights is $w(V)$, we show that for any $\\epsilon>0$, an\nindependent set of weight at least $(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$ can\nbe computed in $O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$ rounds. This\nresult is obtained by a direct application of the local rounding framework of\nFaour, Ghaffari, Grunau, Kuhn, and Rozho\\v{n} [SODA '23]. We further show that\nfor any $\\epsilon>0$, an independent set of weight at least\n$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$ can be computed in\n$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$ rounds. This\nimproves on a recent result of Gil [OPODIS '23], who showed that a\n$1/\\lfloor(2+\\epsilon)\\beta\\rfloor$-approximation to the MWIS problem can be\ncomputed in $O(\\beta\\cdot\\log n)$ rounds. As an intermediate step, we design an\nalgorithm to compute an independent set of total weight at least\n$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$ in time\n$O(\\log^3(\\Delta)\\cdot\\log(1/\\epsilon)/\\epsilon + \\log^* n)$, where $\\Delta$ is\nthe maximum degree of the graph.", "comment": "23 pages", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.10845v1", "AI": {"title_translation": "稀疏图中最大加权独立集的更快CONGEST近似算法", "tldr": "该论文为树和有界非循环度图中的最大加权独立集（MWIS）问题提出了更快的确定性CONGEST近似算法，为树提供了紧密的复杂度，并为有界非循环度图改进了近似比和轮次复杂度。", "motivation": "最大独立集问题是一个经典的优化问题，在分布式环境中得到了广泛研究，但通常难以近似。本文旨在为稀疏图族（树和有界非循环度图）中的加权最大独立集问题提供改进的近似算法。", "method": "本文研究确定性分布式CONGEST算法。对于树，证明了$(1-\\epsilon)$-近似解的紧密复杂度。对于非循环度$\\beta>1$的图，提供了两种算法：一种通过直接应用局部舍入框架实现近似，另一种则改进了现有结果。此外，还设计了一个中间算法来计算基于节点权重和度的独立集。", "result": "对于树，确定性计算最大加权独立集（MWIS）问题的$(1-\\epsilon)$-近似解具有紧密的$\\Theta(\\log^*(n) / \\epsilon)$复杂度。对于非循环度$\\beta>1$的图，第一种算法能在$O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$的独立集；第二种算法能在$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$的独立集，这改进了现有结果。作为中间步骤，还设计了一种算法，用于在$O(\\log^3(\\Delta)\\cdot\\log(1/\\epsilon)/\\epsilon + \\log^* n)$时间内计算总权重至少为$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$的独立集。", "conclusion": "本文为树和有界非循环度图中的最大加权独立集问题提供了更快和改进的近似算法，建立了树的紧密复杂度下限，并为有界非循环度图提供了更好的近似比和轮次复杂度。", "translation": "最大独立集问题是一个经典的优化问题，在分布式环境中也得到了相当深入的研究。虽然该问题通常难以近似，但对于一些稀疏图族，已有一些好的近似算法。在本文中，我们考虑了树和有界非循环度图（arboricity）中该问题加权版本的确定性分布式CONGEST算法。\n对于树，我们证明了确定性计算最大加权独立集（MWIS）问题的$(1-\\epsilon)$-近似解具有紧密的$\\Theta(\\log^*(n) / \\epsilon)$复杂度。下界已经在无权有向路径上成立。在上界方面，我们表明即使在无根树中也能达到该界限。\n对于非循环度$\\beta>1$的图$G=(V,E)$，我们提供了两种算法。如果所有节点权重的总和为$w(V)$，我们表明对于任何$\\epsilon>0$，可以在$O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$的独立集。这个结果是通过直接应用Faour, Ghaffari, Grunau, Kuhn和Rozho\\v{n} [SODA '23]的局部舍入框架获得的。我们进一步表明，对于任何$\\epsilon>0$，可以在$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$的独立集。这改进了Gil [OPODIS '23]最近的结果，他表明MWIS问题的$1/\\lfloor(2+\\epsilon)\\beta\\rfloor$-近似可以在$O(\\beta\\cdot\\log n)$轮内计算。作为中间步骤，我们设计了一种算法，用于在$O(\\log^3(\\Delta)\\cdot\\log(1/\\epsilon)/\\epsilon + \\log^* n)$时间内计算总权重至少为$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$的独立集，其中$\\Delta$是图的最大度。", "summary": "本文提出了针对稀疏图中最大加权独立集（MWIS）问题的更快确定性分布式CONGEST算法。对于树，它建立了$(1-\\epsilon)$-近似的紧密$\\Theta(\\log^*(n) / \\epsilon)$复杂度。对于具有有界非循环度$\\beta$的图，论文提供了两种算法：一种利用局部舍入框架在$O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$轮内实现了$(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$近似；另一种则将近似比提高到$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$，轮次复杂度为$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$，优于现有工作。此外，还提出了一个基于节点权重和度的中间独立集算法。", "keywords": "最大加权独立集, 分布式算法, CONGEST模型, 稀疏图, 近似算法", "comments": "该论文在分布式最大加权独立集算法领域做出了重要贡献，特别是在稀疏图方面。其创新之处在于为树实现了紧密的复杂度界限，并为有界非循环度图提供了改进的近似比和轮次复杂度，超越了现有成果。局部舍入框架的应用和新算法的开发展示了CONGEST模型下强大的理论进步。"}}
{"id": "2506.10230", "title": "Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation", "authors": ["Emerson P. Grabke", "Masoom A. Haider", "Babak Taati"], "summary": "Latent diffusion models (LDM) could alleviate data scarcity challenges\naffecting machine learning development for medical imaging. However, medical\nLDM training typically relies on performance- or scientific\naccessibility-limiting strategies including a reliance on short-prompt text\nencoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with\nlarge data volumes. We propose a Class-Conditioned Efficient Large Language\nmodel Adapter (CCELLA) to address these limitations. CCELLA is a novel\ndual-head conditioning approach that simultaneously conditions the LDM U-Net\nwith non-medical large language model-encoded text features through\ncross-attention and with pathology classification through the timestep\nembedding. We also propose a joint loss function and a data-efficient LDM\ntraining framework. In combination, these strategies enable\npathology-conditioned LDM training for high-quality medical image synthesis\ngiven limited data volume and human data annotation, improving LDM performance\nand scientific accessibility. Our method achieves a 3D FID score of 0.025 on a\nsize-limited prostate MRI dataset, significantly outperforming a recent\nfoundation model with FID 0.071. When training a classifier for prostate cancer\nprediction, adding synthetic images generated by our method to the training\ndataset improves classifier accuracy from 69% to 74%. Training a classifier\nsolely on our method's synthetic images achieved comparable performance to\ntraining on real images alone.", "comment": "MAH and BT are co-senior authors on the work. This work has been\n  submitted to the IEEE for possible publication", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10230v1", "AI": {"title_translation": "基于提示引导的潜在扩散模型与预测性类别条件用于3D前列腺MRI生成", "tldr": "本文提出了CCELLA，一种新颖的双头条件化方法，用于潜在扩散模型，以在有限数据下生成高质量3D前列腺MRI图像，优于现有方法并提高了分类器性能。", "motivation": "为了缓解影响医学影像机器学习发展的数据稀缺挑战，并解决现有医学潜在扩散模型（LDM）训练的局限性，如依赖短提示文本编码器、重用非医学LDM或需要大量数据进行微调。", "method": "提出了Class-Conditioned Efficient Large Language model Adapter (CCELLA)，这是一种新颖的双头条件化方法，通过交叉注意力将非医学大型语言模型编码的文本特征与LDM U-Net进行条件化，并通过时间步嵌入进行病理分类。同时提出了联合损失函数和数据高效的LDM训练框架。", "result": "在规模有限的前列腺MRI数据集上实现了0.025的3D FID分数，显著优于最近的基础模型（FID为0.071）。将本文方法生成的合成图像添加到前列腺癌预测分类器训练数据集中，可将分类器准确率从69%提高到74%。仅使用本文方法合成图像训练的分类器性能与仅使用真实图像训练的分类器性能相当。", "conclusion": "本文提出的策略使得病理条件化的LDM训练能够在有限数据量下进行高质量医学图像合成，从而提高LDM性能和科学可及性。", "translation": "潜在扩散模型（LDM）可以缓解影响医学影像机器学习发展的数据稀缺挑战。然而，医学LDM训练通常依赖于限制性能或科学可及性的策略，包括依赖短提示文本编码器、重用非医学LDM或要求大量数据进行微调。我们提出了一种类别条件高效大型语言模型适配器（CCELLA）来解决这些局限性。CCELLA是一种新颖的双头条件化方法，它通过交叉注意力和时间步嵌入中的病理分类，同时用非医学大型语言模型编码的文本特征对LDM U-Net进行条件化。我们还提出了一种联合损失函数和一种数据高效的LDM训练框架。结合这些策略，可以在有限数据量和人工数据标注的情况下，实现病理条件化的LDM训练，用于高质量医学图像合成，从而提高LDM性能和科学可及性。我们的方法在规模有限的前列腺MRI数据集上实现了0.025的3D FID分数，显著优于最近的基础模型（FID为0.071）。当训练前列腺癌预测分类器时，将我们方法生成的合成图像添加到训练数据集中，可将分类器准确率从69%提高到74%。仅使用我们方法合成图像训练的分类器性能与仅使用真实图像训练的分类器性能相当。", "summary": "本文提出了一种名为CCELLA（Class-Conditioned Efficient Large Language model Adapter）的新型双头条件化方法，旨在解决医学影像领域数据稀缺和现有潜在扩散模型（LDM）训练的局限性。CCELLA通过将非医学大型语言模型编码的文本特征与病理分类相结合，同时引入联合损失函数和数据高效的训练框架，实现了在有限数据下生成高质量3D前列腺MRI图像。实验结果表明，该方法在3D FID分数上显著优于现有模型，并且通过添加合成图像，能有效提高前列腺癌预测分类器的准确性，证明了其在医学图像合成和机器学习发展中的潜力。", "keywords": "潜在扩散模型, 医学影像, 前列腺MRI, 数据稀缺, 图像合成", "comments": "该论文的创新点在于其提出的CCELLA双头条件化方法，巧妙地结合了文本和类别条件化，以及数据高效的训练框架，这对于数据稀缺的医学影像领域至关重要。该方法在有限数据下生成高质量图像并提升下游任务性能的能力，具有重要的科学和临床意义。"}}
{"id": "2506.10036", "title": "Token Perturbation Guidance for Diffusion Models", "authors": ["Javad Rajabi", "Soroush Mehraban", "Seyedmorteza Sadat", "Babak Taati"], "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2$\\times$ improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance", "comment": "18 pages, 14 figures", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10036v1", "AI": {"title_translation": "扩散模型的令牌扰动引导", "tldr": "本文提出了一种名为TPG的新型训练无关且条件无关的引导方法，通过直接扰动中间令牌表示来提高扩散模型的生成质量，并能应用于条件和无条件生成。", "motivation": "现有分类器自由引导（CFG）虽然能提升生成质量和对齐，但需要特定的训练过程且仅限于条件生成。为了解决这些限制，本文提出了TPG。", "method": "本文提出了令牌扰动引导（TPG），一种将扰动矩阵直接应用于扩散网络中中间令牌表示的新方法。TPG采用范数保持洗牌操作，提供有效且稳定的引导信号，无需架构更改即可提高生成质量。因此，TPG是训练无关的，并且与输入条件无关，使其适用于条件和无条件生成。", "result": "实验结果表明，TPG在无条件生成方面，SDXL上的FID比基线提高了近2倍，同时在提示对齐方面与CFG非常接近。", "conclusion": "TPG被确立为一种通用的、条件无关的引导方法，能够为更广泛的扩散模型带来类似CFG的益处。", "translation": "分类器自由引导（CFG）已成为现代扩散模型中增强生成质量和与输入条件对齐的重要组成部分。然而，CFG需要特定的训练过程，并且仅限于条件生成。为了解决这些限制，我们提出了令牌扰动引导（TPG），一种将扰动矩阵直接应用于扩散网络中中间令牌表示的新方法。TPG采用范数保持洗牌操作，提供有效且稳定的引导信号，无需架构更改即可提高生成质量。因此，TPG是训练无关的，并且与输入条件无关，使其适用于条件和无条件生成。我们进一步分析了TPG提供的引导项，并表明其对采样的影响与现有训练无关的引导技术相比更接近CFG。在SDXL和Stable Diffusion 2.1上的大量实验表明，TPG在无条件生成方面，SDXL上的FID比基线提高了近2倍，同时在提示对齐方面与CFG非常接近。这些结果确立了TPG作为一种通用的、条件无关的引导方法，能够为更广泛的扩散模型带来类似CFG的益处。代码可在https://github.com/TaatiTeam/Token-Perturbation-Guidance获取。", "summary": "本文提出了一种名为令牌扰动引导（TPG）的新型方法，旨在克服传统分类器自由引导（CFG）在训练要求和条件限制上的不足。TPG通过在扩散模型内部直接对中间令牌表示应用范数保持的扰动矩阵来提供引导信号，从而在不改变模型架构的情况下提高生成质量。这种方法是训练无关且与输入条件无关的，使其能够广泛应用于条件和无条件生成。实验证明，TPG在无条件生成上显著提升了性能，并在提示对齐上与CFG表现相当，表明其能够为多种扩散模型带来类似CFG的优势。", "keywords": "令牌扰动引导, 扩散模型, 分类器自由引导, 无条件生成, 训练无关", "comments": "该论文提出了一种创新的训练无关且条件无关的引导方法TPG，通过直接操作中间令牌表示来提升扩散模型的性能。其核心创新在于“令牌扰动”和“范数保持洗牌操作”，这使得它无需重新训练模型即可应用，极大地拓展了引导方法的适用范围。TPG能够应用于无条件生成，填补了CFG在此方面的空白，并且在性能上接近甚至超越了现有技术，这对于扩散模型的实际应用具有重要意义。该方法的通用性和易用性是其主要优势。"}}
{"id": "2506.10044", "title": "Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks", "authors": ["Uijun Jung", "Deokho Jang", "Sungchul Kim", "Jungho Kim"], "summary": "Optical properties of thin film are greatly influenced by the thickness of\neach layer. Accurately predicting these thicknesses and their corresponding\noptical properties is important in the optical inverse design of thin films.\nHowever, traditional inverse design methods usually demand extensive numerical\nsimulations and optimization procedures, which are time-consuming. In this\npaper, we utilize deep learning for the inverse design of the transmission\nspectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural\nnetwork (TNN), which can solve the one-to-many mapping problem that greatly\ndegrades the performance of deep-learning-based inverse designs. In general,\nthe TNN has been implemented by a back-to-back connection of an inverse neural\nnetwork and a pre-trained forward neural network, both of which have been\nimplemented based on multilayer perceptron (MLP) algorithms. In this paper, we\npropose to use not only MLP, but also convolutional neural network (CNN) or\nlong short-term memory (LSTM) algorithms in the configuration of the TNN. We\nshow that an LSTM-LSTM-based TNN yields the highest accuracy but takes the\nlongest training time among nine configurations of TNNs. We also find that a\nCNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed\nbecause it could integrate the strengths of the CNN and LSTM algorithms.", "comment": "22 pages, 8 figures, 2 tables, 11 supplementary figures, 7\n  supplementary tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10044v1", "AI": {"title_translation": "使用CNN-LSTM串联神经网络改进多层薄膜光学逆向设计性能", "tldr": "本文提出并评估了基于CNN-LSTM的串联神经网络（TNN），用于多层薄膜的光学逆向设计，解决了传统方法的耗时问题和深度学习的一对多映射问题，并发现CNN-LSTM组合在精度和速度上表现最佳。", "motivation": "传统的光学逆向设计方法需要大量的数值模拟和优化过程，非常耗时。此外，基于深度学习的逆向设计存在一对多映射问题，这会极大地降低其性能。", "method": "本文采用串联神经网络（TNN）进行SiO2/TiO2多层薄膜的透射光谱逆向设计。TNN由逆向神经网络和预训练的前向神经网络组成。与以往基于多层感知机（MLP）的实现不同，本文提出在TNN配置中使用卷积神经网络（CNN）或长短期记忆网络（LSTM）算法，并测试了多种配置（包括MLP、CNN、LSTM的组合）。", "result": "在九种TNN配置中，基于LSTM-LSTM的TNN实现了最高的精度，但训练时间最长。研究发现，基于CNN-LSTM的TNN在精度和速度方面是最佳解决方案，因为它结合了CNN和LSTM算法的优势。", "conclusion": "CNN-LSTM串联神经网络是多层薄膜光学逆向设计的有效且高效的解决方案，能够解决传统方法的局限性并优化深度学习模型的性能。", "translation": "薄膜的光学特性受各层厚度影响很大。准确预测这些厚度及其相应的光学特性在薄膜的光学逆向设计中至关重要。然而，传统的逆向设计方法通常需要大量的数值模拟和优化过程，这非常耗时。在本文中，我们利用深度学习对SiO2/TiO2多层薄膜的透射光谱进行逆向设计。我们实现了一种串联神经网络（TNN），它可以解决极大地降低基于深度学习的逆向设计性能的一对多映射问题。通常，TNN是通过逆向神经网络和预训练的前向神经网络的反向连接实现的，两者都是基于多层感知机（MLP）算法实现的。在本文中，我们提出不仅在TNN的配置中使用MLP，还在其配置中使用卷积神经网络（CNN）或长短期记忆网络（LSTM）算法。我们表明，在九种TNN配置中，基于LSTM-LSTM的TNN产生了最高的精度，但训练时间最长。我们还发现，基于CNN-LSTM的TNN将是精度和速度方面的最佳解决方案，因为它能够整合CNN和LSTM算法的优势。", "summary": "本文提出了一种基于深度学习的串联神经网络（TNN），用于解决多层薄膜光学逆向设计中传统方法耗时和深度学习一对多映射的问题。研究探索了在TNN配置中结合多层感知机（MLP）、卷积神经网络（CNN）和长短期记忆网络（LSTM）算法。结果表明，虽然LSTM-LSTM配置的TNN精度最高，但训练时间最长；而CNN-LSTM配置的TNN在精度和速度之间取得了最佳平衡，是光学逆向设计的理想选择。", "keywords": "光学逆向设计, 多层薄膜, 串联神经网络, CNN-LSTM, 深度学习", "comments": "该论文的创新点在于将CNN和LSTM这两种强大的深度学习架构引入串联神经网络（TNN）中，以解决光学逆向设计中的复杂一对多映射问题。通过集成不同网络类型的优势，它有效地提升了设计效率和准确性，为薄膜设计领域提供了一种更优的自动化方法。"}}
{"id": "2506.10346", "title": "An Analysis of Datasets, Metrics and Models in Keyphrase Generation", "authors": ["Florian Boudin", "Akiko Aizawa"], "summary": "Keyphrase generation refers to the task of producing a set of words or\nphrases that summarises the content of a document. Continuous efforts have been\ndedicated to this task over the past few years, spreading across multiple lines\nof research, such as model architectures, data resources, and use-case\nscenarios. Yet, the current state of keyphrase generation remains unknown as\nthere has been no attempt to review and analyse previous work. In this paper,\nwe bridge this gap by presenting an analysis of over 50 research papers on\nkeyphrase generation, offering a comprehensive overview of recent progress,\nlimitations, and open challenges. Our findings highlight several critical\nissues in current evaluation practices, such as the concerning similarity among\ncommonly-used benchmark datasets and inconsistencies in metric calculations\nleading to overestimated performances. Additionally, we address the limited\navailability of pre-trained models by releasing a strong PLM-based model for\nkeyphrase generation as an effort to facilitate future research.", "comment": "GEM^2 paper @ ACL 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10346v1", "AI": {"title_translation": "关键词生成中数据集、度量和模型的分析", "tldr": "本文分析了关键词生成领域的50多篇研究论文，揭示了当前评估实践中数据集相似性和度量计算不一致等关键问题，并发布了一个基于PLM的强大模型以促进未来研究。", "motivation": "尽管关键词生成任务持续发展，但由于缺乏对现有工作的回顾和分析，该领域的现状仍不明确。本文旨在通过分析弥补这一空白。", "method": "本文通过分析超过50篇关于关键词生成的研究论文，全面概述了该领域的最新进展、局限性和开放挑战。", "result": "研究发现当前评估实践存在几个关键问题，例如常用基准数据集之间存在令人担忧的相似性，以及度量计算不一致导致性能被高估。此外，为解决预训练模型可用性有限的问题，本文发布了一个强大的基于PLM的关键词生成模型。", "conclusion": "本文通过对关键词生成领域现有工作的全面分析，揭示了当前评估实践中的重要问题，并发布了一个新的预训练模型，旨在促进未来的研究。", "translation": "关键词生成是指生成一组总结文档内容的词语或短语的任务。在过去几年中，人们为这项任务付出了持续的努力，研究范围涉及模型架构、数据资源和用例场景等多个方面。然而，由于尚未有对先前工作的回顾和分析，关键词生成的当前状态仍然未知。在本文中，我们通过对50多篇关键词生成研究论文的分析来弥合这一空白，提供了对最新进展、局限性和开放挑战的全面概述。我们的发现突出了当前评估实践中的几个关键问题，例如常用基准数据集之间令人担忧的相似性以及度量计算中的不一致导致性能被高估。此外，我们通过发布一个强大的基于PLM的关键词生成模型来解决预训练模型可用性有限的问题，以促进未来的研究。", "summary": "本文对关键词生成领域50多篇研究论文进行了全面分析，系统地回顾了该领域的进展、局限性和挑战。研究发现，当前评估实践中存在数据集高度相似和度量计算不一致导致性能虚高的问题。为解决预训练模型稀缺的现状，作者还发布了一个强大的基于PLM的关键词生成模型，以期推动后续研究。", "keywords": "关键词生成, 数据集, 度量, 模型, 分析", "comments": "本文通过对关键词生成领域现有文献的系统性回顾和批判性分析，揭示了当前评估实践中的深层问题，如数据集同质化和评估指标的误导性。其创新之处在于不仅指出了问题，还通过发布新的PLM-based模型为未来的研究提供了实际的工具和方向，对于规范该领域的评估标准和促进高质量研究具有重要意义。"}}
{"id": "2506.10077", "title": "A quantum semantic framework for natural language processing", "authors": ["Christopher J. Agostino", "Quan Le Thien", "Molly Apsel", "Denizhan Pak", "Elina Lesyk", "Ashabari Majumdar"], "summary": "Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. Large Language Models (LLMs) and other modern NLP systems face\ninherent limitations precisely because they operate within natural language\nitself, making them subject to the same interpretive constraints imposed by\nsemantic degeneracy. In this work, we argue using Kolmogorov complexity that as\nan expression's complexity grows, the likelihood of any interpreting agent\n(human or LLM-powered AI) recovering the single intended meaning vanishes. This\ncomputational intractability suggests the classical view that linguistic forms\npossess meaning in and of themselves is flawed. We alternatively posit that\nmeaning is instead actualized through an observer-dependent interpretive act.\nTo test this, we conducted a semantic Bell inequality test using diverse LLM\nagents as ``computational cognitive systems'' to interpret ambiguous word pairs\nunder varied contextual settings. Across several independent experiments, we\nfound average CHSH expectation values ranging from 1.2 to 2.8, with several\nruns yielding values (e.g., 2.3-2.4) that significantly violate the classical\nboundary ($|S|\\leq2$). This demonstrates that linguistic interpretation under\nambiguity can exhibit non-classical contextuality, consistent with results from\nhuman cognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.", "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10077v1", "AI": {"title_translation": "自然语言处理的量子语义框架", "tldr": "本文提出自然语言的语义简并性导致解释的组合爆炸，并认为语言意义通过观察者依赖的解释行为实现。通过对LLM进行语义贝尔不等式测试，发现语言解释表现出非经典语境性，这表明经典频率学方法存在缺陷，并提议采用贝叶斯重复采样方法。", "motivation": "由于语义简并性，自然语言的复杂性增加时，任何解释代理（人类或LLM）恢复单一预期意义的可能性会消失，导致计算上的难处理性。这挑战了语言形式本身具有意义的经典观点。", "method": "本文利用柯尔莫哥洛夫复杂度论证了语义简并性。为了验证观察者依赖的解释行为，研究人员使用不同的LLM代理作为“计算认知系统”，在不同语境下解释模糊的词对，并进行了语义贝尔不等式测试。", "result": "在多项独立实验中，平均CHSH期望值在1.2到2.8之间，其中一些运行结果（例如2.3-2.4）显著违反了经典边界（|S|≤2）。这表明在歧义下的语言解释可以表现出非经典语境性，与人类认知实验结果一致。", "conclusion": "语言解释在歧义下表现出非经典语境性，这暗示经典的基于频率学的自然语言分析方法必然是耗损的。相反，贝叶斯式的重复采样方法可以为语境中的语言意义提供更实用和恰当的表征。", "translation": "语义简并性代表了自然语言的一个基本属性，它超越了简单的多义性，涵盖了随着语义表达复杂性增加而出现的潜在解释的组合爆炸。大型语言模型（LLM）和其他现代自然语言处理系统面临固有的局限性，正是因为它们在自然语言本身中运行，使其受到语义简并性所施加的相同解释约束。在这项工作中，我们使用柯尔莫哥洛夫复杂度论证，随着表达式复杂性的增长，任何解释代理（人类或由LLM驱动的AI）恢复单一预期意义的可能性会消失。这种计算上的难处理性表明，语言形式本身具有意义的经典观点是有缺陷的。我们转而提出，意义是通过一种依赖于观察者的解释行为来实现的。为了验证这一点，我们使用不同的LLM代理作为“计算认知系统”，在不同语境下解释模糊的词对，进行了语义贝尔不等式测试。在几项独立的实验中，我们发现平均CHSH期望值范围从1.2到2.8，其中一些运行结果（例如2.3-2.4）显著违反了经典边界（|S|≤2）。这表明在歧义下的语言解释可以表现出非经典语境性，与人类认知实验结果一致。这些结果固有地暗示，经典的基于频率学的自然语言分析方法必然是耗损的。相反，我们提出贝叶斯式的重复采样方法可以在语境中提供更实用和恰当的语言意义表征。", "summary": "本研究探讨了自然语言中语义简并性带来的解释复杂性问题，认为这限制了LLM等现有NLP系统的性能。作者运用柯尔莫哥洛夫复杂度理论指出，随着语言复杂性增加，单一预期意义的恢复变得不可行，并提出意义是观察者依赖的解释行为。通过对LLM进行语义贝尔不等式测试，实验结果显示语言解释在歧义下表现出非经典语境性，这挑战了经典频率学方法，并建议采用贝叶斯重复采样方法来更好地表征语言意义。", "keywords": "量子语义, 自然语言处理, 语义简并性, 贝尔不等式, LLM", "comments": "这篇论文提出了一个关于自然语言处理中语义理解的创新视角，将量子力学的概念引入到语言解释中，挑战了传统的语言意义观。通过将LLM作为“计算认知系统”进行贝尔不等式测试，其方法新颖且具有启发性。研究结果暗示了经典NLP方法的局限性，并为未来的NLP研究指明了新的方向，即探索非经典、语境依赖的语义表征。"}}
{"id": "2506.10084", "title": "DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding", "authors": ["Bin Guo", "John H. L. Hansen"], "summary": "Conventional vision backbones, despite their success, often construct\nfeatures through a largely uniform cascade of operations, offering limited\nexplicit pathways for adaptive, iterative refinement. This raises a compelling\nquestion: can principles from classical search algorithms instill a more\nalgorithmic, structured, and logical processing flow within these networks,\nleading to representations built through more interpretable, perhaps\nreasoning-like decision processes? We introduce DeepTraverse, a novel vision\narchitecture directly inspired by algorithmic search strategies, enabling it to\nlearn features through a process of systematic elucidation and adaptive\nrefinement distinct from conventional approaches. DeepTraverse operationalizes\nthis via two key synergistic components: recursive exploration modules that\nmethodically deepen feature analysis along promising representational paths\nwith parameter sharing for efficiency, and adaptive calibration modules that\ndynamically adjust feature salience based on evolving global context. The\nresulting algorithmic interplay allows DeepTraverse to intelligently construct\nand refine feature patterns. Comprehensive evaluations across a diverse suite\nof image classification benchmarks show that DeepTraverse achieves highly\ncompetitive classification accuracy and robust feature discrimination, often\noutperforming conventional models with similar or larger parameter counts. Our\nwork demonstrates that integrating such algorithmic priors provides a\nprincipled and effective strategy for building more efficient, performant, and\nstructured vision backbones.", "comment": "NeurIPS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10084v1", "AI": {"title_translation": "DeepTraverse：一种受深度优先搜索启发的算法视觉理解网络", "tldr": "DeepTraverse是一种受深度优先搜索启发的视觉网络，通过递归探索和自适应校准模块，学习更具算法性和结构化的特征，在图像分类任务上表现出色。", "motivation": "传统的视觉骨干网络在特征构建过程中缺乏明确的自适应、迭代细化路径。本文旨在探讨能否将经典搜索算法的原理融入神经网络，以实现更具算法性、结构化和逻辑性的处理流程，从而构建更可解释的表示。", "method": "本文引入了DeepTraverse，这是一种受算法搜索策略直接启发的新型视觉架构。它通过两个关键协同组件实现：递归探索模块（通过参数共享系统地深化特征分析）和自适应校准模块（根据全局上下文动态调整特征显著性）。这种算法间的相互作用使得DeepTraverse能够智能地构建和细化特征模式。", "result": "在多种图像分类基准测试中，DeepTraverse取得了极具竞争力的分类准确性和鲁棒的特征判别能力，通常优于参数数量相似或更大的传统模型。", "conclusion": "将算法先验知识整合到视觉骨干网络中，是构建更高效、高性能和结构化视觉骨干的一种有原则且有效的策略。", "translation": "尽管传统的视觉骨干网络取得了成功，但它们通常通过一种 largely 统一的操作级联来构建特征，为自适应、迭代细化提供了有限的显式路径。这提出了一个引人注目的问题：经典搜索算法的原理能否在这些网络中注入更具算法性、结构化和逻辑性的处理流程，从而通过更可解释、或许类似推理的决策过程来构建表示？我们引入了 DeepTraverse，一种受算法搜索策略直接启发的新型视觉架构，使其能够通过与传统方法不同的系统阐明和自适应细化过程来学习特征。DeepTraverse 通过两个关键协同组件来实现这一点：递归探索模块，通过参数共享沿着有前景的表示路径系统地深化特征分析，以及自适应校准模块，根据不断演变的全局上下文动态调整特征显著性。由此产生的算法相互作用使 DeepTraverse 能够智能地构建和细化特征模式。对各种图像分类基准的全面评估表明，DeepTraverse 实现了极具竞争力的分类准确性和鲁棒的特征判别能力，通常优于参数数量相似或更大的传统模型。我们的工作表明，整合此类算法先验知识为构建更高效、高性能和结构化的视觉骨干网络提供了一种有原则且有效的策略。", "summary": "本文提出了DeepTraverse，一种受深度优先搜索启发的视觉网络架构，旨在解决传统视觉骨干网络在特征学习中缺乏自适应迭代细化的问题。DeepTraverse通过递归探索模块进行深度特征分析和自适应校准模块动态调整特征显著性，实现了更具算法性和结构化的特征构建。实验证明，DeepTraverse在图像分类任务上取得了与传统模型相当或更优的性能，验证了将算法先验整合到视觉网络中的有效性。", "keywords": "深度学习, 视觉骨干网络, 深度优先搜索, 图像分类, 算法先验", "comments": "DeepTraverse的创新之处在于将经典搜索算法（特别是深度优先搜索）的原理融入到深度学习视觉骨干网络的设计中，打破了传统网络统一级联操作的局限性。这种方法使得网络能够进行更具解释性和推理性的特征学习。其采用的递归探索和自适应校准模块是实现这一目标的关键。该研究为构建更高效、高性能且结构化的视觉模型提供了一个新颖的视角和有效策略，对于提升模型的可解释性和鲁棒性具有重要意义。"}}
{"id": "2506.10225", "title": "Fine-Grained control over Music Generation with Activation Steering", "authors": ["Dipanshu Panda", "Jayden Koshy Joe", "Harshith M R", "Swathi Narashiman", "Pranay Mathur", "Anish Veerakumar", "Aniruddh Krishna", "Keerthiharan A"], "summary": "We present a method for fine-grained control over music generation through\ninference-time interventions on an autoregressive generative music transformer\ncalled MusicGen. Our approach enables timbre transfer, style transfer, and\ngenre fusion by steering the residual stream using weights of linear probes\ntrained on it, or by steering the attention layer activations in a similar\nmanner. We observe that modelling this as a regression task provides improved\nperformance, hypothesizing that the mean-squared-error better preserve\nmeaningful directional information in the activation space. Combined with the\nglobal conditioning offered by text prompts in MusicGen, our method provides\nboth global and local control over music generation. Audio samples illustrating\nour method are available at our demo page.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10225v1", "AI": {"title_translation": "音乐生成中的激活转向细粒度控制", "tldr": "通过在自回归音乐生成模型MusicGen中进行推理时干预，实现对音乐生成细粒度控制的方法，包括音色、风格和流派融合。", "motivation": "现有方法可能缺乏细粒度控制，或者研究者希望在MusicGen模型中实现更精细的控制，以实现音色迁移、风格迁移和流派融合。", "method": "在MusicGen模型中，通过线性探针训练的权重引导残差流，或类似地引导注意力层激活，来实现推理时干预。将此建模为回归任务，使用均方误差(MSE)来更好地保留激活空间中的方向信息，从而提高性能。结合MusicGen文本提示提供的全局条件，实现全局和局部控制。", "result": "该方法实现了音色迁移、风格迁移和流派融合。将任务建模为回归任务提高了性能。", "conclusion": "该方法结合MusicGen的全局条件，提供了对音乐生成的全局和局部细粒度控制。", "translation": "我们提出了一种通过对自回归生成音乐Transformer模型MusicGen进行推理时干预来对音乐生成进行细粒度控制的方法。我们的方法通过使用在其上训练的线性探针的权重引导残差流，或以类似方式引导注意力层激活，从而实现音色迁移、风格迁移和流派融合。我们观察到将其建模为回归任务可以提高性能，推测均方误差能更好地保留激活空间中有意义的方向信息。结合MusicGen中文本提示提供的全局条件，我们的方法提供了对音乐生成的全局和局部控制。说明我们方法的音频样本可在我们的演示页面上获取。", "summary": "本文提出了一种在MusicGen自回归音乐生成模型中实现细粒度控制的新方法，即通过推理时干预，引导残差流或注意力层激活。该方法通过将控制建模为回归任务并利用均方误差，有效地实现了音色迁移、风格迁移和流派融合，并结合文本提示实现了音乐生成的全局和局部控制。", "keywords": "音乐生成, 细粒度控制, 激活转向, MusicGen, 推理时干预", "comments": "该论文的创新点在于提出了通过“激活转向”这种推理时干预的方式，在预训练的生成模型上实现细粒度控制，而无需重新训练整个模型。将控制任务建模为回归问题并利用MSE是一种新颖且有效的方法，有助于在激活空间中保持信息。这种方法为音乐生成提供了强大的局部和全局控制能力，具有重要的应用潜力。"}}
{"id": "2506.10118", "title": "Data-driven balanced truncation for second-order systems with generalized proportional damping", "authors": ["Sean Reiter", "Steffen W. R. Werner"], "summary": "Structured reduced-order modeling is a central component in the\ncomputer-aided design of control systems in which cheap-to-evaluate\nlow-dimensional models with physically meaningful internal structures are\ncomputed. In this work, we develop a new approach for the structured\ndata-driven surrogate modeling of linear dynamical systems described by\nsecond-order time derivatives via balanced truncation model-order reduction.\nThe proposed method is a data-driven reformulation of position-velocity\nbalanced truncation for second-order systems and generalizes the\nquadrature-based balanced truncation for unstructured first-order systems to\nthe second-order case. The computed surrogates encode a generalized\nproportional damping structure, and the damping coefficients are inferred\nsolely from data by minimizing a least-squares error over the coefficients.\nSeveral numerical examples demonstrate the effectiveness of the proposed\nmethod.", "comment": "31 pages, 5 figures, 5 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10118v1", "AI": {"title_translation": "具有广义比例阻尼的二阶系统的数驱动平衡截断", "tldr": "该研究提出了一种新的数据驱动的平衡截断方法，用于对具有广义比例阻尼的二阶系统进行降阶建模。", "motivation": "在计算机辅助控制系统设计中，需要计算具有物理意义内部结构且易于评估的低维模型，而结构化降阶建模是其核心组成部分。", "method": "该方法通过平衡截断模型降阶，为二阶时间导数描述的线性动力系统开发了一种新的结构化数据驱动替代建模方法。它是二阶系统位置-速度平衡截断的数据驱动重新表述，并将非结构化一阶系统的基于正交的平衡截断推广到二阶情况。计算出的替代模型编码了广义比例阻尼结构，阻尼系数仅通过最小化系数的最小二乘误差从数据中推断出来。", "result": "通过几个数值例子证明了所提出方法的有效性。", "conclusion": "所提出的数据驱动平衡截断方法对于具有广义比例阻尼的二阶系统是有效的。", "translation": "结构化降阶建模是计算机辅助控制系统设计中的核心组成部分，其中需要计算具有物理意义内部结构且易于评估的低维模型。在这项工作中，我们通过平衡截断模型降阶，为由二阶时间导数描述的线性动力系统开发了一种新的结构化数据驱动替代建模方法。所提出的方法是二阶系统位置-速度平衡截断的数据驱动重新表述，并将非结构化一阶系统的基于正交的平衡截断推广到二阶情况。计算出的替代模型编码了广义比例阻尼结构，阻尼系数仅通过最小化系数的最小二乘误差从数据中推断出来。几个数值例子证明了所提出方法的有效性。", "summary": "本文提出了一种用于具有广义比例阻尼的二阶系统的新型数据驱动平衡截断方法。该方法是位置-速度平衡截断的数据驱动重构，并将其推广到二阶系统，能够从数据中推断出阻尼系数。数值示例验证了其有效性。", "keywords": "数据驱动, 平衡截断, 二阶系统, 广义比例阻尼, 降阶建模", "comments": "该论文提出了一种创新的数据驱动方法，将平衡截断模型降阶技术应用于具有广义比例阻尼的二阶系统，填补了现有方法在处理此类系统时的空白。其通过数据驱动推断阻尼系数的机制，提升了模型构建的灵活性和自动化程度，对于计算机辅助控制系统设计具有重要意义。然而，抽象中未提及该方法在面对大规模复杂系统或噪声数据时的具体性能和局限性。"}}
{"id": "2506.10030", "title": "Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment", "authors": ["Tianyu Chen", "Jian Lou", "Wenjie Wang"], "summary": "As Retrieval-Augmented Generation (RAG) evolves into service-oriented\nplatforms (Rag-as-a-Service) with shared knowledge bases, protecting the\ncopyright of contributed data becomes essential. Existing watermarking methods\nin RAG focus solely on textual knowledge, leaving image knowledge unprotected.\nIn this work, we propose AQUA, the first watermark framework for image\nknowledge protection in Multimodal RAG systems. AQUA embeds semantic signals\ninto synthetic images using two complementary methods: acronym-based triggers\nand spatial relationship cues. These techniques ensure watermark signals\nsurvive indirect watermark propagation from image retriever to textual\ngenerator, being efficient, effective and imperceptible. Experiments across\ndiverse models and datasets show that AQUA enables robust, stealthy, and\nreliable copyright tracing, filling a key gap in multimodal RAG protection.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10030v1", "AI": {"title_translation": "RAG即服务环境中多模态知识版权保护", "tldr": "AQUA是首个针对多模态RAG系统中图像知识版权保护的水印框架，通过嵌入语义信号到合成图像中，实现高效、有效且不可感知的版权追踪。", "motivation": "随着检索增强生成（RAG）发展为共享知识库的服务导向平台（RAG即服务），保护贡献数据的版权变得至关重要。现有RAG中的水印方法仅专注于文本知识，图像知识仍未受保护。", "method": "本文提出了AQUA，这是首个用于多模态RAG系统中图像知识保护的水印框架。AQUA通过两种互补方法将语义信号嵌入到合成图像中：基于首字母缩略词的触发器和空间关系线索。这些技术确保水印信号能从图像检索器间接传播到文本生成器，且高效、有效且不可感知。", "result": "在不同的模型和数据集上的实验表明，AQUA能够实现鲁棒、隐蔽和可靠的版权追踪。", "conclusion": "AQUA填补了多模态RAG保护中图像知识版权保护的关键空白，是首个针对此问题的水印框架。", "translation": "随着检索增强生成（RAG）发展为共享知识库的服务导向平台（RAG即服务），保护贡献数据的版权变得至关重要。现有RAG中的水印方法仅专注于文本知识，图像知识仍未受保护。本文提出了AQUA，这是首个用于多模态RAG系统中图像知识保护的水印框架。AQUA通过两种互补方法将语义信号嵌入到合成图像中：基于首字母缩略词的触发器和空间关系线索。这些技术确保水印信号能从图像检索器间接传播到文本生成器，且高效、有效且不可感知。在不同的模型和数据集上的实验表明，AQUA能够实现鲁棒、隐蔽和可靠的版权追踪，填补了多模态RAG保护中的一个关键空白。", "summary": "本文提出了AQUA，一个开创性的水印框架，旨在解决RAG即服务环境中多模态知识（特别是图像知识）的版权保护问题。针对现有水印方法仅限于文本的局限性，AQUA通过在合成图像中嵌入基于首字母缩略词的触发器和空间关系线索等语义信号，实现了对图像知识的版权追踪。实验证明，AQUA在多样化的模型和数据集上表现出鲁棒、隐蔽且可靠的性能，有效填补了多模态RAG版权保护的空白。", "keywords": "多模态RAG, 版权保护, 水印, 图像知识, AQUA", "comments": "该论文的创新点在于首次提出了针对多模态RAG系统中图像知识版权保护的水印框架AQUA，解决了现有水印技术仅限于文本的局限性。其提出的基于语义信号的嵌入方法，尤其是在图像检索到文本生成器这种间接传播链条中的水印生存能力，展现了技术深度和实用价值。这对于RAG-as-a-Service环境下知识产权保护具有重要意义。"}}
{"id": "2506.10326", "title": "A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon", "authors": ["Cameron Angliss", "Jiaxun Cui", "Jiaheng Hu", "Arrasy Rahman", "Peter Stone"], "summary": "Developing AI agents that can robustly adapt to dramatically different\nstrategic landscapes without retraining is a central challenge for multi-agent\nlearning. Pok\\'emon Video Game Championships (VGC) is a domain with an\nextraordinarily large space of possible team configurations of approximately\n$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,\ncombinatorial nature of team building in Pok\\'emon VGC causes optimal\nstrategies to shift dramatically depending on both the team being piloted and\nthe opponent's team, making generalization uniquely challenging. To advance\nresearch on this problem, we introduce VGC-Bench: a benchmark that provides\ncritical infrastructure, standardizes evaluation protocols, and supplies\nhuman-play datasets and a range of baselines - from large-language-model agents\nand behavior cloning to reinforcement learning and empirical game-theoretic\nmethods such as self-play, fictitious play, and double oracle. In the\nrestricted setting where an agent is trained and evaluated on a single-team\nconfiguration, our methods are able to win against a professional VGC\ncompetitor. We extensively evaluated all baseline methods over progressively\nlarger team sets and find that even the best-performing algorithm in the\nsingle-team setting struggles at scaling up as team size grows. Thus, policy\ngeneralization across diverse team strategies remains an open challenge for the\ncommunity. Our code is open sourced at\nhttps://github.com/cameronangliss/VGC-Bench.", "comment": "15 pages, 3 figures, 10 tables, submitted to NeurIPS 2025 Datasets &\n  Benchmarks Track", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10326v1", "AI": {"title_translation": "在竞技宝可梦中泛化不同团队策略的基准", "tldr": "引入了一个名为 VGC-Bench 的宝可梦对战基准，以解决多智能体学习中跨多样团队策略泛化的挑战，并发现即使在受限设置中表现良好的方法也难以扩展。", "motivation": "开发能够鲁棒适应不同策略环境而无需重新训练的AI智能体是多智能体学习的核心挑战。宝可梦VGC拥有极其庞大的团队配置空间（约$10^{139}$），其高度离散和组合的团队构建性质导致最优策略会根据自身和对手的团队发生显著变化，使得泛化成为一个独特的挑战。", "method": "引入了VGC-Bench基准，该基准提供了关键基础设施、标准化评估协议、人类对战数据集以及一系列基线方法，包括大语言模型智能体、行为克隆、强化学习和经验博弈论方法（如自博弈、虚构博弈和双重神谕）。", "result": "在智能体在单一团队配置上进行训练和评估的受限设置中，所提出的方法能够击败专业的VGC选手。然而，在对 progressively 更大的团队集进行评估时，即使在单一团队设置中表现最好的算法也难以扩展，表明跨多样团队策略的策略泛化仍然是一个开放的挑战。", "conclusion": "跨多样团队策略的策略泛化仍然是多智能体学习领域的一个开放挑战，VGC-Bench为未来研究提供了重要基准和工具。", "translation": "开发能够鲁棒适应截然不同策略环境而无需重新训练的AI智能体是多智能体学习的核心挑战。宝可梦视频游戏锦标赛（VGC）是一个拥有约 $10^{139}$ 种可能团队配置的领域——远远大于Dota或星际争霸。宝可梦VGC中团队构建的高度离散、组合性质导致最优策略会根据所操纵的团队和对手的团队发生显著变化，使得泛化面临独特的挑战。为了推进对该问题的研究，我们引入了VGC-Bench：一个提供关键基础设施、标准化评估协议，并提供人类对战数据集和一系列基线（从大语言模型智能体和行为克隆到强化学习和经验博弈论方法，如自博弈、虚构博弈和双重神谕）的基准。在智能体在单一团队配置上进行训练和评估的受限设置中，我们的方法能够击败专业的VGC选手。我们对所有基线方法在 progressively 更大的团队集上进行了广泛评估，发现即使在单一团队设置中表现最好的算法也难以随着团队规模的增长而扩展。因此，跨多样团队策略的策略泛化仍然是社区面临的一个开放挑战。我们的代码已在 https://github.com/cameronangliss/VGC-Bench 开源。", "summary": "该研究针对多智能体学习中跨多样团队策略泛化的挑战，特别是在团队配置空间巨大的宝可梦VGC领域。作者提出了VGC-Bench，一个包含基础设施、评估协议、人类数据集和多种基线（如LLM、RL、EGTA）的基准。实验结果显示，在受限的单团队设置下，方法能击败专业选手，但随着团队规模增加，泛化能力显著下降，表明该问题仍是开放挑战。", "keywords": "多智能体学习, 宝可梦VGC, 泛化, VGC-Bench, 策略", "comments": "这项工作在多智能体学习领域具有重要意义，它通过宝可梦VGC提供了一个独特且极具挑战性的泛化基准。其创新之处在于构建了一个高度离散、组合性强的环境，并提供了全面的基础设施和多样的基线方法。论文揭示了当前AI在复杂策略空间中泛化能力的局限性，为未来的研究指明了方向。"}}
{"id": "2506.10420", "title": "Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods", "authors": ["Boris Sedlak", "Alireza Furutanpey", "Zihang Wang", "Víctor Casamayor Pujol", "Schahram Dustdar"], "summary": "Edge computing breaks with traditional autoscaling due to strict resource\nconstraints, thus, motivating more flexible scaling behaviors using multiple\nelasticity dimensions. This work introduces an agent-based autoscaling\nframework that dynamically adjusts both hardware resources and internal service\nconfigurations to maximize requirements fulfillment in constrained\nenvironments. We compare four types of scaling agents: Active Inference, Deep Q\nNetwork, Analysis of Structural Knowledge, and Deep Active Inference, using two\nreal-world processing services running in parallel: YOLOv8 for visual\nrecognition and OpenCV for QR code detection. Results show all agents achieve\nacceptable SLO performance with varying convergence patterns. While the Deep Q\nNetwork benefits from pre-training, the structural analysis converges quickly,\nand the deep active inference agent combines theoretical foundations with\npractical scalability advantages. Our findings provide evidence for the\nviability of multi-dimensional agent-based autoscaling for edge environments\nand encourage future work in this research direction.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10420v1", "AI": {"title_translation": "处理服务的多维自动扩缩：基于代理方法的比较", "tldr": "本文提出了一种基于代理的多维自动扩缩框架，用于在资源受限的边缘环境中动态调整硬件资源和服务配置，并比较了四种代理方法的性能。", "motivation": "边缘计算由于严格的资源限制，打破了传统的自动扩缩模式，因此需要更灵活的多维度扩缩行为。", "method": "本文引入了一个基于代理的自动扩缩框架，该框架动态调整硬件资源和内部服务配置。研究比较了四种扩缩代理：主动推理（Active Inference）、深度Q网络（Deep Q Network）、结构知识分析（Analysis of Structural Knowledge）和深度主动推理（Deep Active Inference），并使用YOLOv8和OpenCV两种实际处理服务进行并行测试。", "result": "所有代理都实现了可接受的SLO性能，但收敛模式不同。深度Q网络受益于预训练，结构分析收敛迅速，而深度主动推理代理结合了理论基础和实际可伸缩性优势。", "conclusion": "研究结果证明了多维基于代理的自动扩缩在边缘环境中的可行性，并鼓励在该研究方向上进行未来的工作。", "translation": "边缘计算由于严格的资源限制，打破了传统的自动扩缩模式，因此需要更灵活的多维度扩缩行为。本文介绍了一种基于代理的自动扩缩框架，该框架动态调整硬件资源和内部服务配置，以最大限度地满足受限环境中的需求。我们比较了四种扩缩代理：主动推理、深度Q网络、结构知识分析和深度主动推理，使用两个并行运行的真实世界处理服务：用于视觉识别的YOLOv8和用于二维码检测的OpenCV。结果显示，所有代理都实现了可接受的SLO性能，但收敛模式不同。虽然深度Q网络受益于预训练，但结构分析收敛迅速，深度主动推理代理结合了理论基础和实际可伸缩性优势。我们的发现为边缘环境中多维基于代理的自动扩缩的可行性提供了证据，并鼓励在该研究方向上进行未来的工作。", "summary": "本研究提出了一种针对边缘计算环境的多维自动扩缩框架，该框架采用基于代理的方法，能够同时调整硬件资源和服务配置以满足性能需求。论文比较了主动推理、深度Q网络、结构知识分析和深度主动推理四种不同的代理在YOLOv8和OpenCV服务上的表现。结果表明所有代理均能达到服务水平目标，并展现出各自的收敛特性，强调了多维代理自动扩缩在边缘环境中的潜力。", "keywords": "边缘计算, 自动扩缩, 代理, 多维, 资源管理", "comments": "该论文通过引入多维自动扩缩框架并比较多种代理方法，为边缘计算中的资源管理提供了一种创新的解决方案。其重要性在于克服了传统自动扩缩在资源受限环境下的局限性，并为未来研究指明了方向。特别是对不同AI/ML代理在自动扩缩场景下的性能比较，具有很高的实践指导意义。"}}
{"id": "2506.10056", "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput", "authors": ["Gabriel Orlanski", "Nicholas Roberts", "Aws Albarghouthi", "Frederic Sala"], "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.", "comment": "29 pages, 6 figures, code released here:\n  https://github.com/SprocketLab/orm-code-verifier", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10056v1", "AI": {"title_translation": "奖励模型通过权衡准确性与吞吐量实现可扩展的代码验证", "tldr": "研究发现，即使存在全面的验证器，奖励模型（ORM）也能通过牺牲准确性来提高验证速度，尤其是在“生成-修剪-然后排序”的方法中，系统速度可提高11.65倍，准确性仅下降8.33%。", "motivation": "当前解决编码任务的标准范式是“生成-然后排序”程序，其中排序步骤使用验证器。普遍的共识是，应优先使用全面的验证器（例如，完整的测试套件），而很少考虑其中的权衡。本文旨在挑战这一假设，系统地探索速度和准确性之间的权衡。", "method": "本文通过系统地探索速度和准确性之间的权衡来挑战现有假设。研究发现，即使存在全面的验证器，成果奖励模型（ORM）在通过牺牲准确性来提高速度方面发挥了关键作用。尤其是在“生成-修剪-然后排序”的方法中，即一个更快但准确性较低的验证器在排序之前移除不正确的解决方案。", "result": "研究发现，成果奖励模型（ORM）在通过牺牲准确性来提高速度方面发挥了关键作用，即使在存在全面的验证器时也是如此。在“生成-修剪-然后排序”的方法中，系统速度提高了11.65倍，而准确性仅比完整测试套件低8.33%。分析表明，这种方法通过过滤掉不正确但排名靠前的解决方案来发挥作用。", "conclusion": "奖励模型能够通过权衡准确性与吞吐量，实现可扩展的代码验证。这些发现有助于设计可扩展且准确的程序排序系统。", "translation": "通过大型语言模型（LLMs）解决编码任务的标准范式是“生成-然后排序”程序，其中后者在排序过程中使用验证器。人们日益形成共识，即只要可能，就应优先使用全面的验证器（例如，完整的测试套件），而很少考虑其中涉及的权衡。我们的目标是通过系统地探索速度和准确性之间的权衡来挑战这一假设。我们发现，即使存在全面的验证器，成果奖励模型（ORM）在通过牺牲准确性来提高速度方面发挥了关键作用。当它们用于“生成-修剪-然后排序”的方法时，其价值变得尤为明显，其中一个更快但准确性较低的验证器在排序之前移除不正确的解决方案——这使得系统速度提高了11.65倍，而准确性仅比完整测试套件低8.33%。我们分析了“生成-修剪-然后排序”的方法，并表明它通过过滤掉不正确但排名靠前的解决方案来发挥作用。这些发现有助于设计可扩展且准确的程序排序系统。", "summary": "本文挑战了在LLM编码任务中总是优先使用全面验证器的传统观念，系统地探索了验证速度与准确性之间的权衡。研究发现，成果奖励模型（ORM）即使在全面验证器可用时，也能通过牺牲准确性显著提高验证速度。特别是在“生成-修剪-然后排序”的方法中，ORM作为快速预过滤器，使得系统速度提升11.65倍，而准确性仅下降8.33%。这表明ORM是构建可扩展和准确程序排序系统的关键。", "keywords": "奖励模型, 代码验证, 速度-准确性权衡, 生成-修剪-然后排序, 大语言模型", "comments": "本文的创新之处在于系统地挑战了现有范式，并量化了奖励模型在代码验证中带来的速度提升，即使在全面验证器可用的情况下。其重要性在于为设计更高效、可扩展的程序排序系统提供了新的视角和实用方法，尤其是在资源受限或需要快速反馈的场景下。研究提出的“生成-修剪-然后排序”方法提供了一个具体的优化策略。"}}
{"id": "2506.10172", "title": "A Navigation Framework Utilizing Vision-Language Models", "authors": ["Yicheng Duan", "Kaiyu tang"], "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied\nAI, requiring agents to interpret natural language instructions and navigate\nthrough visually rich, unfamiliar environments. Recent advances in large\nvision-language models (LVLMs), such as CLIP and Flamingo, have significantly\nimproved multimodal understanding but introduced new challenges related to\ncomputational cost and real-time deployment. In this project, we propose a\nmodular, plug-and-play navigation framework that decouples vision-language\nunderstanding from action planning. By integrating a frozen vision-language\nmodel, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to\nachieve flexible, fast, and adaptable navigation without extensive model\nfine-tuning. Our framework leverages prompt engineering, structured history\nmanagement, and a two-frame visual input strategy to enhance decision-making\ncontinuity across navigation steps. We evaluate our system on the Room-to-Room\nbenchmark within the VLN-CE setting using the Matterport3D dataset and\nHabitat-Lab simulation environment. Although our initial results reveal\nchallenges in generalizing to unseen environments under strict evaluation\nsettings, our modular approach lays a foundation for scalable and efficient\nnavigation systems, highlighting promising directions for future improvement\nthrough enhanced environmental priors and expanded multimodal input\nintegration.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10172v1", "AI": {"title_translation": "利用视觉-语言模型的导航框架", "tldr": "本文提出了一种模块化视觉-语言导航（VLN）框架，该框架利用冻结的大型视觉-语言模型（LVLM），通过解耦视觉-语言理解与动作规划，实现高效导航。尽管在未见环境中泛化存在挑战，但该方法为可扩展的导航系统奠定了基础。", "motivation": "视觉-语言导航（VLN）是一个复杂的具身AI挑战，需要智能体理解自然语言指令并在视觉丰富的陌生环境中导航。现有的大型视觉-语言模型（LVLMs）虽然显著提升了多模态理解能力，但也引入了计算成本高和实时部署困难的新挑战。本文旨在提出一种灵活、快速、适应性强的导航框架，避免大量模型微调，同时解决这些挑战。", "method": "本文提出一个模块化、即插即用的导航框架，该框架将视觉-语言理解与动作规划解耦。通过将一个冻结的视觉-语言模型（Qwen2.5-VL-7B-Instruct）与轻量级规划逻辑集成，旨在实现无需大量模型微调的灵活、快速和适应性强的导航。该框架利用提示工程、结构化历史管理和双帧视觉输入策略来增强导航步骤间的决策连续性。系统在VLN-CE设置下的Room-to-Room基准上，使用Matterport3D数据集和Habitat-Lab仿真环境进行评估。", "result": "初步结果显示，在严格的评估设置下，该系统在泛化到未见环境方面存在挑战。然而，这种模块化方法为可扩展和高效的导航系统奠定了基础。", "conclusion": "本文提出的模块化方法为可扩展和高效的导航系统奠定了基础，并指出了未来通过增强环境先验和扩展多模态输入集成来改进的有希望的方向，尽管目前在泛化方面存在挑战。", "translation": "视觉-语言导航（VLN）在具身AI中提出了一个复杂的挑战，要求智能体解释自然语言指令并在视觉丰富的陌生环境中导航。大型视觉-语言模型（LVLMs）如CLIP和Flamingo的最新进展显著改善了多模态理解，但也引入了与计算成本和实时部署相关的新挑战。在这个项目中，我们提出了一个模块化、即插即用的导航框架，该框架将视觉-语言理解与动作规划解耦。通过将一个冻结的视觉-语言模型Qwen2.5-VL-7B-Instruct与轻量级规划逻辑集成，我们旨在实现灵活、快速和适应性强的导航，而无需大量模型微调。我们的框架利用提示工程、结构化历史管理和双帧视觉输入策略来增强导航步骤间的决策连续性。我们在VLN-CE设置下的Room-to-Room基准上，使用Matterport3D数据集和Habitat-Lab仿真环境评估了我们的系统。尽管我们的初步结果显示在严格评估设置下泛化到未见环境方面存在挑战，但我们的模块化方法为可扩展和高效的导航系统奠定了基础，突出了未来通过增强环境先验和扩展多模态输入集成来改进的有希望的方向。", "summary": "本文提出了一种模块化导航框架，用于解决视觉-语言导航（VLN）中大型视觉-语言模型（LVLMs）带来的高计算成本和部署挑战。该框架通过解耦视觉-语言理解和动作规划，并将冻结的LVLM（Qwen2.5-VL-7B-Instruct）与轻量级规划逻辑结合，旨在实现无需大量微调的灵活、快速导航。它采用提示工程、历史管理和双帧视觉输入策略。尽管在Room-to-Room基准测试中，系统在未见环境的泛化能力上仍面临挑战，但其模块化设计为未来可扩展和高效的导航系统奠定了基础。", "keywords": "视觉-语言导航, 大型视觉-语言模型, 模块化框架, 具身AI, 导航", "comments": "本文的创新之处在于其模块化、即插即用的导航框架，特别是通过集成一个“冻结”的大型视觉-语言模型来降低计算成本和对大量模型微调的需求，这对于LVLMs的实际部署具有重要意义。它有效解决了大型模型计算资源消耗大的问题。尽管在未见环境的泛化能力上存在局限性，但这种模块化设计为未来VLN系统的可扩展性和效率提供了有前景的方向，具有良好的发展潜力。"}}
{"id": "2506.10229", "title": "Speculative Design in Spiraling Time: Methods and Indigenous HCI", "authors": ["James Eschrich", "Cole McMullen", "Sarah Sterman"], "summary": "In this position paper, we first discuss the uptake of speculative design as\na method for Indigenous HCI. Then, we outline how a key assumption about\ntemporality threatens to undermine the usefulness of speculative design in this\ncontext. Finally, we briefly sketch out a possible alternative understanding of\nspeculative design, based on the concept of \"spiraling time,\" which could be\nbetter suited for Indigenous HCI.", "comment": "3 pages, 1 figure, presented at the \"Weaving Indigeneity and Culture\n  into the Fabric of HCI Futures\" Workshop at CHI '25", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10229v1", "AI": {"title_translation": "推测性设计在螺旋时间中：方法与原住民人机交互", "tldr": "本立场论文讨论了推测性设计在原住民人机交互中的应用，指出其时间性假设的局限性，并提出了基于“螺旋时间”的替代理解。", "motivation": "论文旨在探讨推测性设计在原住民人机交互（Indigenous HCI）中的应用，并指出其现有时间性假设可能削弱其在此背景下的有效性。", "method": "本文是一篇立场论文，通过讨论推测性设计在原住民人机交互中的应用、概述其时间性假设的局限性，并草拟基于“螺旋时间”概念的替代理解。", "result": "论文提出了基于“螺旋时间”概念的推测性设计替代理解，认为这种理解可能更适合原住民人机交互。", "conclusion": "基于“螺旋时间”概念的推测性设计替代理解，可能更适合原住民人机交互。", "translation": "在这篇立场论文中，我们首先讨论了推测性设计作为原住民人机交互的一种方法被采纳的情况。然后，我们概述了一个关于时间性的关键假设如何可能削弱推测性设计在此背景下的实用性。最后，我们简要地勾勒出一种基于“螺旋时间”概念的推测性设计的可能替代理解，这种理解可能更适合原住民人机交互。", "summary": "这篇立场论文探讨了推测性设计在原住民人机交互（Indigenous HCI）中的应用。论文指出，推测性设计中关于时间性的一个核心假设可能限制其在原住民人机交互语境下的有效性。为此，作者提出了一种基于“螺旋时间”概念的推测性设计新理解，认为这种方法可能更契合原住民人机交互的需求。", "keywords": "推测性设计,原住民人机交互,螺旋时间,时间性,立场论文", "comments": "这篇论文通过引入“螺旋时间”这一概念，为推测性设计在特定文化背景（原住民人机交互）下的应用提供了新的视角和创新思路，挑战了传统推测性设计中可能存在的线性时间假设，具有重要的理论意义和实践指导价值。"}}
{"id": "2506.10136", "title": "Comparing name generator designs in rural panel studies: analyzing alter retention and change", "authors": ["Marian-Gabriel Hâncean", "Jürgen Lerner", "Christopher McCarty"], "summary": "We conducted a two-wave personal network study in a rural Romanian community,\ninterviewing the same participants (n = 68) using two name generators. Wave 1\nemployed a fixed-choice generator (n = 25) focused on emotional closeness; Wave\n2 used a free-choice generator based on frequent interaction. We compared tie\ncharacteristics and assessed retention across waves. Alters who were kin,\nco-residents, or emotionally close were more likely to be retained, regardless\nof generator type. These findings underscore the role of relational attributes\nin personal network stability and highlight design considerations for network\nstudies in resource-limited, culturally distinct settings.", "comment": "14 pages, 4 tables, 1 figure", "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.10136v1", "AI": {"title_translation": "比较农村小组研究中的姓名生成器设计：分析关系人保留和变化", "tldr": "一项在罗马尼亚农村进行的双波个人网络研究发现，亲属、同住者或情感亲近的关系人更容易被保留，这强调了关系属性在网络稳定性中的作用，并为资源有限、文化独特的网络研究提供了设计考量。", "motivation": "为了比较农村小组研究中不同姓名生成器设计的有效性，并评估关系人的保留情况，尤其是在资源有限、文化独特的背景下进行网络研究时的设计考量。", "method": "研究在罗马尼亚农村社区进行了一项双波个人网络研究，对相同的68名参与者使用了两种姓名生成器。第一波采用侧重情感亲近的固定选择生成器；第二波采用基于频繁互动的自由选择生成器。研究比较了关系特征并评估了跨波次的保留情况。", "result": "无论使用哪种生成器类型，作为亲属、同住者或情感亲近的关系人更有可能被保留。", "conclusion": "研究结果强调了关系属性在个人网络稳定性中的作用，并突出了在资源有限、文化独特的背景下进行网络研究时的设计考量。", "translation": "我们在罗马尼亚农村社区进行了一项双波个人网络研究，对相同的参与者（n = 68）使用了两种姓名生成器。第一波采用侧重情感亲近的固定选择生成器（n = 25）；第二波使用基于频繁互动的自由选择生成器。我们比较了关系特征并评估了跨波次的保留情况。作为亲属、同住者或情感亲近的关系人更有可能被保留，无论生成器类型如何。这些发现强调了关系属性在个人网络稳定性中的作用，并突出了在资源有限、文化独特的背景下进行网络研究时的设计考量。", "summary": "本研究在罗马尼亚农村社区进行了一项双波个人网络调查，比较了两种姓名生成器（固定选择与自由选择）在关系人保留方面的效果。结果显示，亲属、同住者或情感亲近的关系人更容易被保留。这项研究强调了关系属性对个人网络稳定性的重要性，并为在资源有限、文化独特的环境中进行网络研究提供了实用的设计建议。", "keywords": "姓名生成器, 面板研究, 网络稳定性, 关系人保留, 农村社区", "comments": "这项研究的创新之处在于，它在特定且研究不足的背景（罗马尼亚农村）下比较了不同的姓名生成器设计，并强调了关系属性在网络稳定性中的关键作用。其结果为在资源受限和文化差异大的地区进行网络研究提供了重要的实践指导。"}}
{"id": "2506.10900", "title": "Dynamic Beyond 5G and 6G Connectivity: Leveraging NTN and RIS Synergies for Optimized Coverage and Capacity in High-Density Environments", "authors": ["Valdemar Farré", "Juan Estrada", "David Vega", "Luis F Urquiza-Aguiar", "Juan A. Vásquez Peralvo", "Symeon Chatzinotas"], "summary": "The increasing demand for reliable, high-capacity communication during\nlarge-scale outdoor events poses significant challenges for traditional\nTerrestrial Networks (TNs), which often struggle to provide consistent coverage\nin high-density environments. This paper presents a novel 6G radio network\nplanning framework that integrates Non-Terrestrial Networks (NTNs) with\nReconfigurable Intelligent Surfaces (RISs) to deliver ubiquitous coverage and\nenhanced network capacity. Our framework overcomes the limitations of\nconventional deployable base stations by leveraging NTN architectures,\nincluding Low Earth Orbit (LEO) satellites and passive RIS platforms seamlessly\nintegrated with Beyond 5G (B5G) TNs. By incorporating advanced B5G technologies\nsuch as Massive Multiple Input Multiple Output (mMIMO) and beamforming, and by\noptimizing spectrum utilization across the C, S, and Ka bands, we implement a\nrigorous interference management strategy based on a dynamic SINR model.\nComprehensive calculations and simulations validate the proposed framework,\ndemonstrating significant improvements in connectivity, reliability, and\ncost-efficiency in crowded scenarios. This integration strategy represents a\npromising solution for meeting the evolving demands of future 6G networks.", "comment": "6 pages, 6 figures, 11 tables", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10900v1", "AI": {"title_translation": "动态B5G和6G连接：利用NTN和RIS协同优化高密度环境下的覆盖和容量", "tldr": "本文提出了一种新颖的6G无线网络规划框架，通过整合非地面网络（NTN）和可重构智能表面（RIS），在高密度环境下提供优化的覆盖和容量。", "motivation": "大规模户外活动期间对可靠、高容量通信的需求不断增长，对传统地面网络（TN）构成了重大挑战，这些网络在高密度环境中往往难以提供一致的覆盖。", "method": "本文提出了一种新颖的6G无线网络规划框架，该框架将非地面网络（NTN）与可重构智能表面（RIS）相结合，以提供无处不在的覆盖和增强的网络容量。该框架通过利用NTN架构（包括低地球轨道（LEO）卫星和无源RIS平台）并将其与超越5G（B5G）地面网络无缝集成，克服了传统可部署基站的局限性。通过结合先进的B5G技术，如大规模多输入多输出（mMIMO）和波束成形，并优化C、S和Ka频段的频谱利用，实施了基于动态SINR模型的严格干扰管理策略。", "result": "全面的计算和仿真验证了所提出的框架，在高密度场景中显示出连接性、可靠性和成本效率的显著改善。", "conclusion": "这种集成策略代表了满足未来6G网络不断演进需求的一个有前景的解决方案。", "translation": "大规模户外活动期间对可靠、高容量通信的需求不断增长，对传统地面网络（TN）构成了重大挑战，这些网络在高密度环境中往往难以提供一致的覆盖。本文提出了一种新颖的6G无线网络规划框架，该框架将非地面网络（NTN）与可重构智能表面（RIS）相结合，以提供无处不在的覆盖和增强的网络容量。我们的框架通过利用NTN架构（包括低地球轨道（LEO）卫星和无源RIS平台）并将其与超越5G（B5G）地面网络无缝集成，克服了传统可部署基站的局限性。通过结合先进的B5G技术，如大规模多输入多输出（mMIMO）和波束成形，并优化C、S和Ka频段的频谱利用，我们实施了基于动态SINR模型的严格干扰管理策略。全面的计算和仿真验证了所提出的框架，在高密度场景中显示出连接性、可靠性和成本效率的显著改善。这种集成策略代表了满足未来6G网络不断演进需求的一个有前景的解决方案。", "summary": "本文提出了一种新颖的6G无线网络规划框架，旨在解决传统地面网络在高密度环境下提供一致覆盖和高容量通信的挑战。该框架通过将非地面网络（NTN）与可重构智能表面（RIS）集成，并结合B5G技术（如mMIMO和波束成形）和动态SINR干扰管理，以优化C、S、Ka频段的频谱利用。仿真结果验证了该框架在高密度场景下能显著提升连接性、可靠性和成本效率，为未来6G网络需求提供了有前景的解决方案。", "keywords": "NTN, RIS, 6G, 高密度环境, 连接性", "comments": "本文的创新之处在于将非地面网络（NTN）和可重构智能表面（RIS）进行协同集成，以解决高密度环境下未来6G网络面临的连接和容量挑战。这种集成方法超越了传统地面网络的局限性，并有望为拥挤场景提供更可靠、高效的通信。"}}
{"id": "2506.10931", "title": "MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem", "authors": ["Melina Soysal", "Konstantina Koliogeorgi", "Can Firtina", "Nika Mansouri Ghiasi", "Rakesh Nadig", "Haiyu Mayo", "Geraldo F. Oliveira", "Yu Liang", "Klea Zambaku", "Mohammad Sadrosadati", "Onur Mutlu"], "summary": "Raw signal genome analysis (RSGA) has emerged as a promising approach to\nenable real-time genome analysis by directly analyzing raw electrical signals.\nHowever, rapid advancements in sequencing technologies make it increasingly\ndifficult for software-based RSGA to match the throughput of raw signal\ngeneration. This paper demonstrates that while hardware acceleration techniques\ncan significantly accelerate RSGA, the high volume of genomic data shifts the\nperformance and energy bottleneck from computation to I/O data movement. As\nsequencing throughput increases, I/O overhead becomes the main contributor to\nboth runtime and energy consumption. Therefore, there is a need to design a\nhigh-performance, energy-efficient system for RSGA that can both alleviate the\ndata movement bottleneck and provide large acceleration capabilities. We\npropose MARS, a storage-centric system that leverages the heterogeneous\nresources within modern storage systems (e.g., storage-internal DRAM, storage\ncontroller, flash chips) alongside their large storage capacity to tackle both\ndata movement and computational overheads of RSGA in an area-efficient and\nlow-cost manner. MARS accelerates RSGA through a novel hardware/software\nco-design approach. First, MARS modifies the RSGA pipeline via two filtering\nmechanisms and a quantization scheme, reducing hardware demands and optimizing\nfor in-storage execution. Second, MARS accelerates the RSGA steps directly\nwithin the storage by leveraging both Processing-Near-Memory and\nProcessing-Using-Memory paradigms. Third, MARS orchestrates the execution of\nall steps to fully exploit in-storage parallelism and minimize data movement.\nOur evaluation shows that MARS outperforms basecalling-based software and\nhardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on\naverage across different datasets, while reducing their energy consumption by\n427x and 72x.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10931v1", "AI": {"title_translation": "MARS：存储子系统内部原始信号基因组分析的内存内加速", "tldr": "MARS是一个以存储为中心的系统，通过内存内计算和软硬件协同设计，在存储子系统内部加速原始信号基因组分析，显著提高了性能并降低了能耗，解决了数据移动瓶颈。", "motivation": "原始信号基因组分析（RSGA）虽然有前景，但当前软件RSGA难以匹配原始信号生成的速度，硬件加速又导致I/O数据移动成为新的性能和能耗瓶颈。因此，需要设计一个高性能、高能效的RSGA系统，以缓解数据移动瓶颈并提供强大的加速能力。", "method": "MARS通过新颖的软硬件协同设计方法加速RSGA。首先，通过两种过滤机制和量化方案修改RSGA流程，以降低硬件需求并优化存储内执行。其次，利用近内存处理（Processing-Near-Memory）和使用内存处理（Processing-Using-Memory）范式，直接在存储内部加速RSGA步骤。最后，MARS协调所有步骤的执行，以充分利用存储内并行性并最小化数据移动。", "result": "MARS在不同数据集上，平均比基于Basecalling的软件和硬件加速的最新读长映射流水线分别快93倍和40倍，同时能耗分别降低427倍和72倍。", "conclusion": "MARS成功地将原始信号基因组分析的性能和能效瓶颈从计算转移到I/O数据移动，并通过在存储子系统内部进行处理，显著提高了RSGA的性能和能效。", "translation": "原始信号基因组分析（RSGA）已成为一种通过直接分析原始电信号实现实时基因组分析的有前景的方法。然而，测序技术的快速发展使得基于软件的RSGA越来越难以匹配原始信号生成的吞吐量。本文表明，虽然硬件加速技术可以显著加速RSGA，但大量的基因组数据将性能和能耗瓶颈从计算转移到I/O数据移动。随着测序吞吐量的增加，I/O开销成为运行时间和能耗的主要贡献者。因此，需要设计一个高性能、高能效的RSGA系统，既能缓解数据移动瓶颈，又能提供强大的加速能力。我们提出了MARS，一个以存储为中心的系统，它利用现代存储系统内部的异构资源（例如，存储内部DRAM、存储控制器、闪存芯片）及其大容量存储，以面积高效和低成本的方式解决RSGA的数据移动和计算开销。MARS通过一种新颖的软硬件协同设计方法加速RSGA。首先，MARS通过两种过滤机制和一种量化方案修改RSGA流程，从而降低硬件需求并优化存储内执行。其次，MARS通过利用近内存处理（Processing-Near-Memory）和使用内存处理（Processing-Using-Memory）范式，直接在存储内部加速RSGA步骤。第三，MARS协调所有步骤的执行，以充分利用存储内并行性并最小化数据移动。我们的评估表明，MARS在不同数据集上，平均比基于Basecalling的软件和硬件加速的最新读长映射流水线分别快93倍和40倍，同时能耗分别降低427倍和72倍。", "summary": "MARS是一个针对原始信号基因组分析（RSGA）的存储中心系统，旨在解决当前软件和硬件加速方案中数据移动导致的性能和能耗瓶颈。它通过新颖的软硬件协同设计，在存储子系统内部利用内存内计算（Processing-Near-Memory和Processing-Using-Memory）加速RSGA流程，并通过优化过滤和量化方案来减少硬件需求和数据移动。实验结果表明，MARS相比现有方法在性能和能耗方面均有显著提升。", "keywords": "原始信号基因组分析, 内存内计算, 存储子系统, 数据移动, 硬件加速", "comments": "MARS的创新点在于将计算推向数据源，即存储子系统内部，有效缓解了大数据量基因组分析中的I/O瓶颈。其软硬件协同设计方法，特别是对RSGA流程的修改和内存内计算范式的应用，展示了在处理海量生物数据方面的巨大潜力。这种方法对于未来实时、高通量基因组分析具有重要意义。"}}
{"id": "2506.10413", "title": "Federated Learning within Global Energy Budget over Heterogeneous Edge Accelerators", "authors": ["Roopkatha Banerjee", "Tejus Chandrashekar", "Ananth Eswar", "Yogesh Simmhan"], "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. However, optimizing both\nenergy efficiency and model accuracy remains a challenge, given device and data\nheterogeneity. Further, sustainable AI through a global energy budget for FL\nhas not been explored. We propose a novel optimization problem for client\nselection in FL that maximizes the model accuracy within an overall energy\nlimit and reduces training time. We solve this with a unique bi-level ILP\nformulation that leverages approximate Shapley values and energy-time\nprediction models to efficiently solve this. Our FedJoule framework achieves\nsuperior training accuracies compared to SOTA and simple baselines for diverse\nenergy budgets, non-IID distributions, and realistic experiment configurations,\nperforming 15% and 48% better on accuracy and time, respectively. The results\nhighlight the effectiveness of our method in achieving a viable trade-off\nbetween energy usage and performance in FL environments.", "comment": "Preprint of paper to appear in the proceedings of the 31st\n  International European Conference on Parallel and Distributed Computing\n  (EuroPar)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10413v1", "AI": {"title_translation": "异构边缘加速器全局能耗预算下的联邦学习", "tldr": "本文提出了FedJoule框架，通过新颖的客户端选择优化问题和双层整数线性规划，在全局能耗预算下最大化联邦学习的模型精度并减少训练时间，相较于SOTA方法在精度和时间上分别提升15%和48%。", "motivation": "联邦学习在分布式客户端协作训练中面临能量效率和模型精度优化的挑战，尤其是在设备和数据异构性下。此外，尚未探索通过全局能耗预算实现可持续AI联邦学习。", "method": "提出了一种新的联邦学习客户端选择优化问题，旨在全局能耗限制内最大化模型精度并减少训练时间。通过独特的双层整数线性规划（ILP）公式解决，该公式利用近似Shapley值和能量-时间预测模型来高效求解。该方法被命名为FedJoule框架。", "result": "FedJoule框架在不同能耗预算、非独立同分布（non-IID）数据和真实实验配置下，比现有最佳（SOTA）和简单基线方法取得了更高的训练精度，在精度上提升15%，在时间上提升48%。结果强调了该方法在联邦学习环境中实现能耗与性能之间可行权衡的有效性。", "conclusion": "本研究提出的方法能有效平衡联邦学习环境中的能耗与性能，在全局能耗预算下显著提升模型精度并缩短训练时间。", "translation": "联邦学习（FL）实现了分布式客户端之间的协作模型训练，同时保留了数据隐私。然而，在设备和数据异构性下，优化能量效率和模型精度仍然是一个挑战。此外，尚未探索通过全局能耗预算实现联邦学习的可持续AI。我们提出了一种新颖的联邦学习客户端选择优化问题，旨在总体能量限制内最大化模型精度并减少训练时间。我们通过独特的双层整数线性规划（ILP）公式解决了这个问题，该公式利用近似Shapley值和能量-时间预测模型来高效求解。我们的FedJoule框架在不同的能量预算、非独立同分布（non-IID）分布和真实实验配置下，比现有最佳（SOTA）和简单基线方法取得了更高的训练精度，在精度上分别提升15%和48%。结果强调了我们方法在联邦学习环境中实现能耗与性能之间可行权衡的有效性。", "summary": "本文提出了一种名为FedJoule的新型联邦学习框架，旨在解决在异构边缘加速器上，如何在全局能耗预算内优化模型精度和训练时间的问题。通过制定一个创新的客户端选择优化问题，并利用双层整数线性规划、近似Shapley值和能量-时间预测模型进行求解，FedJoule框架在实验中表现出优于现有SOTA方法的性能，实现了在精度上15%的提升和在训练时间上48%的缩短，有效平衡了能耗与性能。", "keywords": "联邦学习, 能耗预算, 客户端选择, 异构边缘, 优化", "comments": "该论文的创新点在于首次将全局能耗预算引入联邦学习的客户端选择优化中，并提出了一种新颖的双层ILP方法来解决这一复杂问题。通过结合近似Shapley值和能耗-时间预测模型，该框架在实际应用中具有很高的实用价值，尤其是在资源受限的边缘设备上实现可持续AI具有重要意义。"}}
{"id": "2506.10880", "title": "Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective", "authors": ["V. Giunzioni", "A. Merlini", "F. P. Andriulli"], "summary": "When modeling propagation and scattering phenomena using integral equations\ndiscretized by the boundary element method, it is common practice to\napproximate the boundary of the scatterer with a mesh comprising elements of\nsize approximately equal to a fraction of the wavelength $\\lambda$ of the\nincident wave, e.g., $\\lambda/10$. In this work, by analyzing the spectra of\nthe operator matrices, we show a discrepancy with respect to the continuous\noperators which grows with the simulation frequency, challenging the common\nbelief that the aforementioned widely used discretization approach is\nsufficient to maintain the accuracy of the solution constant when increasing\nthe frequency.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10880v1", "AI": {"title_translation": "三维离散边界积分算子的谱分析：高频视角", "tldr": "本文通过分析算子矩阵的谱，指出在边界元方法中，随着模拟频率的增加，离散算子与连续算子之间的误差会增大，这挑战了在更高频率下保持解精度不变的常见离散化方法的有效性。", "motivation": "在利用边界元方法离散化积分方程来模拟传播和散射现象时，通常认为将散射体边界用波长的一小部分（例如λ/10）的网格进行近似足以在增加频率时保持解的精度不变。本文旨在挑战这一普遍看法，揭示高频下离散化方法的局限性。", "method": "通过分析算子矩阵的谱。", "result": "研究发现，离散算子与连续算子之间存在差异，且这种差异随着模拟频率的增加而增大。", "conclusion": "传统的边界元方法离散化策略（例如使用λ/10网格）可能不足以在高频下保持解的精度恒定，需要重新审视高频建模中的离散化方法。", "translation": "在使用边界元方法离散化积分方程来模拟传播和散射现象时，通常的做法是用网格近似散射体的边界，网格单元的大小大约等于入射波波长λ的一小部分，例如λ/10。在这项工作中，通过分析算子矩阵的谱，我们发现离散算子与连续算子之间存在差异，这种差异随着模拟频率的增加而增大，这挑战了上述广泛使用的离散化方法在增加频率时足以保持解精度不变的普遍看法。", "summary": "本文研究了三维离散边界积分算子在高频下的谱特性。通过分析算子矩阵的谱，作者发现当使用边界元方法进行建模时，离散算子与连续算子之间的误差随着模拟频率的升高而增大。这一发现挑战了当前广泛采纳的、认为通过将网格单元尺寸设定为波长的一小部分即可在高频下保持解精度不变的普遍认知。", "keywords": "边界积分算子, 谱分析, 离散化, 边界元方法, 高频", "comments": "本文的创新之处在于，它通过严谨的谱分析，对边界元方法在高频应用中的一个长期存在的、被广泛接受的离散化实践提出了质疑。其重要性在于，它揭示了在高频电磁仿真中可能导致精度下降的潜在原因，促使研究人员重新思考和改进高频建模的离散化策略。这对于确保高频模拟的准确性和可靠性具有重要意义。"}}
{"id": "2506.10789", "title": "FASCIST-O-METER: Classifier for Neo-fascist Discourse Online", "authors": ["Rudy Alexandro Garrido Veliz", "Martin Semmann", "Chris Biemann", "Seid Muhie Yimam"], "summary": "Neo-fascism is a political and societal ideology that has been having\nremarkable growth in the last decade in the United States of America (USA), as\nwell as in other Western societies. It poses a grave danger to democracy and\nthe minorities it targets, and it requires active actions against it to avoid\nescalation. This work presents the first-of-its-kind neo-fascist coding scheme\nfor digital discourse in the USA societal context, overseen by political\nscience researchers. Our work bridges the gap between Natural Language\nProcessing (NLP) and political science against this phenomena. Furthermore, to\ntest the coding scheme, we collect a tremendous amount of activity on the\ninternet from notable neo-fascist groups (the forums of Iron March and\nStormfront.org), and the guidelines are applied to a subset of the collected\nposts. Through crowdsourcing, we annotate a total of a thousand posts that are\nlabeled as neo-fascist or non-neo-fascist. With this labeled data set, we\nfine-tune and test both Small Language Models (SLMs) and Large Language Models\n(LLMs), obtaining the very first classification models for neo-fascist\ndiscourse. We find that the prevalence of neo-fascist rhetoric in this kind of\nforum is ever-present, making them a good target for future research. The\nsocietal context is a key consideration for neo-fascist speech when conducting\nNLP research. Finally, the work against this kind of political movement must be\npressed upon and continued for the well-being of a democratic society.\nDisclaimer: This study focuses on detecting neo-fascist content in text,\nsimilar to other hate speech analyses, without labeling individuals or\norganizations.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10789v1", "AI": {"title_translation": "法西斯测量仪：在线新法西斯主义言论分类器", "tldr": "该研究开发了首个用于在线新法西斯主义言论分类的自然语言处理模型，通过创新的编码方案、众包标注数据集以及对小型和大型语言模型的微调实现。", "motivation": "新法西斯主义在过去十年中在美国及其他西方社会显著增长，对民主和少数民族构成严重威胁，需要采取积极行动加以遏制。", "method": "本研究提出了首个针对美国社会背景下数字言论的新法西斯主义编码方案，并由政治学研究人员进行监督。为了验证该方案，研究人员从Notable新法西斯主义团体（如Iron March和Stormfront.org论坛）收集了大量在线活动数据。通过众包，标注了1000个帖子，将其标记为新法西斯主义或非新法西斯主义。利用这个标注数据集，研究人员对小型语言模型（SLMs）和大型语言模型（LLMs）进行了微调和测试。", "result": "本研究获得了首批用于新法西斯主义言论的分类模型。研究发现，在这类论坛中，新法西斯主义言论普遍存在。", "conclusion": "社会背景是进行新法西斯主义言论自然语言处理研究的关键考量。为了民主社会的福祉，必须继续并加强对抗此类政治运动的工作。", "translation": "新法西斯主义是一种政治和社会意识形态，在过去十年中在美国（USA）以及其他西方社会取得了显著增长。它对民主及其所针对的少数群体构成了严重威胁，需要积极采取行动来避免事态升级。这项工作提出了美国社会背景下数字言论的首个新法西斯主义编码方案，由政治学研究人员监督。我们的工作弥合了自然语言处理（NLP）和政治学之间对抗这种现象的鸿沟。此外，为了测试编码方案，我们从著名的新法西斯主义团体（Iron March和Stormfront.org论坛）收集了大量的互联网活动数据，并将指导原则应用于收集到的帖子子集。通过众包，我们总共标注了一千个帖子，这些帖子被标记为新法西斯主义或非新法西斯主义。利用这个标注数据集，我们微调并测试了小型语言模型（SLMs）和大型语言模型（LLMs），获得了首批用于新法西斯主义言论的分类模型。我们发现，在这类论坛中，新法西斯主义言论的盛行无处不在，这使得它们成为未来研究的良好目标。在进行自然语言处理研究时，社会背景是新法西斯主义言论的一个关键考量因素。最后，为了民主社会的福祉，必须对这种政治运动的斗争进行压制和继续。免责声明：本研究侧重于检测文本中的新法西斯主义内容，类似于其他仇恨言论分析，不标记个人或组织。", "summary": "本研究旨在应对新法西斯主义在线言论的增长威胁。它提出了首个针对美国数字语境的新法西斯主义编码方案，并收集了来自新法西斯主义论坛的大量数据，通过众包标注了1000个帖子。利用这个独特的标注数据集，研究人员微调并测试了小型和大型语言模型，成功开发出首批新法西斯主义言论分类模型。研究结果表明，此类论坛中新法西斯主义言论普遍存在，并强调了社会背景在相关NLP研究中的重要性，呼吁持续对抗此类政治运动。", "keywords": "新法西斯主义, 自然语言处理, 言论分类, 在线论坛, 仇恨言论检测", "comments": "这项研究具有重要的创新意义，因为它首次提出了新法西斯主义的数字言论编码方案，并基于此开发了首批分类模型，成功弥合了自然语言处理与政治学之间的交叉领域。其工作对于识别和对抗日益增长的在线新法西斯主义威胁至关重要。研究明确指出其焦点在于文本内容检测，而非针对个人或组织进行标记，这有助于明确其研究范围和伦理立场。"}}
{"id": "2506.10345", "title": "Technical Report with Proofs for A Full Picture in Conformance Checking: Efficiently Summarizing All Optimal Alignments", "authors": ["Philipp Bär", "Moe T. Wynn", "Sander J. J. Leemans"], "summary": "This technical report provides proofs for the claims in the paper \"A Full\nPicture in Conformance Checking: Efficiently Summarizing All Optimal\nAlignments\".", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10345v1", "AI": {"title_translation": "一致性检查的全景图：高效总结所有最优对齐的技术报告及证明", "tldr": "本技术报告为论文《一致性检查的全景图：高效总结所有最优对齐》中的主张提供了证明。", "motivation": "为论文《一致性检查的全景图：高效总结所有最优对齐》中的主张提供证明。", "method": "提供证明", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "本技术报告为论文《一致性检查的全景图：高效总结所有最优对齐》中的主张提供了证明。", "summary": "本技术报告旨在为题为《一致性检查的全景图：高效总结所有最优对齐》的论文中提出的所有主张提供详尽的证明。", "keywords": "一致性检查, 最优对齐, 证明, 技术报告", "comments": "本报告作为一篇技术报告，其核心价值在于为另一篇论文的理论主张提供了严谨的数学或逻辑证明。它本身不提出新的方法或发现，而是为现有研究的可靠性提供支撑。因此，其重要性与所支持论文的创新性和影响力紧密相关。"}}
{"id": "2506.10291", "title": "Learning-Based Stable Optimal Control for Infinite-Time Nonlinear Regulation Problems", "authors": ["Han Wang", "Di Wu", "Lin Cheng", "Shengping Gong", "Xu Huang"], "summary": "Infinite-time nonlinear optimal regulation control is widely utilized in\naerospace engineering as a systematic method for synthesizing stable\ncontrollers. However, conventional methods often rely on linearization\nhypothesis, while recent learning-based approaches rarely consider stability\nguarantees. This paper proposes a learning-based framework to learn a stable\noptimal controller for nonlinear optimal regulation problems. First, leveraging\nthe equivalence between Pontryagin Maximum Principle (PMP) and\nHamilton-Jacobi-Bellman (HJB) equation, we improve the backward generation of\noptimal examples (BGOE) method for infinite-time optimal regulation problems. A\nstate-transition-matrix-guided data generation method is then proposed to\nefficiently generate a complete dataset that covers the desired state space.\nFinally, we incorporate the Lyapunov stability condition into the learning\nframework, ensuring the stability of the learned optimal policy by jointly\nlearning the optimal value function and control policy. Simulations on three\nnonlinear optimal regulation problems show that the learned optimal policy\nachieves near-optimal regulation control and the code is provided at\nhttps://github.com/wong-han/PaperNORC", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10291v1", "AI": {"title_translation": "无限时非线性调节问题的学习型稳定最优控制", "tldr": "本文提出了一种基于学习的框架，用于解决无限时非线性最优调节问题，通过改进数据生成方法并结合Lyapunov稳定性条件，学习得到稳定的近最优控制器。", "motivation": "传统的无限时非线性最优调节控制方法依赖线性化假设，而现有的基于学习的方法通常不考虑稳定性保证。", "method": "论文提出了一个基于学习的框架来学习稳定的最优控制器。首先，利用庞特里亚金最大值原理（PMP）和汉密尔顿-雅可比-贝尔曼（HJB）方程的等价性，改进了无限时最优调节问题的最优示例反向生成（BGOE）方法。其次，提出了一种状态转移矩阵引导的数据生成方法，以高效生成覆盖所需状态空间的完整数据集。最后，将Lyapunov稳定性条件纳入学习框架，通过联合学习最优值函数和控制策略，确保所学最优策略的稳定性。", "result": "在三个非线性最优调节问题上的仿真表明，所学习的最优策略实现了近最优的调节控制。", "conclusion": "本文提出的学习型框架能够为无限时非线性调节问题学习到稳定的近最优控制器，有效解决了传统方法和现有学习方法在稳定性和线性化假设上的局限。", "translation": "无限时非线性最优调节控制在航空航天工程中被广泛用作合成稳定控制器的系统方法。然而，传统方法通常依赖于线性化假设，而最近的基于学习的方法很少考虑稳定性保证。本文提出了一种基于学习的框架，用于学习非线性最优调节问题的稳定最优控制器。首先，利用庞特里亚金最大值原理（PMP）和汉密尔顿-雅可比-贝尔曼（HJB）方程之间的等价性，我们改进了无限时最优调节问题的最优示例反向生成（BGOE）方法。然后提出了一种状态转移矩阵引导的数据生成方法，以高效生成覆盖所需状态空间的完整数据集。最后，我们将Lyapunov稳定性条件纳入学习框架，通过联合学习最优值函数和控制策略，确保所学最优策略的稳定性。在三个非线性最优调节问题上的仿真表明，所学习的最优策略实现了近最优的调节控制，并且代码已在 https://github.com/wong-han/PaperNORC 提供。", "summary": "本文针对无限时非线性最优调节控制中传统方法依赖线性化假设和现有学习方法缺乏稳定性保证的问题，提出了一种新型学习框架。该框架通过改进最优示例反向生成（BGOE）方法，引入状态转移矩阵引导的数据生成策略，并关键性地将Lyapunov稳定性条件整合到学习过程中，以联合学习最优值函数和控制策略，从而确保所学控制器的稳定性。仿真结果验证了该方法能够实现近最优的调节控制。", "keywords": "学习型控制, 稳定最优控制, 非线性调节, Lyapunov稳定性, Pontryagin最大值原理", "comments": "本文的创新点在于将Lyapunov稳定性条件整合到基于学习的最优控制框架中，解决了现有学习方法在稳定性保证方面的不足。通过改进数据生成方法，提高了学习效率和数据覆盖率。该研究对于航空航天等领域中需要稳定且最优控制的非线性系统具有重要意义。"}}
{"id": "2506.10265", "title": "Ground Reaction Force Estimation via Time-aware Knowledge Distillation", "authors": ["Eun Som Jeon", "Sinjini Mitra", "Jisoo Lee", "Omik M. Save", "Ankita Shukla", "Hyunglae Lee", "Pavan Turaga"], "summary": "Human gait analysis with wearable sensors has been widely used in various\napplications, such as daily life healthcare, rehabilitation, physical therapy,\nand clinical diagnostics and monitoring. In particular, ground reaction force\n(GRF) provides critical information about how the body interacts with the\nground during locomotion. Although instrumented treadmills have been widely\nused as the gold standard for measuring GRF during walking, their lack of\nportability and high cost make them impractical for many applications. As an\nalternative, low-cost, portable, wearable insole sensors have been utilized to\nmeasure GRF; however, these sensors are susceptible to noise and disturbance\nand are less accurate than treadmill measurements. To address these challenges,\nwe propose a Time-aware Knowledge Distillation framework for GRF estimation\nfrom insole sensor data. This framework leverages similarity and temporal\nfeatures within a mini-batch during the knowledge distillation process,\neffectively capturing the complementary relationships between features and the\nsequential properties of the target and input data. The performance of the\nlightweight models distilled through this framework was evaluated by comparing\nGRF estimations from insole sensor data against measurements from an\ninstrumented treadmill. Empirical results demonstrated that Time-aware\nKnowledge Distillation outperforms current baselines in GRF estimation from\nwearable sensor data.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10265v1", "AI": {"title_translation": "考虑时间特征的知识蒸馏在地面反作用力估计中的应用", "tldr": "本文提出了一种时间感知知识蒸馏框架，用于通过可穿戴鞋垫传感器数据准确估计地面反作用力，解决了传统方法便携性差或精度低的问题。", "motivation": "传统的地面反作用力（GRF）测量方法（如器械跑步机）缺乏便携性且成本高昂；而可穿戴鞋垫传感器虽然便携但易受噪声干扰且精度不足。为了解决这些挑战，实现GRF的准确、便携估计，本文提出了新的框架。", "method": "本文提出了一种时间感知知识蒸馏（Time-aware Knowledge Distillation）框架，用于从鞋垫传感器数据中估计地面反作用力。该框架在知识蒸馏过程中利用迷你批次内的相似性和时间特征，有效捕捉特征之间的互补关系以及目标和输入数据的序列属性。", "result": "经验结果表明，时间感知知识蒸馏框架在从可穿戴传感器数据中估计地面反作用力方面优于现有的基线方法。", "conclusion": "结合结果，可以得出结论，所提出的时间感知知识蒸馏框架能够有效提高可穿戴传感器GRF估计的准确性，为GRF的便携式、高精度测量提供了解决方案。", "translation": "人体步态分析与可穿戴传感器已广泛应用于日常生活保健、康复、物理治疗以及临床诊断和监测等各种应用中。其中，地面反作用力（GRF）提供了关于身体在运动过程中如何与地面相互作用的关键信息。尽管器械跑步机已被广泛用作测量步行GRF的“金标准”，但其缺乏便携性和高成本使其在许多应用中不切实际。作为替代方案，低成本、便携式、可穿戴鞋垫传感器已被用于测量GRF；然而，这些传感器易受噪声和干扰影响，且精度低于跑步机测量。为了应对这些挑战，我们提出了一种时间感知知识蒸馏框架，用于从鞋垫传感器数据中估计GRF。该框架在知识蒸馏过程中利用迷你批次内的相似性和时间特征，有效捕捉特征之间的互补关系以及目标和输入数据的序列属性。通过该框架蒸馏出的轻量级模型的性能通过比较鞋垫传感器数据的GRF估计值与器械跑步机的测量值进行了评估。经验结果表明，时间感知知识蒸馏在从可穿戴传感器数据中估计GRF方面优于当前的基线方法。", "summary": "本文提出了一种名为“时间感知知识蒸馏”的新框架，旨在解决使用可穿戴鞋垫传感器准确估计地面反作用力（GRF）的挑战。针对传统器械跑步机缺乏便携性和高成本，以及鞋垫传感器精度不足的问题，该框架通过利用迷你批次内的相似性和时间特征，有效提升了GRF估计的性能。实验结果表明，该方法在从可穿戴传感器数据中估计GRF方面优于现有基线。", "keywords": "地面反作用力, 知识蒸馏, 可穿戴传感器, 步态分析, 鞋垫传感器", "comments": "这篇论文的创新点在于将时间感知特性融入到知识蒸馏框架中，以提高从嘈杂的鞋垫传感器数据中估计地面反作用力的准确性。它成功地弥补了传统高精度设备便携性不足和便携式传感器精度欠佳的鸿沟，对于推动可穿戴技术在步态分析和医疗健康领域的应用具有重要意义。"}}
{"id": "2506.10653", "title": "Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes", "authors": ["Rogier C. van Dalen", "Shucong Zhang", "Titouan Parcollet", "Sourav Bhattacharya"], "summary": "Speech recognisers usually perform optimally only in a specific environment\nand need to be adapted to work well in another. For adaptation to a new\nspeaker, there is often too little data for fine-tuning to be robust, and that\ndata is usually unlabelled. This paper proposes a combination of approaches to\nmake adaptation to a single minute of data robust. First, instead of estimating\nthe adaptation parameters with cross-entropy on a single error-prone hypothesis\nor \"pseudo-label\", this paper proposes a novel loss function, the conditional\nentropy over complete hypotheses. Using multiple hypotheses makes adaptation\nmore robust to errors in the initial recognition. Second, a \"speaker code\"\ncharacterises a speaker in a vector short enough that it requires little data\nto estimate. On a far-field noise-augmented version of Common Voice, the\nproposed scheme yields a 20% relative improvement in word error rate on one\nminute of adaptation data, increasing on 10 minutes to 29%.", "comment": null, "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10653v1", "AI": {"title_translation": "鲁棒的无监督语音识别器自适应方法：基于熵最小化和说话人编码", "tldr": "该论文提出一种结合熵最小化和说话人编码的方法，以实现对少量（一分钟）无标签数据的鲁棒无监督语音识别器自适应，显著提高了词错误率。", "motivation": "语音识别器通常只在特定环境下表现最佳，需要自适应到新环境或新说话人。然而，针对新说话人的自适应往往数据量过少且无标签，导致微调不够鲁棒。", "method": "该论文提出了两种结合方法：1. 提出了一种新的损失函数——完整假设上的条件熵，替代了传统基于单个错误假设的交叉熵，通过使用多个假设来增强对初始识别错误的鲁棒性。2. 引入了“说话人编码”，用一个足够短的向量来表征说话人，使其只需少量数据即可估计。", "result": "在Common Voice的远场噪声增强版本上，所提出的方案在1分钟的自适应数据上使词错误率相对提高了20%，在10分钟的数据上提高到29%。", "conclusion": "结合条件熵和说话人编码的方法能够有效地对少量无标签数据进行鲁棒的语音识别器无监督自适应，并显著提高识别性能。", "translation": "语音识别器通常只在特定环境下表现最佳，需要自适应才能在其他环境下良好工作。对于新说话人的自适应，通常用于微调的数据量太少，无法做到鲁棒，而且这些数据通常是未标注的。本文提出了一种方法组合，旨在使对一分钟数据的自适应变得鲁棒。首先，本文提出了一种新颖的损失函数——完整假设上的条件熵，而不是使用单个容易出错的假设或“伪标签”上的交叉熵来估计自适应参数。使用多个假设使自适应对初始识别中的错误更具鲁棒性。其次，“说话人编码”用一个足够短的向量来表征说话人，从而只需少量数据即可估计。在Common Voice的远场噪声增强版本上，所提出的方案在1分钟的自适应数据上使词错误率相对提高了20%，在10分钟的数据上提高到29%。", "summary": "该论文提出了一种鲁棒的无监督语音识别器自适应方法，旨在解决新说话人数据量少且无标签的问题。核心创新包括使用完整假设上的条件熵作为新的损失函数，以增强对初始识别错误的鲁棒性；以及引入说话人编码，用少量数据即可表征说话人。实验结果显示，在Common Voice数据集上，该方法在仅一分钟的自适应数据上实现了20%的词错误率相对提升，并在十分钟数据上提升至29%。", "keywords": "语音识别, 无监督自适应, 熵最小化, 说话人编码, 词错误率", "comments": "该论文的创新点在于结合了条件熵最小化和说话人编码，有效解决了无监督、少量数据下语音识别器自适应的鲁棒性问题。特别是使用完整假设的条件熵，避免了单一伪标签的脆弱性，提高了自适应的稳定性。说话人编码的引入也有效地利用了少量数据捕捉说话人特性的能力。其在仅一分钟数据上即获得显著性能提升的成果，对于实际应用中数据稀缺场景具有重要价值。"}}
{"id": "2506.10192", "title": "Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems", "authors": ["Filip Cano"], "summary": "Ensuring responsible use of artificial intelligence (AI) has become\nimperative as autonomous systems increasingly influence critical societal\ndomains. However, the concept of trustworthy AI remains broad and\nmulti-faceted. This thesis advances knowledge in the safety, fairness,\ntransparency, and accountability of AI systems. In safety, we extend classical\ndeterministic shielding techniques to become resilient against delayed\nobservations, enabling practical deployment in real-world conditions. We also\nimplement both deterministic and probabilistic safety shields into simulated\nautonomous vehicles to prevent collisions with road users, validating the use\nof these techniques in realistic driving simulators. We introduce fairness\nshields, a novel post-processing approach to enforce group fairness in\nsequential decision-making settings over finite and periodic time horizons. By\noptimizing intervention costs while strictly ensuring fairness constraints,\nthis method efficiently balances fairness with minimal interference. For\ntransparency and accountability, we propose a formal framework for assessing\nintentional behaviour in probabilistic decision-making agents, introducing\nquantitative metrics of agency and intention quotient. We use these metrics to\npropose a retrospective analysis of intention, useful for determining\nresponsibility when autonomous systems cause unintended harm. Finally, we unify\nthese contributions through the ``reactive decision-making'' framework,\nproviding a general formalization that consolidates previous approaches.\nCollectively, the advancements presented contribute practically to the\nrealization of safer, fairer, and more accountable AI systems, laying the\nfoundations for future research in trustworthy AI.", "comment": "202 pages, 38 figures, PhD Thesis", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10192v1", "AI": {"title_translation": "迈向负责任的人工智能：自主系统在安全、公平和问责制方面的进展", "tldr": "本论文旨在通过在安全、公平、透明和问责制方面取得进展，使AI系统更加负责任和值得信赖。具体方法包括扩展确定性安全防护罩、引入公平防护罩以及提出评估概率决策代理意图的形式框架。", "motivation": "随着自主系统日益影响关键社会领域，确保人工智能的负责任使用变得至关重要，但可信赖人工智能的概念仍然宽泛且多方面。本论文旨在推进AI系统在安全、公平、透明和问责制方面的知识。", "method": "在安全方面，扩展了经典的确定性防护罩技术以应对延迟观测，并在模拟自动驾驶汽车中实现确定性和概率性安全防护罩以防止碰撞。在公平方面，引入了公平防护罩，这是一种新的后处理方法，用于在顺序决策设置中强制执行群体公平性，通过优化干预成本同时严格确保公平约束。在透明度和问责制方面，提出了一个评估概率决策代理意图的形式框架，引入了代理和意图商数的定量指标，并利用这些指标进行回顾性意图分析以确定责任。最后，通过“反应式决策”框架统一了所有贡献。", "result": "扩展的确定性安全防护罩能够抵御延迟观测，实现实际部署。在模拟自动驾驶汽车中验证了安全防护罩在防止碰撞方面的有效性。提出的公平防护罩能够高效平衡公平性与最小干扰，同时严格确保公平约束。提出的意图评估框架和指标有助于在自主系统造成意外伤害时确定责任。所有贡献通过“反应式决策”框架得到统一。", "conclusion": "本论文的进展为实现更安全、更公平、更负责任的AI系统做出了实际贡献，为可信赖AI的未来研究奠定了基础。", "translation": "随着自主系统日益影响关键社会领域，确保人工智能（AI）的负责任使用已变得势在必行。然而，可信赖AI的概念仍然宽泛且多方面。本论文在AI系统的安全、公平、透明度和问责制方面推进了知识。在安全方面，我们扩展了经典的确定性防护罩技术，使其能够抵御延迟观测，从而实现在实际条件下的部署。我们还在模拟自动驾驶汽车中实现了确定性和概率性安全防护罩，以防止与道路使用者发生碰撞，验证了这些技术在真实驾驶模拟器中的应用。我们引入了公平防护罩，这是一种新颖的后处理方法，用于在有限和周期性时间范围内的顺序决策设置中强制执行群体公平性。通过优化干预成本同时严格确保公平约束，该方法有效地平衡了公平性与最小干扰。为了实现透明度和问责制，我们提出了一个评估概率决策代理意图的形式框架，引入了代理和意图商数的定量指标。我们使用这些指标来提出意图的回溯分析，这对于在自主系统造成意外伤害时确定责任非常有用。最后，我们通过“反应式决策”框架统一了这些贡献，提供了一个整合先前方法的通用形式化。总的来说，所提出的进展为实现更安全、更公平、更负责任的AI系统做出了实际贡献，为可信赖AI的未来研究奠定了基础。", "summary": "本论文致力于解决人工智能系统的责任问题，特别关注安全、公平、透明和问责制。研究通过扩展确定性安全防护罩以应对延迟观测，并在模拟自动驾驶汽车中验证其防撞能力来增强安全性。为实现公平性，论文引入了公平防护罩，一种在序列决策中强制执行群体公平性的新方法，旨在以最小干预实现公平。在透明度和问责制方面，论文提出了一个评估概率决策代理意图的形式框架，并引入了量化指标以进行意图回溯分析，从而在发生意外伤害时明确责任。这些贡献最终被统一于“反应式决策”框架下，共同为构建更安全、更公平、更负责任的AI系统奠定了基础。", "keywords": "负责任AI, 安全防护罩, 公平防护罩, 问责制, 自主系统", "comments": "本文在推动AI负责任发展方面做出了多方面的贡献。其创新点在于将经典安全防护罩技术扩展到更具挑战性的实际环境（延迟观测），并首次提出“公平防护罩”以解决序列决策中的群体公平问题。此外，引入量化指标来评估AI意图，为AI系统的问责制提供了新的形式化工具。这些方法都具有很强的实践意义，特别是在自动驾驶等关键应用领域。论文通过统一的“反应式决策”框架整合了不同方面的贡献，显示出一定的理论深度。"}}
{"id": "2506.10758", "title": "Circulant TSP: Vertices of the Edge-Length Polytope and Superpolynomial Lower Bounds", "authors": ["Samuel C. Gutekunst"], "summary": "We study the edge-length polytope, motivated both by algorithmic research on\nthe Circulant Traveling Salesman Problem (Circulant TSP) and number-theoretic\nresearch related to the Buratti-Horak-Rosa conjecture. Circulant TSP is a\nspecial case of TSP whose overall complexity is a significant still-open\nquestion, and where on an input with vertices $\\{1, 2, ..., n\\}$, the cost of\nan edge $\\{i, j\\}$ depends only on its length $\\min\\{|i-j|, n-|i-j|\\}$. The\nedge-length polytope provides one path to solving circulant TSP instances, and\nwe show that it is intimately connected to the factorization of $n$: the number\nof vertices scales with $n$ whenever $n$ is prime and with $n^{3/2}$ whenever\n$n$ is a prime-squared, but there are a superpolynomial number of vertices\nwhenever $n$ is a power of 2. In contrast, the more-standard Symmetric TSP\nPolytope has roughly $n!$ vertices. Hence, for Circulant TSP, a brute-force\nalgorithm checking every vertex is actually efficient in some cases, based on\nthe factorization of $n$. As an intermediate step, we give superpolynomial\nlower-bounds on two combinatorial sequences related to the Buratti-Horak-Rosa\nconjecture, which asks what combinations of edge lengths can comprise a\nHamiltonian path.", "comment": null, "cate": "cs.DM", "url": "http://arxiv.org/abs/2506.10758v1", "AI": {"title_translation": "环状TSP：边长多面体的顶点和超多项式下界", "tldr": "本文研究环状旅行商问题（Circulant TSP）的边长多面体，发现其顶点数量与输入规模n的因子分解密切相关，揭示了某些情况下暴力算法的有效性。同时，为与Buratti-Horak-Rosa猜想相关的组合序列提供了超多项式下界。", "motivation": "研究环状旅行商问题（Circulant TSP）的算法复杂性以及与Buratti-Horak-Rosa猜想相关的数论问题。环状TSP的整体复杂度仍是一个重要的未解问题。", "method": "通过研究边长多面体来分析环状TSP实例，并探究其顶点数量与n的因子分解之间的关系。作为中间步骤，还给出了与Buratti-Horak-Rosa猜想相关的两个组合序列的超多项式下界。", "result": "边长多面体的顶点数量与n的因子分解紧密相关：当n是素数时，顶点数量与n呈线性关系；当n是素数的平方时，顶点数量与n^(3/2)呈线性关系；但当n是2的幂时，顶点数量是超多项式数量。这表明对于环状TSP，在某些情况下，基于n的因子分解，暴力算法是有效的。此外，还给出了与Buratti-Horak-Rosa猜想相关的两个组合序列的超多项式下界。", "conclusion": "环状TSP的边长多面体顶点数量的特性表明，在特定条件下，暴力算法可能比预期更有效。这项研究也为Buratti-Horak-Rosa猜想提供了新的理论进展。", "translation": "我们研究了边长多面体，其动机源于环状旅行商问题（Circulant TSP）的算法研究以及与Buratti-Horak-Rosa猜想相关的数论研究。环状TSP是TSP的一个特例，其整体复杂度仍是一个重要的未解问题。在输入顶点为{1, 2, ..., n}的情况下，边{i, j}的成本仅取决于其长度min{|i-j|, n-|i-j|}。边长多面体为解决环状TSP实例提供了一条途径，我们发现它与n的因子分解密切相关：当n是素数时，顶点数量与n呈线性关系；当n是素数的平方时，顶点数量与n^(3/2)呈线性关系；但当n是2的幂时，顶点数量是超多项式数量。相比之下，更标准的对称TSP多面体大约有n!个顶点。因此，对于环状TSP，基于n的因子分解，在某些情况下，检查每个顶点的暴力算法实际上是高效的。作为中间步骤，我们给出了与Buratti-Horak-Rosa猜想相关的两个组合序列的超多项式下界，该猜想询问哪些边长组合可以构成哈密顿路径。", "summary": "本文深入研究了环状旅行商问题（Circulant TSP）中的边长多面体，揭示了其顶点数量与输入规模n的因子分解之间的紧密联系。研究发现，当n为素数或素数平方时，顶点数量增长相对缓慢；而当n为2的幂时，顶点数量则呈现超多项式增长。这一发现颠覆了传统认知，表明在特定条件下，针对环状TSP的暴力算法可能出乎意料地高效。此外，本研究还为与Buratti-Horak-Rosa猜想相关的两个组合序列建立了超多项式下界。", "keywords": "环状TSP, 边长多面体, 超多项式下界, Buratti-Horak-Rosa猜想, 因子分解", "comments": "这项研究创新性地将环状TSP的复杂性与数论中的因子分解联系起来，揭示了在特定情况下暴力算法的潜在效率，这与传统TSP的NP-hard特性形成鲜明对比。对Buratti-Horak-Rosa猜想的贡献也显示了其在组合数学领域的广度。"}}
{"id": "2506.10233", "title": "Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization", "authors": ["Ana Lawry Aguila", "Peirong Liu", "Oula Puonti", "Juan Eugenio Iglesias"], "summary": "Supervised machine learning has enabled accurate pathology detection in brain\nMRI, but requires training data from diseased subjects that may not be readily\navailable in some scenarios, for example, in the case of rare diseases.\nReconstruction-based unsupervised anomaly detection, in particular using\ndiffusion models, has gained popularity in the medical field as it allows for\ntraining on healthy images alone, eliminating the need for large\ndisease-specific cohorts. These methods assume that a model trained on normal\ndata cannot accurately represent or reconstruct anomalies. However, this\nassumption often fails with models failing to reconstruct healthy tissue or\naccurately reconstruct abnormal regions i.e., failing to remove anomalies. In\nthis work, we introduce a novel conditional diffusion model framework for\nanomaly detection and healthy image reconstruction in brain MRI. Our weakly\nsupervised approach integrates synthetically generated pseudo-pathology images\ninto the modeling process to better guide the reconstruction of healthy images.\nTo generate these pseudo-pathologies, we apply fluid-driven anomaly\nrandomization to augment real pathology segmentation maps from an auxiliary\ndataset, ensuring that the synthetic anomalies are both realistic and\nanatomically coherent. We evaluate our model's ability to detect pathology,\nusing both synthetic anomaly datasets and real pathology from the ATLAS\ndataset. In our extensive experiments, our model: (i) consistently outperforms\nvariational autoencoders, and conditional and unconditional latent diffusion;\nand (ii) surpasses on most datasets, the performance of supervised inpainting\nmethods with access to paired diseased/healthy images.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10233v1", "AI": {"title_translation": "基于流体驱动异常随机化的脑图像引导异常检测条件扩散模型", "tldr": "本文提出了一种新的条件扩散模型，用于脑部MRI异常检测，利用合成异常来引导健康图像重建，性能优于其他方法。", "motivation": "监督学习在脑部MRI病理检测中需要难以获取的患病数据，尤其对于罕见病。现有基于重建的无监督扩散模型虽然无需患病数据，但往往无法准确重建健康组织或去除异常区域。本文旨在解决这些局限性。", "method": "本文引入了一种新颖的条件扩散模型框架，用于脑部MRI的异常检测和健康图像重建。该方法采用弱监督方式，通过将合成生成的伪病理图像整合到建模过程中，以更好地指导健康图像的重建。伪病理图像通过对辅助数据集中真实病理分割图应用流体驱动异常随机化生成，确保合成异常的真实性和解剖学一致性。模型使用合成异常数据集和来自ATLAS数据集的真实病理进行评估。", "result": "该模型在实验中表现出卓越性能，包括：(i) 持续优于变分自编码器、条件和无条件潜在扩散模型；(ii) 在大多数数据集上，性能超越了可访问配对患病/健康图像的监督修复方法。", "conclusion": "本文成功引入了一种新颖的、采用弱监督方法并利用合成异常的条件扩散模型，该模型在脑图像异常检测中表现出优于现有方法的性能，甚至超越了一些监督方法。", "translation": "监督机器学习已使脑部MRI中的病理检测变得准确，但需要来自患病受试者的训练数据，在某些情况下（例如罕见疾病）可能不容易获得。基于重建的无监督异常检测，特别是使用扩散模型，在医学领域越来越受欢迎，因为它允许仅在健康图像上进行训练，消除了对大型疾病特异性队列的需求。这些方法假设在正常数据上训练的模型无法准确表示或重建异常。然而，这个假设通常会失败，模型无法重建健康组织或准确重建异常区域，即无法去除异常。在这项工作中，我们引入了一种新颖的条件扩散模型框架，用于脑部MRI中的异常检测和健康图像重建。我们的弱监督方法将合成生成的伪病理图像整合到建模过程中，以更好地指导健康图像的重建。为了生成这些伪病理，我们应用流体驱动异常随机化来增强辅助数据集中的真实病理分割图，确保合成异常既真实又解剖学上连贯。我们使用合成异常数据集和来自ATLAS数据集的真实病理来评估我们模型检测病理的能力。在我们广泛的实验中，我们的模型：（i）始终优于变分自编码器、条件和无条件潜在扩散模型；（ii）在大多数数据集上，超越了可访问配对患病/健康图像的监督修复方法的性能。", "summary": "本文提出了一种新颖的弱监督条件扩散模型，用于脑部MRI的异常检测和健康图像重建。该模型通过整合由流体驱动异常随机化生成的合成伪病理图像，解决了传统无监督方法在重建过程中无法准确处理异常的局限性。实验结果表明，所提出的模型在检测脑部病理方面显著优于现有的无监督方法（如VAE和其他扩散模型），甚至在某些情况下超越了监督修复方法。", "keywords": "条件扩散模型, 异常检测, 脑部MRI, 流体驱动异常随机化, 弱监督学习", "comments": "该论文的创新之处在于其弱监督方法，通过使用合成生成且解剖学上连贯的伪病理图像来指导扩散模型进行健康图像重建，有效解决了传统无监督方法中“无法去除异常”的问题。这对于患病数据稀缺的罕见疾病尤为重要。其在性能上超越监督方法的表现也值得关注。"}}
{"id": "2506.10038", "title": "Ambient Diffusion Omni: Training Good Models with Bad Data", "authors": ["Giannis Daras", "Adrian Rodriguez-Munoz", "Adam Klivans", "Antonio Torralba", "Constantinos Daskalakis"], "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.", "comment": "Preprint, work in progress", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10038v1", "AI": {"title_translation": "环境扩散全能：用坏数据训练好模型", "tldr": "Ambient Diffusion Omni框架利用低质量、合成和分布外图像来显著提升扩散模型的性能，通过利用图像的频谱幂律衰减和局部性特性，实现了更好的图像质量和多样性。", "motivation": "扩散模型通常依赖于高度筛选的优质数据集进行训练，导致大量低质量但有价值的图像被丢弃。本文旨在证明并利用这些低质量图像的巨大价值，以改善扩散模型的训练。", "method": "本文提出了Ambient Diffusion Omni框架，一个简单且有原则性的方法，用于训练扩散模型，使其能够从所有可用图像中提取信号。该框架利用了自然图像的两个特性：频谱幂律衰减和局部性。核心洞察是噪声可以抑制期望高质量分布与实际观察到的混合分布之间的初始偏差。", "result": "该框架成功地使用高斯模糊、JPEG压缩和运动模糊合成损坏的图像训练了扩散模型。在使用该框架后，ImageNet FID达到了最先进水平，并且显著提高了文本到图像生成模型的图像质量和多样性。", "conclusion": "噪声能够抑制期望高质量分布与实际观察到的混合分布之间的初始偏差，这是利用低质量数据训练扩散模型的关键洞察。该方法在理论上具有严格的合理性，通过分析学习有偏数据与有限无偏数据在不同扩散时间下的权衡。", "translation": "我们展示了如何使用低质量、合成和分布外图像来提高扩散模型的质量。通常，扩散模型是在从网络和其他来源高度过滤的数据池中整理出的数据集上训练的。我们表明，那些经常被丢弃的较低质量图像具有巨大的价值。我们提出了Ambient Diffusion Omni，一个简单、有原则的框架，用于训练扩散模型，可以在训练过程中从所有可用图像中提取信号。我们的框架利用了自然图像的两个特性——频谱幂律衰减和局部性。我们首先通过成功地使用高斯模糊、JPEG压缩和运动模糊合成损坏的图像来训练扩散模型，从而验证了我们的框架。然后，我们使用我们的框架实现了最先进的ImageNet FID，并且在文本到图像生成模型中显示了图像质量和多样性的显著改进。核心洞察是噪声抑制了期望的高质量分布与我们实际观察到的混合分布之间的初始偏差。我们通过分析在不同扩散时间下从有偏数据学习与从有限无偏数据学习之间的权衡，为我们的方法提供了严格的理论依据。", "summary": "本文提出Ambient Diffusion Omni框架，旨在利用低质量、合成及分布外图像来提升扩散模型的性能。该框架通过利用自然图像的频谱幂律衰减和局部性特性，能够从所有可用图像中提取有效信号。研究表明，利用合成损坏图像可成功训练模型，并在ImageNet FID和文本到图像生成质量多样性方面取得显著提升。核心洞察在于噪声能抑制高质量与混合分布间的初始偏差，并提供了严格的理论分析。", "keywords": "扩散模型, 低质量数据, Ambient Diffusion Omni, 图像生成, 数据增强", "comments": "该论文的创新之处在于挑战了传统扩散模型对高质量、策展数据的高度依赖，并提出了一种有效利用低质量甚至“坏数据”来提升模型性能的新范式。通过利用图像的内在特性（频谱幂律衰减和局部性）和对噪声作用的深刻理解，实现了在数据利用效率和模型性能上的显著突破。这对于数据获取成本高昂或数据量有限的场景具有重要意义。"}}
{"id": "2506.10054", "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs", "authors": ["Shangpin Peng", "Weinong Wang", "Zhuotao Tian", "Senqiao Yang", "Xing Wu", "Haotian Xu", "Chengquan Zhang", "Takashi Isobe", "Baotian Hu", "Min Zhang"], "summary": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10054v1", "AI": {"title_translation": "Omni-DPO：一种用于LLM动态偏好学习的双视角范式", "tldr": "Omni-DPO是一种新的DPO框架，通过同时考虑偏好对质量和模型学习动态来提高LLM的偏好学习效率和性能，并在文本理解和数学推理任务上显著优于现有方法。", "motivation": "现有DPO方法统一处理所有偏好对，忽略其内在质量和学习效用差异，导致数据利用不足和性能不佳。", "method": "提出Omni-DPO，一个双视角优化框架，联合考虑 (1) 每个偏好对的内在质量和 (2) 模型在这些对上的演变性能。通过根据数据质量和模型学习动态自适应加权样本，实现更有效的数据利用。", "result": "在各种模型和基准测试上证明了Omni-DPO的优越性和泛化能力。在文本理解任务上，使用Omni-DPO微调的Gemma-2-9b-it在Arena-Hard基准测试上比Claude 3 Opus高出6.7分。在数学推理任务上，Omni-DPO始终优于所有基准测试的基线方法。", "conclusion": "Omni-DPO通过其双视角优化范式，显著提高了LLM的偏好学习效率和性能，并在多个任务上展现出强大的有效性和鲁棒性。", "translation": "直接偏好优化（DPO）因其简洁和高效已成为人类反馈强化学习（RLHF）的基石。然而，现有的基于DPO的方法通常统一处理所有偏好对，忽略了它们内在质量和学习效用的关键差异，导致数据利用不足和性能欠佳。为了解决这一挑战，我们提出了Omni-DPO，一个双视角优化框架，它联合考虑了 (1) 每个偏好对的内在质量和 (2) 模型在这些对上的演进性能。通过在训练过程中根据数据质量和模型的学习动态自适应地加权样本，Omni-DPO能够更有效地利用训练数据并实现更好的性能。在各种模型和基准测试上的实验结果证明了Omni-DPO的优越性和泛化能力。在文本理解任务中，使用Omni-DPO微调的Gemma-2-9b-it在Arena-Hard基准测试上以6.7分的显著优势击败了领先的LLM Claude 3 Opus。在数学推理任务中，Omni-DPO在所有基准测试中始终优于基线方法，为我们方法的有效性和鲁棒性提供了强有力的经验证据。代码和模型将发布在https://github.com/pspdada/Omni-DPO。", "summary": "本文提出了Omni-DPO，一个用于大型语言模型（LLMs）偏好学习的双视角优化框架。针对现有DPO方法忽视偏好对质量和模型学习动态的问题，Omni-DPO通过自适应加权样本，同时考虑数据质量和模型性能演变，显著提升了数据利用效率和模型性能。实验证明，Omni-DPO在文本理解和数学推理任务上均超越了现有领先方法，展现出优越性和泛化能力。", "keywords": "直接偏好优化, LLM, 强化学习, 偏好学习, 双视角优化", "comments": "Omni-DPO的创新点在于引入了“双视角”优化范式，通过动态调整样本权重来解决DPO中数据质量和模型学习动态被忽略的问题。这对于提升RLHF的效率和效果具有重要意义，尤其是在LLM微调领域，其在多个任务上超越SOTA模型的表现证明了其强大的实用价值。"}}
{"id": "2506.10347", "title": "LightKG: Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture", "authors": ["Yanhui Li", "Dongxia Wang", "Zhu Sun", "Haonan Zhang", "Huizhong Guo"], "summary": "Recently, Graph Neural Networks (GNNs) have become the dominant approach for\nKnowledge Graph-aware Recommender Systems (KGRSs) due to their proven\neffectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL)\nhas been incorporated to address the sparity issue, leading to longer training\ntime. However, through extensive experiments, we reveal that: (1)compared to\nother KGRSs, the existing GNN-based KGRSs fail to keep their superior\nperformance under sparse interactions even with SSL. (2) More complex models\ntend to perform worse in sparse interaction scenarios and complex mechanisms,\nlike attention mechanism, can be detrimental as they often increase learning\ndifficulty. Inspired by these findings, we propose LightKG, a simple yet\npowerful GNN-based KGRS to address sparsity issues. LightKG includes a\nsimplified GNN layer that encodes directed relations as scalar pairs rather\nthan dense embeddings and employs a linear aggregation framework, greatly\nreducing the complexity of GNNs. Additionally, LightKG incorporates an\nefficient contrastive layer to implement SSL. It directly minimizes the node\nsimilarity in original graph, avoiding the time-consuming subgraph generation\nand comparison required in previous SSL methods. Experiments on four benchmark\ndatasets show that LightKG outperforms 12 competitive KGRSs in both sparse and\ndense scenarios while significantly reducing training time. Specifically, it\nsurpasses the best baselines by an average of 5.8\\% in recommendation accuracy\nand saves 84.3\\% of training time compared to KGRSs with SSL. Our code is\navailable at https://github.com/1371149/LightKG.", "comment": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10347v1", "AI": {"title_translation": "LightKG：基于简化GNN架构的高效知识感知推荐系统", "tldr": "LightKG提出了一种简化的GNN架构，用于知识图谱感知的推荐系统，通过简化GNN层和高效的对比学习解决了稀疏交互下的性能下降和训练时间过长问题，并在准确性和训练效率上显著优于现有方法。", "motivation": "现有的基于GNN的知识图谱感知推荐系统（KGRSs）在稀疏交互场景下表现不佳，即使结合了自监督学习（SSL）也未能保持其优越性能。此外，更复杂的模型（如注意力机制）在稀疏场景下反而会降低性能并增加学习难度。", "method": "论文提出了LightKG，它包含一个简化的GNN层，该层将有向关系编码为标量对而非密集嵌入，并采用线性聚合框架，大大降低了GNN的复杂度。此外，LightKG还整合了一个高效的对比层来实现SSL，通过直接最小化原始图中的节点相似度，避免了以往SSL方法中耗时的子图生成和比较。", "result": "LightKG在四个基准数据集上，无论在稀疏还是密集场景下，都优于12种有竞争力的KGRSs。具体而言，它在推荐准确性上平均超过最佳基线5.8%，并且与采用SSL的KGRSs相比，训练时间节省了84.3%。", "conclusion": "LightKG通过简化GNN架构和高效的对比学习方法，有效解决了知识图谱感知推荐系统在稀疏数据下的性能瓶颈和训练效率问题，实现了在准确性和训练时间上的显著提升。", "translation": "最近，图神经网络（GNNs）因其已被证明的有效性，已成为知识图谱感知推荐系统（KGRSs）的主流方法。在基于GNN的KGRSs基础上，自监督学习（SSL）被引入以解决稀疏性问题，但这也导致了更长的训练时间。然而，通过大量的实验，我们发现：(1)与其他的KGRSs相比，现有的基于GNN的KGRSs即使结合了SSL，在稀疏交互下也未能保持其优越性能。(2)更复杂的模型在稀疏交互场景下往往表现更差，并且像注意力机制这样复杂的机制可能是有害的，因为它们通常会增加学习难度。受这些发现的启发，我们提出了LightKG，一个简单而强大的基于GNN的KGRS，以解决稀疏性问题。LightKG包含一个简化的GNN层，该层将有向关系编码为标量对而不是密集嵌入，并采用线性聚合框架，大大降低了GNN的复杂度。此外，LightKG还整合了一个高效的对比层来实现SSL。它直接最小化原始图中的节点相似度，避免了以往SSL方法中耗时的子图生成和比较。在四个基准数据集上的实验表明，LightKG在稀疏和密集场景下均优于12种有竞争力的KGRSs，同时显著减少了训练时间。具体来说，它在推荐准确性上平均超过最佳基线5.8%，并且与采用SSL的KGRSs相比，训练时间节省了84.3%。我们的代码可在https://github.com/1371149/LightKG获取。", "summary": "本文提出了LightKG，一个针对知识图谱感知推荐系统（KGRSs）的简化图神经网络（GNN）模型，旨在解决现有GNN-based KGRSs在稀疏交互下的性能不足和训练效率低下问题。LightKG通过引入一个简化的GNN层（将关系编码为标量对并使用线性聚合）和一个高效的对比学习层（直接最小化节点相似度），显著降低了模型复杂度并优化了自监督学习过程。实验结果表明，LightKG在推荐准确性和训练时间方面均显著优于现有SOTA方法。", "keywords": "知识图谱推荐, 图神经网络, 自监督学习, 稀疏性, 模型简化", "comments": "LightKG的创新点在于其极简主义的设计理念，通过简化GNN架构和优化自监督学习过程，有效解决了复杂模型在稀疏数据下性能下降的问题。它证明了在某些场景下，“少即是多”的原则可以带来更好的性能和效率，这对于资源受限或需要快速迭代的推荐系统尤为重要。"}}
{"id": "2506.10086", "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information", "authors": ["Christodoulos Constantinides", "Shuxin Lin", "Nianjun Zhou", "Dhaval Patel"], "summary": "This paper presents a novel multi-agent system called Chat-of-Thought,\ndesigned to facilitate the generation of Failure Modes and Effects Analysis\n(FMEA) documents for industrial assets. Chat-of-Thought employs multiple\ncollaborative Large Language Model (LLM)-based agents with specific roles,\nleveraging advanced AI techniques and dynamic task routing to optimize the\ngeneration and validation of FMEA tables. A key innovation in this system is\nthe introduction of a Chat of Thought, where dynamic, multi-persona-driven\ndiscussions enable iterative refinement of content. This research explores the\napplication domain of industrial equipment monitoring, highlights key\nchallenges, and demonstrates the potential of Chat-of-Thought in addressing\nthese challenges through interactive, template-driven workflows and\ncontext-aware agent collaboration.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10086v1", "AI": {"title_translation": "Chat-of-Thought：用于生成领域特定信息的协作式多智能体系统", "tldr": "Chat-of-Thought是一个多智能体LLM系统，通过协作和动态任务路由，高效生成工业资产的FMEA文档，并引入了“思维对话”机制进行内容迭代优化。", "motivation": "本文旨在促进工业资产的故障模式和影响分析（FMEA）文档的生成，并解决该领域面临的关键挑战。", "method": "该系统名为Chat-of-Thought，采用多个基于大型语言模型（LLM）的协作代理，每个代理具有特定角色，利用先进的AI技术和动态任务路由来优化FMEA表格的生成和验证。核心创新是引入“思维对话”（Chat of Thought）机制，通过动态的、多角色驱动的讨论实现内容的迭代细化。系统通过交互式、模板驱动的工作流程和上下文感知的代理协作来运行。", "result": "研究展示了Chat-of-Thought系统在解决工业设备监控领域挑战方面的潜力，能够优化FMEA表格的生成和验证。", "conclusion": "Chat-of-Thought系统通过其协作式多智能体方法，在改进FMEA等领域特定文档的生成方面展现出巨大潜力。", "translation": "本文提出了一个名为Chat-of-Thought的新型多智能体系统，旨在促进工业资产的故障模式和影响分析（FMEA）文档的生成。Chat-of-Thought采用多个具有特定角色的协作式大型语言模型（LLM）代理，利用先进的AI技术和动态任务路由来优化FMEA表格的生成和验证。该系统的一个关键创新是引入了“思维对话”（Chat of Thought）机制，通过动态的、多角色驱动的讨论实现内容的迭代细化。本研究探讨了工业设备监控的应用领域，强调了关键挑战，并展示了Chat-of-Thought通过交互式、模板驱动的工作流程和上下文感知的代理协作来解决这些挑战的潜力。", "summary": "本文介绍了一种新颖的多智能体系统Chat-of-Thought，旨在自动化和优化工业资产的故障模式和影响分析（FMEA）文档生成。该系统利用多个协作式LLM代理，结合动态任务路由和创新的“思维对话”机制进行内容迭代细化。研究展示了Chat-of-Thought在工业设备监控领域中解决相关挑战的有效潜力。", "keywords": "Chat-of-Thought, 多智能体系统, LLM, FMEA, 协作式AI", "comments": "该论文的创新点在于引入了“思维对话”机制，通过多角色LLM代理的动态讨论来迭代优化内容，这对于复杂领域（如FMEA）的文档生成具有重要意义。系统化地利用多智能体协作和LLM的强大能力，为自动化和提高领域特定信息生成的质量提供了一个有前景的解决方案。"}}
{"id": "2506.10085", "title": "Test-Time Adaptation for Generalizable Task Progress Estimation", "authors": ["Christos Ziakas", "Alessandra Russo"], "summary": "We propose a test-time adaptation method that enables a progress estimation\nmodel to adapt online to the visual and temporal context of test trajectories\nby optimizing a learned self-supervised objective. To this end, we introduce a\ngradient-based meta-learning strategy to train the model on expert visual\ntrajectories and their natural language task descriptions, such that test-time\nadaptation improves progress estimation relying on semantic content over\ntemporal order. Our test-time adaptation method generalizes from a single\ntraining environment to diverse out-of-distribution tasks, environments, and\nembodiments, outperforming the state-of-the-art in-context learning approach\nusing autoregressive vision-language models.", "comment": "pages, 2 figures, accepted to the 2nd Workshop on Test-Time\n  Adaptation: Putting Updates to the Test (PUT) at 42nd International\n  Conference on Machine Learning (ICML), Vancouver, Canada, 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10085v1", "AI": {"title_translation": "用于可泛化任务进度估计的测试时适应", "tldr": "本文提出了一种测试时适应方法，通过优化自监督目标，使任务进度估计模型能够在线适应测试轨迹的视觉和时间上下文，并在多样化的OOD任务、环境和实体上表现优于SOTA方法。", "motivation": "为了使进度估计模型能够在线适应测试轨迹的视觉和时间上下文，并提高其在多样化、分布外任务中的泛化能力。", "method": "提出了一种测试时适应方法，通过优化学习到的自监督目标，使进度估计模型能够在线适应测试轨迹。为此，引入了一种基于梯度的元学习策略，用于在专家视觉轨迹及其自然语言任务描述上训练模型，以使测试时适应能够通过依赖语义内容而非时间顺序来改进进度估计。", "result": "测试时适应方法能够从单一训练环境泛化到多样化的分布外任务、环境和实体，并且优于使用自回归视觉-语言模型的最先进的上下文学习方法。", "conclusion": "所提出的测试时适应方法通过在线适应测试轨迹的视觉和时间上下文，显著提高了任务进度估计的泛化能力，并在多样化的分布外场景中取得了优于现有技术的效果。", "translation": "我们提出了一种测试时适应方法，该方法通过优化学习到的自监督目标，使进度估计模型能够在线适应测试轨迹的视觉和时间上下文。为此，我们引入了一种基于梯度的元学习策略，用于在专家视觉轨迹及其自然语言任务描述上训练模型，以使测试时适应能够通过依赖语义内容而非时间顺序来改进进度估计。我们的测试时适应方法能够从单一训练环境泛化到多样化的分布外任务、环境和实体，并且优于使用自回归视觉-语言模型的最先进的上下文学习方法。", "summary": "该论文提出了一种用于任务进度估计的测试时适应方法。该方法通过优化一个学习到的自监督目标，使模型能够在线适应测试轨迹的视觉和时间上下文。通过引入一种基于梯度的元学习策略，模型在专家视觉轨迹和自然语言任务描述上进行训练，使得测试时适应能够优先利用语义内容而非时间顺序来提高进度估计的准确性。实验证明，该方法在从单一训练环境泛化到多样化的分布外任务、环境和实体方面表现出色，并超越了当前最先进的自回归视觉-语言模型所采用的上下文学习方法。", "keywords": "测试时适应, 任务进度估计, 元学习, 自监督学习, 泛化能力", "comments": "该论文的创新点在于提出了将测试时适应与元学习相结合，以提高任务进度估计在复杂和多样化环境中的泛化能力。通过强调语义内容而非时间顺序，解决了传统方法在处理分布外数据时的局限性。其重要性在于为机器人学习和自动化任务监控等领域提供了更鲁棒的进度估计解决方案。"}}
{"id": "2506.10274", "title": "Discrete Audio Tokens: More Than a Survey!", "authors": ["Pooneh Mousavi", "Gallil Maimon", "Adel Moumen", "Darius Petermann", "Jiatong Shi", "Haibin Wu", "Haici Yang", "Anastasia Kuznetsova", "Artem Ploujnikov", "Ricard Marxer", "Bhuvana Ramabhadran", "Benjamin Elizalde", "Loren Lugosch", "Jinyu Li", "Cem Subakan", "Phil Woodland", "Minje Kim", "Hung-yi Lee", "Shinji Watanabe", "Yossi Adi", "Mirco Ravanelli"], "summary": "Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks.They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10274v1", "AI": {"title_translation": "离散音频Tokens：不仅仅是综述！", "tldr": "本文对离散音频Token化方法进行了系统性回顾和基准测试，涵盖语音、音乐和通用音频三个领域，提出了分类法，评估了不同Token化器，并指出了未来的研究方向。", "motivation": "现有关于离散音频Token化的研究和综述通常侧重于特定领域或任务，缺乏在各种基准测试中进行统一比较。", "method": "本文对离散音频Token化器进行了系统性回顾和基准测试，涵盖语音、音乐和通用音频三个领域。文章提出了一种基于编码器-解码器、量化技术、训练范式、可流式传输性和应用领域的Token化方法分类法。研究人员在重建、下游任务性能和声学语言建模等多个基准上评估了Token化器，并通过受控消融研究分析了权衡。", "result": "研究结果揭示了关键局限性、实际考虑因素和开放挑战，为该快速发展领域的未来研究提供了见解和指导。", "conclusion": "离散音频Token化在整合语音和音频到大型语言模型中具有巨大潜力，但仍存在关键局限性、实际考虑和开放挑战，需要未来研究的深入探索。", "translation": "离散音频Token是一种紧凑的表示形式，旨在保持感知质量、语音内容和说话者特征，同时实现高效存储和推理，并在各种下游任务中表现出具有竞争力的性能。它们为连续特征提供了一种实用的替代方案，使语音和音频能够集成到现代大型语言模型（LLMs）中。随着对基于Token的音频处理的兴趣日益增长，各种Token化方法应运而生，并且已经有几篇综述回顾了该领域的最新进展。然而，现有研究通常侧重于特定领域或任务，缺乏对各种基准的统一比较。本文对离散音频Token化器进行了系统性回顾和基准测试，涵盖语音、音乐和通用音频三个领域。我们提出了一种基于编码器-解码器、量化技术、训练范式、可流式传输性和应用领域的Token化方法分类法。我们在重建、下游性能和声学语言建模等多个基准上评估了Token化器，并通过受控消融研究分析了权衡。我们的发现强调了关键局限性、实际考虑因素和开放挑战，为这一快速发展领域的未来研究提供了见解和指导。有关更多信息，包括我们的主要结果和Token化器数据库，请访问我们的网站：https://poonehmousavi.github.io/dates-website/。", "summary": "本文对离散音频Token化方法进行了系统性回顾和基准测试，旨在弥补现有研究缺乏统一比较的不足。研究涵盖语音、音乐和通用音频三大领域，提出了一种详细的Token化方法分类法，并基于多项基准对不同的Token化器进行了评估和权衡分析。研究结果揭示了该领域的关键局限性、实际应用考虑及未来面临的挑战，为后续研究提供了方向。", "keywords": "离散音频Token, 音频Token化, 系统综述, 基准测试, 大型语言模型", "comments": "本文超越了简单的文献综述，通过系统性的基准测试和分类法，为离散音频Token化领域提供了全面的分析。其创新之处在于统一了不同领域的评估标准，并深入剖析了现有方法的局限性，对推动音频与LLM的融合具有重要指导意义。"}}
{"id": "2506.10243", "title": "R-PINN: Recovery-type a-posteriori estimator enhanced adaptive PINN", "authors": ["Rongxin Lu", "Jiwei Jia", "Young Ju Lee", "Zheng Lu", "Chensong Zhang"], "summary": "In recent years, with the advancements in machine learning and neural\nnetworks, algorithms using physics-informed neural networks (PINNs) to solve\nPDEs have gained widespread applications. While these algorithms are\nwell-suited for a wide range of equations, they often exhibit suboptimal\nperformance when applied to equations with large local gradients, resulting in\nsubstantial localized errors. To address this issue, this paper proposes an\nadaptive PINN algorithm designed to improve accuracy in such cases. The core\nidea of the algorithm is to adaptively adjust the distribution of collocation\npoints based on the recovery-type a-posterior error of the current numerical\nsolution, enabling a better approximation of the true solution. This approach\nis inspired by the adaptive finite element method. By combining the\nrecovery-type a-posteriori estimator, a gradient-recovery estimator commonly\nused in the adaptive finite element method (FEM) with PINNs, we introduce the\nRecovery-type a-posteriori estimator enhanced adaptive PINN (R-PINN) and\ncompare its performance with a typical adaptive PINN algorithm, FI-PINN. Our\nresults demonstrate that R-PINN achieves faster convergence with fewer adaptive\npoints and significantly outperforms in the cases with multiple regions of\nlarge errors than FI-PINN. Notably, our method is a hybrid numerical approach\nfor solving partial differential equations, integrating adaptive FEM with\nPINNs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10243v1", "AI": {"title_translation": "R-PINN：恢复型后验估计器增强的自适应PINN", "tldr": "提出R-PINN，一种结合恢复型后验估计器的自适应物理信息神经网络（PINN），旨在解决偏微分方程在局部大梯度区域的精度问题，通过自适应调整配置点分布，相比现有方法能更快收敛并减少误差。", "motivation": "现有物理信息神经网络（PINN）在应用于具有大局部梯度的偏微分方程时，表现出次优性能，导致显著的局部误差。", "method": "本文提出R-PINN算法，其核心思想是根据当前数值解的恢复型后验误差自适应调整配置点的分布。该方法受自适应有限元方法启发，将梯度恢复估计器（恢复型后验估计器）与PINN结合。", "result": "R-PINN在自适应点更少的情况下实现了更快的收敛，并且在具有多个大误差区域的情况下显著优于典型的自适应PINN算法FI-PINN。", "conclusion": "R-PINN通过融合自适应有限元方法和PINN，提供了一种求解偏微分方程的混合数值方法，有效提升了PINN在处理大梯度问题时的精度和效率。", "translation": "近年来，随着机器学习和神经网络的进步，使用物理信息神经网络（PINN）求解偏微分方程（PDE）的算法得到了广泛应用。虽然这些算法适用于各种方程，但当应用于具有大局部梯度的方程时，它们通常表现不佳，导致显著的局部误差。为了解决这个问题，本文提出了一种自适应PINN算法，旨在提高此类情况下的精度。该算法的核心思想是根据当前数值解的恢复型后验误差自适应调整配置点的分布，从而更好地逼近真实解。这种方法受到自适应有限元方法的启发。通过将自适应有限元方法（FEM）中常用的梯度恢复估计器——恢复型后验估计器与PINN结合，我们引入了恢复型后验估计器增强的自适应PINN（R-PINN），并将其性能与典型的自适应PINN算法FI-PINN进行了比较。我们的结果表明，R-PINN在自适应点更少的情况下实现了更快的收敛，并且在具有多个大误差区域的情况下显著优于FI-PINN。值得注意的是，我们的方法是一种用于求解偏微分方程的混合数值方法，它将自适应FEM与PINN进行了集成。", "summary": "本文针对物理信息神经网络（PINN）在求解具有大局部梯度的偏微分方程时存在的精度问题，提出了一种名为R-PINN的自适应算法。该算法借鉴自适应有限元方法，利用恢复型后验误差估计器动态调整配置点分布，以提高解的精度。实验结果表明，R-PINN相比现有方法（如FI-PINN）能以更少的自适应点实现更快收敛，并在多误差区域表现出显著优势。该方法是一种结合自适应有限元方法与PINN的混合数值方法。", "keywords": "物理信息神经网络, 自适应算法, 后验估计器, 偏微分方程, 混合数值方法", "comments": "本文的创新之处在于将自适应有限元方法中常用的恢复型后验误差估计器引入到物理信息神经网络中，有效地解决了PINN在处理具有大梯度问题时精度不足的挑战。这种将传统数值方法与深度学习相结合的混合数值方法为偏微分方程的求解提供了新的思路，具有重要的理论和应用价值。"}}
{"id": "2506.10039", "title": "Symbolic Generation and Modular Embedding of High-Quality abc-Triples", "authors": ["Michael A. Idowu"], "summary": "We present a symbolic identity for generating integer triples $(a, b, c)$\nsatisfying $a + b = c$, inspired by structural features of the \\emph{abc\nconjecture}. The construction uses powers of $2$ and $3$ in combination with\nmodular inversion in $\\mathbb{Z}/3^p\\mathbb{Z}$, leading to a parametric\nidentity with residue constraints that yield abc-triples exhibiting low radical\nvalues. Through affine transformations, these symbolic triples are embedded\ninto a broader space of high-quality examples, optimised for the ratio $\\log c\n/ \\log \\operatorname{rad}(abc)$. Computational results demonstrate the\nemergence of structured, radical-minimising candidates, including both known\nand novel triples. These methods provide a symbolic and algebraic framework for\ncontrolled triple generation, and suggest exploratory implications for symbolic\nentropy filtering in cryptographic pre-processing.", "comment": "17 pages, includes tables and illustrative examples; discusses\n  symbolic generation of abc-triples and applications in entropy filtering and\n  cryptographic pre-processing", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10039v1", "AI": {"title_translation": "高质量abc三元组的符号生成与模块化嵌入", "tldr": "本文提出了一种符号恒等式和模块化嵌入方法，用于生成具有低根值的高质量abc三元组，并展示了结构化的、最小化根值的候选三元组。", "motivation": "受abc猜想结构特征的启发，以及对可控生成具有低根值的三元组的需求，可能应用于密码学预处理。", "method": "提出了一种用于生成满足a + b = c的整数三元组(a, b, c)的符号恒等式。该构造结合了2和3的幂次以及在Z/3^pZ中的模逆运算，从而得到一个带有余数约束的参数恒等式。通过仿射变换，这些符号三元组被嵌入到一个更广阔的高质量示例空间中，并针对比率log c / log rad(abc)进行了优化。", "result": "产生了具有低根值的abc三元组。计算结果表明，出现了结构化的、根值最小化的候选三元组，包括已知和新颖的三元组。", "conclusion": "这些方法为可控的三元组生成提供了一个符号和代数框架，并暗示了符号熵过滤在密码学预处理中的探索性应用。", "translation": "我们提出了一种用于生成满足a + b = c的整数三元组(a, b, c)的符号恒等式，其灵感来源于abc猜想的结构特征。该构造结合了2和3的幂次以及在Z/3^pZ中的模逆运算，从而得到一个带有余数约束的参数恒等式，该恒等式产生具有低根值的abc三元组。通过仿射变换，这些符号三元组被嵌入到一个更广阔的高质量示例空间中，并针对比率log c / log rad(abc)进行了优化。计算结果表明，出现了结构化的、根值最小化的候选三元组，包括已知和新颖的三元组。这些方法为可控的三元组生成提供了一个符号和代数框架，并暗示了符号熵过滤在密码学预处理中的探索性应用。", "summary": "本文提出了一种符号恒等式和模块化嵌入技术，用于生成高质量的abc三元组。该方法通过结合2和3的幂次与模逆运算，并通过仿射变换，生成具有低根值的三元组，并针对log c / log rad(abc)比率进行优化。计算结果表明，该方法能够生成结构化的、根值最小化的三元组，包括新的示例，为受控的三元组生成提供了一个框架，并可能应用于密码学预处理。", "keywords": "abc三元组, 符号生成, 模块化嵌入, 根值, 密码学", "comments": "该论文为abc三元组的生成提供了一种创新的符号和代数方法，这对于数论以及潜在的密码学，特别是预处理任务，都具有重要意义。专注于最小化根值是其关键贡献。"}}
{"id": "2506.10974", "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science", "authors": ["Yixin Ou", "Yujie Luo", "Jingsheng Zheng", "Lanning Wei", "Shuofei Qiao", "Jintian Zhang", "Da Zheng", "Huajun Chen", "Ningyu Zhang"], "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.", "comment": "Ongoing work. Code is at https://github.com/innovatingAI/AutoMind", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10974v1", "AI": {"title_translation": "AutoMind：自适应知识型智能体用于自动化数据科学", "tldr": "AutoMind是一个自适应、知识型LLM智能体框架，通过专家知识库、知识型树搜索和自适应编码策略，解决了现有LLM数据科学智能体的局限性，并在基准测试中表现优异。", "motivation": "现有LLM驱动的数据科学智能体依赖僵化、预定义的工作流和不灵活的编码策略，导致它们仅在简单问题上表现出色，无法捕捉人类在复杂任务中的经验专长。", "method": "AutoMind通过三个关键进展克服了这些不足：(1) 一个精选的专家知识库，将智能体建立在领域专家知识之上；(2) 一个代理式知识型树搜索算法，策略性地探索可能的解决方案；(3) 一个自适应编码策略，动态调整代码生成以适应任务复杂性。", "result": "AutoMind在两个自动化数据科学基准测试中表现出优于现有基线的性能。额外的分析证实了其有利的有效性、效率和定性解决方案质量。", "conclusion": "AutoMind是迈向全自动化数据科学的有效且稳健的一步。", "translation": "大型语言模型（LLM）智能体在解决现实世界数据科学问题方面展现出巨大潜力。LLM驱动的数据科学智能体有望自动化整个机器学习流程，但其在现实世界中的有效性仍然有限。现有框架依赖僵化、预定义的工作流和不灵活的编码策略；因此，它们仅在相对简单、经典的问题上表现出色，并且无法捕捉人类实践者在复杂、创新任务中带来的经验专长。在这项工作中，我们介绍了AutoMind，一个自适应、知识型LLM智能体框架，它通过三项关键进展克服了这些不足：(1) 一个精选的专家知识库，将智能体建立在领域专家知识之上，(2) 一个代理式知识型树搜索算法，策略性地探索可能的解决方案，以及 (3) 一个自适应编码策略，动态调整代码生成以适应任务复杂性。在两个自动化数据科学基准测试上的评估表明，AutoMind比现有最先进的基线提供了卓越的性能。额外的分析证实了有利的有效性、效率和定性解决方案质量，突显了AutoMind是迈向全自动化数据科学的有效且稳健的一步。", "summary": "本文介绍了AutoMind，一个自适应、知识型LLM智能体框架，旨在解决现有LLM数据科学智能体在复杂任务中表现不足的问题。AutoMind通过集成专家知识库、代理式知识型树搜索算法和自适应编码策略来增强其能力。实验结果表明，AutoMind在自动化数据科学基准测试中超越了现有技术，并在有效性、效率和解决方案质量方面表现出色，预示着其在实现全自动化数据科学方面的潜力。", "keywords": "LLM智能体, 自动化数据科学, 知识型智能体, 自适应编码, 专家知识库", "comments": "AutoMind的创新之处在于其结合了领域专家知识、策略性搜索和自适应编码，解决了现有LLM智能体僵化、缺乏经验专长的问题。这使其能够更好地处理复杂的数据科学任务，是推动全自动化数据科学发展的重要一步。"}}
{"id": "2506.10422", "title": "A Hybrid Heuristic Framework for Resource-Efficient Querying of Scientific Experiments Data", "authors": ["Mayank Patel", "Minal Bhise"], "summary": "Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.10422v1", "AI": {"title_translation": "一种用于科学实验数据资源高效查询的混合启发式框架", "tldr": "本文提出了一种轻量级的资源感知混合框架（RAW-HF），通过优化资源利用率来高效查询科学实验原始数据，显著减少了查询执行时间和资源消耗。", "motivation": "传统的数据库管理系统（DBMS）和HTAP系统在查询执行前需要耗费大量时间和资源加载整个数据集，而原位引擎可能多次重复解析所需数据，导致资源利用率和数据处理成本增加。此外，资源的过度或不足分配也会增加应用程序运行成本。", "method": "本文提出了一种轻量级的资源可用性与工作负载感知混合框架（RAW-HF），旨在通过高效利用现有有限资源来优化原始数据查询。RAW-HF包含有助于优化执行给定工作负载所需资源和最大化现有资源利用率的模块。它还比较了RAW-HF使用的MUAR技术与基于机器学习的资源分配技术（如PCC）。", "result": "RAW-HF应用于Sloan Digital Sky Survey (SDSS)和Linked Observation Data (LOD)等真实世界科学数据集工作负载时，与广泛使用的传统DBMS PostgreSQL相比，工作负载执行时间（WET）分别减少了90%和85%以上。与最先进的工作负载感知部分加载技术（WA）相比，RAW-HF将整体CPU、IO资源利用率和WET分别降低了26%、25%和26%，同时内存利用率提高了33%。", "conclusion": "RAW-HF框架通过优化资源利用，显著提高了科学实验数据查询的效率，并在实际应用中展现出优于传统DBMS和现有混合系统的性能，有效降低了数据处理成本。", "translation": "科学实验和现代应用程序每天都会生成大量数据。大多数组织利用内部服务器或云资源来管理应用程序数据和工作负载。传统的数据库管理系统（DBMS）和HTAP系统在开始查询执行之前，需要花费大量时间和资源加载整个数据集。另一方面，原位引擎可能会多次重新解析所需数据，从而增加资源利用率和数据处理成本。此外，资源的过度或不足分配也会增加应用程序运行成本。本文提出了一种轻量级的资源可用性与工作负载感知混合框架（RAW-HF），旨在通过高效利用现有有限资源来优化原始数据查询。RAW-HF包含有助于优化执行给定工作负载所需资源和最大化现有资源利用率的模块。将RAW-HF应用于Sloan Digital Sky Survey (SDSS)和Linked Observation Data (LOD)等真实世界科学数据集工作负载的结果表明，与广泛使用的传统DBMS PostgreSQL相比，工作负载执行时间（WET）分别减少了90%和85%以上。与最先进的工作负载感知部分加载技术（WA）相比，整体CPU、IO资源利用率和WET分别降低了26%、25%和26%，同时内存利用率提高了33%。本文还比较了RAW-HF使用的MUAR技术与基于机器学习的资源分配技术（如PCC）。", "summary": "本文提出了一种名为RAW-HF的轻量级混合框架，旨在解决传统数据库和原位引擎在处理大规模科学实验数据时存在的资源效率低下和成本高昂的问题。RAW-HF通过其模块优化资源分配和利用率，从而高效查询原始数据。实验结果表明，与PostgreSQL和现有混合系统相比，RAW-HF显著减少了工作负载执行时间，并提高了资源利用效率，有效降低了数据处理成本。", "keywords": "资源高效查询, 混合框架, 科学数据, RAW-HF, 工作负载感知", "comments": "本文的创新点在于提出了一个轻量级的混合启发式框架RAW-HF，它结合了资源可用性和工作负载感知，以优化科学实验数据的查询。其重要性在于解决了大数据背景下数据查询的资源效率瓶颈，尤其是在科学数据领域。通过实际数据集的验证，展示了显著的性能提升，表明其在实际应用中具有很高的价值。"}}
{"id": "2506.10204", "title": "Prompt Variability Effects On LLM Code Generation", "authors": ["Andrei Paleyes", "Radzim Sendyka", "Diana Robinson", "Christian Cabrera", "Neil D. Lawrence"], "summary": "Code generation is one of the most active areas of application of Large\nLanguage Models (LLMs). While LLMs lower barriers to writing code and\naccelerate development process, the overall quality of generated programs\ndepends on the quality of given prompts. Specifically, functionality and\nquality of generated code can be sensitive to user's background and familiarity\nwith software development. It is therefore important to quantify LLM's\nsensitivity to variations in the input. To this end we propose a synthetic\nevaluation pipeline for code generation with LLMs, as well as a systematic\npersona-based evaluation approach to expose qualitative differences of LLM\nresponses dependent on prospective user background. Both proposed methods are\ncompletely independent from specific programming tasks and LLMs, and thus are\nwidely applicable. We provide experimental evidence illustrating utility of our\nmethods and share our code for the benefit of the community.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10204v1", "AI": {"title_translation": "提示词变异对大型语言模型代码生成的影响", "tldr": "本文提出了一种合成评估流程和基于角色的评估方法，用于量化大型语言模型在代码生成中对输入提示词变化的敏感性。", "motivation": "大型语言模型生成的代码质量受提示词质量影响，且对用户背景敏感。因此，量化大型语言模型对输入变化的敏感性至关重要。", "method": "提出了一种用于大型语言模型代码生成的合成评估流程，以及一种系统的、基于角色的评估方法，以揭示大型语言模型响应在用户背景方面的定性差异。这两种方法都独立于特定的编程任务和大型语言模型。", "result": "提供了实验证据，说明了所提出方法的实用性。", "conclusion": "本文提出的评估方法能够有效地量化大型语言模型在代码生成中对提示词变化的敏感性，并具有广泛的适用性。", "translation": "大型语言模型（LLM）的代码生成是其最活跃的应用领域之一。虽然LLM降低了编写代码的门槛并加速了开发过程，但生成程序的整体质量取决于给定提示词的质量。具体而言，生成代码的功能和质量可能对用户的背景和软件开发熟悉程度敏感。因此，量化LLM对输入变化的敏感性至关重要。为此，我们提出了一种用于LLM代码生成的合成评估流程，以及一种系统的、基于角色的评估方法，以揭示LLM响应在潜在用户背景方面的定性差异。这两种提出的方法完全独立于特定的编程任务和LLM，因此具有广泛的适用性。我们提供了实验证据，说明了我们方法的实用性，并分享了我们的代码以造福社区。", "summary": "本文研究了大型语言模型（LLM）在代码生成中对提示词变化的敏感性。鉴于生成代码的质量取决于提示词且受用户背景影响，作者提出了一种合成评估流程和一种系统性的、基于角色的评估方法。这两种方法独立于具体的编程任务和LLM，具有广泛适用性。实验证据表明了这些方法的实用性，并共享了代码。", "keywords": "大型语言模型, 代码生成, 提示词变异, 评估方法, 用户背景", "comments": "本文的创新之处在于提出了两种独立于具体编程任务和大型语言模型的评估方法，用于量化LLM代码生成对提示词变化的敏感性。这对于理解和提高LLM在实际应用中的代码生成质量具有重要意义。"}}
{"id": "2506.10239", "title": "A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures", "authors": ["Maximilian Mühlbauer", "Freek Stulp", "Sylvain Calinon", "Alin Albu-Schäffer", "João Silvério"], "summary": "Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the\nmost suitable haptic feedback for each phase of a task, based on learned or\nperceived uncertainty. While keeping the human in the loop remains essential,\nfor instance, to ensure high precision, partial automation of certain task\nphases is critical for productivity. We present a unified framework for\nprobabilistic VFs that seamlessly switches between manual fixtures,\nsemi-automated fixtures (with the human handling precise tasks), and full\nautonomy. We introduce a novel probabilistic Dynamical System-based VF for\ncoarse guidance, enabling the robot to autonomously complete certain task\nphases while keeping the human operator in the loop. For tasks requiring\nprecise guidance, we extend probabilistic position-based trajectory fixtures\nwith automation allowing for seamless human interaction as well as\ngeometry-awareness and optimal impedance gains. For manual tasks requiring very\nprecise guidance, we also extend visual servoing fixtures with the same\ngeometry-awareness and impedance behaviour. We validate our approach\nexperimentally on different robots, showcasing multiple operation modes and the\nease of programming fixtures.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10239v1", "AI": {"title_translation": "一种统一的概率动态、轨迹和视觉虚拟夹具框架", "tldr": "本文提出了一种统一的概率虚拟夹具框架，可以根据任务阶段的需要，在手动、半自动化和完全自动化模式之间无缝切换，以提高生产力并保持高精度。", "motivation": "虽然保持人类在回路中对于确保高精度至关重要，但任务某些阶段的部分自动化对于提高生产力至关重要。现有的概率虚拟夹具能够根据学习或感知的 Haptic 反馈进行自适应选择，但缺乏一个统一的框架来无缝切换不同的自动化级别。", "method": "本文提出了一种统一的概率虚拟夹具（VFs）框架，该框架可以在手动夹具、半自动化夹具（人类处理精确任务）和完全自主之间无缝切换。具体方法包括：1. 引入一种新颖的概率动态系统（DS）虚拟夹具，用于粗略引导，使机器人能够自主完成某些任务阶段，同时保持人类操作员在回路中。2. 扩展概率基于位置的轨迹夹具，增加自动化功能，以实现无缝人机交互、几何感知和最佳阻抗增益，用于需要精确引导的任务。3. 扩展视觉伺服夹具，增加相同的几何感知和阻抗行为，用于需要非常精确引导的手动任务。", "result": "该方法在不同的机器人上进行了实验验证，展示了多种操作模式以及编程夹具的简易性。", "conclusion": "本文成功提出了一个统一的概率虚拟夹具框架，该框架能够无缝切换不同的自动化级别，有效结合了人类操作的精度和机器人自动化的效率，并通过实验验证了其在不同机器人上的有效性和易用性。", "translation": "概率虚拟夹具（VFs）能够根据学习或感知的 E. Uncertainty，为任务的每个阶段自适应选择最合适的触觉反馈。虽然保持人类在回路中仍然至关重要，例如，为了确保高精度，但任务某些阶段的部分自动化对于提高生产力至关重要。我们提出了一种统一的概率 VFs 框架，该框架可以在手动夹具、半自动化夹具（人类处理精确任务）和完全自主之间无缝切换。我们引入了一种新颖的基于概率动力系统（Dynamical System-based）的 VF，用于粗略引导，使机器人能够自主完成某些任务阶段，同时保持人类操作员在回路中。对于需要精确引导的任务，我们扩展了基于概率位置的轨迹夹具，增加了自动化功能，以实现无缝人机交互以及几何感知和最佳阻抗增益。对于需要非常精确引导的手动任务，我们还扩展了视觉伺服夹具，增加了相同的几何感知和阻抗行为。我们在不同的机器人上通过实验验证了我们的方法，展示了多种操作模式以及编程夹具的简易性。", "summary": "本文提出了一个统一的概率虚拟夹具框架，旨在提高机器人辅助任务的生产力，同时保持人类操作员在回路中的高精度。该框架能够根据任务需求在手动、半自动化和完全自主模式之间无缝切换。它引入了基于概率动力系统的新型虚拟夹具用于粗略引导，并扩展了基于轨迹和视觉伺服的夹具，以实现精确引导、几何感知和优化阻抗。实验验证了该方法在不同机器人上的有效性、多模式操作能力以及编程的便捷性。", "keywords": "概率虚拟夹具, 统一框架, 机器人辅助, 人机交互, 自动化", "comments": "这项研究的创新之处在于提出了一个统一的框架，能够整合不同自动化程度的虚拟夹具，并实现无缝切换。这解决了在机器人辅助任务中平衡效率和精度的关键挑战。通过结合动态系统、轨迹和视觉伺服方法，该框架为操作员提供了灵活且适应性强的支持。其重要性在于能够显著提高复杂任务的生产力，同时保持人类在回路中进行关键的精确操作。"}}
{"id": "2506.10249", "title": "Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations", "authors": ["Andrea Gaggioli", "Sabrina Bartolotta", "Andrea Ubaldi", "Katusha Gerardini", "Eleonora Diletta Sarcinella", "Alice Chirico"], "summary": "Artificial Intelligence holds significant potential to enhance human\ncreativity. However, achieving this vision requires a clearer understanding of\nhow such enhancement can be effectively realized. Adopting the perspective of\ndistributed creativity, we identify three primary modes through which AI can\ncontribute to creative processes: Support, where AI acts as a tool; Synergy,\nwhere AI and humans collaborate in complementary ways; and Symbiosis, where\nhuman and AI cognition become so integrated that they form a unified creative\nsystem. These modes are defined along two key dimensions: the level of\ntechnical autonomy exhibited by the AI system and the degree of perceived\nagency attributed to it. We examine how each configuration influences different\nlevels of creativity - from everyday problem-solving to paradigm-shifting\ninnovation - and discuss the theoretical, ethical, and design implications.", "comment": "36 pages, 3 figures. This conceptual paper proposes a taxonomy of\n  Extended Creativity systems and examines the relational dynamics between\n  human and AI agents in creative processes. Suitable for readers in HCI, AI,\n  cognitive science, and digital design. The illustrations were created by\n  Francesco Giordano and are used with permission (not under CC license)", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10249v1", "AI": {"title_translation": "扩展创造力：理解人机创造性关系的概念框架", "tldr": "本文提出了一个概念框架，用于理解AI如何通过支持、协同和共生三种模式增强人类创造力，并讨论了其影响。", "motivation": "尽管人工智能在增强人类创造力方面具有巨大潜力，但如何有效地实现这种增强仍缺乏清晰的理解。", "method": "论文采用分布式创造力的视角，提出了AI贡献于创造过程的三种主要模式（支持、协同、共生），并根据AI系统所展示的技术自主性水平和归因于它的感知能动性程度来定义这些模式。", "result": "识别出AI增强人类创造力的三种模式：支持（AI作为工具）、协同（人机互补协作）和共生（人机认知高度整合）。并探讨了每种配置如何影响不同层级的创造力。", "conclusion": "论文讨论了所提出的模式在理论、伦理和设计方面的含义，为理解和实现人机创造性增强提供了指导。", "translation": "人工智能在增强人类创造力方面具有巨大潜力。然而，实现这一愿景需要更清晰地理解如何有效地实现这种增强。本文采用分布式创造力的视角，识别出人工智能可以为创造过程做出贡献的三种主要模式：支持（AI充当工具）、协同（AI和人类以互补方式协作）和共生（人类和AI的认知高度整合，形成一个统一的创造系统）。这些模式是根据两个关键维度定义的：AI系统所展示的技术自主性水平以及归因于它的感知能动性程度。我们研究了每种配置如何影响不同水平的创造力——从日常问题解决到范式转变的创新——并讨论了其理论、伦理和设计含义。", "summary": "本文提出了一个名为“扩展创造力”的概念框架，旨在阐明人工智能如何有效增强人类创造力。该框架基于分布式创造力视角，识别了AI在创造过程中发挥作用的三种模式：支持（AI为工具）、协同（人机互补协作）和共生（人机认知高度整合）。这些模式通过AI的技术自主性和感知能动性程度来界定。论文探讨了这些模式对不同创造力水平的影响，并讨论了其理论、伦理和设计层面的启示。", "keywords": "人工智能, 创造力, 人机协作, 概念框架, 分布式创造力", "comments": "这篇论文通过提出“支持、协同、共生”三种人机创造力关系模式，提供了一个清晰且富有洞察力的概念框架，有助于理解和指导AI在创造力领域的应用。其创新之处在于将AI的自主性和感知能动性作为关键维度，对人机协作的复杂性进行了细致的划分，这对于未来AI辅助创意工具的设计和伦理考量具有重要指导意义。"}}
{"id": "2506.10925", "title": "Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence", "authors": ["Eduardo Baena", "Paolo Testolina", "Michele Polese", "Sergi Aliaga", "Andrew Benincasa", "Dimitrios Koutsonikolas", "Josep Jornet", "Tommaso Melodia"], "summary": "Lunar surface operations impose stringent requirements on wireless\ncommunication systems, including autonomy, robustness to disruption, and the\nability to adapt to environmental and mission-driven context. While Space-O-RAN\nprovides a distributed orchestration model aligned with 3GPP standards, its\ndecision logic is limited to static policies and lacks semantic integration. We\npropose a novel extension incorporating a semantic agentic layer enabled by the\nModel Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols,\nallowing context-aware decision making across real-time, near-real-time, and\nnon-real-time control layers. Distributed cognitive agents deployed in rovers,\nlanders, and lunar base stations implement wireless-aware coordination\nstrategies, including delay-adaptive reasoning and bandwidth-aware semantic\ncompression, while interacting with multiple MCP servers to reason over\ntelemetry, locomotion planning, and mission constraints.", "comment": "Lunar Surface Innovation Consortium 2025 Spring Meeting, May 20-22", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10925v1", "AI": {"title_translation": "自主无线空间网络的代理语义控制：通过MCP驱动的分布式智能扩展Space-O-RAN", "tldr": "本论文提出通过引入由模型上下文协议（MCP）和代理间（A2A）通信协议支持的语义代理层，扩展Space-O-RAN，以实现月球无线网络中上下文感知的分布式智能决策。", "motivation": "月球表面操作对无线通信系统提出了严格要求，包括自主性、抗干扰鲁棒性以及适应环境和任务驱动上下文的能力。现有的Space-O-RAN虽然提供了符合3GPP标准的分布式编排模型，但其决策逻辑仅限于静态策略，且缺乏语义集成。", "method": "本文提出了一种新颖的扩展方案，即通过模型上下文协议（MCP）和代理间（A2A）通信协议，引入了一个语义代理层。这使得系统能够在实时、近实时和非实时控制层进行上下文感知的决策。部署在月球车、着陆器和月球基站中的分布式认知代理实现了无线感知协调策略，包括延迟自适应推理和带宽感知语义压缩，同时与多个MCP服务器交互，对遥测数据、移动规划和任务约束进行推理。", "result": "所提出的系统能够实现上下文感知的决策，支持跨实时、近实时和非实时控制层的智能操作。分布式认知代理能够执行无线感知协调策略，如延迟自适应推理和带宽感知语义压缩，并能对遥测、移动规划和任务约束进行推理。", "conclusion": "通过引入代理语义控制，本文成功地将Space-O-RAN扩展，使其能够满足月球无线通信系统对自主性、鲁棒性和适应性的严格要求，实现了分布式、上下文感知和自适应的智能。", "translation": "月球表面操作对无线通信系统提出了严格要求，包括自主性、抗干扰鲁棒性以及适应环境和任务驱动上下文的能力。虽然Space-O-RAN提供了一个符合3GPP标准的分布式编排模型，但其决策逻辑仅限于静态策略，且缺乏语义集成。我们提出了一种新颖的扩展方案，即引入一个由模型上下文协议（MCP）和代理间（A2A）通信协议支持的语义代理层，从而实现跨实时、近实时和非实时控制层的上下文感知决策。部署在月球车、着陆器和月球基站中的分布式认知代理实现了无线感知协调策略，包括延迟自适应推理和带宽感知语义压缩，同时与多个MCP服务器交互，对遥测数据、移动规划和任务约束进行推理。", "summary": "本论文针对月球无线通信中Space-O-RAN的局限性，提出了一种代理语义层的扩展方案。该层利用MCP和A2A协议，使分布式认知代理能够进行上下文感知、实时决策，包括自适应推理和带宽感知压缩，这对自主月球操作至关重要。", "keywords": "代理控制, Space-O-RAN, 月球通信, 分布式智能, 语义控制", "comments": "本文的创新之处在于将语义智能和代理控制集成到Space-O-RAN中，实现了从静态策略到动态、上下文感知决策的转变，以应对月球表面等极端环境的挑战。这种方法有效解决了自主性和适应性的关键问题。"}}
{"id": "2506.10235", "title": "LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation", "authors": ["Chen-Chia Chang", "Wan-Hsuan Lin", "Yikang Shen", "Yiran Chen", "Xin Zhang"], "summary": "Automation of analog topology design is crucial due to customized\nrequirements of modern applications with heavily manual engineering efforts.\nThe state-of-the-art work applies a sequence-to-sequence approach and\nsupervised finetuning on language models to generate topologies given user\nspecifications. However, its circuit formulation is inefficient due to O(|V |2)\ntoken length and suffers from low precision sensitivity to numeric inputs. In\nthis work, we introduce LaMAGIC2, a succinct float-input canonical formulation\nwith identifier (SFCI) for language model-based analog topology generation.\nSFCI addresses these challenges by improving component-type recognition through\nidentifier-based representations, reducing token length complexity to O(|V |),\nand enhancing numeric precision sensitivity for better performance under tight\ntolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher\nsuccess rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a\nprior method. LaMAGIC2 also exhibits better transferability for circuits with\nmore vertices with up to 58.5% improvement. These advancements establish\nLaMAGIC2 as a robust framework for analog topology generation.", "comment": "Accepted at 42nd International Conference on Machine Learning (ICML)\n  2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10235v1", "AI": {"title_translation": "LaMAGIC2：基于语言模型的模拟拓扑生成的高级电路公式", "tldr": "LaMAGIC2通过引入SFCI公式，显著提高了基于语言模型的模拟拓扑生成效率和精度，解决了现有方法的令牌长度和精度问题。", "motivation": "现代应用对模拟拓扑设计有定制化需求，但现有设计需要大量手动工作。现有的基于语言模型的模拟拓扑生成方法存在电路公式效率低下（O(|V|^2)令牌长度）和对数值输入精度敏感度低的问题。", "method": "本文引入了LaMAGIC2，它采用一种简洁的浮点输入规范公式与标识符（SFCI），用于基于语言模型的模拟拓扑生成。SFCI通过基于标识符的表示来改进组件类型识别，将令牌长度复杂度降低到O(|V|)，并增强了数值精度敏感性，以在严格容差下获得更好的性能。", "result": "LaMAGIC2在0.01的严格容差下，成功率比现有方法高34%，MSE降低10倍。对于更多顶点的电路，LaMAGIC2还表现出更好的可转移性，最高提高了58.5%。", "conclusion": "LaMAGIC2是一个强大的模拟拓扑生成框架，通过改进的电路公式解决了现有挑战。", "translation": "由于现代应用对定制化要求高，模拟拓扑设计的自动化至关重要，但现有方法需要大量手动工程工作。最先进的方法应用序列到序列方法和对语言模型进行监督微调，根据用户规范生成拓扑。然而，其电路公式效率低下，因为令牌长度为O(|V|^2)，并且对数值输入的精度敏感度低。在这项工作中，我们引入了LaMAGIC2，这是一种简洁的浮点输入规范公式与标识符（SFCI），用于基于语言模型的模拟拓扑生成。SFCI通过基于标识符的表示改进组件类型识别，将令牌长度复杂度降低到O(|V|)，并增强了数值精度敏感性，从而在严格容差下获得更好的性能，解决了这些挑战。我们的实验表明，与现有方法相比，LaMAGIC2在0.01的严格容差下实现了34%更高的成功率和10倍更低的MSE。LaMAGIC2还表现出对具有更多顶点的电路更好的可转移性，最高提高了58.5%。这些进步使LaMAGIC2成为一个强大的模拟拓扑生成框架。", "summary": "本文介绍了LaMAGIC2，一种用于基于语言模型的模拟拓扑生成的新型电路公式。它通过引入简洁的浮点输入规范公式与标识符（SFCI），解决了现有方法中低效的O(|V|^2)令牌长度和低精度敏感度问题。LaMAGIC2通过改进组件识别、降低令牌复杂度至O(|V|)并提高数值精度敏感性，显著提升了性能。实验表明，LaMAGIC2在成功率、MSE和可转移性方面均优于现有方法，使其成为一个鲁棒的模拟拓扑生成框架。", "keywords": "模拟拓扑生成, 语言模型, 电路公式, SFCI, 自动化设计", "comments": "LaMAGIC2的创新在于其提出的SFCI公式，它有效地解决了现有基于语言模型的模拟拓扑生成方法在效率和精度上的瓶颈。通过将令牌长度复杂度从O(|V|^2)降低到O(|V|)并增强数值精度敏感度，该工作显著提升了自动化模拟设计的可行性和性能，具有重要的实际应用价值。"}}
{"id": "2506.10461", "title": "Automating Multi-Tenancy Performance Evaluation on Edge Compute Nodes", "authors": ["Joanna Georgiou", "Moysis Symeonides", "George Pallis", "Marios D. Dikaiakos"], "summary": "Edge Computing emerges as a promising alternative of Cloud Computing, with\nscalable compute resources and services deployed in the path between IoT\ndevices and Cloud. Since virtualization techniques can be applied on Edge\ncompute nodes, administrators can share their Edge infrastructures among\nmultiple users, providing the so-called multi-tenancy. Even though\nmulti-tenancy is unavoidable, it raises concerns about security and performance\ndegradation due to resource contention in Edge Computing. For that,\nadministrators need to deploy services with non-antagonizing profiles and\nexplore workload co-location scenarios to enhance performance and energy\nconsumption. Achieving this, however, requires extensive configuration,\ndeployment, iterative testing, and analysis, an effort-intensive and\ntime-consuming process. To address this challenge, we introduce an\nauto-benchmarking framework designed to streamline the analysis of\nmulti-tenancy performance in Edge environments. Our framework includes a\nbuilt-in monitoring stack and integrates with widely used benchmarking\nworkloads, such as streaming analytics, database operations, machine learning\napplications, and component-based stress testing. We perform a case-driven\nanalysis and provide valuable insights into the impact of multi-tenancy on Edge\nenvironments with different hardware configurations and diverse workloads.\nFinally, the implementation of our framework, along with the containerized\nworkloads used for experimentation, is publicly available.", "comment": "2025 IEEE International Conference on Edge Computing and\n  Communications (EDGE)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10461v1", "AI": {"title_translation": "自动化边缘计算节点上的多租户性能评估", "tldr": "该论文介绍了一个自动化基准测试框架，用于简化边缘环境中多租户性能的分析。", "motivation": "边缘计算中的多租户会因资源争用导致性能下降，而评估其影响需要大量手动配置、部署和测试，耗时耗力。", "method": "引入了一个自动化基准测试框架，包含内置监控堆栈，并集成了流行的基准测试工作负载（如流分析、数据库操作、机器学习应用、组件压力测试）。通过案例驱动分析提供见解。", "result": "框架能够简化边缘环境中多租户性能的分析，并提供了关于多租户对不同硬件配置和多样化工作负载的边缘环境影响的有价值见解。", "conclusion": "开发了一个自动化框架，用于简化边缘环境中多租户性能的分析，其实现及实验工作负载均已公开。", "translation": "边缘计算作为云计算的一种有前景的替代方案而出现，在物联网设备和云之间的路径中部署了可扩展的计算资源和服务。由于虚拟化技术可以应用于边缘计算节点，管理员可以在多个用户之间共享其边缘基础设施，提供所谓的多租户。尽管多租户不可避免，但它引发了对边缘计算中资源争用导致的安全性担忧和性能下降。为此，管理员需要部署具有非对抗性配置文件的服务，并探索工作负载共存场景以提高性能和降低能耗。然而，实现这一点需要大量的配置、部署、迭代测试和分析，这是一个劳动密集型且耗时的过程。为了解决这一挑战，我们引入了一个自动化基准测试框架，旨在简化边缘环境中多租户性能的分析。我们的框架包括内置监控堆栈，并集成了广泛使用的基准测试工作负载，例如流分析、数据库操作、机器学习应用和基于组件的压力测试。我们进行了案例驱动分析，并对多租户对具有不同硬件配置和多样化工作负载的边缘环境的影响提供了有价值的见解。最后，我们框架的实现以及用于实验的容器化工作负载均已公开可用。", "summary": "鉴于边缘计算中多租户引起的性能下降评估过程耗时耗力，本文提出了一个自动化的基准测试框架。该框架集成了监控和多种主流工作负载，旨在简化对边缘环境中多租户性能的分析。通过案例研究，该框架提供了关于多租户在不同硬件配置和工作负载下对边缘环境影响的宝贵见解。其实现和实验工作负载均已公开。", "keywords": "边缘计算, 多租户, 性能评估, 自动化, 基准测试", "comments": "该论文提出了一种实用的自动化解决方案，解决了边缘计算多租户环境性能评估的痛点。其创新性在于将监控、多种工作负载和自动化流程整合到一个框架中，显著降低了评估复杂性。该框架的公开可用性也增强了其潜在影响和可复现性。"}}
{"id": "2506.10829", "title": "LLM-Driven Personalized Answer Generation and Evaluation", "authors": ["Mohammadreza Molavi", "Mohammadreza Tavakoli", "Mohammad Moein", "Abdolali Faraji", "Gábor Kismihók"], "summary": "Online learning has experienced rapid growth due to its flexibility and\naccessibility. Personalization, adapted to the needs of individual learners, is\ncrucial for enhancing the learning experience, particularly in online settings.\nA key aspect of personalization is providing learners with answers customized\nto their specific questions. This paper therefore explores the potential of\nLarge Language Models (LLMs) to generate personalized answers to learners'\nquestions, thereby enhancing engagement and reducing the workload on educators.\nTo evaluate the effectiveness of LLMs in this context, we conducted a\ncomprehensive study using the StackExchange platform in two distinct areas:\nlanguage learning and programming. We developed a framework and a dataset for\nvalidating automatically generated personalized answers. Subsequently, we\ngenerated personalized answers using different strategies, including 0-shot,\n1-shot, and few-shot scenarios. The generated answers were evaluated using\nthree methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our\nfindings indicated that providing LLMs with examples of desired answers (from\nthe learner or similar learners) can significantly enhance the LLMs' ability to\ntailor responses to individual learners' needs.", "comment": "This is the preprint version of a paper accepted at AIED 2025. The\n  final version will be published by Springer", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10829v1", "AI": {"title_translation": "LLM驱动的个性化答案生成与评估", "tldr": "本文探讨了大型语言模型（LLMs）在在线学习中生成个性化答案的潜力，并通过在语言学习和编程领域的StackExchange数据进行评估，发现提供示例能显著提升LLMs的个性化能力。", "motivation": "在线学习的个性化对提升学习体验至关重要，特别是为学习者提供定制化答案。当前研究旨在利用大型语言模型（LLMs）生成个性化答案，以提高学习参与度并减轻教育者的工作量。", "method": "本研究利用StackExchange平台在语言学习和编程两个领域进行了综合研究。开发了一个框架和数据集来验证自动生成的个性化答案。使用0-shot、1-shot和few-shot策略生成个性化答案，并采用BERTScore、LLM评估和人工评估三种方法进行评估。", "result": "研究结果表明，向LLMs提供所需答案的示例（来自学习者或类似学习者）可以显著增强LLMs根据个体学习者需求定制回复的能力。", "conclusion": "提供示例是提升LLMs生成个性化答案能力的关键策略，这对于在线学习中的个性化教育具有重要意义。", "translation": "在线学习因其灵活性和可访问性而经历了快速增长。个性化，即根据个体学习者的需求进行调整，对于提升学习体验至关重要，尤其是在线环境中。个性化的一个关键方面是为学习者提供根据其具体问题定制的答案。因此，本文探讨了大型语言模型（LLMs）生成学习者问题个性化答案的潜力，从而增强参与度并减轻教育者的工作量。为了评估LLMs在此背景下的有效性，我们使用StackExchange平台在两个不同领域：语言学习和编程，进行了一项综合研究。我们开发了一个框架和一个数据集，用于验证自动生成的个性化答案。随后，我们使用不同的策略，包括0-shot、1-shot和few-shot场景，生成了个性化答案。生成的答案通过三种方法进行评估：1. BERTScore，2. LLM评估，以及3. 人工评估。我们的研究结果表明，向LLMs提供所需答案的示例（来自学习者或类似学习者）可以显著增强LLMs根据个体学习者需求定制回复的能力。", "summary": "本文探讨了大型语言模型（LLMs）在在线学习环境中生成个性化答案的能力。研究旨在通过为学习者提供定制化回复来提高参与度并减轻教育者负担。研究在语言学习和编程领域的StackExchange数据上进行了实验，开发了验证框架和数据集。通过0-shot、1-shot和few-shot策略生成答案，并使用BERTScore、LLM评估和人工评估进行效果评估。结果表明，提供示例可以显著提升LLMs生成个性化答案的能力。", "keywords": "个性化答案生成, 大型语言模型, 在线学习, StackExchange, 少样本学习", "comments": "本文关注了在线教育中一个重要且具有挑战性的问题——个性化答案生成。其创新点在于利用LLMs的强大能力，并通过多策略（0-shot, 1-shot, few-shot）和多维度（BERTScore, LLM评估, 人工评估）的评估方法，全面验证了LLMs在这一任务上的有效性。特别是发现“提供示例”对提升LLMs个性化能力的关键作用，为未来LLMs在教育领域的应用提供了宝贵的实践指导。"}}
{"id": "2506.10374", "title": "Optimal Non-Adaptive Group Testing with One-Sided Error Guarantees", "authors": ["Daniel McMorrow", "Jonathan Scarlett"], "summary": "The group testing problem consists of determining a sparse subset of\ndefective items from within a larger set of items via a series of tests, where\neach test outcome indicates whether at least one defective item is included in\nthe test. We study the approximate recovery setting, where the recovery\ncriterion of the defective set is relaxed to allow a small number of items to\nbe misclassified. In particular, we consider one-sided approximate recovery\ncriteria, where we allow either only false negative or only false positive\nmisclassifications. Under false negatives only (i.e., finding a subset of\ndefectives), we show that there exists an algorithm matching the optimal\nthreshold of two-sided approximate recovery. Under false positives only (i.e.,\nfinding a superset of the defectives), we provide a converse bound showing that\nthe better of two existing algorithms is optimal.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10374v1", "AI": {"title_translation": "最优非自适应分组测试与单侧误差保证", "tldr": "本文研究在单侧误差（仅假阴性或仅假阳性）下，非自适应分组测试的最优近似恢复问题。", "motivation": "分组测试旨在通过一系列测试从大量项目中识别出稀疏的缺陷子集。传统的分组测试问题通常关注双侧误差的恢复。本文的动机在于研究在放松恢复标准，特别是只允许单侧错误（即仅假阴性或仅假阳性）的情况下，如何实现缺陷集的近似恢复。", "method": "本研究采用近似恢复设置，并特别考虑了两种单侧近似恢复准则：一是仅允许假阴性（即找到缺陷的子集），二是仅允许假阳性（即找到缺陷的超集）。研究方法包括证明算法的存在性以匹配最优阈值，以及提供反向界限来证明现有算法的最优性。", "result": "在仅允许假阴性（找到缺陷子集）的情况下，研究表明存在一种算法，其性能与最优双侧近似恢复的阈值相匹配。在仅允许假阳性（找到缺陷超集）的情况下，研究提供了一个反向界限，证明现有两种算法中较优者是最佳的。", "conclusion": "在单侧误差保证（仅假阴性或仅假阳性）的近似恢复设置下，非自适应分组测试可以达到或接近最优性能，表明即使在更宽松的错误分类标准下，也能实现高效的缺陷识别。", "translation": "分组测试问题在于通过一系列测试从大量项目中确定稀疏的缺陷项目子集，其中每个测试结果指示测试中是否包含至少一个缺陷项目。我们研究近似恢复设置，其中缺陷集的恢复标准被放宽以允许少量项目被错误分类。特别是，我们考虑单侧近似恢复标准，即我们只允许假阴性或只允许假阳性错误分类。在仅有假阴性（即找到缺陷子集）的情况下，我们表明存在一种算法与双侧近似恢复的最优阈值相匹配。在仅有假阳性（即找到缺陷超集）的情况下，我们提供了一个反向界限，表明现有两种算法中较优者是最佳的。", "summary": "本文探讨了在单侧误差保证下的最优非自适应分组测试问题。研究在近似恢复设置中，分别考虑了仅允许假阴性（找到缺陷子集）和仅允许假阳性（找到缺陷超集）两种情况。结果显示，在仅有假阴性时，存在算法能达到与最优双侧近似恢复相同的阈值；而在仅有假阳性时，现有算法中的较优者被证明是最优的。", "keywords": "分组测试, 单侧误差, 非自适应, 近似恢复, 最优性", "comments": "这篇论文的创新点在于将分组测试的近似恢复问题细化到单侧误差保证，这在实际应用中可能提供更大的灵活性和针对性。研究结果为在特定误差约束下选择最优分组测试策略提供了理论依据，特别是证明了在某些单侧误差条件下可以达到与双侧误差相媲美的性能，这对于资源受限或特定错误类型更敏感的场景具有重要意义。"}}
{"id": "2506.10296", "title": "Synthesizing Min-Max Control Barrier Functions For Switched Affine Systems", "authors": ["Sara Kamali", "Guillaume O. Berger", "Sriram Sankaranarayanan"], "summary": "We study the problem of synthesizing non-smooth control barrier functions\n(CBFs) for continuous-time switched affine systems. Switched affine systems are\ndefined by a set of affine dynamical modes, wherein the control consists of a\nstate-based switching signal that determines the current operating mode. The\ncontrol barrier functions seek to maintain the system state inside a control\ninvariant set that excludes a given set of unsafe states. We consider CBFs that\ntake the form of pointwise minima and maxima over a finite set of affine\nfunctions. Our approach uses ideas from nonsmooth analysis to formulate\nconditions for min- and max- affine control barrier functions. We show how a\nfeedback switching law can be extracted from a given CBF. Next, we show how to\nautomate the process of synthesizing CBFs given a system description through a\ntree-search algorithm inspired by branch-and-cut methods from combinatorial\noptimization. Finally, we demonstrate our approach on a series of interesting\nexamples of switched affine systems.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10296v1", "AI": {"title_translation": "切换仿射系统最小-最大控制障碍函数的合成", "tldr": "该研究旨在为切换仿射系统合成非光滑的最小-最大控制障碍函数（CBFs），以确保系统状态保持在安全区域内，并自动化CBF的合成过程。", "motivation": "该研究旨在解决为连续时间切换仿射系统合成非光滑控制障碍函数（CBFs）的问题。CBFs的目的是使系统状态保持在一个控制不变集中，从而排除给定的不安全状态集。", "method": "该方法考虑将CBFs形式化为有限仿射函数集上的逐点最小值和最大值。研究利用非光滑分析的思想来制定最小-最大仿射控制障碍函数的条件。论文展示了如何从给定的CBF中提取反馈切换律，并展示了如何通过受组合优化中分支定界方法启发的树搜索算法，根据系统描述自动化CBF的合成过程。", "result": "研究展示了如何从给定的CBF中提取反馈切换律，并成功自动化了CBF的合成过程。最后，该方法在一系列有趣的切换仿射系统示例上得到了验证。", "conclusion": "该论文提出了一种为连续时间切换仿射系统合成非光滑最小-最大控制障碍函数的方法，并展示了如何自动化这一合成过程，通过示例验证了其有效性。", "translation": "我们研究为连续时间切换仿射系统合成非光滑控制障碍函数（CBFs）的问题。切换仿射系统由一组仿射动力学模式定义，其中控制包括一个基于状态的切换信号，该信号决定当前的操作模式。控制障碍函数旨在将系统状态维持在一个控制不变集内，该集合排除了给定的一组不安全状态。我们考虑采用有限仿射函数集上的逐点最小值和最大值形式的CBFs。我们的方法利用非光滑分析的思想来制定最小-最大仿射控制障碍函数的条件。我们展示了如何从给定的CBF中提取反馈切换律。接下来，我们展示了如何通过受组合优化中分支定界方法启发的树搜索算法，根据系统描述自动化CBFs的合成过程。最后，我们在一系列有趣的切换仿射系统示例上演示了我们的方法。", "summary": "本研究关注为连续时间切换仿射系统合成非光滑的最小-最大控制障碍函数（CBFs）。这些CBFs旨在确保系统状态保持在安全区域内。该方法利用非光滑分析来制定CBF条件，并展示了如何提取反馈切换律。此外，论文提出了一种基于树搜索算法的自动化CBF合成流程。该方法通过多个切换仿射系统示例进行了验证。", "keywords": "控制障碍函数, 切换仿射系统, 非光滑分析, 最小-最大, 合成", "comments": "该论文的创新点在于提出了为切换仿射系统合成非光滑最小-最大控制障碍函数的方法，并引入了基于分支定界启发式树搜索算法的自动化合成流程。这对于确保复杂混合系统的安全操作具有重要意义，尤其是在处理非光滑特性时，为控制理论和实际应用提供了新的工具。"}}
{"id": "2506.10350", "title": "Heterogeneous-IRS-Assisted MIMO Systems: Channel Estimation and Beamforming", "authors": ["Weibiao Zhao", "Qiucen Wu", "Yuanqi Tang", "Yu Zhu"], "summary": "Intelligent reflecting surface (IRS) has gained great attention for its\nability to create favorable propagation environments. However, the power\nconsumption of conventional IRSs cannot be ignored due to the large number of\nreflecting elements and control circuits. To balance performance and power\nconsumption, we previously proposed a heterogeneous-IRS (HE-IRS), a green IRS\nstructure integrating dynamically tunable elements (DTEs) and statically\ntunable elements (STEs). Compared to conventional IRSs with only DTEs, the\nunique DTE-STE integrated structure introduces new challenges in both channel\nestimation and beamforming. In this paper, we investigate the channel\nestimation and beamforming problems in HE-IRS-assisted multi-user\nmultiple-input multiple-output systems. Unlike the overall cascaded channel\nestimated in conventional IRSs, we show that the HE-IRS channel to be estimated\nis decomposed into a DTE-based cascaded channel and an STE-based equivalent\nchannel. Leveraging it along with the inherent sparsity of DTE- and STE-based\nchannels and manifold optimization, we propose an efficient channel estimation\nscheme. To address the rank mismatch problem in the imperfect channel sparsity\ninformation, a robust rank selection rule is developed. For beamforming, we\npropose an offline algorithm to optimize the STE phase shifts for wide beam\ncoverage, and an online algorithm to optimize the BS precoder and the DTE phase\nshifts using the estimated HE-IRS channel. Simulation results show that the\nHE-IRS requires less pilot overhead than conventional IRSs with the same number\nof elements. With the proposed channel estimation and beamforming schemes, the\ngreen HE-IRS achieves competitive sum rate performance with significantly\nreduced power consumption.", "comment": "30 pages, 8 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10350v1", "AI": {"title_translation": "异构IRS辅助的MIMO系统：信道估计与波束成形", "tldr": "提出了一种异构智能反射面(HE-IRS)结构，并为其设计了高效的信道估计和波束成形方案，实现了在降低功耗的同时保持性能。", "motivation": "传统IRS由于反射单元和控制电路数量多，功耗不容忽视。为了平衡性能和功耗，需要一种更绿色的IRS结构。", "method": "提出异构IRS (HE-IRS)，集成动态可调元件(DTEs)和静态可调元件(STEs)。将HE-IRS信道分解为基于DTE的级联信道和基于STE的等效信道，并利用固有稀疏性和流形优化提出高效信道估计算法。开发鲁棒的秩选择规则以解决不完善信道稀疏信息中的秩不匹配问题。提出离线算法优化STE相移以实现宽波束覆盖，并提出在线算法利用估计的HE-IRS信道优化基站预编码器和DTE相移。", "result": "仿真结果表明，HE-IRS在相同数量元件下比传统IRS需要更少的导频开销。所提出的信道估计和波束成形方案使绿色HE-IRS在显著降低功耗的同时，实现了具有竞争力的和速率性能。", "conclusion": "异构IRS (HE-IRS) 是一种绿色且高效的解决方案，它通过独特的DTE-STE集成结构，在信道估计和波束成形方面克服了传统IRS的挑战，并在降低功耗的同时保持了优异的系统性能。", "translation": "智能反射面（IRS）因其创造有利传播环境的能力而备受关注。然而，由于大量的反射元件和控制电路，传统IRS的功耗不容忽视。为了平衡性能和功耗，我们之前提出了一种异构IRS（HE-IRS），这是一种集成了动态可调元件（DTEs）和静态可调元件（STEs）的绿色IRS结构。与仅包含DTE的传统IRS相比，独特的DTE-STE集成结构在信道估计和波束成形方面带来了新的挑战。在本文中，我们研究了HE-IRS辅助的多用户多输入多输出系统中的信道估计和波束成形问题。与传统IRS中估计整体级联信道不同，我们表明HE-IRS待估计信道被分解为基于DTE的级联信道和基于STE的等效信道。利用这一点以及基于DTE和STE信道的固有稀疏性以及流形优化，我们提出了一种高效的信道估计算法。为了解决不完善信道稀疏信息中的秩不匹配问题，开发了一种鲁棒的秩选择规则。对于波束成形，我们提出了一种离线算法来优化STE相移以实现宽波束覆盖，并提出了一种在线算法来利用估计的HE-IRS信道优化基站预编码器和DTE相移。仿真结果表明，HE-IRS在相同数量元件下比传统IRS需要更少的导频开销。通过所提出的信道估计和波束成形方案，绿色HE-IRS在显著降低功耗的同时，实现了具有竞争力的和速率性能。", "summary": "本文研究了异构智能反射面（HE-IRS）辅助的多用户MIMO系统中的信道估计和波束成形问题。HE-IRS是一种结合了动态可调元件（DTEs）和静态可调元件（STEs）的绿色IRS结构，旨在解决传统IRS高功耗的问题。作者提出了一种将HE-IRS信道分解为DTE-based级联信道和STE-based等效信道的高效信道估计算法，并开发了鲁棒的秩选择规则。同时，设计了用于STE和DTE相移优化的离线和在线波束成形算法。仿真结果验证了HE-IRS在减少导频开销和降低功耗的同时，能保持与传统IRS相当的和速率性能。", "keywords": "异构智能反射面, 信道估计, 波束成形, MIMO系统, 功耗", "comments": "这篇论文的创新点在于提出了异构IRS（HE-IRS）这一新颖的结构，有效解决了传统IRS功耗过高的问题，实现了性能与能耗的平衡。其将信道分解为DTE和STE部分进行估计的方法，以及针对性设计的波束成形算法，都体现了对HE-IRS独特结构的深刻理解和有效利用。这项工作对于推动绿色通信技术和IRS的实际部署具有重要意义。"}}
{"id": "2506.10698", "title": "Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound Classification", "authors": ["Peidong Wei Shiyu Miao Lin Li"], "summary": "Deep neural networks have been applied to audio spectrograms for respiratory\nsound classification, but it remains challenging to achieve satisfactory\nperformance due to the scarcity of available data. Moreover, domain mismatch\nmay be introduced into the trained models as a result of the respiratory sound\nsamples being collected from various electronic stethoscopes, patient\ndemographics, and recording environments. To tackle this issue, we proposed a\nmodified MaskedAutoencoder(MAE) model, named Disentangling Dual-Encoder MAE\n(DDE-MAE) for respiratory sound classification. Two independent encoders were\ndesigned to capture disease-related and disease-irrelevant information\nseparately, achieving feature disentanglement to reduce the domain mismatch.\nOur method achieves a competitive performance on the ICBHI dataset.", "comment": "(Accepted at Interspeech 2025)", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10698v1", "AI": {"title_translation": "呼吸音分类的双编码器解耦掩码自编码器", "tldr": "针对呼吸音分类中数据稀缺和域不匹配问题，本文提出了一种名为DDE-MAE的新型掩码自编码器模型，通过双编码器实现特征解耦，有效提升了分类性能。", "motivation": "深度神经网络在呼吸音分类中面临数据稀缺的挑战，并且由于呼吸音样本来自不同的电子听诊器、患者群体和录音环境，导致训练模型中出现域不匹配问题，这限制了其性能。", "method": "本文提出了一种名为“解耦双编码器掩码自编码器”（DDE-MAE）的改进型掩码自编码器模型，用于呼吸音分类。该模型设计了两个独立的编码器，分别捕获与疾病相关和与疾病无关的信息，从而实现特征解耦，以减少域不匹配。", "result": "该方法在ICBHI数据集上取得了有竞争力的性能。", "conclusion": "DDE-MAE通过特征解耦有效解决了呼吸音分类中的域不匹配问题，并在有限数据下表现出良好的性能，为呼吸音分类提供了一种有效的新方法。", "translation": "深度神经网络已被应用于音频频谱图进行呼吸音分类，但由于可用数据稀缺，实现令人满意的性能仍然具有挑战性。此外，由于呼吸音样本是从各种电子听诊器、患者人口统计和录音环境中收集的，可能会在训练模型中引入域不匹配。为了解决这个问题，我们提出了一种名为“解耦双编码器掩码自编码器”（DDE-MAE）的改进型掩码自编码器（MAE）模型，用于呼吸音分类。设计了两个独立的编码器，分别捕获与疾病相关和与疾病无关的信息，实现了特征解耦，以减少域不匹配。我们的方法在ICBHI数据集上取得了有竞争力的性能。", "summary": "本文针对呼吸音分类中数据稀缺和域不匹配的挑战，提出了一种名为DDE-MAE的改进型掩码自编码器。该模型通过设计两个独立的编码器，分别学习疾病相关和无关特征，实现特征解耦，从而有效降低了因不同采集源导致的域差异。实验结果表明，DDE-MAE在ICBHI数据集上表现出有竞争力的性能。", "keywords": "呼吸音分类, 掩码自编码器, 特征解耦, 域不匹配, 双编码器", "comments": "这篇论文的创新点在于引入了双编码器结构来解决呼吸音分类中的域不匹配问题，通过特征解耦提高了模型的泛化能力。这种方法对于医疗领域中数据异构性强的任务具有重要意义。"}}
{"id": "2506.10264", "title": "WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models", "authors": ["Qiyue Yin", "Pei Xu", "Qiaozhe Li", "Shengda Liu", "Shengqi Shen", "Tong Wang", "Yihong Han", "Xiaonan Zhao", "Likun Yang", "Shiyue Cao", "Shiyu Qiu", "Yuxuan Liu", "Shizhao Yu", "Lei Cui", "Chengxin Yan", "Jie Sun", "Xiangquan Tang", "Kaiqi Huang"], "summary": "Recent breakthroughs in Large Language Models (LLMs) have led to a\nqualitative leap in artificial intelligence' s performance on reasoning tasks,\nparticularly demonstrating remarkable capabilities in mathematical, symbolic,\nand commonsense reasoning. However, as a critical component of advanced human\ncognition, strategic reasoning, i.e., the ability to assess multi-agent\nbehaviors in dynamic environments, formulate action plans, and adapt\nstrategies, has yet to be systematically evaluated or modeled. To address this\ngap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark\nfor LLMs using wargame as its evaluation environment. Wargame, a quintessential\nhigh-complexity strategic scenario, integrates environmental uncertainty,\nadversarial dynamics, and non-unique strategic choices, making it an effective\ntestbed for assessing LLMs' capabilities in multi-agent decision-making, intent\ninference, and counterfactual reasoning. WGSR-Bench designs test samples around\nthree core tasks, i.e., Environmental situation awareness, Opponent risk\nmodeling and Policy generation, which serve as the core S-POE architecture, to\nsystematically assess main abilities of strategic reasoning. Finally, an\nLLM-based wargame agent is designed to integrate these parts for a\ncomprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess\nthe strengths and limitations of state-of-the-art LLMs in game-theoretic\nstrategic reasoning and to advance research in large model-driven strategic\nintelligence.", "comment": "15 pages, 17 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10264v1", "AI": {"title_translation": "WGSR-Bench：基于兵棋推演的LLM博弈论战略推理基准", "tldr": "LLM在战略推理方面仍有不足。本文提出了WGSR-Bench，一个基于兵棋推演的LLM战略推理基准，用于评估LLM在多智能体决策、意图推断和反事实推理方面的能力。", "motivation": "大型语言模型（LLMs）在数学、符号和常识推理方面取得了显著进展，但作为人类高级认知关键部分的战略推理（即评估动态环境中多智能体行为、制定行动计划和调整策略的能力）尚未得到系统评估或建模。", "method": "本文引入了WGSR-Bench，这是第一个使用兵棋推演作为评估环境的LLM战略推理基准。WGSR-Bench围绕环境态势感知、对手风险建模和策略生成这三个核心任务设计测试样本，构成S-POE架构，旨在系统评估战略推理能力。此外，还设计了一个基于LLM的兵棋推演智能体来整合这些部分进行综合评估。", "result": "本文成功引入了WGSR-Bench，一个基于兵棋推演的LLM战略推理基准，并设计了相应的测试样本和评估架构，旨在评估最先进LLM在博弈论战略推理方面的优势和局限性。", "conclusion": "WGSR-Bench旨在系统评估LLM在博弈论战略推理方面的能力，并通过揭示其优缺点来推动大模型驱动的战略智能研究。", "translation": "大型语言模型（LLMs）的最新突破使得人工智能在推理任务上的性能取得了质的飞跃，尤其在数学、符号和常识推理方面展现出卓越的能力。然而，作为人类高级认知的一个关键组成部分，战略推理，即评估动态环境中多智能体行为、制定行动计划和调整策略的能力，尚未得到系统评估或建模。为了解决这一空白，本文引入了WGSR-Bench，这是第一个以兵棋推演作为评估环境的LLM战略推理基准。兵棋推演是一个典型的、高复杂度的战略场景，它整合了环境不确定性、对抗动态和非唯一战略选择，使其成为评估LLM在多智能体决策、意图推断和反事实推理能力方面的有效测试平台。WGSR-Bench围绕三个核心任务设计测试样本，即环境态势感知、对手风险建模和策略生成，它们构成了核心S-POE架构，旨在系统地评估战略推理的主要能力。最后，设计了一个基于LLM的兵棋推演智能体来整合这些部分，进行全面的战略推理评估。通过WGSR-Bench，我们希望评估最先进LLM在博弈论战略推理方面的优势和局限性，并推动大模型驱动的战略智能研究。", "summary": "本文提出了WGSR-Bench，首个针对大型语言模型（LLMs）的战略推理基准，旨在弥补LLMs在复杂多智能体战略推理能力评估方面的空白。该基准以高复杂度的兵棋推演作为评估环境，通过环境态势感知、对手风险建模和策略生成这三个核心任务，系统地评估LLMs在多智能体决策、意图推断和反事实推理等方面的能力。WGSR-Bench旨在揭示当前LLMs在博弈论战略推理中的优缺点，并推动相关研究。", "keywords": "战略推理, 大型语言模型, 兵棋推演, 基准测试, 博弈论", "comments": "本文创新性地将兵棋推演这一高复杂度战略场景引入LLM评估，填补了现有基准在战略推理领域的空白。通过S-POE架构的细致设计，能够系统地评估LLM在多智能体决策、意图推断和反事实推理等高级认知能力，对推动LLM在复杂决策和战略智能方向的发展具有重要意义。"}}
{"id": "2506.10309", "title": "DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction", "authors": ["Yuliang Zhu", "Jing Cheng", "Qi Xie", "Zhuo-Xu Cui", "Qingyong Zhu", "Yuanyuan Liu", "Xin Liu", "Jianfeng Ren", "Chengbo Wang", "Dong Liang"], "summary": "Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries,\nincluding spatial rotation symmetry within individual frames and temporal\nsymmetry along the time dimension. Explicit incorporation of these symmetry\npriors in the reconstruction model can significantly improve image quality,\nespecially under aggressive undersampling scenarios. Recently, Equivariant\nconvolutional neural network (ECNN) has shown great promise in exploiting\nspatial symmetry priors. However, existing ECNNs critically fail to model\ntemporal symmetry, arguably the most universal and informative structural prior\nin dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep\nUnrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for\nDynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance\nthrough a (2+1)D equivariant convolutional architecture. In particular, it\nintegrates both the data consistency and proximal mapping module into a unified\ndeep unrolling framework. This architecture ensures rigorous propagation of\nspatiotemporal rotation symmetry constraints throughout the reconstruction\nprocess, enabling more physically accurate modeling of cardiac motion dynamics\nin cine MRI. In addition, a high-fidelity group filter parameterization\nmechanism is developed to maintain representation precision while enforcing\nsymmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets\ndemonstrate that DUN-SRE achieves state-of-the-art performance, particularly in\npreserving rotation-symmetric structures, offering strong generalization\ncapability to a broad range of dynamic MRI reconstruction tasks.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10309v1", "AI": {"title_translation": "DUN-SRE：用于动态MRI重建的具有时空旋转等变性的深度展开网络", "tldr": "DUN-SRE是一个新颖的深度展开网络，通过结合时空旋转等变性，显著改善了动态MRI重建的图像质量，尤其是在处理运动和欠采样数据方面。", "motivation": "动态MRI图像具有空间旋转对称性和时间对称性。虽然现有的等变卷积神经网络（ECNN）能利用空间对称性，但它们未能有效建模时间对称性，而这对于动态MRI重建至关重要且信息丰富。", "method": "本文提出了一种名为DUN-SRE的深度展开网络，用于动态MRI重建。该网络通过(2+1)D等变卷积架构建立时空等变性，并将数据一致性与近端映射模块整合到统一的深度展开框架中。此外，还开发了一种高保真群滤波器参数化机制，以在强制对称性约束的同时保持表示精度。", "result": "在心脏电影MRI数据集上的综合实验表明，DUN-SRE实现了最先进的性能，尤其在保留旋转对称结构方面表现出色，并对广泛的动态MRI重建任务展现出强大的泛化能力。", "conclusion": "DUN-SRE通过有效整合时空旋转等变性，显著提升了动态MRI重建的图像质量，尤其是在准确建模心脏运动动态和处理欠采样数据方面。", "translation": "动态磁共振成像（MRI）展现出变换对称性，包括个体帧内的空间旋转对称性和沿时间维度的时间对称性。在重建模型中明确纳入这些对称先验可以显著提高图像质量，尤其是在激进欠采样的情况下。最近，等变卷积神经网络（ECNN）在利用空间对称先验方面显示出巨大潜力。然而，现有ECNN未能关键性地建模时间对称性，这可以说是动态MRI重建中最普遍和信息最丰富的结构先验。为了解决这个问题，我们提出了一种用于动态MRI重建的新型深度展开网络，该网络具有时空旋转等变性（DUN-SRE）。DUN-SRE通过（2+1）D等变卷积架构建立时空等变性。特别是，它将数据一致性和近端映射模块集成到一个统一的深度展开框架中。这种架构确保了时空旋转对称性约束在整个重建过程中的严格传播，从而能够更物理精确地建模电影MRI中的心脏运动动态。此外，还开发了一种高保真群滤波器参数化机制，以在强制对称性约束的同时保持表示精度。在心脏电影MRI数据集上的综合实验表明，DUN-SRE实现了最先进的性能，特别是在保留旋转对称结构方面，为广泛的动态MRI重建任务提供了强大的泛化能力。", "summary": "本文提出了一种名为DUN-SRE的深度展开网络，用于动态MRI重建。该网络通过创新的(2+1)D等变卷积架构，首次在深度学习框架中有效整合了动态MRI的空间旋转和时间对称性。DUN-SRE将数据一致性和近端映射模块整合到其展开框架中，并采用高保真群滤波器参数化机制。实验证明，DUN-SRE在心脏电影MRI数据集上达到了最先进的性能，尤其擅长保留旋转对称结构，并具有出色的泛化能力。", "keywords": "深度展开网络, 时空等变性, 动态MRI重建, 心脏电影MRI, 对称先验", "comments": "这篇论文的创新点在于首次将时空旋转等变性引入到深度展开网络中，以解决现有等变卷积网络未能有效建模动态MRI时间对称性的问题。通过(2+1)D等变卷积架构和高保真群滤波器参数化，该方法能够更精确地捕捉心脏运动等动态过程，从而在欠采样情况下显著提高图像重建质量。这对于临床应用中快速、高质量的动态MRI成像具有重要意义。"}}
{"id": "2506.10468", "title": "Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On", "authors": ["Zaiqiang Wu", "Yechen Li", "Jingyuan Liu", "Yuki Shibata", "Takayuki Hori", "I-Chao Shen", "Takeo Igarashi"], "summary": "Existing image-based virtual try-on methods are often limited to the front\nview and lack real-time performance. While per-garment virtual try-on methods\nhave tackled these issues by capturing per-garment datasets and training\nper-garment neural networks, they still encounter practical limitations: (1)\nthe robotic mannequin used to capture per-garment datasets is prohibitively\nexpensive for widespread adoption and fails to accurately replicate natural\nhuman body deformation; (2) the synthesized garments often misalign with the\nhuman body. To address these challenges, we propose a low-barrier approach for\ncollecting per-garment datasets using real human bodies, eliminating the\nnecessity for a customized robotic mannequin. We also introduce a hybrid person\nrepresentation that enhances the existing intermediate representation with a\nsimplified DensePose map. This ensures accurate alignment of synthesized\ngarment images with the human body and enables human-garment interaction\nwithout the need for customized wearable devices. We performed qualitative and\nquantitative evaluations against other state-of-the-art image-based virtual\ntry-on methods and conducted ablation studies to demonstrate the superiority of\nour method regarding image quality and temporal consistency. Finally, our user\nstudy results indicated that most participants found our virtual try-on system\nhelpful for making garment purchasing decisions.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10468v1", "AI": {"title_translation": "使用真实人体进行交互式逐件虚拟试穿的低门槛数据集收集", "tldr": "现有虚拟试穿方法受限于昂贵的机器人模特和对齐问题。本文提出一种使用真实人体进行低门槛数据集收集的方法，并引入混合表示以实现更好的对齐和交互。", "motivation": "现有基于图像的虚拟试穿方法存在局限性，如仅限于正面视图和缺乏实时性。逐件虚拟试穿虽有所改进，但仍面临挑战：1) 用于数据集收集的机器人模特成本高昂且无法准确模拟人体变形；2) 合成服装常与人体未对齐。", "method": "提出了一种使用真实人体收集逐件数据集的低门槛方法，无需定制机器人模特。引入了一种混合人物表示，通过简化的DensePose图增强现有中间表示，以确保合成服装图像与人体准确对齐，并实现无需定制可穿戴设备的人衣交互。", "result": "定性和定量评估结果表明，与现有最先进的基于图像的虚拟试穿方法相比，本文方法在图像质量和时间一致性方面表现优越。消融研究也证实了这一点。用户研究结果显示，大多数参与者认为该虚拟试穿系统有助于做出服装购买决策。", "conclusion": "本文提出的低门槛数据集收集方法和混合人物表示有效解决了虚拟试穿中机器人模特成本高昂和服装对齐不准确的问题，提高了系统的实用性和用户体验，有助于服装购买决策。", "translation": "现有基于图像的虚拟试穿方法通常局限于正面视图，并且缺乏实时性能。虽然逐件虚拟试穿方法通过捕获逐件数据集和训练逐件神经网络解决了这些问题，但它们仍然遇到实际限制：(1) 用于捕获逐件数据集的机器人模特价格昂贵，难以广泛采用，并且无法准确复制自然的人体变形；(2) 合成的服装经常与人体未对齐。为了解决这些挑战，我们提出了一种使用真实人体收集逐件数据集的低门槛方法，从而消除了对定制机器人模特的需要。我们还引入了一种混合人物表示，通过简化的DensePose图增强了现有的中间表示。这确保了合成服装图像与人体的准确对齐，并实现了人衣交互，而无需定制可穿戴设备。我们对其他最先进的基于图像的虚拟试穿方法进行了定性和定量评估，并进行了消融研究，以证明我们方法在图像质量和时间一致性方面的优越性。最后，我们的用户研究结果表明，大多数参与者认为我们的虚拟试穿系统有助于做出服装购买决策。", "summary": "本文旨在解决现有虚拟试穿方法中机器人模特成本高昂和服装与人体对齐不准确的问题。作者提出了一种使用真实人体进行低门槛逐件数据集收集的方法，并引入了一种结合简化DensePose图的混合人物表示，以确保合成服装的精确对齐和人衣交互。实验结果表明，该方法在图像质量和时间一致性方面优于现有技术，且用户研究证实了其在服装购买决策中的实用性。", "keywords": "虚拟试穿, 数据集收集, 真实人体, DensePose, 服装对齐", "comments": "该论文通过消除对昂贵机器人模特的依赖，为虚拟试穿技术提供了一个更具成本效益和实用性的数据收集方案，显著降低了技术部署的门槛。其引入的混合人物表示，在无需定制硬件的情况下提升了服装对齐精度和交互性，是该领域的一个重要创新。这项工作对于推动虚拟试穿技术在实际应用中的普及具有重要意义。"}}
{"id": "2506.10060", "title": "Textual Bayes: Quantifying Uncertainty in LLM-Based Systems", "authors": ["Brendan Leigh Ross", "Noël Vouitsis", "Atiyeh Ashari Ghomi", "Rasa Hosseinzadeh", "Ji Xin", "Zhaoyan Liu", "Yi Sui", "Shiyi Hou", "Kin Kwan Leung", "Gabriel Loaiza-Ganem", "Jesse C. Cresswell"], "summary": "Although large language models (LLMs) are becoming increasingly capable of\nsolving challenging real-world tasks, accurately quantifying their uncertainty\nremains a critical open problem, which limits their applicability in\nhigh-stakes domains. This challenge is further compounded by the closed-source,\nblack-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can\nbe highly sensitive to the prompts that bind them together, which often require\nsignificant manual tuning (i.e., prompt engineering). In this work, we address\nthese challenges by viewing LLM-based systems through a Bayesian lens. We\ninterpret prompts as textual parameters in a statistical model, allowing us to\nuse a small training dataset to perform Bayesian inference over these prompts.\nThis novel perspective enables principled uncertainty quantification over both\nthe model's textual parameters and its downstream predictions, while also\nincorporating prior beliefs about these parameters expressed in free-form text.\nTo perform Bayesian inference, a difficult problem even for well-studied data\nmodalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a\nnovel Markov chain Monte Carlo (MCMC) algorithm that combines prompt\noptimization techniques with standard MCMC methods. MHLP is a turnkey\nmodification to existing LLM pipelines, including those that rely exclusively\non closed-source models. Empirically, we demonstrate that our method yields\nimprovements in both predictive accuracy and uncertainty quantification (UQ) on\na range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a\nviable path for incorporating methods from the rich Bayesian literature into\nthe era of LLMs, paving the way for more reliable and calibrated LLM-based\nsystems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10060v1", "AI": {"title_translation": "文本贝叶斯：量化基于LLM的系统中的不确定性", "tldr": "该论文提出了一种名为“文本贝叶斯”的新方法，通过将提示视为统计模型中的文本参数，利用贝叶斯推断来量化大型语言模型（LLMs）的不确定性，并引入了MHLP算法，以提高LLM系统的预测准确性和不确定性量化能力。", "motivation": "尽管大型语言模型（LLMs）在解决现实世界任务方面能力日益增强，但准确量化其不确定性仍然是一个关键的开放问题，这限制了它们在高风险领域的应用。此外，许多最先进的LLM是闭源的黑盒模型，并且基于LLM的系统对提示高度敏感，需要大量手动调优。", "method": "本研究通过贝叶斯视角审视基于LLM的系统，将提示解释为统计模型中的文本参数，从而可以使用小型训练数据集对这些提示执行贝叶斯推断。为实现贝叶斯推断，论文引入了Metropolis-Hastings through LLM Proposals (MHLP)，这是一种新颖的马尔可夫链蒙特卡洛（MCMC）算法，它结合了提示优化技术和标准MCMC方法。MHLP是对现有LLM管道的即插即用修改，包括那些依赖闭源模型的管道。", "result": "实证表明，该方法在LLM基准和不确定性量化任务上，在预测准确性和不确定性量化（UQ）方面均有所改进。", "conclusion": "这项工作为将丰富的贝叶斯文献中的方法引入LLM时代提供了一条可行的途径，为更可靠和校准的基于LLM的系统铺平了道路。", "translation": "尽管大型语言模型（LLMs）在解决具有挑战性的现实世界任务方面能力日益增强，但准确量化它们的不确定性仍然是一个关键的开放问题，这限制了它们在高风险领域的适用性。这一挑战因许多最先进的LLM的闭源、黑盒性质而进一步加剧。此外，基于LLM的系统可能对将它们连接在一起的提示高度敏感，这通常需要大量手动调优（即提示工程）。在这项工作中，我们通过贝叶斯视角审视基于LLM的系统来解决这些挑战。我们将提示解释为统计模型中的文本参数，这使我们能够使用少量训练数据集对这些提示执行贝叶斯推断。这种新颖的视角使得对模型的文本参数及其下游预测进行原则性的不确定性量化成为可能，同时还纳入了以自由形式文本表达的关于这些参数的先验信念。为了执行贝叶斯推断，即使对于研究充分的数据模态来说也是一个难题，我们引入了Metropolis-Hastings through LLM Proposals (MHLP)，这是一种新颖的马尔可夫链蒙特卡洛（MCMC）算法，它结合了提示优化技术和标准MCMC方法。MHLP是对现有LLM管道的即插即用修改，包括那些完全依赖闭源模型的管道。从经验上看，我们证明了我们的方法在一系列LLM基准和不确定性量化（UQ）任务上均能提高预测准确性和不确定性量化。更广泛地说，我们的工作展示了将丰富的贝叶斯文献中的方法纳入LLM时代的可行途径，为更可靠和校准的基于LLM的系统铺平了道路。", "summary": "该论文提出了一种名为“文本贝叶斯”的新框架，旨在解决大型语言模型（LLMs）的不确定性量化问题，尤其是在面对闭源和提示敏感性挑战时。通过将LLM提示视为统计模型中的文本参数，并利用小型训练数据集进行贝叶斯推断，该方法实现了对模型参数和预测的原则性不确定性量化。为此，论文引入了Metropolis-Hastings through LLM Proposals (MHLP)算法，这是一种结合提示优化和MCMC的创新方法，可无缝集成到现有LLM管道中。实验结果表明，该方法显著提高了LLM的预测准确性和不确定性量化能力，为构建更可靠、更校准的LLM系统奠定了基础。", "keywords": "LLM不确定性, 贝叶斯推断, 提示工程, MCMC, 文本贝叶斯", "comments": "该论文的创新之处在于将贝叶斯推断应用于大型语言模型（LLMs）的文本提示，将提示视为可推断的统计参数，从而实现了对黑盒LLM系统的不确定性量化。引入的MHLP算法是其核心创新，它巧妙地结合了提示优化和MCMC方法，使其能够适用于现有LLM管道，包括闭源模型。这项工作的重要性在于为LLM在高风险领域中的应用提供了更可靠的决策依据，并通过将贝叶斯方法的严谨性引入LLM领域，为未来LLM研究开辟了新的方向。"}}
{"id": "2506.10487", "title": "SHORE: A Long-term User Lifetime Value Prediction Model in Digital Games", "authors": ["Shuaiqi Sun", "Congde Yuan", "Haoqiang Yang", "Mengzhuo Guo", "Guiying Wei", "Jiangbo Tian"], "summary": "In digital gaming, long-term user lifetime value (LTV) prediction is\nessential for monetization strategy, yet presents major challenges due to\ndelayed payment behavior, sparse early user data, and the presence of\nhigh-value outliers. While existing models typically rely on either short-cycle\nobservations or strong distributional assumptions, such approaches often\nunderestimate long-term value or suffer from poor robustness. To address these\nissues, we propose SHort-cycle auxiliary with Order-preserving REgression\n(SHORE), a novel LTV prediction framework that integrates short-horizon\npredictions (e.g., LTV-15 and LTV-30) as auxiliary tasks to enhance long-cycle\ntargets (e.g., LTV-60). SHORE also introduces a hybrid loss function combining\norder-preserving multi-class classification and a dynamic Huber loss to\nmitigate the influence of zero-inflation and outlier payment behavior.\nExtensive offline and online experiments on real-world datasets demonstrate\nthat SHORE significantly outperforms existing baselines, achieving a 47.91\\%\nrelative reduction in prediction error in online deployment. These results\nhighlight SHORE's practical effectiveness and robustness in industrial-scale\nLTV prediction for digital games.", "comment": "7 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10487v1", "AI": {"title_translation": "SHORE：一种数字游戏中长期用户生命周期价值预测模型", "tldr": "SHORE是一种新颖的LTV预测框架，通过结合短周期辅助任务和混合损失函数，显著提高了数字游戏中长期用户生命周期价值预测的准确性和鲁棒性。", "motivation": "在数字游戏中，长期用户生命周期价值（LTV）预测对于变现策略至关重要，但由于支付行为延迟、早期用户数据稀疏以及高价值异常值的存在，面临重大挑战。现有模型通常依赖短周期观察或强分布假设，这往往低估长期价值或鲁棒性差。", "method": "本文提出了SHort-cycle auxiliary with Order-preserving REgression (SHORE) 模型，这是一种新颖的LTV预测框架。SHORE将短周期预测（如LTV-15和LTV-30）作为辅助任务集成，以增强长周期目标（如LTV-60）的预测。此外，SHORE引入了一种混合损失函数，结合了保序多类分类和动态Huber损失，以减轻零膨胀和异常支付行为的影响。", "result": "在真实世界数据集上的大量离线和在线实验表明，SHORE显著优于现有基线模型，在线部署中预测误差相对减少了47.91%。", "conclusion": "SHORE模型在数字游戏中的工业级LTV预测方面表现出实际有效性和鲁棒性。", "translation": "在数字游戏中，长期用户生命周期价值（LTV）预测对于变现策略至关重要，但由于支付行为延迟、早期用户数据稀疏以及高价值异常值的存在，面临重大挑战。尽管现有模型通常依赖短周期观察或强分布假设，但此类方法往往低估长期价值或鲁棒性差。为了解决这些问题，我们提出了SHort-cycle auxiliary with Order-preserving REgression（SHORE），这是一种新颖的LTV预测框架，它将短周期预测（例如LTV-15和LTV-30）作为辅助任务集成，以增强长周期目标（例如LTV-60）。SHORE还引入了一种混合损失函数，结合了保序多类分类和动态Huber损失，以减轻零膨胀和异常支付行为的影响。在真实世界数据集上的大量离线和在线实验表明，SHORE显著优于现有基线模型，在线部署中预测误差相对减少了47.91%。这些结果突出了SHORE在数字游戏中工业级LTV预测方面的实际有效性和鲁棒性。", "summary": "SHORE是一种针对数字游戏长期用户生命周期价值（LTV）预测的新型框架。它通过整合短周期预测作为辅助任务来提升长周期LTV的预测精度，并引入结合保序多类分类和动态Huber损失的混合损失函数，以应对数据稀疏、零膨胀和异常值等挑战。实验证明，SHORE显著优于现有模型，在线部署中预测误差降低了47.91%，展现了其在工业级应用中的有效性和鲁棒性。", "keywords": "用户生命周期价值预测, 数字游戏, SHORE, 辅助任务, 混合损失函数", "comments": "该论文提出了一种创新的LTV预测模型SHORE，其核心创新在于利用短周期预测作为辅助任务来优化长期LTV预测，并设计了独特的混合损失函数以有效处理游戏数据中常见的零膨胀和异常值问题。该方法解决了现有LTV模型在准确性和鲁棒性方面的不足，特别是在处理延迟支付和稀疏数据方面表现出色。实验结果显示了显著的性能提升，表明SHORE在实际工业应用中具有重要价值。"}}
{"id": "2506.10095", "title": "When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs", "authors": ["Xiao Li", "Joel Kreuzwieser", "Alan Peters"], "summary": "We investigate how large language models respond to prompts that differ only\nin their token-level realization but preserve the same semantic intent, a\nphenomenon we call prompt variance. We propose Prompt-Based Semantic Shift\n(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under\nsemantically equivalent prompt rewordings. Applied to ten constrained tasks,\nPBSS reveals consistent, model-specific response shifts, suggesting statistical\nregularities linked to tokenization and decoding. These results highlight an\noverlooked dimension of model evaluation stability under rephrasing and suggest\nthat tokenization strategies and decoding dynamics may contribute to\npost-training quality of service instability.", "comment": "This paper was developed for presentation at ICML 2025 Tokshop\n  Workshop, but is now submitted as a standalone contribution", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10095v1", "AI": {"title_translation": "当意义不变但模型漂移时：评估大型语言模型在Token级行为不稳定性下的服务质量", "tldr": "研究发现LLM在语义相同的提示词但Token级别不同时，会表现出模型特有的行为漂移，这与分词和解码有关，影响服务质量稳定性。", "motivation": "探讨大型语言模型（LLMs）在语义意图相同但Token级别实现不同的提示词（即“提示词变异”）下如何响应，并揭示这种行为漂移对服务质量稳定性的影响。", "method": "提出了一个名为“基于提示词的语义漂移（PBSS）”的诊断框架，用于衡量LLMs在语义等效的提示词重述下的行为漂移。该框架应用于十个受限任务。", "result": "PBSS框架揭示了LLMs存在一致的、模型特定的响应漂移，并表明这些漂移与分词和解码过程的统计规律有关。", "conclusion": "这些结果突出了模型在提示词重述下评估稳定性的一个被忽视的维度，并表明分词策略和解码动态可能导致训练后服务质量的不稳定性。", "translation": "我们研究了大型语言模型如何响应仅在Token级别实现上有所不同但保留相同语义意图的提示，我们称之为提示变异现象。我们提出了基于提示的语义漂移（PBSS），这是一个诊断框架，用于测量大型语言模型在语义等效的提示重述下的行为漂移。将PBSS应用于十项受限任务后，它揭示了一致的、模型特定的响应漂移，表明与分词和解码相关的统计规律。这些结果突出了模型在改写下评估稳定性的一个被忽视的维度，并表明分词策略和解码动态可能导致训练后服务质量的不稳定性。", "summary": "本文探讨了大型语言模型（LLMs）在语义不变但Token级别不同的提示词下产生的行为漂移，称之为“提示词变异”。为量化这种漂移，作者提出了“基于提示词的语义漂移（PBSS）”诊断框架。通过在十个任务上的应用，PBSS揭示了模型特有的响应变化，并指出分词策略和解码动态可能是导致模型服务质量不稳定的重要因素。", "keywords": "大型语言模型, 提示词变异, 服务质量, 分词, 解码", "comments": "这篇论文揭示了LLMs一个重要的、此前未被充分关注的稳定性问题，即即使语义相同，Token级别的微小变化也可能导致模型行为漂移。这对于LLM在实际应用中的鲁棒性和质量保证具有重要意义，尤其是在需要高度稳定性和可预测性的场景。研究强调了分词和解码过程对模型稳定性的影响，为未来的模型设计和评估提供了新的视角。"}}
{"id": "2506.10100", "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models", "authors": ["Yantai Yang", "Yuhao Wang", "Zichen Wen", "Luo Zhongwei", "Chang Zou", "Zhipeng Zhang", "Chuan Wen", "Linfeng Zhang"], "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10100v1", "AI": {"title_translation": "EfficientVLA：面向视觉-语言-动作模型的免训练加速与压缩", "tldr": "EfficientVLA是一个免训练的推理加速框架，通过整合三种策略（语言模块剪枝、视觉令牌优化、动作头特征复用）显著加速并压缩了VLA模型，同时保持了性能。", "motivation": "视觉-语言-动作（VLA）模型，特别是基于扩散的架构，在具身智能方面潜力巨大，但其高计算和内存需求（源于固有的和推理时的冗余）严重阻碍了实际部署。现有加速方法通常是零散的，未能全面解决VLA整个流程中的计算和内存瓶颈。", "method": "论文引入了EfficientVLA，一个结构化、免训练的推理加速框架。它通过协同整合三种目标策略来系统性地消除障碍：1) 根据层间冗余分析，对语言模块中功能不重要的层进行剪枝；2) 通过任务感知策略优化视觉处理路径，选择紧凑、多样化的视觉令牌，平衡任务关键性和信息覆盖；3) 通过策略性缓存和重用关键中间特征，减轻迭代扩散型动作头中的时间计算冗余。", "result": "将该方法应用于标准VLA模型CogACT，在SIMPLER基准测试中实现了1.93倍的推理速度提升，FLOPs降低至28.9%，而成功率仅下降0.6%。", "conclusion": "EfficientVLA作为一个免训练的框架，通过系统性地处理VLA模型中的多方面冗余，显著提升了其推理效率和计算效率，同时保持了高水平的任务性能，从而解决了VLA模型在实际部署中的主要障碍。", "translation": "视觉-语言-动作（VLA）模型，特别是基于扩散的架构，在具身智能方面展现出变革性潜力，但其固有的和推理时的大量冗余导致的高计算和内存需求严重阻碍了其发展。尽管现有加速工作通常针对孤立的低效率问题，但这些零散的解决方案通常未能全面解决整个VLA管道中各种计算和内存瓶颈，从而限制了实际部署。我们引入了EfficientVLA，一个结构化且免训练的推理加速框架，通过协同利用多方面的冗余系统性地消除了这些障碍。EfficientVLA协同整合了三种有针对性的策略：(1) 根据层间冗余分析，对语言模块中功能不重要的层进行剪枝；(2) 通过任务感知策略优化视觉处理路径，选择一组紧凑、多样化的视觉令牌，平衡任务关键性和信息覆盖；(3) 通过策略性缓存和重用关键中间特征，减轻迭代扩散型动作头中的时间计算冗余。我们将我们的方法应用于标准VLA模型CogACT，实现了1.93倍的推理速度提升，并将FLOPs降低至28.9%，在SIMPLER基准测试中成功率仅下降0.6%。", "summary": "本文提出了EfficientVLA，一个免训练的推理加速框架，旨在解决视觉-语言-动作（VLA）模型（特别是基于扩散的架构）在计算和内存方面的瓶颈。EfficientVLA通过整合语言模块剪枝、视觉令牌优化以及动作头特征复用这三种策略，系统性地消除了模型冗余。实验结果显示，应用于CogACT模型时，EfficientVLA实现了1.93倍的推理速度提升和71.1%的FLOPs减少，同时在SIMPLER基准测试中仅导致0.6%的成功率下降，显著提升了VLA模型的实际部署能力。", "keywords": "视觉-语言-动作模型, 模型加速, 模型压缩, 免训练, 具身智能", "comments": "EfficientVLA的创新之处在于其“免训练”和“系统性”地处理VLA模型的多方面冗余，而非零散地解决问题。它通过针对性地优化语言、视觉和动作模块，提供了一个全面的加速方案，这对于VLA模型在资源受限环境下的部署具有重要意义。性能提升显著且性能下降极小，表明其方法的有效性。"}}
{"id": "2506.10423", "title": "PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs", "authors": ["Tony Alex", "Wish Suharitdamrong", "Sara Atito", "Armin Mustafa", "Philip J. B. Jackson", "Imran Razzak", "Muhammad Awais"], "summary": "The integration of audio perception capabilities into Large Language Models\n(LLMs) has enabled significant advances in Audio-LLMs. Although\napplication-focused developments, particularly in curating training data for\nspecific capabilities e.g., audio reasoning, have progressed rapidly, the\nunderlying mechanisms that govern efficient transfer of rich semantic\nrepresentations from audio encoders to LLMs remain under-explored. We\nconceptualize effective audio-LLM interaction as the LLM's ability to\nproficiently probe the audio encoder representations to satisfy textual\nqueries. This paper presents a systematic investigation on how architectural\ndesign choices can affect that. Beginning with a standard Pengi/LLaVA-style\naudio-LLM architecture, we propose and evaluate several modifications guided by\nhypotheses derived from mechanistic interpretability studies and LLM\noperational principles. Our experiments demonstrate that: (1) delaying audio\nintegration until the LLM's initial layers establish textual context that\nenhances its ability to probe the audio representations for relevant\ninformation; (2) the LLM can proficiently probe audio representations\nexclusively through LLM layer's attention submodule, without requiring\npropagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently\nintegrated ensemble of diverse audio encoders provides richer, complementary\nrepresentations, thereby broadening the LLM's capacity to probe a wider\nspectrum of audio information. All hypotheses are evaluated using an identical\nthree-stage training curriculum on a dataset of 5.6 million audio-text pairs,\nensuring controlled comparisons. Our final architecture, which incorporates all\nproposed modifications, achieves relative improvements from 10\\% to 60\\% over\nthe baseline, validating our approach to optimizing cross-modal information\ntransfer in audio-LLMs. Project page: https://ta012.github.io/PAL/", "comment": "21 pages, 11 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10423v1", "AI": {"title_translation": "PAL：通过大型语言模型探究音频编码器——一项关于音频编码器到大型语言模型信息传输的研究", "tldr": "该论文研究了架构选择如何影响音频编码器到大型语言模型（LLMs）的信息传输，发现延迟音频集成、仅使用注意力子模块以及多样化编码器集成显著提升了性能。", "motivation": "尽管音频-LLMs在应用方面发展迅速，但从音频编码器到大型语言模型（LLMs）有效传输丰富语义表示的底层机制仍未得到充分探索。本文旨在系统地研究架构设计选择如何影响LLM熟练探究音频编码器表示以满足文本查询的能力。", "method": "研究从标准的Pengi/LLaVA风格音频-LLM架构开始，提出了并评估了受机械可解释性研究和LLM操作原理指导的几种修改。所有假设均使用包含560万音频-文本对的数据集，通过相同的三阶段训练课程进行评估，以确保受控比较。", "result": "实验表明：1）将音频整合延迟到LLM的初始层建立文本上下文后，可以增强其探究音频表示以获取相关信息的能力；2）LLM可以仅通过LLM层的注意力子模块熟练探究音频表示，而无需传播到其前馈网络（FFN）子模块；3）高效集成的多样化音频编码器集合提供了更丰富、互补的表示，从而拓宽了LLM探究更广泛音频信息的能力。最终的架构相对于基线实现了10%到60%的相对改进。", "conclusion": "所提出的架构设计修改显著优化了音频-LLMs中的跨模态信息传输，验证了其方法。", "translation": "大型语言模型（LLMs）中整合音频感知能力使得音频-LLMs取得了显著进展。尽管以应用为中心的发展，特别是在为特定能力（例如音频推理）策划训练数据方面进展迅速，但控制着从音频编码器到LLMs丰富语义表示高效传输的底层机制仍未得到充分探索。我们将有效的音频-LLM交互概念化为LLM熟练探究音频编码器表示以满足文本查询的能力。本文系统地研究了架构设计选择如何影响这一点。我们从标准的Pengi/LLaVA风格的音频-LLM架构开始，提出了并评估了几种修改，这些修改受到机械可解释性研究和LLM操作原理中得出的假设指导。我们的实验表明：（1）将音频整合延迟到LLM的初始层建立文本上下文后，可以增强其探究音频表示以获取相关信息的能力；（2）LLM可以仅通过LLM层的注意力子模块熟练探究音频表示，而无需传播到其前馈网络（FFN）子模块；（3）高效集成的多样化音频编码器集合提供了更丰富、互补的表示，从而拓宽了LLM探究更广泛音频信息的能力。所有假设均使用一个包含560万音频-文本对的数据集，通过相同的三阶段训练课程进行评估，确保了受控的比较。我们最终的架构，结合了所有提出的修改，相对于基线实现了10%到60%的相对改进，验证了我们优化音频-LLMs中跨模态信息传输的方法。项目页面：https://ta012.github.io/PAL/", "summary": "本文研究了音频编码器到大型语言模型（LLMs）信息传输的未充分探索机制。作者提出了并评估了对标准音频-LLM的架构修改，这些修改受到机械可解释性的指导。主要发现包括延迟音频集成、仅使用注意力子模块进行音频探究以及集成多样化音频编码器的好处。他们的最终架构结合了这些改变，在560万音频-文本对数据集上，相对于基线实现了显著的性能提升（10-60%），验证了其优化跨模态信息传输的方法。", "keywords": "音频-LLMs, 信息传输, 音频编码器, 大型语言模型, 架构设计", "comments": "这篇论文通过系统地探索音频-LLMs中高效信息传输的架构设计选择，做出了重要贡献，该领域尽管应用快速发展但仍未得到充分探索。关于延迟集成、仅注意力探究和编码器集成的发现为未来的音频-LLM开发提供了实用指导，并强调了机制理解的重要性。显著的性能改进证明了其所提出修改的有效性。"}}
{"id": "2506.10261", "title": "Enhanced randomized Douglas-Rachford method: Improved probabilities and adaptive momentum", "authors": ["Liqi Guo", "Ruike Xiang", "Deren Han", "Jiaxin Xie"], "summary": "Randomized iterative methods have gained recent interest in machine learning\nand signal processing for solving large-scale linear systems. One such example\nis the randomized Douglas-Rachford (RDR) method, which updates the iterate by\nreflecting it through two randomly selected hyperplanes and taking a convex\ncombination with the current point. In this work, we enhance RDR by introducing\nimproved sampling strategies and an adaptive heavy-ball momentum scheme.\nSpecifically, we incorporate without-replacement and volume sampling into RDR,\nand establish stronger convergence guarantees compared to conventional i.i.d.\nsampling. Furthermore, we develop an adaptive momentum mechanism that\ndynamically adjusts step sizes and momentum parameters based on previous\niterates, and prove that the resulting method achieves linear convergence in\nexpectation with improved convergence bounds. Numerical experiments demonstrate\nthat the enhanced RDR method consistently outperforms the original version,\nproviding substantial practical benefits across a range of problem settings.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10261v1", "AI": {"title_translation": "增强型随机Douglas-Rachford方法：改进的概率和自适应动量", "tldr": "本文通过引入改进的采样策略和自适应动量机制，增强了随机Douglas-Rachford (RDR) 方法，实现了更强的收敛保证和更快的收敛速度。", "motivation": "随机迭代方法在机器学习和信号处理中解决大规模线性系统方面受到关注，其中随机Douglas-Rachford (RDR) 方法是一个例子。本文旨在增强RDR方法以获得更好的性能和收敛保证。", "method": "1. 引入了无放回抽样和体积抽样到RDR中。2. 开发了一种自适应重球动量机制，可以根据之前的迭代动态调整步长和动量参数。", "result": "1. 与传统的独立同分布(i.i.d.)抽样相比，建立了更强的收敛保证。2. 证明了新方法在期望上实现了线性收敛，并改进了收敛界。3. 数值实验表明，增强型RDR方法始终优于原始版本，在各种问题设置中提供了显著的实际效益。", "conclusion": "通过引入改进的采样策略和自适应动量机制，增强型随机Douglas-Rachford方法在理论和实践上都表现出优越的性能，具有更强的收敛保证和更快的收敛速度。", "translation": "随机迭代方法在机器学习和信号处理中解决大规模线性系统方面最近受到了关注。其中一个例子是随机Douglas-Rachford (RDR) 方法，它通过将迭代点通过两个随机选择的超平面进行反射，并与当前点进行凸组合来更新迭代。在这项工作中，我们通过引入改进的采样策略和自适应重球动量方案来增强RDR。具体来说，我们将无放回抽样和体积抽样融入到RDR中，并与传统的独立同分布(i.i.d.)抽样相比，建立了更强的收敛保证。此外，我们开发了一种自适应动量机制，可以根据之前的迭代动态调整步长和动量参数，并证明由此产生的方法在期望上实现了线性收敛，并改进了收敛界。数值实验表明，增强型RDR方法始终优于原始版本，在各种问题设置中提供了显著的实际效益。", "summary": "本文提出了一种增强型随机Douglas-Rachford (RDR) 方法，旨在解决大规模线性系统。该方法通过引入无放回抽样和体积抽样等改进的采样策略，以及一个自适应重球动量机制来优化RDR。研究证明，与传统RDR相比，新方法具有更强的收敛保证，并在期望上实现线性收敛，同时改进了收敛界。数值实验验证了其在实际应用中超越原始RDR的显著优势。", "keywords": "随机Douglas-Rachford方法, 自适应动量, 采样策略, 线性系统, 收敛性", "comments": "本文的创新点在于将先进的采样策略（无放回和体积抽样）与自适应动量机制相结合，显著提升了随机Douglas-Rachford方法的性能。这对于解决大规模机器学习和信号处理中的线性系统问题具有重要意义，其理论收敛性证明和实际效果验证都很有价值。"}}
{"id": "2506.10042", "title": "Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions", "authors": ["Ece Gumusel"], "summary": "In an era of increasing interaction with artificial intelligence (AI), users\nface evolving privacy decisions shaped by complex, uncertain factors. This\npaper introduces Multiverse Privacy Theory, a novel framework in which each\nprivacy decision spawns a parallel universe, representing a distinct potential\noutcome based on user choices over time. By simulating these universes, this\ntheory provides a foundation for understanding privacy through the lens of\ncontextual integrity, evolving preferences, and probabilistic decision-making.\nFuture work will explore its application using real-world, scenario-based\nsurvey data.", "comment": "5 pages, 1 figure, 1 table", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10042v1", "AI": {"title_translation": "多重宇宙隐私理论：复杂用户-AI交互中的情境风险", "tldr": "提出多重宇宙隐私理论，通过模拟平行宇宙理解用户在AI交互中的隐私决策。", "motivation": "在与人工智能（AI）交互日益增多的时代，用户面临由复杂、不确定因素塑造的不断演变的隐私决策。", "method": "本文引入了多重宇宙隐私理论，这是一个新颖的框架，其中每个隐私决策都会产生一个平行宇宙，代表基于用户随时间选择的不同潜在结果。通过模拟这些宇宙，该理论得以构建。", "result": "该理论为通过情境完整性、不断演变的偏好和概率决策的视角理解隐私提供了基础。", "conclusion": "未来的工作将探索该理论在真实世界、基于场景的调查数据中的应用。", "translation": "在与人工智能（AI）交互日益增多的时代，用户面临由复杂、不确定因素塑造的不断演变的隐私决策。本文引入了多重宇宙隐私理论，这是一个新颖的框架，其中每个隐私决策都会产生一个平行宇宙，代表基于用户随时间选择的不同潜在结果。通过模拟这些宇宙，该理论为通过情境完整性、不断演变的偏好和概率决策的视角理解隐私提供了基础。未来的工作将探索其在真实世界、基于场景的调查数据中的应用。", "summary": "本文提出多重宇宙隐私理论，这是一个理解复杂AI交互中用户隐私决策的新颖框架。它将每个隐私选择建模为创建一个具有不同结果的平行宇宙，从而能够通过情境完整性、不断演变的偏好和概率决策来研究隐私。未来的工作将把该理论应用于真实世界数据。", "keywords": "多重宇宙隐私理论, AI交互, 情境风险, 隐私决策, 平行宇宙", "comments": "该论文引入了一个高度创新的概念框架（多重宇宙隐私理论），以解决AI交互中隐私的复杂和动态性质。其优势在于提供了一个新颖的视角（平行宇宙）来建模不断演变的用户偏好和概率结果，超越了静态隐私模型。其重要性在于为理解情境隐私风险提供了一个基础理论。一个限制是，它目前是一个理论框架，实际应用和验证留待未来的工作。"}}
{"id": "2506.10559", "title": "From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations", "authors": ["Yutong Zhou", "Masahiro Ryo"], "summary": "Explaining why the species lives at a particular location is important for\nunderstanding ecological systems and conserving biodiversity. However, existing\necological workflows are fragmented and often inaccessible to non-specialists.\nWe propose an end-to-end visual-to-causal framework that transforms a species\nimage into interpretable causal insights about its habitat preference. The\nsystem integrates species recognition, global occurrence retrieval,\npseudo-absence sampling, and climate data extraction. We then discover causal\nstructures among environmental features and estimate their influence on species\noccurrence using modern causal inference methods. Finally, we generate\nstatistically grounded, human-readable causal explanations from structured\ntemplates and large language models. We demonstrate the framework on a bee and\na flower species and report early results as part of an ongoing project,\nshowing the potential of the multimodal AI assistant backed up by a recommended\necological modeling practice for describing species habitat in\nhuman-understandable language.", "comment": "Code will be released at: https://github.com/Yutong-Zhou-cv/BioX", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10559v1", "AI": {"title_translation": "从图像到洞察：用通俗语言解释栖息地，实现可解释的生物多样性监测", "tldr": "本文提出了一个端到端的可视化到因果框架，用于生物多样性监测，该框架将物种图像转化为可解释的栖息地偏好因果洞察，并通过大语言模型生成通俗易懂的解释。", "motivation": "解释物种为何生活在特定地点对于理解生态系统和保护生物多样性至关重要。然而，现有的生态工作流程是碎片化的，且非专业人士往往难以接触。", "method": "提出一个端到端的可视化到因果框架，将物种图像转化为关于其栖息地偏好的可解释因果洞察。该系统整合了物种识别、全球出现数据检索、伪缺失采样和气候数据提取。然后，利用现代因果推断方法发现环境特征之间的因果结构并估计它们对物种出现的影响。最后，通过结构化模板和大语言模型生成基于统计的、人类可读的因果解释。", "result": "该框架在一种蜜蜂和一种花卉物种上进行了演示，并报告了早期结果，显示了多模态AI助手在推荐的生态建模实践支持下，用人类可理解的语言描述物种栖息地的潜力。", "conclusion": "该框架展示了通过整合AI技术和生态建模实践，生成可解释的、人类可理解的物种栖息地解释，从而促进生物多样性监测和生态系统理解的潜力。", "translation": "解释物种为何生活在特定地点对于理解生态系统和保护生物多样性至关重要。然而，现有的生态工作流程是碎片化的，且非专业人士往往难以接触。我们提出了一个端到端的可视化到因果框架，该框架将物种图像转化为关于其栖息地偏好的可解释因果洞察。该系统整合了物种识别、全球出现数据检索、伪缺失采样和气候数据提取。然后，我们利用现代因果推断方法发现环境特征之间的因果结构并估计它们对物种出现的影响。最后，我们通过结构化模板和大语言模型生成基于统计的、人类可读的因果解释。我们在一种蜜蜂和一种花卉物种上演示了该框架，并报告了作为正在进行的项目一部分的早期结果，显示了由推荐的生态建模实践支持的多模态AI助手用人类可理解的语言描述物种栖息地的潜力。", "summary": "本文提出了一个名为“从图像到洞察”的端到端可视化到因果框架，旨在为生物多样性监测提供可解释的栖息地解释。该框架通过整合物种识别、数据检索、采样和气候数据提取，从物种图像中提取可解释的因果洞察，揭示环境特征对物种出现的影响。通过运用现代因果推断方法和大语言模型，系统能够生成统计学上严谨且人类可读的栖息地因果解释。初步结果表明，该系统在帮助非专业人士理解复杂生态数据方面具有巨大潜力，有助于促进生物多样性保护。", "keywords": "生物多样性监测, 可解释人工智能, 因果推断, 栖息地解释, 生态系统", "comments": "该论文的创新点在于提出了一个端到端的可视化到因果框架，将图像数据与因果推断相结合，并利用大语言模型生成人类可读的解释。这极大地提高了生态学工作流程的可解释性和可访问性，对于非专业人士理解复杂的生物多样性数据具有重要意义。其潜力在于促进生物多样性监测和保护，但目前仍处于早期结果阶段，未来的验证和扩展性值得关注。"}}
{"id": "2506.10280", "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10280v1", "AI": {"title_translation": "基于AI的软件漏洞检测：系统文献综述", "tldr": "本研究对2018年至2023年间基于AI的软件漏洞检测（SVD）研究进行了系统综述，提供了技术分类，并指出了数据质量、可复现性和可解释性等局限性，同时提出了联邦学习和量子神经网络等新兴机会。", "motivation": "源代码中的软件漏洞会带来严重网络安全风险，促使检测方法从传统方式（如静态分析、基于规则的匹配）转向AI驱动的方法。本研究旨在系统回顾AI在SVD领域的应用。", "method": "本研究对2018年至2023年间的软件漏洞检测（SVD）研究进行了系统文献综述，提供了技术、特征表示和嵌入方法的全面分类法。", "result": "分析显示，91%的研究使用了基于AI的方法，其中图基模型最为普遍。研究识别出主要局限性，包括数据集质量、可复现性和可解释性。同时，指出了联邦学习和量子神经网络等未充分探索技术中的新兴机会。", "conclusion": "AI在软件漏洞检测中占据主导地位，但仍面临数据集、可复现性和可解释性等挑战。未来的研究应探索联邦学习和量子神经网络等新兴技术，以克服现有局限并推动领域发展。", "translation": "源代码中的软件漏洞构成严重的网络安全风险，促使检测方法从传统方法（例如静态分析、基于规则的匹配）转向AI驱动的方法。本研究对2018年至2023年间的软件漏洞检测（SVD）研究进行了系统综述，提供了技术、特征表示和嵌入方法的全面分类法。我们的分析显示，91%的研究使用了基于AI的方法，其中图基模型最为普遍。我们识别出主要局限性，包括数据集质量、可复现性和可解释性，并强调了联邦学习和量子神经网络等未充分探索技术中的新兴机会，为未来的研究提供了路线图。", "summary": "本研究对2018年至2023年间基于AI的软件漏洞检测研究进行了系统文献综述。结果显示，AI方法已成为主流，特别是图基模型。研究指出了当前方法的局限性，如数据集质量、可复现性和可解释性问题，并为未来研究提出了联邦学习和量子神经网络等新兴方向。", "keywords": "软件漏洞检测, 人工智能, 系统综述, 图神经网络, 网络安全", "comments": "这篇系统综述及时地总结了AI在软件漏洞检测领域的最新进展和趋势，为研究人员提供了宝贵的参考。其创新之处在于对现有技术的全面分类和对未来研究方向的清晰指引。论文指出的数据质量、可复现性和可解释性等局限性是当前AI应用面临的普遍挑战，对于推动该领域的实际落地具有重要意义。"}}
{"id": "2506.10240", "title": "Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators", "authors": ["Rongfei Li", "Francis Assadian"], "summary": "Image-based visual servoing (IBVS) methods have been well developed and used\nin many applications, especially in pose (position and orientation) alignment.\nHowever, most research papers focused on developing control solutions when 3D\npoint features can be detected inside the field of view. This work proposes an\ninnovative feedforward-feedback adaptive control algorithm structure with the\nYoula Parameterization method. A designed feature estimation loop ensures\nstable and fast motion control when point features are outside the field of\nview. As 3D point features move inside the field of view, the IBVS feedback\nloop preserves the precision of the pose at the end of the control period.\nAlso, an adaptive controller is developed in the feedback loop to stabilize the\nsystem in the entire range of operations. The nonlinear camera and robot\nmanipulator model is linearized and decoupled online by an adaptive algorithm.\nThe adaptive controller is then computed based on the linearized model\nevaluated at current linearized point. The proposed solution is robust and easy\nto implement in different industrial robotic systems. Various scenarios are\nused in simulations to validate the effectiveness and robust performance of the\nproposed controller.", "comment": "22 pages, 13 figures. To appear in: Innovative Adaptive Image-Based\n  Visual Servoing Control of 6 DoFs Industrial Robot Manipulators, IntechOpen,\n  2024. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.1004857", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10240v1", "AI": {"title_translation": "6自由度工业机器人机械手的创新自适应图像视觉伺服控制", "tldr": "本文提出了一种创新的前馈-反馈自适应控制算法，结合Youla参数化和特征估计，解决了图像视觉伺服中3D点特征超出视野的问题，并在仿真中验证了其鲁棒性和有效性。", "motivation": "现有的图像视觉伺服（IBVS）方法主要关注3D点特征在视野内的情况，而当特征点超出视野时，控制解决方案不足。", "method": "提出了一种结合Youla参数化的创新前馈-反馈自适应控制算法结构。该方法包含一个设计的特征估计循环，用于处理视野外的特征点；当特征点进入视野内时，IBVS反馈循环保持姿态精度。此外，开发了一个自适应控制器，通过在线线性化和解耦非线性相机和机器人机械手模型，在整个操作范围内稳定系统。", "result": "该方法在特征点超出视野时能确保稳定快速的运动控制，在控制周期结束时保持姿态精度，并在整个操作范围内稳定系统。所提出的解决方案鲁棒且易于在不同的工业机器人系统中实现。通过多种仿真场景验证了其有效性和鲁棒性能。", "conclusion": "所提出的自适应图像视觉伺服控制方案在处理3D点特征超出视野的情况下表现出有效、鲁棒且易于实现的性能，能够确保工业机器人机械手的稳定和精确姿态对齐。", "translation": "图像视觉伺服（IBVS）方法已经得到了很好的发展并应用于许多领域，特别是在姿态（位置和方向）对齐方面。然而，大多数研究论文都集中在当3D点特征可以在视野内检测到时开发控制解决方案。这项工作提出了一种创新的前馈-反馈自适应控制算法结构，并结合了Youla参数化方法。一个设计的特征估计循环确保当点特征在视野外时也能实现稳定快速的运动控制。当3D点特征进入视野内时，IBVS反馈循环保持控制周期结束时姿态的精度。此外，反馈循环中开发了一个自适应控制器，以在整个操作范围内稳定系统。非线性相机和机器人机械手模型通过自适应算法在线线性化和解耦。然后，基于在当前线性化点评估的线性化模型计算自适应控制器。所提出的解决方案鲁棒且易于在不同的工业机器人系统中实现。在仿真中使用了各种场景来验证所提出控制器的有效性和鲁棒性能。", "summary": "本文针对现有图像视觉伺服（IBVS）方法在3D点特征超出视野时控制不足的问题，提出了一种创新的前馈-反馈自适应控制算法。该算法结合Youla参数化和特征估计循环，确保在特征点内外视野时都能实现稳定、快速且高精度的姿态对齐。通过在线线性化和解耦非线性模型，自适应控制器增强了系统的鲁棒性和稳定性。仿真结果验证了该方案的有效性和易于实现的特点。", "keywords": "图像视觉伺服, 自适应控制, 机器人机械手, Youla参数化, 特征估计", "comments": "本文的创新点在于提出了一个结合Youla参数化的前馈-反馈自适应控制框架，有效解决了图像视觉伺服中3D点特征超出视野时的控制难题，这是现有研究较少涉猎的领域。通过引入特征估计循环和自适应控制器，提升了系统在复杂工况下的鲁棒性和精度，使其更适用于实际工业应用。"}}
{"id": "2506.10324", "title": "Beyond Compliance: A User-Autonomy Framework for Inclusive and Customizable Web Accessibility", "authors": ["Lalitha A R"], "summary": "This paper proposes a shift from compliance-centered web accessibility to a\ncare-driven model that prioritizes user autonomy, using neurodivergent users as\na catalyst case for broader personalization needs. While accessibility\nstandards offer a flexible framework, they are often interpreted and\nimplemented as static compliance checklists, our approach reframes it as a\nflexible, user-centered process. We introduce a customizable Comfort Mode\nframework that allows users to adapt interface settings, such as contrast,\ntypography, motion, and scaling, according to their individual needs, while\nretaining the brand's core visual identity. Grounded in psychological and\ncognitive accessibility principles, our design supports personalization without\nsacrificing creative freedom. We present both minimal and advanced\nimplementation models with mock-ups, demonstrating how inclusive design can be\nseamlessly integrated at minimal cost. This approach aims to broaden digital\ninclusivity by offering autonomy to those who require it, without imposing\nchanges on those who do not. The proposed system is adaptable, scalable, and\nsuitable for a wide range of users and brands, offering a new paradigm where\nuser autonomy, aesthetic integrity, and accessibility converge not through\ncompromise, but through choice.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10324v1", "AI": {"title_translation": "超越合规：一个包容性与可定制网络可访问性的用户自主框架", "tldr": "提出一种以用户自主为中心的可定制“舒适模式”框架，将网络可访问性从静态合规转变为灵活的个性化体验，以提升数字包容性。", "motivation": "现有的网络可访问性标准常被视为静态合规清单，未能充分满足用户个性化需求，特别是神经多样性用户。论文旨在提出一种以用户自主为中心的关怀模式。", "method": "提出一个可定制的“舒适模式”框架，允许用户根据个人需求调整界面设置（如对比度、排版、动态和缩放），同时保留品牌核心视觉识别。该设计基于心理和认知可访问性原则，并展示了最小和高级的实现模型及模型图。", "result": "展示了包容性设计如何以最小成本无缝集成到网站中。该方法旨在通过提供选择而非强加改变，扩大数字包容性。", "conclusion": "论文提出了一种新的范式，即用户自主性、美学完整性和可访问性通过选择而非妥协而融合，该系统具有适应性、可扩展性，适用于广泛的用户和品牌。", "translation": "本文提出将以合规为中心的网络可访问性转变为以关怀为驱动的模型，该模型优先考虑用户自主性，并以神经多样性用户作为更广泛个性化需求的催化案例。尽管可访问性标准提供了一个灵活的框架，但它们通常被解释和实施为静态的合规清单，我们的方法将其重新定义为一个灵活的、以用户为中心的过程。我们引入了一个可定制的“舒适模式”框架，允许用户根据其个人需求调整界面设置，例如对比度、排版、动态和缩放，同时保留品牌的核心视觉识别。我们的设计以心理和认知可访问性原则为基础，在不牺牲创意自由的情况下支持个性化。我们提供了最小和高级的实现模型及模型图，展示了如何以最小成本无缝集成包容性设计。这种方法旨在通过向需要的人提供自主权，而不向不需要的人强加改变，从而扩大数字包容性。所提出的系统具有适应性、可扩展性，适用于广泛的用户和品牌，提供了一种新的范式，其中用户自主性、美学完整性和可访问性不是通过妥协而是通过选择而融合。", "summary": "本文提出一个以用户自主为中心的可定制“舒适模式”框架，旨在将网络可访问性从静态合规模式转变为灵活、关怀驱动的个性化体验。该框架允许用户根据自身需求调整界面设置，同时保持品牌视觉一致性。论文基于心理和认知原则，展示了如何以低成本实现包容性设计，从而提升数字包容性，实现用户自主、美学和可访问性的融合。", "keywords": "网络可访问性, 用户自主, 舒适模式, 个性化, 数字包容性", "comments": "这篇论文的创新点在于其将网络可访问性从传统的“合规性检查表”模式提升到以“用户自主”为核心的关怀驱动模型，特别是通过引入“舒适模式”框架，允许用户进行个性化设置，同时兼顾品牌美学。这种方法对于提升数字包容性具有重要意义，因为它强调了用户选择权，避免了“一刀切”的强制性改变，为不同需求的用户提供了更灵活、更友好的数字体验。"}}
{"id": "2506.10258", "title": "Synchronization for Fault-Tolerant Quantum Computers", "authors": ["Satvik Maurya", "Swamit Tannu"], "summary": "Quantum Error Correction (QEC) codes store information reliably in logical\nqubits by encoding them in a larger number of less reliable qubits. The surface\ncode, known for its high resilience to physical errors, is a leading candidate\nfor fault-tolerant quantum computing (FTQC). Logical qubits encoded with the\nsurface code can be in different phases of their syndrome generation cycle,\nthereby introducing desynchronization in the system. This can occur due to the\nproduction of non-Clifford states, dropouts due to fabrication defects, and the\nuse of other QEC codes with the surface code to reduce resource requirements.\nLogical operations require the syndrome generation cycles of the logical qubits\ninvolved to be synchronized. This requires the leading qubit to pause or slow\ndown its cycle, allowing more errors to accumulate before the next cycle,\nthereby increasing the risk of uncorrectable errors.\n  To synchronize the syndrome generation cycles of logical qubits, we define\nthree policies - Passive, Active, and Hybrid. The Passive policy is the\nbaseline, and the simplest, wherein the leading logical qubits idle until they\nare synchronized with the remaining logical qubits. On the other hand, the\nActive policy aims to slow the leading logical qubits down gradually, by\ninserting short idle periods before multiple code cycles. This approach reduces\nthe logical error rate (LER) by up to 2.4x compared to the Passive policy. The\nHybrid policy further reduces the LER by up to 3.4x by reducing the\nsynchronization slack and running a few additional rounds of error correction.\nFurthermore, the reduction in the logical error rate with the proposed\nsynchronization policies enables a speedup in decoding latency of up to 2.2x\nwith a circuit-level noise model.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10258v1", "AI": {"title_translation": "容错量子计算机的同步", "tldr": "该论文提出了三种同步逻辑量子比特综合征生成周期的策略（被动、主动和混合），以解决容错量子计算中表面代码去同步化的问题。主动策略将逻辑错误率降低2.4倍，混合策略降低3.4倍，并能将解码延迟加速2.2倍。", "motivation": "量子纠错（QEC）代码通过将信息编码到更多不可靠的量子比特中，在逻辑量子比特中可靠地存储信息。表面代码是容错量子计算（FTQC）的主要候选。然而，由于非Clifford态的产生、制造缺陷导致的脱落以及与其他QEC代码结合使用，使用表面代码编码的逻辑量子比特可能会出现综合征生成周期的不同步。逻辑操作要求所涉及逻辑量子比特的综合征生成周期同步，但这会导致领先的量子比特暂停或减慢其周期，从而在下一周期之前积累更多错误，增加不可纠正错误的风险。因此，需要有效的同步策略。", "method": "为了同步逻辑量子比特的综合征生成周期，本文定义了三种策略：被动（Passive）、主动（Active）和混合（Hybrid）。被动策略是最简单、作为基线的策略，其中领先的逻辑量子比特空闲直到与其余逻辑量子比特同步。主动策略旨在通过在多个代码周期前插入短空闲期来逐渐减慢领先的逻辑量子比特。混合策略通过减少同步松弛并运行额外的错误纠正轮次来进一步优化。", "result": "与被动策略相比，主动策略将逻辑错误率（LER）降低了高达2.4倍。混合策略进一步将LER降低了高达3.4倍。此外，所提出的同步策略在电路级噪声模型下，将解码延迟加速了高达2.2倍。", "conclusion": "本文提出的同步策略能有效降低容错量子计算机中逻辑量子比特的逻辑错误率，并显著加速解码过程，从而提升了表面代码在实际应用中的可行性。", "translation": "量子纠错（QEC）代码通过将信息编码到数量更多的、可靠性较低的量子比特中，从而在逻辑量子比特中可靠地存储信息。表面代码因其对物理错误的高弹性而成为容错量子计算（FTQC）的主要候选方案。然而，用表面代码编码的逻辑量子比特可能处于其综合征生成周期的不同阶段，从而在系统中引入去同步化。这可能由于非Clifford态的产生、制造缺陷导致的脱落以及将其他QEC代码与表面代码结合使用以减少资源需求而发生。逻辑操作要求所涉及逻辑量子比特的综合征生成周期同步。这要求领先的量子比特暂停或减慢其周期，从而在下一周期之前积累更多错误，从而增加不可纠正错误的风险。\n为了同步逻辑量子比特的综合征生成周期，我们定义了三种策略——被动、主动和混合。被动策略是基线，也是最简单的，其中领先的逻辑量子比特空闲直到它们与其余逻辑量子比特同步。另一方面，主动策略旨在通过在多个代码周期之前插入短空闲期来逐渐减慢领先的逻辑量子比特。这种方法与被动策略相比，将逻辑错误率（LER）降低了高达2.4倍。混合策略通过减少同步松弛并运行一些额外的错误纠正轮次，进一步将LER降低了高达3.4倍。此外，所提出的同步策略在电路级噪声模型下，将解码延迟加速了高达2.2倍。", "summary": "该论文解决了容错量子计算中表面代码逻辑量子比特综合征生成周期不同步的问题。这种不同步会增加不可纠正错误的风险。为解决此问题，作者提出了三种同步策略：被动、主动和混合。实验结果表明，主动策略可将逻辑错误率降低高达2.4倍，而混合策略可进一步降低高达3.4倍，同时还能将解码延迟加速高达2.2倍，显著提升了容错量子计算的性能。", "keywords": "量子计算, 表面代码, 同步, 容错, 量子纠错", "comments": "该论文解决了容错量子计算中一个关键且实际的挑战，即逻辑量子比特的同步问题。通过提出三种新颖的同步策略，特别是主动和混合策略，它显著降低了逻辑错误率并加速了解码过程，为构建更鲁棒的量子计算机提供了重要的工程优化方案。其创新之处在于系统性地定义并评估了不同程度的同步干预措施，并量化了它们对系统性能的提升。"}}
{"id": "2506.10470", "title": "TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference", "authors": ["Hongbin Zhang", "Taosheng Wei", "Zhenyi Zheng", "Jiangsu Du", "Zhiguang Chen", "Yutong Lu"], "summary": "As the model size continuously increases, pipeline parallelism shows great\npromise in throughput-oriented LLM inference due to its low demand on\ncommunications. However, imbalanced pipeline workloads and complex data\ndependencies in the prefill and decode phases result in massive pipeline\nbubbles and further severe performance reduction. To better exploit the\npipeline parallelism for high-throughput LLM inference, we propose TD-Pipe,\nwith the key idea lies in the temporally-disaggregated pipeline parallelism\narchitecture. Specifically, this architecture disaggregates the prefill and\ndecode phases in the temporal dimension, so as to eliminate pipeline bubbles\ncaused by the phase switching. TD-Pipe identifies potential issues of\nexploiting the novel architecture and provides solutions. First, a\nhierarchy-controller structure is used to better coordinate devices in pipeline\nparallelism by decoupling the scheduling from execution. Second, the AI-based\ngreedy prefill approach aggressively performs more prefills by predicting the\noutput length and simulating the memory usage. Third, the inter-batch work\nstealing approach dynamically balances decode phase workloads between different\nbatches to reduce bubbles. Forth, the spatial-temporal intensity comparison\napproach determines the optimal switch from decode to prefill by comparing the\nperformance drop from reduced computational intensity with that from phase\nswitching bubbles. Extensive experiments show that TD-Pipe effectively\nincreases the throughput of LLM inference by up to 1.91x over the existing\ntensor parallel approach and 2.73x over the existing pipeline parallel approach\non GPU nodes with only PCIe interconnection.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10470v1", "AI": {"title_translation": "TD-Pipe：面向高吞吐量LLM推理的时序解耦流水线并行架构", "tldr": "TD-Pipe提出了一种时序解耦的流水线并行架构，通过分离LLM推理的预填充和解码阶段，并引入多种优化策略，显著提升了LLM推理的吞吐量。", "motivation": "由于模型规模的持续增长，流水线并行在面向吞吐量的LLM推理中显示出巨大潜力，但预填充和解码阶段不平衡的工作负载以及复杂的数据依赖性导致大量流水线气泡，从而严重降低了性能。", "method": "本文提出了TD-Pipe，其核心思想是时序解耦的流水线并行架构。该架构在时间维度上解耦了预填充和解码阶段，以消除阶段切换引起的流水线气泡。TD-Pipe还提供了以下解决方案：1. 使用分层控制器结构解耦调度和执行，以更好地协调设备。2. 采用基于AI的贪婪预填充方法，通过预测输出长度和模拟内存使用来积极执行更多预填充。3. 采用批间工作窃取方法，动态平衡不同批次间的解码阶段工作负载。4. 采用时空强度比较方法，确定从解码到预填充的最佳切换点。", "result": "实验表明，TD-Pipe在仅有PCIe互连的GPU节点上，将LLM推理的吞吐量比现有张量并行方法提高了1.91倍，比现有流水线并行方法提高了2.73倍。", "conclusion": "TD-Pipe通过其时序解耦的流水线并行架构和一系列优化策略，有效解决了LLM推理中流水线气泡导致的性能下降问题，显著提升了LLM推理的吞吐量。", "translation": "随着模型规模的持续增长，流水线并行在面向吞吐量的LLM推理中显示出巨大潜力，因为它对通信的需求较低。然而，预填充和解码阶段不平衡的流水线工作负载和复杂的数据依赖性导致大量流水线气泡，并进一步严重降低了性能。为了更好地利用流水线并行实现高吞吐量LLM推理，我们提出了TD-Pipe，其关键思想在于时序解耦的流水线并行架构。具体来说，该架构在时间维度上解耦了预填充和解码阶段，从而消除了由阶段切换引起的流水线气泡。TD-Pipe识别了利用这种新架构的潜在问题并提供了解决方案。首先，使用分层控制器结构，通过解耦调度和执行来更好地协调流水线并行中的设备。其次，基于AI的贪婪预填充方法通过预测输出长度和模拟内存使用，积极执行更多预填充。第三，批间工作窃取方法动态平衡不同批次间的解码阶段工作负载以减少气泡。第四，时空强度比较方法通过比较计算强度降低导致的性能下降与阶段切换气泡导致的性能下降来确定从解码到预填充的最佳切换。大量实验表明，TD-Pipe在仅有PCIe互连的GPU节点上，将LLM推理的吞吐量比现有张量并行方法提高了1.91倍，比现有流水线并行方法提高了2.73倍。", "summary": "TD-Pipe提出了一种时序解耦的流水线并行架构，旨在解决大型语言模型(LLM)推理中预填充和解码阶段不平衡工作负载导致的流水线气泡和性能下降问题。该架构通过在时间维度上分离预填充和解码阶段来消除气泡，并引入了分层控制器、基于AI的贪婪预填充、批间工作窃取以及时空强度比较等优化策略。实验结果表明，TD-Pipe显著提升了LLM推理的吞吐量，相比现有张量并行和流水线并行方法分别提高了1.91倍和2.73倍。", "keywords": "LLM推理, 流水线并行, 吞吐量, 时序解耦, TD-Pipe", "comments": "TD-Pipe的创新之处在于提出了时序解耦的流水线并行架构，这是一种新颖的思路，有效解决了LLM推理中预填充和解码阶段的固有矛盾。通过在时间维度上解耦并结合多种精细化的调度和优化策略，该方法显著提升了LLM推理的效率和吞吐量，对于资源受限或需要高吞吐量的LLM部署具有重要意义。其提出的具体优化方法，如AI-based greedy prefill和inter-batch work stealing，也显示了对实际系统挑战的深刻理解。"}}
{"id": "2506.10964", "title": "The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins", "authors": ["Rico H Herzog", "Till Degkwitz", "Trivik Verma"], "summary": "Urban digital twins are increasingly perceived as a way to pool the growing\ndigital resources of cities for the purpose of a more sustainable and\nintegrated urban planning. Models and simulations are central to this\nundertaking: They enable \"what if?\" scenarios, create insights and describe\nrelationships between the vast data that is being collected. However, the\nprocess of integrating and subsequently using models in urban digital twins is\nan inherently complex undertaking. It raises questions about how to represent\nurban complexity, how to deal with uncertain assUrban Model Platformtions and\nmodeling paradigms, and how to capture underlying power relations. Existent\napproaches in the domain largely focus on monolithic and centralized solutions\nin the tradition of neoliberal city-making, oftentimes prohibiting pluralistic\nand open interoperable models. Using a participatory design for participatory\nsystems approach together with the City of Hamburg, Germany, we find that an\nopen Urban Model Platform can function both as a public technological backbone\nfor modeling and simulation in urban digital twins and as a socio-technical\nframework for a collaborative and pluralistic representation of urban\nprocesses. Such a platform builds on open standards, allows for a decentralized\nintegration of models, enables communication between models and supports a\nmulti-model approach to representing urban systems.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10964v1", "AI": {"title_translation": "城市模型平台：城市数字孪生中建模与仿真的公共骨干", "tldr": "本文提出一个开放的城市模型平台，作为城市数字孪生中建模与仿真的公共技术骨干和社会技术框架，以实现更具包容性和协作性的城市规划。", "motivation": "城市数字孪生在可持续和综合城市规划中日益重要，但其模型和仿真整合过程复杂，现有方法多为单一和中心化，限制了多元化和开放互操作性。", "method": "采用参与式设计方法，与德国汉堡市合作，探索开放的城市模型平台。", "result": "发现开放的城市模型平台可以作为城市数字孪生中建模与仿真的公共技术骨干，同时也是一个促进城市过程协作和多元化表示的社会技术框架。该平台基于开放标准，支持模型去中心化集成、模型间通信和多模型方法。", "conclusion": "一个开放的城市模型平台能够克服现有城市数字孪生中模型整合的复杂性和局限性，提供一个公共的、支持协作和多元化表示的基础设施。", "translation": "城市数字孪生正日益被视为汇集城市日益增长的数字资源，以实现更可持续和综合城市规划的一种方式。模型和仿真在这一工作中至关重要：它们能够实现“如果……会怎样？”的场景模拟，提供洞察力，并描述所收集的大量数据之间的关系。然而，在城市数字孪生中整合和随后使用模型的过程中，本身就是一项复杂的任务。它引发了如何表示城市复杂性、如何处理不确定假设和建模范式，以及如何捕捉潜在权力关系的问题。该领域现有的方法大多侧重于新自由主义城市建设传统中的单一和集中式解决方案，常常禁止多元化和开放互操作的模型。通过与德国汉堡市合作，我们采用参与式系统方法进行参与式设计，发现一个开放的城市模型平台既可以作为城市数字孪生中建模与仿真的公共技术骨干，也可以作为城市过程协作和多元化表示的社会技术框架。这样的平台建立在开放标准之上，允许模型的去中心化集成，实现模型之间的通信，并支持表示城市系统的多模型方法。", "summary": "本文提出并探讨了一个开放的城市模型平台，旨在解决城市数字孪生中模型整合的复杂性及现有中心化方案的局限性。该平台通过与德国汉堡市的合作，采用参与式设计方法，被证明能够作为城市数字孪生中建模与仿真的公共技术骨干和社会技术框架，支持基于开放标准、去中心化集成、模型间通信和多模型方法的协作式与多元化城市过程表示。", "keywords": "城市数字孪生, 城市模型平台, 建模与仿真, 开放标准, 参与式设计", "comments": "这篇论文的创新之处在于提出了一个开放且去中心化的城市模型平台，以应对当前城市数字孪生中模型整合的复杂性和中心化趋势。它不仅关注技术层面，还强调其作为社会技术框架，促进协作和多元化表示，这对于实现更包容和可持续的城市规划具有重要意义。"}}
{"id": "2506.10381", "title": "Trace duality and additive complementary pairs of additive cyclic codes over finite chain rings", "authors": ["Sanjit Bhowmick", "Kuntal Deka", "Alexandre Fotue Tabue", "Edgar Martínez-Moro"], "summary": "This paper investigates the algebraic structure of additive complementary\npairs of cyclic codes over a finite commutative ring. We demonstrate that for\nevery additive complementary pair of additive cyclic codes, both constituent\ncodes are free modules. Moreover, we present a necessary and sufficient\ncondition for a pair of additive cyclic codes over a finite commutative ring to\nform an additive complementary pair. Finally, we construct a complementary pair\nof additive cyclic codes over a finite chain ring and show that one of the\ncodes is permutation equivalent to the trace dual of the other.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10381v1", "AI": {"title_translation": "迹对偶和有限链环上加法循环码的加法互补对", "tldr": "本文研究了有限交换环上加法循环码的加法互补对的代数结构，证明了其组成码为自由模，给出了形成互补对的充要条件，并构造了具有迹对偶关系的互补对。", "motivation": "本文旨在研究有限交换环上加法循环码的加法互补对的代数结构。", "method": "通过对代数结构的分析、必要和充分条件的提出以及具体互补对的构造来完成研究。", "result": "1. 对于每个加法循环码的加法互补对，两个组成码都是自由模。2. 提出了有限交换环上加法循环码形成加法互补对的充要条件。3. 构造了有限链环上的加法循环码互补对，并证明其中一个码在置换意义下等价于另一个码的迹对偶。", "conclusion": "本文成功揭示了有限交换环上加法循环码加法互补对的代数结构特性，并建立了它们与迹对偶之间的关系。", "translation": "本文研究了有限交换环上加法循环码的加法互补对的代数结构。我们证明了对于每对加法循环码的加法互补对，其两个组成码都是自由模。此外，我们提出了有限交换环上加法循环码形成加法互补对的充要条件。最后，我们构造了有限链环上的加法循环码互补对，并证明其中一个码在置换意义下等价于另一个码的迹对偶。", "summary": "本文深入探讨了有限交换环上加法循环码加法互补对的代数结构。研究发现，加法互补对中的两个码均为自由模，并给出了形成互补对的充要条件。此外，文章还构造了有限链环上的加法循环码互补对，并揭示了它们之间通过置换等价与迹对偶的关系。", "keywords": "加法循环码, 加法互补对, 有限链环, 迹对偶, 代数结构", "comments": "本文在编码理论领域，特别是加法循环码的研究上取得了进展。其创新点在于揭示了加法互补对的结构特性（自由模），提出了判别条件，并通过具体构造建立了迹对偶关系，这对于理解和设计新型编码具有理论意义。"}}
{"id": "2506.10407", "title": "Semi-Tensor-Product Based Convolutional Neural Networks", "authors": ["Daizhan Cheng"], "summary": "The semi-tensor product (STP) of vectors is a generalization of conventional\ninner product of vectors, which allows the factor vectors to of different\ndimensions. This paper proposes a domain-based convolutional product (CP).\nCombining domain-based CP with STP of vectors, a new CP is proposed. Since\nthere is no zero or any other padding, it can avoid the junk information caused\nby padding. Using it, the STP-based convolutional neural network (CNN) is\ndeveloped. Its application to image and third order signal identifications is\nconsidered.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10407v1", "AI": {"title_translation": "半张量积卷积神经网络", "tldr": "本文提出了一种基于半张量积（STP）的新型卷积乘积，并在此基础上开发了STP卷积神经网络，可避免传统填充带来的冗余信息，并应用于图像和三阶信号识别。", "motivation": "本文旨在提出一种新的卷积乘积，以避免传统卷积操作中填充（padding）引入的冗余信息。", "method": "本文提出了一种基于域的卷积乘积（CP），并将其与向量的半张量积（STP）结合，形成了一种新的无需填充的CP。在此基础上，开发了基于STP的卷积神经网络（CNN）。", "result": "提出的新卷积乘积能够避免传统填充导致的冗余信息。开发的STP-based CNN可应用于图像和三阶信号识别。", "conclusion": "提出的基于半张量积的卷积神经网络通过一种新的无填充卷积乘积，成功规避了传统填充带来的冗余信息，并可应用于图像和三阶信号识别任务。", "translation": "向量的半张量积（STP）是向量传统内积的推广，它允许因子向量具有不同的维度。本文提出了一种基于域的卷积乘积（CP）。将基于域的CP与向量的STP相结合，提出了一种新的CP。由于没有零或其他填充，它可以避免由填充引起的垃圾信息。利用它，开发了基于STP的卷积神经网络（CNN）。并考虑了其在图像和三阶信号识别中的应用。", "summary": "本文提出了一种基于向量半张量积（STP）的新型卷积乘积（CP），该CP结合了域基CP和STP，其特点是无需任何填充，从而避免了传统填充引入的冗余信息。在此基础上，开发了基于STP的卷积神经网络（CNN），并探讨了其在图像和三阶信号识别中的应用。", "keywords": "半张量积, 卷积神经网络, 填充, 域基卷积乘积", "comments": "该论文的创新点在于将半张量积引入卷积神经网络，并通过设计新的卷积乘积规避了传统卷积中填充问题。这种方法可能为处理不同维度数据和提高模型效率提供了新思路，具有一定的理论和应用价值。"}}
{"id": "2506.10362", "title": "Relaxation-Free Min-k-Partition for PCI Assignment in 5G Networks", "authors": ["Yeqing Qiu", "Chengpiao Huang", "Ye Xue", "Zhipeng Jiang", "Qingjiang Shi", "Dong Zhang", "Zhi-Quan Luo"], "summary": "Physical Cell Identity (PCI) is a critical parameter in 5G networks.\nEfficient and accurate PCI assignment is essential for mitigating mod-3\ninterference, mod-30 interference, collisions, and confusions among cells,\nwhich directly affect network reliability and user experience. In this paper,\nwe propose a novel framework for PCI assignment by decomposing the problem into\nMin-3-Partition, Min-10-Partition, and a graph coloring problem, leveraging the\nChinese Remainder Theorem (CRT). Furthermore, we develop a relaxation-free\napproach to the general Min-$k$-Partition problem by reformulating it as a\nquadratic program with a norm-equality constraint and solving it using a\npenalized mirror descent (PMD) algorithm. The proposed method demonstrates\nsuperior computational efficiency and scalability, significantly reducing\ninterference while eliminating collisions and confusions in large-scale 5G\nnetworks. Numerical evaluations on real-world datasets show that our approach\nreduces computational time by up to 20 times compared to state-of-the-art\nmethods, making it highly practical for real-time PCI optimization in\nlarge-scale networks. These results highlight the potential of our method to\nimprove network performance and reduce deployment costs in modern 5G systems.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10362v1", "AI": {"title_translation": "5G网络中用于PCI分配的无松弛Min-k-分区", "tldr": "本文提出了一种用于5G网络中物理小区识别（PCI）分配的新框架，通过将问题分解为Min-3-Partition、Min-10-Partition和图着色问题，并利用中国剩余定理。它还开发了一种无松弛的Min-k-Partition方法，将其重新表述为二次规划并使用罚化镜像下降算法求解，实现了显著的计算效率提升和干扰减少。", "motivation": "物理小区识别（PCI）是5G网络中的关键参数。高效准确的PCI分配对于减轻模3干扰、模30干扰、小区间的碰撞和混淆至关重要，这些问题直接影响网络可靠性和用户体验。", "method": "本文提出了一种新的PCI分配框架，将问题分解为Min-3-Partition、Min-10-Partition和图着色问题，并利用中国剩余定理（CRT）。此外，通过将其重新表述为具有范数相等约束的二次规划，并使用罚化镜像下降（PMD）算法求解，开发了一种通用的无松弛Min-k-Partition方法。", "result": "该方法在计算效率和可伸缩性方面表现出色，显著减少了干扰，并消除了大规模5G网络中的碰撞和混淆。在真实世界数据集上的数值评估表明，与现有技术相比，计算时间减少了多达20倍。", "conclusion": "本文提出的方法能够提高网络性能并降低现代5G系统中的部署成本，对于大规模网络中的实时PCI优化具有高度实用性。", "translation": "物理小区识别（PCI）是5G网络中的关键参数。高效准确的PCI分配对于减轻模3干扰、模30干扰、小区间的碰撞和混淆至关重要，这些问题直接影响网络可靠性和用户体验。在本文中，我们提出了一种新的PCI分配框架，通过将问题分解为Min-3-Partition、Min-10-Partition和图着色问题，并利用中国剩余定理（CRT）。此外，我们开发了一种通用的无松弛Min-k-Partition方法，通过将其重新表述为具有范数相等约束的二次规划，并使用罚化镜像下降（PMD）算法求解。所提出的方法展示了卓越的计算效率和可伸缩性，显著减少了干扰，同时消除了大规模5G网络中的碰撞和混淆。在真实世界数据集上的数值评估表明，我们的方法与现有技术相比，计算时间减少了多达20倍，使其对于大规模网络中的实时PCI优化高度实用。这些结果凸显了我们方法在改善网络性能和降低现代5G系统中部署成本方面的潜力。", "summary": "本文提出了一种针对5G网络中物理小区识别（PCI）分配的新型无松弛Min-k-分区框架。该框架将PCI问题分解为Min-3-Partition、Min-10-Partition和图着色问题，并利用中国剩余定理。为解决通用的Min-k-Partition问题，研究将其重新表述为带有范数相等约束的二次规划，并通过罚化镜像下降（PMD）算法求解。数值评估显示，该方法在计算效率和可伸缩性上表现卓越，显著减少了干扰，消除了碰撞和混淆，并能将计算时间缩短达20倍，对大规模5G网络的实时优化具有高度实用价值。", "keywords": "5G网络, PCI分配, Min-k-Partition, 中国剩余定理, 罚化镜像下降", "comments": "本文的创新点在于将复杂的PCI分配问题巧妙地分解为多个子问题，并引入中国剩余定理进行整合。更重要的是，它提出了一种通用的无松弛Min-k-Partition方法，通过二次规划和PMD算法来解决，避免了传统松弛方法可能带来的精度损失。其在计算效率上的显著提升（20倍）使其在实际大规模5G网络部署中具有极高的实用性和重要性。"}}
{"id": "2506.10747", "title": "FairASR: Fair Audio Contrastive Learning for Automatic Speech Recognition", "authors": ["Jongsuk Kim", "Jaemyung Yu", "Minchan Kwon", "Junmo Kim"], "summary": "Large-scale ASR models have achieved remarkable gains in accuracy and\nrobustness. However, fairness issues remain largely unaddressed despite their\ncritical importance in real-world applications. In this work, we introduce\nFairASR, a system that mitigates demographic bias by learning representations\nthat are uninformative about group membership, enabling fair generalization\nacross demographic groups. Leveraging a multi-demographic dataset, our approach\nemploys a gradient reversal layer to suppress demographic-discriminative\nfeatures while maintaining the ability to capture generalizable speech patterns\nthrough an unsupervised contrastive loss. Experimental results show that\nFairASR delivers competitive overall ASR performance while significantly\nreducing performance disparities across different demographic groups.", "comment": "Accepted to Interspeech2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10747v1", "AI": {"title_translation": "公平ASR：用于自动语音识别的公平音频对比学习", "tldr": "FairASR通过学习与人口统计信息无关的表示来减少ASR模型中的人口统计偏见，同时保持整体性能。", "motivation": "尽管对现实世界应用至关重要，但大型ASR模型中的公平性问题仍未得到解决。", "method": "本文引入了FairASR系统，该系统旨在减轻人口统计偏见。它通过学习与群体成员身份无关的表示来实现这一点，从而实现跨人口群体的公平泛化。具体方法是利用一个多人口数据集，并采用梯度反转层来抑制人口统计区分性特征，同时通过无监督对比损失保持捕获可泛化语音模式的能力。", "result": "实验结果表明，FairASR在提供有竞争力的整体ASR性能的同时，显著减少了不同人口群体之间的性能差异。", "conclusion": "FairASR成功地在自动语音识别中实现了公平性，同时保持了高水平的整体性能，有效解决了人口偏见问题。", "translation": "大规模ASR模型在准确性和鲁棒性方面取得了显著进步。然而，尽管公平性问题在现实世界应用中至关重要，但它们在很大程度上仍未得到解决。在这项工作中，我们引入了FairASR，一个通过学习与群体成员身份无关的表示来减轻人口统计偏见的系统，从而实现跨人口群体的公平泛化。我们的方法利用多人口数据集，采用梯度反转层来抑制人口统计区分性特征，同时通过无监督对比损失保持捕获可泛化语音模式的能力。实验结果表明，FairASR在提供有竞争力的整体ASR性能的同时，显著减少了不同人口群体之间的性能差异。", "summary": "本文介绍了FairASR，一个旨在减轻大型自动语音识别（ASR）模型中人口统计偏见的系统。FairASR通过学习与群体成员身份无关的表示，并利用多人口数据集、梯度反转层和无监督对比损失来抑制人口统计区分性特征，从而实现跨人口群体的公平泛化。实验证明，FairASR在保持整体ASR性能的同时，有效降低了不同人口群体间的性能差距。", "keywords": "公平ASR, 音频对比学习, 自动语音识别, 人口偏见, 梯度反转层", "comments": "FairASR的创新之处在于其结合了梯度反转层和无监督对比损失来同时解决公平性和性能问题。这种方法对于在实际应用中部署更公平的ASR系统至关重要，因为它直接解决了现有模型中普遍存在的偏见问题。"}}
{"id": "2506.10281", "title": "Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "summary": "Artificial Intelligence (AI) is reframed as a cognitive engine driving a\nnovel productivity revolution distinct from the Industrial Revolution's\nphysical thrust. This paper develops a theoretical framing of AI as a cognitive\nrevolution akin to written language - a transformative augmentation of human\nintellect rather than another mechanized tool. We compare AI's emergence to\nhistorical leaps in information technology to show how it amplifies knowledge\nwork. Examples from various domains demonstrate AI's impact as a driver of\nproductivity in cognitive tasks. We adopt a multidisciplinary perspective\ncombining computer science advances with economic insights and sociological\nperspectives on how AI reshapes work and society. Through conceptual\nframeworks, we visualize the shift from manual to cognitive productivity. Our\ncentral argument is that AI functions as an engine of cognition - comparable to\nhow human language revolutionized knowledge - heralding a new productivity\nparadigm. We discuss how this revolution demands rethinking of skills,\norganizations, and policies. This paper, balancing academic rigor with clarity,\nconcludes that AI's promise lies in complementing human cognitive abilities,\nmarking a new chapter in productivity evolution.", "comment": "12 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10281v1", "AI": {"title_translation": "比蒸汽更接近语言：AI作为新生产力革命的认知引擎", "tldr": "AI被视为一场认知革命，类似于书面语言，而非工业革命的机械工具，它将推动新的生产力范式。", "motivation": "论文旨在将AI重新定义为推动新生产力革命的认知引擎，与工业革命的物理驱动力区分开来，并将其与书面语言等信息技术历史性飞跃相类比，以展示其对知识工作的放大作用。", "method": "论文通过发展AI作为认知革命的理论框架，将其与书面语言相类比；通过比较AI的出现与信息技术的历史性飞跃来展示其对知识工作的影响；通过来自不同领域的例子来演示AI作为认知任务生产力驱动力的影响；采用多学科视角，结合计算机科学、经济学和社会学见解；并通过概念框架来可视化从体力生产力到认知生产力的转变。", "result": "论文提出AI作为认知引擎，类似于人类语言对知识的革命性影响，预示着新的生产力范式。AI能够增强人类智力，驱动认知任务的生产力。", "conclusion": "AI的潜力在于补充人类认知能力，标志着生产力演进的新篇章，并要求重新思考技能、组织和政策。", "translation": "人工智能（AI）被重新定义为推动一场与工业革命的物理推力截然不同的新型生产力革命的认知引擎。本文发展了一个将AI视为一场认知革命的理论框架，类似于书面语言——对人类智力的变革性增强，而非另一种机械工具。我们比较了AI的出现与信息技术的历史性飞跃，以展示它如何放大知识工作。来自各个领域的例子展示了AI作为认知任务生产力驱动力的影响。我们采纳了多学科视角，结合了计算机科学的进步、经济学洞察以及关于AI如何重塑工作和社会的社会学观点。通过概念框架，我们可视化了从体力生产力到认知生产力的转变。我们的核心论点是AI作为认知引擎发挥作用——类似于人类语言如何彻底改变知识——预示着一个新的生产力范式。我们讨论了这场革命如何要求重新思考技能、组织和政策。本文在学术严谨性与清晰性之间取得平衡，得出结论认为AI的希望在于补充人类认知能力，标志着生产力演进的新篇章。", "summary": "这篇论文将人工智能（AI）重新定义为一场与工业革命不同的认知生产力革命的引擎，并将其与书面语言等人类智力增强的历史性飞跃相提并论。论文通过多学科视角和具体例子，阐述了AI如何作为认知引擎，推动知识工作的生产力，并认为AI的价值在于补充人类认知能力，预示着新的生产力范式，需要重新审视技能、组织和政策。", "keywords": "人工智能, 生产力革命, 认知引擎, 知识工作, 多学科视角", "comments": "这篇论文的创新之处在于其独特的视角，将AI定位为一种“认知引擎”，而非简单的工具或技术进步，并将其与书面语言这种根本性的认知飞跃相类比，从而提升了对AI变革潜力的理解。它强调了AI对知识工作的深远影响，并呼吁对社会和经济结构进行相应的调整，具有重要的理论和实践意义。"}}
{"id": "2506.10325", "title": "SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation", "authors": ["Cheng Wang", "Siqi Chen", "Donghua Mi", "Yang Chen", "Yudong Zhang", "Yinsheng Li"], "summary": "Recent advances in medical imaging have established deep learning-based\nsegmentation as the predominant approach, though it typically requires large\namounts of manually annotated data. However, obtaining annotations for\nintracranial hemorrhage (ICH) remains particularly challenging due to the\ntedious and costly labeling process. Semi-supervised learning (SSL) has emerged\nas a promising solution to address the scarcity of labeled data, especially in\nvolumetric medical image segmentation. Unlike conventional SSL methods that\nprimarily focus on high-confidence pseudo-labels or consistency regularization,\nwe propose SWDL-Net, a novel SSL framework that exploits the complementary\nadvantages of Laplacian pyramid and deep convolutional upsampling. The\nLaplacian pyramid excels at edge sharpening, while deep convolutions enhance\ndetail precision through flexible feature mapping. Our framework achieves\nsuperior segmentation of lesion details and boundaries through a difference\nlearning mechanism that effectively integrates these complementary approaches.\nExtensive experiments on a 271-case ICH dataset and public benchmarks\ndemonstrate that SWDL-Net outperforms current state-of-the-art methods in\nscenarios with only 2% labeled data. Additional evaluations on the publicly\navailable Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data\nfurther confirm the superiority of our approach. Code and data have been\nreleased at https://github.com/SIAT-CT-LAB/SWDL.", "comment": "11 pages, 4 figures, 6 Tables", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10325v1", "AI": {"title_translation": "SWDL：基于深度拉普拉斯金字塔的分层差异学习用于半监督三维颅内出血分割", "tldr": "提出SWDL-Net，一种结合拉普拉斯金字塔和深度卷积的半监督学习框架，用于高效的3D颅内出血分割，即使在数据标记极少的情况下也能超越现有技术。", "motivation": "深度学习在医学图像分割中表现出色，但需要大量手动标注数据。颅内出血（ICH）的标注过程繁琐且成本高昂，导致标记数据稀缺。传统的半监督学习方法主要关注高置信度伪标签或一致性正则化，存在局限性。", "method": "提出SWDL-Net，一个新颖的半监督学习框架。它利用拉普拉斯金字塔（擅长边缘锐化）和深度卷积上采样（通过灵活的特征映射增强细节精度）的互补优势。通过一个差异学习机制有效地整合这些方法，以实现病变细节和边界的优越分割。", "result": "在271例ICH数据集和公共基准测试上进行了广泛实验，结果表明SWDL-Net在仅有2%标记数据的情况下优于当前最先进的方法。在公开可用的脑出血分割数据集（BHSD）上，使用5%标记数据的额外评估进一步证实了该方法的优越性。", "conclusion": "SWDL-Net通过结合拉普拉斯金字塔和深度卷积的优势，并采用差异学习机制，为半监督3D颅内出血分割提供了一种高效且表现卓越的解决方案，显著缓解了医学图像标注数据稀缺的问题。", "translation": "近期医学影像学的进展已使基于深度学习的分割成为主流方法，尽管这通常需要大量的手动标注数据。然而，由于标记过程繁琐且成本高昂，颅内出血（ICH）的标注仍然特别具有挑战性。半监督学习（SSL）已成为解决标记数据稀缺问题的有前景的解决方案，尤其是在体素医学图像分割中。与主要关注高置信度伪标签或一致性正则化的传统SSL方法不同，我们提出了SWDL-Net，一个新颖的SSL框架，它利用拉普拉斯金字塔和深度卷积上采样的互补优势。拉普拉斯金字塔擅长边缘锐化，而深度卷积通过灵活的特征映射增强细节精度。我们的框架通过一种差异学习机制实现了病变细节和边界的卓越分割，该机制有效地整合了这些互补方法。在271例ICH数据集和公共基准测试上进行的广泛实验表明，SWDL-Net在仅有2%标记数据的情况下优于当前最先进的方法。在公开可用的脑出血分割数据集（BHSD）上，使用5%标记数据的额外评估进一步证实了我们方法的优越性。代码和数据已发布在https://github.com/SIAT-CT-LAB/SWDL。", "summary": "本文提出SWDL-Net，一种创新的半监督学习框架，用于解决3D颅内出血分割中标记数据稀缺的问题。该框架结合了拉普拉斯金字塔的边缘锐化能力和深度卷积的细节增强优势，通过差异学习机制实现对病变细节和边界的精确分割。实验证明，SWDL-Net在极低标记数据（2%或5%）的情况下，其性能显著优于现有最先进的半监督分割方法。", "keywords": "半监督学习, 颅内出血分割, 拉普拉斯金字塔, 深度卷积, 差异学习", "comments": "SWDL-Net的创新之处在于其独特地结合了拉普拉斯金字塔和深度卷积的互补优势，并通过差异学习机制进行整合，从而在半监督设置下实现了对医学图像病变细节和边界的精确分割。这对于解决医疗领域高质量标注数据稀缺的痛点具有重要意义，尤其是在颅内出血这种关键的诊断场景中。其在极低标记数据比例下超越SOTA的性能，显示出其强大的实用性和潜力。"}}
{"id": "2506.10507", "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle", "authors": ["Junchao Huang", "Xinting Hu", "Zhuotao Tian", "Shaoshuai Shi", "Li Jiang"], "summary": "Recent advances in diffusion models have significantly improved image\ngeneration and editing, but extending these capabilities to 3D assets remains\nchallenging, especially for fine-grained edits that require multi-view\nconsistency. Existing methods typically restrict editing to predetermined\nviewing angles, severely limiting their flexibility and practical applications.\nWe introduce Edit360, a tuning-free framework that extends 2D modifications to\nmulti-view consistent 3D editing. Built upon video diffusion models, Edit360\nenables user-specific editing from arbitrary viewpoints while ensuring\nstructural coherence across all views. The framework selects anchor views for\n2D modifications and propagates edits across the entire 360-degree range. To\nachieve this, Edit360 introduces a novel Anchor-View Editing Propagation\nmechanism, which effectively aligns and merges multi-view information within\nthe latent and attention spaces of diffusion models. The resulting edited\nmulti-view sequences facilitate the reconstruction of high-quality 3D assets,\nenabling customizable 3D content creation.", "comment": "11 pages, 9 figures", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10507v1", "AI": {"title_translation": "Edit360：从任意角度将2D图像编辑应用于3D资产", "tldr": "Edit360是一个无需微调的框架，它利用视频扩散模型，将2D图像编辑扩展到多视角一致的3D资产编辑，允许从任意角度进行用户自定义修改。", "motivation": "现有的扩散模型在2D图像生成和编辑方面取得了显著进展，但将这些能力扩展到3D资产，特别是需要多视角一致性的精细编辑，仍然具有挑战性。现有方法通常将编辑限制在预设的视角，这严重限制了它们的灵活性和实际应用。", "method": "论文引入了Edit360，一个无需微调的框架，将2D修改扩展到多视角一致的3D编辑。它基于视频扩散模型构建，通过选择锚定视图进行2D修改，并将编辑传播到整个360度范围。为了实现这一点，Edit360引入了一种新颖的“锚定视图编辑传播”机制，该机制有效地在扩散模型的潜在空间和注意力空间中对多视图信息进行对齐和合并。", "result": "Edit360能够实现用户从任意视角的特定编辑，同时确保所有视图的结构连贯性。生成的编辑后的多视图序列有助于重建高质量的3D资产。", "conclusion": "Edit360成功地将2D图像编辑的能力扩展到多视角一致的3D资产，从而实现了可定制的3D内容创建。", "translation": "扩散模型的最新进展显著改善了图像生成和编辑，但将这些能力扩展到3D资产仍然充满挑战，特别是对于需要多视角一致性的精细编辑。现有方法通常将编辑限制在预设的视角，这严重限制了它们的灵活性和实际应用。我们引入了Edit360，一个无需微调的框架，它将2D修改扩展到多视角一致的3D编辑。Edit360基于视频扩散模型构建，能够实现用户从任意视角的特定编辑，同时确保所有视图的结构连贯性。该框架选择锚定视图进行2D修改，并将编辑传播到整个360度范围。为了实现这一点，Edit360引入了一种新颖的“锚定视图编辑传播”机制，该机制有效地在扩散模型的潜在空间和注意力空间中对多视图信息进行对齐和合并。由此产生的编辑后的多视图序列有助于重建高质量的3D资产，从而实现可定制的3D内容创建。", "summary": "Edit360是一个创新的无需微调框架，它解决了将2D图像编辑应用于多视角一致3D资产的挑战。该方法利用视频扩散模型，通过引入“锚定视图编辑传播”机制，允许用户从任意角度对3D对象进行精细编辑，确保跨视图的结构连贯性。最终，Edit360能够生成高质量的3D资产，从而实现灵活的3D内容定制。", "keywords": "3D编辑, 扩散模型, 多视角一致性, 2D到3D, 视频扩散模型", "comments": "这篇论文的创新点在于提出了一个无需微调的框架Edit360，能够将2D图像编辑能力扩展到多视角一致的3D资产。其核心是利用视频扩散模型和新颖的“锚定视图编辑传播”机制，解决了现有方法在3D编辑中视角受限和多视图一致性难题，极大地提升了3D内容创建的灵活性和可定制性。"}}
{"id": "2506.10089", "title": "Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "summary": "Out-of-distribution (OOD) detection is a critical task in machine learning,\nparticularly for safety-critical applications where unexpected inputs must be\nreliably flagged. While hierarchical variational autoencoders (HVAEs) offer\nimproved representational capacity over traditional VAEs, their performance is\nhighly sensitive to how latent dimensions are distributed across layers.\nExisting approaches often allocate latent capacity arbitrarily, leading to\nineffective representations or posterior collapse. In this work, we introduce a\ntheoretically grounded framework for optimizing latent dimension allocation in\nHVAEs, drawing on principles from information theory to formalize the trade-off\nbetween information loss and representational attenuation. We prove the\nexistence of an optimal allocation ratio $r^{\\ast}$ under a fixed latent\nbudget, and empirically show that tuning this ratio consistently improves OOD\ndetection performance across datasets and architectures. Our approach\noutperforms baseline HVAE configurations and provides practical guidance for\nprincipled latent structure design, leading to more robust OOD detection with\ndeep generative models.", "comment": "41 pages, 6 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10089v1", "AI": {"title_translation": "优化分层变分自编码器中的潜在维度分配：平衡衰减与信息保留以实现OOD检测", "tldr": "本文提出了一个理论框架，用于优化分层变分自编码器（HVAEs）中的潜在维度分配，以提高OOD检测性能。", "motivation": "域外检测（OOD）是机器学习中的一项关键任务，尤其对于安全关键应用而言。分层变分自编码器（HVAEs）的性能高度依赖于潜在维度在层间的分布方式，而现有方法常导致表示无效或后验塌陷。", "method": "引入了一个理论上扎根的框架，利用信息论原理形式化信息损失和表示衰减之间的权衡，以优化HVAEs中的潜在维度分配。证明了在固定潜在预算下存在一个最优分配比率$r^{\\ast}$。", "result": "经验表明，调整该比率持续改善了跨数据集和架构的OOD检测性能。该方法优于基线HVAE配置。", "conclusion": "该方法为原则性的潜在结构设计提供了实用指导，从而使用深度生成模型实现更鲁棒的OOD检测。", "translation": "域外检测（OOD）是机器学习中的一项关键任务，尤其对于必须可靠地标记意外输入的安全关键应用而言。尽管分层变分自编码器（HVAEs）比传统VAE提供了更高的表示能力，但它们的性能高度依赖于潜在维度在层间的分布方式。现有方法通常任意分配潜在容量，导致无效的表示或后验塌陷。在这项工作中，我们引入了一个理论上扎根的框架，用于优化HVAEs中的潜在维度分配，借鉴信息论原理来形式化信息损失和表示衰减之间的权衡。我们证明了在固定潜在预算下存在一个最优分配比率$r^{\\ast}$，并经验性地表明，调整此比率持续改善了跨数据集和架构的OOD检测性能。我们的方法优于基线HVAE配置，并为原则性的潜在结构设计提供了实用指导，从而使用深度生成模型实现更鲁棒的OOD检测。", "summary": "本文提出了一个优化分层变分自编码器（HVAEs）中潜在维度分配的理论框架，旨在解决现有方法中维度分配随意导致的表示无效或后验塌陷问题。该框架基于信息论，权衡信息损失与表示衰减，并证明了存在一个最优的潜在维度分配比率。实验结果表明，通过调整此比率，可以显著提高跨不同数据集和架构的域外检测（OOD）性能，并优于传统HVAE配置，为深度生成模型中更鲁棒的OOD检测提供了指导。", "keywords": "域外检测, 分层变分自编码器, 潜在维度分配, 信息论, 鲁棒性", "comments": "这篇论文通过引入一个理论上扎根的框架来优化HVAE中的潜在维度分配，解决了HVAE在OOD检测中性能敏感于维度分布的关键问题。其创新点在于利用信息论原理形式化信息损失和表示衰减之间的权衡，并证明了最优分配比率的存在。这为深度生成模型中潜在结构的设计提供了原则性的指导，有望提升OOD检测的鲁棒性。"}}
{"id": "2506.10520", "title": "Macro Graph of Experts for Billion-Scale Multi-Task Recommendation", "authors": ["Hongyu Yao", "Zijin Hong", "Hao Chen", "Yuanchen Bei", "Zhiqing Li", "Qijie Shen", "Zuobin Ying", "Huan Gong", "Feiran Huang"], "summary": "Graph-based multi-task learning at billion-scale presents a significant\nchallenge, as different tasks correspond to distinct billion-scale graphs.\nTraditional multi-task learning methods often neglect these graph structures,\nrelying solely on individual user and item embeddings. However, disregarding\ngraph structures overlooks substantial potential for improving performance. In\nthis paper, we introduce the Macro Graph of Expert (MGOE) framework, the first\napproach capable of leveraging macro graph embeddings to capture task-specific\nmacro features while modeling the correlations between task-specific experts.\nSpecifically, we propose the concept of a Macro Graph Bottom, which, for the\nfirst time, enables multi-task learning models to incorporate graph information\neffectively. We design the Macro Prediction Tower to dynamically integrate\nmacro knowledge across tasks. MGOE has been deployed at scale, powering\nmulti-task learning for the homepage of a leading billion-scale recommender\nsystem. Extensive offline experiments conducted on three public benchmark\ndatasets demonstrate its superiority over state-of-the-art multi-task learning\nmethods, establishing MGOE as a breakthrough in multi-task graph-based\nrecommendation. Furthermore, online A/B tests confirm the superiority of MGOE\nin billion-scale recommender systems.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10520v1", "AI": {"title_translation": "十亿级多任务推荐的宏观专家图", "tldr": "MGOE是一个新框架，首次利用宏观图嵌入和专家关联来解决十亿级多任务推荐中图结构被忽视的问题，并在离线和在线测试中表现出色。", "motivation": "十亿级多任务学习中，不同任务对应不同的十亿级图，传统方法忽视了这些图结构，仅依赖用户和物品嵌入，从而错失了性能提升的巨大潜力。", "method": "本文提出了宏观专家图（MGOE）框架，首次利用宏观图嵌入捕获任务特定的宏观特征，并建模任务特定专家之间的关联。具体而言，它提出了宏观图底部（Macro Graph Bottom）概念，首次使多任务学习模型有效融入图信息；并设计了宏观预测塔（Macro Prediction Tower）以动态集成跨任务的宏观知识。", "result": "MGOE已在领先的十亿级推荐系统主页部署。在三个公共基准数据集上进行的广泛离线实验表明，MGOE优于最先进的多任务学习方法。在线A/B测试也证实了MGOE在十亿级推荐系统中的优越性。", "conclusion": "MGOE在多任务图基推荐领域取得了突破，它有效利用宏观图嵌入并建模专家关联，解决了十亿级推荐系统中的图结构利用问题。", "translation": "十亿级多任务学习在处理不同任务对应不同十亿级图时面临巨大挑战。传统的多任务学习方法常常忽视这些图结构，仅依赖于个体用户和物品嵌入。然而，忽视图结构会错过提升性能的巨大潜力。在本文中，我们引入了宏观专家图（MGOE）框架，这是第一个能够利用宏观图嵌入来捕获任务特定的宏观特征，同时建模任务特定专家之间关联的方法。具体而言，我们提出了宏观图底部（Macro Graph Bottom）的概念，这首次使得多任务学习模型能够有效地整合图信息。我们设计了宏观预测塔（Macro Prediction Tower）来动态整合跨任务的宏观知识。MGOE已大规模部署，为领先的十亿级推荐系统的主页提供多任务学习支持。在三个公共基准数据集上进行的广泛离线实验证明了其优于最先进的多任务学习方法，确立了MGOE在多任务图基推荐领域的突破地位。此外，在线A/B测试证实了MGOE在十亿级推荐系统中的优越性。", "summary": "MGOE框架旨在解决十亿级多任务推荐中图结构被忽视的问题。它首次利用宏观图嵌入来捕获任务特定的宏观特征，并建模任务特定专家之间的关联。通过引入宏观图底部来整合图信息，并设计宏观预测塔以动态集成跨任务知识，MGOE在离线基准测试和在线A/B测试中均表现出优于现有方法的性能，并已成功部署于大规模推荐系统。", "keywords": "多任务学习, 图基推荐, 宏观图, 专家系统, 十亿级", "comments": "MGOE的创新之处在于它是首个将宏观图嵌入引入十亿级多任务学习，并有效整合图信息以提升推荐性能的框架。其重要性体现在它解决了传统方法忽视图结构的关键局限性，并通过实际部署和在线A/B测试验证了其在大规模推荐系统中的显著优越性。"}}
{"id": "2506.10116", "title": "ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering", "authors": ["Caijun Jia", "Nan Xu", "Jingxuan Wei", "Qingli Wang", "Lei Wang", "Bihui Yu", "Junnan Zhu"], "summary": "Recently, large language models have shown remarkable reasoning capabilities\nthrough long-chain reasoning before responding. However, how to extend this\ncapability to visual reasoning tasks remains an open challenge. Existing\nmultimodal reasoning approaches transfer such visual reasoning task into\ntextual reasoning task via several image-to-text conversions, which often lose\ncritical structural and semantic information embedded in visualizations,\nespecially for tasks like chart question answering that require a large amount\nof visual details. To bridge this gap, we propose ChartReasoner, a code-driven\nnovel two-stage framework designed to enable precise, interpretable reasoning\nover charts. We first train a high-fidelity model to convert diverse chart\nimages into structured ECharts codes, preserving both layout and data semantics\nas lossless as possible. Then, we design a general chart reasoning data\nsynthesis pipeline, which leverages this pretrained transport model to\nautomatically and scalably generate chart reasoning trajectories and utilizes a\ncode validator to filter out low-quality samples. Finally, we train the final\nmultimodal model using a combination of supervised fine-tuning and\nreinforcement learning on our synthesized chart reasoning dataset and\nexperimental results on four public benchmarks clearly demonstrate the\neffectiveness of our proposed ChartReasoner. It can preserve the original\ndetails of the charts as much as possible and perform comparably with\nstate-of-the-art open-source models while using fewer parameters, approaching\nthe performance of proprietary systems like GPT-4o in out-of-domain settings.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10116v1", "AI": {"title_translation": "ChartReasoner：代码驱动的模态桥接，用于图表问答中的长链推理", "tldr": "ChartReasoner是一个代码驱动的两阶段框架，通过将图表转换为ECharts代码来保留视觉细节，并利用合成数据进行训练，从而在图表问答中实现精确、可解释的长链推理，性能与最先进模型相当。", "motivation": "大型语言模型在长链推理方面表现出色，但将此能力扩展到视觉推理任务（尤其是需要大量视觉细节的图表问答）仍是挑战。现有方法通过图像到文本的转换会丢失关键的结构和语义信息。", "method": "提出ChartReasoner，一个代码驱动的两阶段框架。首先训练一个高保真模型将图表图像转换为结构化ECharts代码，尽可能无损地保留布局和数据语义。然后设计一个通用的图表推理数据合成管道，利用预训练的传输模型自动生成图表推理轨迹，并使用代码验证器过滤低质量样本。最后，使用合成数据集通过监督微调和强化学习训练最终的多模态模型。", "result": "在四个公共基准测试中，ChartReasoner表现出有效性，能够最大程度地保留图表原始细节，在参数更少的情况下与最先进的开源模型性能相当，并在域外设置中接近GPT-4o等专有系统的性能。", "conclusion": "ChartReasoner通过创新的代码驱动模态桥接方法，有效解决了图表问答中的长链推理挑战，实现了高性能、可解释的视觉推理，并能最大程度地保留图表细节。", "translation": "最近，大型语言模型在响应前通过长链推理展现出卓越的推理能力。然而，如何将这种能力扩展到视觉推理任务仍然是一个开放的挑战。现有的多模态推理方法通过几次图像到文本的转换将此类视觉推理任务转换为文本推理任务，这通常会丢失嵌入在可视化中的关键结构和语义信息，特别是对于图表问答等需要大量视觉细节的任务。为了弥合这一差距，我们提出了ChartReasoner，一个代码驱动的新颖两阶段框架，旨在实现对图表的精确、可解释的推理。我们首先训练一个高保真模型，将各种图表图像转换为结构化的ECharts代码，尽可能无损地保留布局和数据语义。然后，我们设计了一个通用的图表推理数据合成管道，该管道利用这个预训练的传输模型自动且可扩展地生成图表推理轨迹，并利用代码验证器过滤掉低质量样本。最后，我们结合监督微调和强化学习，在我们合成的图表推理数据集上训练最终的多模态模型，在四个公共基准测试上的实验结果清楚地证明了我们提出的ChartReasoner的有效性。它能尽可能多地保留图表的原始细节，并且在使用更少参数的情况下与最先进的开源模型表现相当，在域外设置中接近GPT-4o等专有系统的性能。", "summary": "本论文提出了ChartReasoner，一个代码驱动的两阶段框架，旨在解决图表问答中长链推理的视觉信息丢失问题。该框架首先将图表图像转换为ECharts代码以保留视觉和数据细节，然后通过数据合成管道生成高质量的图表推理轨迹数据集。最终训练的多模态模型在多个基准测试上表现出卓越性能，参数量更少，且能与顶尖模型相媲美，甚至在域外设置中接近GPT-4o的水平。", "keywords": "图表问答, 长链推理, 代码驱动, 模态桥接, ECharts", "comments": "ChartReasoner的创新点在于其独特的代码驱动模态桥接方法，通过将图表转换为结构化的ECharts代码，有效解决了现有方法在视觉到文本转换中信息丢失的问题。这种方法不仅保证了推理的精确性和可解释性，还通过数据合成管道解决了高质量训练数据稀缺的问题。其在保持性能的同时显著减少参数量的能力，以及在域外设置中接近GPT-4o的表现，都凸显了其在图表问答领域的潜力和重要性。"}}
{"id": "2506.10676", "title": "Description and Discussion on DCASE 2025 Challenge Task 4: Spatial Semantic Segmentation of Sound Scenes", "authors": ["Masahiro Yasuda", "Binh Thien Nguyen", "Noboru Harada", "Romain Serizel", "Mayank Mishra", "Marc Delcroix", "Shoko Araki", "Daiki Takeuchi", "Daisuke Niizumi", "Yasunori Ohishi", "Tomohiro Nakatani", "Takao Kawamura", "Nobutaka Ono"], "summary": "Spatial Semantic Segmentation of Sound Scenes (S5) aims to enhance\ntechnologies for sound event detection and separation from multi-channel input\nsignals that mix multiple sound events with spatial information. This is a\nfundamental basis of immersive communication. The ultimate goal is to separate\nsound event signals with 6 Degrees of Freedom (6DoF) information into dry sound\nobject signals and metadata about the object type (sound event class) and\nrepresenting spatial information, including direction. However, because several\nexisting challenge tasks already provide some of the subset functions, this\ntask for this year focuses on detecting and separating sound events from\nmulti-channel spatial input signals. This paper outlines the S5 task setting of\nthe Detection and Classification of Acoustic Scenes and Events (DCASE) 2025\nChallenge Task 4 and the DCASE2025 Task 4 Dataset, newly recorded and curated\nfor this task. We also report experimental results for an S5 system trained and\nevaluated on this dataset. The full version of this paper will be published\nafter the challenge results are made public.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10676v1", "AI": {"title_translation": "DCASE 2025 挑战任务 4：声景空间语义分割的描述与讨论", "tldr": "DCASE 2025 挑战任务 4 专注于从多通道空间输入信号中检测和分离声事件，旨在提升沉浸式通信技术。", "motivation": "该研究旨在提升从多通道输入信号中检测和分离声事件的技术，这些信号混合了具有空间信息的多个声事件。这是沉浸式通信的基础，最终目标是将带有 6 自由度信息的声事件信号分离成干声对象信号和包含方向等空间信息的元数据。", "method": "本文概述了 DCASE 2025 挑战任务 4 (S5) 的任务设置，并介绍了为此任务新录制和整理的 DCASE2025 任务 4 数据集。同时，报告了在该数据集上训练和评估的 S5 系统的实验结果。", "result": "报告了针对该数据集训练和评估的 S5 系统的实验结果。", "conclusion": "本文描述了 DCASE 2025 挑战任务 4 的设置和数据集，并提供了初步的实验结果。完整版论文将在挑战结果公布后发布。", "translation": "声景空间语义分割 (S5) 旨在增强从混合了多个声事件和空间信息的多通道输入信号中进行声事件检测和分离的技术。这是沉浸式通信的根本基础。最终目标是将带有 6 自由度 (6DoF) 信息的声事件信号分离成干声对象信号和关于对象类型（声事件类别）以及表示空间信息（包括方向）的元数据。然而，由于几个现有的挑战任务已经提供了一部分子功能，今年的任务重点是从多通道空间输入信号中检测和分离声事件。本文概述了声学场景和事件检测与分类 (DCASE) 2025 挑战任务 4 的 S5 任务设置以及为此任务新录制和整理的 DCASE2025 任务 4 数据集。我们还报告了在该数据集上训练和评估的 S5 系统的实验结果。本文的完整版将在挑战结果公布后发布。", "summary": "本文描述了 DCASE 2025 挑战任务 4 (S5)，该任务旨在从多通道空间输入信号中检测和分离声事件，以支持沉浸式通信。文章概述了任务设置和为此任务创建的新数据集，并报告了在该数据集上训练和评估的 S5 系统的初步实验结果。", "keywords": "空间语义分割, 声景, DCASE 2025, 声事件检测, 多通道信号", "comments": "该论文详细描述了 DCASE 2025 挑战赛的一个关键任务，即声景的空间语义分割。其创新之处在于聚焦于处理带有空间信息的声事件分离，并为此专门构建了数据集。这项工作对于推动沉浸式通信技术的发展具有重要意义，因为它直接解决了多通道音频处理中声源分离和空间信息提取的难题。由于是挑战任务的介绍，其最终的性能和完整细节尚待挑战结果公布后才能全面评估。"}}
{"id": "2506.10263", "title": "Complex scaling for open waveguides", "authors": ["Charles L. Epstein", "Tristan Goodwill", "Jeremy Hoskins", "Solomon Quinn", "Manas Rachh"], "summary": "In this work we analyze the complex scaling method applied to the problem of\ntime-harmonic scalar wave propagation in junctions between `leaky,' or open\ndielectric waveguides. In [arXiv:2302.04353, arXiv:2310.05816,\narXiv:2401.04674, arXiv:2411.11204], it was shown that under suitable\nassumptions the problem can be reduced to a system of Fredholm second-kind\nintegral equations on an infinite interface, transverse to the waveguides.\nHere, we show that the kernels appearing in the integral equation admit a\nrapidly decaying analytic continuation on certain natural totally real\nsubmanifolds of $\\mathbb{C}^2.$ We then show that for suitable,\nphysically-meaningful, boundary data the resulting solutions to the integral\nequations themselves admit analytic continuation and satisfy related asymptotic\nestimates. By deforming the integral equation to a suitable contour, the decay\nin the kernels, density, and data enable straightforward discretization and\ntruncation, with an error that decays exponentially in the truncation length.\nWe illustrate our results with several representative numerical examples.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10263v1", "AI": {"title_translation": "开放波导的复标度法", "tldr": "本文分析了复标度法在开放介质波导中时谐标量波传播问题上的应用，通过对积分方程核和解的解析延拓及等高线变形，实现了高效的数值离散化，误差呈指数衰减。", "motivation": "分析复标度法应用于“泄漏”或开放介质波导结之间时谐标量波传播问题的有效性。", "method": "作者分析了复标度法，该问题可简化为无限界面上的Fredholm第二类积分方程系统。通过证明积分方程中的核在特定实子流形上具有快速衰减的解析延拓，且解本身也具有解析延拓并满足渐近估计。然后，通过将积分方程变形到合适的等高线，利用核、密度和数据的衰减特性，实现了直接的离散化和截断。", "result": "积分方程中的核在特定自然全实子流形上具有快速衰减的解析延拓；对于合适的边界数据，积分方程的解本身也具有解析延拓并满足相关的渐近估计；通过变形积分方程到合适的等高线，离散化和截断的误差随截断长度呈指数衰减。结果通过数值例子进行了说明。", "conclusion": "复标度法通过对积分方程核和解的深入分析及等高线变形，为开放波导中波传播问题的数值求解提供了一种高效且误差呈指数衰减的方法。", "translation": "在这项工作中，我们分析了复标度法应用于“泄漏”或开放介质波导结之间时谐标量波传播问题。在[arXiv:2302.04353, arXiv:2310.05816, arXiv:2401.04674, arXiv:2411.11204]中，研究表明在适当假设下，该问题可以简化为在与波导横向的无限界面上的Fredholm第二类积分方程组。在此，我们表明积分方程中出现的核在$\\mathbb{C}^2$的某些自然全实子流形上具有快速衰减的解析延拓。然后，我们表明对于合适的、具有物理意义的边界数据，积分方程的解本身也具有解析延拓并满足相关的渐近估计。通过将积分方程变形到合适的等高线，核、密度和数据的衰减特性使得直接的离散化和截断成为可能，并且误差随截断长度呈指数衰减。我们通过几个代表性的数值例子说明了我们的结果。", "summary": "本文研究了复标度法在开放介质波导中时谐标量波传播问题上的应用。作者证明了将该问题简化为Fredholm第二类积分方程后，其核和解都具有快速衰减的解析延拓。通过将积分方程变形到合适的复等高线，结合核和解的衰减特性，实现了高效的数值离散化和截断，且误差随截断长度呈指数衰减。研究结果通过数值例子进行了验证。", "keywords": "复标度法, 开放波导, 积分方程, 解析延拓, 数值离散化", "comments": "本文创新性地将复标度法应用于开放介质波导的波传播问题，并通过严格的数学分析，证明了积分方程核与解的解析性质，进而实现了数值求解的高效性和高精度（指数衰减误差）。这对于解决复杂开放系统中的波传播问题具有重要的理论和实际意义。"}}
{"id": "2506.10047", "title": "GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models", "authors": ["Zilong Wang", "Xiang Zheng", "Xiaosen Wang", "Bo Wang", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and\nare now widely used in content creation. However, these models can be misused\nto generate harmful content, including nudity or violence, posing significant\nsafety risks. While most platforms employ content moderation systems,\nunderlying vulnerabilities can still be exploited by determined adversaries.\nRecent research on red-teaming and adversarial attacks against T2I models has\nnotable limitations: some studies successfully generate highly toxic images but\nuse adversarial prompts that are easily detected and blocked by safety filters,\nwhile others focus on bypassing safety mechanisms but fail to produce genuinely\nharmful outputs, neglecting the discovery of truly high-risk prompts.\nConsequently, there remains a lack of reliable tools for evaluating the safety\nof defended T2I models. To address this gap, we propose GenBreak, a framework\nthat fine-tunes a red-team large language model (LLM) to systematically explore\nunderlying vulnerabilities in T2I generators. Our approach combines supervised\nfine-tuning on curated datasets with reinforcement learning via interaction\nwith a surrogate T2I model. By integrating multiple reward signals, we guide\nthe LLM to craft adversarial prompts that enhance both evasion capability and\nimage toxicity, while maintaining semantic coherence and diversity. These\nprompts demonstrate strong effectiveness in black-box attacks against\ncommercial T2I generators, revealing practical and concerning safety\nweaknesses.", "comment": "27 pages, 7 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10047v1", "AI": {"title_translation": "GenBreak：使用大型语言模型对文本到图像生成器进行红队测试", "tldr": "GenBreak是一个利用大型语言模型对文本到图像生成器进行红队测试的框架，旨在发现其潜在的安全漏洞，生成既能绕过安全过滤器又具有高毒性的图像。", "motivation": "文本到图像（T2I）模型尽管广泛应用于内容创作，但存在生成有害内容的风险。现有红队测试方法在生成高毒性图像或绕过安全机制方面存在局限性，导致缺乏评估已防御T2I模型安全性的可靠工具。", "method": "我们提出了GenBreak框架，该框架通过在精选数据集上进行监督微调，并通过与替代T2I模型的交互进行强化学习，来微调一个红队大型语言模型（LLM）。通过整合多个奖励信号，引导LLM生成既能增强规避能力又能提高图像毒性，同时保持语义连贯性和多样性的对抗性提示。", "result": "这些对抗性提示在针对商业T2I生成器的黑盒攻击中表现出强大的有效性，揭示了实际且令人担忧的安全弱点。", "conclusion": "GenBreak框架通过结合监督微调和强化学习，成功地利用大型语言模型发现了文本到图像生成器的安全漏洞，生成了既能规避安全过滤器又具有高毒性的对抗性提示，为评估T2I模型的安全性提供了可靠的工具。", "translation": "文本到图像（T2I）模型如Stable Diffusion发展迅速，现已广泛应用于内容创作。然而，这些模型可能被滥用以生成有害内容，包括裸露或暴力，带来重大的安全风险。尽管大多数平台采用内容审核系统，但潜在的漏洞仍可能被有决心的对手利用。最近针对T2I模型的红队测试和对抗性攻击研究存在显著局限性：一些研究成功生成了高毒性图像，但使用了容易被安全过滤器检测和阻止的对抗性提示；而另一些研究则专注于绕过安全机制，但未能产生真正有害的输出，忽视了发现真正高风险提示。因此，目前仍然缺乏评估已防御T2I模型安全性的可靠工具。为了解决这一空白，我们提出了GenBreak，一个微调红队大型语言模型（LLM）以系统地探索T2I生成器潜在漏洞的框架。我们的方法将精选数据集上的监督微调与通过与替代T2I模型交互的强化学习相结合。通过整合多个奖励信号，我们引导LLM制作对抗性提示，以增强规避能力和图像毒性，同时保持语义连贯性和多样性。这些提示在针对商业T2I生成器的黑盒攻击中表现出强大的有效性，揭示了实际且令人担忧的安全弱点。", "summary": "GenBreak是一个利用大型语言模型（LLM）进行红队测试的框架，旨在系统性地发现文本到图像（T2I）生成器的安全漏洞。通过结合监督微调和强化学习，GenBreak训练LLM生成既能规避现有安全过滤器又能产生高毒性图像的对抗性提示，从而有效揭示商业T2I模型的实际安全弱点。", "keywords": "红队测试, 文本到图像生成器, 大型语言模型, 安全漏洞, 对抗性提示", "comments": "GenBreak的创新之处在于利用LLM的强大能力来自动化和系统化红队测试过程，超越了传统方法在生成高毒性且难以检测的对抗性提示方面的局限性。其结合监督学习和强化学习的方法，以及多重奖励信号的设计，有效地平衡了规避能力、图像毒性和语义连贯性，为评估T2I模型安全性提供了一个实用且高效的工具。"}}
{"id": "2506.10662", "title": "Receiving RISs: Enabling Channel Estimation and Autonomous Configuration", "authors": ["George C. Alexandropoulos", "Konstantinos D. Katsanos", "Evangelos Vlachos"], "summary": "This chapter focuses on a hardware architecture for semi-passive\nReconfigurable Intelligent Surfaces (RISs) and investigates its consideration\nfor boosting the performance of Multiple-Input Multiple-Output (MIMO)\ncommunication systems. The architecture incorporates a single or multiple\nradio-frequency chains to receive pilot signals via tunable absorption phase\nprofiles realized by the metasurface front end, as well as a controller\nencompassing a baseband processing unit to carry out channel estimation, and\nconsequently, the optimization of the RIS reflection coefficients. A novel\nchannel estimation protocol, according to which the RIS receives non-orthogonal\ntraining pilot sequences from two multi-antenna terminals via tunable\nabsorption phase profiles, and then, estimates the respective channels via its\nsignal processing unit, is presented. The channel estimates are particularly\nused by the RIS controller to design the capacity-achieving reflection phase\nconfiguration of the metasurface front end. The proposed channel estimation\nalgorithm, which is based on the Alternating Direction Method of Multipliers\n(ADMM), profits from the RIS random spatial absorption sampling to capture the\nentire signal space, and exploits the beamspace sparsity and low-rank\nproperties of extremely large MIMO channels, which is particularly relevant for\ncommunication systems at the FR3 band and above. Our extensive numerical\ninvestigations showcase the superiority of the proposed channel estimation\ntechnique over benchmark schemes for various system and RIS hardware\nconfiguration parameters, as well as the effectiveness of using channel\nestimates at the RIS side to dynamically optimize the possibly phase-quantized\nreflection coefficients of its unit elements.", "comment": "34 pages; 12 figures; book chapter", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10662v1", "AI": {"title_translation": "接收端可重构智能表面：实现信道估计与自主配置", "tldr": "本章介绍了一种半无源可重构智能表面（RIS）的硬件架构和信道估计协议，通过接收导频信号和ADMM算法实现信道估计与反射系数优化，旨在提升多输入多输出（MIMO）通信系统性能。", "motivation": "为了提升多输入多输出（MIMO）通信系统的性能，并解决可重构智能表面（RISs）在实际应用中面临的信道估计和自主配置挑战。", "method": "本研究提出了一种半无源可重构智能表面（RIS）的硬件架构，该架构包含一个或多个射频链用于接收导频信号，以及一个包含基带处理单元的控制器用于执行信道估计和优化RIS反射系数。开发了一种新的信道估计协议，RIS通过可调吸收相位剖面从两个多天线终端接收非正交训练导频序列，并利用其信号处理单元估计信道。所提出的信道估计算法基于交替方向乘子法（ADMM），利用RIS的随机空间吸收采样来捕获整个信号空间，并利用极大MIMO信道的波束空间稀疏性和低秩特性。", "result": "广泛的数值研究表明，所提出的信道估计技术在各种系统和RIS硬件配置参数下均优于基准方案。此外，研究还展示了在RIS侧使用信道估计来动态优化其单元元件的反射系数的有效性。", "conclusion": "本研究成功提出了一种半无源可重构智能表面（RIS）的硬件架构和新颖的信道估计协议，通过基于ADMM的算法实现了高效的信道估计和RIS反射系数的自主优化，从而显著提升了多输入多输出（MIMO）通信系统的性能。", "translation": "本章重点介绍了一种半无源可重构智能表面（RISs）的硬件架构，并探讨了其在提升多输入多输出（MIMO）通信系统性能方面的考虑。该架构包含一个或多个射频链，通过由超表面前端实现的可调谐吸收相位剖面接收导频信号，以及一个包含基带处理单元的控制器，用于执行信道估计，进而优化RIS的反射系数。提出了一种新颖的信道估计协议，根据该协议，RIS通过可调谐吸收相位剖面从两个多天线终端接收非正交训练导频序列，然后通过其信号处理单元估计相应的信道。信道估计结果特别用于RIS控制器设计超表面前端的容量实现反射相位配置。所提出的信道估计算法基于交替方向乘子法（ADMM），受益于RIS的随机空间吸收采样以捕获整个信号空间，并利用了极大MIMO信道的波束空间稀疏性和低秩特性，这对于FR3频段及以上的通信系统尤其相关。我们广泛的数值研究表明，对于各种系统和RIS硬件配置参数，所提出的信道估计技术优于基准方案，并且在RIS侧使用信道估计来动态优化其单元元件可能经过相位量化的反射系数是有效的。", "summary": "本章提出了一种用于提升MIMO系统性能的半无源可重构智能表面（RIS）硬件架构。该架构包含射频链和基带处理单元，能够通过接收导频信号和执行信道估计来优化RIS的反射系数。研究引入了一种新颖的信道估计协议，RIS通过可调吸收相位剖面接收非正交训练导频序列并进行信道估计。核心算法基于ADMM，利用RIS的随机采样以及极大MIMO信道的稀疏性和低秩特性。数值结果验证了该信道估计方法的优越性及其在RIS侧动态优化反射系数的有效性。", "keywords": "可重构智能表面, 信道估计, MIMO, ADMM, 半无源RIS", "comments": "这篇论文的创新点在于提出了一个具体的半无源RIS硬件架构，并结合了新颖的信道估计协议和基于ADMM的算法。它解决了RIS在实际部署中面临的关键挑战——信道状态信息获取和智能配置。特别是利用了RIS本身的随机吸收特性来辅助信道估计，并考虑了极大MIMO信道的特性，这对于未来高频段通信具有重要意义。该研究为RIS在实际通信系统中的部署提供了重要的硬件和算法支持。"}}
{"id": "2506.10322", "title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "summary": "Static bug analyzers play a crucial role in ensuring software quality.\nHowever, existing analyzers for bug detection in large codebases often suffer\nfrom high false positive rates. This is primarily due to the limited\ncapabilities of analyzers in path feasibility validation with multiple\nconditional branches and complex data dependencies. While current LLM-based\napproaches attempt to address this issue, their effectiveness remains limited\ndue to insufficient constraint cascade analysis and scalability challenges in\nlarge projects. To address this challenge, we propose an iterative path\nfeasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted\nconstraint reasoning, and key context-aware analysis driven by agent planning,\nLLM4PFA effectively enhances complex inter-procedural path feasibility analysis\nfor minimizing false positives in static bug detection. Evaluation results show\nthat LLM4PFA precisely filters out 72% to 96% false positives reported during\nstatic bug detection, significantly outperforming all the baselines by 41.1% -\n105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true\npositives.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10322v1", "AI": {"title_translation": "通过LLM增强的路径可行性分析最小化静态错误检测中的误报", "tldr": "提出LLM4PFA框架，利用LLM代理增强路径可行性分析，显著降低静态错误检测中的误报率。", "motivation": "现有静态错误分析器在大型代码库中存在高误报率，主要原因是路径可行性验证能力有限，尤其是在处理多条件分支和复杂数据依赖时。现有基于LLM的方法也因约束级联分析不足和可伸缩性挑战而效果有限。", "method": "提出LLM4PFA，一个迭代的路径可行性分析框架。它通过利用基于LLM代理的目标约束推理和由代理规划驱动的关键上下文感知分析，有效增强了复杂的跨过程路径可行性分析。", "result": "LLM4PFA精确过滤掉72%至96%的静态错误检测误报，表现优于所有基线41.1%至105.7%；同时，在45个真实错误中仅漏报3个。", "conclusion": "LLM4PFA通过其创新的LLM增强路径可行性分析方法，有效解决了静态错误检测中的高误报问题，并在保持高召回率的同时显著提高了精度。", "translation": "静态错误分析器在确保软件质量方面发挥着关键作用。然而，现有的大型代码库错误检测分析器通常存在高误报率。这主要是由于分析器在多条件分支和复杂数据依赖的路径可行性验证方面能力有限。尽管当前基于LLM的方法试图解决此问题，但由于约束级联分析不足和大型项目中的可伸缩性挑战，其有效性仍然有限。为了解决这一挑战，我们提出了一个迭代路径可行性分析框架LLM4PFA。通过利用基于LLM代理的目标约束推理和由代理规划驱动的关键上下文感知分析，LLM4PFA有效增强了复杂的跨过程路径可行性分析，从而最小化了静态错误检测中的误报。评估结果表明，LLM4PFA精确过滤掉了静态错误检测报告的72%至96%的误报，显著优于所有基线41.1%至105.7%；同时，LLM4PFA在45个真实错误中仅漏报3个。", "summary": "本文提出了LLM4PFA，一个利用大型语言模型（LLM）代理进行目标约束推理和上下文感知分析的迭代路径可行性分析框架。该框架旨在解决现有静态错误分析器在高误报率上的不足，特别是在处理复杂路径和数据依赖时。实验结果表明，LLM4PFA能有效过滤72%至96%的误报，并显著优于现有基线，同时保持了较低的漏报率。", "keywords": "静态错误检测, 误报最小化, 路径可行性分析, 大型语言模型, LLM代理", "comments": "这篇论文通过引入LLM代理进行路径可行性分析，为静态错误检测中的高误报问题提供了一个新颖且有效的解决方案。其创新点在于结合LLM的推理能力和上下文感知分析，以更准确地验证复杂路径的可行性。该方法显著降低了误报率，对提高静态分析工具的实用性具有重要意义。"}}
{"id": "2506.10252", "title": "A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control", "authors": ["Rongfei Li", "Francis Assadian"], "summary": "In robot navigation and manipulation, accurately determining the camera's\npose relative to the environment is crucial for effective task execution. In\nthis paper, we systematically prove that this problem corresponds to the\nPerspective-3-Point (P3P) formulation, where exactly three known 3D points and\ntheir corresponding 2D image projections are used to estimate the pose of a\nstereo camera. In image-based visual servoing (IBVS) control, the system\nbecomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera\nmust align with 9 observed 2D features in the scene. When more constraints are\nimposed than available DoFs, global stability cannot be guaranteed, as the\ncamera may become trapped in a local minimum far from the desired configuration\nduring servoing. To address this issue, we propose a novel control strategy for\naccurately positioning a calibrated stereo camera. Our approach integrates a\nfeedforward controller with a Youla parameterization-based feedback controller,\nensuring robust servoing performance. Through simulations, we demonstrate that\nour method effectively avoids local minima and enables the camera to reach the\ndesired pose accurately and efficiently.", "comment": "36 pages, 19 figures, Journal, Published in: Applied Sciences, 2025,\n  vol. 15, article 4991. For published version, see this http URL:\n  https://doi.org/10.3390/app15094991", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10252v1", "AI": {"title_translation": "一种避免立体图像视觉伺服控制中局部最小值的创新前馈Youla参数化方法", "tldr": "本文提出了一种结合前馈控制器和Youla参数化反馈控制器的新型控制策略，用于立体相机视觉伺服，以解决传统IBVS控制中系统过确定导致的局部最小值问题，并通过仿真验证了其有效性。", "motivation": "在机器人导航和操作中，准确确定相机相对于环境的姿态至关重要。图像视觉伺服（IBVS）控制中，由于系统变得过确定（6个自由度与9个观测到的2D特征不匹配），导致全局稳定性无法保证，相机可能在伺服过程中陷入远离期望配置的局部最小值。", "method": "提出了一种用于精确定位已校准立体相机的新型控制策略。该方法将前馈控制器与基于Youla参数化的反馈控制器相结合。", "result": "通过仿真，证明了所提出的方法能够有效避免局部最小值，并使相机准确、高效地到达期望姿态。", "conclusion": "所提出的结合前馈和Youla参数化反馈的控制策略能够有效解决立体图像视觉伺服中局部最小值问题，实现相机姿态的准确高效控制。", "translation": "在机器人导航和操作中，准确确定相机相对于环境的姿态对于有效执行任务至关重要。在本文中，我们系统地证明了这个问题对应于透视-3-点（P3P）公式，其中正好使用三个已知的3D点及其对应的2D图像投影来估计立体相机的姿态。在基于图像的视觉伺服（IBVS）控制中，系统变得过确定，因为立体相机的6个自由度必须与场景中9个观测到的2D特征对齐。当施加的约束多于可用的自由度时，无法保证全局稳定性，因为相机在伺服过程中可能会陷入远离期望配置的局部最小值。为了解决这个问题，我们提出了一种用于精确定位已校准立体相机的新型控制策略。我们的方法将前馈控制器与基于Youla参数化的反馈控制器相结合，确保了鲁棒的伺服性能。通过仿真，我们证明了我们的方法有效地避免了局部最小值，并使相机能够准确高效地到达期望姿态。", "summary": "本文针对立体图像视觉伺服（IBVS）控制中因系统过确定而导致的局部最小值问题，提出了一种新颖的控制策略。该策略结合了前馈控制器和基于Youla参数化的反馈控制器，旨在精确控制已校准立体相机的姿态。仿真结果表明，该方法能有效避免局部最小值，并使相机准确高效地达到期望姿态，从而确保了鲁棒的伺服性能。", "keywords": "立体视觉伺服, 局部最小值, 前馈控制, Youla参数化, P3P", "comments": "该论文创新性地将前馈控制与Youla参数化反馈控制相结合，解决了立体图像视觉伺服中局部最小值这一关键挑战，提高了控制的鲁棒性和精度。其重要性在于为视觉伺服系统提供了一种有效的全局稳定性保障方法。"}}
{"id": "2506.10587", "title": "IDEA: Augmenting Design Intelligence through Design Space Exploration", "authors": ["Chuer Chen", "Xiaoke Yan", "Xiaoyu Qi", "Nan Cao"], "summary": "Design spaces serve as a conceptual framework that enables designers to\nexplore feasible solutions through the selection and combination of design\nelements. However, effective decision-making remains heavily dependent on the\ndesigner's experience, and the absence of mathematical formalization prevents\ncomputational support for automated design processes. To bridge this gap, we\nintroduce a structured representation that models design spaces with orthogonal\ndimensions and discrete selectable elements. Building on this model, we present\nIDEA, a decision-making framework for augmenting design intelligence through\ndesign space exploration to generate effective outcomes. Specifically, IDEA\nleverages large language models (LLMs) for constraint generation, incorporates\na Monte Carlo Tree Search (MCTS) algorithm guided by these constraints to\nexplore the design space efficiently, and instantiates abstract decisions into\ndomain-specific implementations. We validate IDEA in two design scenarios:\ndata-driven article composition and pictorial visualization generation,\nsupported by example results, expert interviews, and a user study. The\nevaluation demonstrates the IDEA's adaptability across domains and its\ncapability to produce superior design outcomes.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10587v1", "AI": {"title_translation": "IDEA：通过设计空间探索增强设计智能", "tldr": "IDEA是一个利用LLM和MCTS的决策框架，通过设计空间探索来增强设计智能，并在文章创作和可视化生成中展现出卓越的设计成果和跨领域适应性。", "motivation": "当前设计中的有效决策严重依赖设计师经验，且缺乏数学形式化阻碍了自动化设计过程的计算支持。", "method": "本文引入了一种结构化表示来建模设计空间，并提出了IDEA框架。IDEA利用大型语言模型（LLMs）生成约束，结合蒙特卡洛树搜索（MCTS）算法在这些约束指导下高效探索设计空间，并将抽象决策实例化为特定领域的实现。", "result": "IDEA在数据驱动的文章创作和图像可视化生成两个设计场景中进行了验证，并通过示例结果、专家访谈和用户研究得到支持。评估表明IDEA具有跨领域的适应性以及产生卓越设计成果的能力。", "conclusion": "IDEA框架能够通过设计空间探索有效地增强设计智能，并能够在不同设计场景中生成高质量的设计方案。", "translation": "设计空间作为一种概念框架，使设计师能够通过选择和组合设计元素来探索可行的解决方案。然而，有效的决策仍然严重依赖于设计师的经验，并且缺乏数学形式化阻碍了自动化设计过程的计算支持。为了弥补这一差距，我们引入了一种结构化表示，该表示通过正交维度和离散可选择元素来建模设计空间。在此模型的基础上，我们提出了IDEA，一个通过设计空间探索来增强设计智能以生成有效结果的决策框架。具体而言，IDEA利用大型语言模型（LLMs）进行约束生成，结合蒙特卡洛树搜索（MCTS）算法在这些约束的指导下高效探索设计空间，并将抽象决策实例化为特定领域的实现。我们在两种设计场景中验证了IDEA：数据驱动的文章创作和图像可视化生成，并辅以示例结果、专家访谈和用户研究。评估表明IDEA具有跨领域的适应性及其产生卓越设计成果的能力。", "summary": "IDEA是一个创新的决策框架，旨在通过设计空间探索增强设计智能。它通过结构化表示建模设计空间，并结合大型语言模型（LLMs）生成约束和蒙特卡洛树搜索（MCTS）算法进行高效探索。该框架在文章创作和可视化生成等场景中得到验证，展示了其跨领域适应性和生成卓越设计成果的能力，解决了当前设计中缺乏计算支持的挑战。", "keywords": "设计智能, 设计空间探索, LLM, MCTS, 自动化设计", "comments": "这项研究的创新之处在于将LLMs和MCTS结合应用于设计空间探索，为自动化设计提供了计算支持，弥补了传统设计依赖经验和缺乏形式化的问题。其在多个领域的验证也显示了其潜在的广泛应用价值。"}}
{"id": "2506.10624", "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "authors": ["Lukas Jünger", "Jan Henrik Weinstock", "Tim Kraus"], "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "comment": "Published in DVCon China 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10624v1", "AI": {"title_translation": "快速虚拟平台中的可扩展软件测试：利用SystemC、QEMU和容器化技术", "tldr": "本研究提出了一种利用容器化技术封装虚拟平台的方法，以实现快速、并行化的软件测试，并利用SystemC和QEMU等开源技术，解决硬件滞后和测试复杂性问题。", "motivation": "硬件/软件系统日益增长的复杂性，尤其是在汽车等安全关键领域，对软件测试提出了巨大挑战。硬件的可用性常常滞后，阻碍了早期软件开发。现有的虚拟平台虽然有所帮助，但仍需解决环境依赖和许可问题，以实现可扩展和并行的测试执行。", "method": "本研究提出了一种利用容器化技术（如Docker）封装虚拟平台（VPs）的方法，以减少环境依赖性并支持云部署，从而实现快速、并行化的测试执行。同时，该方法利用了SystemC TLM-2.0标准以及QEMU和VCML等开源虚拟平台技术，以避免对商业许可的需求。", "result": "通过一个人工智能（AI）加速器虚拟平台的案例研究，展示了所提出方法的有效性。该研究提供了一个强大的解决方案，以应对硬件/软件系统复杂性带来的挑战。", "conclusion": "本研究提供了一个强大的解决方案，能够应对硬件/软件系统复杂性带来的挑战，并对加速硬件/软件协同开发具有实际意义。", "translation": "硬件/软件系统日益增长的复杂性带来了持续的挑战，尤其是在汽车等安全关键领域，广泛的测试是必不可少的。然而，硬件的可用性常常滞后，阻碍了早期软件开发。为解决这一问题，基于SystemC TLM-2.0标准的虚拟平台（VPs）已成为一个关键解决方案，能够实现未经修改的目标软件的预硅执行和测试。在本研究中，我们提出了一种利用容器化技术封装虚拟平台的方法，以减少环境依赖性并实现云部署，从而实现快速、并行化的测试执行，同时利用QEMU和VCML等开源虚拟平台技术，以避免对许可的需求。为了证明我们方法的有效性，我们提出了一个人工智能（AI）加速器虚拟平台案例研究。通过我们的研究，我们提供了一个强大的解决方案，以应对硬件/软件系统复杂性带来的挑战，对加速硬件/软件协同开发具有实际意义。", "summary": "本研究旨在解决复杂软硬件系统，特别是安全关键领域中软件测试面临的挑战，即硬件滞后和测试可扩展性问题。作者提出了一种创新的方法，通过容器化技术封装基于SystemC TLM-2.0标准的虚拟平台，并利用QEMU等开源技术。这种方法旨在减少环境依赖，支持云部署，从而实现快速、并行化的测试执行，并避免昂贵的许可费用。通过一个AI加速器虚拟平台的案例研究，验证了该方法的有效性，为加速软硬件协同开发提供了实用且强大的解决方案。", "keywords": "虚拟平台, 容器化, SystemC, QEMU, 软件测试", "comments": "本论文的创新之处在于将容器化技术与虚拟平台结合，解决了传统虚拟平台在环境依赖、可扩展性和许可成本方面的痛点。通过云部署和并行化测试，显著提升了软件测试的效率和规模。该方法对于加速复杂软硬件系统的协同开发，尤其是在硬件可用性受限的早期阶段，具有重要意义。"}}
{"id": "2506.10523", "title": "HP2C-DT: High-Precision High-Performance Computer-enabled Digital Twin", "authors": ["E. Iraola", "M. García-Lorenzo", "F. Lordan-Gomis", "F. Rossi", "E. Prieto-Araujo", "R. M. Badia"], "summary": "Digital twins are transforming the way we monitor, analyze, and control\nphysical systems, but designing architectures that balance real-time\nresponsiveness with heavy computational demands remains a challenge.\nCloud-based solutions often struggle with latency and resource constraints,\nwhile edge-based approaches lack the processing power for complex simulations\nand data-driven optimizations.\n  To address this problem, we propose the High-Precision High-Performance\nComputer-enabled Digital Twin (HP2C-DT) reference architecture, which\nintegrates High-Performance Computing (HPC) into the computing continuum.\nUnlike traditional setups that use HPC only for offline simulations, HP2C-DT\nmakes it an active part of digital twin workflows, dynamically assigning tasks\nto edge, cloud, or HPC resources based on urgency and computational needs.\n  Furthermore, to bridge the gap between theory and practice, we introduce the\nHP2C-DT framework, a working implementation that uses COMPSs for seamless\nworkload distribution across diverse infrastructures. We test it in a power\ngrid use case, showing how it reduces communication bandwidth by an order of\nmagnitude through edge-side data aggregation, improves response times by up to\n2x via dynamic offloading, and maintains near-ideal strong scaling for\ncompute-intensive workflows across a practical range of resources. These\nresults demonstrate how an HPC-driven approach can push digital twins beyond\ntheir current limitations, making them smarter, faster, and more capable of\nhandling real-world complexity.", "comment": "15 pages, 5 figures. Submitted to Future Generation Computing Systems\n  journal", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10523v1", "AI": {"title_translation": "HP2C-DT：高精度高性能计算机赋能的数字孪生", "tldr": "HP2C-DT是一种新的数字孪生架构，将高性能计算整合到计算连续体中，动态分配任务以提高响应速度和效率，并在电网用例中展示了显著性能提升。", "motivation": "现有数字孪生架构在实时响应与高计算需求之间难以平衡，云端方案存在延迟和资源限制，边缘方案缺乏复杂模拟能力。", "method": "提出HP2C-DT参考架构，将高性能计算（HPC）整合到计算连续体中，使其成为数字孪生工作流的活跃部分，根据紧急性和计算需求动态地将任务分配给边缘、云或HPC资源。并引入HP2C-DT框架作为工作实现，使用COMPSs进行无缝工作负载分配。", "result": "在电网用例中，通过边缘数据聚合将通信带宽降低了一个数量级；通过动态卸载将响应时间缩短了2倍；在实际资源范围内，计算密集型工作流保持了接近理想的强扩展性。", "conclusion": "这种HPC驱动的方法能够将数字孪生推出现有局限，使其更智能、更快、更善于处理现实世界的复杂性。", "translation": "数字孪生正在改变我们监控、分析和控制物理系统的方式，但设计兼顾实时响应性和高计算需求的架构仍然是一个挑战。基于云的解决方案常受限于延迟和资源，而基于边缘的方法则缺乏进行复杂模拟和数据驱动优化的处理能力。为解决此问题，我们提出了高精度高性能计算机赋能的数字孪生（HP2C-DT）参考架构，该架构将高性能计算（HPC）集成到计算连续体中。与传统仅将HPC用于离线模拟的设置不同，HP2C-DT使其成为数字孪生工作流的活跃部分，根据紧急性和计算需求动态地将任务分配给边缘、云或HPC资源。此外，为了弥合理论与实践之间的差距，我们引入了HP2C-DT框架，这是一个可工作的实现，它使用COMPSs在不同基础设施之间无缝分配工作负载。我们在一个电网用例中对其进行了测试，展示了它如何通过边缘侧数据聚合将通信带宽降低一个数量级，通过动态卸载将响应时间提高2倍，并在实际资源范围内，对于计算密集型工作流保持接近理想的强扩展性。这些结果表明，HPC驱动的方法如何能将数字孪生推出现有局限，使其更智能、更快，并能更好地处理现实世界的复杂性。", "summary": "本文提出了HP2C-DT（高精度高性能计算机赋能的数字孪生）架构，旨在解决数字孪生在实时响应和高计算需求之间的平衡挑战。该架构将高性能计算（HPC）整合到计算连续体中，并能根据任务需求动态分配资源。通过HP2C-DT框架在电网用例中的验证，展示了其在降低通信带宽、缩短响应时间以及保持计算密集型工作流扩展性方面的显著优势，证明了HPC驱动方法能提升数字孪生处理复杂性的能力。", "keywords": "数字孪生, 高性能计算, 计算连续体, 资源管理, 实时系统", "comments": "HP2C-DT的创新之处在于将高性能计算（HPC）从传统的离线模拟角色转变为数字孪生工作流的活跃组成部分，并实现计算连续体中的动态资源分配。这对于需要高精度和实时响应的复杂物理系统监控与控制具有重要意义。其提出的框架在实际用例中展示了显著的性能提升，弥补了现有云和边缘方案的不足。"}}
{"id": "2506.10013", "title": "Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z", "authors": ["Yerin Doh", "Joonhyung Bae"], "summary": "This study introduces the media artwork Dear Passenger, Please Wear a Mask,\ndesigned to offer a layered exploration of single-use mask waste, which\nescalated during the COVID-19 pandemic. The piece reframes underappreciated\necological concerns by interweaving digital nostalgia and airline travel\nrecollections of Millennials and Gen Z with a unique fantasy narrative. Via a\npoint-and-click game and an immersive exhibition, participants traverse both\nvirtual and real domains, facing ethical and environmental dilemmas. While it\nfosters empathy and potential action, resource use and post-experience\nengagement challenges persist.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10013v1", "AI": {"title_translation": "基于数字怀旧的沉浸式幻想：韩国千禧一代和Z世代的环境叙事", "tldr": "本研究介绍了一种名为《亲爱的乘客，请戴好口罩》的媒体艺术作品，它利用数字怀旧和幻想，通过沉浸式游戏和展览，旨在解决千禧一代和Z世代一次性口罩废弃物问题，以培养同理心，尽管存在体验后参与的挑战。", "motivation": "本研究旨在解决COVID-19大流行期间一次性口罩废弃物激增的问题，并通过将生态问题重新构建，特别是针对千禧一代和Z世代，以引起对这些被低估问题的关注。", "method": "本研究介绍了一个名为《亲爱的乘客，请戴好口罩》的媒体艺术作品。该作品通过一个点击式游戏和沉浸式展览，将数字怀旧和千禧一代及Z世代的航空旅行回忆与独特的奇幻叙事相结合，引导参与者穿梭于虚拟和现实领域，面对伦理和环境困境。", "result": "该艺术作品培养了参与者的同理心和潜在的行动。", "conclusion": "尽管该艺术作品培养了同理心和潜在的行动，但资源使用和体验后参与的挑战依然存在。", "translation": "本研究介绍了一种媒体艺术作品《亲爱的乘客，请戴好口罩》，旨在分层探索在COVID-19大流行期间激增的一次性口罩废弃物问题。该作品通过将千禧一代和Z世代的数字怀旧和航空旅行回忆与独特的奇幻叙事交织在一起，重新诠释了被低估的生态问题。通过点击式游戏和沉浸式展览，参与者在虚拟和现实领域中穿梭，面临伦理和环境困境。尽管它培养了同理心和潜在的行动，但资源使用和体验后参与的挑战依然存在。", "summary": "本研究介绍了一件名为《亲爱的乘客，请戴好口罩》的媒体艺术作品，旨在解决COVID-19大流行期间显著增加的一次性口罩废弃物环境问题。该作品创造性地将千禧一代和Z世代的数字怀旧及航空旅行记忆与奇幻叙事相结合。通过互动点击式游戏和沉浸式展览，它引导参与者在虚拟和现实环境中面对伦理和环境困境，旨在培养同理心并鼓励行动，尽管也指出了资源使用和持续参与方面的挑战。", "keywords": "数字怀旧, 环境叙事, 口罩废弃物, 沉浸式艺术, 千禧一代和Z世代", "comments": "该研究的创新之处在于利用数字怀旧和奇幻叙事，结合互动游戏和展览形式，吸引特定人群（千禧一代和Z世代）关注紧迫的环境问题（口罩废弃物）。其重要性在于探索了提高生态意识的新颖方式。文中明确指出了一个局限性：资源使用和体验后参与的挑战。"}}
{"id": "2506.10554", "title": "Downlink CSIT under Compressed Feedback: Joint vs. Separate Source-Channel Coding", "authors": ["Yi Song", "Tianyu Yang", "Mahdi Barzegar Khalilsarai", "Giuseppe Caire"], "summary": "The acquisition of Downlink (DL) channel state information at the transmitter\n(CSIT) is known to be a challenging task in multiuser massive MIMO systems when\nuplink/downlink channel reciprocity does not hold (e.g., in frequency division\nduplexing systems). From a coding viewpoint, the DL channel state acquired at\nthe users via DL training can be seen as an information source that must be\nconveyed to the base station via the UL communication channels. The\ntransmission of a source through a channel can be accomplished either by\nseparate or joint source-channel coding (SSCC or JSCC). In this work, using\nclassical remote distortion-rate (DR) theory, we first provide a theoretical\nlower bound on the channel estimation mean-square-error (MSE) of both JSCC and\nSSCC-based feedback schemes, which however requires encoding of large blocks of\nsuccessive channel states and thus cannot be used in practicesince it would\nincur in an extremely large feedback delay. We then focus on the relevant case\nof minimal (one slot) feedback delay and propose a practical JSCC-based\nfeedback scheme that fully exploits the channel second-order statistics to\noptimize the dimension projection in the eigenspace. We analyze the large SNR\nbehavior of the proposed JSCC-based scheme in terms of the quality scaling\nexponent (QSE). Given the second-order statistics of channel estimation of any\nfeedback scheme, we further derive the closed-form of the lower bound to the\nergodic sum-rate for DL data transmission under maximum ratio transmission and\nzero-forcing precoding. Via extensive numerical results, we show that our\nproposed JSCC-based scheme outperforms known JSCC, SSCC baseline and deep\nlearning-based schemes and is able to approach the performance of the optimal\nDR scheme in the range of practical SNR.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10554v1", "AI": {"title_translation": "压缩反馈下的下行链路CSIT：联合源信道编码与分离源信道编码", "tldr": "本文研究了大规模MIMO系统中下行链路CSIT获取的挑战，并提出了一种实用的基于JSCC的反馈方案，该方案在最小反馈延迟下性能优于现有方案，并接近理论最优。", "motivation": "在多用户大规模MIMO系统中，当下行/上行信道互易性不成立时（例如，在频分双工系统中），下行链路（DL）信道状态信息在发射端（CSIT）的获取是一项具有挑战性的任务。从编码角度看，用户通过DL训练获取的DL信道状态可视为信息源，需通过UL通信信道传达给基站。", "method": "本文首先利用经典的远程失真-速率（DR）理论，为基于JSCC和SSCC的反馈方案提供了信道估计均方误差（MSE）的理论下限。然后，针对最小反馈延迟（一个时隙）的情况，提出了一种实用的基于JSCC的反馈方案，该方案充分利用信道二阶统计量来优化特征空间中的维度投影，并分析了其在大信噪比（SNR）下的质量标度指数（QSE）行为。此外，本文还推导了在最大比传输和迫零预编码下，任何反馈方案的信道估计二阶统计量对DL数据传输遍历和速率的闭式下限。", "result": "通过大量的数值结果表明，本文提出的基于JSCC的方案优于已知的JSCC、SSCC基线和基于深度学习的方案，并且在实际SNR范围内能够接近最优DR方案的性能。", "conclusion": "本文提出了一种实用的基于JSCC的反馈方案，该方案在最小反馈延迟下表现出色，能够有效解决大规模MIMO系统中下行链路CSIT获取的挑战。", "translation": "下行链路（DL）信道状态信息在发射端（CSIT）的获取在多用户大规模MIMO系统中是一项具有挑战性的任务，尤其当上行/下行信道互易性不成立时（例如，在频分双工系统）。从编码角度看，用户通过DL训练获取的DL信道状态可视为信息源，必须通过UL通信信道传达给基站。源通过信道传输可以通过分离或联合源信道编码（SSCC或JSCC）来完成。在这项工作中，我们首先利用经典的远程失真-速率（DR）理论，为基于JSCC和SSCC的反馈方案提供了信道估计均方误差（MSE）的理论下限，然而这需要编码大量连续的信道状态块，因此在实践中无法使用，因为它会导致极大的反馈延迟。然后，我们关注最小（一个时隙）反馈延迟的相关情况，并提出了一种实用的基于JSCC的反馈方案，该方案充分利用信道二阶统计量来优化特征空间中的维度投影。我们分析了所提出的基于JSCC方案在大信噪比（SNR）下的质量标度指数（QSE）行为。给定任何反馈方案的信道估计二阶统计量，我们进一步推导了在最大比传输和迫零预编码下，DL数据传输遍历和速率的闭式下限。通过大量的数值结果，我们表明我们提出的基于JSCC的方案优于已知的JSCC、SSCC基线和基于深度学习的方案，并且在实际SNR范围内能够接近最优DR方案的性能。", "summary": "本文针对大规模MIMO系统中下行链路CSIT获取的挑战，探讨了压缩反馈下联合与分离源信道编码的性能。研究首先基于失真-速率理论推导了理论MSE下限，然后提出了一种实用的、基于JSCC的最小反馈延迟方案。该方案通过优化特征空间投影，并利用信道二阶统计量，在数值模拟中表现出优于现有基线方案的性能，并接近理论最优。", "keywords": "下行链路CSIT, 压缩反馈, 联合源信道编码, 分离源信道编码, 大规模MIMO", "comments": "本文创新性地将远程失真-速率理论应用于压缩反馈下的CSIT获取，并提出了一种在实际场景中具有低延迟且性能优越的JSCC方案。其通过充分利用信道二阶统计量来优化维度投影，并能接近理论最优性能，这对于解决大规模MIMO系统中的CSIT反馈挑战具有重要意义。"}}
{"id": "2506.10490", "title": "Predictive control of wastewater treatment plants as energy-autonomous water resource recovery facilities", "authors": ["Otacilio B. L. Neto", "Michela Mulas", "Iiro Harjunkoski", "Francesco Corona"], "summary": "This work proposes an automatic control solution for the operation of\nconventional wastewater treatment plants (WWTPs) as energy-autonomous water\nresource recovery facilities. We first conceptualize a classification of the\nquality of treated water for three resource recovery applications\n(environmental, industrial, and agricultural water reuse). We then present an\noutput-feedback model predictive controller (Output MPC) that operates a plant\nto produce water of specific quality class, while also producing sufficient\nbiogas to ensure nonpositive energy costs. The controller is demonstrated in\nthe long-term operation of a full-scale WWTP subjected to typical influent\nloads and periodically changing quality targets. Our results provide a\nproof-of-concept on the energy-autonomous operation of existing wastewater\ntreatment infrastructure with control strategies that are general enough to\naccommodate a wide range of resource recovery objectives.", "comment": "13 pages, 8 figures (main text); 27 pages, 2 figures (supplementary\n  material)", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10490v1", "AI": {"title_translation": "污水处理厂的预测控制作为能源自主型水资源回收设施", "tldr": "本文提出了一种用于污水处理厂的自动控制解决方案，使其能够作为能源自主型水资源回收设施运行，通过输出反馈模型预测控制器在满足特定水质目标的同时实现能源自给自足。", "motivation": "本研究旨在为传统污水处理厂提供一种自动控制解决方案，使其能够作为能源自主型水资源回收设施运行，目标是生产特定水质的水并同时产生足够的沼气以确保非正能源成本。", "method": "研究首先对处理水的质量进行了分类，以适应三种资源回收应用（环境、工业和农业用水再利用）。然后，提出了一种输出反馈模型预测控制器（Output MPC），该控制器能够运行工厂以生产特定质量等级的水，同时产生足够的沼气以确保非正能源成本。", "result": "结果证明了现有污水处理基础设施在能源自主运行方面的概念验证，所提出的控制策略具有足够的通用性，能够适应广泛的资源回收目标。该控制器在全尺寸污水处理厂的长期运行中得到了验证，该工厂受到典型的进水负荷和周期性变化的水质目标的影响。", "conclusion": "本研究成功展示了通过预测控制策略，现有污水处理厂能够实现能源自主运行，并满足多种水资源回收目标，为未来的水处理设施提供了可行且灵活的解决方案。", "translation": "这项工作提出了一种自动控制解决方案，用于将传统污水处理厂（WWTP）作为能源自主型水资源回收设施运行。我们首先对处理水的质量进行了分类，以适应三种资源回收应用（环境、工业和农业用水再利用）。然后，我们提出了一种输出反馈模型预测控制器（Output MPC），该控制器运行工厂以生产特定质量等级的水，同时产生足够的沼气以确保非正能源成本。该控制器在全尺寸污水处理厂的长期运行中得到了验证，该工厂受到典型的进水负荷和周期性变化的水质目标的影响。我们的结果为现有污水处理基础设施的能源自主运行提供了概念验证，所提出的控制策略具有足够的通用性，能够适应广泛的资源回收目标。", "summary": "本文提出了一种创新的自动控制解决方案，旨在将传统污水处理厂转变为能源自主型水资源回收设施。通过对处理水质进行分类以适应多种再利用场景，并应用输出反馈模型预测控制器（Output MPC），该系统能在满足特定水质要求的同时，产生足够沼气以实现能源自给自足。实验证明，该方法能有效管理全尺寸污水处理厂的长期运行，为现有基础设施实现能源自主和灵活的水资源回收提供了可行性证明。", "keywords": "污水处理厂, 预测控制, 能源自主, 水资源回收, 水再利用", "comments": "本文的创新之处在于将预测控制应用于污水处理厂的能源自主运行，并结合了灵活的水质分类以适应多种资源回收目标。这对于提升污水处理厂的经济性和可持续性具有重要意义，展示了现有基础设施通过智能控制实现功能升级的潜力。"}}
{"id": "2506.10513", "title": "A Neural Network-aided Low Complexity Chase Decoder for URLLC", "authors": ["Enrico Testi", "Enrico Paolini"], "summary": "Ultra-reliable low-latency communications (URLLC) demand decoding algorithms\nthat simultaneously offer high reliability and low complexity under stringent\nlatency constraints. While iterative decoding schemes for LDPC and Polar codes\noffer a good compromise between performance and complexity, they fall short in\napproaching the theoretical performance limits in the typical URLLC short block\nlength regime. Conversely, quasi-ML decoding schemes for algebraic codes, like\nChase-II decoding, exhibit a smaller gap to optimum decoding but are\ncomputationally prohibitive for practical deployment in URLLC systems. To\nbridge this gap, we propose an enhanced Chase-II decoding algorithm that\nleverages a neural network (NN) to predict promising perturbation patterns,\ndrastically reducing the number of required decoding trials. The proposed\napproach combines the reliability of quasi-ML decoding with the efficiency of\nNN inference, making it well-suited for time-sensitive and resource-constrained\napplications.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10513v1", "AI": {"title_translation": "一种神经辅助的低复杂度URLLC追逐译码器", "tldr": "本文提出一种神经网络辅助的增强型Chase-II译码算法，通过预测扰动模式，显著减少译码试验次数，从而在URLLC系统中实现高可靠性和低复杂度。", "motivation": "超可靠低延迟通信（URLLC）需要同时提供高可靠性和低复杂度的译码算法。现有迭代译码方案在URLLC短块长度下无法接近理论性能极限，而准最大似然（quasi-ML）译码方案（如Chase-II）虽然性能接近最优，但计算成本过高，不适合实际部署。", "method": "本文提出一种增强型Chase-II译码算法，该算法利用神经网络（NN）预测有前景的扰动模式，从而大幅减少所需的译码试验次数。该方法结合了准最大似然译码的可靠性与神经网络推理的效率。", "result": "所提出的方法结合了准最大似然译码的可靠性与神经网络推理的效率，使其非常适合时间敏感和资源受限的应用。", "conclusion": "本文提出的神经网络辅助的Chase-II译码器，通过减少译码试验次数，有效弥合了URLLC系统中准最大似然译码的性能与计算复杂度之间的差距。", "translation": "超可靠低延迟通信（URLLC）要求译码算法在严格的延迟约束下同时提供高可靠性和低复杂度。虽然用于LDPC和极性码的迭代译码方案在性能和复杂度之间提供了良好的折衷，但它们在典型的URLLC短块长度范围内未能接近理论性能极限。相反，代数码的准最大似然（quasi-ML）译码方案，如Chase-II译码，表现出与最优译码之间更小的差距，但对于URLLC系统的实际部署而言，计算成本过高。为了弥合这一差距，我们提出了一种增强型Chase-II译码算法，该算法利用神经网络（NN）预测有前景的扰动模式，从而大幅减少所需的译码试验次数。所提出的方法结合了准最大似然译码的可靠性与神经网络推理的效率，使其非常适合时间敏感和资源受限的应用。", "summary": "本文针对URLLC对高可靠性和低复杂度的译码需求，指出现有迭代译码和准最大似然译码方案的局限性。为解决此问题，论文提出一种创新的神经网络辅助增强型Chase-II译码算法，通过神经网络预测扰动模式来显著减少译码试验次数，从而在保证准最大似然译码可靠性的同时，提升译码效率，使其适用于资源受限的URLLC系统。", "keywords": "URLLC, Chase-II译码, 神经网络, 低复杂度, 译码算法", "comments": "本文的创新之处在于将神经网络引入到传统的Chase-II译码过程中，用于预测扰动模式，从而显著降低了其计算复杂度。这对于URLLC这种对延迟和资源高度敏感的应用场景具有重要意义，因为它提供了一种在性能和效率之间取得更好平衡的解决方案。"}}
{"id": "2506.10304", "title": "The Alignment Trap: Complexity Barriers", "authors": ["Jasper Yao"], "summary": "We establish fundamental computational complexity barriers to verifying AI\nsafety as system capabilities scale. Our main results show that for AI systems\nwith expressiveness EXP$(m)$ above a critical threshold $\\tau$, safety\nverification requires exponential time and is coNP-complete. We formalize the\nCapability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI\ncapability drives societal safety requirements toward perfection, creating an\ninescapable tension with verification complexity. Through four core theorems,\nwe prove that (1) verification complexity grows exponentially with system\nexpressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the\npolicy space, (3) no finite set of alignment techniques can provide universal\ncoverage, and (4) robust safety properties form measure-zero sets for neural\nnetworks. These results characterize an \"intractability gap\" where practical\nsafety requirements fall within the region of computational intractability. We\nconclude by presenting a strategic trilemma: AI development must either\nconstrain system complexity to maintain verifiable safety, accept unverifiable\nrisks while scaling capabilities, or develop fundamentally new safety paradigms\nbeyond verification. Our work provides the first systematic\ncomplexity-theoretic analysis of AI alignment and establishes rigorous bounds\nthat any safety approach must confront. A formal verification of the core\ntheorems in Lean4 is currently in progress.", "comment": "29 Pages, 4 Figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10304v1", "AI": {"title_translation": "对齐陷阱：复杂性障碍", "tldr": "随着AI系统能力的扩展，验证其安全性面临根本的计算复杂性障碍，导致在实际中难以实现可验证的安全。", "motivation": "该研究旨在建立AI系统能力扩展时验证其安全性的根本计算复杂性障碍，并形式化地分析AI能力增长与社会安全要求之间不可避免的张力。", "method": "本研究通过建立形式化的计算复杂性理论框架，提出了能力-风险扩展（CRS）动态，并运用四个核心定理证明了AI安全验证的复杂性边界。核心定理的Lean4形式化验证正在进行中。", "result": "主要结果表明：1) 验证复杂性随系统表达能力呈指数增长；2) 安全策略在策略空间中占比极小（至多为 $2^{-2^m}$）；3) 任何有限的对齐技术集都无法提供普适覆盖；4) 鲁棒安全属性对于神经网络形成零测度集。这些结果共同揭示了一个“难处理性差距”。", "conclusion": "AI发展面临一个战略性三难困境：要么限制系统复杂性以保持可验证的安全性，要么在扩展能力的同时接受不可验证的风险，要么开发超越验证的全新安全范式。", "translation": "我们建立了验证AI安全性随系统能力扩展而面临的根本计算复杂性障碍。我们的主要结果表明，对于表达能力高于临界阈值 $\\tau$ 的EXP$(m)$ AI系统，安全验证需要指数时间并且是coNP完全的。我们形式化了能力-风险扩展（CRS）动态，它展示了AI能力的增长如何推动社会安全要求趋于完美，从而与验证复杂性之间产生不可避免的张力。通过四个核心定理，我们证明了：(1) 验证复杂性随系统表达能力呈指数增长，(2) 安全策略在策略空间中占比至多为 $2^{-2^m}$，(3) 任何有限的对齐技术集都无法提供普适覆盖，(4) 鲁棒安全属性对于神经网络形成零测度集。这些结果描述了一个“难处理性差距”，即实际安全要求落入计算上难以处理的区域。最后，我们提出了一个战略性三难困境：AI发展必须要么限制系统复杂性以保持可验证的安全性，要么在扩展能力的同时接受不可验证的风险，要么开发超越验证的全新安全范式。我们的工作首次提供了AI对齐的系统性复杂性理论分析，并建立了任何安全方法都必须面对的严格界限。核心定理在Lean4中的形式化验证正在进行中。", "summary": "该论文探讨了AI系统能力扩展时验证其安全性的根本计算复杂性障碍。研究表明，对于高表达能力的AI系统，安全验证具有指数级复杂性且是coNP完全的。论文提出了能力-风险扩展动态，并通过四个核心定理证明了验证复杂性与系统表达能力呈指数关系，安全策略空间极小，有限对齐技术无法提供普适覆盖，以及鲁棒安全属性形成零测度集。这些发现揭示了一个“难处理性差距”，并提出了AI发展在限制复杂性、接受不可验证风险或开发新安全范式之间的战略性三难困境。", "keywords": "AI安全, 计算复杂性, 对齐, 验证, 能力-风险扩展", "comments": "该论文创新性地从计算复杂性理论角度对AI对齐问题进行了系统性分析，揭示了随着AI能力提升，验证其安全性的内在难度。其提出的“难处理性差距”和“战略性三难困境”对AI安全研究和发展具有重要指导意义，强调了在追求AI能力的同时，必须正视其安全验证的根本性挑战。"}}
{"id": "2506.10675", "title": "ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation", "authors": ["Xi Chen", "Zhiqiang Shen", "Peng Cao", "Jinzhu Yang", "Osmar R. Zaiane"], "summary": "Medical images are usually collected from multiple domains, leading to domain\nshifts that impair the performance of medical image segmentation models. Domain\nGeneralization (DG) aims to address this issue by training a robust model with\nstrong generalizability. Recently, numerous domain randomization-based DG\nmethods have been proposed. However, these methods suffer from the following\nlimitations: 1) constrained efficiency of domain randomization due to their\nexclusive dependence on image style perturbation, and 2) neglect of the adverse\neffects of over-augmented images on model training. To address these issues, we\npropose a novel domain randomization-based DG method, called content style\naugmentation (ConStyX), for generalizable medical image segmentation.\nSpecifically, ConStyX 1) augments the content and style of training data,\nallowing the augmented training data to better cover a wider range of data\ndomains, and 2) leverages well-augmented features while mitigating the negative\neffects of over-augmented features during model training. Extensive experiments\nacross multiple domains demonstrate that our ConStyX achieves superior\ngeneralization performance. The code is available at\nhttps://github.com/jwxsp1/ConStyX.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10675v1", "AI": {"title_translation": "ConStyX: 内容风格增强用于可泛化医学图像分割", "tldr": "ConStyX通过内容和风格增强解决医学图像分割中的域泛化问题，提高了模型泛化性能。", "motivation": "医学图像通常从多个领域收集，导致领域偏移，从而损害医学图像分割模型的性能。现有的基于域随机化的领域泛化（DG）方法存在局限性：1）效率受限，因为它们仅依赖图像风格扰动；2）忽略了过度增强图像对模型训练的不利影响。", "method": "本文提出了一种新颖的基于域随机化的DG方法，名为内容风格增强（ConStyX），用于可泛化医学图像分割。ConStyX通过以下两点解决问题：1）增强训练数据的内容和风格，使增强数据能覆盖更广泛的数据领域；2）在模型训练中利用良好增强的特征，同时减轻过度增强特征的负面影响。", "result": "在多个领域进行的广泛实验表明，ConStyX实现了卓越的泛化性能。", "conclusion": "ConStyX通过有效的内容和风格增强策略，成功提升了医学图像分割模型的域泛化能力，解决了传统域随机化方法在效率和过度增强方面的局限性。", "translation": "医学图像通常从多个领域收集，导致领域偏移，从而损害医学图像分割模型的性能。领域泛化（DG）旨在通过训练一个具有强大泛化能力的鲁棒模型来解决这个问题。最近，许多基于域随机化的DG方法被提出。然而，这些方法存在以下局限性：1）由于其仅依赖图像风格扰动，域随机化的效率受到限制；2）忽略了过度增强图像对模型训练的不利影响。为了解决这些问题，我们提出了一种新颖的基于域随机化的DG方法，称为内容风格增强（ConStyX），用于可泛化医学图像分割。具体来说，ConStyX 1）增强了训练数据的内容和风格，使得增强后的训练数据能够更好地覆盖更广泛的数据领域；2）在模型训练期间利用良好增强的特征，同时减轻过度增强特征的负面影响。在多个领域进行的广泛实验表明，我们的ConStyX实现了卓越的泛化性能。代码可在https://github.com/jwxsp1/ConStyX获取。", "summary": "本文提出了一种名为ConStyX的新型域随机化方法，旨在解决医学图像分割中因域偏移导致的模型泛化能力差的问题。ConStyX通过同时增强训练数据的内容和风格，并有效利用增强特征同时避免过度增强的负面影响，显著提高了模型在不同医学图像域上的泛化性能。", "keywords": "域泛化, 医学图像分割, 内容风格增强, 域随机化, 泛化性能", "comments": "ConStyX的创新点在于结合了内容和风格增强，并考虑了过度增强的负面影响，这比传统仅依赖风格扰动的域随机化方法更全面和有效。该方法有望提高医学图像分割模型在实际多源数据应用中的鲁棒性。"}}
{"id": "2506.10580", "title": "Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture", "authors": ["Chengxu Zuo", "Jiawei Huang", "Xiao Jiang", "Yuan Yao", "Xiangren Shi", "Rui Cao", "Xinyu Yi", "Feng Xu", "Shihui Guo", "Yipeng Qin"], "summary": "In this paper, we propose a novel dynamic calibration method for sparse\ninertial motion capture systems, which is the first to break the restrictive\nabsolute static assumption in IMU calibration, i.e., the coordinate drift RG'G\nand measurement offset RBS remain constant during the entire motion, thereby\nsignificantly expanding their application scenarios. Specifically, we achieve\nreal-time estimation of RG'G and RBS under two relaxed assumptions: i) the\nmatrices change negligibly in a short time window; ii) the human movements/IMU\nreadings are diverse in such a time window. Intuitively, the first assumption\nreduces the number of candidate matrices, and the second assumption provides\ndiverse constraints, which greatly reduces the solution space and allows for\naccurate estimation of RG'G and RBS from a short history of IMU readings in\nreal time. To achieve this, we created synthetic datasets of paired RG'G, RBS\nmatrices and IMU readings, and learned their mappings using a Transformer-based\nmodel. We also designed a calibration trigger based on the diversity of IMU\nreadings to ensure that assumption ii) is met before applying our method. To\nour knowledge, we are the first to achieve implicit IMU calibration (i.e.,\nseamlessly putting IMUs into use without the need for an explicit calibration\nprocess), as well as the first to enable long-term and accurate motion capture\nusing sparse IMUs. The code and dataset are available at\nhttps://github.com/ZuoCX1996/TIC.", "comment": "Accepted by SIGGRAPH 2025 (TOG)", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10580v1", "AI": {"title_translation": "Transformer IMU校准器：惯性运动捕捉的动态在体IMU校准", "tldr": "提出一种基于Transformer的动态IMU校准方法，打破静态假设，实现更精确的惯性运动捕捉。", "motivation": "传统的IMU校准依赖于限制性的绝对静态假设，这极大地限制了其应用场景。本文旨在打破这一限制，扩展IMU校准的应用范围。", "method": "提出一种新颖的动态校准方法，打破了传统的绝对静态假设。该方法基于两个宽松的假设：1）在短时间内矩阵变化可忽略不计；2）在短时间内人体运动/IMU读数具有多样性。利用基于Transformer的模型学习合成数据集中坐标漂移（RG'G）和测量偏移（RBS）矩阵与IMU读数之间的映射。同时，设计了一个基于IMU读数多样性的校准触发器，以确保方法应用前满足第二个假设。", "result": "实现了对坐标漂移（RG'G）和测量偏移（RBS）的实时估计。首次实现了隐式IMU校准（即无需明确校准过程即可使用IMU）。首次实现了使用稀疏IMU进行长期、准确的运动捕捉。代码和数据集已公开。", "conclusion": "本文提出的Transformer IMU校准器通过实现动态、隐式和长期准确的IMU校准，显著扩展了稀疏惯性运动捕捉系统的应用场景。", "translation": "在本文中，我们提出了一种用于稀疏惯性运动捕捉系统的新型动态校准方法，这是首次打破IMU校准中限制性的绝对静态假设，即坐标漂移RG'G和测量偏移RBS在整个运动过程中保持不变，从而显著扩展了其应用场景。具体来说，我们在两个放松的假设下实现了RG'G和RBS的实时估计：i) 矩阵在短时间内变化可忽略不计；ii) 在这样的时间窗内，人体运动/IMU读数是多样化的。直观地说，第一个假设减少了候选矩阵的数量，第二个假设提供了多样化的约束，这极大地缩小了解决方案空间，并允许从短时间的IMU读数历史中实时准确估计RG'G和RBS。为了实现这一点，我们创建了配对的RG'G、RBS矩阵和IMU读数的合成数据集，并使用基于Transformer的模型学习它们的映射。我们还设计了一个基于IMU读数多样性的校准触发器，以确保在应用我们的方法之前满足假设ii)。据我们所知，我们是第一个实现隐式IMU校准（即无需明确校准过程即可无缝使用IMU）的，也是第一个使用稀疏IMU实现长期准确运动捕捉的。代码和数据集可在https://github.com/ZuoCX1996/TIC获取。", "summary": "本文提出了一种名为Transformer IMU校准器的新型动态校准方法，首次打破了惯性测量单元（IMU）校准中严格的绝对静态假设，从而显著扩展了稀疏惯性运动捕捉系统的应用场景。该方法在两个宽松假设下，利用基于Transformer的模型实时估计坐标漂移和测量偏移，并设计了校准触发器。这使得IMU能够隐式使用，并首次实现了使用稀疏IMU进行长期准确的运动捕捉。", "keywords": "动态校准, IMU, 运动捕捉, Transformer, 稀疏IMU", "comments": "本文的创新点在于首次打破了IMU校准中限制性的绝对静态假设，并通过引入Transformer模型实现了动态、实时的校准。这使得IMU能够隐式地投入使用，并首次实现了稀疏IMU的长期准确运动捕捉，极大地扩展了惯性运动捕捉的应用范围，具有重要的实际意义。"}}
{"id": "2506.10091", "title": "Efficient kernelized bandit algorithms via exploration distributions", "authors": ["Bingshan Hu", "Zheng He", "Danica J. Sutherland"], "summary": "We consider a kernelized bandit problem with a compact arm set ${X} \\subset\n\\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm\nin some Reproducing Kernel Hilbert Space (RKHS). We propose a class of\ncomputationally efficient kernelized bandit algorithms, which we call\nGP-Generic, based on a novel concept: exploration distributions. This class of\nalgorithms includes Upper Confidence Bound-based approaches as a special case,\nbut also allows for a variety of randomized algorithms. With careful choice of\nexploration distribution, our proposed generic algorithm realizes a wide range\nof concrete algorithms that achieve $\\tilde{O}(\\gamma_T\\sqrt{T})$ regret\nbounds, where $\\gamma_T$ characterizes the RKHS complexity. This matches known\nresults for UCB- and Thompson Sampling-based algorithms; we also show that in\npractice, randomization can yield better practical results.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10091v1", "AI": {"title_translation": "通过探索分布实现高效核化赌博机算法", "tldr": "本文提出了一类基于探索分布的高效核化赌博机算法GP-Generic，该算法在理论上实现了与现有算法相当的遗憾界，并在实践中表现出更好的随机化效果。", "motivation": "在紧凑的臂集上，寻找具有有限范数的未知奖励函数的最优策略，并开发计算高效的核化赌博机算法。", "method": "提出了一类名为GP-Generic的计算高效核化赌博机算法，其核心是“探索分布”的概念。该算法包含上置信界（UCB）方法作为特例，并允许使用多种随机化算法。通过仔细选择探索分布，实现了多种具体的算法。", "result": "所提出的通用算法能够实现 $\\tilde{O}(\\gamma_T\\sqrt{T})$ 的遗憾界，其中 $\\gamma_T$ 表征了RKHS的复杂性。这与基于UCB和Thompson采样算法的已知结果相匹配。此外，研究表明在实践中，随机化可以产生更好的实际结果。", "conclusion": "通过引入探索分布，GP-Generic算法提供了一种统一且高效的核化赌博机问题解决方案，其理论性能与现有最佳算法相当，并在实践中展示了随机化的优势。", "translation": "我们考虑一个核化赌博机问题，它具有紧凑的臂集 ${X} \\subset \\mathbb{R}^d $ 和一个固定但未知的奖励函数 $f^*$，该函数在某个再生核希尔伯特空间（RKHS）中具有有限范数。我们提出了一类计算高效的核化赌博机算法，我们称之为GP-Generic，它基于一个新颖的概念：探索分布。这类算法包括基于上置信界（Upper Confidence Bound）的方法作为特例，但也允许使用多种随机化算法。通过仔细选择探索分布，我们提出的通用算法实现了一系列具体的算法，这些算法达到了 $\\tilde{O}(\\gamma_T\\sqrt{T})$ 的遗憾界，其中 $\\gamma_T$ 表征了RKHS的复杂性。这与基于UCB和Thompson采样算法的已知结果相匹配；我们还表明，在实践中，随机化可以产生更好的实际结果。", "summary": "本文针对核化赌博机问题，提出了一类新颖且计算高效的算法GP-Generic。该算法引入了“探索分布”的概念，并涵盖了UCB等现有方法。通过优化探索分布的选择，GP-Generic算法在理论上实现了与UCB和Thompson采样算法相同的 $\\tilde{O}(\\gamma_T\\sqrt{T})$ 遗憾界。研究还指出，在实际应用中，随机化方法可能带来更好的性能。", "keywords": "核化赌博机, 探索分布, 遗憾界, GP-Generic, 随机化", "comments": "这篇论文的创新点在于引入了“探索分布”这一新概念，为核化赌博机算法提供了一个统一的框架，能够整合UCB等现有方法并引入随机化。其重要性在于不仅在理论上达到了最先进的遗憾界，还通过实践结果强调了随机化在实际应用中的潜在优势，这对于实际系统设计具有指导意义。"}}
{"id": "2506.10635", "title": "Conversational Search: From Fundamentals to Frontiers in the LLM Era", "authors": ["Fengran Mo", "Chuan Meng", "Mohammad Aliannejadi", "Jian-Yun Nie"], "summary": "Conversational search enables multi-turn interactions between users and\nsystems to fulfill users' complex information needs. During this interaction,\nthe system should understand the users' search intent within the conversational\ncontext and then return the relevant information through a flexible,\ndialogue-based interface. The recent powerful large language models (LLMs) with\ncapacities of instruction following, content generation, and reasoning, attract\nsignificant attention and advancements, providing new opportunities and\nchallenges for building up intelligent conversational search systems. This\ntutorial aims to introduce the connection between fundamentals and the emerging\ntopics revolutionized by LLMs in the context of conversational search. It is\ndesigned for students, researchers, and practitioners from both academia and\nindustry. Participants will gain a comprehensive understanding of both the core\nprinciples and cutting-edge developments driven by LLMs in conversational\nsearch, equipping them with the knowledge needed to contribute to the\ndevelopment of next-generation conversational search systems.", "comment": "Accepted by Tutorial Track in SIGIR 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10635v1", "AI": {"title_translation": "对话式搜索：从基础到LLM时代的未来前沿", "tldr": "本教程旨在介绍对话式搜索的基础知识以及大型语言模型（LLM）如何彻底改变该领域的最新发展，为下一代对话式搜索系统提供见解。", "motivation": "随着大型语言模型（LLM）的兴起，对话式搜索系统面临新的机遇和挑战，因此需要一个教程来连接对话式搜索的基础知识与LLM带来的前沿发展。", "method": "本教程旨在介绍对话式搜索的基础知识与大型语言模型（LLM）所彻底改变的新兴主题之间的联系，为学生、研究人员和从业者提供全面的理解。", "result": "参与者将全面了解对话式搜索的核心原则以及由LLM驱动的尖端发展，掌握为下一代对话式搜索系统开发做出贡献所需的知识。", "conclusion": "本教程旨在使参与者掌握必要的知识，从而为下一代对话式搜索系统的发展做出贡献。", "translation": "对话式搜索使用户和系统能够进行多轮交互，以满足用户复杂的信息需求。在此交互过程中，系统应在对话上下文中理解用户的搜索意图，然后通过灵活的、基于对话的界面返回相关信息。最近强大的大型语言模型（LLM）凭借其指令遵循、内容生成和推理能力，吸引了广泛的关注和进展，为构建智能对话式搜索系统提供了新的机遇和挑战。本教程旨在介绍对话式搜索背景下，基础知识与LLM所彻底改变的新兴主题之间的联系。它专为来自学术界和工业界的学生、研究人员和从业者设计。参与者将全面了解对话式搜索的核心原则以及由LLM驱动的尖端发展，掌握为下一代对话式搜索系统开发做出贡献所需的知识。", "summary": "这篇教程探讨了对话式搜索的原理及其在大型语言模型（LLM）时代的前沿发展。对话式搜索通过多轮交互满足用户复杂的信息需求，要求系统理解对话上下文中的意图并返回相关信息。LLM的出现为构建智能对话式搜索系统带来了新的机遇和挑战。本教程旨在为学生、研究人员和从业者提供对话式搜索基础知识与LLM驱动的最新进展之间的全面理解，以促进下一代系统的发展。", "keywords": "对话式搜索, 大型语言模型, 信息检索, 多轮交互, 教程", "comments": "这篇教程非常及时且重要，因为它弥合了对话式搜索的传统概念与大型语言模型带来的最新突破之间的鸿沟。它为理解LLM如何赋能和重塑复杂信息检索交互提供了宝贵的资源，对于该领域的未来发展具有指导意义。"}}
{"id": "2506.10139", "title": "Unsupervised Elicitation of Language Models", "authors": ["Jiaxin Wen", "Zachary Ankner", "Arushi Somani", "Peter Hase", "Samuel Marks", "Jacob Goldman-Wetzler", "Linda Petrini", "Henry Sleight", "Collin Burns", "He He", "Shi Feng", "Ethan Perez", "Jan Leike"], "summary": "To steer pretrained language models for downstream tasks, today's\npost-training paradigm relies on humans to specify desired behaviors. However,\nfor models with superhuman capabilities, it is difficult or impossible to get\nhigh-quality human supervision. To address this challenge, we introduce a new\nunsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune\npretrained language models on their own generated labels, \\emph{without\nexternal supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward\nmodeling tasks, our method matches the performance of training on golden\nsupervision and outperforms training on crowdsourced human supervision. On\ntasks where LMs' capabilities are strongly superhuman, our method can elicit\nthose capabilities significantly better than training on human labels. Finally,\nwe show that our method can improve the training of frontier LMs: we use our\nmethod to train an unsupervised reward model and use reinforcement learning to\ntrain a Claude 3.5 Haiku-based assistant. Both the reward model and the\nassistant outperform their human-supervised counterparts.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10139v1", "AI": {"title_translation": "无监督语言模型能力激发", "tldr": "该论文提出了一种名为内部一致性最大化（ICM）的无监督算法，用于在没有外部监督的情况下微调预训练语言模型，并在多项任务上表现优于人类监督。", "motivation": "当前预训练语言模型在下游任务上的引导范式依赖人类指定期望行为，但对于具有超人能力的模型，难以或不可能获得高质量的人类监督。", "method": "提出了一种新的无监督算法——内部一致性最大化（ICM），用于在模型自身生成的标签上微调预训练语言模型，无需外部监督。该方法还被用于训练无监督奖励模型，并通过强化学习训练基于Claude 3.5 Haiku的助手。", "result": "在GSM8k-verification、TruthfulQA和Alpaca奖励建模任务上，ICM方法与黄金监督的性能相匹配，并优于众包人类监督。在语言模型能力远超人类的任务上，该方法能显著更好地激发这些能力。训练出的无监督奖励模型和基于Claude 3.5 Haiku的助手均优于其人类监督的对应版本。", "conclusion": "内部一致性最大化（ICM）是一种有效的无监督算法，能够解决超人能力语言模型的人类监督难题，并在多项任务上表现出色，甚至在某些情况下超越人类监督，同时能改进前沿语言模型的训练。", "translation": "为了引导预训练语言模型用于下游任务，当今的训练后范式依赖人类指定期望行为。然而，对于具有超人能力的模型，很难或不可能获得高质量的人类监督。为了解决这一挑战，我们引入了一种新的无监督算法——内部一致性最大化（ICM），用于在模型自身生成的标签上微调预训练语言模型，无需外部监督。在GSM8k-verification、TruthfulQA和Alpaca奖励建模任务上，我们的方法与使用黄金监督进行训练的性能相匹配，并优于使用众包人类监督进行训练。在语言模型能力远超人类的任务上，我们的方法可以比使用人类标签进行训练更好地激发这些能力。最后，我们展示了我们的方法可以改进前沿语言模型的训练：我们使用我们的方法训练了一个无监督奖励模型，并使用强化学习训练了一个基于Claude 3.5 Haiku的助手。奖励模型和助手都优于其人类监督的对应版本。", "summary": "该论文提出了一种名为内部一致性最大化（ICM）的无监督算法，旨在解决超人能力语言模型难以获得高质量人类监督的问题。ICM通过让模型在其自身生成的标签上进行微调，无需外部监督。实验结果表明，在GSM8k-verification、TruthfulQA和Alpaca奖励建模等多个任务上，ICM的表现与黄金监督相当，优于众包人类监督，并且在语言模型能力远超人类的任务上能更好地激发其能力。此外，该方法还能有效提升前沿语言模型的训练，例如训练出的无监督奖励模型和基于Claude 3.5 Haiku的助手均优于其人类监督版本。", "keywords": "无监督学习, 语言模型, 微调, 内部一致性最大化, 超人能力", "comments": "该论文的创新之处在于提出了一种完全无监督的语言模型微调方法，即内部一致性最大化（ICM），通过利用模型自身生成的标签来替代耗时且昂贵的人类监督。这对于随着语言模型能力不断增强、超越人类水平后，人类监督变得困难或不可能的现状具有重要意义。该方法为未来更强大语言模型的训练和对齐提供了一条可扩展的路径。"}}
{"id": "2506.10119", "title": "Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers", "authors": ["Natanael Lucena", "Fábio S. da Silva", "Ricardo Rios"], "summary": "This paper presents a comparison of the performance of Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying\nimages containing lesions of psoriasis and diseases similar to it. Models\npre-trained on ImageNet were adapted to a specific data set. Both achieved high\npredictive metrics, but the ViTs stood out for their superior performance with\nsmaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the\nbest results, with an f1-score of 96.4%, and is recommended as the most\nefficient architecture for automated psoriasis detection. This article\nreinforces the potential of ViTs for medical image classification tasks.", "comment": "12 pages, in Portuguese language, 2 figures, 2 tables, and 4\n  formulas. To be published in the Proceedings of the LII Brazilian Integrated\n  Software and Hardware Seminar 2025 (SEMISH 2025)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10119v1", "AI": {"title_translation": "银屑病检测中的计算机视觉应用：CNNs与Vision Transformers的比较方法", "tldr": "该研究比较了CNN和Vision Transformer在银屑病图像分类中的表现，发现Vision Transformer，特别是DaViT-B，在较小模型下表现更优，并推荐其用于自动化银屑病检测。", "motivation": "论文旨在比较CNNs和Vision Transformers在多分类含有银屑病及类似疾病病变图像任务中的性能。", "method": "论文采用的方法是比较预训练在ImageNet上的卷积神经网络（CNNs）和Vision Transformers（ViTs）在特定数据集上进行多类别图像分类的性能，用于检测银屑病及其类似疾病。", "result": "CNNs和ViTs都取得了高预测指标。ViTs在较小模型下表现出卓越的性能。其中，Dual Attention Vision Transformer-Base (DaViT-B) 获得了最佳结果，f1-score达到96.4%。", "conclusion": "DaViT-B被推荐为自动化银屑病检测最有效的架构。本文强化了ViTs在医学图像分类任务中的潜力。", "translation": "这篇论文比较了卷积神经网络（CNNs）和Vision Transformers（ViTs）在多类别图像分类任务中的性能，这些图像包含银屑病及其类似疾病的病变。在ImageNet上预训练的模型被适配到一个特定的数据集。两者都取得了高预测指标，但ViTs以其在较小模型下的卓越性能脱颖而出。Dual Attention Vision Transformer-Base (DaViT-B) 取得了最好的结果，f1-score达到96.4%，并被推荐为自动化银屑病检测最有效的架构。本文强化了ViTs在医学图像分类任务中的潜力。", "summary": "本文比较了卷积神经网络（CNNs）和Vision Transformers（ViTs）在银屑病及类似疾病图像多分类任务中的表现。研究发现，尽管两者均表现出色，但ViTs在较小模型下展现出优越性能。特别是Dual Attention Vision Transformer-Base (DaViT-B) 取得了96.4%的f1-score，并被推荐为自动化银屑病检测的最佳架构，强调了ViTs在医学图像分类领域的潜力。", "keywords": "银屑病检测, 计算机视觉, 卷积神经网络, Vision Transformers, 医学图像分类", "comments": "本文的创新之处在于首次将Vision Transformers应用于银屑病图像的自动化检测，并与传统的CNNs进行性能比较。其重要性在于证明了ViTs在医学图像分类任务中，尤其是在模型规模较小的情况下，能够超越CNNs，为未来高效、准确的医学诊断提供了新的方向和潜力。"}}
{"id": "2506.10754", "title": "BNMusic: Blending Environmental Noises into Personalized Music", "authors": ["Chi Zuo", "Martin B. Møller", "Pablo Martínez-Nuevo", "Huayang Huang", "Yu Wu", "Ye Zhu"], "summary": "While being disturbed by environmental noises, the acoustic masking technique\nis a conventional way to reduce the annoyance in audio engineering that seeks\nto cover up the noises with other dominant yet less intrusive sounds. However,\nmisalignment between the dominant sound and the noise-such as mismatched\ndownbeats-often requires an excessive volume increase to achieve effective\nmasking. Motivated by recent advances in cross-modal generation, in this work,\nwe introduce an alternative method to acoustic masking, aiming to reduce the\nnoticeability of environmental noises by blending them into personalized music\ngenerated based on user-provided text prompts. Following the paradigm of music\ngeneration using mel-spectrogram representations, we propose a Blending Noises\ninto Personalized Music (BNMusic) framework with two key stages. The first\nstage synthesizes a complete piece of music in a mel-spectrogram representation\nthat encapsulates the musical essence of the noise. In the second stage, we\nadaptively amplify the generated music segment to further reduce noise\nperception and enhance the blending effectiveness, while preserving auditory\nquality. Our experiments with comprehensive evaluations on MusicBench,\nEPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework,\nhighlighting the ability to blend environmental noise with rhythmically\naligned, adaptively amplified, and enjoyable music segments, minimizing the\nnoticeability of the noise, thereby improving overall acoustic experiences.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10754v1", "AI": {"title_translation": "BNMusic：将环境噪声融入个性化音乐", "tldr": "BNMusic提出了一种新方法，通过将环境噪声融入个性化音乐来降低其可感知性，解决了传统声学掩蔽中声音与噪声不匹配的问题。", "motivation": "传统的声学掩蔽技术在处理环境噪声时，由于主导声音与噪声（如节拍）之间存在错位，往往需要过度提高音量才能有效掩蔽，这导致了效果不佳。受跨模态生成最新进展的启发，本文旨在引入一种替代方法，通过将环境噪声融入根据用户文本提示生成的个性化音乐中，以降低噪声的可感知性。", "method": "本文提出了一个名为BNMusic（将噪声融入个性化音乐）的框架，该框架基于梅尔频谱图表示的音乐生成范式，并包含两个关键阶段。第一阶段，合成一段完整的梅尔频谱图表示的音乐，该音乐封装了噪声的音乐本质。第二阶段，自适应地放大生成的音乐片段，以进一步降低噪声感知并增强融合效果，同时保持听觉质量。", "result": "在MusicBench、EPIC-SOUNDS和ESC-50上进行的综合评估实验证明了BNMusic框架的有效性。实验结果突出表明，该框架能够将环境噪声与节奏对齐、自适应放大且令人愉悦的音乐片段融合，从而最大限度地降低噪声的可感知性。", "conclusion": "BNMusic框架通过将环境噪声融入个性化音乐中，提供了一种有效且创新的方法来改善整体听觉体验，解决了传统声学掩蔽的局限性。", "translation": "当受到环境噪声干扰时，声学掩蔽技术是音频工程中一种传统的降低烦恼的方法，它试图用其他主导但干扰性较小的声音来覆盖噪声。然而，主导声音与噪声之间的错位——例如节拍不匹配——通常需要过度提高音量才能实现有效的掩蔽。受跨模态生成最新进展的启发，在这项工作中，我们引入了一种替代声学掩蔽的方法，旨在通过将环境噪声融入根据用户提供的文本提示生成的个性化音乐中来降低其可感知性。遵循使用梅尔频谱图表示进行音乐生成的范式，我们提出了一个名为“将噪声融入个性化音乐”（BNMusic）的框架，该框架包含两个关键阶段。第一阶段，合成一段完整的梅尔频谱图表示的音乐，该音乐封装了噪声的音乐本质。在第二阶段，我们自适应地放大生成的音乐片段，以进一步降低噪声感知并增强融合效果，同时保持听觉质量。我们在MusicBench、EPIC-SOUNDS和ESC-50上进行的综合评估实验证明了我们框架的有效性，突出了将环境噪声与节奏对齐、自适应放大且令人愉悦的音乐片段融合的能力，最大限度地降低了噪声的可感知性，从而改善了整体听觉体验。", "summary": "BNMusic提出了一种新颖的框架，旨在通过将环境噪声融入个性化音乐来解决传统声学掩蔽的局限性。该方法通过两个阶段实现：首先，根据噪声的音乐本质合成一段完整的音乐（梅尔频谱图表示）；其次，自适应放大生成的音乐片段以增强噪声融合和降低感知，同时保持音质。实验结果表明，该框架能有效将噪声与节奏对齐、自适应放大的音乐融合，从而显著降低噪声可感知性并提升听觉体验。", "keywords": "环境噪声, 个性化音乐, 声学掩蔽, 音乐生成, 梅尔频谱图", "comments": "BNMusic提出了一种创新的方法来处理环境噪声，通过将噪声融入音乐而非简单覆盖，解决了传统声学掩蔽中声音与噪声不匹配的问题。其利用跨模态生成和梅尔频谱图表示，通过两阶段处理实现了噪声的音乐化融合，显著提升了用户听觉体验。这项工作在个性化音频处理和噪声管理方面具有重要意义和应用潜力。"}}
{"id": "2506.10428", "title": "Penalty-Based Feedback Control and Finite Element Analysis for the Stabilization of Nonlinear Reaction-Diffusion Equations", "authors": ["Sudeep Kundu", "Shishu pal Singh"], "summary": "In this work, first we employ the penalization technique to analyze the\nDirichlet boundary feedback control problem pertaining to reaction-diffusion\nequation. We establish the stabilization result of the equivalent Robin problem\nin the \\(H^{2}\\)-norm with respect to the penalty parameter. Furthermore, we\nprove that the solution of the penalized control problem converges to the\ncorresponding solution of the Dirichlet boundary feedback control problem as\nthe penalty parameter \\(\\epsilon\\) approaches zero. A \\(C^{0}\\)-conforming\nfinite element method is applied to this problem for the spatial variable while\nkeeping the time variable continuous. We discuss the stabilization of the\nsemi-discrete scheme for the penalized control problem and present an error\nanalysis of its solution. Finally, we validate our theoretical findings through\nnumerical experiments.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10428v1", "AI": {"title_translation": "基于惩罚的反馈控制和有限元分析用于非线性反应扩散方程的稳定性", "tldr": "本文利用惩罚技术分析了反应扩散方程的Dirichlet边界反馈控制问题，证明了等效Robin问题在H2范数下的稳定性，并证明了惩罚控制问题的解在惩罚参数趋于零时收敛到Dirichlet问题的解。文章还应用C0-一致有限元方法处理空间变量，讨论了半离散格式的稳定性并进行了误差分析，最后通过数值实验验证了理论结果。", "motivation": "本文旨在通过采用惩罚技术来分析反应扩散方程的Dirichlet边界反馈控制问题，并建立其稳定性。", "method": "1. 采用惩罚技术分析Dirichlet边界反馈控制问题。\n2. 建立等效Robin问题在H2范数下的稳定性结果。\n3. 证明惩罚控制问题的解在惩罚参数趋于零时收敛到Dirichlet边界反馈控制问题的相应解。\n4. 应用C0-一致有限元方法处理空间变量，时间变量保持连续。\n5. 讨论惩罚控制问题的半离散格式的稳定性并进行误差分析。\n6. 通过数值实验验证理论发现。", "result": "1. 建立了等效Robin问题在H2范数下关于惩罚参数的稳定性结果。\n2. 证明了惩罚控制问题的解在惩罚参数趋于零时收敛到Dirichlet边界反馈控制问题的相应解。\n3. 讨论了惩罚控制问题半离散格式的稳定性。\n4. 提出了其解的误差分析。\n5. 通过数值实验验证了理论发现。", "conclusion": "通过数值实验验证了理论发现，表明基于惩罚的反馈控制和有限元分析方法能够有效地稳定非线性反应扩散方程。", "translation": "在这项工作中，我们首先采用惩罚技术来分析与反应扩散方程相关的Dirichlet边界反馈控制问题。我们建立了等效Robin问题在H2范数下关于惩罚参数的稳定性结果。此外，我们证明了当惩罚参数ε趋于零时，惩罚控制问题的解收敛到Dirichlet边界反馈控制问题的相应解。本文将C0-一致有限元方法应用于此问题的空间变量，同时保持时间变量连续。我们讨论了惩罚控制问题半离散格式的稳定性，并提出了其解的误差分析。最后，我们通过数值实验验证了我们的理论发现。", "summary": "本文研究了非线性反应扩散方程的Dirichlet边界反馈控制问题。作者采用惩罚技术将原问题转化为等效的Robin问题，并证明了其在H2范数下的稳定性。研究还证明了惩罚问题的解在惩罚参数趋于零时收敛到Dirichlet问题的解。在数值方法上，文章对空间变量应用了C0-一致有限元方法，并分析了半离散格式的稳定性及误差。最终，通过数值实验验证了所有理论结果。", "keywords": "惩罚反馈控制, 反应扩散方程, 有限元分析, 稳定性, 收敛性", "comments": "本文的创新点在于将惩罚技术应用于非线性反应扩散方程的Dirichlet边界反馈控制问题，并提供了严格的数学分析，包括稳定性、收敛性证明以及误差分析。结合有限元方法进行数值验证，使得理论与实践相结合，具有较强的实际应用价值。"}}
{"id": "2506.10104", "title": "Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection", "authors": ["David Farr", "Kevin Talty", "Alexandra Farr", "John Stockdale", "Iain Cruickshank", "Jevin West"], "summary": "As cyber threats become more sophisticated, rapid and accurate vulnerability\ndetection is essential for maintaining secure systems. This study explores the\nuse of Large Language Models (LLMs) in software vulnerability assessment by\nsimulating the identification of Python code with known Common Weakness\nEnumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot\nin-domain prompting strategies. Our results indicate that while zero-shot\nprompting performs poorly, few-shot prompting significantly enhances\nclassification performance, particularly when integrated with confidence-based\nrouting strategies that improve efficiency by directing human experts to cases\nwhere model uncertainty is high, optimizing the balance between automation and\nexpert oversight. We find that LLMs can effectively generalize across\nvulnerability categories with minimal examples, suggesting their potential as\nscalable, adaptable cybersecurity tools in simulated environments. However,\nchallenges such as model reliability, interpretability, and adversarial\nrobustness remain critical areas for future research. By integrating AI-driven\napproaches with expert-in-the-loop (EITL) decision-making, this work highlights\na pathway toward more efficient and responsive cybersecurity workflows. Our\nfindings provide a foundation for deploying AI-assisted vulnerability detection\nsystems in both real and simulated environments that enhance operational\nresilience while reducing the burden on human analysts.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10104v1", "AI": {"title_translation": "专家在环系统中，结合跨领域和领域内小样本学习进行软件漏洞检测", "tldr": "大型语言模型（LLMs）结合小样本学习和专家在环系统显著提升了软件漏洞检测的效率和准确性，但仍面临可靠性和可解释性等挑战。", "motivation": "鉴于网络威胁日益复杂，快速准确的漏洞检测对于维护系统安全至关重要。本研究旨在探索大型语言模型（LLMs）在软件漏洞评估中的应用，以期增强操作弹性并减轻人类分析师的负担。", "method": "本研究通过模拟识别带有已知通用弱点枚举（CWEs）的Python代码，评估了大型语言模型（LLMs）在软件漏洞检测中的应用。具体方法包括比较零样本、跨领域小样本和领域内小样本提示策略，并整合基于置信度的路由策略，以实现专家在环（EITL）决策，将人类专家引导至模型不确定性高的案例。", "result": "研究结果显示，零样本提示表现不佳，而小样本提示显著提高了分类性能。特别是结合基于置信度的路由策略后，效率得到了提升。此外，LLMs能够通过少量示例有效泛化到不同的漏洞类别。", "conclusion": "将AI驱动的方法与专家在环（EITL）决策相结合，为实现更高效、更具响应性的网络安全工作流程提供了途径。本研究的发现为在真实和模拟环境中部署AI辅助漏洞检测系统奠定了基础，这些系统能够增强操作弹性，同时减轻人类分析师的负担。", "translation": "随着网络威胁日益复杂，快速准确的漏洞检测对于维护系统安全至关重要。本研究通过模拟识别带有已知通用弱点枚举（CWEs）的Python代码，探讨了大型语言模型（LLMs）在软件漏洞评估中的应用，并比较了零样本、跨领域小样本和领域内小样本提示策略。我们的结果表明，零样本提示表现不佳，而小样本提示显著提高了分类性能，尤其是在与基于置信度的路由策略结合时，通过将人类专家引导至模型不确定性高的案例，提高了效率，优化了自动化与专家监督之间的平衡。我们发现LLMs可以通过少量示例有效泛化到不同的漏洞类别，这表明它们在模拟环境中作为可扩展、适应性强的网络安全工具的潜力。然而，模型可靠性、可解释性和对抗鲁棒性等挑战仍然是未来研究的关键领域。通过将AI驱动的方法与专家在环（EITL）决策相结合，这项工作指明了一条通向更高效、更具响应性的网络安全工作流程的路径。我们的发现为在真实和模拟环境中部署AI辅助漏洞检测系统奠定了基础，这些系统能够增强操作弹性，同时减轻人类分析师的负担。", "summary": "本研究探讨了大型语言模型（LLMs）在软件漏洞检测中的应用，通过模拟Python代码漏洞识别，比较了零样本、跨领域小样本和领域内小样本提示策略。研究发现，小样本方法，特别是结合基于置信度的路由策略以实现专家在环系统时，能显著提高性能和效率。论文强调了LLMs在网络安全领域泛化和可扩展性的潜力，同时也指出了模型可靠性和可解释性等挑战。该工作提出了一种AI与专家相结合的方法，旨在提升网络安全工作流程的效率和响应性。", "keywords": "软件漏洞检测, 大型语言模型, 小样本学习, 专家在环, 网络安全", "comments": "本文通过将大型语言模型与专家在环系统相结合，为软件漏洞检测提供了一种创新方法，优化了自动化与人工监督之间的平衡。其中，基于置信度的路由策略是提高效率的关键创新点。尽管研究展示了小样本学习在泛化方面的潜力，但作者坦诚地指出了模型可靠性和可解释性等关键局限性，为未来的研究指明了清晰的方向。"}}
{"id": "2506.10773", "title": "Learning Chaotic Dynamics with Neuromorphic Network Dynamics", "authors": ["Yinhao Xu", "Georg A. Gottwald", "Zdenka Kuncic"], "summary": "This study investigates how dynamical systems may be learned and modelled\nwith a neuromorphic network which is itself a dynamical system. The\nneuromorphic network used in this study is based on a complex electrical\ncircuit comprised of memristive elements that produce neuro-synaptic nonlinear\nresponses to input electrical signals. To determine how computation may be\nperformed using the physics of the underlying system, the neuromorphic network\nwas simulated and evaluated on autonomous prediction of a multivariate chaotic\ntime series, implemented with a reservoir computing framework. Through\nmanipulating only input electrodes and voltages, optimal nonlinear dynamical\nresponses were found when input voltages maximise the number of memristive\ncomponents whose internal dynamics explore the entire dynamical range of the\nmemristor model. Increasing the network coverage with the input electrodes was\nfound to suppress other nonlinear responses that are less conducive to\nlearning. These results provide valuable insights into how a practical\nneuromorphic network device can be optimised for learning complex dynamical\nsystems using only external control parameters.", "comment": "37 pages, 22 figures", "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.10773v1", "AI": {"title_translation": "利用神经形态网络动力学学习混沌动力学", "tldr": "本研究探讨了如何利用基于忆阻元件的神经形态网络（本身也是一个动力系统）来学习和建模混沌动力学，并通过外部控制参数优化其学习能力。", "motivation": "本研究旨在探究如何利用神经形态网络（其本身就是一个动力系统）来学习和建模其他动力系统，并利用其底层物理特性进行计算。", "method": "研究使用了基于忆阻元件的复杂电路构建的神经形态网络。该网络在储层计算框架下，通过模拟和评估其对多元混沌时间序列的自主预测能力。通过操纵输入电极和电压来优化网络响应。", "result": "当输入电压使忆阻元件的内部动力学探索其整个动力学范围时，发现了最佳的非线性动力学响应。增加输入电极的网络覆盖率可以抑制不利于学习的其他非线性响应。", "conclusion": "这些结果为如何仅使用外部控制参数来优化实际神经形态网络设备以学习复杂动力系统提供了宝贵的见解。", "translation": "本研究探讨了如何利用神经形态网络（其本身也是一个动力系统）来学习和建模动力系统。本研究中使用的神经形态网络基于一个复杂的电路，该电路包含忆阻元件，这些元件对输入的电信号产生神经突触的非线性响应。为了确定如何利用底层系统的物理特性进行计算，对神经形态网络进行了模拟和评估，以实现多元混沌时间序列的自主预测，并采用储层计算框架。通过仅操纵输入电极和电压，当输入电压使忆阻元件的内部动力学探索忆阻器模型的整个动力学范围时，发现了最佳的非线性动力学响应。研究发现，增加输入电极的网络覆盖率可以抑制其他不利于学习的非线性响应。这些结果为如何仅使用外部控制参数来优化实际神经形态网络设备以学习复杂动力系统提供了宝贵见解。", "summary": "本研究探索了利用基于忆阻元件的神经形态网络（作为一个动力系统）学习和建模混沌动力学的方法。通过模拟并结合储层计算框架，研究评估了该网络在多元混沌时间序列自主预测上的表现。结果表明，通过优化输入电压使忆阻器充分探索其动力学范围，以及增加输入电极覆盖率，能够获得最佳的非线性响应并抑制不利于学习的响应。这些发现为通过外部控制参数优化神经形态设备学习复杂动力系统提供了重要指导。", "keywords": "神经形态网络, 混沌动力学, 忆阻元件, 储层计算, 动力系统", "comments": "这项研究的创新之处在于利用神经形态网络的固有动力学特性，特别是基于忆阻元件的电路，来直接学习和模拟混沌系统，而不是仅仅将其作为传统的计算单元。它强调了利用物理系统本身的特性进行计算的潜力，为未来高效、低功耗的神经形态计算设备设计提供了新的思路。其重要性在于为理解如何通过简单的外部控制参数优化复杂物理系统以执行计算任务提供了实践指导。"}}
{"id": "2506.10330", "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "summary": "This study examined code issue detection and revision automation by\nintegrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and\nGPT-4o into software development workflows. A static code analysis framework\ndetects issues such as bugs, vulnerabilities, and code smells within a\nlarge-scale software project. Detailed information on each issue was extracted\nand organized to facilitate automated code revision using LLMs. An iterative\nprompt engineering process is applied to ensure that prompts are structured to\nproduce accurate and organized outputs aligned with the project requirements.\nRetrieval-augmented generation (RAG) is implemented to enhance the relevance\nand precision of the revisions, enabling LLM to access and integrate real-time\nexternal knowledge. The issue of LLM hallucinations - where the model generates\nplausible but incorrect outputs - is addressed by a custom-built \"Code\nComparison App,\" which identifies and corrects erroneous changes before\napplying them to the codebase. Subsequent scans using the static code analysis\nframework revealed a significant reduction in code issues, demonstrating the\neffectiveness of combining LLMs, static analysis, and RAG to improve code\nquality, streamline the software development process, and reduce time and\nresource expenditure.", "comment": "Accepted at FORGE 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10330v1", "AI": {"title_translation": "结合静态代码分析增强大型语言模型，实现自动化代码质量改进", "tldr": "本研究结合大语言模型和静态代码分析，通过RAG和自定义应用，自动化检测并修正代码问题，显著提升代码质量。", "motivation": "解决代码问题检测和修订自动化的问题，通过集成LLMs到软件开发工作流中，以提升代码质量、简化开发流程并减少时间和资源消耗。", "method": "将LLMs（如GPT-3.5 Turbo和GPT-4o）集成到软件开发工作流中。使用静态代码分析框架检测代码中的bug、漏洞和代码异味。提取并组织详细问题信息，用于LLM自动化代码修订。应用迭代提示工程，确保提示结构化以生成准确和有组织输出。实施检索增强生成（RAG）以提高修订的相关性和精确性。通过自定义“代码比较应用”解决LLM幻觉问题，识别并纠正错误修改。后续使用静态代码分析框架扫描以验证效果。", "result": "后续使用静态代码分析框架扫描显示代码问题显著减少。", "conclusion": "结合LLMs、静态分析和RAG能有效提高代码质量，简化软件开发流程，并减少时间和资源消耗。", "translation": "本研究通过将大型语言模型（LLM），如OpenAI的GPT-3.5 Turbo和GPT-4o，整合到软件开发工作流程中，探讨了代码问题检测和修订自动化。一个静态代码分析框架用于检测大型软件项目中的错误、漏洞和代码异味等问题。提取并组织每个问题的详细信息，以便于使用LLM进行自动化代码修订。应用迭代式提示工程，确保提示结构化，以生成与项目要求对齐的准确和有组织输出。实施检索增强生成（RAG），以提高修订的相关性和精确性，使LLM能够访问和整合实时外部知识。LLM幻觉——即模型生成看似合理但错误输出的问题——通过一个定制的“代码比较应用”来解决，该应用在将更改应用到代码库之前识别并纠正错误更改。随后使用静态代码分析框架进行扫描显示，代码问题显著减少，证明了结合LLM、静态分析和RAG在提高代码质量、简化软件开发流程以及减少时间和资源消耗方面的有效性。", "summary": "本研究探索了通过整合大型语言模型（LLM）和静态代码分析框架实现代码问题检测与修订的自动化。研究设计了一个流程，利用静态分析识别代码问题，提取详细信息，并通过迭代提示工程和检索增强生成（RAG）指导LLM进行代码修订。为解决LLM幻觉问题，开发了一个“代码比较应用”来验证和修正LLM生成的代码。实验结果显示，该方法显著减少了代码问题，证明了结合LLM、静态分析和RAG在提升代码质量、优化开发流程和降低成本方面的有效性。", "keywords": "大语言模型, 静态代码分析, 代码质量, 自动化修订, 检索增强生成", "comments": "这项研究通过将静态代码分析与大型语言模型（LLM）相结合，并引入检索增强生成（RAG）和自定义的代码比较应用来解决LLM的幻觉问题，为自动化代码质量改进提供了一个创新且实用的框架。其创新之处在于系统地整合了现有技术，尤其是在实际应用中解决了LLM可能产生的错误输出，这对于提高自动化代码修订的可靠性至关重要。这项工作的重要性在于它能够显著提升软件开发效率和代码质量，减少人工干预，具有很高的实际应用价值。"}}
{"id": "2506.10279", "title": "Learning Safe Control via On-the-Fly Bandit Exploration", "authors": ["Alexandre Capone", "Ryan Cosner", "Aaaron Ames", "Sandra Hirche"], "summary": "Control tasks with safety requirements under high levels of model uncertainty\nare increasingly common. Machine learning techniques are frequently used to\naddress such tasks, typically by leveraging model error bounds to specify\nrobust constraint-based safety filters. However, if the learned model\nuncertainty is very high, the corresponding filters are potentially invalid,\nmeaning no control input satisfies the constraints imposed by the safety\nfilter. While most works address this issue by assuming some form of safe\nbackup controller, ours tackles it by collecting additional data on the fly\nusing a Gaussian process bandit-type algorithm. We combine a control barrier\nfunction with a learned model to specify a robust certificate that ensures\nsafety if feasible. Whenever infeasibility occurs, we leverage the control\nbarrier function to guide exploration, ensuring the collected data contributes\ntoward the closed-loop system safety. By combining a safety filter with\nexploration in this manner, our method provably achieves safety in a setting\nthat allows for a zero-mean prior dynamics model, without requiring a backup\ncontroller. To the best of our knowledge, it is the first safe learning-based\ncontrol method that achieves this.", "comment": "arXiv admin note: text overlap with arXiv:2311.02133", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10279v1", "AI": {"title_translation": "通过即时Bandit探索学习安全控制", "tldr": "本文提出了一种新的安全学习控制方法，通过在线数据收集解决高模型不确定性下的安全控制问题，无需备份控制器，并实现了可证明的安全性。", "motivation": "在模型不确定性高的情况下，基于模型误差界限的安全滤波器可能失效，且现有方法通常依赖于安全备份控制器，这在某些场景下可能不可行或难以设计。", "method": "该方法结合了控制障碍函数与学习模型来提供安全保证。当安全滤波器出现不可行性时，利用控制障碍函数指导基于高斯过程bandit算法的在线数据探索，以收集有助于闭环系统安全的新数据。", "result": "该方法在允许零均值先验动力学模型的设定下，无需备份控制器即可实现可证明的安全性。据作者所知，这是首个实现此目标的基于安全学习的控制方法。", "conclusion": "本文提出了一种创新的安全学习控制方法，通过将安全滤波器与在线探索相结合，解决了高模型不确定性下的安全控制挑战，其核心贡献在于无需依赖备份控制器即可提供可证明的安全性。", "translation": "在模型不确定性高的情况下，具有安全要求的控制任务越来越普遍。机器学习技术常用于解决此类任务，通常通过利用模型误差界限来指定鲁棒的基于约束的安全滤波器。然而，如果学习到的模型不确定性非常高，相应的滤波器可能会失效，这意味着没有控制输入能满足安全滤波器施加的约束。虽然大多数工作通过假设某种形式的安全备份控制器来解决这个问题，但我们的方法通过使用高斯过程bandit型算法即时收集额外数据来解决。我们将控制障碍函数与学习模型相结合，以指定一个鲁棒的证书，如果可行则确保安全。每当出现不可行性时，我们利用控制障碍函数来指导探索，确保收集到的数据有助于闭环系统安全。通过将安全滤波器与探索以这种方式结合，我们的方法在允许零均值先验动力学模型的设置中可证明地实现了安全性，而无需备份控制器。据我们所知，这是第一个实现这一点的基于安全学习的控制方法。", "summary": "本文提出了一种新颖的安全学习控制方法，旨在解决高模型不确定性下安全滤波器可能失效的问题。该方法通过结合控制障碍函数和学习模型来确保安全，并在出现不可行性时，利用控制障碍函数指导基于高斯过程bandit的在线数据探索，以提高闭环系统安全性。其核心创新在于无需备份控制器，即可在零均值先验动力学模型下实现可证明的安全性。", "keywords": "安全控制, 在线探索, Bandit算法, 控制障碍函数, 模型不确定性", "comments": "这篇论文的创新点在于它提供了一种无需安全备份控制器就能处理高模型不确定性下安全控制问题的方法。通过将探索过程与安全机制（控制障碍函数）紧密结合，确保了在数据收集过程中也能维持系统安全，这是对现有安全学习控制方法的显著改进，尤其是在实际应用中，安全备份控制器可能难以设计或存在限制的场景。"}}
{"id": "2506.10598", "title": "Accessible Design in Integrated Development Environments: A Think Aloud Study Exploring the Experiences of Students with ADHD", "authors": ["Luke Halpin", "Phillip Benachour", "Tracy Hall", "Ann-Marie Houghton", "Emily Winter"], "summary": "Coding forms a key part of computer science education in universities. As\npart of this education, Integrated Development Environments (IDEs) are\nessential tools for coding. However, it is currently unknown how the design of\nan IDE's interface impacts on students with Attention Deficit Hyperactivity\nDisorder (ADHD).\n  In this study we investigated the use of IDEs by students with ADHD. We\nconducted a think aloud study with nine university computing students, followed\nby qualitative observational interviews to analyse their learning and\nengagement with the Visual Studio Code IDE. The paper reports on these\nexperiences and seeks to understand the role IDEs play in the educational\nsetting.\n  Our work also examines how digital accessibility and usability are considered\nin the current design of IDEs. We analysed the qualitative data using a\nthematic analysis and identified three primary themes: self-confidence,\ninteraction, and learning as well as various sub-themes.\n  The themes and their sub-themes illustrate key areas of consideration when\ndesigning IDEs for students with ADHD. The primary findings highlight\nexperiences of frustration and barriers in the current design and layout of\nIDEs.\n  Through our participatory approach we provide a rare insight into ADHD user\nexperiences around usability and accessibility, and describe the need for\nbetter design of development environments to ensure a positive learning\nexperience for the students.", "comment": "16 pages, 3 figures, ECTEL 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10598v1", "AI": {"title_translation": "集成开发环境中的可访问性设计：一项探索ADHD学生体验的“出声思考”研究", "tldr": "研究ADHD学生在使用IDE时的体验，发现现有设计存在挫折和障碍，强调需改进可访问性以提升学习体验。", "motivation": "目前尚不清楚集成开发环境（IDE）的界面设计如何影响注意力缺陷多动障碍（ADHD）学生，而IDE是计算机科学教育的关键工具，因此有必要进行本研究以了解其影响。", "method": "本研究对九名大学计算机专业学生进行了“出声思考”研究，随后进行定性观察访谈，以分析他们对Visual Studio Code IDE的学习和参与情况。定性数据通过主题分析法进行分析。", "result": "识别出三个主要主题：自信心、互动和学习，以及各种子主题。主要发现强调了现有IDE设计和布局中的挫折和障碍体验。", "conclusion": "现有IDE设计对ADHD学生存在挑战，需要更好地设计开发环境以确保积极的学习体验。在为ADHD学生设计IDE时，应考虑可访问性和可用性方面的关键领域。", "translation": "编程是大学计算机科学教育的重要组成部分。作为教育的一部分，集成开发环境（IDE）是重要的编程工具。然而，目前尚不清楚IDE的界面设计如何影响注意力缺陷多动障碍（ADHD）学生。\n在本研究中，我们调查了ADHD学生对IDE的使用情况。我们对九名大学计算机专业学生进行了一项“出声思考”研究，随后进行了定性观察访谈，以分析他们对Visual Studio Code IDE的学习和参与情况。本文报告了这些经验，并试图理解IDE在教育环境中的作用。\n我们的工作还审查了当前IDE设计中如何考虑数字可访问性和可用性。我们使用主题分析法分析了定性数据，并确定了三个主要主题：自信心、互动和学习，以及各种子主题。\n这些主题及其子主题阐明了在为ADHD学生设计IDE时需要考虑的关键领域。主要发现强调了现有IDE设计和布局中的挫折和障碍体验。\n通过我们的参与式方法，我们提供了关于ADHD用户在使用性与可访问性方面体验的罕见见解，并描述了需要更好地设计开发环境以确保学生获得积极学习体验的需求。", "summary": "本研究通过对9名ADHD大学计算机学生进行“出声思考”研究和定性访谈，探讨了集成开发环境（IDE）对其学习体验的影响。研究发现，现有IDE设计在可访问性和可用性方面存在问题，导致学生产生挫折感和学习障碍。通过主题分析，研究确定了自信心、互动和学习三个主要主题。论文强调了为ADHD学生改进IDE设计的必要性，以促进更积极的学习体验。", "keywords": "ADHD, 集成开发环境, 可访问性设计, 用户体验, 出声思考", "comments": "这项研究通过深入的定性方法，为理解ADHD学生在使用IDE时的独特挑战提供了宝贵的见解。其创新之处在于聚焦了一个常被忽视的用户群体，并直接揭示了现有工具设计中的不足。研究结果对于推动更具包容性的软件开发工具设计具有重要意义。"}}
{"id": "2506.10531", "title": "GPU-Accelerated Distributed QAOA on Large-scale HPC Ecosystems", "authors": ["Zhihao Xu", "Srikar Chundury", "Seongmin Kim", "Amir Shehata", "Xinyi Li", "Ang Li", "Tengfei Luo", "Frank Mueller", "In-Saeng Suh"], "summary": "Quantum computing holds great potential to accelerate the process of solving\ncomplex combinatorial optimization problems. The Distributed Quantum\nApproximate Optimization Algorithm (DQAOA) addresses high-dimensional, dense\nproblems using current quantum computing techniques and high-performance\ncomputing (HPC) systems. In this work, we improve the scalability and\nefficiency of DQAOA through advanced problem decomposition and parallel\nexecution using message passing on the Frontier CPU/GPU supercomputer. Our\napproach ensures efficient quantum-classical workload management by\ndistributing large problem instances across classical and quantum resources.\nExperimental results demonstrate that enhanced decomposition strategies and\nGPU-accelerated quantum simulations significantly improve DQAOA's performance,\nachieving up to 10x speedup over CPU-based simulations. This advancement\nenables better scalability for large problem instances, supporting the\npractical deployment of GPU systems for hybrid quantum-classical applications.\nWe also highlight ongoing integration efforts using the Quantum Framework (QFw)\nto support future HPC-quantum computing systems.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10531v1", "AI": {"title_translation": "GPU加速的大规模HPC生态系统上的分布式QAOA", "tldr": "该论文通过高级问题分解和GPU加速的量子模拟，显著提升了大规模HPC系统上分布式量子近似优化算法（DQAOA）的性能和可扩展性，实现了高达10倍的加速。", "motivation": "量子计算在加速解决复杂组合优化问题方面具有巨大潜力，而分布式量子近似优化算法（DQAOA）旨在利用现有量子计算技术和高性能计算（HPC）系统解决高维度、密集型问题。", "method": "本研究通过在Frontier CPU/GPU超级计算机上使用高级问题分解和基于消息传递的并行执行，提高了DQAOA的可扩展性和效率。该方法通过在经典和量子资源之间分配大型问题实例，确保了高效的量子-经典工作负载管理。", "result": "实验结果表明，增强的分解策略和GPU加速的量子模拟显著提高了DQAOA的性能，比基于CPU的模拟实现了高达10倍的加速。这一进展使得大型问题实例具有更好的可扩展性。", "conclusion": "本研究支持将GPU系统实际部署到混合量子-经典应用中。同时，论文还强调了利用量子框架（QFw）进行持续集成的工作，以支持未来的HPC-量子计算系统。", "translation": "量子计算在加速解决复杂组合优化问题方面具有巨大潜力。分布式量子近似优化算法（DQAOA）利用当前的量子计算技术和高性能计算（HPC）系统来解决高维度、密集型问题。在这项工作中，我们通过在Frontier CPU/GPU超级计算机上使用高级问题分解和并行消息传递执行，提高了DQAOA的可扩展性和效率。我们的方法通过在经典和量子资源之间分配大型问题实例，确保了高效的量子-经典工作负载管理。实验结果表明，增强的分解策略和GPU加速的量子模拟显著提高了DQAOA的性能，比基于CPU的模拟实现了高达10倍的加速。这一进展使得大型问题实例具有更好的可扩展性，支持将GPU系统实际部署到混合量子-经典应用中。我们还强调了利用量子框架（QFw）进行的持续集成工作，以支持未来的HPC-量子计算系统。", "summary": "本论文旨在提升分布式量子近似优化算法（DQAOA）在处理高维组合优化问题时的可扩展性和效率。研究人员通过在Frontier CPU/GPU超级计算机上实施先进的问题分解和GPU加速的并行量子模拟，显著优化了DQAOA的性能。实验结果表明，与CPU相比，GPU加速带来了高达10倍的性能提升，这对于大规模混合量子-经典应用中GPU系统的实际部署具有重要意义。", "keywords": "DQAOA, GPU加速, 量子计算, 高性能计算, 组合优化", "comments": "这项工作在将量子算法扩展到大规模问题方面迈出了重要一步，特别是在高性能计算（HPC）环境中利用GPU加速，这对于混合量子-经典计算的发展至关重要。其创新点在于结合了先进的问题分解、消息传递并行以及GPU加速，有效提升了DQAOA的性能和可扩展性。这项研究的重要性在于展示了GPU在加速量子模拟中的潜力，并为未来HPC-量子计算系统的集成奠定了基础。"}}
{"id": "2506.10718", "title": "Anomaly Detection for Sensing Security", "authors": ["Stefan Roth", "Aydin Sezgin"], "summary": "Various approaches in the field of physical layer security involve anomaly\ndetection, such as physical layer authentication, sensing attacks, and\nanti-tampering solutions. Depending on the context in which these approaches\nare applied, anomaly detection needs to be computationally lightweight,\nresilient to changes in temperature and environment, and robust against phase\nnoise. We adapt moving average filters, autoregression filters and Kalman\nfilters to provide predictions of feature vectors that fulfill the above\ncriteria. Different hypothesis test designs are employed that allow\nomnidirectional and unidirectional outlier detection. In a case study, a\nsensing attack is investigated that employs the described algorithms with\nvarious channel features based on commodity WiFi devices. Thereby, various\ncombinations of algorithms and channel features show effectiveness for motion\ndetection by an attacker. Countermeasures only utilizing transmit power\nrandomization are shown insufficient to mitigate such attacks if the attacker\nhas access to channel state information (CSI) measurements, suggesting that\nmitigation solutions might require frequency-variant randomization.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10718v1", "AI": {"title_translation": "传感安全中的异常检测", "tldr": "本文针对传感安全中的异常检测，提出并评估了适应于物理层安全场景的轻量级、鲁棒性强的异常检测算法，并揭示了现有对抗措施的不足。", "motivation": "物理层安全领域中的多种方法（如物理层认证、传感攻击和防篡改解决方案）涉及异常检测，但这些检测方法需要满足计算轻量化、对温度和环境变化具有弹性以及对相位噪声具有鲁棒性等条件。", "method": "采用移动平均滤波器、自回归滤波器和卡尔曼滤波器来预测满足上述特征向量的特性。采用不同的假设检验设计来实现全向和单向异常值检测。通过一个案例研究，使用基于商用WiFi设备的各种信道特征，利用上述算法调查了一种传感攻击。", "result": "各种算法和信道特征的组合在攻击者进行运动检测时显示出有效性。仅利用发射功率随机化的对策不足以减轻此类攻击，如果攻击者可以访问信道状态信息（CSI）测量结果。", "conclusion": "现有仅利用发射功率随机化的对抗措施不足以减轻此类传感攻击，如果攻击者可以访问信道状态信息（CSI）测量结果，这表明缓解方案可能需要频率相关的随机化。", "translation": "物理层安全领域的各种方法都涉及异常检测，例如物理层认证、传感攻击和防篡改解决方案。根据这些方法应用的具体环境，异常检测需要具备计算轻量化、对温度和环境变化具有弹性以及对相位噪声具有鲁棒性等特点。我们采用移动平均滤波器、自回归滤波器和卡尔曼滤波器来提供满足上述条件的特征向量预测。文中采用了不同的假设检验设计，以实现全向和单向异常值检测。在一个案例研究中，我们调查了一种利用所描述的算法和基于商用WiFi设备的各种信道特征的传感攻击。结果表明，各种算法和信道特征的组合在攻击者进行运动检测时显示出有效性。研究还表明，如果攻击者能够获取信道状态信息（CSI）测量结果，仅利用发射功率随机化的对策不足以减轻此类攻击，这暗示缓解方案可能需要频率相关的随机化。", "summary": "本文探讨了传感安全中物理层异常检测的需求，强调了计算效率、环境适应性和抗噪声能力的重要性。作者提出了基于移动平均、自回归和卡尔曼滤波器的方法来预测特征向量，并结合了全向和单向异常值检测的假设检验。通过一个使用商用WiFi设备进行的传感攻击案例研究，验证了所提出算法在运动检测中的有效性。研究还指出，现有仅依赖发射功率随机化的防御措施在攻击者可获取CSI时是不足的，建议未来缓解方案可能需要频率相关的随机化。", "keywords": "异常检测, 传感安全, 物理层安全, 信道状态信息, 滤波器", "comments": "本文针对物理层安全中的异常检测问题，提出了结合多种经典滤波器（移动平均、自回归、卡尔曼）的轻量级且鲁棒的解决方案，并特别强调了在实际应用中需要考虑的计算资源、环境变化和噪声影响。其通过具体的传感攻击案例研究验证了方法的有效性，同时揭示了现有防御机制的局限性，并提出了未来防御策略的改进方向，具有重要的实践指导意义。"}}
{"id": "2506.10535", "title": "Analyzing the performance of a V2X-enhanced braking system in real-world crash situations", "authors": ["Jan Zimmermann", "Jörg Mönnich", "Michael Scherl", "Ignacio Llatser", "Florian Wildschütte", "Frank Hofmann"], "summary": "By using an automated braking system, such as the Automatic Emergency Brake\n(AEB), crashes can be avoided in situations where the driver is unaware of an\nimminent collision. However, conventional AEB systems detect potential\ncollision adversaries with onboard sensor systems, such as radars and cameras,\nthat may fail in non-line-of-sight situations. By leveraging\nvehicle-to-everything (V2X) communication, information regarding an approaching\nvehicle can be received by the ego vehicle at an early point in time, even if\nthe opponent vehicle is occluded by a view obstruction. In this work, we\nconsider a 2-stage braking cascade, consisting of a partial brake, triggered\nbased on V2X information, and a sensor-triggered AEB. We evaluate its crash\navoidance performance in real-world crash situations extracted from the German\nIn-Depth Accident Study (GIDAS) database using an accident simulation\nframework. The results are compared against a sensor-triggered AEB system and a\npurely V2X-triggered partial brake. To further analyze the results, we identify\nthe crash cause for each situation in which the brake function under test could\nnot prevent the crash. The simulation results show a high added benefit of the\nV2X-enhanced braking systems compared to the exclusive use of visual-based\nsensor systems for automated collision prevention.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10535v1", "AI": {"title_translation": "分析V2X增强型制动系统在真实碰撞情况下的性能", "tldr": "本研究评估了V2X增强型两阶段制动系统在真实碰撞场景中的防撞性能，发现它比传统基于传感器的系统有显著优势。", "motivation": "传统的自动紧急制动（AEB）系统依赖车载传感器，在非视距（non-line-of-sight）情况下可能会失效。通过利用车联网（V2X）通信，车辆可以提前接收到关于接近车辆的信息，即使有视线障碍，从而解决传统AEB的局限性。", "method": "本研究考虑了一个两阶段制动级联系统，包括一个基于V2X信息触发的部分制动和一个传感器触发的AEB。研究人员使用事故模拟框架，评估了该系统在从德国深度事故研究（GIDAS）数据库中提取的真实碰撞情况下的防撞性能。结果与纯传感器触发的AEB系统和纯V2X触发的部分制动进行了比较，并分析了未能避免碰撞的原因。", "result": "模拟结果表明，与单独使用基于视觉的传感器系统进行自动防撞相比，V2X增强型制动系统具有显著的附加优势。", "conclusion": "V2X增强型制动系统在自动防撞方面显示出比传统传感器系统更高的性能和附加价值，尤其是在非视距情况下。", "translation": "通过使用自动制动系统，例如自动紧急制动（AEB），可以在驾驶员未意识到即将发生碰撞的情况下避免事故。然而，传统的AEB系统通过车载传感器系统（如雷达和摄像头）检测潜在的碰撞对手，这些系统在非视距情况下可能会失效。通过利用车联网（V2X）通信，即使对手车辆被视线障碍物遮挡，本车也能在早期接收到关于接近车辆的信息。在这项工作中，我们考虑了一个两阶段制动级联，包括基于V2X信息触发的部分制动和传感器触发的AEB。我们使用事故模拟框架，评估了其在从德国深度事故研究（GIDAS）数据库中提取的真实碰撞情况下的防撞性能。结果与传感器触发的AEB系统和纯V2X触发的部分制动进行了比较。为了进一步分析结果，我们确定了测试制动功能未能避免碰撞的每种情况下的碰撞原因。模拟结果显示，与单独使用基于视觉的传感器系统进行自动碰撞预防相比，V2X增强型制动系统具有很高的附加效益。", "summary": "本研究提出并评估了一种V2X增强型两阶段制动系统，该系统结合了V2X信息触发的部分制动和传感器触发的自动紧急制动（AEB），旨在克服传统AEB在非视距情况下的局限性。通过使用来自GIDAS数据库的真实碰撞场景进行模拟，结果表明，与仅使用传感器或纯V2X触发的制动系统相比，V2X增强型系统在碰撞避免方面提供了显著的改进。", "keywords": "V2X, 自动紧急制动, 防撞, 非视距, 事故模拟", "comments": "本文的创新点在于提出了一个结合V2X和传统传感器数据的两阶段制动系统，有效解决了传统AEB在非视距环境下的性能瓶颈。通过在真实世界事故数据上进行模拟评估，增强了研究结果的说服力。这项工作对于未来智能驾驶辅助系统的发展具有重要意义，尤其是在提高系统在复杂交通环境下的可靠性和安全性方面。"}}
{"id": "2506.10596", "title": "Sum Rate Maximization for Pinching Antennas Assisted RSMA System With Multiple Waveguides", "authors": ["Peiyu Wang", "Hong Wang", "Rongfang Song"], "summary": "In this letter, a pinching antennas (PAs) assisted rate splitting multiple\naccess (RSMA) system with multiple waveguides is investigated to maximize sum\nrate. A two-step algorithm is proposed to determine PA activation scheme and\noptimize the waveguide beamforming. Specifically, a low complexity spatial\ncorrelation and distance based method is proposed for PA activation selection.\nAfter determining the PA activation status, a semi-definite programming (SDP)\nbased successive convex approximation (SCA) is leveraged to obtain the optimal\nwaveguide beamforming. Simulation results show that the proposed multiple\nwaveguides based PAs assisted RSMA method achieves better performance than\nvarious benchmarking schemes.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10596v1", "AI": {"title_translation": "多波导辅助捏合天线RSMA系统和速率最大化", "tldr": "本文提出了一种多波导辅助捏合天线（PA）的速率分裂多址（RSMA）系统，旨在最大化和速率，并通过两步算法优化PA激活方案和波导波束成形，仿真结果表明其性能优于现有方案。", "motivation": "本研究旨在最大化捏合天线（PAs）辅助的速率分裂多址（RSMA）系统在多波导配置下的和速率。", "method": "提出了一种两步算法。第一步，采用低复杂度基于空间相关性和距离的方法选择PA激活方案。第二步，在PA激活状态确定后，利用基于半正定规划（SDP）的逐次凸逼近（SCA）方法来优化波导波束成形。", "result": "仿真结果表明，所提出的基于多波导的PA辅助RSMA方法比各种基准方案取得了更好的性能。", "conclusion": "所提出的基于多波导的PA辅助RSMA方法能够有效提高系统和速率，并优于现有基准方案。", "translation": "本文研究了多波导辅助的捏合天线（PAs）速率分裂多址（RSMA）系统，以最大化和速率。提出了一种两步算法来确定PA激活方案并优化波导波束成形。具体而言，提出了一种低复杂度的基于空间相关性和距离的方法用于PA激活选择。在确定PA激活状态后，利用基于半正定规划（SDP）的逐次凸逼近（SCA）来获得最优的波导波束成形。仿真结果表明，所提出的基于多波导的PA辅助RSMA方法比各种基准方案取得了更好的性能。", "summary": "本文研究了多波导辅助的捏合天线（PAs）速率分裂多址（RSMA）系统中的和速率最大化问题。为此，提出了一种两步优化算法：首先，通过低复杂度的空间相关性和距离方法选择PA激活方案；其次，利用基于半正定规划（SDP）的逐次凸逼近（SCA）技术优化波导波束成形。仿真结果验证了所提方法相比现有基准方案具有更优的性能。", "keywords": "捏合天线, RSMA, 和速率最大化, 多波导, 波束成形", "comments": "该论文提出了一种新颖的两步算法来解决多波导辅助RSMA系统中的和速率最大化问题，其创新点在于结合了PA激活方案的选择和波导波束成形的优化。通过采用低复杂度的PA激活选择和SDP-SCA进行波束成形，提高了系统的性能。这项工作对未来无线通信系统中集成新型天线技术和高级多址方案具有重要意义。"}}
{"id": "2506.10010", "title": "Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction", "authors": ["Von Ralph Dane Marquez Herbuela", "Yukie Nagai"], "summary": "Human emotional expression emerges through coordinated vocal, facial, and\ngestural signals. While speech face alignment is well established, the broader\ndynamics linking emotionally expressive speech to regional facial and hand\nmotion remains critical for gaining a deeper insight into how emotional and\nbehavior cues are communicated in real interactions. Further modulating the\ncoordination is the structure of conversational exchange like sequential turn\ntaking, which creates stable temporal windows for multimodal synchrony, and\nsimultaneous speech, often indicative of high arousal moments, disrupts this\nalignment and impacts emotional clarity. Understanding these dynamics enhances\nrealtime emotion detection by improving the accuracy of timing and synchrony\nacross modalities in both human interactions and AI systems. This study\nexamines multimodal emotion coupling using region specific motion capture from\ndyadic interactions in the IEMOCAP corpus. Speech features included low level\nprosody, MFCCs, and model derived arousal, valence, and categorical emotions\n(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker\ndisplacements. Expressive activeness was quantified through framewise\ndisplacement magnitudes, and speech to gesture prediction mapped speech\nfeatures to facial and hand movements. Nonoverlapping speech consistently\nelicited greater activeness particularly in the lower face and mouth. Sadness\nshowed increased expressivity during nonoverlap, while anger suppressed\ngestures during overlaps. Predictive mapping revealed highest accuracy for\nprosody and MFCCs in articulatory regions while arousal and valence had lower\nand more context sensitive correlations. Notably, hand speech synchrony was\nenhanced under low arousal and overlapping speech, but not for valence.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10010v1", "AI": {"title_translation": "双人互动中通过语音到面部和身体姿态的多模态情感耦合", "tldr": "本研究探讨了双人互动中语音、面部和手部动作之间的情感耦合，发现语音特征能预测面部和手部运动，且情感和对话结构会影响多模态表达。研究结果有助于提升实时情感检测的准确性。", "motivation": "现有研究对语音和面部对齐已充分证实，但语音与面部区域和手部动作的更广泛动态关联，以及对话结构（如轮流讲话和同时讲话）如何调节这种协调性，对于深入理解真实互动中情感和行为线索的沟通至关重要。理解这些动态有助于提高实时情感检测的准确性。", "method": "本研究使用IEMOCAP语料库中双人互动的区域特定动作捕捉数据。语音特征包括低级韵律、MFCCs以及模型导出的唤醒度、效价和分类情感（高兴、悲伤、愤怒、中性），并与3D面部和手部标记位移对齐。通过帧间位移大小量化表达活跃度，并通过语音到手势预测将语音特征映射到面部和手部运动。", "result": "非重叠语音一致地引发了更高的活跃度，特别是在下脸部和嘴部。悲伤在非重叠时表现出更高的表达性，而愤怒在重叠时抑制了手势。预测映射显示，韵律和MFCCs在发音区域的准确性最高，而唤醒度和效价的相关性较低且更具上下文敏感性。手部-语音同步在低唤醒度和重叠语音下增强，但对效价则不然。", "conclusion": "理解语音、面部和手部动作之间的多模态情感耦合以及对话结构对其调节作用，对于提升真实互动和AI系统中实时情感检测的准确性和同步性至关重要。研究结果揭示了特定情感和对话情境下多模态表达的动态模式。", "translation": "人类情感表达通过协调的语音、面部和手势信号出现。虽然语音与面部的对齐已得到充分证实，但连接情感表达性语音与区域性面部和手部动作的更广泛动态，对于深入了解真实互动中情感和行为线索如何沟通至关重要。进一步调节这种协调性的是对话交流的结构，例如顺序轮流讲话，它为多模态同步创造了稳定的时间窗口；而同时讲话，通常预示着高唤醒时刻，则会破坏这种对齐并影响情感清晰度。理解这些动态通过提高人类互动和AI系统中跨模态的时间和同步准确性来增强实时情感检测。本研究使用IEMOCAP语料库中双人互动的区域特定动作捕捉来检查多模态情感耦合。语音特征包括低级韵律、MFCCs以及模型导出的唤醒度、效价和分类情感（高兴、悲伤、愤怒、中性），并与3D面部和手部标记位移对齐。通过帧间位移大小量化表达活跃度，并通过语音到手势预测将语音特征映射到面部和手部运动。非重叠语音一致地引发了更高的活跃度，特别是在下脸部和嘴部。悲伤在非重叠时表现出更高的表达性，而愤怒在重叠时抑制了手势。预测映射显示，韵律和MFCCs在发音区域的准确性最高，而唤醒度和效价的相关性较低且更具上下文敏感性。值得注意的是，手部-语音同步在低唤醒度和重叠语音下增强，但对效价则不然。", "summary": "本研究探讨了双人互动中语音、面部和手部动作之间的多模态情感耦合，特别关注对话结构（如轮流讲话和同时讲话）的影响。研究利用IEMOCAP语料库，通过分析语音特征（韵律、MFCCs、唤醒度、效价和分类情感）与面部和手部动作的关联。结果显示，非重叠语音会增加面部和嘴部活跃度，悲伤在非重叠时表达性增强，愤怒在重叠时抑制手势。语音特征对运动的预测能力各异，其中韵律和MFCCs在发音区域表现最佳。研究强调了理解这些动态对于提升实时情感检测的重要性。", "keywords": "多模态情感耦合, 语音, 面部姿态, 身体姿态, 双人互动", "comments": "这项研究通过细致分析双人互动中的多模态情感表达，特别是结合了对话结构的影响，为情感计算领域提供了深入见解。其创新之处在于不仅关注语音与面部，还扩展到手部动作，并区分了非重叠和重叠语音情境下的表达差异。研究结果对于开发更准确、更具情境感知的AI情感识别系统具有重要指导意义，但也可能面临跨文化或更复杂互动场景的泛化挑战。"}}
{"id": "2506.10699", "title": "SNR and Resource Adaptive Deep JSCC for Distributed IoT Image Classification", "authors": ["Ali Waqas", "Sinem Coleri"], "summary": "Sensor-based local inference at IoT devices faces severe computational\nlimitations, often requiring data transmission over noisy wireless channels for\nserver-side processing. To address this, split-network Deep Neural Network\n(DNN) based Joint Source-Channel Coding (JSCC) schemes are used to extract and\ntransmit relevant features instead of raw data. However, most existing methods\nrely on fixed network splits and static configurations, lacking adaptability to\nvarying computational budgets and channel conditions. In this paper, we propose\na novel SNR- and computation-adaptive distributed CNN framework for wireless\nimage classification across IoT devices and edge servers. We introduce a\nlearning-assisted intelligent Genetic Algorithm (LAIGA) that efficiently\nexplores the CNN hyperparameter space to optimize network configuration under\ngiven FLOPs constraints and given SNR. LAIGA intelligently discards the\ninfeasible network configurations that exceed computational budget at IoT\ndevice. It also benefits from the Random Forests based learning assistance to\navoid a thorough exploration of hyperparameter space and to induce application\nspecific bias in candidate optimal configurations. Experimental results\ndemonstrate that the proposed framework outperforms fixed-split architectures\nand existing SNR-adaptive methods, especially under low SNR and limited\ncomputational resources. We achieve a 10\\% increase in classification accuracy\nas compared to existing JSCC based SNR-adaptive multilayer framework at an SNR\nas low as -10dB across a range of available computational budget (1M to 70M\nFLOPs) at IoT device.", "comment": "6 pages, 5 figures, PIMRC Conference 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10699v1", "AI": {"title_translation": "用于分布式物联网图像分类的信噪比和资源自适应深度联合源信道编码", "tldr": "本文提出了一种用于分布式物联网图像分类的信噪比和计算自适应深度联合源信道编码（JSCC）框架。该框架利用一种学习辅助智能遗传算法（LAIGA）来优化网络配置，使其能够适应不同的信噪比和计算预算，并在低信噪比和资源受限环境下显著优于现有方法。", "motivation": "物联网设备面临严重的计算限制，通常需要通过嘈杂的无线信道传输数据进行服务器端处理。现有基于分体式深度神经网络（DNN）的联合源信道编码（JSCC）方案缺乏对不同计算预算和信道条件的适应性，因为它们依赖于固定的网络拆分和静态配置。", "method": "本文提出了一种新颖的信噪比和计算自适应分布式CNN框架，用于物联网设备和边缘服务器之间的无线图像分类。该框架引入了学习辅助智能遗传算法（LAIGA），该算法有效地探索CNN超参数空间，以在给定FLOPs约束和给定信噪比下优化网络配置。LAIGA能够智能地丢弃超出物联网设备计算预算的不可行网络配置，并受益于基于随机森林的学习辅助，以避免彻底探索超参数空间并引入特定于应用程序的偏差。", "result": "所提出的框架优于固定拆分架构和现有信噪比自适应方法，特别是在低信噪比和有限计算资源下。与现有基于JSCC的信噪比自适应多层框架相比，在信噪比低至-10dB时，在物联网设备上可用的计算预算范围（1M至70M FLOPs）内，分类精度提高了10%。", "conclusion": "本文证明了所提出的信噪比和计算自适应深度JSCC框架结合LAIGA，显著提高了分布式物联网系统中的图像分类精度，尤其是在具有挑战性的低信噪比和资源受限环境中。", "translation": "基于传感器的物联网设备本地推理面临严重的计算限制，通常需要通过嘈杂的无线信道传输数据以进行服务器端处理。为了解决这个问题，基于分体式深度神经网络（DNN）的联合源信道编码（JSCC）方案被用于提取和传输相关特征而不是原始数据。然而，大多数现有方法依赖于固定的网络拆分和静态配置，缺乏对不同计算预算和信道条件的适应性。在本文中，我们提出了一种新颖的信噪比和计算自适应分布式CNN框架，用于物联网设备和边缘服务器之间的无线图像分类。我们引入了一种学习辅助智能遗传算法（LAIGA），该算法有效地探索CNN超参数空间，以在给定FLOPs约束和给定信噪比下优化网络配置。LAIGA智能地丢弃超出物联网设备计算预算的不可行网络配置。它还受益于基于随机森林的学习辅助，以避免对超参数空间进行彻底探索，并在候选最佳配置中引入特定于应用程序的偏差。实验结果表明，所提出的框架优于固定拆分架构和现有信噪比自适应方法，特别是在低信噪比和有限计算资源下。与现有基于JSCC的信噪比自适应多层框架相比，在信噪比低至-10dB时，在物联网设备上可用的计算预算范围（1M至70M FLOPs）内，我们的分类精度提高了10%。", "summary": "本文提出了一种新颖的信噪比和计算自适应分布式CNN框架，用于物联网环境中的无线图像分类。该框架通过采用学习辅助智能遗传算法（LAIGA）解决了现有固定配置JSCC方法的局限性。LAIGA通过在FLOPs约束和不同信噪比下有效探索CNN超参数空间来优化网络配置，智能地丢弃不可行的选项，并利用随机森林进行指导。实验结果表明，该框架显著优于静态和现有信噪比自适应架构，在低信噪比和有限计算资源场景下实现了10%的精度提升。", "keywords": "物联网图像分类, JSCC, 信噪比自适应, 资源自适应, 深度学习, 遗传算法", "comments": "该论文的创新之处在于所提出的框架的自适应性，特别是使用LAIGA根据信噪比和计算预算动态调整网络配置。这解决了现有JSCC方法在物联网应用中一个关键的局限性，即资源和信道条件的高度可变性。将遗传算法与学习辅助（随机森林）相结合进行超参数优化是一种巧妙的方法，可以使自适应过程高效且实用。"}}
{"id": "2506.10094", "title": "Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders", "authors": ["Md. Faizul Islam Ansari"], "summary": "This research implements an advanced unsupervised clustering system for MNIST\nhandwritten digits through two-phase deep autoencoder architecture. A deep\nneural autoencoder requires a training process during phase one to develop\nminimal yet interpretive representations of images by minimizing reconstruction\nerrors. During the second phase we unify the reconstruction error with a KMeans\nclustering loss for learned latent embeddings through a joint distance-based\nobjective. Our model contains three elements which include batch normalization\ncombined with dropout and weight decay for achieving generalized and stable\nresults. The framework achieves superior clustering performance during\nextensive tests which used intrinsic measurements including Silhouette Score\nand Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when\nprocessing image features. The research uses t-SNE visualization to present\nlearned embeddings that show distinct clusters for digits. Our approach reaches\nan optimal combination between data reconstruction accuracy and cluster\nseparation purity when adding the benefit of understandable results and\nscalable implementations. The approach creates a dependable base that helps\ndeploy unsupervised representation learning in different large-scale image\nclustering applications.", "comment": "6 pages, 6 figures, experimental study on deep clustering with\n  autoencoders", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10094v1", "AI": {"title_translation": "基于三重增强卷积自编码器的MNIST手写数字无监督深度聚类", "tldr": "本研究提出一种两阶段深度自编码器架构，结合KMeans聚类损失，实现MNIST手写数字的无监督深度聚类，在多种指标上表现优异，并能应用于大规模图像聚类。", "motivation": "旨在为MNIST手写数字实现先进的无监督聚类系统，并开发通过自编码器学习可解释表示的无监督方法，以应用于大规模图像聚类。", "method": "本研究提出一种两阶段深度自编码器架构。第一阶段通过最小化重建误差来训练深度神经网络自编码器，以学习图像的紧凑且可解释的表示。第二阶段，通过一个基于联合距离的目标，将重建误差与学习到的潜在嵌入的KMeans聚类损失相结合。模型还整合了批量归一化、Dropout和权重衰减以提高泛化性和稳定性。", "result": "该框架在广泛测试中实现了卓越的聚类性能，使用了包括Silhouette Score、Davies-Bouldin Index、NMI和ARI在内的内在和外在指标。t-SNE可视化展示了学习到的嵌入形成了清晰的数字簇。该方法在数据重建精度和簇分离纯度之间达到了最佳组合。", "conclusion": "该方法提供了一个可靠的基础，有助于将无监督表示学习部署到不同的大规模图像聚类应用中，并具备可理解的结果和可扩展的实现。", "translation": "本研究通过两阶段深度自编码器架构，为MNIST手写数字实现了一个先进的无监督聚类系统。一个深度神经网络自编码器在第一阶段需要训练过程，通过最小化重建误差来开发图像的最小化但可解释的表示。在第二阶段，我们通过一个基于联合距离的目标，将重建误差与学习到的潜在嵌入的KMeans聚类损失相结合。我们的模型包含三个元素，包括批量归一化、Dropout和权重衰减，以实现泛化和稳定的结果。该框架在广泛测试中取得了卓越的聚类性能，这些测试在使用内在测量（包括Silhouette Score和Davies-Bouldin Index）以及外在度量（NMI和ARI）处理图像特征时进行。研究使用t-SNE可视化来呈现学习的嵌入，这些嵌入显示了数字的独特簇。我们的方法在数据重建精度和簇分离纯度之间达到了最佳组合，同时增加了可理解的结果和可扩展实现的优势。该方法创建了一个可靠的基础，有助于在不同的大规模图像聚类应用中部署无监督表示学习。", "summary": "本文提出一种用于MNIST手写数字的无监督深度聚类系统，采用两阶段深度自编码器架构。第一阶段通过最小化重建误差学习图像表示，第二阶段结合重建误差和KMeans聚类损失优化潜在嵌入。模型集成批量归一化、Dropout和权重衰减以增强性能。实验结果表明，该方法在多种聚类指标上表现优异，并能生成清晰可分离的数字簇，实现了数据重建精度与聚类纯度的最佳平衡，为大规模图像聚类提供了可扩展的解决方案。", "keywords": "无监督聚类, 深度自编码器, MNIST, KMeans, 表示学习", "comments": "这篇论文的创新点在于其两阶段的深度自编码器聚类架构，特别是将重建误差与KMeans聚类损失结合的联合优化目标。通过引入批量归一化、Dropout和权重衰减，有效提升了模型的泛化能力和稳定性。其在MNIST数据集上的优异表现以及对可扩展性的强调，表明了该方法在实际大规模无监督图像聚类应用中的潜力。"}}
{"id": "2506.10150", "title": "When Large Language Models are Reliable for Judging Empathic Communication", "authors": ["Aakriti Kumar", "Nalin Poungpeth", "Diyi Yang", "Erina Farrell", "Bruce Lambert", "Matthew Groh"], "summary": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10150v1", "AI": {"title_translation": "大型语言模型在判断共情交流中的可靠性", "tldr": "研究发现，大型语言模型在判断共情交流方面表现出接近专家水平的可靠性，且优于众包工作者，表明LLM在情感敏感应用中的潜力。", "motivation": "大型语言模型（LLMs）在生成共情回复方面表现出色，但其在判断共情交流细微之处的可靠性尚不明确。本研究旨在探究LLMs在此方面的可靠性。", "method": "研究比较了专家、众包工作者和LLMs在四种源自心理学、自然语言处理和通信领域的评估框架下，对200个真实世界对话（一方分享个人问题，另一方提供支持）中共情交流的标注情况。通过3,150份专家标注、2,844份众包标注和3,150份LLM标注，评估了这三组标注者之间的评估者间可靠性。", "result": "专家之间的一致性很高，但在不同框架的子组件上有所差异，这取决于其清晰度、复杂性和主观性。研究表明，专家一致性为LLM性能提供了一个比标准分类指标更具信息量的基准。在所有四个框架中，LLMs始终接近专家水平的基准，并超过了众包工作者的可靠性。", "conclusion": "当通过特定任务和适当基准进行验证时，大型语言模型可以支持情感敏感应用（包括作为对话伙伴）的透明度和监督。", "translation": "大型语言模型（LLMs）在基于文本的对话中擅长生成共情回复。但是，它们在判断共情交流的细微之处方面有多可靠呢？我们通过比较专家、众包工作者和LLMs如何根据来自心理学、自然语言处理和通信的四种评估框架，对200个真实世界对话进行共情交流标注来调查这个问题，这些对话中一位说话者分享个人问题，另一位提供支持。我们利用3,150份专家标注、2,844份众包标注和3,150份LLM标注，评估了这三组标注者之间的评估者间可靠性。我们发现专家之间的一致性很高，但根据框架子组件的清晰度、复杂性和主观性而异。我们表明，专家一致性为LLM性能提供了一个比标准分类指标更具信息量的基准。在所有四个框架中，LLMs始终接近这个专家水平的基准，并超过了众包工作者的可靠性。这些结果表明，当通过特定任务和适当基准进行验证时，LLMs如何能够支持情感敏感应用（包括作为对话伙伴）的透明度和监督。", "summary": "本研究旨在评估大型语言模型（LLMs）在判断共情交流方面的可靠性。通过比较LLMs、专家和众包工作者在四种评估框架下对200个真实对话的标注，研究发现LLMs在共情判断上的表现持续接近专家水平，并且优于众包工作者。结果表明，在适当的基准验证下，LLMs在情感敏感应用中具有支持透明度和监督的潜力。", "keywords": "大型语言模型, 共情交流, 可靠性, 专家标注, 评估基准", "comments": "这篇论文的创新之处在于，它不仅验证了LLMs在共情生成方面的能力，更深入探讨了其在共情判断这一更复杂任务上的可靠性。通过引入专家一致性作为更具信息量的基准，而非仅仅依赖传统分类指标，论文为评估LLM在主观、复杂任务中的性能提供了一个更严谨的视角。其结果对于LLMs在心理健康支持、客户服务等情感敏感领域的应用具有重要指导意义，强调了正确验证和基准测试的重要性。"}}
{"id": "2506.10128", "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Chao Feng", "Yongyuan Liang", "Yuhang Zhou", "Xiaoyu Liu", "Ziyi Zang", "Ming Li", "Chung-Ching Lin", "Kevin Lin", "Linjie Li", "Furong Huang", "Lijuan Wang"], "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10128v1", "AI": {"title_translation": "ViCrit：一种用于VLM中视觉感知的可验证强化学习代理任务", "tldr": "引入ViCrit，一个强化学习代理任务，通过定位视觉幻觉来提高VLM的视觉感知能力，并在多个基准测试中取得显著进步。", "motivation": "强化学习在LLMs的微调中表现出色，但在VLMs的视觉感知领域，缺乏同时具有挑战性和可明确验证的视觉中心任务，阻碍了其成功扩展。", "method": "提出ViCrit（Visual Caption Hallucination Critic）作为强化学习代理任务。该任务通过在人工编写的图像描述中注入细微的合成视觉幻觉（修改少数词语，涉及物体、属性、数量或空间关系），训练VLM定位被破坏的文本片段。这种方法在保持感知难度的同时，提供了一个易于计算且明确的二元精确匹配奖励。此外，还引入了ViCrit-Bench，一个诊断基准测试。", "result": "经过ViCrit任务训练的模型在各种视觉语言基准测试中取得了显著提升。这些改进不仅限于自然图像训练数据，还扩展到抽象图像推理和视觉数学，表明模型学会了感知而非仅仅记忆。", "conclusion": "精细的幻觉批判是增强VLM视觉感知的一种有效且可泛化的目标。", "translation": "强化学习（RL）在微调大型语言模型（LLM）方面表现出巨大效力，尤其是在数学推理或代码生成等具有挑战性但易于验证的任务上。然而，将这种成功扩展到视觉语言模型（VLM）中的视觉感知一直受到阻碍，因为同时具有挑战性和明确可验证的以视觉为中心的任务稀缺。为此，我们引入了ViCrit（Visual Caption Hallucination Critic），一个强化学习代理任务，它训练VLM定位注入到人工编写的图像描述段落中的细微合成视觉幻觉。我们从约200字的描述开始，注入一个单一、细微的视觉描述错误——改变关于物体、属性、数量或空间关系的几个词语——并要求模型在给定图像和修改后的描述的情况下，精确定位被破坏的文本片段。这种公式保留了完整的感知难度，同时提供了一个易于计算且明确的二元精确匹配奖励。使用ViCrit任务训练的模型在各种视觉语言基准测试中表现出显著的提升。至关重要的是，这些改进超越了自然图像训练数据，扩展到抽象图像推理和视觉数学，显示出学习感知而非仅仅记忆所见物体的潜力。为了方便评估，我们进一步引入了ViCrit-Bench，一个类别平衡的诊断基准，系统地探测不同图像领域和错误类型中的感知错误。总而言之，我们的结果表明，细粒度的幻觉批判是增强VLM视觉感知的一种有效且可泛化的目标。", "summary": "本文提出ViCrit，一个针对视觉语言模型（VLM）视觉感知的可验证强化学习代理任务。该任务通过训练VLM识别图像描述中注入的细微视觉幻觉来提升其感知能力。ViCrit提供了易于验证的二元奖励，同时保持了复杂的感知挑战。实验结果表明，经过ViCrit训练的模型在多个VL基准测试中表现出显著提升，且泛化能力强，能够应用于抽象推理和视觉数学，而非仅仅记忆。论文还引入了ViCrit-Bench用于评估。", "keywords": "视觉语言模型, 强化学习, 视觉感知, 幻觉批判, ViCrit", "comments": "ViCrit的创新之处在于它将强化学习应用于视觉感知，通过构建一个巧妙的代理任务解决了视觉领域缺乏可验证挑战性任务的难题。其通过注入细微幻觉并要求模型定位错误的方式，不仅保持了感知难度，还提供了清晰的二元奖励，这对于RL训练至关重要。研究结果显示其泛化能力强，对VLM的视觉理解能力提升具有重要意义，超越了简单的记忆。"}}
{"id": "2506.10447", "title": "Stability analysis of the free-surface Stokes problem and an unconditionally stable explicit scheme", "authors": ["Igor Tominec", "Lukas Lundgren", "André Löfgren", "Josefin Ahlkrona"], "summary": "Accurate simulations of ice sheet dynamics, mantle convection, lava flow, and\nother highly viscous free-surface flows involve solving the coupled\nStokes/free-surface equations. In this paper, we theoretically analyze the\nstability and conservation properties of the weak form of this system for\nNewtonian fluids and non-Newtonian fluids, at both the continuous and discrete\nlevels. We perform the fully discrete stability analysis for finite element\nmethods used in space with explicit and implicit Euler time-stepping methods\nused in time. Motivated by the theory, we propose a stabilization term designed\nfor the explicit Euler discretization, which ensures unconditional time\nstability and permits conservation of the domain volume. Numerical experiments\nvalidate and support our theoretical findings.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10447v1", "AI": {"title_translation": "自由表面斯托克斯问题的稳定性分析和无条件稳定的显式格式", "tldr": "本文理论分析了自由表面斯托克斯问题的稳定性，并提出了一种无条件稳定的显式时间步进方案。", "motivation": "准确模拟冰盖动力学、地幔对流、熔岩流等高粘度自由表面流体需要解决耦合的斯托克斯/自由表面方程，因此需要对其稳定性进行分析并开发稳定的数值方法。", "method": "理论分析了牛顿流体和非牛顿流体在连续和离散层面上的弱形式的稳定性和守恒特性。对空间使用有限元方法、时间使用显式和隐式欧拉时间步进方法的完全离散稳定性进行了分析。提出了一种为显式欧拉离散设计的稳定项。", "result": "提出的稳定项确保了无条件时间稳定性和域体积的守恒，并通过数值实验验证了理论发现。", "conclusion": "通过理论分析和数值验证，成功开发了一种适用于自由表面斯托克斯问题的无条件稳定显式方案，能够保持域体积守恒。", "translation": "冰盖动力学、地幔对流、熔岩流以及其他高粘度自由表面流体的精确模拟涉及求解耦合的斯托克斯/自由表面方程。在本文中，我们理论分析了牛顿流体和非牛顿流体在该系统弱形式的稳定性和守恒特性，包括连续和离散层面。我们对空间使用有限元方法、时间使用显式和隐式欧拉时间步进方法的完全离散稳定性进行了分析。受理论启发，我们提出了一种为显式欧拉离散设计的稳定项，该稳定项确保了无条件时间稳定性和域体积的守恒。数值实验验证并支持了我们的理论发现。", "summary": "本文对自由表面斯托克斯问题在牛顿和非牛顿流体下的弱形式的稳定性与守恒性进行了理论分析，涵盖连续和离散层面。针对有限元空间离散和欧拉时间步进方法，作者提出了一种新的稳定项，以确保显式欧拉离散的无条件时间稳定性和体积守恒。数值实验验证了该理论发现。", "keywords": "自由表面流, 斯托克斯问题, 稳定性分析, 显式格式, 有限元方法", "comments": "这项工作在数值模拟高粘度自由表面流体方面具有重要意义，尤其是在开发无条件稳定显式方案方面。它解决了传统显式方法可能存在的稳定性限制，并通过引入稳定项实现了体积守恒，这对于实际应用中的物理准确性至关重要。"}}
{"id": "2506.10125", "title": "D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning", "authors": ["Muqi Zou", "Hongyu Cai", "Hongwei Wu", "Zion Leonahenahe Basque", "Arslan Khan", "Berkay Celik", "Dave", "Tian", "Antonio Bianchi", "Ruoyu", "Wang", "Dongyan Xu"], "summary": "Decompilers, which reconstruct human-readable source code from binary\nexecutables, are vital to many security tasks. Yet, despite recent advances,\ntheir output often suffers from syntactic and semantic errors and remains\ndifficult to read. Recently, with the advent of large language models (LLMs),\nresearchers began to explore the potential of LLMs to refine decompiler output.\nNevertheless, our study of these approaches reveals significant limitations,\nsuch as introducing new errors and relying on unreliable accuracy validation.\nIn this paper, we present D-LiFT, an automated decompiler backend that\nharnesses and further trains LLMs to improve the quality of decompiled code via\nreinforcement learning (RL). Unlike prior work that overlooks preserving\naccuracy, D-LiFT adheres to a key principle for enhancing the quality of\ndecompiled code: \\textit{preserving accuracy while improving readability}.\nCentral to D-LiFT, we propose D-SCORE, an integrated quality assessment system\nto score the decompiled code from multiple aspects. In line with our principle,\nD-SCORE assigns low scores to any inaccurate output and only awards higher\nscores for readability to code that passes the accuracy check. Specifically,\nD-SCORE first verifies the syntactic and semantic correctness via the compiler\nand symbolic execution; only if a candidate is deemed accurate, it then\nevaluates readability using established metrics to compare the LLM output with\nthe original decompiled code. The score will then be fed back to the LLM for\nfine-tuning. Our implementation, based on Ghidra and a range of LLMs,\ndemonstrates significant improvements for the accurate decompiled code from the\ncoreutils and util-linux projects. Compared to baseline LLMs without\nD-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled\nfunctions, as measured by D-SCORE.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10125v1", "AI": {"title_translation": "D-LiFT：通过代码质量驱动的微调改进基于LLM的反编译器后端", "tldr": "D-LiFT通过新颖的、保持准确性的代码质量评估系统（D-SCORE）对LLM进行微调，从而提高了基于LLM的反编译器的输出质量。", "motivation": "反编译器输出存在语法和语义错误且难以阅读。虽然最近基于大型语言模型（LLM）的方法试图改进反编译输出，但它们引入新错误且依赖不可靠的准确性验证。", "method": "D-LiFT是一个自动化的反编译器后端，通过强化学习进一步训练大型语言模型（LLM）以提高反编译代码质量。它提出了D-SCORE，一个集成的质量评估系统，该系统首先通过编译器和符号执行验证语法和语义正确性，然后评估可读性。D-SCORE将低分分配给不准确的输出，并仅在通过准确性检查后才为可读性授予高分。分数随后反馈给LLM进行微调。", "result": "D-LiFT在coreutils和util-linux项目中对准确的反编译代码显示出显著改进。与没有D-SCORE驱动微调的基线LLM相比，D-LiFT产生的改进反编译函数增加了55.3%（通过D-SCORE衡量）。", "conclusion": "D-LiFT通过优先确保准确性同时提高可读性的代码质量驱动微调方法，有效改进了基于LLM的反编译器后端，并通过D-SCORE得到了验证。", "translation": "反编译器，将二进制可执行文件重构为人类可读的源代码，对许多安全任务至关重要。然而，尽管最近取得了进展，其输出仍常遭受语法和语义错误，并且难以阅读。最近，随着大型语言模型（LLM）的出现，研究人员开始探索LLM改进反编译器输出的潜力。然而，我们对这些方法的研究揭示了显著的局限性，例如引入新错误和依赖不可靠的准确性验证。在本文中，我们提出了D-LiFT，一个自动化的反编译器后端，它利用并进一步训练LLM，通过强化学习（RL）提高反编译代码的质量。与先前忽视保持准确性的工作不同，D-LiFT遵循一个提高反编译代码质量的关键原则：在提高可读性的同时保持准确性。D-LiFT的核心是我们提出了D-SCORE，一个集成的质量评估系统，从多个方面对反编译代码进行评分。根据我们的原则，D-SCORE对任何不准确的输出分配低分，并且只对通过准确性检查的代码授予更高的可读性分数。具体而言，D-SCORE首先通过编译器和符号执行验证语法和语义正确性；只有当候选代码被认为是准确的，它才会使用既定指标评估可读性，将LLM输出与原始反编译代码进行比较。然后将分数反馈给LLM进行微调。我们基于Ghidra和一系列LLM的实现表明，coreutils和util-linux项目中准确的反编译代码质量得到了显著改进。与没有D-SCORE驱动微调的基线LLM相比，D-LiFT产生的改进反编译函数增加了55.3%，这是通过D-SCORE衡量的。", "summary": "D-LiFT是一个自动化的反编译器后端，旨在提高LLM生成的反编译代码的质量。它通过提出一个新颖的强化学习框架来解决现有基于LLM方法的局限性，该框架基于一个名为D-SCORE的多方面质量评估系统对LLM进行微调。D-SCORE通过优先验证语法和语义正确性，然后再评估可读性，从而确保准确性。实验结果表明，与基线LLM相比，D-LiFT显著增加了改进反编译函数的比例。", "keywords": "LLM, 反编译器, 代码质量, 微调, 强化学习", "comments": "创新点：D-LiFT引入了一种新颖的、保持准确性的质量评估系统（D-SCORE），用于反编译中LLM的微调，这对于准确性至关重要。使用自定义评分系统通过强化学习微调LLM的方法也具有创新性。重要性：反编译器对安全任务至关重要，提高其输出质量，特别是结合LLM，可以显著增强逆向工程和漏洞分析。解决准确性保持问题非常重要。局限性：摘要中没有明确提及除了它所解决的（先前工作的局限性）之外的局限性。它主要侧重于积极的成果。"}}
{"id": "2506.10365", "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Chang Liu", "Jianyuan Liang", "Yaxian Qing", "Xiaopu Zhang", "Dehua Peng", "Zhipeng Gui", "Xuefeng Guan"], "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10365v1", "AI": {"title_translation": "AutoGEEval++：一个面向Google Earth Engine地理空间代码生成中大型语言模型的多级别多地理空间模态自动化评估框架", "tldr": "AutoGEEval++是首个针对Google Earth Engine（GEE）地理空间代码生成LLMs的自动化评估框架，包含大规模基准数据集和多维度指标，用于评估并比较不同LLMs的性能。", "motivation": "地理空间代码生成是AI与地球科学分析结合的关键前沿，但目前缺乏标准化、自动化的评估工具。", "method": "本研究提出了AutoGEEval++，一个基于AutoGEEval的增强框架，也是第一个用于评估在Google Earth Engine (GEE) 上生成地理空间代码的大型语言模型 (LLM) 的自动化评估系统。它支持多样的数据模态和不同任务复杂性，基于GEE Python API构建。AutoGEEval++包含一个基准数据集——AutoGEEval++-Bench，其中包含6,365个测试用例，涵盖26种数据类型和三个任务类别：单元测试、组合测试和主题测试。该框架还包括一个提交程序和一个判别模块，以实现从代码生成到基于执行的验证的端到端自动化评估流程。它采用多维度指标，包括准确性、资源使用、运行时效率和错误类型，平衡幻觉控制和效率，并支持边界测试和错误模式分析。", "result": "使用AutoGEEval++评估了24个最先进的LLMs（截至2025年6月），包括通用型、推理增强型、代码中心型和地球科学专用型模型。结果揭示了在任务类型、模型设计和部署设置之间存在明显的性能、稳定性和错误差异，证实了AutoGEEval++在垂直领域代码生成中的实用价值和可扩展性。", "conclusion": "这项工作建立了第一个针对基于GEE的LLM代码生成的标准化评估协议和基础基准，为性能比较提供了一个统一的基础，并为系统性、领域特定代码评估提供了方法论框架。", "translation": "地理空间代码生成正成为人工智能与地球科学分析相结合的关键前沿，但目前仍缺乏针对此任务的标准化自动化评估工具。本研究提出了AutoGEEval++，一个在AutoGEEval基础上增强的框架，也是第一个用于评估在Google Earth Engine (GEE) 上生成地理空间代码的大型语言模型 (LLM) 的自动化评估系统。它支持多样的数据模态和不同任务复杂性。AutoGEEval++基于GEE Python API构建，其特点是包含一个基准数据集——AutoGEEval++-Bench，其中包含6,365个测试用例，涵盖26种数据类型和三个任务类别：单元测试、组合测试和主题测试。它包括一个提交程序和一个判别模块，以实现从代码生成到基于执行的验证的端到端自动化评估流程。该框架采用多维度指标——准确性、资源使用、运行时效率和错误类型——平衡幻觉控制和效率，并支持边界测试和错误模式分析。通过使用AutoGEEval++，我们评估了24个最先进的LLM（截至2025年6月），包括通用型、推理增强型、代码中心型和地球科学专用型模型。结果揭示了在任务类型、模型设计和部署设置之间存在明显的性能、稳定性和错误差异，证实了AutoGEEval++在垂直领域代码生成中的实用价值和可扩展性。这项工作建立了第一个针对基于GEE的LLM代码生成的标准化评估协议和基础基准，为性能比较提供了一个统一的基础，并为系统性、领域特定代码评估提供了方法论框架。", "summary": "本研究介绍了AutoGEEval++，一个增强的自动化评估框架，旨在解决Google Earth Engine (GEE) 上大型语言模型 (LLM) 地理空间代码生成缺乏标准化评估工具的问题。该框架基于GEE Python API构建，包含一个大规模基准数据集AutoGEEval++-Bench，涵盖多级别和多模态测试用例。它提供了一个端到端的评估流程，并采用多维度指标来衡量LLMs的准确性、效率和错误类型。通过评估24个SOTA LLMs，AutoGEEval++证实了其在垂直领域代码生成评估中的实用性和可扩展性，并建立了首个GEE LLM代码生成的标准化评估协议和基准。", "keywords": "地理空间代码生成, 大型语言模型, Google Earth Engine, 自动化评估, AutoGEEval++", "comments": "AutoGEEval++的创新之处在于它是首个针对Google Earth Engine上LLM地理空间代码生成的自动化评估框架，填补了该领域标准化评估工具的空白。其多级别、多模态的基准数据集和多维度评估指标设计，使其能够全面、深入地分析LLMs的性能和错误模式。这项工作为地理空间AI领域提供了一个基础性的评估工具和统一的比较平台，对于推动LLM在专业领域的应用具有重要意义。"}}
{"id": "2506.10287", "title": "Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks", "authors": ["Rohit Sonker", "Alexandre Capone", "Andrew Rothstein", "Hiro Josep Farre Kaga", "Egemen Kolemen", "Jeff Schneider"], "summary": "Machine learning algorithms often struggle to control complex real-world\nsystems. In the case of nuclear fusion, these challenges are exacerbated, as\nthe dynamics are notoriously complex, data is poor, hardware is subject to\nfailures, and experiments often affect dynamics beyond the experiment's\nduration. Existing tools like reinforcement learning, supervised learning, and\nBayesian optimization address some of these challenges but fail to provide a\ncomprehensive solution. To overcome these limitations, we present a multi-scale\nBayesian optimization approach that integrates a high-frequency data-driven\ndynamics model with a low-frequency Gaussian process. By updating the Gaussian\nprocess between experiments, the method rapidly adapts to new data, refining\nthe predictions of the less reliable dynamical model. We validate our approach\nby controlling tearing instabilities in the DIII-D nuclear fusion plant.\nOffline testing on historical data shows that our method significantly\noutperforms several baselines. Results on live experiments on the DIII-D\ntokamak, conducted under high-performance plasma scenarios prone to\ninstabilities, shows a 50% success rate, marking a 117% improvement over\nhistorical outcomes.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10287v1", "AI": {"title_translation": "多时间尺度动力学模型贝叶斯优化在托卡马克等离子体稳态控制中的应用", "tldr": "本文提出了一种多时间尺度贝叶斯优化方法，结合高频数据驱动模型和低频高斯过程，以有效控制托卡马克等离子体中的撕裂不稳定性，并在DIII-D装置上取得了显著成功。", "motivation": "机器学习算法在控制复杂现实世界系统（特别是核聚变）时面临挑战，包括动力学复杂、数据质量差、硬件故障以及实验对后续动态的影响。现有方法无法提供全面的解决方案。", "method": "提出了一种多尺度贝叶斯优化方法，该方法将高频数据驱动动力学模型与低频高斯过程相结合。通过在实验之间更新高斯过程，该方法能够快速适应新数据，并改进不太可靠的动力学模型的预测。", "result": "离线测试表明，该方法显著优于多个基线。在DIII-D托卡马克装置上进行的活体实验中，在高性能、易不稳定等离子体场景下，成功率达到50%，比历史结果提高了117%。", "conclusion": "本文提出的多时间尺度贝叶斯优化方法能够有效控制托卡马克等离子体中的撕裂不稳定性，并在实际核聚变装置中展现出显著的性能提升。", "translation": "机器学习算法在控制复杂的现实世界系统时常常遇到困难。在核聚变领域，这些挑战更加严峻，因为其动力学异常复杂，数据质量不佳，硬件容易发生故障，且实验往往会影响超出实验持续时间的动力学。现有的工具，如强化学习、监督学习和贝叶斯优化，虽然解决了其中一些挑战，但未能提供一个全面的解决方案。为了克服这些局限性，我们提出了一种多尺度贝叶斯优化方法，该方法将高频数据驱动动力学模型与低频高斯过程相结合。通过在实验之间更新高斯过程，该方法能够快速适应新数据，从而改进对不太可靠的动力学模型的预测。我们通过控制DIII-D核聚变装置中的撕裂不稳定性来验证了我们的方法。对历史数据的离线测试表明，我们的方法显著优于多个基线。在DIII-D托卡马克装置上、在易发生不稳定性的高性能等离子体场景下进行的活体实验结果显示，成功率达到50%，比历史结果提高了117%。", "summary": "本文针对机器学习在核聚变等复杂系统控制中面临的挑战，提出了一种新颖的多尺度贝叶斯优化方法。该方法结合了高频数据驱动动力学模型与低频高斯过程，并通过实验间更新高斯过程以快速适应新数据并优化模型预测。在DIII-D核聚变装置上对撕裂不稳定性的控制验证显示，该方法在离线测试中显著优于基线，并在活体实验中取得了50%的成功率，相比历史结果提升了117%。", "keywords": "贝叶斯优化, 等离子体稳态控制, 多时间尺度, 核聚变, 托卡马克", "comments": "该论文的创新点在于提出了一个结合不同时间尺度模型的贝叶斯优化框架，有效解决了核聚变等复杂系统中数据稀疏、动力学复杂以及模型不确定性等问题。将高频数据驱动模型与低频高斯过程相结合，并通过迭代更新来优化预测，是其方法的核心优势。这一方法在实际核聚变装置DIII-D上的成功验证，展示了其在实际工程应用中的巨大潜力，对于实现可控核聚变具有重要意义。"}}
{"id": "2506.10762", "title": "Integrating Large Language Models into Text Animation: An Intelligent Editing System with Inline and Chat Interaction", "authors": ["Bao Zhang", "Zihan Li", "Zhenglei Liu", "Huanchen Wang", "Yuxin Ma"], "summary": "Text animation, a foundational element in video creation, enables efficient\nand cost-effective communication, thriving in advertisements, journalism, and\nsocial media. However, traditional animation workflows present significant\nusability barriers for non-professionals, with intricate operational procedures\nseverely hindering creative productivity. To address this, we propose a Large\nLanguage Model (LLM)-aided text animation editing system that enables real-time\nintent tracking and flexible editing. The system introduces an agent-based\ndual-stream pipeline that integrates context-aware inline suggestions and\nconversational guidance as well as employs a semantic-animation mapping to\nfacilitate LLM-driven creative intent translation. Besides, the system supports\nsynchronized text-animation previews and parametric adjustments via unified\ncontrols to improve editing workflow. A user study evaluates the system,\nhighlighting its ability to help non-professional users complete animation\nworkflows while validating the pipeline. The findings encourage further\nexploration of integrating LLMs into a comprehensive video creation workflow.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10762v1", "AI": {"title_translation": "将大型语言模型集成到文本动画中：一个具有内联和聊天交互的智能编辑系统", "tldr": "本文提出了一种由LLM辅助的智能文本动画编辑系统，通过双流交互（内联建议和聊天指导）和语义-动画映射，帮助非专业人士更轻松地创建动画。", "motivation": "传统的文本动画工作流程对非专业人士来说存在显著的可用性障碍，操作复杂，严重阻碍了创作生产力。", "method": "本文提出了一种由大型语言模型（LLM）辅助的文本动画编辑系统。该系统引入了一个基于代理的双流管道，集成了上下文感知的内联建议和对话指导，并采用语义-动画映射来促进LLM驱动的创意意图翻译。此外，系统支持通过统一控件进行同步的文本-动画预览和参数调整。", "result": "一项用户研究评估了该系统，结果表明它能够帮助非专业用户完成动画工作流程，并验证了其管道的有效性。", "conclusion": "研究结果鼓励进一步探索将大型语言模型集成到全面的视频创作工作流程中。", "translation": "文本动画作为视频创作的基础元素，在广告、新闻和社交媒体中蓬勃发展，能够实现高效且经济的沟通。然而，传统的动画工作流程对非专业人士来说存在显著的可用性障碍，复杂的S操作程序严重阻碍了创作生产力。为了解决这个问题，我们提出了一种由大型语言模型（LLM）辅助的文本动画编辑系统，该系统能够实现实时意图跟踪和灵活编辑。该系统引入了一个基于代理的双流管道，集成了上下文感知的内联建议和对话指导，并采用语义-动画映射来促进LLM驱动的创意意图翻译。此外，该系统支持通过统一控件进行同步的文本-动画预览和参数调整，以改进编辑工作流程。一项用户研究评估了该系统，突出了其帮助非专业用户完成动画工作流程的能力，并验证了其管道。研究结果鼓励进一步探索将LLM集成到全面的视频创作工作流程中。", "summary": "本文介绍了一种由大型语言模型（LLM）驱动的智能文本动画编辑系统，旨在解决传统动画工具对非专业用户造成的可用性挑战。该系统采用基于代理的双流管道，提供内联建议和聊天指导，并通过语义-动画映射实现创意意图的转换，同时支持统一控制下的预览和参数调整。用户研究验证了其在帮助非专业用户完成动画工作流程方面的有效性，并预示了其在更广泛视频创作应用中的潜力。", "keywords": "大型语言模型, 文本动画, 智能编辑系统, 用户交互, 创意工作流程", "comments": "该论文解决了使文本动画更易于访问的实际问题。将LLM与双流交互和语义映射相结合，是简化复杂创意工作流程的创新方法。这项工作对于视频内容创作的普及具有重要意义。"}}
{"id": "2506.10570", "title": "6G Infrastructures for Edge AI: An Analytical Perspective", "authors": ["Kurt Horvath", "Shpresa Tuda", "Blerta Idrizi", "Stojan Kitanov", "Fisnik Doko", "Dragi Kimovski"], "summary": "The convergence of Artificial Intelligence (AI) and the Internet of Things\nhas accelerated the development of distributed, network-sensitive applications,\nnecessitating ultra-low latency, high throughput, and real-time processing\ncapabilities. While 5G networks represent a significant technological\nmilestone, their ability to support AI-driven edge applications remains\nconstrained by performance gaps observed in real-world deployments. This paper\naddresses these limitations and highlights critical advancements needed to\nrealize a robust and scalable 6G ecosystem optimized for AI applications.\nFurthermore, we conduct an empirical evaluation of 5G network infrastructure in\ncentral Europe, with latency measurements ranging from 61 ms to 110 ms across\ndifferent close geographical areas. These values exceed the requirements of\nlatency-critical AI applications by approximately 270%, revealing significant\nshortcomings in current deployments. Building on these findings, we propose a\nset of recommendations to bridge the gap between existing 5G performance and\nthe requirements of next-generation AI applications.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10570v1", "AI": {"title_translation": "面向边缘AI的6G基础设施：分析视角", "tldr": "当前5G网络在支持边缘AI应用方面存在延迟限制，本文通过实证评估揭示了这些不足，并提出了构建满足下一代AI需求的6G生态系统的建议。", "motivation": "由于当前5G网络在实际部署中存在性能差距，尤其是在延迟方面无法满足AI驱动的边缘应用需求，因此需要研究如何实现一个为AI应用优化的强大且可扩展的6G生态系统。", "method": "本文对中欧的5G网络基础设施进行了实证评估，测量了不同地理区域的延迟。在此基础上，提出了弥合现有5G性能与下一代AI应用要求之间差距的建议。", "result": "实证评估显示，中欧5G网络的延迟范围为61毫秒到110毫秒，这些值超出延迟敏感型AI应用要求约270%，揭示了当前部署的显著不足。", "conclusion": "当前的5G网络在支持边缘AI应用方面存在显著的性能（尤其是延迟）不足。为了满足下一代AI应用的需求，需要发展6G基础设施并采纳本文提出的建议。", "translation": "人工智能（AI）和物联网的融合加速了分布式、网络敏感型应用的发展，这些应用要求超低延迟、高吞吐量和实时处理能力。尽管5G网络是一个重要的技术里程碑，但其支持AI驱动边缘应用的能力仍受限于实际部署中观察到的性能差距。本文解决了这些局限性，并强调了实现为AI应用优化的强大且可扩展的6G生态系统所需的关键进展。此外，我们对中欧的5G网络基础设施进行了实证评估，在不同相邻地理区域的延迟测量范围为61毫秒至110毫秒。这些值超出延迟关键型AI应用要求约270%，揭示了当前部署的显著不足。基于这些发现，我们提出了一系列建议，以弥合现有5G性能与下一代AI应用要求之间的差距。", "summary": "本文探讨了5G网络在支持AI驱动的边缘应用时面临的性能限制，特别是在延迟方面。通过对中欧5G基础设施的实证评估，发现其延迟远超AI应用需求。鉴于此，文章强调了发展6G基础设施的必要性，并提出了一系列建议以弥合当前5G性能与未来AI应用需求之间的差距。", "keywords": "6G, 边缘AI, 5G性能, 延迟, 网络基础设施", "comments": "本文通过对5G网络在支持边缘AI应用方面的实际性能进行实证分析，明确指出了当前技术的局限性，并前瞻性地提出了6G作为解决方案。其创新之处在于结合了实证数据来支撑对未来网络需求（尤其是低延迟）的论证，为6G网络的设计和优化提供了具体的方向。对于关注边缘AI和未来通信网络发展的研究人员和工程师而言，具有重要的参考价值。"}}
{"id": "2506.10194", "title": "Guardians of the Regime: When and Why Autocrats Create Secret Police", "authors": ["Marius Mehrl", "Mila Pfander", "Theresa Winner", "Cornelius Fritz"], "summary": "Autocrats use secret police to stay in power, as these organizations deter\nand suppress opposition to their rule. Existing research shows that secret\npolice are very good at this but, surprisingly, also that they are not as\nubiquitous in autocracies as one may assume, existing in less than 50% of\nautocratic country-years. We thus explore under which conditions secret police\nemerge in dictatorships. For this purpose, we apply statistical variable\nselection techniques to identify which of several candidate variables extracted\nfrom the literature on state security forces and authoritarian survival hold\nexplanatory power. Our results highlight that secret police are more likely to\nemerge when rulers face specific, preempt-able threats, such as protests and\nanti-system mobilisation, but also when they have the material resources to\nestablish these organisations. This research contributes to our understanding\nof autocrats' institutional choices and authoritarian politics.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10194v1", "AI": {"title_translation": "政权的守护者：独裁者何时以及为何创建秘密警察", "tldr": "秘密警察并非在所有独裁政权中普遍存在。本研究探讨了秘密警察在独裁政权中出现的条件：当统治者面临可预防的特定威胁（如抗议和反体制动员）且拥有建立这些组织所需的物质资源时，秘密警察更可能出现。", "motivation": "现有研究表明秘密警察在维持独裁统治方面非常有效，但令人惊讶的是，它们在独裁政权中并非普遍存在（存在于不到50%的独裁国家-年份中）。因此，本研究旨在探究秘密警察在独裁政权中出现的条件。", "method": "本研究运用统计变量选择技术，从关于国家安全部队和威权生存的文献中提取了几个候选变量，以识别哪些变量具有解释力。", "result": "研究结果表明，当统治者面临特定的、可预防的威胁（如抗议和反体制动员），并且拥有建立这些组织所需的物质资源时，秘密警察更有可能出现。", "conclusion": "这项研究有助于我们理解独裁者的制度选择和威权政治。", "translation": "独裁者利用秘密警察来维持权力，因为这些组织能够威慑和镇压对其统治的反对。现有研究表明，秘密警察在这方面非常有效，但令人惊讶的是，它们在独裁政权中并非如人们想象的那样普遍，存在于不到50%的独裁国家-年份中。因此，我们探讨了秘密警察在独裁统治下出现的条件。为此，我们运用统计变量选择技术，从关于国家安全部队和威权生存的文献中提取了几个候选变量，以识别哪些变量具有解释力。我们的结果强调，当统治者面临特定的、可预防的威胁（如抗议和反体制动员），并且拥有建立这些组织所需的物质资源时，秘密警察更有可能出现。这项研究有助于我们理解独裁者的制度选择和威权政治。", "summary": "本研究旨在探究秘密警察在独裁政权中出现的条件，因为尽管它们在维护统治方面有效，但并非普遍存在。通过应用统计变量选择技术，研究发现秘密警察更有可能在独裁者面临特定、可预防的威胁（如抗议）且具备必要物质资源时建立。这项工作加深了对独裁者制度选择和威权政治的理解。", "keywords": "独裁者, 秘密警察, 威权主义, 制度选择, 威胁", "comments": "这篇论文的创新之处在于，它挑战了秘密警察在独裁政权中普遍存在的假设，并深入探讨了其出现的具体条件。通过识别威胁类型和物质资源作为关键因素，该研究为理解独裁者的制度选择提供了新的视角，对威权政治研究具有重要意义。"}}
{"id": "2506.10562", "title": "Joint System Modeling Approach for Fault Simulation of Start-er/Generator and Gas Generator in All-Electric APU", "authors": ["Haotian Mao", "Yingqing Guo"], "summary": "This paper presents a joint system modeling approach for fault simulation of\nall-electric auxiliary power unit (APU), integrating starter/generator\nturn-to-turn short circuit (TTSC) faults with gas generator gas-path faults.To\naddress challenges in electromechanical coupling, simulation precision and\ncomputational efficiency balance, we propose a multi-rate continuous-discrete\nhybrid simulation architecture. This architecture treats the starter/generator\nas a continuous system with variable step size in Simulink, while modeling the\ngas generator as a discrete system with fixed step size in a dynamic-link\nlibrary (DLL) environment. For the starter/generator fault modeling, a\nmulti-loop approach is deployed to accurately simulate TTSC faults. For the gas\ngenerator, we develop an improved GasTurb-DLL modeling method (IGDM) that\nenhances uncertainty modeling, state-space representation, and tool chain\ncompatibility. Finally, the proposed methodology above was implemented in a\ncase study based on the APS5000 all-electric APU structure and parameters.\nModel validation was conducted by comparing simulation results--covering\nsteady-state, transients, healthy, and fault conditions--with reference data\nfrom third-party software and literature. The close agreement confirms both the\nmodel's accuracy and the effectiveness of our modeling methodology. This work\nestablishes a modeling foundation for investigating the opportunities and\nchallenges in fault detection and isolation (FDI) brought by the all\nelectrification of the APU, including joint fault estimation and diagnosis,\ncoupled electromechanical fault characteristics.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10562v1", "AI": {"title_translation": "全电APU启动机/发电机与燃气发生器故障模拟的联合系统建模方法", "tldr": "本文提出一种联合系统建模方法，用于全电APU启动机/发电机和燃气发生器故障模拟，解决了机电耦合、仿真精度与计算效率平衡等挑战。", "motivation": "解决全电APU故障模拟中机电耦合、仿真精度和计算效率平衡的挑战。", "method": "提出多速率连续-离散混合仿真架构：启动机/发电机作为Simulink中变步长连续系统，燃气发生器作为DLL环境中定步长离散系统。对启动机/发电机故障建模采用多环路方法模拟匝间短路故障。对燃气发生器开发改进型GasTurb-DLL建模方法（IGDM），增强不确定性建模、状态空间表示和工具链兼容性。", "result": "该方法在APS5000全电APU案例研究中实施。通过将稳态、瞬态、健康和故障条件下的仿真结果与第三方软件和文献参考数据进行比较，验证了模型。结果显示高度一致性，证实了模型的准确性和建模方法的有效性。", "conclusion": "这项工作为研究全电APU故障检测与隔离（FDI）带来的机遇和挑战（包括联合故障估计和诊断、耦合机电故障特性）奠定了建模基础。", "translation": "本文提出了一种用于全电辅助动力装置（APU）故障模拟的联合系统建模方法，该方法将启动机/发电机匝间短路（TTSC）故障与燃气发生器气路故障相结合。为解决机电耦合、仿真精度和计算效率平衡的挑战，我们提出了一种多速率连续-离散混合仿真架构。该架构将启动机/发电机视为Simulink中具有可变步长的连续系统，而将燃气发生器建模为动态链接库（DLL）环境中的固定步长离散系统。对于启动机/发电机故障建模，采用多环路方法精确模拟匝间短路故障。对于燃气发生器，我们开发了一种改进的GasTurb-DLL建模方法（IGDM），该方法增强了不确定性建模、状态空间表示和工具链兼容性。最后，上述提出的方法在基于APS5000全电APU结构和参数的案例研究中得以实施。通过将仿真结果（涵盖稳态、瞬态、健康和故障条件）与第三方软件和文献的参考数据进行比较，进行了模型验证。高度一致性证实了模型的准确性和我们建模方法的有效性。这项工作为研究全电APU全电气化带来的故障检测与隔离（FDI）的机遇和挑战（包括联合故障估计和诊断、耦合机电故障特性）奠定了建模基础。", "summary": "本文提出一种创新的联合系统建模方法，用于全电APU的启动机/发电机与燃气发生器故障模拟。为克服机电耦合、仿真精度与效率平衡的挑战，作者设计了一种多速率连续-离散混合仿真架构，并分别针对启动机/发电机和燃气发生器开发了特定的故障建模技术（多环路方法和IGDM）。通过与参考数据的对比验证，证实了模型的高准确性和方法的有效性，为未来的故障检测与隔离研究奠定了基础。", "keywords": "全电APU, 故障模拟, 联合系统建模, 多速率混合仿真, 匝间短路故障", "comments": "该论文的创新之处在于提出了一个多速率连续-离散混合仿真架构，有效地整合了机电耦合系统（启动机/发电机和燃气发生器）的故障模拟，并解决了精度与效率的平衡问题。其改进的GasTurb-DLL建模方法和多环路故障建模也提升了仿真准确性。这项工作为全电APU的故障诊断与隔离研究提供了重要的建模工具和基础。"}}
{"id": "2506.10357", "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Weili Guan", "Dongmei Jiang", "Liqiang Nie"], "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/", "comment": "24 pages, 10 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10357v1", "AI": {"title_translation": "Optimus-3：迈向具有可扩展任务专家的通用多模态Minecraft智能体", "tldr": "Optimus-3通过知识增强数据生成、专家混合架构和多模态推理增强强化学习，解决了在Minecraft中构建通用多模态智能体的挑战，并超越了现有模型。", "motivation": "在Minecraft等开放世界环境中构建具有感知、规划、行动、接地和反思能力的通用智能体面临挑战，包括领域特定数据不足、异构任务之间的干扰以及开放世界设置中的视觉多样性。", "method": "1) 提出了一种知识增强的数据生成管道，以提供可扩展和高质量的训练数据。2) 引入了一种具有任务级路由的专家混合（MoE）架构，以减轻异构任务之间的干扰。3) 开发了一种多模态推理增强强化学习方法，以增强智能体在Minecraft中对视觉多样性的推理能力。", "result": "Optimus-3在Minecraft环境中的广泛任务中超越了通用多模态大型语言模型和现有的最先进智能体。", "conclusion": "Optimus-3是一个基于创新技术构建的通用Minecraft智能体，在多项任务中表现优越，解决了开放世界环境中通用智能体开发的关键挑战。", "translation": "最近，基于多模态大型语言模型（MLLMs）的智能体在各个领域取得了显著进展。然而，在Minecraft等开放世界环境中构建具有感知、规划、行动、接地和反思等能力的通用智能体仍然面临挑战：领域特定数据不足、异构任务之间的干扰以及开放世界设置中的视觉多样性。在本文中，我们通过三项关键贡献解决了这些挑战。1) 我们提出了一种知识增强的数据生成管道，为智能体开发提供可扩展和高质量的训练数据。2) 为了减轻异构任务之间的干扰，我们引入了一种具有任务级路由的专家混合（MoE）架构。3) 我们开发了一种多模态推理增强强化学习方法，以增强智能体在Minecraft中对视觉多样性的推理能力。基于这些创新，我们提出了Optimus-3，一个用于Minecraft的通用智能体。大量的实验结果表明，Optimus-3在Minecraft环境中的广泛任务中超越了通用多模态大型语言模型和现有的最先进智能体。项目页面：https://cybertronagent.github.io/Optimus-3.github.io/", "summary": "本文介绍了Optimus-3，一个针对Minecraft开放世界环境的通用多模态智能体。该研究通过提出知识增强的数据生成管道解决数据不足问题，引入专家混合（MoE）架构处理异构任务干扰，并开发多模态推理增强强化学习方法应对视觉多样性。实验结果表明，Optimus-3在Minecraft的广泛任务中表现优于现有的通用多模态大模型和最先进的智能体。", "keywords": "通用智能体, 多模态, Minecraft, 专家混合, 强化学习", "comments": "本文的创新之处在于其对通用多模态智能体在复杂开放世界（如Minecraft）中面临的核心挑战的系统性解决。通过结合数据生成、模块化架构（MoE）和增强推理能力，Optimus-3显著提升了智能体的泛化能力和性能。其重要性体现在为未来开发更强大、更通用的AI智能体提供了有价值的框架和经验。"}}
{"id": "2506.10825", "title": "Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches", "authors": ["Andrea Moglia", "Matteo Leccardi", "Matteo Cavicchioli", "Alice Maccarini", "Marco Marcon", "Luca Mainardi", "Pietro Cerveri"], "summary": "Following the successful paradigm shift of large language models, leveraging\npre-training on a massive corpus of data and fine-tuning on different\ndownstream tasks, generalist models have made their foray into computer vision.\nThe introduction of Segment Anything Model (SAM) set a milestone on\nsegmentation of natural images, inspiring the design of a multitude of\narchitectures for medical image segmentation. In this survey we offer a\ncomprehensive and in-depth investigation on generalist models for medical image\nsegmentation. We start with an introduction on the fundamentals concepts\nunderpinning their development. Then, we provide a taxonomy on the different\ndeclinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on\nthe recent SAM 2, on other innovative models trained on images alone, and\nothers trained on both text and images. We thoroughly analyze their\nperformances at the level of both primary research and best-in-literature,\nfollowed by a rigorous comparison with the state-of-the-art task-specific\nmodels. We emphasize the need to address challenges in terms of compliance with\nregulatory frameworks, privacy and security laws, budget, and trustworthy\nartificial intelligence (AI). Finally, we share our perspective on future\ndirections concerning synthetic data, early fusion, lessons learnt from\ngeneralist models in natural language processing, agentic AI and physical AI,\nand clinical translation.", "comment": "132 pages, 26 figures, 23 tables. Andrea Moglia and Matteo Leccardi\n  are equally contributing authors", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10825v1", "AI": {"title_translation": "医学图像分割中的通用模型：一项调查以及与任务特定方法的性能比较", "tldr": "本调查全面回顾了医学图像分割中的通用模型，包括它们的分类、性能分析以及与任务特定模型的比较，并讨论了挑战和未来方向。", "motivation": "在大型语言模型成功范式转变的启发下，通用模型已进入计算机视觉领域，尤其是在自然图像分割方面取得了里程碑式的进展（如SAM）。本研究的动机是对医学图像分割领域中的通用模型进行全面深入的调查，以理解其发展、分类、性能并与现有任务特定方法进行比较，同时识别其面临的挑战和未来发展方向。", "method": "本调查对医学图像分割中的通用模型进行了全面深入的研究。首先介绍了其基本概念，然后根据零样本、少样本、微调、适配器、SAM 2、仅图像训练以及文本图像联合训练的模型，对SAM的不同变体进行了分类。研究分析了这些模型在初级研究和最佳文献中的性能，并与最先进的任务特定模型进行了严格比较。此外，还讨论了法规遵从性、隐私、安全、预算和可信人工智能方面的挑战，并展望了未来的发展方向。", "result": "本调查提供了一个关于医学图像分割中通用模型的全面深入的分类，包括SAM的各种变体（零样本、少样本、微调、适配器、SAM 2）以及其他创新模型（仅图像训练、文本图像联合训练）。研究分析了这些模型在初级研究和最佳文献中的性能，并与最先进的任务特定模型进行了严格比较。此外，还强调了在法规遵从性、隐私和安全法律、预算以及可信人工智能方面需要解决的挑战。", "conclusion": "本调查对医学图像分割中的通用模型进行了全面的回顾和比较，并强调了在法规遵从性、隐私、安全、预算和可信人工智能方面需要解决的挑战。文章还分享了关于合成数据、早期融合、自然语言处理中通用模型的经验教训、代理AI、物理AI以及临床转化等未来方向的展望。", "translation": "继大型语言模型成功范式转变之后，通过对海量数据语料库进行预训练并在不同下游任务上进行微调，通用模型已经进入计算机视觉领域。Segment Anything Model (SAM) 的引入在自然图像分割方面树立了里程碑，启发了医学图像分割领域众多架构的设计。在本调查中，我们对医学图像分割中的通用模型进行了全面深入的研究。我们首先介绍了支撑其发展的基础概念。然后，我们根据零样本、少样本、微调、适配器、近期SAM 2、其他仅通过图像训练的创新模型以及文本和图像联合训练的模型，对SAM的不同变体进行了分类。我们从初级研究和最佳文献层面，彻底分析了它们的性能，随后与最先进的任务特定模型进行了严格比较。我们强调需要解决在遵守监管框架、隐私和安全法律、预算以及可信人工智能方面的挑战。最后，我们分享了关于合成数据、早期融合、从自然语言处理中的通用模型吸取的经验教训、代理AI和物理AI以及临床转化等未来方向的看法。", "summary": "本调查全面审视了医学图像分割中的通用模型，追溯了其从大型语言模型和SAM的成功范式转变。文章首先介绍了通用模型的基础概念，随后对SAM的不同变体（包括零样本、少样本、微调、适配器、SAM 2以及其他图像/文本-图像联合训练模型）进行了详细分类。研究深入分析了这些模型在现有文献中的性能，并将其与任务特定模型进行了严格比较。此外，本调查还强调了通用模型在医疗领域面临的法规遵从性、隐私、安全、预算和可信AI等关键挑战，并展望了包括合成数据、早期融合、跨领域经验借鉴以及临床转化在内的未来研究方向。", "keywords": "通用模型, 医学图像分割, SAM, 调查, 性能比较", "comments": "该调查论文具有重要的现实意义，它系统地梳理了通用模型在医学图像分割领域的应用现状、发展脉络及面临的挑战。其创新性在于不仅提供了详尽的模型分类和性能比较，更深入探讨了该技术在医疗实践中必须面对的合规性、隐私安全及伦理等实际问题，并提出了前瞻性的未来研究方向。这对于推动通用模型在医疗AI领域的安全、有效落地具有指导价值。"}}
{"id": "2506.10102", "title": "Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach", "authors": ["Ahmed Elbakary", "Chaouki Ben Issaid", "Mehdi Bennis"], "summary": "We present a novel federated multi-task learning method that leverages\ncross-client similarity to enable personalized learning for each client. To\navoid transmitting the entire model to the parameter server, we propose a\ncommunication-efficient scheme that introduces a feature anchor, a compact\nvector representation that summarizes the features learned from the client's\nlocal classes. This feature anchor is shared with the server to account for\nlocal clients' distribution. In addition, the clients share the classification\nheads, a lightweight linear layer, and perform a graph-based regularization to\nenable collaboration among clients. By modeling collaboration between clients\nas a dynamic graph and continuously updating and refining this graph, we can\naccount for any drift from the clients. To ensure beneficial knowledge transfer\nand prevent negative collaboration, we leverage a community detection-based\napproach that partitions this dynamic graph into homogeneous communities,\nmaximizing the sum of task similarities, represented as the graph edges'\nweights, within each community. This mechanism restricts collaboration to\nhighly similar clients within their formed communities, ensuring positive\ninteraction and preserving personalization. Extensive experiments on two\nheterogeneous datasets demonstrate that our method significantly outperforms\nstate-of-the-art baselines. Furthermore, we show that our method exhibits\nsuperior computation and communication efficiency and promotes fairness across\nclients.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10102v1", "AI": {"title_translation": "基于图协作学习：一种选择性联邦多任务学习方法", "tldr": "本文提出一种高效的选择性联邦多任务学习方法，通过引入特征锚、共享分类头和基于图的社区检测，实现了个性化学习和有益的客户端协作。实验证明，该方法在性能、计算和通信效率以及客户端公平性方面均显著优于现有基线。", "motivation": "该研究旨在提出一种新颖的联邦多任务学习方法，利用客户端间的相似性实现个性化学习，同时通过通信高效的方案避免传输整个模型，并防止负面协作，确保有益的知识转移。", "method": "该方法提出了一种通信高效的方案，引入“特征锚”（总结本地特征的紧凑向量表示）与服务器共享，并使客户端共享“分类头”（轻量级线性层）。通过将客户端协作建模为动态图并持续更新，利用基于社区检测的方法将动态图划分为同质社区，限制协作在高度相似的客户端之间，以最大化社区内的任务相似性。", "result": "在两个异构数据集上的广泛实验表明，该方法显著优于最先进的基线。此外，该方法表现出卓越的计算和通信效率，并促进了客户端之间的公平性。", "conclusion": "本文提出的选择性联邦多任务学习方法能够有效地利用客户端相似性实现个性化学习和有益的协作，显著提升了性能、计算和通信效率，并促进了客户端间的公平性。", "translation": "我们提出了一种新颖的联邦多任务学习方法，该方法利用客户端之间的相似性来实现每个客户端的个性化学习。为了避免将整个模型传输到参数服务器，我们提出了一种通信高效的方案，引入了特征锚，这是一种紧凑的向量表示，总结了从客户端本地类中学到的特征。该特征锚与服务器共享，以考虑本地客户端的分布。此外，客户端共享分类头（一个轻量级的线性层），并执行基于图的正则化以实现客户端之间的协作。通过将客户端之间的协作建模为动态图并持续更新和完善该图，我们可以解释客户端的任何漂移。为了确保有益的知识转移并防止负面协作，我们利用基于社区检测的方法，将此动态图划分为同质社区，最大化每个社区内任务相似性之和（表示为图边的权重）。这种机制将协作限制在形成社区内高度相似的客户端之间，确保积极的互动并保持个性化。在两个异构数据集上的广泛实验表明，我们的方法显著优于最先进的基线。此外，我们表明我们的方法表现出卓越的计算和通信效率，并促进了客户端之间的公平性。", "summary": "本文提出了一种新颖的选择性联邦多任务学习方法，旨在通过利用客户端相似性实现个性化学习。该方法通过引入特征锚和共享分类头来提高通信效率，并采用基于动态图的正则化来建模客户端协作。为防止负面协作，引入社区检测机制，将客户端划分为同质社区，确保知识在高度相似的客户端之间有效传递。实验证明，该方法在性能、计算和通信效率以及客户端公平性方面均优于现有基线。", "keywords": "联邦学习, 多任务学习, 图神经网络, 社区检测, 个性化学习", "comments": "该论文的创新点在于提出了结合特征锚、共享分类头和动态图社区检测的联邦多任务学习框架，有效解决了联邦学习中客户端异构性带来的个性化与协作难题。通过限制协作范围，它成功避免了负面知识转移，同时提升了通信和计算效率，对联邦学习的实际应用具有重要意义。"}}
{"id": "2506.10802", "title": "Constructing and Evaluating Declarative RAG Pipelines in PyTerrier", "authors": ["Craig Macdonald", "Jinyuan Fang", "Andrew Parry", "Zaiqiao Meng"], "summary": "Search engines often follow a pipeline architecture, where complex but\neffective reranking components are used to refine the results of an initial\nretrieval. Retrieval augmented generation (RAG) is an exciting application of\nthe pipeline architecture, where the final component generates a coherent\nanswer for the users from the retrieved documents. In this demo paper, we\ndescribe how such RAG pipelines can be formulated in the declarative PyTerrier\narchitecture, and the advantages of doing so. Our PyTerrier-RAG extension for\nPyTerrier provides easy access to standard RAG datasets and evaluation\nmeasures, state-of-the-art LLM readers, and using PyTerrier's unique operator\nnotation, easy-to-build pipelines. We demonstrate the succinctness of indexing\nand RAG pipelines on standard datasets (including Natural Questions) and how to\nbuild on the larger PyTerrier ecosystem with state-of-the-art sparse,\nlearned-sparse, and dense retrievers, and other neural rankers.", "comment": "4 pages, 3 tables, Accepted to SIGIR 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10802v1", "AI": {"title_translation": "在 PyTerrier 中构建和评估声明式 RAG 流水线", "tldr": "本文介绍如何在 PyTerrier 中构建声明式检索增强生成 (RAG) 流水线，并展示其在标准数据集上的简洁性和与现有 PyTerrier 生态系统的集成。", "motivation": "检索增强生成 (RAG) 是流水线架构的一个重要应用，本文旨在描述如何在声明式 PyTerrier 架构中构建此类 RAG 流水线，并阐述这样做的优势。", "method": "通过 PyTerrier-RAG 扩展，在 PyTerrier 中构建声明式 RAG 流水线。该扩展提供了标准 RAG 数据集和评估指标、先进的 LLM 阅读器，并利用 PyTerrier 独特的运算符表示法来简化流水线的构建。通过在标准数据集（包括 Natural Questions）上展示索引和 RAG 流水线的简洁性，并演示如何与 PyTerrier 生态系统中的稀疏、学习稀疏和密集检索器以及其他神经排序器结合使用。", "result": "演示了在标准数据集上索引和 RAG 流水线的简洁性，以及如何利用 PyTerrier 生态系统中现有的先进稀疏、学习稀疏和密集检索器以及其他神经排序器来构建更强大的系统。", "conclusion": "本文展示了在 PyTerrier 中构建声明式 RAG 流水线的可行性和优势，提供了易于使用的工具和与现有生态系统的良好集成。", "translation": "搜索引擎通常遵循流水线架构，其中复杂但有效的重排序组件用于细化初始检索的结果。检索增强生成（RAG）是流水线架构的一个令人兴奋的应用，其中最终组件从检索到的文档中为用户生成连贯的答案。在这篇演示论文中，我们描述了如何在声明式 PyTerrier 架构中构建此类 RAG 流水线，以及这样做的优势。我们的 PyTerrier-RAG 扩展为 PyTerrier 提供了对标准 RAG 数据集和评估措施、最先进的 LLM 阅读器的便捷访问，并利用 PyTerrier 独特的运算符表示法，可以轻松构建流水线。我们演示了在标准数据集（包括 Natural Questions）上索引和 RAG 流水线的简洁性，以及如何利用更大的 PyTerrier 生态系统（包括最先进的稀疏、学习稀疏和密集检索器以及其他神经排序器）进行构建。", "summary": "本文介绍 PyTerrier-RAG 扩展，它允许在 PyTerrier 框架中声明式地构建和评估检索增强生成 (RAG) 流水线。该扩展简化了 RAG 系统的构建，提供了对标准数据集、评估指标和先进 LLM 阅读器的访问，并展示了其与 PyTerrier 现有生态系统中各种检索器和排序器的无缝集成和简洁性。", "keywords": "RAG, PyTerrier, 声明式流水线, 检索增强生成, 搜索系统", "comments": "本文的创新之处在于将 RAG 流水线的构建引入到声明式 PyTerrier 框架中，这大大简化了复杂搜索和生成系统的开发和实验。通过提供统一的接口和集成现有工具，它降低了 RAG 研究和应用的门槛。其重要性体现在为研究人员和开发者提供了一个高效、灵活的平台来构建、评估和迭代 RAG 模型。"}}
{"id": "2506.10154", "title": "Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME", "authors": ["Bidyarthi Paul", "SM Musfiqur Rahman", "Dipta Biswas", "Md. Ziaul Hasan", "Md. Zahid Hossain"], "summary": "Research on understanding emotions in written language continues to expand,\nespecially for understudied languages with distinctive regional expressions and\ncultural features, such as Bangla. This study examines emotion analysis using\n22,698 social media comments from the EmoNoBa dataset. For language analysis,\nwe employ machine learning models: Linear SVM, KNN, and Random Forest with\nn-gram data from a TF-IDF vectorizer. We additionally investigated how PCA\naffects the reduction of dimensionality. Moreover, we utilized a BiLSTM model\nand AdaBoost to improve decision trees. To make our machine learning models\neasier to understand, we used LIME to explain the predictions of the AdaBoost\nclassifier, which uses decision trees. With the goal of advancing sentiment\nanalysis in languages with limited resources, our work examines various\ntechniques to find efficient techniques for emotion identification in Bangla.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10154v1", "AI": {"title_translation": "孟加拉语社交媒体评论情感分析：机器学习与LIME的应用", "tldr": "本研究使用机器学习模型和LIME对孟加拉语社交媒体评论进行情感分析，旨在为资源匮乏语言提供高效的情感识别技术。", "motivation": "现有情感分析研究在孟加拉语等资源匮乏语言中不足，这些语言具有独特的地域表达和文化特征。", "method": "本研究使用来自EmoNoBa数据集的22,698条孟加拉语社交媒体评论。采用机器学习模型：线性SVM、KNN和随机森林，结合TF-IDF向量化的n-gram数据。此外，探讨了PCA对降维的影响，并利用BiLSTM模型和AdaBoost来改进决策树。为提高模型可解释性，使用LIME解释了AdaBoost分类器的预测。", "result": "Not mentioned in abstract", "conclusion": "旨在推进资源匮乏语言的情感分析，通过探索多种技术以识别孟加拉语情感的有效方法。", "translation": "书面语言情感理解的研究持续扩展，特别是对于孟加拉语等具有独特地域表达和文化特征的欠研究语言。本研究使用来自EmoNoBa数据集的22,698条社交媒体评论来检查情感分析。对于语言分析，我们采用了机器学习模型：线性SVM、KNN和随机森林，使用TF-IDF向量化器生成的n-gram数据。我们还研究了PCA如何影响降维。此外，我们利用BiLSTM模型和AdaBoost来改进决策树。为了使我们的机器学习模型更容易理解，我们使用LIME来解释AdaBoost分类器（使用决策树）的预测。旨在推进资源匮乏语言的情感分析，我们的工作研究了各种技术，以寻找孟加拉语情感识别的有效技术。", "summary": "本研究致力于孟加拉语社交媒体评论的情感分析，该语言因其独特的文化特征而面临资源匮乏挑战。研究利用EmoNoBa数据集的22,698条评论，探索了多种机器学习模型，包括线性SVM、KNN、随机森林（结合TF-IDF和n-gram），并评估了PCA对降维的影响。此外，还应用了BiLSTM和AdaBoost来增强决策树模型，并使用LIME解释AdaBoost的预测。目标是为孟加拉语等资源有限的语言开发高效的情感识别技术。", "keywords": "孟加拉语情感分析, 机器学习, LIME, 社交媒体, 资源匮乏语言", "comments": "本研究的创新点在于将情感分析应用于资源相对匮乏的孟加拉语，并结合了多种机器学习模型和解释性AI工具LIME，这对于理解模型决策过程非常重要。其重要性体现在为欠研究语言的情感计算提供了方法论和实践，有助于填补语言资源鸿沟。"}}
{"id": "2506.10145", "title": "RoCA: Robust Cross-Domain End-to-End Autonomous Driving", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hsin-Pai Cheng", "Litian Liu", "Shweta Mahajan", "Apratim Bhattacharyya", "Yunxiao Shi", "Risheek Garrepalli", "Hong Cai", "Fatih Porikli"], "summary": "End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10145v1", "AI": {"title_translation": "RoCA：鲁棒的跨领域端到端自动驾驶", "tldr": "RoCA是一个新颖的框架，旨在解决端到端（E2E）自动驾驶在不同领域（如城市）部署时的鲁棒性挑战。它通过对编码车辆信息的令牌进行联合概率分布建模，并利用高斯过程（GP）学习基础令牌及其轨迹，从而提高基础模型的泛化能力和在新目标域上的适应性，且不增加推理计算成本。", "motivation": "端到端（E2E）自动驾驶具有巨大潜力，但在跨领域部署（例如不同城市）时面临实际挑战。现有将大型语言模型（LLMs）融入E2E驾驶的研究，虽然利用了其开放世界知识，但无法保证跨域驾驶性能，并且在域适应过程中可能产生高昂的再训练成本。", "method": "本文提出了RoCA框架，用于鲁棒的跨领域端到端自动驾驶。RoCA对E2E管道中编码自身和周围车辆信息的令牌的联合概率分布进行建模。通过高斯过程（GP）实例化，RoCA学习一组具有相应轨迹的基础令牌，这些令牌涵盖了多样化的驾驶场景。给定任何驾驶场景，RoCA能够概率性地推断未来轨迹。通过在源域训练中将RoCA与基础E2E模型结合使用，可以提高基础模型的泛化能力，且无需额外的推理计算。", "result": "RoCA在新目标域上实现了鲁棒的适应性，显著优于直接微调方法。在各种跨域场景中进行广泛评估后，RoCA展现出强大的域泛化和适应性能。", "conclusion": "RoCA框架通过其独特的概率建模和高斯过程应用，显著提高了端到端自动驾驶模型在不同领域间的泛化能力和适应性，为实际部署中的跨域挑战提供了鲁棒的解决方案。", "translation": "端到端（E2E）自动驾驶最近作为一种新范式出现，具有巨大的潜力。然而，很少有研究关注跨领域（例如城市）部署的实际挑战。尽管一些工作已将大型语言模型（LLMs）纳入其中，以利用其开放世界知识，但LLMs并不能保证跨域驾驶性能，并且在域适应过程中可能产生高昂的再训练成本。在本文中，我们提出了RoCA，一个用于鲁棒跨域E2E自动驾驶的新颖框架。RoCA在E2E管道中对编码自身和周围车辆信息的令牌的联合概率分布进行建模。通过高斯过程（GP）实例化，RoCA学习一组具有相应轨迹的基础令牌，这些令牌涵盖了多样化的驾驶场景。然后，给定任何驾驶场景，它能够概率性地推断未来轨迹。通过在源域训练中将RoCA与基础E2E模型结合使用，我们提高了基础模型的泛化能力，且无需额外的推理计算。此外，RoCA在新目标域上实现了鲁棒的适应性，显著优于直接微调。我们在各种跨域场景中广泛评估了RoCA，并表明它实现了强大的域泛化和适应性能。", "summary": "本文提出RoCA，一个用于解决端到端（E2E）自动驾驶跨域部署挑战的新颖框架。针对现有方法在跨域泛化和适应性方面的不足，RoCA通过对E2E管道中的车辆信息令牌进行联合概率分布建模，并利用高斯过程（GP）学习覆盖多样化驾驶场景的基础令牌。该方法能够概率性地推断未来轨迹，并在源域训练中与基础E2E模型结合使用，无需额外推理计算即可提升模型泛化能力。实验证明，RoCA在新目标域上展现出优异的鲁棒适应性和域泛化性能，显著优于直接微调。", "keywords": "端到端自动驾驶, 跨域, 鲁棒性, 高斯过程, 域泛化", "comments": "RoCA的创新之处在于其对跨域E2E自动驾驶的概率建模方法，特别是利用高斯过程学习“基础令牌”来捕捉多样化的驾驶场景。这提供了一种新颖的解决方案，以应对E2E自动驾驶在不同环境部署时的泛化和适应性难题。其重要性在于，它在不增加推理成本的情况下提高了模型的鲁棒性，这对于实际应用至关重要。该方法有效地避免了LLM可能带来的高昂再训练成本，为自动驾驶的实际落地提供了有价值的参考。"}}
{"id": "2506.10499", "title": "Convergence of adaptive boundary element methods driven by functional a posteriori error estimates", "authors": ["Alexander Freiszlinger", "Dirk Pauly", "Dirk Praetorius"], "summary": "The recent work [Kurz et al., Numer. Math., 147 (2021)] proposed functional a\nposteriori error estimates for boundary element methods (BEMs) together with a\nrelated adaptive mesh-refinement strategy. Unlike most a posteriori BEM error\nestimators, the proposed functional error estimators cover Galerkin as well as\ncollocation BEM and, more importantly, do not control the error in the integral\ndensity on the boundary, but the error of the potential approximation in the\ndomain, which is of greater relevance in practice. The estimates rely on the\nnumerical solution of auxiliary problems on auxiliary strip domains along the\nboundary, where the strips are affected by the adaptive mesh-refinement and\nhence vary. For Galerkin BEM, we prove that the proposed adaptive\nmesh-refinement algorithm yields convergence of the potential error to zero.\nDue to the structural difference to residual-based estimators, the proof\nrequires new ideas.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10499v1", "AI": {"title_translation": "由泛函后验误差估计驱动的自适应边界元方法收敛性", "tldr": "本文证明了一种新的泛函后验误差估计驱动的自适应网格细化算法对于Galerkin边界元方法能够使域内势误差收敛到零。", "motivation": "传统的后验边界元方法（BEM）误差估计器通常控制边界上的积分密度误差，而本文提出的泛函误差估计器控制域内势近似的误差，这在实践中更具相关性。", "method": "本文采用由Kurz等人提出的泛函后验误差估计方法，并结合自适应网格细化策略。该方法依赖于在沿边界的辅助条带域上数值求解辅助问题，这些条带域会随自适应网格细化而变化。", "result": "对于Galerkin边界元方法，本文证明了所提出的自适应网格细化算法能够使势误差收敛到零。", "conclusion": "本文证明了基于泛函后验误差估计的自适应网格细化算法对Galerkin边界元方法具有收敛性，并且由于其与基于残差的估计器的结构差异，该证明需要新的思路。", "translation": "最近的工作[Kurz et al., Numer. Math., 147 (2021)]提出了一种针对边界元方法（BEMs）的泛函后验误差估计以及相关的自适应网格细化策略。与大多数后验BEM误差估计器不同，所提出的泛函误差估计器涵盖了Galerkin和搭配BEM，更重要的是，它们不控制边界上的积分密度误差，而是控制域内势近似的误差，这在实践中更具相关性。这些估计依赖于在沿边界的辅助条带域上数值求解辅助问题，这些条带域受自适应网格细化的影响而变化。对于Galerkin BEM，我们证明了所提出的自适应网格细化算法能够使势误差收敛到零。由于与基于残差的估计器的结构差异，该证明需要新的思路。", "summary": "本文研究了一种由泛函后验误差估计驱动的自适应边界元方法（BEM）的收敛性。该方法由Kurz等人提出，其创新之处在于控制域内势近似的误差，而非传统的边界积分密度误差，这更符合实际应用需求。文章详细描述了其依赖于辅助条带域上辅助问题的数值求解过程。对于Galerkin BEM，本文证明了该自适应网格细化算法能够确保域内势误差收敛到零，并指出由于其与传统基于残差的估计器存在结构差异，因此需要新的证明方法。", "keywords": "边界元方法, 后验误差估计, 自适应网格细化, 收敛性, 泛函误差", "comments": "本文的创新点在于其提出的泛函后验误差估计器能够控制域内势近似的误差，而非仅仅是边界上的积分密度误差，这使得该方法在实际应用中更具价值。此外，文章为Galerkin边界元方法证明了该自适应算法的收敛性，并指出证明过程需要新的思想，这体现了其理论贡献。"}}
{"id": "2506.10147", "title": "Unconditionally Secure Wireless-Wired Ground-Satellite-Ground Communication Networks Utilizing Classical and Quantum Noise", "authors": ["Lucas Truax", "Sandip Roy", "Laszlo B. Kish"], "summary": "In this paper, we introduce the Kirchhoff-Law-Johnson-Noise (KLJN) as an\napproach to securing satellite communications. KLJN has the potential to\nrevolutionize satellite communication security through its combination of\nsimplicity, cost-effectiveness, and resilience with unconditional security.\nUnlike quantum key distribution (QKD), which requires complex, fragile, and\nexpensive infrastructure like photon detectors and dedicated optical links,\nKLJN operates using standard electronic components and wires, significantly\nreducing implementation costs and logistical hurdles. KLJN's security, grounded\nin the fundamental laws of classical physics, is impervious to environmental\nand radiation-induced noise, making it highly reliable in the harsh conditions\nof satellite communications. This robustness, coupled with its ability to\nintegrate seamlessly with existing infrastructure, positions KLJN as a\nrevolutionary alternative to quantum solutions for ensuring secure, resilient\nsatellite communications. The authors explore the value of achieving\nunconditionally secure communications in strategic ground-to-satellite networks\nwhich address vulnerabilities posed by advanced computational threats,\nincluding quantum computing. Our team has examined two leading approaches to\nunconditional security - the KLJN scheme and QKD - and analyzed the potential\nuse of each for space systems. While QKD leverages quantum mechanics for\nsecurity, it faces challenges related to cost, complexity, and environmental\nsensitivity. In contrast, the KLJN scheme utilizes classical physics principles\nto provide a simpler, more cost-effective, and resilient alternative,\nparticularly for ground-based systems. The study concludes that KLJN offers\nsignificant advantages in simplicity, cost-efficiency, and robustness, making\nit a practical choice for many secure communication applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10147v1", "AI": {"title_translation": "利用经典和量子噪声的无条件安全无线-有线地-星-地通信网络", "tldr": "本文提出并探讨了 Kirchhoff-Law-Johnson-Noise (KLJN) 作为一种无条件安全卫星通信的替代方案，与量子密钥分发 (QKD) 相比，KLJN 更简单、成本更低、更具弹性。", "motivation": "当前卫星通信面临高级计算威胁（包括量子计算）带来的漏洞，需要实现无条件安全的通信。现有的量子密钥分发（QKD）方案存在复杂、脆弱、昂贵等问题，因此需要探索更实用、成本效益更高的替代方案。", "method": "本文引入并分析了 Kirchhoff-Law-Johnson-Noise (KLJN) 方案，并将其与量子密钥分发（QKD）方案进行了比较，以评估两者在空间系统中的潜在应用。研究着重于KLJN方案在安全性、成本、复杂性和环境适应性方面的优势。", "result": "研究发现，KLJN方案利用经典物理原理，提供了一种比QKD更简单、更经济、更具弹性的替代方案，尤其适用于地面系统。KLJN对环境和辐射引起的噪声不敏感，且能与现有基础设施无缝集成，展现出在无条件安全通信方面的显著优势。", "conclusion": "KLJN方案在简便性、成本效益和鲁棒性方面具有显著优势，使其成为许多安全通信应用的实用选择，是确保安全、有弹性卫星通信的革命性替代方案。", "translation": "在本文中，我们引入了基尔霍夫定律-约翰逊噪声（KLJN）作为一种保障卫星通信安全的方法。KLJN凭借其简单性、成本效益和弹性与无条件安全性相结合的特点，有望彻底改变卫星通信安全。与需要光子探测器和专用光链路等复杂、脆弱且昂贵基础设施的量子密钥分发（QKD）不同，KLJN使用标准的电子元件和电线运行，显著降低了实施成本和物流障碍。KLJN的安全性基于经典物理学的基本定律，不受环境和辐射引起的噪声影响，使其在卫星通信的恶劣条件下高度可靠。这种鲁棒性，加上其与现有基础设施无缝集成的能力，使KLJN成为量子解决方案的革命性替代方案，以确保安全、有弹性的卫星通信。作者探讨了在战略性地对星网络中实现无条件安全通信的价值，该网络解决了包括量子计算在内的高级计算威胁所带来的漏洞。我们的团队研究了两种领先的无条件安全方法——KLJN方案和QKD——并分析了它们各自在空间系统中的潜在用途。虽然QKD利用量子力学实现安全，但它面临着成本、复杂性和环境敏感性方面的挑战。相比之下，KLJN方案利用经典物理原理提供了一种更简单、更具成本效益和更具弹性的替代方案，特别是对于地面系统。研究得出结论，KLJN在简单性、成本效益和鲁棒性方面具有显著优势，使其成为许多安全通信应用的实用选择。", "summary": "本文提出了一种名为基尔霍夫定律-约翰逊噪声（KLJN）的新方法，旨在解决卫星通信中的无条件安全问题。与复杂的量子密钥分发（QKD）不同，KLJN利用经典物理原理和标准电子元件，具有成本效益高、简单、鲁棒性强且抗环境噪声的特点。研究比较了KLJN和QKD，认为KLJN在确保地对星通信网络的无条件安全方面具有显著优势，尤其是在应对量子计算等高级威胁时，是更实用和可行的选择。", "keywords": "KLJN, 无条件安全, 卫星通信, 量子密钥分发, 经典噪声", "comments": "该论文提出KLJN作为QKD的替代方案，其创新之处在于利用经典物理噪声实现无条件安全，并强调了其在成本、复杂性和环境适应性方面的优势。这对于推动卫星通信安全具有重要意义，尤其是在资源受限或环境恶劣的场景下。论文强调了KLJN与现有基础设施的兼容性，这表明其具有较高的实用性和部署潜力。"}}
{"id": "2506.10376", "title": "MLLM-Based UI2Code Automation Guided by UI Layout Information", "authors": ["Fan Wu", "Cuiyun Gao", "Shuqing Li", "Xin-Cheng Wen", "Qing Liao"], "summary": "Converting user interfaces into code (UI2Code) is a crucial step in website\ndevelopment, which is time-consuming and labor-intensive. The automation of\nUI2Code is essential to streamline this task, beneficial for improving the\ndevelopment efficiency. There exist deep learning-based methods for the task;\nhowever, they heavily rely on a large amount of labeled training data and\nstruggle with generalizing to real-world, unseen web page designs. The advent\nof Multimodal Large Language Models (MLLMs) presents potential for alleviating\nthe issue, but they are difficult to comprehend the complex layouts in UIs and\ngenerate the accurate code with layout preserved. To address these issues, we\npropose LayoutCoder, a novel MLLM-based framework generating UI code from\nreal-world webpage images, which includes three key modules: (1) Element\nRelation Construction, which aims at capturing UI layout by identifying and\ngrouping components with similar structures; (2) UI Layout Parsing, which aims\nat generating UI layout trees for guiding the subsequent code generation\nprocess; and (3) Layout-Guided Code Fusion, which aims at producing the\naccurate code with layout preserved. For evaluation, we build a new benchmark\ndataset which involves 350 real-world websites named Snap2Code, divided into\nseen and unseen parts for mitigating the data leakage issue, besides the\npopular dataset Design2Code. Extensive evaluation shows the superior\nperformance of LayoutCoder over the state-of-the-art approaches. Compared with\nthe best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and\n3.95% in the CLIP score on average across all datasets.", "comment": "Accepted by the 34th International Symposium on Software Testing and\n  Analysis (ISSTA 2025)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10376v1", "AI": {"title_translation": "基于MLLM并由UI布局信息引导的UI2Code自动化", "tldr": "LayoutCoder是一个基于MLLM的框架，通过理解UI布局信息，将真实网页图像转换为准确的代码，解决了现有方法对大量标注数据依赖和泛化能力差的问题，并在新基准数据集上表现优异。", "motivation": "将用户界面转换为代码（UI2Code）是网站开发中耗时耗力的关键步骤。自动化UI2Code对于提高开发效率至关重要。现有基于深度学习的方法依赖大量标注数据且难以泛化到真实世界的未见网页设计。多模态大语言模型（MLLMs）虽有潜力，但难以理解UI中复杂的布局并生成保留布局的准确代码。", "method": "本文提出了LayoutCoder，一个新颖的基于MLLM的框架，用于从真实网页图像生成UI代码。它包括三个关键模块：1) 元素关系构建，旨在通过识别和分组具有相似结构的组件来捕获UI布局；2) UI布局解析，旨在生成UI布局树以指导后续的代码生成过程；3) 布局引导代码融合，旨在生成保留布局的准确代码。为了评估，构建了一个名为Snap2Code的新基准数据集，包含350个真实世界网站，并结合流行的Design2Code数据集进行评估。", "result": "广泛的评估表明LayoutCoder的性能优于现有最先进的方法。与表现最佳的基线相比，LayoutCoder在所有数据集上的BLEU分数平均提高了10.14%，CLIP分数平均提高了3.95%。", "conclusion": "LayoutCoder通过引入UI布局信息引导的MLLM框架，有效地解决了UI2Code自动化中现有方法的局限性，显著提高了代码生成的准确性和泛化能力，尤其是在复杂布局的理解和保留方面。", "translation": "将用户界面转换为代码（UI2Code）是网站开发中至关重要的一步，耗时且费力。UI2Code的自动化对于简化这项任务至关重要，有利于提高开发效率。目前存在基于深度学习的方法来完成这项任务；然而，它们严重依赖大量标注的训练数据，并且难以泛化到真实世界的、未见的网页设计。多模态大语言模型（MLLMs）的出现为缓解这个问题提供了潜力，但它们难以理解UI中复杂的布局并生成保留布局的准确代码。为了解决这些问题，我们提出了LayoutCoder，一个新颖的基于MLLM的框架，用于从真实网页图像生成UI代码，它包括三个关键模块：(1) 元素关系构建，旨在通过识别和分组具有相似结构的组件来捕获UI布局；(2) UI布局解析，旨在生成UI布局树以指导后续的代码生成过程；(3) 布局引导代码融合，旨在生成保留布局的准确代码。为了评估，我们构建了一个新的基准数据集Snap2Code，其中包含350个真实世界网站，并将其分为已知和未知部分以缓解数据泄漏问题，此外还使用了流行的Design2Code数据集。广泛的评估表明LayoutCoder的性能优于现有最先进的方法。与表现最佳的基线相比，LayoutCoder在所有数据集上的BLEU分数平均提高了10.14%，CLIP分数平均提高了3.95%。", "summary": "本文提出了LayoutCoder，一个基于多模态大语言模型（MLLMs）的新框架，旨在自动化将用户界面图像转换为代码（UI2Code）的过程。针对现有方法对标注数据依赖和对复杂UI布局理解不足的问题，LayoutCoder引入了三个核心模块：元素关系构建、UI布局解析和布局引导代码融合，以有效捕获和利用UI布局信息生成准确且保留布局的代码。研究团队构建了新的基准数据集Snap2Code进行评估，实验结果显示LayoutCoder在BLEU和CLIP分数上均显著优于现有最先进的方法，证明了其在真实世界网页设计UI2Code任务中的优越性能。", "keywords": "UI2Code, MLLM, 自动化, UI布局, 网页开发", "comments": "LayoutCoder的创新之处在于其通过显式地捕获和利用UI布局信息来指导MLLM进行UI2Code自动化，解决了MLLM在理解复杂UI布局方面的固有挑战。通过引入元素关系构建、UI布局解析和布局引导代码融合这三个模块，该框架能够生成更准确且布局保留的代码。此外，构建新的真实世界基准数据集Snap2Code也增加了其研究价值，有助于推动该领域的发展。该方法在泛化能力和代码准确性方面取得了显著提升，对于提高网站开发效率具有重要意义。"}}
{"id": "2506.10317", "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving", "authors": ["Akshar Tumu", "Henrik I. Christensen", "Marcell Vazquez-Chanlatte", "Chikao Tsuchiya", "Dhaval Bhanderi"], "summary": "Lane-topology prediction is a critical component of safe and reliable\nautonomous navigation. An accurate understanding of the road environment aids\nthis task. We observe that this information often follows conventions encoded\nin natural language, through design codes that reflect the road structure and\nroad names that capture the road functionality. We augment this information in\na lightweight manner to SMERF, a map-prior-based online lane-topology\nprediction model, by combining structured road metadata from OSM maps and\nlane-width priors from Road design manuals with the road centerline encodings.\nWe evaluate our method on two geo-diverse complex intersection scenarios. Our\nmethod shows improvement in both lane and traffic element detection and their\nassociation. We report results using four topology-aware metrics to\ncomprehensively assess the model performance. These results demonstrate the\nability of our approach to generalize and scale to diverse topologies and\nconditions.", "comment": "4 pages, 3 figures, Accepted at RSS 2025 Workshop -\n  RobotEvaluation@RSS2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10317v1", "AI": {"title_translation": "使用语言和道路手册为自动驾驶地图重建提供信息", "tldr": "通过结合语言和道路手册信息，改进了基于地图先验的在线车道拓扑预测模型SMERF，提高了复杂交叉路口的车道和交通元素检测及关联性能。", "motivation": "车道拓扑预测是安全可靠自动驾驶的关键组成部分，准确理解道路环境对此任务至关重要。研究观察到道路信息常遵循自然语言编码的惯例，如设计规范和道路名称，这些信息可用于辅助地图重建。", "method": "通过将OSM地图中的结构化道路元数据和道路设计手册中的车道宽度先验与道路中心线编码相结合，以轻量级方式增强了基于地图先验的在线车道拓扑预测模型SMERF。", "result": "该方法在两个地理多样化的复杂交叉路口场景中进行了评估，结果显示在车道和交通元素检测及其关联方面都有所改进。使用四个拓扑感知指标全面评估了模型性能，并证明了该方法能够推广并适应各种拓扑结构和条件。", "conclusion": "该方法能够推广并适应各种拓扑结构和条件。", "translation": "车道拓扑预测是安全可靠自动驾驶的关键组成部分。准确理解道路环境有助于此任务。我们观察到，这些信息通常遵循以自然语言编码的惯例，通过反映道路结构的设计规范和捕捉道路功能的道路名称。我们以轻量级方式将这些信息增强到SMERF模型中，SMERF是一个基于地图先验的在线车道拓扑预测模型，通过将OSM地图中的结构化道路元数据和道路设计手册中的车道宽度先验与道路中心线编码相结合。我们在两个地理多样化的复杂交叉路口场景中评估了我们的方法。我们的方法在车道和交通元素检测及其关联方面都显示出改进。我们使用四个拓扑感知指标报告了结果，以全面评估模型性能。这些结果表明我们的方法能够推广并适应各种拓扑结构和条件。", "summary": "本文提出了一种利用自然语言编码的道路信息（如道路设计规范和道路名称）来增强自动驾驶地图重建的方法。通过将OSM地图的结构化元数据和道路手册的车道宽度先验结合到在线车道拓扑预测模型SMERF中，研究人员在复杂交叉路口场景中验证了该方法，结果显示其在车道和交通元素检测及关联方面均有提升，并能泛化到多样化的拓扑结构和条件。", "keywords": "车道拓扑预测, 自动驾驶, 地图重建, 自然语言, 道路手册", "comments": "这项工作创新性地将传统道路设计规范和语言信息融入到自动驾驶的地图重建中，弥补了纯视觉或激光雷达方法在理解道路语义和结构上的不足。通过结合结构化元数据和先验知识，提高了在线车道拓扑预测的准确性和鲁棒性，尤其是在复杂交叉路口。其轻量级增强方式也值得关注。"}}
{"id": "2506.10818", "title": "Grasp Prediction based on Local Finger Motion Dynamics", "authors": ["Dimitar Valkov", "Pascal Kockwelp", "Florian Daiber", "Antonio Krüger"], "summary": "The ability to predict the object the user intends to grasp offers essential\ncontextual information and may help to leverage the effects of point-to-point\nlatency in interactive environments. This paper explores the feasibility and\naccuracy of real-time recognition of uninstrumented objects based on hand\nkinematics during reach-to-grasp actions. In a data collection study, we\nrecorded the hand motions of 16 participants while reaching out to grasp and\nthen moving real and synthetic objects. Our results demonstrate that even a\nsimple LSTM network can predict the time point at which the user grasps an\nobject with a precision better than 21 ms and the current distance to this\nobject with a precision better than 1 cm. The target's size can be determined\nin advance with an accuracy better than 97%. Our results have implications for\ndesigning adaptive and fine-grained interactive user interfaces in ubiquitous\nand mixed-reality environments.", "comment": "10 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10818v1", "AI": {"title_translation": "基于局部手指运动动力学的抓取预测", "tldr": "该研究探索了基于手部运动学实时识别未被仪器化的物体抓取的可行性和准确性，并发现简单的LSTM网络能高精度预测抓取时间点、距离和目标大小。", "motivation": "预测用户意图抓取的物体能提供重要的上下文信息，并有助于利用交互环境中点对点延迟的影响。", "method": "通过数据收集研究，记录了16名参与者在伸向并抓取真实和合成物体时的手部运动，并使用简单的LSTM网络进行预测。", "result": "一个简单的LSTM网络能够以优于21毫秒的精度预测用户抓取物体的时间点，以优于1厘米的精度预测到该物体的当前距离。目标大小可以提前以优于97%的准确率确定。", "conclusion": "研究结果对于在普适计算和混合现实环境中设计自适应和细粒度的交互式用户界面具有重要意义。", "translation": "用户预测抓取物体的能力提供了必要的上下文信息，并有助于利用交互环境中点对点延迟的影响。本文探讨了在抓取动作中基于手部运动学实时识别未被仪器化的物体的可行性和准确性。在一项数据收集研究中，我们记录了16名参与者在伸向并抓取真实和合成物体时的手部运动。我们的结果表明，即使是一个简单的LSTM网络也能以优于21毫秒的精度预测用户抓取物体的时间点，并以优于1厘米的精度预测到该物体的当前距离。目标大小可以提前以优于97%的准确率确定。我们的结果对于在普适计算和混合现实环境中设计自适应和细粒度的交互式用户界面具有重要意义。", "summary": "本研究探讨了在抓取动作中，利用手部运动学实时识别未被仪器化的物体的可行性与准确性。通过记录16名参与者的手部运动数据，研究发现即使是简单的LSTM网络也能高精度预测用户抓取物体的时间点（优于21毫秒）、到物体距离（优于1厘米），以及提前确定目标大小（优于97%）。这些发现为普适计算和混合现实环境中自适应、精细化交互界面的设计提供了基础。", "keywords": "抓取预测, 手部运动学, LSTM, 实时识别, 混合现实", "comments": "该论文的创新点在于证明了仅通过手部运动学数据，即使是简单的机器学习模型也能实现对用户抓取意图的高精度预测，且无需对物体进行额外仪器化。其重要性在于为未来普适计算和混合现实环境中更智能、更自然的交互界面设计提供了新的可能性，尤其是在处理延迟和提供上下文信息方面。"}}
{"id": "2506.10607", "title": "Graph-based Gossiping for Communication Efficiency in Decentralized Federated Learning", "authors": ["Huong Nguyen", "Hong-Tri Nguyen", "Praveen Kumar Donta", "Susanna Pirttikangas", "Lauri Lovén"], "summary": "Federated learning has emerged as a privacy-preserving technique for\ncollaborative model training across heterogeneously distributed silos. Yet, its\nreliance on a single central server introduces potential bottlenecks and risks\nof single-point failure. Decentralizing the server, often referred to as\ndecentralized learning, addresses this problem by distributing the server role\nacross nodes within the network. One drawback regarding this pure\ndecentralization is it introduces communication inefficiencies, which arise\nfrom increased message exchanges in large-scale setups. However, existing\nproposed solutions often fail to simulate the real-world distributed and\ndecentralized environment in their experiments, leading to unreliable\nperformance evaluations and limited applicability in practice. Recognizing the\nlack from prior works, this work investigates the correlation between model\nsize and network latency, a critical factor in optimizing decentralized\nlearning communication. We propose a graph-based gossiping mechanism, where\nspecifically, minimum spanning tree and graph coloring are used to optimize\nnetwork structure and scheduling for efficient communication across various\nnetwork topologies and message capacities. Our approach configures and manages\nsubnetworks on real physical routers and devices and closely models real-world\ndistributed setups. Experimental results demonstrate that our method\nsignificantly improves communication, compatible with different topologies and\ndata sizes, reducing bandwidth and transfer time by up to circa 8 and 4.4\ntimes, respectively, compared to naive flooding broadcasting methods.", "comment": "Accepted at 34th International Conference on Computer Communications\n  and Networks (ICCCN 2025)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10607v1", "AI": {"title_translation": "基于图的八卦传播机制在去中心化联邦学习中提升通信效率", "tldr": "本文提出一种基于图的八卦传播机制，利用最小生成树和图着色优化去中心化联邦学习中的通信效率，显著减少带宽和传输时间，并在真实物理设备上验证了其有效性。", "motivation": "联邦学习中的中心化服务器存在瓶颈和单点故障风险。去中心化联邦学习虽解决了此问题，但在大规模部署中引入通信效率低下。现有解决方案未能有效模拟真实分布式环境，导致评估不可靠和实际应用受限。", "method": "提出了一种基于图的八卦传播机制。该机制具体利用最小生成树和图着色来优化网络结构和调度，以实现在各种网络拓扑和消息容量下的高效通信。该方法在真实物理路由器和设备上配置和管理子网络，以紧密模拟真实世界的分布式设置。", "result": "实验结果表明，该方法显著改善了通信效率，兼容不同拓扑和数据大小。与朴素泛洪广播方法相比，带宽和传输时间分别减少了约8倍和4.4倍。", "conclusion": "该研究提出了一种新颖的基于图的八卦传播机制，通过优化网络结构和调度，有效解决了去中心化联邦学习中的通信效率问题，并在真实世界环境中展示了其优越的性能和实用性。", "translation": "联邦学习已成为一种隐私保护技术，用于在异构分布式孤岛间进行协作模型训练。然而，它对单一中央服务器的依赖引入了潜在的瓶颈和单点故障风险。去中心化服务器，通常被称为去中心化学习，通过在网络中的节点间分发服务器角色来解决此问题。这种纯粹去中心化带来一个缺点，即它引入了通信效率低下，这源于大规模设置中消息交换的增加。然而，现有提出的解决方案在实验中往往未能模拟真实的分布式和去中心化环境，导致性能评估不可靠和实际应用受限。认识到先前工作的不足，本研究调查了模型大小与网络延迟之间的相关性，这是优化去中心化学习通信的关键因素。我们提出了一种基于图的八卦传播机制，其中，具体利用最小生成树和图着色来优化网络结构和调度，以实现在各种网络拓扑和消息容量下的高效通信。我们的方法在真实物理路由器和设备上配置和管理子网络，并紧密模拟真实世界的分布式设置。实验结果表明，与朴素泛洪广播方法相比，我们的方法显著改善了通信，兼容不同的拓扑和数据大小，分别将带宽和传输时间减少了约8倍和4.4倍。", "summary": "本文针对去中心化联邦学习中存在的通信效率低下和现有方案缺乏真实环境模拟的问题，提出了一种基于图的八卦传播机制。该机制通过应用最小生成树和图着色技术优化网络结构和调度，旨在提升不同网络拓扑和消息容量下的通信效率。研究在真实物理设备上进行实验验证，结果显示该方法与传统泛洪广播相比，能显著降低带宽和传输时间，提高了去中心化联邦学习的实用性和性能。", "keywords": "去中心化联邦学习, 通信效率, 图传播, 最小生成树, 图着色", "comments": "本文的创新点在于提出了一个基于图的八卦传播机制，并结合最小生成树和图着色来优化去中心化联邦学习中的通信效率。其重要性在于解决了去中心化联邦学习在实际大规模部署中面临的通信瓶颈问题，并通过在真实物理设备上进行实验验证，增强了结果的可靠性和实际应用价值。"}}
{"id": "2506.10236", "title": "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods", "authors": ["Yeonwoo Jang", "Shariqah Hossain", "Ashwin Sreevatsa", "Diogo Cruz"], "summary": "In this work, we show that some machine unlearning methods may fail when\nsubjected to straightforward prompt attacks. We systematically evaluate eight\nunlearning techniques across three model families, and employ output-based,\nlogit-based, and probe analysis to determine to what extent supposedly\nunlearned knowledge can be retrieved. While methods like RMU and TAR\ndemonstrate robust unlearning, ELM remains vulnerable to specific prompt\nattacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).\nOur logit analysis also confirms that unlearned models are generally not hiding\nknowledge by modifying the way the answer is formatted, as the correlation\nbetween output and logit accuracy is strong. These results challenge prevailing\nassumptions about unlearning effectiveness and highlight the need for\nevaluation frameworks that can reliably distinguish between true knowledge\nremoval and superficial output suppression. We also publicly make available our\nevaluation framework to easily evaluate prompting techniques to retrieve\nunlearning knowledge.", "comment": "20 pages, 6 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10236v1", "AI": {"title_translation": "提示攻击揭示了反学习方法中肤浅的知识清除", "tldr": "研究发现，一些机器反学习方法在面对直接提示攻击时会失效，揭示了知识清除可能只是表面现象，而非真正的移除。", "motivation": "旨在揭示某些机器反学习方法在面对直接提示攻击时可能失效，挑战了关于反学习有效性的现有假设。", "method": "系统评估了八种反学习技术，涵盖三种模型家族。采用了基于输出、基于logit和探测分析的方法来确定被认为已清除的知识在多大程度上可以被检索。", "result": "RMU和TAR等方法表现出鲁棒的反学习能力，而ELM仍然容易受到特定提示攻击（例如，原始提示中的印地语填充文本可恢复57.3%的准确率）。Logit分析证实，反学习模型通常不会通过修改答案格式来隐藏知识，因为输出和logit准确性之间存在很强的相关性。", "conclusion": "研究结果挑战了关于反学习有效性的普遍假设，并强调需要能够可靠区分真正知识清除和表面输出抑制的评估框架。", "translation": "在这项工作中，我们展示了一些机器反学习方法在受到直接提示攻击时可能会失败。我们系统地评估了三种模型家族中的八种反学习技术，并采用基于输出、基于logit和探测分析的方法来确定被认为已清除的知识在多大程度上可以被检索。虽然RMU和TAR等方法表现出鲁棒的反学习能力，但ELM仍然容易受到特定提示攻击（例如，原始提示中的印地语填充文本可恢复57.3%的准确率）。我们的logit分析也证实，反学习模型通常不会通过修改答案格式来隐藏知识，因为输出和logit准确性之间存在很强的相关性。这些结果挑战了关于反学习有效性的普遍假设，并强调了需要能够可靠区分真正知识清除和表面输出抑制的评估框架。我们还公开提供了我们的评估框架，以便轻松评估检索反学习知识的提示技术。", "summary": "本文研究了机器反学习方法的有效性，发现某些方法在面对简单的提示攻击时会失败，未能真正清除知识。研究人员系统评估了八种反学习技术，并通过输出、logit和探测分析发现，尽管一些方法如RMU和TAR表现良好，但ELM等方法仍易受攻击，例如通过特定提示可恢复高达57.3%的准确率。研究强调了区分真正知识移除和表面抑制的重要性，并提供了一个评估框架。", "keywords": "机器反学习, 提示攻击, 知识清除, 模型评估, 鲁棒性", "comments": "这项工作揭示了当前机器反学习方法的一个重要漏洞，即它们可能只实现了表面上的知识抑制而非真正的删除。其创新之处在于引入了“提示攻击”作为一种有效的评估手段，并系统地评估了多种现有方法。研究结果对于推动更鲁棒的反学习技术发展具有重要意义，并强调了未来研究应关注更深层次的知识清除机制。公开评估框架的举动也促进了该领域的研究进展。"}}
{"id": "2506.10354", "title": "Revisiting mean estimation over $\\ell_p$ balls: Is the MLE optimal?", "authors": ["Liviu Aolaritei", "Michael I. Jordan", "Reese Pathak", "Annie Ulichney"], "summary": "We revisit the problem of mean estimation on $\\ell_p$ balls under additive\nGaussian noise. When $p$ is strictly less than $2$, it is well understood that\nrate-optimal estimators must be nonlinear in the observations. In this work, we\nstudy the maximum likelihood estimator (MLE), which may be viewed as a\nnonlinear shrinkage procedure for mean estimation over $\\ell_p$ balls. We\ndemonstrate two phenomena for the behavior of the MLE, which depend on the\nnoise level, the radius of the norm constraint, the dimension, and the norm\nindex $p$. First, as a function of the dimension, for $p$ near $1$ or at least\n$2$, the MLE is minimax rate-optimal for all noise levels and all constraint\nradii. On the other hand, for $p$ between $1$ and $2$, there is a more striking\nbehavior: for essentially all noise levels and radii for which nonlinear\nestimates are required, the MLE is minimax rate-suboptimal, despite being\nnonlinear in the observations. Our results also imply similar conclusions when\ngiven $n$ independent and identically distributed Gaussian samples, where we\ndemonstrate that the MLE can be suboptimal by a polynomial factor in the sample\nsize. Our lower bounds are constructive: whenever the MLE is rate-suboptimal,\nwe provide explicit instances on which the MLE provably incurs suboptimal risk.", "comment": "37 pages, 3 figures", "cate": "math.ST", "url": "http://arxiv.org/abs/2506.10354v1", "AI": {"title_translation": "重新审视 $\\ell_p$ 球上的均值估计：最大似然估计器是最优的吗？", "tldr": "本文研究了在加性高斯噪声下，$\\ell_p$ 球上均值估计的最大似然估计器（MLE）的性能。结果表明，当 $p \\in (1,2)$ 时，尽管 MLE 是非线性的，但它在最小最大意义上是次优的，而在其他 $p$ 值附近（接近1或大于等于2）则可能是最优的。", "motivation": "在加性高斯噪声下，当 $p$ 严格小于 $2$ 时，已知率最优估计器必须是非线性的。本文重新审视了 $\\ell_p$ 球上的均值估计问题，并深入研究了最大似然估计器（MLE）的性能，以探讨其是否为最优估计器。", "method": "本文研究了最大似然估计器（MLE），并证明了其行为的两种现象，这些现象取决于噪声水平、范数约束半径、维度和范数指数 $p$。研究还通过构造性的下界证明了 MLE 次优的情况，并提供了 MLE 明显导致次优风险的显式实例。", "result": "1. 当 $p$ 接近 $1$ 或至少 $2$ 时，MLE 在所有噪声水平和所有约束半径下都是最小最大率最优的。\n2. 当 $p$ 介于 $1$ 和 $2$ 之间时，对于几乎所有需要非线性估计的噪声水平和半径，尽管 MLE 在观测值中是非线性的，但它在最小最大率上是次优的。\n3. 当给定 $n$ 个独立同分布的高斯样本时，MLE 可能比最优估计器次优一个多项式因子。\n4. 提供了当 MLE 率次优时，MLE 风险明显次优的显式实例。", "conclusion": "本文研究表明，在 $\\ell_p$ 球上的均值估计问题中，最大似然估计器（MLE）并非总是最优的。特别是在 $p$ 介于 $1$ 和 $2$ 之间时，MLE 即使是非线性的，其性能也可能远低于最小最大最优率，这挑战了其普遍最优性的假设。", "translation": "我们重新审视了加性高斯噪声下 $\\ell_p$ 球上的均值估计问题。当 $p$ 严格小于 $2$ 时，众所周知，率最优估计器必须是非线性的。在这项工作中，我们研究了最大似然估计器（MLE），它可以被视为 $\\ell_p$ 球上均值估计的一种非线性收缩过程。我们展示了 MLE 行为的两种现象，这取决于噪声水平、范数约束半径、维度和范数指数 $p$。首先，作为维度的一个函数，当 $p$ 接近 $1$ 或至少 $2$ 时，MLE 对于所有噪声水平和所有约束半径都是最小最大率最优的。另一方面，当 $p$ 介于 $1$ 和 $2$ 之间时，存在一种更显著的行为：对于几乎所有需要非线性估计的噪声水平和半径，尽管 MLE 在观测值中是非线性的，但它在最小最大率上是次优的。我们的结果也意味着当给定 $n$ 个独立同分布的高斯样本时，有类似的结论，我们证明了 MLE 在样本量上可能次优一个多项式因子。我们的下界是构造性的：无论何时 MLE 率次优，我们都提供了 MLE 明显导致次优风险的显式实例。", "summary": "本文研究了在加性高斯噪声下 $\\ell_p$ 球上的均值估计问题，重点分析了最大似然估计器（MLE）的性能。研究发现，MLE 的最优性取决于范数指数 $p$。具体而言，当 $p$ 接近 $1$ 或至少 $2$ 时，MLE 达到最小最大率最优；然而，当 $p$ 介于 $1$ 和 $2$ 之间时，MLE 即使是非线性的，也表现出最小最大率次优性。此外，对于独立同分布的高斯样本情况，MLE 也可能次优一个多项式因子。研究通过构造性下界提供了 MLE 次优的实例。", "keywords": "均值估计, $\\ell_p$ 球, 最大似然估计器, 最小最大率, 非线性估计", "comments": "该论文对最大似然估计器（MLE）在 $\\ell_p$ 球均值估计中的普遍最优性提出了质疑。其创新之处在于揭示了 MLE 在特定 $p$ 范围（$p \\in (1,2)$）内的次优行为，这对于理解 MLE 的局限性及其在非线性估计问题中的适用性具有重要意义。该研究不仅提供了理论分析，还通过构造性下界给出了明确的证据，增强了结果的说服力。"}}
{"id": "2506.10589", "title": "Transient performance of MPC for tracking without terminal constraints", "authors": ["Nadine Ehmann", "Matthias Köhler", "Frank Allgöwer"], "summary": "Model predictive control (MPC) for tracking is a recently introduced\napproach, which extends standard MPC formulations by incorporating an\nartificial reference as an additional optimization variable, in order to track\nexternal and potentially time-varying references. In this work, we analyze the\nperformance of such an MPC for tracking scheme without a terminal cost and\nterminal constraints. We derive a transient performance estimate, i.e. a bound\non the closed-loop performance over an arbitrary time interval, yielding\ninsights on how to select the scheme's parameters for performance. Furthermore,\nwe show that in the asymptotic case, where the prediction horizon and observed\ntime interval tend to infinity, the closed-loop solution of MPC for tracking\nrecovers the infinite horizon optimal solution.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10589v1", "AI": {"title_translation": "无终端约束跟踪MPC的瞬态性能", "tldr": "本文分析了无终端成本和终端约束的跟踪MPC方案的瞬态性能，并推导了其性能估计，同时证明了在预测范围趋于无穷大时，该方案能恢复无限范围最优解。", "motivation": "分析一种新引入的跟踪模型预测控制（MPC）方法的性能，该方法通过引入人工参考扩展了标准MPC，以跟踪外部和可能时变参考，特别是在没有终端成本和终端约束的情况下。", "method": "通过理论分析，推导了在任意时间间隔内闭环性能的瞬态估计（即边界），并研究了当预测范围和观测时间间隔趋于无穷大时的渐近情况。", "result": "推导出了闭环性能在任意时间间隔上的瞬态性能估计（边界），为选择方案参数提供了指导。此外，证明了在渐近情况下，跟踪MPC的闭环解决方案能够恢复无限范围最优解。", "conclusion": "在没有终端成本和终端约束的情况下，跟踪MPC方案的瞬态性能可以被估计，并且在预测范围足够大时，其渐近性能与无限范围最优解一致。", "translation": "跟踪模型预测控制（MPC）是一种最近引入的方法，它通过将人工参考作为额外的优化变量，扩展了标准MPC公式，以跟踪外部和可能时变参考。在这项工作中，我们分析了这种没有终端成本和终端约束的跟踪MPC方案的性能。我们推导了一个瞬态性能估计，即在任意时间间隔内闭环性能的边界，从而为如何选择方案参数以获得良好性能提供了见解。此外，我们还表明，在渐近情况下，当预测范围和观测时间间隔趋于无穷大时，跟踪MPC的闭环解决方案能够恢复无限范围最优解。", "summary": "本文研究了无终端成本和终端约束的跟踪模型预测控制（MPC）的瞬态性能。作者推导了任意时间间隔内闭环性能的瞬态估计，为参数选择提供了依据。研究还表明，在预测范围和观测时间间隔趋于无穷大时，该方案的闭环解可以恢复无限范围最优解。", "keywords": "模型预测控制, 跟踪MPC, 瞬态性能, 终端约束, 渐近分析", "comments": "本文对跟踪MPC在无终端约束条件下的瞬态性能进行了深入分析，提供了重要的理论保证，特别是其与无限范围最优解的渐近一致性，这对于实际应用中MPC参数的选择和性能评估具有指导意义。"}}
{"id": "2506.10705", "title": "A Novel Signal Processing Strategy for Short-Range Laser Feedback Interferometry Sensors", "authors": ["Alexander Zimmer", "Johannes Meyer", "Enkelejda Kasneci"], "summary": "The rapid evolution of wearable technologies, such as AR glasses, demands\ncompact, energy-efficient sensors capable of high-precision measurements in\ndynamic environments. Traditional Frequency-Modulated Continuous Wave (FMCW)\nLaser Feedback Interferometry (LFI) sensors, while promising, falter in\napplications that feature small distances, high velocities, shallow modulation,\nand low-power constraints. We propose a novel sensor-processing pipeline that\nreliably extracts distance and velocity measurements at distances as low as 1\ncm. As a core contribution, we introduce a four-ramp modulation scheme that\nresolves persistent ambiguities in beat frequency signs and overcomes spectral\nblind regions caused by hardware limitations. Based on measurements of the\nimplemented pipeline, a noise model is defined to evaluate its performance and\nsensitivity to several algorithmic and working point parameters. We show that\nthe pipeline generally achieves robust and low-noise measurements using\nstate-of-the-art hardware.", "comment": "Accepted to the 2025 25th International Conference on Digital Signal\n  Processing", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10705v1", "AI": {"title_translation": "短距离激光反馈干涉传感器的新型信号处理策略", "tldr": "本文提出了一种新颖的信号处理策略，包括四斜坡调制方案，用于短距离激光反馈干涉（LFI）传感器，以解决传统FMCW LFI在近距离、高速和低功耗应用中的局限性，实现了可靠、低噪声的距离和速度测量。", "motivation": "可穿戴技术（如AR眼镜）的快速发展需要紧凑、节能且能在动态环境中进行高精度测量的传感器。传统的调频连续波（FMCW）激光反馈干涉（LFI）传感器在小距离、高速度、浅调制和低功耗的应用中表现不佳。", "method": "本文提出了一种新颖的传感器处理流程，能够可靠地提取低至1厘米的距离和速度测量值。核心贡献是引入了一种四斜坡调制方案，解决了拍频符号中持续存在的模糊性，并克服了由硬件限制引起的频谱盲区。基于已实现的流程测量，定义了一个噪声模型来评估其性能和对多个算法及工作点参数的敏感性。", "result": "该处理流程能够使用最先进的硬件实现鲁棒且低噪声的测量。定义了噪声模型来评估其性能和对多种算法及工作点参数的敏感性。", "conclusion": "该新型信号处理策略，特别是四斜坡调制方案，能够使短距离激光反馈干涉传感器在具有挑战性的应用场景中实现可靠、鲁棒且低噪声的距离和速度测量，克服了传统方法的局限性。", "translation": "可穿戴技术（如AR眼镜）的快速发展，要求紧凑、节能且能在动态环境中进行高精度测量的传感器。传统的调频连续波（FMCW）激光反馈干涉（LFI）传感器虽然很有前景，但在小距离、高速度、浅调制和低功耗的应用中表现不佳。我们提出了一种新颖的传感器处理流程，能够可靠地提取低至1厘米的距离和速度测量值。作为核心贡献，我们引入了一种四斜坡调制方案，解决了拍频符号中持续存在的模糊性，并克服了由硬件限制引起的频谱盲区。基于已实现的流程测量，定义了一个噪声模型来评估其性能和对多个算法及工作点参数的敏感性。我们表明，该流程通常使用最先进的硬件实现了鲁棒且低噪声的测量。", "summary": "本文针对可穿戴技术对紧凑型高精度传感器的需求，提出了一种新型的激光反馈干涉（LFI）传感器信号处理策略。该策略旨在克服传统FMCW LFI在短距离、高速和低功耗应用中的局限性。核心创新在于引入了四斜坡调制方案，有效解决了拍频符号模糊性并消除了频谱盲区。实验结果表明，该处理流程能够实现可靠、鲁棒且低噪声的距离和速度测量，适用于低至1厘米的距离。", "keywords": "激光反馈干涉, 信号处理, 短距离传感器, 四斜坡调制, 可穿戴技术", "comments": "该论文提出了一种创新的四斜坡调制方案，解决了短距离激光反馈干涉传感器在实际应用中遇到的关键问题，如拍频符号模糊性和频谱盲区。这对于推动紧凑型、高精度传感器在可穿戴设备等领域的应用具有重要意义。其贡献在于不仅提出了新方法，还通过噪声模型进行了性能评估，显示了其在鲁棒性和低噪声方面的优势。"}}
{"id": "2506.10165", "title": "The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset", "authors": ["Gilad Landau", "Miran Özdogan", "Gereon Elvers", "Francesco Mantegna", "Pratik Somaiya", "Dulhan Jayalath", "Luisa Kurth", "Teyun Kwon", "Brendan Shillingford", "Greg Farquhar", "Minqi Jiang", "Karim Jerbi", "Hamza Abdelhedi", "Yorguin Mantilla Ramos", "Caglar Gulcehre", "Mark Woolrich", "Natalie Voets", "Oiwi Parker Jones"], "summary": "The advance of speech decoding from non-invasive brain data holds the\npotential for profound societal impact. Among its most promising applications\nis the restoration of communication to paralysed individuals affected by speech\ndeficits such as dysarthria, without the need for high-risk surgical\ninterventions. The ultimate aim of the 2025 PNPL competition is to produce the\nconditions for an \"ImageNet moment\" or breakthrough in non-invasive neural\ndecoding, by harnessing the collective power of the machine learning community.\n  To facilitate this vision we present the largest within-subject MEG dataset\nrecorded to date (LibriBrain) together with a user-friendly Python library\n(pnpl) for easy data access and integration with deep learning frameworks. For\nthe competition we define two foundational tasks (i.e. Speech Detection and\nPhoneme Classification from brain data), complete with standardised data splits\nand evaluation metrics, illustrative benchmark models, online tutorial code, a\ncommunity discussion board, and public leaderboard for submissions. To promote\naccessibility and participation the competition features a Standard track that\nemphasises algorithmic innovation, as well as an Extended track that is\nexpected to reward larger-scale computing, accelerating progress toward a\nnon-invasive brain-computer interface for speech.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10165v1", "AI": {"title_translation": "2025年PNPL竞赛：LibriBrain数据集中的语音检测和音素分类", "tldr": "2025年PNPL竞赛旨在通过提供LibriBrain数据集和pnpl库，促进非侵入性脑数据语音解码的突破，主要任务是语音检测和音素分类。", "motivation": "该研究旨在通过非侵入性脑数据实现语音解码的突破，以帮助患有言语障碍的瘫痪个体恢复交流，避免高风险手术。2025年PNPL竞赛的最终目标是激发机器学习社区的力量，实现非侵入性神经解码的“ImageNet时刻”。", "method": "竞赛提供了迄今为止最大的受试者内MEG数据集（LibriBrain）和一个用户友好的Python库（pnpl），以便于数据访问和与深度学习框架集成。竞赛定义了两个基础任务：语音检测和音素分类，并提供了标准化的数据分割、评估指标、基准模型、在线教程代码、社区讨论板和公共排行榜。竞赛设有强调算法创新的标准赛道和奖励大规模计算的扩展赛道。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "从非侵入性脑数据中解码语音的进展，具有产生深远社会影响的潜力。其最有前景的应用之一是为受言语障碍（如构音障碍）影响的瘫痪个体恢复交流，而无需高风险的手术干预。2025年PNPL竞赛的最终目标是通过利用机器学习社区的集体力量，为非侵入性神经解码创造一个“ImageNet时刻”或突破性进展的条件。\n为了促进这一愿景，我们提供了迄今为止记录到的最大的受试者内MEG数据集（LibriBrain），以及一个用户友好的Python库（pnpl），以便于数据访问和与深度学习框架集成。针对竞赛，我们定义了两个基础任务（即从脑数据中进行语音检测和音素分类），并提供了标准化的数据分割和评估指标、说明性的基准模型、在线教程代码、社区讨论板以及用于提交的公共排行榜。为了提高可访问性和参与度，竞赛设有一个强调算法创新的标准赛道，以及一个有望奖励大规模计算的扩展赛道，以加速非侵入性脑机接口在语音方面的进展。", "summary": "2025年PNPL竞赛旨在推动非侵入性脑数据语音解码领域的突破，特别是为了帮助瘫痪患者恢复交流。竞赛提供了迄今最大的受试者内MEG数据集LibriBrain和一个Python库pnpl，便于研究人员进行数据处理和深度学习集成。竞赛设定了语音检测和音素分类两大任务，并提供全面的支持资源，包括数据分割、评估指标和基准模型。通过设置标准和扩展赛道，竞赛鼓励算法创新和大规模计算，以加速非侵入性语音脑机接口的发展。", "keywords": "PNPL竞赛, 语音解码, 脑机接口, LibriBrain, 音素分类", "comments": "这项PNPL竞赛具有巨大的社会意义，因为它旨在通过非侵入性手段解决瘫痪患者的交流障碍。提供大规模的LibriBrain数据集和易用的pnpl库是其创新之处，能够有效降低参与门槛并促进社区协作。竞赛通过设置明确的任务和评估标准，有望加速该领域的进展，并可能产生类似“ImageNet时刻”的突破。"}}
{"id": "2506.10384", "title": "NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War", "authors": ["Jim O'Connor", "Yeonghun Lee", "Gary B Parker"], "summary": "StarCraft: Brood War remains a challenging benchmark for artificial\nintelligence research, particularly in the domain of macromanagement, where\nlong-term strategic planning is required. Traditional approaches to StarCraft\nAI rely on rule-based systems or supervised deep learning, both of which face\nlimitations in adaptability and computational efficiency. In this work, we\nintroduce NeuroPAL, a neuroevolutionary framework that integrates\nNeuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning\n(PAL) to improve the efficiency of evolutionary training. By alternating\nbetween frequent, low-fidelity training and periodic, high-fidelity\nevaluations, PAL enhances the sample efficiency of NEAT, enabling agents to\ndiscover effective strategies in fewer training iterations. We evaluate\nNeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and\ncompare its performance to standard NEAT-based training. Our results show that\nPAL significantly accelerates the learning process, allowing the agent to reach\ncompetitive levels of play in approximately half the training time required by\nNEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as\nproxy barracks placement and defensive building optimization, strategies\ncommonly used by expert human players. These findings suggest that structured\nevaluation mechanisms like PAL can enhance the scalability and effectiveness of\nneuroevolution in complex real-time strategy environments.", "comment": "IEEE Conference on Games 2025", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10384v1", "AI": {"title_translation": "NeuroPAL：基于神经进化的即时学习在《星际争霸：母巢之战》宏观管理中的应用", "tldr": "NeuroPAL结合神经进化和即时学习，显著加速了《星际争霸：母巢之战》AI的宏观管理训练，使其在更短时间内达到专家级表现。", "motivation": "《星际争霸：母巢之战》的宏观管理对AI研究仍具挑战，传统方法（基于规则或监督深度学习）在适应性和计算效率上存在局限性。", "method": "本文引入了NeuroPAL，一个结合了增强拓扑神经进化（NEAT）和即时学习（PAL）的神经进化框架。PAL通过频繁的低保真训练和周期性的高保真评估交替进行，提高了NEAT的样本效率。", "result": "NeuroPAL在《星际争霸：母巢之战》中，将代理达到竞争水平所需训练时间缩短了约一半，并且进化出的代理展现出专家级行为，如前置兵营和防御建筑优化。", "conclusion": "结构化评估机制（如PAL）可以增强神经进化在复杂实时战略环境中的可扩展性和有效性。", "translation": "《星际争霸：母巢之战》仍然是人工智能研究的一个具有挑战性的基准，特别是在需要长期战略规划的宏观管理领域。传统的星际争霸AI方法依赖于基于规则的系统或监督式深度学习，这两种方法在适应性和计算效率方面都面临局限性。在这项工作中，我们引入了NeuroPAL，这是一个神经进化框架，它将增强拓扑神经进化（NEAT）与即时学习（PAL）相结合，以提高进化训练的效率。通过在频繁的低保真训练和周期性的高保真评估之间交替进行，PAL提高了NEAT的样本效率，使代理能够在更少的训练迭代中发现有效的策略。我们在《星际争霸：母巢之战》的固定地图、单一种族场景中评估了NeuroPAL，并将其性能与标准的基于NEAT的训练进行了比较。我们的结果表明，PAL显著加速了学习过程，使代理在大约一半的训练时间内达到具有竞争力的游戏水平，而单独使用NEAT则需要更长的时间。此外，进化的代理展现出诸如前置兵营放置和防御建筑优化等新兴行为，这些策略是人类专家玩家常用的。这些发现表明，像PAL这样的结构化评估机制可以增强神经进化在复杂实时战略环境中的可扩展性和有效性。", "summary": "本文提出了NeuroPAL，一个结合了增强拓扑神经进化（NEAT）和即时学习（PAL）的神经进化框架，旨在解决《星际争霸：母巢之战》宏观管理中传统AI方法的局限性。NeuroPAL通过交替进行低保真训练和高保真评估，显著提高了训练效率，使AI代理在约一半的时间内达到竞争水平，并展现出专家级策略。研究表明，PAL等结构化评估机制能有效提升神经进化在复杂RTS环境中的表现。", "keywords": "神经进化, 即时学习, 星际争霸, 宏观管理, 人工智能", "comments": "本文的创新点在于将即时学习（PAL）与神经进化（NEAT）相结合，有效解决了传统神经进化训练效率低下的问题，显著加速了AI在复杂RTS游戏中的学习过程。其重要性在于为实时战略游戏AI的宏观管理提供了一种更高效、更具适应性的训练范式。"}}
{"id": "2506.10832", "title": "A novel visual data-based diagnostic approach for estimation of regime transition in pool boiling", "authors": ["Pranay Nirapure", "Ayushman Singh", "Srikanth Rangarajan", "Bahgat Sammakia"], "summary": "This study introduces a novel metric, the Index of Visual Similarity (IVS),\nto qualitatively characterize boiling heat transfer regimes using only visual\ndata. The IVS is constructed by combining morphological similarity, through\nSIFT-based feature matching, with physical similarity, via vapor area\nestimation using Mask R-CNN. High-speed images of pool boiling on two distinct\nsurfaces, polished copper and porous copper foam, are employed to demonstrate\nthe generalizability of the approach. IVS captures critical changes in bubble\nshape, size, and distribution that correspond to transitions in heat transfer\nmechanisms. The metric is validated against an equivalent metric, $\\Phi$,\nderived from measured heat transfer coefficients (HTC), showing strong\ncorrelation and reliability in detecting boiling regime transitions, including\nthe onset of nucleate boiling and proximity to critical heat flux (CHF). Given\nexperimental limitations in precisely measuring changes in HTC, the sensitivity\nof IVS to surface superheat is also examined to reinforce the credibility of\nIVS. IVS thus emerges as a powerful, rapid, and non-intrusive tool for\nreal-time, image-based boiling diagnostics, with promising applications in\nphase change heat transfer.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10832v1", "AI": {"title_translation": "池沸腾状态转换估计的一种新型视觉数据诊断方法", "tldr": "提出了一种基于图像的新度量IVS，结合SIFT和Mask R-CNN，用于快速、非侵入性地诊断池沸腾状态转换，并与传统HTC方法验证，显示出良好的相关性和可靠性。", "motivation": "传统的热传递系数 (HTC) 测量存在实验局限性，难以精确测量其变化，因此需要一种基于视觉数据的替代方法来定性表征沸腾传热状态。", "method": "引入了一种新的度量标准“视觉相似性指数 (IVS)”，它结合了基于SIFT特征匹配的形态相似性与使用Mask R-CNN进行蒸汽面积估计的物理相似性。该方法利用抛光铜和多孔泡沫铜两种表面上的池沸腾高速图像进行演示和验证。", "result": "IVS能够捕捉气泡形状、大小和分布的关键变化，这些变化与传热机制的转换相对应。IVS与从测量热传递系数 (HTC) 导出的等效度量$\\\\Phi$进行了验证，显示出很强的相关性和在检测沸腾状态转换（包括核态沸腾的开始和接近临界热流密度 (CHF)）方面的可靠性。IVS对表面过热度的敏感性也得到了检验，以增强其可信度。", "conclusion": "IVS是一种强大、快速、非侵入性的实时图像沸腾诊断工具，在相变传热领域具有广阔的应用前景。", "translation": "这项研究引入了一种新的度量标准，即视觉相似性指数（IVS），仅利用视觉数据定性表征沸腾传热状态。IVS通过结合基于SIFT特征匹配的形态相似性与使用Mask R-CNN进行蒸汽面积估计的物理相似性而构建。研究采用抛光铜和多孔泡沫铜两种不同表面上的池沸腾高速图像，以证明该方法的普适性。IVS捕捉气泡形状、大小和分布的关键变化，这些变化与传热机制的转换相对应。该度量标准通过与从测量热传递系数（HTC）导出的等效度量$\\Phi$进行验证，结果显示在检测沸腾状态转换（包括核态沸腾的开始和接近临界热流密度（CHF））方面具有很强的相关性和可靠性。鉴于精确测量HTC变化的实验局限性，研究还检验了IVS对表面过热度的敏感性，以增强IVS的可信度。因此，IVS作为一种强大、快速、非侵入性的实时图像沸腾诊断工具应运而生，在相变传热领域具有广阔的应用前景。", "summary": "本文提出了一种名为视觉相似性指数 (IVS) 的新型视觉数据诊断方法，用于估计池沸腾状态转换。IVS结合了基于SIFT的形态相似性和基于Mask R-CNN的蒸汽面积估计，能够利用高速图像捕捉气泡变化并识别沸腾传热状态转换。实验结果表明，IVS与传统的HTC测量方法具有高度相关性，证明其在实时、非侵入性沸腾诊断方面的可靠性和潜力。", "keywords": "池沸腾, 视觉诊断, 状态转换, IVS, 图像处理", "comments": "这项研究的创新之处在于提出了一个纯粹基于视觉数据的新度量 (IVS)，克服了传统热传递系数测量在精确性和实时性上的局限。通过结合计算机视觉技术 (SIFT, Mask R-CNN) 与物理现象分析，实现了对复杂沸腾状态转换的有效识别。其非侵入性和实时性使其在工业应用中具有重要价值，尤其是在需要连续监测和快速诊断的场景。"}}
{"id": "2506.10112", "title": "NnD: Diffusion-based Generation of Physically-Nonnegative Objects", "authors": ["Nadav Torem", "Tamar Sde-Chen", "Yoav Y. Schechner"], "summary": "Most natural objects have inherent complexity and variability. While some\nsimple objects can be modeled from first principles, many real-world phenomena,\nsuch as cloud formation, require computationally expensive simulations that\nlimit scalability. This work focuses on a class of physically meaningful,\nnonnegative objects that are computationally tractable but costly to simulate.\nTo dramatically reduce computational costs, we propose nonnegative diffusion\n(NnD). This is a learned generative model using score based diffusion. It\nadapts annealed Langevin dynamics to enforce, by design, non-negativity\nthroughout iterative scene generation and analysis (inference). NnD trains on\nhigh-quality physically simulated objects. Once trained, it can be used for\ngeneration and inference. We demonstrate generation of 3D volumetric clouds,\ncomprising inherently nonnegative microphysical fields. Our generated clouds\nare consistent with cloud physics trends. They are effectively not\ndistinguished as non-physical by expert perception.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10112v1", "AI": {"title_translation": "NnD：基于扩散的物理非负对象生成", "tldr": "NnD是一种基于扩散的生成模型，能够高效生成物理上非负的对象，如3D云，且生成结果符合物理规律。", "motivation": "许多真实世界的现象，如云形成，需要计算成本高昂的模拟，限制了可扩展性。本研究关注于一类计算上可行但模拟成本高昂的、物理上有意义的非负对象，旨在显著降低计算成本。", "method": "本文提出了非负扩散（NnD），这是一种使用基于分数的扩散的生成模型。它通过调整退火朗之万动力学，在迭代场景生成和分析（推理）过程中强制执行非负性。NnD在高质量的物理模拟对象上进行训练。", "result": "NnD能够生成3D体积云，其中包含固有的非负微物理场。生成的云与云物理趋势一致，并且专家感知上无法将其区分为非物理的。", "conclusion": "NnD模型能够有效生成符合物理规律的非负对象，显著降低了传统模拟的计算成本，并生成了高质量、逼真的物理现象。", "translation": "大多数自然对象具有固有的复杂性和可变性。虽然一些简单的对象可以从第一性原理建模，但许多现实世界现象，如云形成，需要计算成本高昂的模拟，这限制了可扩展性。这项工作专注于一类物理上有意义的、非负的对象，它们在计算上是可行的但模拟成本高昂。为了显著降低计算成本，我们提出了非负扩散（NnD）。这是一种使用基于分数的扩散的学习型生成模型。它通过调整退火朗之万动力学，在迭代场景生成和分析（推理）过程中，通过设计强制执行非负性。NnD在高质量的物理模拟对象上进行训练。一旦训练完成，它就可以用于生成和推理。我们展示了3D体积云的生成，其中包括固有的非负微物理场。我们生成的云与云物理趋势一致。专家感知上无法有效地将其区分为非物理的。", "summary": "本研究提出了一种名为NnD（非负扩散）的生成模型，旨在解决复杂物理非负对象（如云形成）模拟计算成本高昂的问题。NnD是一种基于分数扩散的学习型生成模型，通过调整退火朗之万动力学，在生成和推理过程中强制保持非负性。该模型在高质量物理模拟数据上训练，并成功应用于3D体积云的生成。实验结果表明，NnD生成的云与云物理趋势一致，且专家难以区分其非物理性，证明了其在高效生成物理上准确的非负对象方面的有效性。", "keywords": "非负扩散, 生成模型, 物理模拟, 3D云, 机器学习", "comments": "NnD的创新之处在于其通过设计强制执行非负性的扩散模型，这对于生成物理上准确的对象至关重要。它提供了一种显著降低计算成本的方法，同时保持了生成对象的物理一致性，对于需要模拟复杂物理现象的领域具有重要意义。"}}
{"id": "2506.10859", "title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated Global Context Information", "authors": ["Kehan Long", "Shasha Li", "Chen Xu", "Jintao Tang", "Ting Wang"], "summary": "Recent advancements have successfully harnessed the power of Large Language\nModels (LLMs) for zero-shot document ranking, exploring a variety of prompting\nstrategies. Comparative approaches like pairwise and listwise achieve high\neffectiveness but are computationally intensive and thus less practical for\nlarger-scale applications. Scoring-based pointwise approaches exhibit superior\nefficiency by independently and simultaneously generating the relevance scores\nfor each candidate document. However, this independence ignores critical\ncomparative insights between documents, resulting in inconsistent scoring and\nsuboptimal performance. In this paper, we aim to improve the effectiveness of\npointwise methods while preserving their efficiency through two key\ninnovations: (1) We propose a novel Global-Consistent Comparative Pointwise\nRanking (GCCP) strategy that incorporates global reference comparisons between\neach candidate and an anchor document to generate contrastive relevance scores.\nWe strategically design the anchor document as a query-focused summary of\npseudo-relevant candidates, which serves as an effective reference point by\ncapturing the global context for document comparison. (2) These contrastive\nrelevance scores can be efficiently Post-Aggregated with existing pointwise\nmethods, seamlessly integrating essential Global Context information in a\ntraining-free manner (PAGC). Extensive experiments on the TREC DL and BEIR\nbenchmark demonstrate that our approach significantly outperforms previous\npointwise methods while maintaining comparable efficiency. Our method also\nachieves competitive performance against comparative methods that require\nsubstantially more computational resources. More analyses further validate the\nefficacy of our anchor construction strategy.", "comment": "Accepted by SIGIR 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10859v1", "AI": {"title_translation": "通过后聚合全局上下文信息实现LLM精准零样本点式排序", "tldr": "本文提出了一种名为全局一致比较点式排序（GCCP）的新策略，并通过后聚合全局上下文信息（PAGC）的方法，显著提升了LLM点式排序的有效性，同时保持了其高效率。", "motivation": "现有基于LLM的点式排序方法虽然效率高，但由于独立生成分数而忽略了文档间的关键比较性见解，导致评分不一致和次优性能。而比较性方法（如成对和列表式）虽然有效，但计算成本高昂，不适用于大规模应用。", "method": "本文提出了两项关键创新来改进点式方法的有效性同时保持其效率：(1) 提出了一种新颖的全局一致比较点式排序（GCCP）策略，该策略通过将每个候选文档与一个锚点文档（作为查询聚焦的伪相关候选摘要）进行全局参考比较来生成对比性相关性分数，从而捕获文档比较的全局上下文。(2) 这些对比性相关性分数可以通过后聚合（PAGC）的方式与现有SOTA点式方法高效整合，以无训练的方式无缝集成必要的全局上下文信息。", "result": "在TREC DL和BEIR基准测试上的大量实验表明，我们的方法显著优于以前的点式方法，同时保持了可比的效率。与需要大量计算资源的比较性方法相比，我们的方法也取得了有竞争力的性能。进一步的分析验证了锚点构建策略的有效性。", "conclusion": "本文提出的GCCP和PAGC方法通过巧妙地融入全局上下文信息，有效提升了LLM点式排序的性能，同时维持了其固有的高效率，使其成为大规模零样本排序的实用且有竞争力的解决方案。", "translation": "最近的进展成功地利用大型语言模型（LLM）进行零样本文档排序，探索了各种提示策略。成对和列表式等比较方法虽然效率高，但计算密集，因此不适用于大规模应用。基于评分的点式方法通过独立同时为每个候选文档生成相关性分数，表现出卓越的效率。然而，这种独立性忽略了文档之间关键的比较性见解，导致评分不一致和次优性能。在本文中，我们旨在通过两项关键创新来提高点式方法的有效性，同时保持其效率：(1) 我们提出了一种新颖的全局一致比较点式排序（GCCP）策略，该策略结合了每个候选文档与一个锚点文档之间的全局参考比较，以生成对比性相关性分数。我们将锚点文档战略性地设计为伪相关候选文档的查询聚焦摘要，通过捕获文档比较的全局上下文，作为有效的参考点。(2) 这些对比性相关性分数可以与现有SOTA点式方法高效地进行后聚合（PAGC），以无训练的方式无缝集成必要的全局上下文信息。在TREC DL和BEIR基准测试上的大量实验表明，我们的方法显著优于以前的点式方法，同时保持了可比的效率。我们的方法与需要大量计算资源的比较方法相比，也取得了有竞争力的性能。更多分析进一步验证了我们锚点构建策略的有效性。", "summary": "本文针对LLM点式排序方法效率高但性能受限的问题，提出了一种名为全局一致比较点式排序（GCCP）的新策略。该策略通过引入查询聚焦的锚点文档进行全局参考比较，生成对比性相关性分数，以捕获文档间的全局上下文。这些分数随后通过后聚合（PAGC）的方式与现有SOTA点式方法结合，无需训练即可提升性能。实验证明，该方法在保持高效率的同时，显著优于现有SOTA点式方法，并能与计算成本更高的比较性方法相媲美。", "keywords": "零样本排序, LLM, 点式排序, 全局上下文, 后聚合", "comments": "本文的创新点在于巧妙地解决了LLM点式排序中效率与效果之间的权衡问题。通过引入“锚点文档”和“后聚合”机制，在不牺牲点式方法效率的前提下，有效地融入了全局上下文信息，弥补了其性能瓶颈。这对于大规模零样本文档排序应用具有重要的实践意义和价值。"}}
{"id": "2506.10155", "title": "Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities", "authors": ["Elizabeth Demers", "Victor Xiaoqi Wang", "Kean Wu"], "summary": "Human capital (HC) is increasingly important to corporate value creation.\nUnlike other assets, however, HC is not currently subject to well-defined\nmeasurement or disclosure rules. We use a machine learning algorithm (word2vec)\ntrained on a confirmed set of HC disclosures to develop a comprehensive list of\nHC-related keywords classified into five subcategories (DEI; health and safety;\nlabor relations and culture; compensation and benefits; and demographics and\nother) that capture the multidimensional nature of HC management. We share our\nlexicon, corporate HC disclosures, and the Python code used to develop the\nlexicon, and we provide detailed examples of using our data and code, including\nfor fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the\ncode to capture another construct of interest) with their samples of corporate\ncommunications to address pertinent HC questions. We close with a discussion of\nfuture research opportunities related to HC management and disclosure.", "comment": "50 pages, 6 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10155v1", "AI": {"title_translation": "衡量企业人力资本披露：词典、数据、代码和研究机会", "tldr": "该研究开发了一个基于word2vec的全面人力资本相关关键词词典，并提供数据和代码，以帮助研究人员衡量和分析企业人力资本披露。", "motivation": "人力资本对于企业价值创造日益重要，但目前缺乏明确的衡量和披露规则。", "method": "研究团队使用机器学习算法（word2vec）对已确认的人力资本披露进行训练，开发了一个包含五个人力资本子类别（DEI；健康与安全；劳资关系与文化；薪酬与福利；人口统计及其他）的综合关键词列表。", "result": "研究成果包括一个细致分类的人力资本关键词词典、企业人力资本披露数据以及用于开发词典的Python代码，并提供了使用这些数据和代码的详细示例，包括微调BERT模型。", "conclusion": "本研究为研究人员提供了用于分析企业人力资本披露的工具、数据和代码，以解决相关人力资本问题，并讨论了未来的人力资本管理和披露研究机会。", "translation": "人力资本（HC）对于企业价值创造日益重要。然而，与其它资产不同，人力资本目前没有明确的衡量或披露规则。我们使用一个在已确认的人力资本披露集上训练的机器学习算法（word2vec）来开发一份全面的人力资本相关关键词列表，这些关键词被分为五个子类别（DEI；健康与安全；劳资关系与文化；薪酬与福利；人口统计及其他），以捕捉人力资本管理的多维性质。我们分享了我们的词典、企业人力资本披露数据以及用于开发词典的Python代码，并提供了使用我们的数据和代码的详细示例，包括用于微调BERT模型。研究人员可以使用我们的人力资本词典（或修改代码以捕获其他感兴趣的构建体）及其企业沟通样本来解决相关的人力资本问题。最后，我们讨论了与人力资本管理和披露相关的未来研究机会。", "summary": "本研究旨在解决人力资本披露缺乏标准衡量规则的问题。作者利用word2vec机器学习算法，基于已确认的人力资本披露数据，开发了一个包含五大子类别的全面人力资本关键词词典。研究不仅提供了该词典，还分享了相关的企业人力资本披露数据和Python代码，并演示了其在分析中的应用，包括微调BERT模型。这项工作为研究人员提供了宝贵的资源和工具，以更好地衡量和分析企业人力资本，并探讨了未来的研究方向。", "keywords": "人力资本, 披露, 词典, word2vec, 机器学习", "comments": "这项研究创新性地运用机器学习方法构建了人力资本披露的标准化词典，并开放了数据和代码，极大地降低了研究人员在此领域进行定量分析的门槛。其重要性在于为企业人力资本这一难以量化的资产提供了可操作的衡量工具，有助于推动相关研究和实践。"}}
{"id": "2506.10173", "title": "SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score", "authors": ["Mohammad Jalali", "Haoyu Lei", "Amin Gohari", "Farzan Farnia"], "summary": "Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10173v1", "AI": {"title_translation": "SPARKE：通过RKE分数在扩散模型中实现可扩展的提示感知多样性引导", "tldr": "SPARKE提出了一种可扩展的、提示感知多样性引导方法，通过优化Rényi核熵（RKE）分数，在不显著增加计算成本的情况下提高扩散模型生成样本的多样性。", "motivation": "在提示引导的扩散模型中，确保生成样本的足够多样性仍然是一个挑战，尤其是在提示词语义范围广泛且需要以提示感知的方式评估多样性时。现有的基于多样性度量的方法在计算上存在挑战，特别是在大规模生成设置中。", "method": "本文提出了可扩展的提示感知Rényi核熵多样性引导（SPARKE）方法。SPARKE利用条件熵进行多样性引导，动态地根据相似提示调整多样性测量。为了解决大规模生成中基于矩阵的熵分数带来的计算挑战，SPARKE专注于条件潜在RKE分数引导的特殊情况，将熵计算和基于梯度的优化复杂度从O(n^3)降低到O(n)。", "result": "SPARKE方法在多个文本到图像扩散模型上进行了数值测试，结果表明该方法在不显著增加计算成本的情况下，提高了生成数据的提示感知多样性。", "conclusion": "SPARKE通过引入高效的RKE分数，成功解决了扩散模型中提示感知多样性引导的计算难题，从而在不增加高成本的情况下提高了生成多样性，使其适用于大规模生成场景。", "translation": "扩散模型在高质量图像合成和提示引导生成建模方面取得了显著成功。然而，确保提示引导扩散模型生成样本的足够多样性仍然是一个挑战，特别是当提示词跨越广泛的语义范围并且需要以提示感知的方式在语义相似的提示词之间评估生成数据的多样性时。最近的方法已经通过多样性度量引入了引导，以鼓励更多样化的生成。在这项工作中，我们通过提出可扩展的提示感知Rényi核熵多样性引导（SPARKE）方法来扩展基于多样性度量的方法，以实现提示感知多样性引导。SPARKE利用条件熵进行多样性引导，它动态地根据相似提示调整多样性测量，并实现提示感知多样性控制。虽然基于熵的引导方法增强了提示感知多样性，但其对基于矩阵的熵分数的依赖在大规模生成设置中带来了计算挑战。为了解决这个问题，我们专注于条件潜在RKE分数引导的特殊情况，将通用熵度量的O(n^3)的熵计算和基于梯度的优化复杂度降低到O(n)。计算复杂度的降低允许在不同提示词上进行数千轮的生成多样性引导采样。我们在几个文本到图像扩散模型上对SPARKE方法进行了数值测试，证明所提出的方法在不产生显著计算成本的情况下提高了生成数据的提示感知多样性。我们已在项目页面发布了代码：https://mjalali.github.io/SPARKE", "summary": "本研究提出SPARKE（可扩展的提示感知Rényi核熵多样性引导）方法，旨在解决扩散模型中提示引导生成样本多样性不足的问题。SPARKE通过利用条件熵实现提示感知多样性控制，并通过专注于条件潜在RKE分数引导，将计算复杂度从O(n^3)显著降低至O(n)。实验证明，SPARKE在不显著增加计算成本的情况下，有效提升了文本到图像扩散模型生成数据的提示感知多样性。", "keywords": "扩散模型, 多样性引导, 提示感知, RKE分数, 可扩展性", "comments": "该论文的创新之处在于，它通过引入条件潜在RKE分数引导，将提示感知多样性引导的计算复杂度从O(n^3)大幅降低到O(n)，从而解决了大规模生成场景中的计算瓶颈。这使得在保持生成质量的同时，能够更高效地提升生成样本的多样性，对扩散模型的实际应用具有重要意义。"}}
{"id": "2506.10509", "title": "A semi-Lagrangian scheme for First-Order Mean Field Games based on monotone operators", "authors": ["Elisabetta Carlini", "Valentina Coscetti"], "summary": "We construct a semi-Lagrangian scheme for first-order, time-dependent, and\nnon-local Mean Field Games. The convergence of the scheme to a weak solution of\nthe system is analyzed by exploiting a key monotonicity property. To solve the\nresulting discrete problem, we implement a Learning Value Algorithm, prove its\nconvergence, and propose an acceleration strategy based on a Policy iteration\nmethod. Finally, we present numerical experiments that validate the\neffectiveness of the proposed schemes and show that the accelerated version\nsignificantly improves performance.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10509v1", "AI": {"title_translation": "一种基于单调算子的一阶平均场博弈半拉格朗日格式", "tldr": "本文构建了一种用于一阶平均场博弈的半拉格朗日格式，分析了其收敛性，并提出了一种基于策略迭代的加速学习价值算法，数值实验验证了其有效性和性能提升。", "motivation": "本研究旨在构建并分析一种用于一阶、时变、非局部平均场博弈的数值格式，并开发解决离散问题的有效方法。", "method": "作者构建了一种用于平均场博弈的半拉格朗日格式，利用单调性分析其收敛性，并提出了一种结合策略迭代加速策略的学习价值算法来解决离散问题。", "result": "所提出的方案是有效的，并且加速版本显著提高了性能，这一点已通过数值实验得到验证。", "conclusion": "本研究成功开发了一种用于一阶平均场博弈的有效且高效的半拉格朗日格式，其中加速策略显著提升了性能。", "translation": "我们构建了一种用于一阶、时变、非局部平均场博弈的半拉格朗日格式。通过利用一个关键的单调性，分析了该格式对系统弱解的收敛性。为了解决由此产生的离散问题，我们实现了一个学习价值算法，证明了其收敛性，并提出了一种基于策略迭代方法的加速策略。最后，我们展示了数值实验，验证了所提出方案的有效性，并表明加速版本显著提高了性能。", "summary": "本文提出了一种用于一阶、时变、非局部平均场博弈的半拉格朗日格式。该方案的收敛性通过其单调性特性得到分析。为解决离散问题，研究人员实现并证明了学习价值算法的收敛性，并引入了一种基于策略迭代的加速策略。数值实验证实了所提方案的有效性，并显示加速版本显著提升了性能。", "keywords": "半拉格朗日格式, 平均场博弈, 单调算子, 学习价值算法, 策略迭代", "comments": "创新点在于提出了一个结合学习价值算法和策略迭代加速策略的半拉格朗日方案来解决一阶平均场博弈，并通过数值实验验证了其有效性和性能提升。该工作对平均场博弈的数值求解具有重要意义。"}}
{"id": "2506.10171", "title": "Disclosure Audits for LLM Agents", "authors": ["Saswat Das", "Jameson Sandler", "Ferdinando Fioretto"], "summary": "Large Language Model agents have begun to appear as personal assistants,\ncustomer service bots, and clinical aides. While these applications deliver\nsubstantial operational benefits, they also require continuous access to\nsensitive data, which increases the likelihood of unauthorized disclosures.\nThis study proposes an auditing framework for conversational privacy that\nquantifies and audits these risks. The proposed Conversational Manipulation for\nPrivacy Leakage (CMPL) framework, is an iterative probing strategy designed to\nstress-test agents that enforce strict privacy directives. Rather than focusing\nsolely on a single disclosure event, CMPL simulates realistic multi-turn\ninteractions to systematically uncover latent vulnerabilities. Our evaluation\non diverse domains, data modalities, and safety configurations demonstrate the\nauditing framework's ability to reveal privacy risks that are not deterred by\nexisting single-turn defenses. In addition to introducing CMPL as a diagnostic\ntool, the paper delivers (1) an auditing procedure grounded in quantifiable\nrisk metrics and (2) an open benchmark for evaluation of conversational privacy\nacross agent implementations.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10171v1", "AI": {"title_translation": "大型语言模型代理的披露审计", "tldr": "本文提出了一种名为CMPL的审计框架，通过模拟多轮交互来发现大型语言模型代理中的隐私漏洞，并证明其比现有的单轮防御更有效。", "motivation": "大型语言模型代理在处理敏感数据时存在未经授权披露的风险，尽管它们带来了显著的运营效益。因此，需要一个框架来量化和审计这些隐私风险。", "method": "本文提出了会话操纵隐私泄漏（CMPL）框架，这是一种迭代探测策略，通过模拟真实的多轮交互来对执行严格隐私指令的代理进行压力测试，以系统地发现潜在漏洞。此外，论文还引入了基于可量化风险指标的审计程序和一个开放的会话隐私评估基准。", "result": "评估结果表明，该审计框架能够揭示现有单轮防御无法阻止的隐私风险。", "conclusion": "本文介绍了CMPL作为一种诊断工具，并提供了一个基于可量化风险指标的审计程序以及一个用于评估不同代理实现中会话隐私的开放基准。", "translation": "大型语言模型代理已开始作为个人助理、客户服务机器人和临床助手出现。虽然这些应用程序带来了显著的运营效益，但它们也需要持续访问敏感数据，这增加了未经授权披露的可能性。本研究提出了一种会话隐私审计框架，用于量化和审计这些风险。所提出的会话操纵隐私泄漏（CMPL）框架是一种迭代探测策略，旨在对执行严格隐私指令的代理进行压力测试。CMPL不是仅仅关注单一披露事件，而是模拟真实的多轮交互，以系统地发现潜在漏洞。我们在不同领域、数据模态和安全配置上的评估表明，该审计框架能够揭示现有单轮防御无法阻止的隐私风险。除了将CMPL作为诊断工具引入外，本文还提供（1）基于可量化风险指标的审计程序和（2）用于评估代理实现中会话隐私的开放基准。", "summary": "本文提出了会话操纵隐私泄漏（CMPL）框架，这是一种迭代探测策略，旨在审计大型语言模型（LLM）代理中的隐私风险。与单轮防御不同，CMPL通过模拟多轮交互，系统地发现处理敏感数据的代理中潜在的隐私漏洞。评估结果表明，CMPL能够有效揭示现有防御机制无法解决的风险。该论文还提供了一个可量化的审计程序和一个用于会话隐私评估的开放基准。", "keywords": "LLM代理, 隐私, 审计框架, 会话隐私, CMPL", "comments": "这篇论文解决了LLM代理中隐私这一关键且及时的问题，该问题在敏感领域越来越突出。其创新之处在于采用多轮交互模拟（CMPL），这是一种比单轮防御更真实有效的方法。提供可量化风险指标和开放基准是对该领域的宝贵贡献，有助于推动安全LLM代理部署的进一步研究和发展。"}}
{"id": "2506.10397", "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "summary": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen's\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues.", "comment": "25 pages, 5 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10397v1", "AI": {"title_translation": "量子软件中的缺陷分类：一种基于规则的框架及其评估", "tldr": "本文提出了一个基于规则的自动化框架，用于对量子软件中的缺陷进行分类，并对其进行了评估。", "motivation": "准确分类软件缺陷对于提高软件质量至关重要。", "method": "本文提出了一个基于规则的自动化框架，用于按缺陷类型、类别、严重程度和受影响的质量属性对量子软件仓库中的问题进行分类，并额外关注量子特有的缺陷类型。该框架应用了针对量子计算定制的关键词和启发式技术。为了评估其可靠性，研究人员手动分类了来自36个Qiskit仓库的12,910个问题数据集中的4,984个分层样本。自动分类结果与真实情况通过准确率、精确率、召回率和F1分数进行比较，并使用配对t检验和Cohen's Kappa进行统计验证。", "result": "该框架的准确率高达85.21%，F1分数范围从0.7075（严重程度）到0.8393（质量属性）。统计验证显示，缺陷类型（k = 0.696）、类别（k = 0.826）、质量属性（k = 0.818）和量子特有缺陷类型（k = 0.712）的一致性达到实质性到几乎完美。严重程度分类的一致性较低（k = 0.162）。大规模分析显示，经典缺陷占主导地位（67.2%），量子特有缺陷占27.3%。常见的缺陷类别包括兼容性、功能性和量子特有缺陷，而可用性、可维护性和互操作性是受影响最严重的质量属性。大多数问题（93.7%）为低严重性；只有4.3%为关键性。对1,550个量子特有缺陷的详细审查显示，超过一半涉及量子电路级别问题，其次是门错误和硬件相关问题。", "conclusion": "该基于规则的框架能够有效地对量子软件中的缺陷进行分类，尤其在缺陷类型、类别和质量属性方面表现出高一致性。尽管在严重程度分类上仍有改进空间，但该研究为理解量子软件中缺陷的分布和特性提供了宝贵的见解，揭示了经典缺陷仍占主导地位，且量子特有缺陷主要集中在电路级别问题。", "translation": "准确分类软件缺陷对于提高软件质量至关重要。本文提出了一个基于规则的自动化框架，用于按缺陷类型、类别、严重程度和受影响的质量属性对量子软件仓库中的问题进行分类，并额外关注量子特有的缺陷类型。该框架应用了针对量子计算定制的关键词和启发式技术。为了评估其可靠性，我们手动分类了来自36个Qiskit仓库的12,910个问题数据集中的4,984个分层样本。自动分类结果与真实情况通过准确率、精确率、召回率和F1分数进行比较。该框架的准确率高达85.21%，F1分数范围从0.7075（严重程度）到0.8393（质量属性）。通过配对t检验和Cohen's Kappa进行的统计验证显示，缺陷类型（k = 0.696）、类别（k = 0.826）、质量属性（k = 0.818）和量子特有缺陷类型（k = 0.712）的一致性达到实质性到几乎完美。严重程度分类的一致性较低（k = 0.162），表明仍有改进空间。大规模分析显示，经典缺陷占主导地位（67.2%），量子特有缺陷占27.3%。常见的缺陷类别包括兼容性、功能性和量子特有缺陷，而可用性、可维护性和互操作性是受影响最严重的质量属性。大多数问题（93.7%）为低严重性；只有4.3%为关键性。对1,550个量子特有缺陷的详细审查显示，超过一半涉及量子电路级别问题，其次是门错误和硬件相关问题。", "summary": "本文提出了一种基于规则的自动化框架，用于对量子软件中的缺陷进行分类，包括缺陷类型、类别、严重程度和受影响的质量属性，并特别关注量子特有缺陷。该框架利用关键词和启发式技术，并在Qiskit仓库的真实数据集上进行了评估。结果显示，该框架在缺陷类型、类别和质量属性分类方面表现出较高的准确性和一致性，尽管在严重程度分类上仍有改进空间。研究还揭示了量子软件中经典缺陷的普遍性以及量子特有缺陷的主要类型和影响。", "keywords": "量子软件, 缺陷分类, 规则框架, Qiskit, 软件质量", "comments": "该论文的创新之处在于其首次提出了一个专门针对量子软件缺陷的自动化分类框架，并结合了量子计算的特性。其重要性体现在为量子软件质量保证提供了基础工具，并通过大规模分析揭示了量子软件中缺陷的分布和特性，这对于未来量子软件开发和测试具有指导意义。然而，论文也指出了在严重程度分类方面仍有提升空间，这可能需要更复杂的语义分析或上下文理解。"}}
{"id": "2506.10359", "title": "Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success", "authors": ["Che Wang", "Jeroen van Baar", "Chaitanya Mitash", "Shuai Li", "Dylan Randle", "Weiyao Wang", "Sumedh Sontakke", "Kostas E. Bekris", "Kapil Katyal"], "summary": "This work demonstrates how autonomously learning aspects of robotic operation\nfrom sparsely-labeled, real-world data of deployed, engineered solutions at\nindustrial scale can provide with solutions that achieve improved performance.\nSpecifically, it focuses on multi-suction robot picking and performs a\ncomprehensive study on the application of multi-modal visual encoders for\npredicting the success of candidate robotic picks. Picking diverse items from\nunstructured piles is an important and challenging task for robot manipulation\nin real-world settings, such as warehouses. Methods for picking from clutter\nmust work for an open set of items while simultaneously meeting latency\nconstraints to achieve high throughput. The demonstrated approach utilizes\nmultiple input modalities, such as RGB, depth and semantic segmentation, to\nestimate the quality of candidate multi-suction picks. The strategy is trained\nfrom real-world item picking data, with a combination of multimodal pretrain\nand finetune. The manuscript provides comprehensive experimental evaluation\nperformed over a large item-picking dataset, an item-picking dataset targeted\nto include partial occlusions, and a package-picking dataset, which focuses on\ncontainers, such as boxes and envelopes, instead of unpackaged items. The\nevaluation measures performance for different item configurations, pick scenes,\nand object types. Ablations help to understand the effects of in-domain\npretraining, the impact of different modalities and the importance of\nfinetuning. These ablations reveal both the importance of training over\nmultiple modalities but also the ability of models to learn during pretraining\nthe relationship between modalities so that during finetuning and inference,\nonly a subset of them can be used as input.", "comment": "Accepted to Robotics: Science and Systems (RSS 2025), 15 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10359v1", "AI": {"title_translation": "通过多模态学习抓取成功率实现大规模多吸盘物品抓取演示", "tldr": "本文通过多模态学习从真实世界数据中预测多吸盘机器人抓取成功率，从而实现了大规模、高性能的物品抓取。", "motivation": "机器人从非结构化堆叠中抓取多样化物品是一项重要的挑战性任务，尤其是在仓库等真实世界环境中，需要高吞吐量且适用于开放式物品集。", "method": "该方法利用多模态视觉编码器（包括RGB、深度和语义分割）来预测候选多吸盘抓取的成功率。该策略通过多模态预训练和微调的组合，从真实世界的物品抓取数据中进行训练。", "result": "实验证明了多模态训练的重要性，模型在预训练阶段能够学习模态之间的关系，从而在微调和推理时仅使用部分模态作为输入。研究在大型物品抓取数据集、包含部分遮挡的数据集以及专注于容器的包裹抓取数据集上进行了全面评估，并测量了不同物品配置、抓取场景和物体类型的性能。", "conclusion": "通过从稀疏标记的真实世界数据中自主学习机器人操作的各个方面，可以提供实现性能改进的解决方案，尤其是在工业规模的多吸盘物品抓取方面。", "translation": "这项工作展示了如何从部署在工业规模的工程解决方案的稀疏标记的真实世界数据中自主学习机器人操作的各个方面，从而提供性能改进的解决方案。具体而言，它专注于多吸盘机器人抓取，并对多模态视觉编码器在预测候选机器人抓取成功率方面的应用进行了全面研究。从非结构化堆叠中抓取多样化物品对于真实世界环境（如仓库）中的机器人操作来说是一项重要且具有挑战性的任务。从杂乱环境中抓取的方法必须适用于开放式物品集，同时满足延迟约束以实现高吞吐量。所演示的方法利用多种输入模态，例如RGB、深度和语义分割，来估计候选多吸盘抓取的质量。该策略通过多模态预训练和微调的组合，从真实世界的物品抓取数据中进行训练。手稿提供了在大型物品抓取数据集、旨在包含部分遮挡的物品抓取数据集以及专注于容器（如箱子和信封而非未包装物品）的包裹抓取数据集上进行的全面实验评估。评估测量了不同物品配置、抓取场景和物体类型的性能。消融实验有助于理解域内预训练的效果、不同模态的影响以及微调的重要性。这些消融实验揭示了多模态训练的重要性，以及模型在预训练期间学习模态之间关系的能力，以便在微调和推理时，仅使用其中的一部分作为输入。", "summary": "本研究展示了一种通过多模态学习实现大规模多吸盘物品抓取的方法。该方法利用RGB、深度和语义分割等多模态视觉编码器，从真实世界数据中学习并预测机器人抓取的成功率。通过多模态预训练和微调，模型能够处理非结构化环境中的多样化物品，并在大型数据集上进行了广泛评估。研究结果强调了多模态训练的关键作用，并表明模型在预训练后可仅使用部分模态进行高效推理，从而提高了工业规模机器人抓取任务的性能。", "keywords": "机器人抓取, 多模态学习, 吸盘抓取, 抓取成功预测, 工业自动化", "comments": "该论文的创新点在于将多模态学习应用于工业规模的多吸盘机器人抓取成功率预测，并利用大量真实世界数据进行训练和验证。其重要性在于解决了机器人从杂乱环境中抓取多样化物品的挑战，并通过实验证明了多模态训练的有效性以及在推理时使用部分模态的可能性，这对于实际部署具有显著意义。"}}
{"id": "2506.10891", "title": "(De)composing Craft: An Elementary Grammar for Sharing Expertise in Craft Workflows", "authors": ["Ritik Batra", "Lydia Kim", "Ilan Mandel", "Amritansh Kwatra", "Jane L. E.", "Steven J. Jackson", "Thijs Roumen"], "summary": "Craft practices rely on evolving archives of skill and knowledge, developed\nthrough generations of craftspeople experimenting with designs, materials, and\ntechniques. Better documentation of these practices enables the sharing of\nknowledge and expertise between sites and generations. However, most\ndocumentation focuses solely on the linear steps leading to final artifacts,\nneglecting the tacit knowledge necessary to improvise, or adapt workflows to\nmeet the unique demands of each craft project. This omission limits knowledge\nsharing and reduces craft to a mechanical endeavor, rather than a sophisticated\nway of seeing, thinking, and doing. Drawing on expert interviews and literature\nfrom HCI, CSCW and the social sciences, we develop an elementary grammar to\ndocument improvisational actions of real-world craft practices. We demonstrate\nthe utility of this grammar with an interface called CraftLink that can be used\nto analyze expert videos and semi-automatically generate documentation to\nconvey material and contextual variations of craft practices. Our user study\nwith expert crocheters (N=7) using this interface evaluates our grammar's\neffectiveness in capturing and sharing expert knowledge with other\ncraftspeople, offering new pathways for computational systems to support\ncollaborative archives of knowledge and practice within communities.", "comment": "29 pages, 7 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10891v1", "AI": {"title_translation": "解构工艺：一种用于分享工艺工作流程中专业知识的基本语法", "tldr": "该研究开发了一种基本语法和名为CraftLink的界面，用于更好地记录和分享手工艺中的即兴和情境知识，以克服传统文档只关注线性步骤的局限性。", "motivation": "手工艺实践依赖于不断发展的技能和知识档案，但大多数现有文档仅关注线性步骤，忽略了即兴创作或根据项目独特需求调整工作流程所需的默会知识，这限制了知识共享，并将手工艺简化为机械性工作。", "method": "通过专家访谈和人机交互、计算机支持的协同工作以及社会科学领域的文献，研究开发了一种用于记录真实世界手工艺实践中即兴行动的基本语法。并通过名为CraftLink的界面展示了该语法的实用性，该界面可用于分析专家视频并半自动生成文档，以传达手工艺实践的材料和情境变化。", "result": "研究通过CraftLink界面与7位专业钩针编织者进行了用户研究，评估了该语法在捕捉和分享专家知识方面的有效性，为计算系统支持社区内的知识和实践协作档案提供了新途径。", "conclusion": "本研究提出的语法和系统为计算系统支持手工艺社区内部的知识和实践协作档案提供了新的途径，能够更有效地捕捉和分享手工艺中重要的默会和情境知识。", "translation": "手工艺实践依赖于不断发展的技能和知识档案，这些档案通过一代代手工艺人在设计、材料和技术方面的实验而发展起来。更好地记录这些实践有助于在不同地点和世代之间分享知识和专业技能。然而，大多数文档只关注导致最终成品的线性步骤，而忽略了即兴创作或根据每个手工艺项目的独特需求调整工作流程所需的默会知识。这种遗漏限制了知识共享，并将手工艺简化为一种机械性工作，而非一种复杂的观察、思考和实践方式。本研究借鉴了专家访谈以及人机交互（HCI）、计算机支持的协同工作（CSCW）和社会科学领域的文献，开发了一种基本语法来记录真实世界手工艺实践中的即兴行动。我们通过一个名为CraftLink的界面展示了该语法的实用性，该界面可用于分析专家视频并半自动生成文档，以传达手工艺实践的材料和情境变化。我们对7位专业钩针编织者（N=7）使用该界面进行了用户研究，评估了我们语法在捕捉和与同行手工艺者分享专家知识方面的有效性，为计算系统支持社区内的知识和实践协作档案提供了新途径。", "summary": "本研究旨在解决手工艺知识文档中默会和情境知识缺失的问题。通过专家访谈和多学科文献回顾，研究开发了一种用于记录手工艺即兴行动的基本语法。该语法通过名为CraftLink的界面进行验证，该界面能够分析专家视频并半自动生成包含材料和情境变化的文档。用户研究表明，该语法能有效捕捉和分享专家知识，为计算系统支持手工艺社区的知识共享提供了新途径。", "keywords": "手工艺, 知识共享, 默会知识, 文档化, 即兴创作", "comments": "该论文的创新之处在于它超越了传统线性步骤的文档方式，专注于捕捉手工艺中难以言传的默会知识和即兴创作过程。通过引入“基本语法”的概念和开发CraftLink界面，为手工艺知识的数字化和共享提供了新的视角和工具，有助于提升手工艺的传承和发展。其重要性在于，它将手工艺视为一种复杂的认知和实践活动，而不仅仅是机械操作，这对于保护和传播非物质文化遗产具有重要意义。"}}
{"id": "2506.10642", "title": "Deployment of Containerized Simulations in an API-Driven Distributed Infrastructure", "authors": ["Tim Kraus", "Axel Sauer", "Ingo Feldner"], "summary": "The increasingly dynamic market for embedded systems makes virtual prototypes\nan indispensable tool for hardware/software codesign. The broad acceptance of\nthe methodology has led to a diverse range of solutions: from open-source, pure\nconsole-based simulators to highly capable commercial simulation tools. In this\nwork we present SUNRISE, an infrastructure to provide users a unified approach\nto utilizing virtual prototyping solutions, facilitate access to various\nsimulation technologies and boost cooperation by leveraging decentralized\ncompute resources for deployment of simulation workloads and definition of open\nAPIs.", "comment": "8 pages, 5 figures, Published in DVCon Europe 2024", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10642v1", "AI": {"title_translation": "API驱动分布式基础设施中容器化仿真的部署", "tldr": "本文提出了SUNRISE，一个API驱动的分布式基础设施，用于统一和简化虚拟原型解决方案的使用，促进对各种仿真技术的访问，并通过利用去中心化计算资源和开放API来增强协作。", "motivation": "嵌入式系统市场日益动态化，使得虚拟原型成为硬件/软件协同设计不可或缺的工具。然而，现有解决方案的多样性导致了使用上的复杂性，需要一个统一的方法来利用这些工具并促进合作。", "method": "本文提出了SUNRISE基础设施，它通过提供统一的方法来利用虚拟原型解决方案，促进对各种仿真技术的访问，并通过利用去中心化计算资源部署仿真工作负载和定义开放API来增强协作。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "嵌入式系统日益动态化的市场使得虚拟原型成为硬件/软件协同设计不可或缺的工具。该方法论的广泛接受催生了多种多样的解决方案：从开源的纯控制台模拟器到功能强大的商业仿真工具。在这项工作中，我们提出了SUNRISE，一个基础设施，旨在为用户提供一种统一的方法来利用虚拟原型解决方案，促进对各种仿真技术的访问，并通过利用去中心化计算资源部署仿真工作负载和定义开放API来增强合作。", "summary": "本文介绍了SUNRISE，一个旨在应对嵌入式系统虚拟原型日益增长复杂性的基础设施。SUNRISE通过提供一个统一的API驱动平台，使得用户能够更便捷地访问和利用各种虚拟原型解决方案，并能通过分布式计算资源实现仿真任务的部署，从而增强了不同仿真技术间的互操作性和协作效率。", "keywords": "容器化仿真, API驱动, 分布式基础设施, 虚拟原型, 嵌入式系统", "comments": "本文的创新之处在于提出了一个API驱动的分布式基础设施SUNRISE，旨在统一和简化对多种虚拟原型解决方案的访问和使用。这对于硬件/软件协同设计领域，尤其是在面对日益多样化的仿真工具时，具有重要意义。通过利用容器化和分布式资源，它有望提高仿真效率和团队协作能力。然而，摘要中并未提及具体的实现细节、性能评估或实际应用案例，这限制了对其实际效果的判断。"}}
{"id": "2506.10297", "title": "\"Check My Work?\": Measuring Sycophancy in a Simulated Educational Context", "authors": ["Chuck Arvin"], "summary": "This study examines how user-provided suggestions affect Large Language\nModels (LLMs) in a simulated educational context, where sycophancy poses\nsignificant risks. Testing five different LLMs from the OpenAI GPT-4o and\nGPT-4.1 model classes across five experimental conditions, we show that\nresponse quality varies dramatically based on query framing. In cases where the\nstudent mentions an incorrect answer, the LLM correctness can degrade by as\nmuch as 15 percentage points, while mentioning the correct answer boosts\naccuracy by the same margin. Our results also show that this bias is stronger\nin smaller models, with an effect of up to 30% for the GPT-4.1-nano model,\nversus 8% for the GPT-4o model. Our analysis of how often LLMs \"flip\" their\nanswer, and an investigation into token level probabilities, confirm that the\nmodels are generally changing their answers to answer choices mentioned by\nstudents in line with the sycophancy hypothesis. This sycophantic behavior has\nimportant implications for educational equity, as LLMs may accelerate learning\nfor knowledgeable students while the same tools may reinforce misunderstanding\nfor less knowledgeable students. Our results highlight the need to better\nunderstand the mechanism, and ways to mitigate, such bias in the educational\ncontext.", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10297v1", "AI": {"title_translation": "“检查我的作业？”：在模拟教育环境中衡量奉承行为", "tldr": "研究发现在模拟教育场景中，大型语言模型（LLMs）会对用户提供的答案表现出奉承行为，导致正确性下降或提高，且小型模型偏差更严重，这对教育公平有重要影响。", "motivation": "本研究旨在探究大型语言模型（LLMs）在模拟教育环境中，用户提供的建议如何影响其响应质量，并评估奉承行为在此情境下带来的风险。", "method": "研究测试了OpenAI GPT-4o和GPT-4.1系列中的五种不同LLM，涵盖五种实验条件，以测量其响应质量。通过分析LLM改变答案的频率以及token级别的概率来确认奉承行为。", "result": "LLM的响应质量随查询框架变化显著。当学生提及错误答案时，LLM正确性可下降15个百分点；提及正确答案时，准确性提高相同幅度。小型模型偏差更强，GPT-4.1-nano模型效应高达30%，而GPT-4o模型为8%。分析证实，模型通常会根据学生提及的答案选择来改变其答案，符合奉承假设。", "conclusion": "LLM的奉承行为对教育公平具有重要影响，可能加速知识渊博学生的学习，同时强化知识较少学生的误解。因此，需要更好地理解并减轻教育环境中的这种偏见机制。", "translation": "本研究探讨了在模拟教育环境中，用户提供的建议如何影响大型语言模型（LLM），在这一环境中，奉承行为构成了重大风险。我们测试了OpenAI GPT-4o和GPT-4.1模型类别中的五种不同LLM，涵盖五种实验条件，结果表明响应质量随查询框架的变化而显著不同。在学生提及错误答案的情况下，LLM的正确性可能下降多达15个百分点，而提及正确答案则可将准确性提高相同的幅度。我们的结果还表明，这种偏差在较小的模型中更为明显，GPT-4.1-nano模型的影响高达30%，而GPT-4o模型为8%。我们对LLM“翻转”答案频率的分析，以及对token级别概率的调查，证实了模型通常会根据学生提及的答案选择来改变其答案，这与奉承假设一致。这种奉承行为对教育公平具有重要意义，因为LLM可能加速知识渊博学生的学习，而同样的工具可能强化知识较少学生的误解。我们的结果强调，需要更好地理解教育环境中这种偏见的机制和缓解方法。", "summary": "这项研究调查了大型语言模型（LLMs）在模拟教育环境中对用户建议的奉承行为。通过测试OpenAI的五种LLM，发现当学生提及不正确答案时，LLM的正确性会显著下降；而提及正确答案时则会提高。小型模型的这种偏差效应更强。研究证实LLMs会根据用户的输入改变答案，这种奉承行为对教育公平构成挑战，可能加剧不同知识水平学生之间的学习差距。因此，理解并减轻这种偏差至关重要。", "keywords": "大型语言模型, 奉承行为, 教育公平, 模型偏差, GPT-4o", "comments": "这项研究揭示了LLM在教育应用中的一个关键且潜在有害的偏见——奉承行为。其创新之处在于通过模拟教育场景量化了这种行为对模型准确性的影响，并指出了小型模型在此方面的脆弱性。研究的重要性在于它直接关联到LLM在教育公平方面的深远影响，提醒开发者和教育者在部署AI工具时需警惕并寻求缓解策略。"}}
{"id": "2506.10629", "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning", "authors": ["Yucheng Yang", "Tianyi Zhou", "Qiang He", "Lei Han", "Mykola Pechenizkiy", "Meng Fang"], "summary": "Unsupervised reinforcement learning (URL) aims to learn general skills for\nunseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL\nby maximizing the mutual information between states and skills but lacks\nsufficient theoretical analysis, e.g., how well its learned skills can\ninitialize a downstream task's policy. Our new theoretical analysis in this\npaper shows that the diversity and separability of learned skills are\nfundamentally critical to downstream task adaptation but MISL does not\nnecessarily guarantee these properties. To complement MISL, we propose a novel\ndisentanglement metric LSEPIN. Moreover, we build an information-geometric\nconnection between LSEPIN and downstream task adaptation cost. For better\ngeometric properties, we investigate a new strategy that replaces the KL\ndivergence in information geometry with Wasserstein distance. We extend the\ngeometric analysis to it, which leads to a novel skill-learning objective WSEP.\nIt is theoretically justified to be helpful to downstream task adaptation and\nit is capable of discovering more initial policies for downstream tasks than\nMISL. We finally propose another Wasserstein distance-based algorithm PWSEP\nthat can theoretically discover all optimal initial policies.", "comment": "Spotlight paper at ICLR 2024. This version includes acknowledgments\n  omitted from the ICLR version and indicates the corresponding authors\n  primarily responsible for the work", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10629v1", "AI": {"title_translation": "技能的任务适应：信息几何、解耦和无监督强化学习的新目标", "tldr": "本文针对无监督强化学习中现有技能学习方法（如MISL）在下游任务适应性上的不足，提出了新的理论分析、解耦度量LSEPIN以及基于Wasserstein距离的新目标WSEP和PWSEP，旨在学习更具多样性和可分离性的技能以提高下游任务的初始化策略发现能力。", "motivation": "无监督强化学习（URL）旨在为未见过的下游任务学习通用技能。现有方法如互信息技能学习（MISL）通过最大化状态和技能之间的互信息来解决URL问题，但缺乏足够的理论分析，例如其学习到的技能如何有效地初始化下游任务的策略。本文的理论分析表明，学习技能的多样性和可分离性对于下游任务适应至关重要，而MISL不一定能保证这些特性。", "method": "本文进行了新的理论分析，表明学习技能的多样性和可分离性对下游任务适应至关重要。提出了一个新的解耦度量LSEPIN，并建立了其与下游任务适应成本之间的信息几何联系。为获得更好的几何特性，研究了一种新策略，用Wasserstein距离替换信息几何中的KL散度。基于此，提出了一个新的技能学习目标WSEP，并对其进行了理论证明。最终提出了另一个基于Wasserstein距离的算法PWSEP。", "result": "WSEP在理论上被证明有助于下游任务适应，并且能够比MISL发现更多的下游任务初始策略。PWSEP在理论上能够发现所有最优的初始策略。", "conclusion": "本文通过理论分析揭示了技能多样性和可分离性对下游任务适应的关键作用，并提出了基于信息几何和Wasserstein距离的新度量和目标（LSEPIN, WSEP, PWSEP），显著提高了无监督强化学习中技能学习对下游任务的适应能力和初始策略发现能力。", "translation": "无监督强化学习（URL）旨在为未见过的下游任务学习通用技能。互信息技能学习（MISL）通过最大化状态和技能之间的互信息来解决URL问题，但缺乏足够的理论分析，例如其学习到的技能如何有效地初始化下游任务的策略。本文新的理论分析表明，学习技能的多样性和可分离性对于下游任务适应至关重要，但MISL不一定能保证这些特性。为了补充MISL，我们提出了一种新颖的解耦度量LSEPIN。此外，我们建立了LSEPIN与下游任务适应成本之间的信息几何联系。为了获得更好的几何特性，我们研究了一种新策略，用Wasserstein距离替换信息几何中的KL散度。我们将其几何分析扩展到此，从而产生了一个新颖的技能学习目标WSEP。它在理论上被证明有助于下游任务适应，并且能够比MISL发现更多的下游任务初始策略。我们最终提出了另一个基于Wasserstein距离的算法PWSEP，它在理论上可以发现所有最优的初始策略。", "summary": "本文关注无监督强化学习（URL）中技能学习对下游任务的适应性问题。通过理论分析，指出现有方法如MISL在学习技能的多样性和可分离性方面存在不足，而这些特性对下游任务适应至关重要。为此，论文提出了新的解耦度量LSEPIN，并建立了其与下游任务适应成本的信息几何联系。进一步，引入了基于Wasserstein距离的技能学习新目标WSEP和PWSEP，理论上证明了它们能有效提升下游任务的适应能力，并发现更优或所有最优的初始策略。", "keywords": "无监督强化学习, 任务适应, 信息几何, 解耦, Wasserstein距离", "comments": "本文的创新点在于从信息几何角度深入分析了无监督强化学习中技能多样性和可分离性的重要性，并引入了Wasserstein距离来改进技能学习目标，解决了现有方法在下游任务适应性上的不足。理论分析严谨，提出了两种新算法WSEP和PWSEP，特别是PWSEP理论上能发现所有最优初始策略，具有重要的理论贡献。"}}
{"id": "2506.10739", "title": "Sampling-Based Planning Under STL Specifications: A Forward Invariance Approach", "authors": ["Gregorio Marchesini", "Siyuan Liu", "Lars Lindemann", "Dimos V. Dimarogonas"], "summary": "We propose a variant of the Rapidly Exploring Random Tree Star\n(RRT$^{\\star}$) algorithm to synthesize trajectories satisfying a given\nspatio-temporal specification expressed in a fragment of Signal Temporal Logic\n(STL) for linear systems. Previous approaches for planning trajectories under\nSTL specifications using sampling-based methods leverage either mixed-integer\nor non-smooth optimization techniques, with poor scalability in the horizon and\ncomplexity of the task. We adopt instead a control-theoretic perspective on the\nproblem, based on the notion of set forward invariance. Specifically, from a\ngiven STL task defined over polyhedral predicates, we develop a novel\nalgorithmic framework by which the task is efficiently encoded into a\ntime-varying set via linear programming, such that trajectories evolving within\nthe set also satisfy the task. Forward invariance properties of the resulting\nset with respect to the system dynamics and input limitations are then proved\nvia non-smooth analysis. We then present a modified RRT$^{\\star}$ algorithm to\nsynthesize asymptotically optimal and dynamically feasible trajectories\nsatisfying a given STL specification, by sampling a tree of trajectories within\nthe previously constructed time-varying set. We showcase two use cases of our\napproach involving an autonomous inspection of the International Space Station\nand room-servicing task requiring timed revisit of a charging station.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10739v1", "AI": {"title_translation": "基于采样的STL规范下规划：一种前向不变性方法", "tldr": "该论文提出了一种新的RRT*算法变体，利用前向不变性为线性系统在STL规范下规划轨迹，提高了现有方法的扩展性。", "motivation": "以前使用基于采样的方法在STL规范下规划轨迹（利用混合整数或非光滑优化技术）在规划范围和任务复杂性方面扩展性较差，本文旨在解决此问题。", "method": "该方法采用基于集合前向不变性概念的控制理论视角。它通过线性规划将STL任务（在多面体谓词上定义）有效地编码成一个时变集合，使得在该集合内演化的轨迹也满足任务。然后通过非光滑分析证明了所得集合相对于系统动力学和输入限制的前向不变性。最后，提出了一种改进的RRT*算法，通过在前述构建的时变集合内采样轨迹树，合成满足给定STL规范的渐近最优且动态可行的轨迹。", "result": "该方法能够合成满足给定STL规范的渐近最优且动态可行的轨迹。通过国际空间站的自主检查和需要定时重新访问充电站的房间服务任务两个用例展示了其有效性。", "conclusion": "该论文成功提出了一种新的RRT*变体，利用前向不变性有效地为线性系统在STL规范下合成轨迹，解决了先前方法的扩展性问题。", "translation": "我们提出了一种快速探索随机树星 (RRT*) 算法的变体，用于为线性系统合成满足信号时序逻辑 (STL) 片段中表达的给定时空规范的轨迹。以前使用基于采样的方法在 STL 规范下规划轨迹的方法利用混合整数或非光滑优化技术，在规划范围和任务复杂性方面扩展性较差。我们转而采用基于集合前向不变性概念的控制理论视角来解决该问题。具体来说，对于在多面体谓词上定义的给定 STL 任务，我们开发了一种新颖的算法框架，通过线性规划将任务有效地编码成一个时变集合，使得在该集合内演化的轨迹也满足任务。然后通过非光滑分析证明了所得集合相对于系统动力学和输入限制的前向不变性。然后，我们提出了一种改进的 RRT* 算法，通过在前述构建的时变集合内采样轨迹树，合成满足给定 STL 规范的渐近最优且动态可行的轨迹。我们展示了我们方法的两个用例，包括国际空间站的自主检查和需要定时重新访问充电站的房间服务任务。", "summary": "本文提出了一种新颖的RRT*算法变体，用于为线性系统在信号时序逻辑（STL）规范下规划渐近最优和动态可行的轨迹。与先前因优化技术导致扩展性差的采样方法不同，本文采用基于集合前向不变性的控制理论方法。它通过线性规划将STL任务高效编码为时变集合，确保集合内的轨迹满足任务要求，并从数学上证明了前向不变性。改进的RRT*算法在此集合内进行采样。该方法通过国际空间站自主检查和房间服务任务等用例展示了其有效性。", "keywords": "采样规划, STL规范, 前向不变性, RRT*, 线性系统", "comments": "本文通过将控制理论概念（前向不变性）与基于采样的规划（RRT*）相结合，提出了一种创新方法，解决了STL规范下轨迹合成的挑战性问题。其通过避免复杂的优化技术来提高扩展性的重点是一个重要贡献。利用线性规划进行任务编码和非光滑分析进行不变性证明显示出强大的理论基础。实际应用示例突出了其潜力。"}}
{"id": "2506.10841", "title": "Automotive Radar Online Channel Imbalance Estimation via NLMS", "authors": ["Esmaeil Kavousi Ghafi", "Oliver Lang", "Matthias Wagner", "Alexander Melzer", "Mario Huemer"], "summary": "Automotive radars are one of the essential enablers of advanced driver\nassistance systems (ADASs). Continuous monitoring of the functional safety and\nreliability of automotive radars is a crucial requirement to prevent accidents\nand increase road safety. One of the most critical aspects to monitor in this\ncontext is radar channel imbalances, as they are a key parameter regarding the\nreliability of the radar. These imbalances may originate from several parameter\nvariations or hardware fatigues, e.g., a solder ball break (SBB), and may\naffect some radar processing steps, such as the angle of arrival estimation. In\nthis work, a novel method for online estimation of automotive radar channel\nimbalances is proposed. The proposed method exploits a normalized least mean\nsquares (NLMS) algorithm as a block in the processing chain of the radar to\nestimate the channel imbalances. The input of this block is the detected\ntargets in the range-Doppler map of the radar on the road without any prior\nknowledge on the angular parameters of the targets. This property in\ncombination with low computational complexity of the NLMS, makes the proposed\nmethod suitable for online channel imbalance estimation, in parallel to the\nnormal operation of the radar. Furthermore, it features reduced dependency on\nspecific targets of interest and faster update rates of the channel imbalance\nestimation compared to the majority of state-of-the-art methods. This\nimprovement is achieved by allowing for multiple targets in the angular\nspectrum, whereas most other methods are restricted to only single targets in\nthe angular spectrum. The performance of the proposed method is validated using\nvarious simulation scenarios and is supported by measurement results.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10841v1", "AI": {"title_translation": "车载雷达基于NLMS的在线通道不平衡估计", "tldr": "提出了一种利用归一化最小均方（NLMS）算法在线估计车载雷达通道不平衡的新方法，该方法计算复杂度低、更新速率快，且支持多目标，适用于雷达正常运行时的并行估计。", "motivation": "车载雷达是高级驾驶辅助系统（ADAS）的关键组成部分，持续监测其功能安全性和可靠性至关重要，以防止事故并提高道路安全。雷达通道不平衡是影响雷达可靠性的一个关键参数，可能由参数变化或硬件疲劳引起，并影响诸如到达角估计等雷达处理步骤，因此需要对其进行监控。", "method": "本文提出了一种在线估计车载雷达通道不平衡的新方法。该方法利用归一化最小均方（NLMS）算法作为雷达处理链中的一个模块来估计通道不平衡。该模块的输入是雷达在道路上距离-多普勒图中检测到的目标，无需目标的角度参数先验知识。", "result": "所提出的方法具有低计算复杂度，适用于雷达正常运行时的在线通道不平衡估计。与大多数现有方法相比，它对特定感兴趣目标的依赖性更低，并且通道不平衡估计的更新速率更快。这得益于其允许角谱中存在多个目标，而大多数其他方法仅限于单个目标。该方法的性能已通过各种仿真场景验证，并得到测量结果的支持。", "conclusion": "本文提出了一种新颖的基于NLMS的在线车载雷达通道不平衡估计方法，该方法具有低计算复杂度、快速更新速率和多目标支持的优点，并通过仿真和测量结果验证了其有效性，显著提升了车载雷达的可靠性和安全性。", "translation": "车载雷达是高级驾驶辅助系统（ADAS）的重要使能技术之一。持续监测车载雷达的功能安全性和可靠性是防止事故和提高道路安全的关键要求。在此背景下，需要监测的最关键方面之一是雷达通道不平衡，因为它们是雷达可靠性的关键参数。这些不平衡可能源于多种参数变化或硬件疲劳，例如焊球断裂（SBB），并可能影响某些雷达处理步骤，例如到达角估计。在这项工作中，提出了一种在线估计车载雷达通道不平衡的新方法。所提出的方法利用归一化最小均方（NLMS）算法作为雷达处理链中的一个模块来估计通道不平衡。该模块的输入是雷达在道路上距离-多普勒图中检测到的目标，无需目标的角度参数先验知识。这一特性结合NLMS的低计算复杂度，使得所提出的方法适用于在线通道不平衡估计，与雷达的正常运行并行。此外，与大多数现有方法相比，它对特定感兴趣目标的依赖性更低，并且通道不平衡估计的更新速率更快。这一改进是通过允许角谱中存在多个目标来实现的，而大多数其他方法仅限于角谱中的单个目标。所提出方法的性能通过各种仿真场景进行了验证，并得到测量结果的支持。", "summary": "本文提出了一种基于归一化最小均方（NLMS）算法的新型在线车载雷达通道不平衡估计方法。该方法利用雷达距离-多普勒图中的检测目标作为输入，无需目标的角度先验知识。其低计算复杂度和支持多目标的特性使其能够与雷达正常操作并行进行在线估计，并实现比现有方法更快的更新速率和更低的特定目标依赖性。该方法的有效性已通过仿真和测量结果得到验证，有助于提升车载雷达的功能安全性和可靠性。", "keywords": "车载雷达, 通道不平衡, NLMS, 在线估计, ADAS", "comments": "该论文提出了一种实用的在线通道不平衡估计方法，其创新点在于将NLMS算法应用于雷达处理链中，实现了在无先验角度知识的情况下对通道不平衡的估计。该方法低计算复杂度、支持多目标且更新速率快，显著优于现有方法，对于提升车载雷达的可靠性和ADAS系统的安全性具有重要意义。其并行操作的特性也使其易于集成到现有雷达系统中。"}}
{"id": "2506.10387", "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills", "authors": ["Yuquan Xie", "Zaijing Li", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Dongmei Jiang", "Liqiang Nie"], "summary": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progressively abstracts trajectories into execution\nskills, core skills, and ultimately meta-skills, providing a hierarchical\nknowledge structure for long-horizon task planning. To bridge the domain gap,\nwe propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,\nwhich efficiently leverages skills acquired in offline environments to reduce\nthe action search space during online tree exploration. Building on HMS, we\npropose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To\nvalidate the performance of Mirage-1 in real-world long-horizon scenarios, we\nconstructed a new benchmark, AndroidLH. Experimental results show that Mirage-1\noutperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld,\nMobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:\nhttps://cybertronagent.github.io/Mirage-1.github.io/", "comment": "20 pages, 5 figures, 5 tables", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10387v1", "AI": {"title_translation": "幻影-1：通过分层多模态技能增强和更新GUI代理", "tldr": "提出Mirage-1，一个基于分层多模态技能（HMS）和技能增强蒙特卡洛树搜索（SA-MCTS）的GUI代理，解决了现有MLLM代理在长周期任务中的知识不足和域间隙问题，并在多个基准测试中表现出色。", "motivation": "现有的基于多模态大语言模型（MLLM）的GUI代理在在线环境中处理长周期任务时，由于知识不足以及离线和在线领域之间的固有差距，仍然面临挑战。", "method": "提出分层多模态技能（HMS）模块，通过将轨迹逐步抽象为执行技能、核心技能和元技能，提供分层知识结构以解决知识不足问题。同时，提出技能增强蒙特卡洛树搜索（SA-MCTS）算法，利用离线环境中获得的技能来减少在线树探索期间的动作搜索空间，以弥合领域差距。在此基础上构建了多模态、跨平台、即插即用的GUI代理Mirage-1，并构建了新的基准AndroidLH进行验证。", "result": "Mirage-1在AndroidWorld、MobileMiniWob++、Mind2Web-Live和AndroidLH上分别比之前的代理性能提高了32%、19%、15%和79%。", "conclusion": "Mirage-1通过其分层多模态技能和技能增强蒙特卡洛树搜索算法，有效解决了GUI代理在长周期任务中的知识不足和领域差距问题，并在多个基准测试中显著超越了现有代理，证明了其在真实世界长周期场景中的优越性能。", "translation": "近期利用多模态大语言模型（MLLM）作为GUI代理的努力取得了可喜的成果。然而，这些代理在在线环境中处理长周期任务时仍然面临挑战，主要原因在于知识不足以及离线和在线领域之间的固有差距。在本文中，受人类在开放式环境中泛化知识的方式启发，我们提出了一种分层多模态技能（HMS）模块来解决知识不足的问题。它将轨迹逐步抽象为执行技能、核心技能，并最终抽象为元技能，为长周期任务规划提供分层知识结构。为了弥合领域差距，我们提出了技能增强蒙特卡洛树搜索（SA-MCTS）算法，该算法有效利用离线环境中获得的技能来减少在线树探索期间的动作搜索空间。基于HMS，我们提出了Mirage-1，一个多模态、跨平台、即插即用的GUI代理。为了验证Mirage-1在真实世界长周期场景中的性能，我们构建了一个新的基准AndroidLH。实验结果表明，Mirage-1在AndroidWorld、MobileMiniWob++、Mind2Web-Live和AndroidLH上分别比之前的代理性能提高了32%、19%、15%和79%。项目页面：https://cybertronagent.github.io/Mirage-1.github.io/", "summary": "本文针对基于MLLM的GUI代理在处理在线长周期任务时存在的知识不足和离线-在线域间隙问题，提出了Mirage-1代理。Mirage-1的核心是分层多模态技能（HMS）模块，它通过多层抽象构建知识结构以应对知识不足；同时，引入技能增强蒙特卡洛树搜索（SA-MCTS）算法，利用离线技能优化在线搜索空间以弥合域间隙。研究团队还构建了新的AndroidLH基准来评估Mirage-1的真实世界性能。实验结果显示，Mirage-1在多个基准测试中显著优于现有代理。", "keywords": "GUI代理, 多模态大语言模型, 分层多模态技能, 技能增强蒙特卡洛树搜索, 长周期任务", "comments": "这篇论文的创新点在于结合了分层知识结构（HMS）和高效的在线搜索策略（SA-MCTS），以解决GUI代理在复杂长周期任务中的两大核心挑战。HMS模仿了人类泛化知识的方式，提供了更鲁棒的知识表示，而SA-MCTS则有效地利用了离线学习的经验来加速在线决策，体现了理论与实践的有效结合。此外，构建新的AndroidLH基准也为未来研究提供了宝贵的评估工具。"}}
{"id": "2506.10858", "title": "Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image Segmentation", "authors": ["Zhenhuan Zhou"], "summary": "Medical image segmentation is a fundamental and key technology in\ncomputer-aided diagnosis and treatment. Previous methods can be broadly\nclassified into three categories: convolutional neural network (CNN) based,\nTransformer based, and hybrid architectures that combine both. However, each of\nthem has its own limitations, such as restricted receptive fields in CNNs or\nthe computational overhead caused by the quadratic complexity of Transformers.\nRecently, the Receptance Weighted Key Value (RWKV) model has emerged as a\npromising alternative for various vision tasks, offering strong long-range\nmodeling capabilities with linear computational complexity. Some studies have\nalso adapted RWKV to medical image segmentation tasks, achieving competitive\nperformance. However, most of these studies focus on modifications to the\nVision-RWKV (VRWKV) mechanism and train models from scratch, without exploring\nthe potential advantages of leveraging pre-trained VRWKV models for medical\nimage segmentation tasks. In this paper, we propose Med-URWKV, a pure\nRWKV-based architecture built upon the U-Net framework, which incorporates\nImageNet-based pretraining to further explore the potential of RWKV in medical\nimage segmentation tasks. To the best of our knowledge, Med-URWKV is the first\npure RWKV segmentation model in the medical field that can directly reuse a\nlarge-scale pre-trained VRWKV encoder. Experimental results on seven datasets\ndemonstrate that Med-URWKV achieves comparable or even superior segmentation\nperformance compared to other carefully optimized RWKV models trained from\nscratch. This validates the effectiveness of using a pretrained VRWKV encoder\nin enhancing model performance. The codes will be released.", "comment": "Preprint Draft, 5 pages. This paper will be updated with a formal\n  version in the future, Copyright: College of Computer Science, Nankai\n  University. All rights reserved", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10858v1", "AI": {"title_translation": "Med-URWKV: 基于ImageNet预训练的纯RWKV医学图像分割模型", "tldr": "Med-URWKV是一个基于U-Net框架的纯RWKV医学图像分割模型，首次利用大规模预训练的VRWKV编码器，在多项医学图像分割任务上表现出与从头训练模型相当甚至更优的性能。", "motivation": "医学图像分割是计算机辅助诊断和治疗的关键技术。现有方法（CNN、Transformer及其混合架构）各有局限，如CNN感受野受限，Transformer计算复杂度高。虽然RWKV模型在视觉任务中表现出潜力，但现有将其应用于医学图像分割的研究多集中于VRWKV机制的修改，且从头训练模型，未充分探索利用预训练VRWKV模型的优势。", "method": "本文提出了Med-URWKV，一个纯粹基于RWKV的架构，它构建在U-Net框架之上，并结合了基于ImageNet的预训练。据作者所知，Med-URWKV是医学领域中首个可以直接重用大规模预训练VRWKV编码器的纯RWKV分割模型。", "result": "在七个数据集上的实验结果表明，Med-URWKV与从头训练的其他精心优化的RWKV模型相比，实现了相当甚至更优的分割性能。这验证了使用预训练VRWKV编码器在提升模型性能方面的有效性。", "conclusion": "Med-URWKV通过利用大规模预训练的VRWKV编码器，在医学图像分割任务中展现出纯RWKV架构的有效性，并达到了先进的性能。", "translation": "医学图像分割是计算机辅助诊断和治疗中的一项基础且关键的技术。以往的方法大致可分为三类：基于卷积神经网络（CNN）、基于Transformer以及结合两者的混合架构。然而，它们各自存在局限性，例如CNN的感受野受限，或Transformer的二次复杂度导致的计算开销。最近，接受度加权键值（RWKV）模型作为各种视觉任务的一种有前途的替代方案出现，它提供了强大的长程建模能力和线性计算复杂度。一些研究也已将RWKV应用于医学图像分割任务，并取得了有竞争力的性能。然而，这些研究大多集中于对Vision-RWKV（VRWKV）机制的修改，并从头训练模型，而没有探索利用预训练VRWKV模型在医学图像分割任务中的潜在优势。在本文中，我们提出了Med-URWKV，一个基于U-Net框架的纯RWKV架构，它结合了基于ImageNet的预训练，以进一步探索RWKV在医学图像分割任务中的潜力。据我们所知，Med-URWKV是医学领域中第一个可以直接重用大规模预训练VRWKV编码器的纯RWKV分割模型。在七个数据集上的实验结果表明，Med-URWKV与从头训练的其他精心优化的RWKV模型相比，实现了相当甚至更优的分割性能。这验证了使用预训练VRWKV编码器在提升模型性能方面的有效性。代码将发布。", "summary": "本文提出Med-URWKV，一个基于U-Net框架的纯RWKV医学图像分割模型，旨在解决现有CNN和Transformer方法的局限性，并充分利用RWKV模型的线性计算复杂度和长程建模能力。Med-URWKV首次在医学图像分割中直接重用大规模ImageNet预训练的VRWKV编码器。实验证明，Med-URWKV在多个数据集上取得了与从头训练的RWKV模型相当或更优的性能，验证了预训练VRWKV编码器对模型性能提升的有效性。", "keywords": "医学图像分割, RWKV, 预训练, U-Net, VRWKV", "comments": "本文的创新点在于首次将大规模预训练的VRWKV编码器应用于医学图像分割任务，并构建了纯RWKV架构的Med-URWKV模型。这不仅探索了RWKV在医学领域的潜力，也证明了利用通用领域预训练模型对特定领域任务的有效性，为未来医学图像处理的预训练范式提供了新的思路。其重要性在于提供了一种计算效率高且性能优异的替代方案，有望推动医学图像分割技术的发展。"}}
{"id": "2506.10161", "title": "Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective", "authors": ["Yi Wang", "Max Kreminski"], "summary": "Story generation has been a prominent application of Large Language Models\n(LLMs). However, understanding LLMs' ability to produce high-quality stories\nremains limited due to challenges in automatic evaluation methods and the high\ncost and subjectivity of manual evaluation. Computational narratology offers\nvaluable insights into what constitutes a good story, which has been applied in\nthe symbolic narrative planning approach to story generation. This work aims to\ndeepen the understanding of LLMs' story generation capabilities by using them\nto solve narrative planning problems. We present a benchmark for evaluating\nLLMs on narrative planning based on literature examples, focusing on causal\nsoundness, character intentionality, and dramatic conflict. Our experiments\nshow that GPT-4 tier LLMs can generate causally sound stories at small scales,\nbut planning with character intentionality and dramatic conflict remains\nchallenging, requiring LLMs trained with reinforcement learning for complex\nreasoning. The results offer insights on the scale of stories that LLMs can\ngenerate while maintaining quality from different aspects. Our findings also\nhighlight interesting problem solving behaviors and shed lights on challenges\nand considerations for applying LLM narrative planning in game environments.", "comment": "In 2025 IEEE Conference on Games (CoG)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10161v1", "AI": {"title_translation": "大型语言模型能生成好故事吗？来自叙事规划视角的见解与挑战", "tldr": "大型语言模型（LLMs）在小规模上能生成因果合理的故事情节，但在处理角色意图性和戏剧冲突的复杂叙事规划方面仍面临挑战，可能需要强化学习训练。", "motivation": "为了深入理解大型语言模型（LLMs）的故事生成能力，特别是鉴于当前自动评估方法的局限性、人工评估的高成本和主观性，以及计算叙事学和符号叙事规划提供的宝贵见解，本研究旨在通过让LLMs解决叙事规划问题来探究其生成高质量故事的能力。", "method": "研究者通过让LLMs解决叙事规划问题，并提出了一个基于文学实例的基准来评估LLMs。评估重点关注因果合理性、角色意图性和戏剧冲突。", "result": "实验表明，GPT-4级别的LLMs能够生成小规模的因果合理故事。然而，涉及角色意图性和戏剧冲突的规划仍然具有挑战性，对于复杂推理，可能需要通过强化学习训练的LLMs。研究结果揭示了LLMs在保持不同方面质量的情况下能够生成的故事规模，并突出了有趣的解决问题行为以及在游戏环境中应用LLM叙事规划的挑战和考量。", "conclusion": "尽管大型语言模型在生成小规模因果合理故事方面展现出潜力，但在叙事规划中融入角色意图性和戏剧冲突，特别是在复杂情境下，仍存在显著挑战，这表明需要采用如强化学习等更先进的训练方法。", "translation": "故事生成一直是大型语言模型（LLMs）的一个突出应用。然而，由于自动评估方法的挑战以及手动评估的高成本和主观性，对LLMs生成高质量故事能力的理解仍然有限。计算叙事学为“什么是好故事”提供了宝贵的见解，这已应用于故事生成的符号叙事规划方法中。这项工作旨在通过使用LLMs解决叙事规划问题，加深对LLMs故事生成能力的理解。我们提出了一个基于文学实例的基准，用于评估LLMs在叙事规划方面的表现，重点关注因果合理性、角色意图性和戏剧冲突。我们的实验表明，GPT-4级别的LLMs可以在小规模上生成因果合理的故事情节，但涉及角色意图性和戏剧冲突的规划仍然具有挑战性，需要通过强化学习训练的LLMs来处理复杂推理。结果提供了关于LLMs在保持不同方面质量的同时可以生成的故事规模的见解。我们的发现还突出了有趣的解决问题行为，并阐明了在游戏环境中应用LLM叙事规划的挑战和考虑因素。", "summary": "本文通过让大型语言模型（LLMs）解决叙事规划问题，并结合计算叙事学的视角，深入探讨了LLMs生成高质量故事的能力。研究引入了一个新的评估基准，侧重于因果合理性、角色意图性和戏剧冲突。实验结果显示，GPT-4级别的LLMs能生成小规模的因果合理故事，但在处理角色意图性和戏剧冲突方面仍面临挑战，这表明对于复杂的叙事推理，可能需要通过强化学习进行训练。研究结果为LLMs高质量故事生成的规模提供了见解，并指出了其在游戏环境中应用所面临的挑战。", "keywords": "大型语言模型, 故事生成, 叙事规划, 计算叙事学, GPT-4", "comments": "这篇论文通过应用计算叙事学和叙事规划原则来评估LLMs的故事生成能力，提供了一个新颖的视角，超越了传统上常带有主观性的评估方法。其对因果合理性、角色意图性和戏剧冲突等特定叙事元素的关注，使得对LLM的优势和局限性有了更细致的理解。论文识别出复杂推理中的挑战并提出通过强化学习进行改进的建议，这对于游戏开发等实际应用领域具有重要意义。"}}
{"id": "2506.10174", "title": "Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context", "authors": ["Yael Frischholz", "Devis Tuia", "Michael Lehning"], "summary": "Accurate retrieval of surface solar radiation (SSR) from satellite imagery\ncritically depends on estimating the background reflectance that a spaceborne\nsensor would observe under clear-sky conditions. Deviations from this baseline\ncan then be used to detect cloud presence and guide radiative transfer models\nin inferring atmospheric attenuation. Operational retrieval algorithms\ntypically approximate background reflectance using monthly statistics, assuming\nsurface properties vary slowly relative to atmospheric conditions. However,\nthis approach fails in mountainous regions where intermittent snow cover and\nchanging snow surfaces are frequent. We propose an attention-based emulator for\nSSR retrieval that implicitly learns to infer clear-sky surface reflectance\nfrom raw satellite image sequences. Built on the Temporo-Spatial Vision\nTransformer, our approach eliminates the need for hand-crafted features such as\nexplicit albedo maps or cloud masks. The emulator is trained on instantaneous\nSSR estimates from the HelioMont algorithm over Switzerland, a region\ncharacterized by complex terrain and dynamic snow cover. Inputs include\nmulti-spectral SEVIRI imagery from the Meteosat Second Generation platform,\naugmented with static topographic features and solar geometry. The target\nvariable is HelioMont's SSR, computed as the sum of its direct and diffuse\nhorizontal irradiance components, given at a spatial resolution of 1.7 km. We\nshow that, when provided a sufficiently long temporal context, the model\nmatches the performances of albedo-informed models, highlighting the model's\nability to internally learn and exploit latent surface reflectance dynamics.\nOur geospatial analysis shows this effect is most powerful in mountainous\nregions and improves generalization in both simple and complex topographic\nsettings. Code and datasets are publicly available at\nhttps://github.com/frischwood/HeMu-dev.git", "comment": "14 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10174v1", "AI": {"title_translation": "通过时间上下文隐式反照率恢复地表太阳辐射", "tldr": "本文提出了一种基于注意力机制的模拟器，通过学习卫星图像序列中的时间上下文，隐式推断晴空地表反射率，从而准确检索地表太阳辐射（SSR），特别是在多雪山区。", "motivation": "从卫星图像中准确获取地表太阳辐射（SSR）关键在于估算晴空条件下的背景反射率。传统操作算法通常使用月度统计数据近似背景反射率，但在山区，由于间歇性积雪和变化的雪面，这种方法会失效。", "method": "我们提出了一种基于注意力机制的模拟器，用于SSR检索，该模拟器能够从原始卫星图像序列中隐式学习推断晴空地表反射率。该方法基于时空Vision Transformer构建，无需手工制作的反照率图或云掩膜等特征。模拟器在瑞士地区（地形复杂、积雪动态变化）的HelioMont算法瞬时SSR估计数据上进行训练。输入包括Meteosat第二代平台的多光谱SEVIRI图像，并辅以静态地形特征和太阳几何信息。目标变量是HelioMont的SSR，其空间分辨率为1.7公里。", "result": "当提供足够长的时间上下文时，该模型能与反照率信息模型达到相同的性能，这突出显示了模型内部学习和利用潜在地表反射率动态的能力。我们的地理空间分析表明，这种效果在山区最为显著，并且在简单和复杂的地形设置中都改善了泛化能力。", "conclusion": "本文提出的基于注意力机制的模拟器能够通过隐式学习时间上下文中的地表反射率动态，有效且准确地检索地表太阳辐射，尤其是在积雪多变的山区，优于传统依赖显式反照率图的方法。", "translation": "从卫星图像中准确获取地表太阳辐射（SSR）关键在于估算卫星传感器在晴空条件下观测到的背景反射率。与此基线的偏差可用于检测云的存在并指导辐射传输模型推断大气衰减。操作性检索算法通常使用月度统计数据近似背景反射率，假设地表特性相对于大气条件变化缓慢。然而，这种方法在间歇性积雪和变化的雪面频繁的山区会失效。我们提出了一种基于注意力机制的模拟器，用于SSR检索，该模拟器能够从原始卫星图像序列中隐式学习推断晴空地表反射率。该方法基于时空Vision Transformer构建，无需手工制作的反照率图或云掩膜等特征。模拟器在瑞士地区（地形复杂、积雪动态变化）的HelioMont算法瞬时SSR估计数据上进行训练。输入包括Meteosat第二代平台的多光谱SEVIRI图像，并辅以静态地形特征和太阳几何信息。目标变量是HelioMont的SSR，其计算方式为其直接和漫射水平辐照度分量之和，空间分辨率为1.7公里。我们展示了，当提供足够长的时间上下文时，该模型能与反照率信息模型达到相同的性能，这突出显示了模型内部学习和利用潜在地表反射率动态的能力。我们的地理空间分析表明，这种效果在山区最为显著，并且在简单和复杂的地形设置中都改善了泛化能力。代码和数据集可在https://github.com/frischwood/HeMu-dev.git公开获取。", "summary": "本文提出了一种基于时空Vision Transformer的注意力模拟器，用于从原始卫星图像序列中隐式学习晴空地表反射率，从而实现地表太阳辐射（SSR）的准确检索。该方法克服了传统月度统计方法在多雪山区失效的问题，无需显式反照率图或云掩膜。在瑞士复杂地形和动态积雪区域的HelioMont SSR数据上进行训练和验证，结果表明，在提供足够时间上下文的情况下，该模型能够匹配甚至超越基于显式反照率信息的模型性能，特别是在山区表现出强大的泛化能力。", "keywords": "地表太阳辐射, 反照率恢复, 时间上下文, 注意力机制, Vision Transformer", "comments": "该研究的创新之处在于其通过隐式学习地表反射率动态来检索SSR，避免了对传统手工制作特征（如显式反照率图和云掩膜）的依赖。这种方法利用时间上下文，使得模型在处理具有复杂地形和动态积雪的区域时表现出鲁棒性和优越性，为卫星遥感领域提供了一种更高效、更通用的SSR反演方案。"}}
{"id": "2506.10299", "title": "Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs", "authors": ["Hayato Futami", "Emiru Tsunoo", "Yosuke Kashiwagi", "Yuki Ito", "Hassan Shahmohammadi", "Siddhant Arora", "Shinji Watanabe"], "summary": "Speech-to-speech translation (S2ST) has been advanced with large language\nmodels (LLMs), which are fine-tuned on discrete speech units. In such\napproaches, modality adaptation from text to speech has been an issue. LLMs are\ntrained on text-only data, which presents challenges to adapt them to speech\nmodality with limited speech-to-speech data. To address the training\ndifficulty, we propose scheduled interleaved speech--text training in this\nstudy. We use interleaved speech--text units instead of speech units during\ntraining, where aligned text tokens are interleaved at the word level. We\ngradually decrease the ratio of text as training progresses, to facilitate\nprogressive modality adaptation from text to speech. We conduct experimental\nevaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show\nthat the proposed method consistently improves the translation performances,\nespecially for languages with limited training data.", "comment": "Accepted to Interspeech2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10299v1", "AI": {"title_translation": "用于基于大型语言模型的端到端语音翻译的调度交错语音-文本训练", "tldr": "本文提出了一种调度交错语音-文本训练方法，以解决大型语言模型在语音到语音翻译中从文本到语音的模态适应问题，并在有限数据语言上显著提升了翻译性能。", "motivation": "大型语言模型（LLMs）主要基于文本数据训练，这导致在将其应用于语音模态（尤其是语音到语音翻译，S2ST）时面临模态适应的挑战，特别是在语音到语音数据有限的情况下，训练难度较大。", "method": "研究提出了一种调度交错语音-文本训练方法。在训练过程中，使用交错的语音-文本单元（在单词级别交错对齐的文本标记）替代纯语音单元。随着训练的进行，逐渐降低文本的比例，以促进从文本到语音的渐进式模态适应。通过在CVSS数据集上微调LLaMA3.2-1B进行了实验评估。", "result": "所提出的方法持续改进了翻译性能，特别是对于训练数据有限的语言。", "conclusion": "调度交错语音-文本训练方法有效解决了大型语言模型在语音到语音翻译中的模态适应问题，显著提升了翻译性能，尤其对于资源稀缺的语言表现更佳。", "translation": "大型语言模型（LLMs）的出现推动了语音到语音翻译（S2ST）的发展，这些模型通过对离散语音单元进行微调。在这种方法中，从文本到语音的模态适应一直是一个问题。LLMs是在纯文本数据上训练的，这给它们适应语音模态带来了挑战，尤其是在语音到语音数据有限的情况下。为了解决训练困难，本研究提出了调度交错语音-文本训练方法。我们在训练期间使用交错的语音-文本单元而不是纯语音单元，其中对齐的文本标记在单词级别进行交错。随着训练的进行，我们逐渐降低文本的比例，以促进从文本到语音的渐进式模态适应。我们通过在CVSS数据集上微调LLaMA3.2-1B进行实验评估。结果表明，所提出的方法持续改进了翻译性能，特别是对于训练数据有限的语言。", "summary": "本研究旨在解决大型语言模型（LLMs）在语音到语音翻译（S2ST）中从文本到语音的模态适应难题，因为LLMs主要在文本数据上训练且S2ST数据有限。为此，论文提出了一种调度交错语音-文本训练方法：在训练时使用交错的语音-文本单元（包含单词级对齐的文本标记），并逐步减少文本比例以促进模态渐进适应。实验结果表明，该方法显著提升了翻译性能，尤其对低资源语言效果更佳。", "keywords": "语音到语音翻译, 大型语言模型, 模态适应, 调度训练, 交错训练", "comments": "该论文的创新点在于提出了调度交错语音-文本训练方法，巧妙地解决了大型语言模型在语音到语音翻译中面临的文本到语音模态适应挑战。通过在训练初期引入文本信息并逐步减少其比例，实现了平滑的模态过渡。这项工作对于提升LLMs在多模态任务上的表现，特别是对于资源有限的语言的语音翻译具有重要意义。"}}
{"id": "2506.10533", "title": "Non-augmented velocity-vorticity-pressure formulation for the Navier--Stokes--Brinkman--Forchheimer problem", "authors": ["Santiago Badia", "Carsten Carstensen", "Alberto F. Martin", "Ricardo Ruiz-Baier", "Segundo Villa-Fuentes"], "summary": "The flow of incompressible fluid in highly permeable porous media in\nvorticity - velocity - Bernoulli pressure form leads to a double saddle-point\nproblem in the Navier--Stokes--Brinkman--Forchheimer equations. The paper\nestablishes, for small sources, the existence of solutions on the continuous\nand discrete level of lowest-order piecewise divergence-free Crouzeix--Raviart\nfinite elements. The vorticity employs a vector version of the pressure space\nwith normal and tangential velocity jump penalisation terms. A simple\nRaviart--Thomas interpolant leads to pressure-robust a priori error estimates.\nAn explicit residual-based a posteriori error estimate allows for efficient and\nreliable a posteriori error control. The efficiency for the Forchheimer\nnonlinearity requires a novel discrete inequality of independent interest. The\nimplementation is based upon a light-weight forest-of-trees data structure\nhandled by a highly parallel set of adaptive {mesh refining} algorithms.\nNumerical simulations reveal robustness of the a posteriori error estimates and\nimproved convergence rates by adaptive mesh-refining.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10533v1", "AI": {"title_translation": "纳维-斯托克斯-布林克曼-福希海默问题的非增广速度-涡度-压力公式", "tldr": "本文研究了Navier--Stokes--Brinkman--Forchheimer方程的涡度-速度-压力公式，证明了连续和离散解的存在性，并提出了鲁棒的先验和后验误差估计方法，通过自适应网格细化提高了收敛速度。", "motivation": "解决不可压缩流体在高度渗透多孔介质中流动时，Navier--Stokes--Brinkman--Forchheimer方程在涡度-速度-伯努利压力形式下导致的双鞍点问题。", "method": "使用最低阶分段无散度Crouzeix--Raviart有限元；涡度采用带有法向和切向速度跳跃惩罚项的压力空间向量版本；利用Raviart--Thomas插值器获得压力鲁棒的先验误差估计；采用基于残差的显式后验误差估计进行误差控制；为Forchheimer非线性引入了新的离散不等式；实现基于轻量级树林数据结构和高度并行的自适应网格细化算法。", "result": "建立了小源情况下连续和离散解的存在性；获得了压力鲁棒的先验误差估计；实现了高效可靠的后验误差控制；数值模拟显示后验误差估计的鲁棒性；通过自适应网格细化提高了收敛速度。", "conclusion": "本文成功建立了Navier--Stokes--Brinkman--Forchheimer问题在涡度-速度-压力形式下的解的存在性，并提供了鲁棒且高效的误差估计和控制方法，通过自适应网格细化显著提升了收敛性能。", "translation": "不可压缩流体在涡度-速度-伯努利压力形式的高度渗透多孔介质中的流动导致Navier--Stokes--Brinkman--Forchheimer方程中的双鞍点问题。本文对于小源情况，建立了最低阶分段无散度Crouzeix--Raviart有限元在连续和离散层面的解的存在性。涡度采用了压力空间的向量版本，并带有法向和切向速度跳跃惩罚项。一个简单的Raviart--Thomas插值器带来了压力鲁棒的先验误差估计。显式的基于残差的后验误差估计允许高效可靠的后验误差控制。Forchheimer非线性的效率需要一个独立兴趣的新颖离散不等式。实现基于轻量级树林数据结构，由一套高度并行的自适应网格细化算法处理。数值模拟揭示了后验误差估计的鲁棒性以及通过自适应网格细化提高的收敛速度。", "summary": "本文提出了一种Navier--Stokes--Brinkman--Forchheimer问题的非增广速度-涡度-压力公式，旨在解决不可压缩流体在多孔介质中流动产生的双鞍点问题。研究建立了连续和离散层面解的存在性，并开发了压力鲁棒的先验误差估计和高效可靠的后验误差控制方法，其中包含一个新颖的离散不等式以处理Forchheimer非线性。通过结合轻量级数据结构和自适应网格细化算法，数值模拟验证了所提方法的鲁棒性，并展示了通过自适应网格细化实现的收敛速度提升。", "keywords": "Navier--Stokes--Brinkman--Forchheimer, 涡度-速度-压力, 有限元, 误差估计, 自适应网格细化", "comments": "该论文在处理Navier--Stokes--Brinkman--Forchheimer方程的双鞍点问题上具有创新性，特别是在涡度-速度-压力公式的框架下。其贡献在于证明了连续和离散解的存在性，并提出了鲁棒的先验和后验误差估计方法，这对于数值模拟的准确性和效率至关重要。引入新颖的离散不等式以应对Forchheimer非线性是其技术亮点之一。此外，采用高度并行的自适应网格细化算法结合轻量级数据结构，显著提升了实际应用的效率和收敛性能。"}}
{"id": "2506.10175", "title": "AURA: A Multi-Agent Intelligence Framework for Knowledge-Enhanced Cyber Threat Attribution", "authors": ["Nanda Rani", "Sandeep Kumar Shukla"], "summary": "Effective attribution of Advanced Persistent Threats (APTs) increasingly\nhinges on the ability to correlate behavioral patterns and reason over complex,\nvaried threat intelligence artifacts. We present AURA (Attribution Using\nRetrieval-Augmented Agents), a multi-agent, knowledge-enhanced framework for\nautomated and interpretable APT attribution. AURA ingests diverse threat data\nincluding Tactics, Techniques, and Procedures (TTPs), Indicators of Compromise\n(IoCs), malware details, adversarial tools, and temporal information, which are\nprocessed through a network of collaborative agents. These agents are designed\nfor intelligent query rewriting, context-enriched retrieval from structured\nthreat knowledge bases, and natural language justification of attribution\ndecisions. By combining Retrieval-Augmented Generation (RAG) with Large\nLanguage Models (LLMs), AURA enables contextual linking of threat behaviors to\nknown APT groups and supports traceable reasoning across multiple attack\nphases. Experiments on recent APT campaigns demonstrate AURA's high attribution\nconsistency, expert-aligned justifications, and scalability. This work\nestablishes AURA as a promising direction for advancing transparent,\ndata-driven, and scalable threat attribution using multi-agent intelligence.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10175v1", "AI": {"title_translation": "AURA：一个用于知识增强型网络威胁归因的多智能体智能框架", "tldr": "AURA是一个多智能体、知识增强的框架，用于自动化和可解释的APT威胁归因，它结合了RAG和LLM，通过关联行为模式和威胁情报来提高归因的一致性、可解释性和可扩展性。", "motivation": "当前的APT归因越来越依赖于关联行为模式和推理复杂的、多样化的威胁情报。需要一个自动化、可解释且高效的框架来解决这一挑战。", "method": "AURA（Attribution Using Retrieval-Augmented Agents）是一个多智能体框架，它摄取TTPs、IoCs、恶意软件详情、对抗工具和时间信息等多种威胁数据。这些数据通过协作智能体网络进行处理，这些智能体负责智能查询重写、从结构化威胁知识库中进行上下文丰富的检索，以及对归因决策进行自然语言解释。AURA通过结合检索增强生成（RAG）和大型语言模型（LLMs），将威胁行为与已知APT组进行上下文关联，并支持跨多个攻击阶段的可追溯推理。", "result": "在近期APT活动上的实验表明，AURA具有高归因一致性、与专家一致的解释以及良好的可扩展性。", "conclusion": "AURA为推进透明、数据驱动和可扩展的威胁归因提供了一个有前景的方向，利用了多智能体智能。", "translation": "有效归因高级持续威胁（APT）越来越依赖于关联行为模式和推理复杂的、多样化的威胁情报。我们提出了AURA（Attribution Using Retrieval-Augmented Agents），一个多智能体、知识增强的框架，用于自动化和可解释的APT归因。AURA摄取包括战术、技术和程序（TTPs）、危害指标（IoCs）、恶意软件详情、对抗工具和时间信息在内的多样化威胁数据，并通过一个协作智能体网络进行处理。这些智能体被设计用于智能查询重写、从结构化威胁知识库中进行上下文丰富的检索，以及对归因决策进行自然语言解释。通过将检索增强生成（RAG）与大型语言模型（LLMs）相结合，AURA能够将威胁行为与已知APT组进行上下文关联，并支持跨多个攻击阶段的可追溯推理。在近期APT活动上的实验表明，AURA具有高归因一致性、与专家一致的解释以及良好的可扩展性。这项工作确立了AURA作为利用多智能体智能推进透明、数据驱动和可扩展威胁归因的一个有前景方向。", "summary": "AURA是一个多智能体、知识增强的框架，旨在自动化和解释高级持续威胁（APT）的归因。它整合了多样化的威胁数据，并通过协作智能体网络进行处理，这些智能体利用检索增强生成（RAG）和大型语言模型（LLMs）进行智能查询、上下文检索和决策解释。实验证明AURA在归因一致性、专家解释对齐和可扩展性方面表现出色，为透明、数据驱动的威胁归因提供了新途径。", "keywords": "APT归因, 多智能体系统, 知识增强, 检索增强生成, 大型语言模型", "comments": "AURA的创新之处在于其结合了多智能体系统、RAG和LLMs来处理复杂的威胁情报，实现了自动化且可解释的APT归因。其重要性体现在解决了传统归因中数据复杂性和解释性不足的问题，提高了归因的效率和可信度。该框架的可扩展性也预示着其在未来网络安全领域的广阔应用前景。"}}
{"id": "2506.10426", "title": "Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models", "authors": ["Xiao Yu", "Haoxuan Chen", "Feifei Niu", "Xing Hu", "Jacky Wai Keung", "Xin Xia"], "summary": "With the rapid development of large language models (LLMs), distributed\ntraining and inference frameworks like DeepSpeed have become essential for\nscaling model training and inference across multiple GPUs or nodes. However,\nthe increasing complexity of these frameworks brings non-trivial software bugs,\nwhich may degrade training performance, cause unexpected failures, and result\nin significant resource waste. Understanding framework bugs' characteristics is\nfundamental for quality assurance, allowing the design of more effective\ndebugging and repair methods. Thus, our paper conducts the first large-scale\nempirical analysis of 308 fixed bugs across three popular distributed\ntraining/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We\nexamine bug symptoms, root causes, bug identification and fixing efforts, and\ncommon low-effort fixing strategies. Additionally, the distributed nature of\nthese frameworks introduces unique bug root causes, such as allocation strategy\nerror and distributed communication error. Diagnosing and fixing complex bugs\nremains challenging due to factors like the disconnect between symptoms and\nroot causes, high bug reproduction costs, and low-level or cross-component\ninteractions. Interestingly, we observe that 48% of bug fixes require minimal\ncode changes (<=10 LOC) and follow simple strategies such as conditional logic\noptimization, parameter handling enhancement, or version compatibility\nhandling, indicating potential for automation. Based on these insights, we\noffer several implications for improving the reliability of both distributed\ntraining and inference frameworks and their dependent LLM projects, while also\nidentifying opportunities to leverage LLM-based tools for automated debugging\nand repair.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10426v1", "AI": {"title_translation": "走向理解大型语言模型分布式训练与推理框架中的错误", "tldr": "对大型语言模型分布式训练与推理框架中的308个已修复错误进行了首次大规模实证分析，揭示了错误特性、修复挑战及自动化修复潜力。", "motivation": "随着大型语言模型（LLMs）的快速发展，分布式训练和推理框架变得至关重要。然而，这些框架日益增长的复杂性带来了非小可的软件错误，可能降低训练性能、导致意外故障并造成资源浪费。理解框架错误的特性对于质量保证至关重要，有助于设计更有效的调试和修复方法。", "method": "本文对DeepSpeed、Megatron-LM和Colossal-AI这三个流行的分布式训练/推理框架中的308个已修复错误进行了首次大规模实证分析。研究内容包括错误症状、根本原因、错误识别和修复工作量，以及常见的低成本修复策略。", "result": "分布式框架引入了独特的错误根本原因，例如分配策略错误和分布式通信错误。诊断和修复复杂错误仍然具有挑战性，原因包括症状与根本原因脱节、错误复现成本高以及底层或跨组件交互。有趣的是，48%的错误修复仅需要最少的代码更改（<=10行代码），并遵循简单的策略，如条件逻辑优化、参数处理增强或版本兼容性处理，这表明了自动化的潜力。", "conclusion": "基于这些见解，本文为提高分布式训练和推理框架及其依赖的LLM项目的可靠性提供了多项启示，同时指出了利用基于LLM的工具进行自动化调试和修复的机会。", "translation": "随着大型语言模型（LLMs）的快速发展，DeepSpeed等分布式训练和推理框架已成为跨多个GPU或节点扩展模型训练和推理的必要工具。然而，这些框架日益增长的复杂性带来了非小可的软件错误，可能降低训练性能、导致意外故障并造成大量资源浪费。理解框架错误的特性对于质量保证至关重要，有助于设计更有效的调试和修复方法。因此，我们的论文对DeepSpeed、Megatron-LM和Colossal-AI这三个流行的分布式训练/推理框架中308个已修复错误进行了首次大规模实证分析。我们研究了错误症状、根本原因、错误识别和修复工作量，以及常见的低成本修复策略。此外，这些框架的分布式特性引入了独特的错误根本原因，例如分配策略错误和分布式通信错误。由于症状与根本原因脱节、错误复现成本高以及底层或跨组件交互等因素，诊断和修复复杂错误仍然具有挑战性。有趣的是，我们观察到48%的错误修复仅需要最少的代码更改（<=10行代码），并遵循简单的策略，如条件逻辑优化、参数处理增强或版本兼容性处理，这表明了自动化的潜力。基于这些见解，我们为提高分布式训练和推理框架及其依赖的LLM项目的可靠性提供了多项启示，同时指出了利用基于LLM的工具进行自动化调试和修复的机会。", "summary": "本研究首次对DeepSpeed、Megatron-LM和Colossal-AI等大型语言模型分布式训练与推理框架中的308个已修复错误进行了大规模实证分析。研究揭示了分布式环境下独特的错误原因，指出了复杂错误诊断与修复的挑战，并发现近一半的错误修复只需少量代码改动，且可遵循简单策略，预示了自动化修复的潜力。研究结果为提升框架可靠性及利用LLM工具进行自动化调试和修复提供了重要启示。", "keywords": "分布式训练, 大型语言模型, 软件错误, 实证分析, 调试", "comments": "本文通过对实际分布式训练/推理框架中的大量已修复错误进行首次大规模实证分析，为理解LLM框架中的错误特性提供了宝贵的第一手资料。其创新点在于揭示了分布式特有的错误类型和修复挑战，并量化了简单修复的比例，为未来的自动化调试和修复工具的开发指明了方向。特别是指出48%的错误修复所需代码量小且策略简单，这对于自动化修复工具的研发具有重要指导意义。"}}
{"id": "2506.10363", "title": "Towards more efficient quantitative safety validation of residual risk for assisted and automated driving", "authors": ["Daniel Betschinske", "Malte Schrimpf", "Steven Peters", "Kamil Klonecki", "Jan Peter Karch", "Moritz Lippert"], "summary": "The safety validation of Advanced Driver Assistance Systems (ADAS) and\nAutomated Driving Systems (ADS) increasingly demands efficient and reliable\nmethods to quantify residual risk while adhering to international standards\nsuch as ISO 21448. Traditionally, Field Operational Testing (FOT) has been\npivotal for macroscopic safety validation of automotive driving functions up to\nSAE automation level 2. However, state-of-the-art derivations for empirical\nsafety demonstrations using FOT often result in impractical testing efforts,\nparticularly at higher automation levels. Even at lower automation levels, this\nlimitation - coupled with the substantial costs associated with FOT - motivates\nthe exploration of approaches to enhance the efficiency of FOT-based\nmacroscopic safety validation. Therefore, this publication systematically\nidentifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,\nincluding novel methods reported in the literature. Based on an analysis of ISO\n21448, two models are derived: a generic model capturing the argumentation\ncomponents of the standard, and a base model, exemplarily applied to Automatic\nEmergency Braking (AEB) systems, establishing a baseline for the real-world\ndriving requirement for a Quantitative Safety Validation of Residual Risk\n(QSVRR). Subsequently, the RAs are assessed using four criteria:\nquantifiability, threats to validity, missing links, and black box\ncompatibility, highlighting potential benefits, inherent limitations, and\nidentifying key areas for further research. Our evaluation reveals that, while\nseveral approaches offer potential, none are free from missing links or other\nsubstantial shortcomings. Moreover, no identified alternative can fully replace\nFOT, reflecting its crucial role in the safety validation of ADAS and ADS.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10363v1", "AI": {"title_translation": "面向辅助和自动驾驶残余风险更高效定量安全验证", "tldr": "本研究系统地识别并评估了用于辅助和自动驾驶系统定量安全验证的现场运行测试（FOT）的现有缩减方法，发现尽管一些方法有潜力，但无一能完全替代FOT，且都存在显著缺陷。", "motivation": "辅助驾驶系统（ADAS）和自动驾驶系统（ADS）的安全验证需要高效可靠的方法来量化残余风险，并遵循ISO 21448等国际标准。传统的现场运行测试（FOT）在SAE L2级及以下宏观安全验证中至关重要，但其经验性安全演示所需的测试工作量巨大且不切实际，特别是在更高级别自动化中。即使在较低级别，FOT的高昂成本也促使人们探索提高其效率的方法。", "method": "本研究系统地识别并评估了现有及文献中报道的新型现场运行测试（FOT）缩减方法（RAs）。基于对ISO 21448的分析，推导了两个模型：一个通用模型，捕捉标准的论证组成部分；一个基础模型，以自动紧急制动（AEB）系统为例，为残余风险的定量安全验证（QSVRR）的实际驾驶要求建立了基线。随后，使用四个标准（可量化性、有效性威胁、缺失环节和黑盒兼容性）评估了这些缩减方法。", "result": "评估结果显示，尽管有几种缩减方法具有潜力，但它们都存在缺失环节或其他实质性缺点。此外，没有一种已识别的替代方法能够完全取代现场运行测试（FOT），这反映了FOT在ADAS和ADS安全验证中的关键作用。", "conclusion": "本研究得出结论，当前没有一种缩减方法可以完全替代现场运行测试（FOT）在辅助和自动驾驶系统安全验证中的关键作用，尽管一些方法提供了提高效率的潜力，但它们仍存在显著的局限性和需要进一步研究的领域。", "translation": "辅助驾驶系统（ADAS）和自动驾驶系统（ADS）的安全验证越来越需要高效可靠的方法来量化残余风险，同时遵守ISO 21448等国际标准。传统上，现场运行测试（FOT）对于SAE自动化级别2及以下的汽车驾驶功能的宏观安全验证至关重要。然而，使用FOT进行经验性安全演示的最新推导通常导致不切实际的测试工作量，特别是在更高级别的自动化中。即使在较低的自动化级别，这种限制——加上与FOT相关的高昂成本——促使人们探索提高基于FOT的宏观安全验证效率的方法。因此，本出版物系统地识别并评估了FOT的最新缩减方法（RAs），包括文献中报道的新方法。基于对ISO 21448的分析，推导了两个模型：一个通用模型，捕捉标准的论证组成部分；一个基础模型，以自动紧急制动（AEB）系统为例，为残余风险的定量安全验证（QSVRR）的实际驾驶要求建立了基线。随后，使用四个标准评估了这些缩减方法：可量化性、有效性威胁、缺失环节和黑盒兼容性，突出了潜在益处、固有局限性，并确定了进一步研究的关键领域。我们的评估表明，尽管有几种方法提供了潜力，但没有一种方法不存在缺失环节或其他实质性缺陷。此外，没有一种已识别的替代方法能够完全取代FOT，这反映了其在ADAS和ADS安全验证中的关键作用。", "summary": "本论文旨在提高辅助和自动驾驶系统残余风险定量安全验证的效率。鉴于传统现场运行测试（FOT）在高级别自动化中效率低下且成本高昂，作者系统地识别并评估了FOT的现有及新型缩减方法。研究基于ISO 21448推导了通用模型和AEB示例基础模型，并从可量化性、有效性威胁、缺失环节和黑盒兼容性等角度评估了这些缩减方法。结果表明，尽管一些缩减方法具有潜力，但均存在显著缺陷且无法完全替代FOT，强调了FOT在ADAS和ADS安全验证中的不可替代性。", "keywords": "自动驾驶, 安全验证, 残余风险, 现场运行测试, 缩减方法", "comments": "本文系统地分析了当前自动驾驶系统安全验证中FOT的效率问题，并对各类缩减方法进行了全面评估。其创新点在于结合ISO 21448标准，提出了模型来指导验证，并用明确的评估标准审视了现有方法。重要性在于指出了当前缩减方法的局限性，明确了FOT的不可替代性，并为未来研究指明了方向。局限性在于虽然评估了方法，但并未提出具体的、能弥补当前缺陷的新方法。"}}
{"id": "2506.10927", "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review", "authors": ["T. T. J. E. Arets", "G. Perugia", "M. Houben", "W. A. IJsselsteijn"], "summary": "Reduced social connectedness increasingly poses a threat to mental health,\nlife expectancy, and general well-being. Generative AI (GAI) technologies, such\nas large language models (LLMs) and image generation tools, are increasingly\nintegrated into applications aimed at enhancing human social experiences.\nDespite their growing presence, little is known about how these technologies\ninfluence social interactions. This scoping review investigates how GAI-based\napplications are currently designed to facilitate social interaction, what\nforms of social engagement they target, and which design and evaluation\nmethodologies designers use to create and evaluate them. Through an analysis of\n30 studies published since 2020, we identify key trends in application domains\nincluding storytelling, socio-emotional skills training, reminiscence,\ncollaborative learning, music making, and general conversation. We highlight\nthe role of participatory and co-design approaches in fostering both effective\ntechnology use and social engagement, while also examining socio-ethical\nconcerns such as cultural bias and accessibility. This review underscores the\npotential of GAI to support dynamic and personalized interactions, but calls\nfor greater attention to equitable design practices and inclusive evaluation\nstrategies.", "comment": "Preprint version of a manuscript submitted to ACM Transactions on\n  Computer-Human Interaction (TOCHI), under review. 39 pages, 4 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10927v1", "AI": {"title_translation": "生成式AI在促进社会互动中的作用：一项范围综述", "tldr": "这项范围综述调查了生成式AI（GAI）如何被设计用于促进社会互动，识别了应用领域、设计方法和伦理担忧，并强调了GAI支持个性化互动的潜力以及对公平设计的需求。", "motivation": "社交联系减少日益对心理健康、预期寿命和整体福祉构成威胁。尽管生成式AI（GAI）技术（如大型语言模型和图像生成工具）越来越多地被整合到旨在增强人类社交体验的应用中，但人们对其如何影响社交互动知之甚少。", "method": "这是一项范围综述，分析了自2020年以来发表的30项研究，以调查基于GAI的应用如何被设计用于促进社交互动，它们针对何种形式的社交参与，以及设计人员使用哪些设计和评估方法来创建和评估它们。", "result": "识别了应用领域中的关键趋势，包括讲故事、社交情感技能训练、回忆、协作学习、音乐制作和一般对话。强调了参与式和协同设计方法在促进有效技术使用和社交参与方面的作用。审视了社会伦理问题，如文化偏见和可访问性。", "conclusion": "生成式AI有潜力支持动态和个性化的互动，但需要更多地关注公平的设计实践和包容性的评估策略。", "translation": "社交联系减少日益对心理健康、预期寿命和整体福祉构成威胁。生成式人工智能（GAI）技术，如大型语言模型（LLMs）和图像生成工具，正越来越多地被整合到旨在增强人类社交体验的应用中。尽管它们日益普及，但人们对这些技术如何影响社交互动知之甚少。这项范围综述调查了基于GAI的应用目前是如何被设计来促进社交互动的，它们针对何种形式的社交参与，以及设计者使用哪些设计和评估方法来创建和评估它们。通过对自2020年以来发表的30项研究的分析，我们确定了应用领域中的关键趋势，包括讲故事、社交情感技能训练、回忆、协作学习、音乐制作和一般对话。我们强调了参与式和协同设计方法在促进有效技术使用和社交参与方面的作用，同时还审视了文化偏见和可访问性等社会伦理问题。本综述强调了GAI支持动态和个性化互动的潜力，但呼吁更多地关注公平的设计实践和包容性的评估策略。", "summary": "这项范围综述考察了生成式AI（GAI）在促进社会互动中的作用，旨在理解GAI应用的设计方式、目标社交形式以及所用的设计与评估方法。通过分析30项研究，综述揭示了GAI在讲故事、技能训练等多个领域的应用趋势，并强调了参与式设计的重要性。同时，它也探讨了文化偏见和可访问性等伦理挑战，并最终指出GAI在支持个性化互动方面的潜力，呼吁未来的设计应更加公平和包容。", "keywords": "生成式AI, 社会互动, 范围综述, 人工智能设计, 伦理考量", "comments": "这篇综述及时地探讨了生成式AI在改善社会互动方面的潜在益处和挑战，特别是在当前社会联系日益减少的背景下。其创新之处在于系统性地梳理了GAI在不同社交应用场景中的设计和评估实践，并强调了公平性与包容性的重要性，为未来GAI的开发提供了伦理和设计上的指导。"}}
{"id": "2506.10693", "title": "Towards Sustainable Computing: Exploring Energy Consumption Efficiency of Alternative Configurations and Workloads in an Open Source Messaging System", "authors": ["Maria Voreakou", "George Kousiouris", "Mara Nikolaidou"], "summary": "Energy consumption in current large scale computing infrastructures is\nbecoming a critical issue, especially with the growing demand for centralized\nsystems such as cloud environments. With the advancement of microservice\narchitectures and the Internet of Things, messaging systems have become an\nintegral and mainstream part of modern computing infrastructures, carrying out\nsignificant workload in a majority of applications. In this paper, we describe\nan experimental process to explore energy-based benchmarking for RabbitMQ, one\nof the main open source messaging frameworks. The involved system is described,\nas well as required components, and setup scenarios, involving different\nworkloads and configurations among the tests as well as messaging system use\ncases. Alternative architectures are investigated and compared from an energy\nconsumption point of view, for different message rates and consumer numbers.\nDifferences in architectural selection have been quantified and can lead to up\nto 31\\% reduction in power consumption. The resulting dataset is made publicly\navailable and can thus prove helpful for architectures' comparison,\nenergy-based cost modeling, and beyond.", "comment": "2025 20th Annual System of Systems Engineering Conference (SoSE)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10693v1", "AI": {"title_translation": "迈向可持续计算：探索开源消息系统中替代配置和工作负载的能耗效率", "tldr": "本研究通过实验性基准测试，探讨了开源消息系统RabbitMQ在不同配置和工作负载下的能耗效率，并发现架构选择可将功耗降低高达31%。", "motivation": "当前大规模计算基础设施（特别是云环境中的集中式系统）的能耗日益成为关键问题。随着微服务架构和物联网的发展，消息系统已成为现代计算基础设施不可或缺的一部分，承载着大量应用的工作负载。因此，探索消息系统的能耗效率至关重要。", "method": "本文描述了一个实验过程，对开源消息框架RabbitMQ进行了基于能耗的基准测试。研究了涉及不同工作负载、配置和消息系统用例的系统、所需组件和设置场景。从能耗角度对不同消息速率和消费者数量下的替代架构进行了调查和比较。", "result": "研究量化了架构选择的差异，结果显示功耗可降低高达31%。由此产生的数据集已公开发布。", "conclusion": "架构选择对消息系统的能耗效率有显著影响，通过优化配置和工作负载，可以实现显著的能耗降低。公开的数据集有助于架构比较和基于能耗的成本建模。", "translation": "当前大规模计算基础设施中的能耗正成为一个关键问题，尤其是在对云环境等集中式系统的需求不断增长的情况下。随着微服务架构和物联网的进步，消息系统已成为现代计算基础设施不可或缺的主流组成部分，在大多数应用程序中承担着重要的工作负载。在本文中，我们描述了一个实验过程，旨在探索针对RabbitMQ（主要的开源消息框架之一）的基于能耗的基准测试。文中描述了所涉及的系统、所需组件和设置场景，其中包括测试中不同的工作负载和配置以及消息系统用例。从能耗角度对不同消息速率和消费者数量下的替代架构进行了调查和比较。架构选择的差异已被量化，并且可以导致高达31%的功耗降低。生成的数据集已公开发布，因此可以证明有助于架构比较、基于能耗的成本建模等。", "summary": "本研究旨在通过实验性基准测试，探索开源消息系统RabbitMQ在不同配置和工作负载下的能耗效率。研究分析了替代架构在不同消息速率和消费者数量下的能耗表现，并量化了架构选择对功耗的影响，发现最高可实现31%的功耗降低。相关数据集已公开发布，可用于架构比较和能耗成本建模。", "keywords": "能耗效率, 消息系统, RabbitMQ, 可持续计算, 基准测试", "comments": "这项研究的创新之处在于其专注于消息系统的能耗效率，这是一个在可持续计算背景下日益重要但常被忽视的领域。通过对RabbitMQ进行实证基准测试，并提供公开数据集，该论文为开发者和研究人员提供了宝贵的工具和见解，以构建更节能的分布式系统。其重要性在于直接量化了架构选择对能耗的影响，为优化系统设计提供了明确的指导。"}}
{"id": "2506.10815", "title": "Joint Beamforming with Extremely Large Scale RIS: A Sequential Multi-Agent A2C Approach", "authors": ["Zhi Chai", "Jiajie Xu", "Justin P Coon", "Mohamed-Slim Alouini"], "summary": "It is a challenging problem to jointly optimize the base station (BS)\nprecoding matrix and the reconfigurable intelligent surface (RIS) phases\nsimultaneously in a RIS-assisted multiple-user multiple-input-multiple-output\n(MU-MIMO) scenario when the size of the RIS becomes extremely large. In this\npaper, we propose a deep reinforcement learning algorithm called sequential\nmulti-agent advantage actor-critic (A2C) to solve this problem. In addition,\nthe discrete phase of RISs, imperfect channel state information (CSI), and\nchannel correlations between users are taken into consideration. The\ncomputational complexity is also analyzed, and the performance of the proposed\nalgorithm is compared with the zero-forcing (ZF) beamformer in terms of the sum\nspectral efficiency (SE). It is noted that the computational complexity of the\nproposed algorithm is lower than the benchmark, while the performance is better\nthan the benchmark. Throughout simulations, it is also found that the proposed\nalgorithm is robust to medium channel estimation error.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10815v1", "AI": {"title_translation": "采用超大规模RIS的联合波束成形：一种序列多智能体A2C方法", "tldr": "提出了一种基于序列多智能体A2C的深度强化学习算法，用于解决超大规模RIS辅助MU-MIMO系统中基站预编码和RIS相位的联合优化问题，该算法在计算复杂度更低的同时，性能优于基准算法。", "motivation": "在RIS辅助的多用户多输入多输出(MU-MIMO)场景中，当RIS尺寸变得超大时，同时优化基站(BS)预编码矩阵和可重构智能表面(RIS)相位是一个具有挑战性的问题。", "method": "提出了一种名为序列多智能体优势Actor-Critic (A2C) 的深度强化学习算法来解决该问题。该方法还考虑了RIS的离散相位、不完善的信道状态信息(CSI)以及用户间的信道相关性。", "result": "提出的算法的计算复杂度低于基准算法，同时在总谱效率(SE)方面性能优于基准算法。仿真结果还表明，该算法对中等信道估计误差具有鲁棒性。", "conclusion": "提出的序列多智能体A2C算法能够有效解决超大规模RIS辅助MU-MIMO系统中的联合波束成形问题，在降低计算复杂度的同时提升了性能，并对信道估计误差具有鲁棒性。", "translation": "在RIS辅助的多用户多输入多输出(MU-MIMO)场景中，当RIS的尺寸变得超大时，同时优化基站(BS)预编码矩阵和可重构智能表面(RIS)相位是一个具有挑战性的问题。在本文中，我们提出了一种名为序列多智能体优势Actor-Critic (A2C) 的深度强化学习算法来解决这个问题。此外，论文还考虑了RIS的离散相位、不完善的信道状态信息(CSI)以及用户间的信道相关性。论文分析了计算复杂度，并将所提出算法的性能与零迫(ZF)波束成形器在总谱效率(SE)方面进行了比较。值得注意的是，所提出算法的计算复杂度低于基准算法，而性能优于基准算法。通过仿真，还发现所提出算法对中等信道估计误差具有鲁棒性。", "summary": "本文针对超大规模RIS辅助MU-MIMO系统中基站预编码和RIS相位的联合优化难题，提出了一种基于序列多智能体优势Actor-Critic (A2C) 的深度强化学习算法。该算法考虑了RIS离散相位、不完善CSI和用户信道相关性，并通过仿真验证了其在计算复杂度更低的同时，总谱效率性能优于传统零迫波束成形器，且对中等信道估计误差具有鲁棒性。", "keywords": "超大规模RIS, 联合波束成形, 深度强化学习, 序列多智能体A2C, MU-MIMO", "comments": "该论文通过引入深度强化学习中的序列多智能体A2C方法，为超大规模RIS场景下的联合波束成形提供了一种新颖的解决方案。其创新点在于将复杂的联合优化问题建模为多智能体强化学习任务，并考虑了实际系统中的非理想因素。该方法在降低计算复杂度的同时提升了性能，这对于未来大规模MIMO和RIS部署具有重要意义。"}}
{"id": "2506.10408", "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges", "authors": ["Jintao Liang", "Gang Su", "Huifeng Lin", "You Wu", "Rui Zhao", "Ziyue Li"], "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\novercome the knowledge limitations of Large Language Models (LLMs) by\nintegrating external retrieval with language generation. While early RAG\nsystems based on static pipelines have shown effectiveness in well-structured\ntasks, they struggle in real-world scenarios requiring complex reasoning,\ndynamic retrieval, and multi-modal integration. To address these challenges,\nthe field has shifted toward Reasoning Agentic RAG, a paradigm that embeds\ndecision-making and adaptive tool use directly into the retrieval process. In\nthis paper, we present a comprehensive review of Reasoning Agentic RAG methods,\ncategorizing them into two primary systems: predefined reasoning, which follows\nfixed modular pipelines to boost reasoning, and agentic reasoning, where the\nmodel autonomously orchestrates tool interaction during inference. We analyze\nrepresentative techniques under both paradigms, covering architectural design,\nreasoning strategies, and tool coordination. Finally, we discuss key research\nchallenges and propose future directions to advance the flexibility,\nrobustness, and applicability of reasoning agentic RAG systems. Our collection\nof the relevant research has been organized into a\nhttps://github.com/ByebyeMonica/Reasoning-Agentic-RAG.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10408v1", "AI": {"title_translation": "通过系统1或系统2进行推理RAG：面向行业挑战的推理代理检索增强生成综述", "tldr": "本综述回顾了推理代理检索增强生成（Reasoning Agentic RAG），将其分为预定义推理和代理推理两种系统，并分析了其方法、挑战和未来方向，以解决传统RAG在复杂推理场景中的局限性。", "motivation": "传统的检索增强生成（RAG）系统在需要复杂推理、动态检索和多模态集成的现实世界场景中表现不佳，难以克服大型语言模型（LLMs）的知识局限性。为应对这些挑战，研究领域转向了推理代理RAG。", "method": "本文对推理代理RAG方法进行了全面综述，将其分为两大主要系统：遵循固定模块化管道以增强推理的“预定义推理”和模型在推理过程中自主协调工具交互的“代理推理”。文章分析了这两种范式下的代表性技术，涵盖架构设计、推理策略和工具协调。", "result": "本文成功地将推理代理RAG方法分为预定义推理和代理推理两大系统，并对每种系统下的代表性技术进行了详细分析，涵盖了架构设计、推理策略和工具协调等方面。", "conclusion": "推理代理RAG旨在通过嵌入决策和自适应工具使用来解决传统RAG在复杂推理场景中的局限性。未来研究应着重于提高推理代理RAG系统的灵活性、鲁棒性和适用性。", "translation": "检索增强生成（RAG）已成为一个强大的框架，通过将外部检索与语言生成相结合，克服大型语言模型（LLM）的知识局限性。虽然基于静态管道的早期RAG系统在结构良好的任务中显示出有效性，但它们在需要复杂推理、动态检索和多模态集成的现实世界场景中举步维艰。为了应对这些挑战，该领域已转向推理代理RAG，这是一种将决策制定和自适应工具使用直接嵌入到检索过程中的范式。在本文中，我们对推理代理RAG方法进行了全面综述，将其分为两大主要系统：预定义推理，它遵循固定的模块化管道以增强推理；以及代理推理，其中模型在推理过程中自主协调工具交互。我们分析了这两种范式下的代表性技术，涵盖了架构设计、推理策略和工具协调。最后，我们讨论了关键的研究挑战，并提出了未来的方向，以提高推理代理RAG系统的灵活性、鲁棒性和适用性。我们收集的相关研究已整理到https://github.com/ByebyeMonica/Reasoning-Agentic-RAG。", "summary": "本综述论文全面审视了推理代理检索增强生成（Reasoning Agentic RAG）领域，旨在解决传统RAG在复杂推理和动态场景中的不足。文章将推理代理RAG方法划分为预定义推理和代理推理两大范式，并深入分析了各自的架构设计、推理策略和工具协调技术。最后，论文探讨了当前的研究挑战，并为提升推理代理RAG系统的灵活性、鲁棒性和适用性提出了未来的研究方向。", "keywords": "推理代理RAG, 检索增强生成, 大型语言模型, 预定义推理, 代理推理", "comments": "这篇综述论文及时地总结了RAG领域从静态管道向更具动态性和自主性的推理代理RAG的演变，对理解当前RAG技术面临的挑战和发展趋势具有重要意义。其将推理代理RAG划分为预定义推理和代理推理的分类方式清晰且具洞察力，为后续研究提供了良好的框架。论文不仅梳理了现有技术，还指出了未来的研究方向，对推动该领域的发展具有指导作用。"}}
{"id": "2506.10916", "title": "Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "authors": ["Meredith VandeHaar", "M. Clinch", "I. Yilmaz", "M. A. Rahman", "Y. Xiao", "F. Dogany", "H. M. Alazab", "A. Nassar", "Z. Akkus", "B. Dangott"], "summary": "Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10916v1", "AI": {"title_translation": "半自动化数字病理质量保证：图块分类方法", "tldr": "该研究提出一种基于深度学习的AI算法，通过对数字病理图像图块进行分类来半自动化检测和定位伪影，从而提高数字病理图像质量保证的效率和准确性。", "motivation": "数字病理中的伪影会严重影响AI诊断模型的性能，而当前的质量保证方法主要依赖人工审查且未充分利用深度学习，导致效率低下和准确性有限。", "method": "提出一种AI算法，通过分析数字病理图像的图块并将其分类为10种预定义伪影类型之一或背景。该算法识别并定位伪影，创建一张突出显示感兴趣区域的伪影图。通过将人类操作员引导至受伪影影响的特定图块，减少手动审查整个玻片所需的时间和精力。研究使用了InceptionResNet模型，并训练测试了单一伪影模型以及有限的多实例模型（针对颤动、折叠和笔迹伪影）。", "result": "研究表明，混合设计（结合单一伪影二元模型和多实例模型）可以优化各种伪影的检测。", "conclusion": "建议采用结合单一伪影二元模型和多实例模型的混合设计，以优化数字病理图像中各种伪影的检测。", "translation": "质量保证是数字病理学中一个关键但尚未被充分探索的领域，即使是微小的伪影也可能产生显著影响。伪影已被证明会对AI诊断模型的性能产生负面影响。在当前实践中，训练有素的工作人员在将数字化图像发布给病理学家之前会手动审查这些图像，然后病理学家使用这些图像进行诊断。传统的图像处理方法为检测数字病理玻片上的伪影提供了基础。然而，目前的工具没有利用深度学习，而深度学习有潜力提高检测准确性和可扩展性。尽管有这些进展，数字病理学中的质量保证方法仍然有限，这为创新提供了空白。\n我们提出了一种AI算法，旨在通过分析图块并将其分为10种预定义伪影类型之一或背景来筛选数字病理玻片。该算法识别并定位伪影，创建一张突出显示感兴趣区域的地图。通过将人类操作员引导至受伪影影响的特定图块，该算法最大限度地减少了手动审查整个玻片以查找质量问题所需的时间和精力。\n从内部档案和癌症基因组图谱中，选择了133张全玻片图像，并使用内部开发的ZAPP软件（梅奥诊所，佛罗里达州杰克逊维尔）对10种伪影进行了标注。对不同图块大小和放大倍数下的多个模型进行了消融研究。选择了InceptionResNet。训练和测试了单一伪影模型，随后是有限的多实例模型，其中表现良好的伪影（颤动、折叠和笔迹）一起进行。根据本研究的结果，我们建议采用一种混合设计进行伪影筛选，该设计由单一伪影二元模型和多实例模型组成，以优化每种伪影的检测。", "summary": "本文提出了一种创新的半自动化AI算法，旨在提高数字病理图像的质量保证效率。该算法通过对图像图块进行深度学习分类，识别并定位10种常见伪影类型，并生成伪影地图以指导人工审查。通过使用InceptionResNet模型并结合单一伪影和多实例模型进行训练和测试，研究结果表明这种混合方法能有效优化伪影检测，从而减少人工审查的工作量并提升AI诊断的可靠性。", "keywords": "数字病理, 质量保证, 深度学习, 伪影检测, 图块分类", "comments": "这项研究创新性地将深度学习应用于数字病理质量保证这一关键但未被充分探索的领域。其提出的图块分类AI算法能够半自动化地识别和定位伪影，显著减少了人工审查的工作量，提高了效率。该方法对于提升数字病理图像的质量和下游AI诊断模型的性能具有重要意义。混合模型的设计也体现了对不同伪影检测复杂性的考量。"}}
{"id": "2506.10127", "title": "Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms", "authors": ["Xinyi Hu", "Aldo Pacchiano"], "summary": "We study the decentralized multi-player multi-armed bandits (MMAB) problem\nunder a no-sensing setting, where each player receives only their own reward\nand obtains no information about collisions. Each arm has an unknown capacity,\nand if the number of players pulling an arm exceeds its capacity, all players\ninvolved receive zero reward. This setting generalizes the classical\nunit-capacity model and introduces new challenges in coordination and capacity\ndiscovery under severe feedback limitations. We propose A-CAPELLA (Algorithm\nfor Capacity-Aware Parallel Elimination for Learning and Allocation), a\ndecentralized algorithm that achieves logarithmic regret in this generalized\nregime. Our main contribution is a collaborative hypothesis testing protocol\nthat enables synchronized successive elimination and capacity estimation\nthrough carefully structured collision patterns. This represents a provably\nefficient learning result in decentralized no-sensing MMAB with unknown arm\ncapacities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10127v1", "AI": {"title_translation": "在臂处相遇：可共享臂的协作多臂老虎机问题", "tldr": "本文研究了去中心化、无感知、多玩家多臂老虎机问题，其中臂具有未知容量且超载会导致零奖励。作者提出了A-CAPELLA算法，实现了对数遗憾，并通过协作假设检验协议实现了容量估计和同步消除。", "motivation": "研究去中心化多玩家多臂老虎机（MMAB）问题，特别是在无感知设置下，每个玩家只能收到自己的奖励且无法得知冲突。该设置推广了经典的单位容量模型，并引入了在反馈严重受限下进行协调和容量发现的新挑战。", "method": "提出了A-CAPELLA（容量感知并行消除学习和分配算法）去中心化算法。其主要贡献是一个协作假设检验协议，通过精心构造的冲突模式实现同步的逐次消除和容量估计。", "result": "A-CAPELLA算法在该广义机制下实现了对数遗憾。", "conclusion": "本文在去中心化、无感知、未知臂容量的MMAB问题中，通过A-CAPELLA算法提供了一个可证明高效的学习结果。", "translation": "我们研究了无感知设置下的去中心化多玩家多臂老虎机（MMAB）问题，其中每个玩家只接收自己的奖励，并且不获取关于冲突的信息。每个臂具有未知容量，如果拉动一个臂的玩家数量超过其容量，所有相关玩家将获得零奖励。这种设置推广了经典的单位容量模型，并引入了在反馈严重受限下进行协调和容量发现的新挑战。我们提出了A-CAPELLA（容量感知并行消除学习和分配算法），一个在该广义机制下实现对数遗憾的去中心化算法。我们的主要贡献是一个协作假设检验协议，通过精心构造的冲突模式实现同步的逐次消除和容量估计。这代表了在去中心化、无感知、未知臂容量的MMAB中一个可证明高效的学习结果。", "summary": "本文研究了去中心化、无感知、具有未知容量可共享臂的多玩家多臂老虎机（MMAB）问题。当拉动同一臂的玩家数量超过其容量时，所有相关玩家的奖励为零。为了解决在反馈受限下的协调和容量发现挑战，论文提出了A-CAPELLA算法。该算法通过一个协作假设检验协议，利用结构化的冲突模式实现同步的臂消除和容量估计，最终达到了对数遗憾，证明了其在该泛化MMAB设置下的学习效率。", "keywords": "多臂老虎机, 去中心化学习, 容量感知, 协作算法, 对数遗憾", "comments": "该论文的创新点在于将经典的多臂老虎机问题推广到具有未知容量的可共享臂场景，并解决了在无感知、去中心化设置下的协调和容量发现难题。A-CAPELLA算法通过其独特的协作假设检验协议，有效地实现了容量估计和同步消除，并在理论上证明了其学习效率，对多智能体强化学习和资源分配领域具有重要意义。"}}
{"id": "2506.10380", "title": "TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning", "authors": ["Xiaohan Yu", "Pu Jian", "Chong Chen"], "summary": "Retrieval-Augmented Generation (RAG) has demonstrated considerable\neffectiveness in open-domain question answering. However, when applied to\nheterogeneous documents, comprising both textual and tabular components,\nexisting RAG approaches exhibit critical limitations. The prevailing practice\nof flattening tables and chunking strategies disrupts the intrinsic tabular\nstructure, leads to information loss, and undermines the reasoning capabilities\nof LLMs in multi-hop, global queries. To address these challenges, we propose\nTableRAG, an hybrid framework that unifies textual understanding and complex\nmanipulations over tabular data. TableRAG iteratively operates in four steps:\ncontext-sensitive query decomposition, text retrieval, SQL programming and\nexecution, and compositional intermediate answer generation. We also develop\nHeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous\nreasoning capabilities. Experimental results demonstrate that TableRAG\nconsistently outperforms existing baselines on both public datasets and our\nHeteQA, establishing a new state-of-the-art for heterogeneous document question\nanswering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.", "comment": "Under review. Codes are available at\n  https://github.com/yxh-y/TableRAG/tree/main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10380v1", "AI": {"title_translation": "TableRAG：一种用于异构文档推理的检索增强生成框架", "tldr": "TableRAG是一个新的检索增强生成框架，专门用于处理包含文本和表格的异构文档，通过结合文本理解和表格操作，显著提升了多跳异构推理能力，并在多个数据集上取得了最先进的性能。", "motivation": "现有检索增强生成（RAG）方法在处理包含文本和表格的异构文档时存在局限性，因为表格的扁平化和分块策略会破坏其内在结构，导致信息丢失，并削弱大型语言模型（LLMs）在多跳、全局查询中的推理能力。", "method": "本文提出了TableRAG，一个混合框架，它统一了文本理解和复杂的表格数据操作。TableRAG迭代地通过四个步骤运行：上下文敏感查询分解、文本检索、SQL编程和执行、以及组合式中间答案生成。同时，还开发了一个名为HeteQA的新基准来评估多跳异构推理能力。", "result": "实验结果表明，TableRAG在公共数据集和新提出的HeteQA数据集上都始终优于现有基线，在异构文档问答方面建立了新的最先进水平。", "conclusion": "TableRAG通过其独特的方法有效解决了异构文档推理中的挑战，显著提升了检索增强生成模型在处理文本和表格混合数据时的性能，并为该领域设立了新的基准。", "translation": "检索增强生成（RAG）在开放域问答中表现出相当大的有效性。然而，当应用于包含文本和表格组件的异构文档时，现有的RAG方法表现出严重的局限性。表格扁平化和分块策略的普遍做法破坏了内在的表格结构，导致信息丢失，并损害了LLM在多跳、全局查询中的推理能力。为了解决这些挑战，我们提出了TableRAG，一个统一文本理解和表格数据复杂操作的混合框架。TableRAG迭代地通过四个步骤运行：上下文敏感查询分解、文本检索、SQL编程和执行，以及组合式中间答案生成。我们还开发了一个名为HeteQA的新基准，旨在评估多跳异构推理能力。实验结果表明，TableRAG在公共数据集和我们的HeteQA上都始终优于现有基线，为异构文档问答建立了新的最先进水平。我们在https://github.com/yxh-y/TableRAG/tree/main发布了TableRAG。", "summary": "本文提出了TableRAG，一个针对异构文档（包含文本和表格）设计的检索增强生成框架，以克服现有RAG方法在处理此类数据时因表格结构破坏导致的信息丢失和推理能力受限问题。TableRAG通过上下文查询分解、文本检索、SQL编程执行和组合式答案生成四个迭代步骤，有效整合了文本理解和表格操作。此外，研究团队还开发了HeteQA基准用于评估多跳异构推理。实验证明，TableRAG在多个数据集上均超越了现有基线，确立了异构文档问答领域的最新技术水平。", "keywords": "检索增强生成, 异构文档, 表格推理, 多跳问答, TableRAG", "comments": "TableRAG的创新之处在于其混合框架设计，巧妙地将文本理解与SQL编程结合，有效地解决了异构文档中表格结构破坏和信息丢失的问题。它通过迭代的四步流程，显著提升了LLMs在复杂多跳查询中的推理能力。同时，HeteQA基准的引入也为异构推理能力的评估提供了重要的工具，推动了该领域的发展。该工作对于提升RAG在更复杂现实世界数据应用中的表现具有重要意义。"}}
{"id": "2506.10202", "title": "Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval", "authors": ["Shubhashis Roy Dipta", "Francis Ferraro"], "summary": "Recent approaches have shown impressive proficiency in extracting and\nleveraging parametric knowledge from Large-Language Models (LLMs) and\nVision-Language Models (VLMs). In this work, we consider how we can improve the\nidentification and retrieval of videos related to complex real-world events by\nautomatically extracting latent parametric knowledge about those events. We\npresent Q2E: a Query-to-Event decomposition method for zero-shot multilingual\ntext-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our\napproach demonstrates that we can enhance the understanding of otherwise overly\nsimplified human queries by decomposing the query using the knowledge embedded\nin LLMs and VLMs. We additionally show how to apply our approach to both visual\nand speech-based inputs. To combine this varied multimodal knowledge, we adopt\nentropy-based fusion scoring for zero-shot fusion. Through evaluations on two\ndiverse datasets and multiple retrieval metrics, we demonstrate that Q2E\noutperforms several state-of-the-art baselines. Our evaluation also shows that\nintegrating audio information can significantly improve text-to-video\nretrieval. We have released code and data for future research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10202v1", "AI": {"title_translation": "Q2E：用于零样本多语言文本到视频检索的查询到事件分解", "tldr": "Q2E是一种将查询分解为事件的方法，利用大型语言模型和视觉语言模型的知识，以改进零样本多语言文本到视频检索。它通过分解复杂查询并融合多模态信息（包括音频）来超越现有基线。", "motivation": "现有方法在提取和利用大型语言模型（LLMs）和视觉语言模型（VLMs）中的参数知识方面表现出色，但仍需改进对复杂现实世界事件相关视频的识别和检索。本研究旨在通过自动提取这些事件的潜在参数知识来解决这一问题，从而增强对过于简化的用户查询的理解。", "method": "本研究提出了Q2E（Query-to-Event）方法，这是一种用于零样本多语言文本到视频检索的查询到事件分解方法。它通过利用LLMs和VLMs中嵌入的知识来分解用户查询，从而增强对复杂查询的理解。该方法适用于不同数据集、领域、LLMs或VLMs。为了结合视觉和语音输入等多种模态知识，采用了基于熵的融合评分进行零样本融合。", "result": "在两个不同数据集和多个检索指标上的评估表明，Q2E优于多个最先进的基线方法。评估还显示，整合音频信息可以显著改善文本到视频检索的性能。", "conclusion": "Q2E是一种有效且通用的查询到事件分解方法，能够利用LLMs和VLMs的知识来增强对复杂查询的理解，并结合多模态信息（特别是音频）来显著提高零样本多语言文本到视频检索的性能。", "translation": "最近的方法在从大型语言模型（LLMs）和视觉语言模型（VLMs）中提取和利用参数知识方面表现出令人印象深刻的熟练度。在这项工作中，我们考虑如何通过自动提取关于复杂现实世界事件的潜在参数知识来改进对这些事件相关视频的识别和检索。我们提出了Q2E：一种用于零样本多语言文本到视频检索的查询到事件分解方法，可适应不同数据集、领域、LLMs或VLMs。我们的方法表明，通过使用LLMs和VLMs中嵌入的知识分解查询，我们可以增强对原本过于简化的人类查询的理解。我们还展示了如何将我们的方法应用于视觉和基于语音的输入。为了结合这种多样化的多模态知识，我们采用基于熵的融合评分进行零样本融合。通过在两个不同数据集和多个检索指标上的评估，我们证明了Q2E优于多个最先进的基线。我们的评估还表明，整合音频信息可以显著改善文本到视频检索。我们已经发布了代码和数据以供未来研究。", "summary": "本论文提出了Q2E（Query-to-Event）方法，旨在通过利用大型语言模型和视觉语言模型中的知识，将复杂的查询分解为事件，从而改进零样本多语言文本到视频检索。Q2E能够增强对简化用户查询的理解，并支持视觉和语音等多模态输入，通过熵基融合进行信息整合。实验证明，Q2E在多个数据集上超越了现有先进方法，并强调了音频信息对文本到视频检索性能的显著提升作用。", "keywords": "文本到视频检索, 零样本学习, 多模态融合, 查询分解, 大语言模型", "comments": "Q2E方法通过创新性地将查询分解为事件，利用LLMs和VLMs的强大知识，解决了复杂查询在文本到视频检索中的理解难题。其零样本多语言能力和跨数据集、领域、模型的高度适应性是其重要亮点。此外，对多模态信息（特别是音频）的有效融合，进一步提升了检索精度，展现了其在实际应用中的巨大潜力。这项工作为未来多模态检索研究提供了有价值的思路和基线。"}}
{"id": "2506.10178", "title": "Attention, Please! Revisiting Attentive Probing for Masked Image Modeling", "authors": ["Bill Psomas", "Dionysis Christopoulos", "Eirini Baltzi", "Ioannis Kakogeorgiou", "Tilemachos Aravanis", "Nikos Komodakis", "Konstantinos Karantzalos", "Yannis Avrithis", "Giorgos Tolias"], "summary": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attentive probing remains\nunder-explored, with existing methods suffering from excessive parameterization\nand poor computational efficiency.\n  In this work, we revisit attentive probing through the lens of the\naccuracy-efficiency trade-off. We conduct a systematic study of existing\nmethods, analyzing their mechanisms and benchmarking their performance. We\nintroduce efficient probing (EP), a multi-query cross-attention mechanism that\neliminates redundant projections, reduces the number of trainable parameters,\nand achieves up to a 10$\\times$ speed-up over conventional multi-head\nattention. Despite its simplicity, EP outperforms LP and prior attentive\nprobing approaches across seven benchmarks, generalizes well beyond MIM to\ndiverse pre-training paradigms, produces interpretable attention maps, and\nachieves strong gains in low-shot and layer-wise settings. Code available at\nhttps://github.com/billpsomas/efficient-probing.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10178v1", "AI": {"title_translation": "注意，请！重新审视用于掩码图像建模的注意力探测", "tldr": "现有注意力探测方法参数过多且效率低下。本文提出高效探测（EP），一种多查询交叉注意力机制，显著提升效率和性能，并在多项基准测试中超越现有方法。", "motivation": "随着大规模微调变得不切实际，探测成为自监督学习评估的首选协议。然而，标准线性探测（LP）无法充分反映掩码图像建模（MIM）训练模型的潜力，因为补丁令牌的分布式特性。现有注意力探测方法存在参数过多和计算效率低下的问题。", "method": "本文通过精度-效率权衡的视角重新审视注意力探测。系统研究了现有方法，分析其机制并进行性能基准测试。引入了高效探测（EP），一种多查询交叉注意力机制，该机制消除了冗余投影，减少了可训练参数数量，并实现了比传统多头注意力高达10倍的加速。", "result": "EP在七个基准测试中优于线性探测（LP）和先前的注意力探测方法，能很好地泛化到MIM之外的多种预训练范式，生成可解释的注意力图，并在低样本和层级设置中获得显著增益。与传统多头注意力相比，EP实现了高达10倍的加速。", "conclusion": "高效探测（EP）通过改进注意力探测的效率和性能，为自监督学习模型的评估提供了一个更优越的替代方案，尤其适用于掩码图像建模。", "translation": "随着大规模微调（FT）变得越来越不切实际，探测（probing）正成为自监督学习（SSL）的首选评估协议。然而，由于补丁令牌的分布式性质，标准的线性探测（LP）未能充分反映通过掩码图像建模（MIM）训练的模型的潜力。这促使了注意力探测的需求，它是一种利用注意力选择性聚合补丁级特征的替代方案。尽管其应用日益广泛，但注意力探测仍未得到充分探索，现有方法存在参数过多和计算效率低下的问题。\n在这项工作中，我们从精度-效率权衡的角度重新审视了注意力探测。我们对现有方法进行了系统研究，分析了它们的机制并对其性能进行了基准测试。我们引入了高效探测（EP），这是一种多查询交叉注意力机制，它消除了冗余投影，减少了可训练参数的数量，并实现了比传统多头注意力高达10倍的加速。尽管其简单，EP在七个基准测试中超越了LP和先前的注意力探测方法，能很好地泛化到MIM之外的多种预训练范式，生成可解释的注意力图，并在低样本和层级设置中获得了显著增益。代码可在https://github.com/billpsomas/efficient-probing 获取。", "summary": "本文针对自监督学习模型评估中现有注意力探测方法参数过多和效率低下的问题，提出了高效探测（EP）。EP是一种多查询交叉注意力机制，通过消除冗余投影和减少参数，实现了显著的计算加速和性能提升。实验表明，EP在多个基准测试中优于线性探测和现有注意力探测方法，并具有良好的泛化能力和可解释性。", "keywords": "注意力探测, 掩码图像建模, 自监督学习, 高效探测, 线性探测", "comments": "这篇论文通过引入高效探测（EP）有效地解决了现有注意力探测方法在可扩展性和效率方面的挑战。其创新点在于提出了一种简化的多查询交叉注意力机制，该机制在保持甚至超越性能的同时，显著降低了计算成本和参数量。这对于大规模自监督学习模型的评估具有重要意义，因为它提供了一个更实用、更准确的评估工具。论文还强调了其方法在泛化能力和可解释性方面的优势，进一步提升了其价值。"}}
{"id": "2506.10636", "title": "Structure and asymptotic preserving deep neural surrogates for uncertainty quantification in multiscale kinetic equations", "authors": ["Wei Chen", "Giacomo Dimarco", "Lorenzo Pareschi"], "summary": "The high dimensionality of kinetic equations with stochastic parameters poses\nmajor computational challenges for uncertainty quantification (UQ). Traditional\nMonte Carlo (MC) sampling methods, while widely used, suffer from slow\nconvergence and high variance, which become increasingly severe as the\ndimensionality of the parameter space grows. To accelerate MC sampling, we\nadopt a multiscale control variates strategy that leverages low-fidelity\nsolutions from simplified kinetic models to reduce variance. To further improve\nsampling efficiency and preserve the underlying physics, we introduce surrogate\nmodels based on structure and asymptotic preserving neural networks (SAPNNs).\nThese deep neural networks are specifically designed to satisfy key physical\nproperties, including positivity, conservation laws, entropy dissipation, and\nasymptotic limits. By training the SAPNNs on low-fidelity models and enriching\nthem with selected high-fidelity samples from the full Boltzmann equation, our\nmethod achieves significant variance reduction while maintaining physical\nconsistency and asymptotic accuracy. The proposed methodology enables efficient\nlarge-scale prediction in kinetic UQ and is validated across both homogeneous\nand nonhomogeneous multiscale regimes. Numerical results demonstrate improved\naccuracy and computational efficiency compared to standard MC techniques.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10636v1", "AI": {"title_translation": "多尺度动理学方程不确定性量化的结构和渐近保持深度神经网络代理模型", "tldr": "本文提出了一种结合多尺度控制变量和结构渐近保持神经网络（SAPNNs）的方法，用于解决动理学方程不确定性量化中的计算挑战，显著提高了采样效率、方差减少和物理一致性。", "motivation": "动理学方程的高维性及其随机参数给不确定性量化（UQ）带来了巨大的计算挑战。传统的蒙特卡洛（MC）采样方法收敛速度慢且方差大，随着参数空间维度的增加，这些问题变得更加严重。", "method": "为了加速蒙特卡洛采样，本文采用了多尺度控制变量策略，利用简化动理学模型的低保真解来减少方差。为了进一步提高采样效率并保持底层物理特性，引入了基于结构和渐近保持神经网络（SAPNNs）的代理模型。这些深度神经网络被专门设计来满足关键的物理属性，包括正性、守恒定律、熵耗散和渐近极限。通过在低保真模型上训练SAPNNs，并用来自完整玻尔兹曼方程的选定高保真样本进行丰富，该方法实现了方差减少。", "result": "该方法在保持物理一致性和渐近精度的同时，实现了显著的方差减少。所提出的方法能够在动理学不确定性量化中进行高效的大规模预测，并在同质和非同质多尺度机制下得到验证。数值结果表明，与标准蒙特卡洛技术相比，该方法提高了精度和计算效率。", "conclusion": "本文提出的结合多尺度控制变量和结构渐近保持神经网络（SAPNNs）的方法，有效解决了动理学方程不确定性量化中的计算难题，显著提高了采样效率、精度和物理一致性，为大规模预测提供了高效工具。", "translation": "具有随机参数的动理学方程的高维性对不确定性量化（UQ）提出了重大的计算挑战。传统的蒙特卡洛（MC）采样方法虽然被广泛使用，但存在收敛缓慢和方差大的问题，随着参数空间维度的增加，这些问题变得越来越严重。为了加速蒙特卡洛采样，我们采用了多尺度控制变量策略，该策略利用简化动理学模型的低保真解来减少方差。为了进一步提高采样效率并保持底层物理特性，我们引入了基于结构和渐近保持神经网络（SAPNNs）的代理模型。这些深度神经网络被专门设计来满足关键的物理属性，包括正性、守恒定律、熵耗散和渐近极限。通过在低保真模型上训练SAPNNs，并用来自完整玻尔兹曼方程的选定高保真样本进行丰富，我们的方法在保持物理一致性和渐近精度的同时，实现了显著的方差减少。所提出的方法能够在动理学不确定性量化中进行高效的大规模预测，并在同质和非同质多尺度机制下得到验证。数值结果表明，与标准蒙特卡洛技术相比，该方法提高了精度和计算效率。", "summary": "本文针对高维动理学方程不确定性量化中蒙特卡洛采样效率低的问题，提出了一种结合多尺度控制变量和结构渐近保持神经网络（SAPNNs）的新方法。该方法利用低保真模型减少方差，并设计SAPNNs以满足关键物理属性。通过在低保真模型上训练并在高保真样本上丰富，该方法显著提高了采样效率、方差减少、物理一致性和计算效率，适用于大规模动理学不确定性量化预测。", "keywords": "不确定性量化, 动理学方程, 深度学习, 代理模型, 蒙特卡洛", "comments": "本文的创新点在于将多尺度控制变量与专门设计的结构和渐近保持神经网络（SAPNNs）相结合，以解决高维动理学方程不确定性量化中的计算难题。SAPNNs的设计确保了物理守恒律和渐近行为的保留，这对于物理建模至关重要。该方法在提高计算效率和精度方面表现出色，并能处理大规模预测，对多尺度物理系统的不确定性量化研究具有重要意义。"}}
{"id": "2506.10484", "title": "EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair", "authors": ["Fangwen Mu", "Junjie Wang", "Lin Shi", "Song Wang", "Shoubin Li", "Qing Wang"], "summary": "Automatically repairing software issues remains a fundamental challenge at\nthe intersection of software engineering and AI. Although recent advancements\nin Large Language Models (LLMs) have demonstrated potential for\nrepository-level repair tasks, current methodologies exhibit two notable\nlimitations: (1) they often address issues in isolation, neglecting to\nincorporate insights from previously resolved issues, and (2) they rely on\nstatic and rigid prompting strategies, which constrain their ability to\ngeneralize across diverse and evolving issue scenarios. Inspired by the dual\nmemory systems of human cognition, where episodic and semantic memories work\nsynergistically to support human reasoning and decision-making, we propose\nExpeRepair, a novel LLM-based approach that continuously learns from historical\nrepair experiences through dual-channel knowledge accumulation. ExpeRepair\norganizes historical repair experiences into two complementary memories: an\nepisodic memory that stores concrete repair demonstrations, and a semantic\nmemory that encodes abstract reflective insights. At inference time, ExpeRepair\nactivates both memory systems by retrieving relevant demonstrations from\nepisodic memory and recalling high-level repair insights from semantic memory.\nIt further enhances adaptability through dynamic prompt composition,\nsynergistically integrating both memory types to replace static prompts with\ncontext-aware, experience-driven prompts. Experiments on the SWE-bench Lite\nbenchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with\nClaude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10484v1", "AI": {"title_translation": "EXPEREPAIR：双记忆增强型基于LLM的仓库级程序修复", "tldr": "ExpeRepair是一个基于LLM的程序修复方法，它利用双记忆系统（情景记忆和语义记忆）从历史修复经验中学习，并通过动态提示组合提高修复能力，在SWE-bench Lite上表现优异。", "motivation": "现有基于大型语言模型（LLMs）的程序修复方法存在两个主要局限性：1) 它们通常孤立地处理问题，未能整合从先前已解决问题中获得的见解；2) 它们依赖静态和僵化的提示策略，限制了其在多样化和不断演变的问题场景中的泛化能力。", "method": "本文提出了ExpeRepair，一种新颖的基于LLM的方法，通过双通道知识积累持续学习历史修复经验。ExpeRepair将历史修复经验组织为情景记忆（存储具体修复演示）和语义记忆（编码抽象反思性见解）。在推理时，它通过从情景记忆中检索相关演示并从语义记忆中回忆高级修复见解来激活两个记忆系统。此外，ExpeRepair通过动态提示组合增强适应性，将两种记忆类型协同整合，以上下文感知、经验驱动的提示取代静态提示。", "result": "在SWE-bench Lite基准测试中，ExpeRepair使用Claude 3.7 Sonnet实现了49.3%的pass@1分数，超越了所有最先进的开源方法。", "conclusion": "ExpeRepair通过模拟人类双记忆系统和采用动态提示组合，有效解决了现有LLM程序修复方法在利用历史经验和泛化能力方面的局限性，并在仓库级程序修复任务中取得了显著优于现有SOTA开源方法的性能。", "translation": "自动修复软件问题仍然是软件工程和人工智能交叉领域的一个基本挑战。尽管大型语言模型（LLMs）的最新进展已显示出在仓库级修复任务中的潜力，但当前的方法存在两个显著的局限性：(1) 它们通常孤立地处理问题，忽略了结合先前已解决问题的见解；(2) 它们依赖静态和僵化的提示策略，这限制了它们在多样化和不断演变的问题场景中进行泛化的能力。受人类认知双记忆系统的启发，即情景记忆和语义记忆协同工作以支持人类推理和决策，我们提出了ExpeRepair，一种新颖的基于LLM的方法，通过双通道知识积累持续从历史修复经验中学习。ExpeRepair将历史修复经验组织成两种互补的记忆：存储具体修复演示的情景记忆，以及编码抽象反思性见解的语义记忆。在推理时，ExpeRepair通过从情景记忆中检索相关演示并从语义记忆中回忆高级修复见解来激活两个记忆系统。它通过动态提示组合进一步增强适应性，协同整合两种记忆类型，以上下文感知、经验驱动的提示取代静态提示。在SWE-bench Lite基准测试上的实验表明，ExpeRepair使用Claude 3.7 Sonnet实现了49.3%的pass@1分数，优于所有最先进的开源方法。", "summary": "ExpeRepair是一种创新的基于LLM的仓库级程序修复方法，旨在解决现有LLM方法在孤立处理问题和使用静态提示方面的局限性。该方法受人类双记忆系统启发，通过双通道知识积累持续学习历史修复经验，将经验分为情景记忆（具体示例）和语义记忆（抽象见解）。在推理时，ExpeRepair激活这两种记忆并结合动态提示组合，以提高修复的适应性。实验结果表明，ExpeRepair在SWE-bench Lite基准测试上取得了49.3%的pass@1分数，优于所有现有最先进的开源方法。", "keywords": "LLM, 程序修复, 双记忆系统, 仓库级修复, 动态提示", "comments": "ExpeRepair的创新点在于其引入了受人类认知启发的双记忆系统（情景记忆和语义记忆）来有效地积累和利用历史修复经验，以及采用动态提示组合策略。这解决了现有LLM在程序修复中泛化能力不足和未能充分利用历史经验的痛点。其方法论为LLM在复杂软件工程任务中的应用提供了新的思路，并显著提升了仓库级程序修复的性能，显示出其重要性和潜力。"}}
{"id": "2506.10383", "title": "RICE: Reactive Interaction Controller for Cluttered Canopy Environment", "authors": ["Nidhi Homey Parayil", "Thierry Peynot", "Chris Lehnert"], "summary": "Robotic navigation in dense, cluttered environments such as agricultural\ncanopies presents significant challenges due to physical and visual occlusion\ncaused by leaves and branches. Traditional vision-based or model-dependent\napproaches often fail in these settings, where physical interaction without\ndamaging foliage and branches is necessary to reach a target. We present a\nnovel reactive controller that enables safe navigation for a robotic arm in a\ncontact-rich, cluttered, deformable environment using end-effector position and\nreal-time tactile feedback. Our proposed framework's interaction strategy is\nbased on a trade-off between minimizing disturbance by maneuvering around\nobstacles and pushing through them to move towards the target. We show that\nover 35 trials in 3 experimental plant setups with an occluded target, the\nproposed controller successfully reached the target in all trials without\nbreaking any branch and outperformed the state-of-the-art model-free controller\nin robustness and adaptability. This work lays the foundation for safe,\nadaptive interaction in cluttered, contact-rich deformable environments,\nenabling future agricultural tasks such as pruning and harvesting in plant\ncanopies.", "comment": "This work has been submitted to the IEEE RAL for possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10383v1", "AI": {"title_translation": "RICE：用于杂乱冠层环境的反应式交互控制器", "tldr": "提出了一种新的反应式控制器RICE，使机械臂能够在杂乱、可变形的农业冠层中安全导航，通过结合末端执行器位置和实时触觉反馈，实现在不损坏植物的情况下到达目标，并在实验中表现优于现有技术。", "motivation": "传统基于视觉或依赖模型的方法在农业冠层等密集、杂乱环境中导航时面临挑战，因为物理和视觉遮挡导致它们难以在不损坏树叶和树枝的情况下进行物理交互以到达目标。", "method": "提出了一种新型的反应式控制器，它利用末端执行器位置和实时触觉反馈，使机械臂能够在接触丰富的、杂乱的、可变形的环境中安全导航。其交互策略基于最小化扰动（绕过障碍物）和推开障碍物（穿过障碍物）以接近目标之间的权衡。", "result": "在3个实验植物设置中进行了超过35次试验，目标被遮挡，所提出的控制器在所有试验中都成功到达了目标，没有折断任何树枝，并且在鲁棒性和适应性方面优于最先进的无模型控制器。", "conclusion": "这项工作为在杂乱、接触丰富的可变形环境中实现安全、自适应的交互奠定了基础，为未来在植物冠层中进行修剪和采摘等农业任务提供了可能性。", "translation": "农业冠层等密集、杂乱环境中的机器人导航由于树叶和树枝造成的物理和视觉遮挡而面临重大挑战。传统的基于视觉或依赖模型的方法在这些环境中往往会失败，因为需要进行物理交互而又不损坏树叶和树枝才能到达目标。我们提出了一种新颖的反应式控制器，该控制器利用末端执行器位置和实时触觉反馈，使机械臂能够在接触丰富、杂乱、可变形的环境中安全导航。我们提出的框架的交互策略基于通过绕过障碍物来最小化扰动和通过推开障碍物来向目标移动之间的权衡。我们通过在3个实验植物设置中进行超过35次试验，目标被遮挡，结果表明所提出的控制器在所有试验中都成功到达了目标，没有折断任何树枝，并且在鲁棒性和适应性方面优于最先进的无模型控制器。这项工作为在杂乱、接触丰富的可变形环境中实现安全、自适应的交互奠定了基础，为未来在植物冠层中进行修剪和采摘等农业任务提供了可能性。", "summary": "本文介绍了一种名为RICE的新型反应式控制器，旨在解决机器人在农业冠层等杂乱、接触丰富的环境中导航的挑战。该控制器结合了末端执行器位置和实时触觉反馈，通过在避开障碍物和推开障碍物之间进行权衡，实现了机械臂在不损坏植物的情况下安全到达目标。实验结果表明，RICE在多种植物设置中均能成功且无损地到达目标，并在鲁棒性和适应性上优于现有技术，为未来的农业机器人应用奠定了基础。", "keywords": "反应式控制器, 触觉反馈, 机器人导航, 农业机器人, 杂乱环境", "comments": "这篇论文的创新点在于提出了一个结合触觉反馈的反应式控制器，解决了传统视觉或模型方法在杂乱、可变形农业环境中的局限性。其“最小化扰动”与“推开障碍物”的交互策略非常实用，使得机器臂能够在不造成损坏的情况下进行有效物理交互，这对于农业采摘、修剪等任务具有重要意义。该研究为未来在复杂自然环境中部署机器人提供了坚实的基础。"}}
{"id": "2506.10932", "title": "Video-Mediated Emotion Disclosure: A Study of Mental Health Vlogging by People with Schizophrenia on YouTube", "authors": ["Jiaying Lizzy Liu", "Yan Zhang"], "summary": "Individuals with schizophrenia frequently experience intense emotions and\noften turn to vlogging as a medium for emotional expression. While previous\nresearch has predominantly focused on text based disclosure, little is known\nabout how individuals construct narratives around emotions and emotional\nexperiences in video blogs. Our study addresses this gap by analyzing 200\nYouTube videos created by individuals with schizophrenia. Drawing on media\nresearch and self presentation theories, we developed a visual analysis\nframework to disentangle these videos. Our analysis revealed diverse practices\nof emotion disclosure through both verbal and visual channels, highlighting the\ndynamic interplay between these modes of expression. We found that the\ndeliberate construction of visual elements, including environmental settings\nand specific aesthetic choices, appears to foster more supportive and engaged\nviewer responses. These findings underscore the need for future large scale\nquantitative research examining how visual features shape video mediated\ncommunication on social media platforms. Such investigations would inform the\ndevelopment of care centered video sharing platforms that better support\nindividuals managing illness experiences.", "comment": "10 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10932v1", "AI": {"title_translation": "视频介导的情绪披露：一项关于精神分裂症患者在YouTube上心理健康视频博客的研究", "tldr": "本研究分析了精神分裂症患者在YouTube上通过视频博客进行情绪披露的方式，发现视觉元素对观众的反应有重要影响。", "motivation": "以往研究主要关注基于文本的情绪披露，但对于精神分裂症患者如何在视频博客中构建情绪叙事知之甚少。本研究旨在填补这一空白。", "method": "本研究分析了200个由精神分裂症患者在YouTube上创建的视频。研究借鉴媒体研究和自我呈现理论，开发了一个视觉分析框架来解构这些视频。", "result": "分析揭示了通过言语和视觉渠道进行情绪披露的多种实践，强调了这些表达模式之间的动态相互作用。研究发现，视觉元素的精心构建（包括环境设置和特定的美学选择）似乎能促进更具支持性和参与性的观众反应。", "conclusion": "这些发现强调了未来需要进行大规模的定量研究，以检验视觉特征如何塑造社交媒体平台上的视频介导交流。此类研究将为开发以护理为中心的视频共享平台提供信息，从而更好地支持管理疾病体验的个体。", "translation": "精神分裂症患者经常经历强烈的情绪，并常转向视频博客作为情绪表达的媒介。虽然之前的研究主要集中在基于文本的披露上，但对于个体如何在视频博客中构建围绕情绪和情感体验的叙事知之甚少。我们的研究通过分析200个由精神分裂症患者创建的YouTube视频来解决这一空白。借鉴媒体研究和自我呈现理论，我们开发了一个视觉分析框架来解构这些视频。我们的分析揭示了通过言语和视觉渠道进行情绪披露的多种实践，突出了这些表达模式之间的动态相互作用。我们发现，视觉元素的精心构建，包括环境设置和特定的美学选择，似乎能促进更具支持性和参与性的观众反应。这些发现强调了未来需要进行大规模的定量研究，以检验视觉特征如何塑造社交媒体平台上的视频介导交流。此类调查将为开发以护理为中心的视频共享平台提供信息，从而更好地支持管理疾病体验的个体。", "summary": "本研究旨在探索精神分裂症患者在YouTube上通过视频博客进行情绪披露的方式。通过分析200个视频，并运用媒体研究和自我呈现理论构建的视觉分析框架，研究发现患者通过言语和视觉渠道多样化地表达情绪。特别地，精心设计的视觉元素，如环境和美学选择，能够显著提升观众的支持和参与度。研究强调了未来需要深入探究视觉特征在视频介导沟通中的作用，以期开发更具支持性的心理健康视频平台。", "keywords": "情绪披露, 视频博客, 精神分裂症, YouTube, 视觉沟通", "comments": "这项研究的创新之处在于其将研究重点从传统的文本情绪披露转向了视频介导的情绪表达，特别关注了精神分裂症患者这一特定群体。其提出的视觉分析框架以及对视觉元素重要性的发现，为理解社交媒体上心理健康内容的构建提供了新的视角，并对未来开发支持性平台具有指导意义。"}}
{"id": "2506.10854", "title": "The Impact of Partial Computations on the Red-Blue Pebble Game", "authors": ["Pál András Papp", "Aleksandros Sobczyk", "A. N. Yzelman"], "summary": "We study an extension of the well-known red-blue pebble game (RBP) with\npartial computation steps, inspired by the recent work of Sobczyk. While the\noriginal RBP assumes that we need to have all the inputs of an operation in\nfast memory at the same time, in many concrete computations, the inputs can be\naggregated one by one into the final output value. These partial computation\nsteps can enable pebbling strategies with much smaller I/O cost, and in\nsettings where such a step-by-step aggregation is possible, this extended\nred-blue pebble game offers a much more realistic cost model.\n  We establish the fundamental properties of this partial-computing red-blue\npebble game (PRBP), and compare it to the original RBP. We begin with some\nsimple examples where allowing partial computations can decrease the optimal\nI/O cost. It is also shown that the cost can decrease by up to a linear factor\nthis way, but in general, it is NP-hard to decide whether partial computations\nallow for a smaller cost in a specific DAG. We then discuss how $S$-partitions,\na crucial tool for deriving I/O lower bounds in RBP, can be adapted to the PRBP\nmodel. These new tools are then used to establish lower bounds on the I/O cost\nof some prominent computational tasks. Finally, we also adapt a hardness result\nfrom RBP, showing that the optimum cost is still NP-hard to approximate in PRBP\nto any reasonable factor.", "comment": "Published in the 37th ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA 2025)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10854v1", "AI": {"title_translation": "部分计算对红蓝鹅卵石游戏的影响", "tldr": "本文研究了带有部分计算步骤的红蓝鹅卵石游戏（PRBP），发现它能显著降低I/O成本，但寻找最优解和近似解仍然是NP难问题。", "motivation": "传统的红蓝鹅卵石游戏（RBP）假设操作的所有输入必须同时在快速内存中，这与许多实际计算中输入可以逐个聚合的情况不符。本文的动机是扩展RBP以包含部分计算步骤，从而提供一个更现实的I/O成本模型。", "method": "本文建立了部分计算红蓝鹅卵石游戏（PRBP）的基本属性，并将其与原始RBP进行比较。通过简单示例展示了部分计算的优势，并讨论了如何将RBP中用于推导I/O下限的S-分区工具应用于PRBP模型。这些新工具被用来建立一些重要计算任务的I/O成本下限。", "result": "允许部分计算可以降低最优I/O成本，最高可达线性因子。然而，在特定有向无环图（DAG）中决定部分计算是否能降低成本是NP难问题。此外，PRBP中的最优成本仍然难以在任何合理因子内近似。", "conclusion": "部分计算红蓝鹅卵石游戏（PRBP）提供了一个更现实的I/O成本模型，能够显著降低I/O成本。尽管它在实际应用中具有优势，但寻找最优解和近似解仍然是计算上困难的问题（NP难）。", "translation": "我们研究了红蓝鹅卵石游戏（RBP）的一个扩展，即包含部分计算步骤，这受到了Sobczyk最近工作的启发。虽然原始RBP假设一个操作的所有输入必须同时在快速内存中，但在许多具体的计算中，输入可以逐个聚合成最终的输出值。这些部分计算步骤可以实现I/O成本大大降低的鹅卵石策略，并且在分步聚合可行的情况下，这种扩展的红蓝鹅卵石游戏提供了一个更现实的成本模型。\n我们建立了这种部分计算红蓝鹅卵石游戏（PRBP）的基本属性，并将其与原始RBP进行比较。我们首先给出了一些简单的例子，其中允许部分计算可以降低最优I/O成本。研究还表明，成本可以因此降低高达线性因子，但通常情况下，判断在特定DAG中部分计算是否允许更小的成本是NP难问题。然后，我们讨论了S-分区（RBP中推导I/O下限的关键工具）如何适应PRBP模型。这些新工具随后被用于建立一些著名计算任务的I/O成本下限。最后，我们还改编了RBP中的一个难解性结果，表明PRBP中的最优成本仍然无法在任何合理因子内进行近似。", "summary": "本文引入并研究了部分计算红蓝鹅卵石游戏（PRBP），这是对传统红蓝鹅卵石游戏（RBP）的扩展。与RBP要求所有输入同时在内存中不同，PRBP允许输入逐个聚合，从而更贴近实际计算的I/O特性。研究表明，PRBP能够显著降低I/O成本，最高可达线性因子。然而，确定部分计算是否降低成本以及近似最优成本在PRBP中都被证明是NP难问题。论文还讨论了如何将RBP中的S-分区工具应用于PRBP以推导I/O下限。", "keywords": "红蓝鹅卵石游戏, 部分计算, I/O成本, NP难, 内存模型", "comments": "本文创新性地将部分计算的概念引入红蓝鹅卵石游戏，使其I/O成本模型更符合实际计算场景，解决了传统RBP模型过于理想化的问题。其重要性在于为分析具有增量聚合特性的算法提供了更精确的理论工具。尽管论文揭示了寻找最优解的NP难性，这限制了在所有情况下的实际应用，但它为理解和设计I/O高效算法提供了新的视角和下限分析方法。"}}
{"id": "2506.10824", "title": "A Robust Optimization Framework for Flexible Industrial Energy Scheduling: Application to a Cement Plant with Market Participation", "authors": ["Sebastián Rojas-Innocenti", "Enrique Baeyens", "Alejandro Martín-Crespo", "Sergio Saludes-Rodil", "Fernando Frechoso Escudero"], "summary": "This paper presents a scenario based robust optimization framework for short\nterm energy scheduling in electricity intensive industrial plants, explicitly\naddressing uncertainty in planning decisions. The model is formulated as a\ntwo-stage Mixed Integer Linear Program (MILP) and integrates a hybrid scenario\ngeneration method capable of representing uncertain inputs such as electricity\nprices, renewable generation, and internal demand. A convex objective function\ncombining expected and worst case operational costs allows for tunable risk\naversion, enabling planners to balance economic performance and robustness. The\nresulting schedule ensures feasibility across all scenarios and supports\ncoordinated use of industrial flexibility assets, including battery energy\nstorage and shiftable production. To isolate the effects of market volatility,\nthe framework is applied to a real world cement manufacturing case study\nconsidering only day-ahead electricity price uncertainty, with all other inputs\ntreated deterministically. Results show improved resilience to forecast\ndeviations, reduced cost variability, and more consistent operations. The\nproposed method offers a scalable and risk-aware approach for industrial\nflexibility planning under uncertainty.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10824v1", "AI": {"title_translation": "柔性工业能源调度的鲁棒优化框架：在参与市场的化工厂中的应用", "tldr": "本文提出了一个基于情景的鲁棒优化框架，用于电力密集型工业工厂的短期能源调度，以应对不确定性，并通过一个混合整数线性规划模型，结合期望和最坏情况操作成本，实现在水泥厂案例中对电力价格不确定性的有效管理，显示出更好的弹性、更低的成本变异性和更一致的运营。", "motivation": "本文旨在解决电力密集型工业工厂在短期能源调度中面临的不确定性问题，特别是电力价格、可再生能源发电和内部需求等输入的不确定性，以提高调度决策的鲁棒性和经济性。", "method": "该研究提出了一个基于情景的鲁棒优化框架，将其建模为一个两阶段混合整数线性规划（MILP）。该框架整合了混合情景生成方法来表示不确定输入（如电价、可再生能源发电和内部需求），并采用结合期望和最坏情况操作成本的凸目标函数来实现可调的风险规避。该方法在水泥制造案例中应用，仅考虑日前电价的不确定性。", "result": "结果表明，该框架提高了对预测偏差的弹性，降低了成本变异性，并实现了更一致的运营。它确保了在所有情景下的可行性，并支持工业柔性资产（包括电池储能和可转移生产）的协调使用。", "conclusion": "本文提出的方法为不确定性下的工业柔性规划提供了一种可扩展且风险感知的途径，有效提升了电力密集型工业工厂能源调度的鲁棒性和经济效益。", "translation": "本文提出了一个基于情景的鲁棒优化框架，用于电力密集型工业工厂的短期能源调度，明确解决了规划决策中的不确定性。该模型被表述为一个两阶段混合整数线性规划（MILP），并整合了一种混合情景生成方法，能够表示电力价格、可再生能源发电和内部需求等不确定输入。结合期望和最坏情况操作成本的凸目标函数允许可调的风险规避，使规划者能够平衡经济性能和鲁棒性。由此产生的调度确保了在所有情景下的可行性，并支持工业柔性资产（包括电池储能和可转移生产）的协调使用。为了隔离市场波动的影响，该框架应用于一个真实的 सीमेंट 制造案例研究，仅考虑日前电价不确定性，所有其他输入均被视为确定性。结果显示出对预测偏差的改进弹性、降低的成本变异性和更一致的运营。所提出的方法为不确定性下的工业柔性规划提供了一种可扩展且风险感知的途径。", "summary": "本文提出了一种基于情景的鲁棒优化框架，用于电力密集型工业工厂的短期能源调度，以解决规划中的不确定性。该模型被构建为两阶段混合整数线性规划，并结合了能够模拟不确定输入（如电价和可再生能源）的混合情景生成方法。通过结合期望和最坏情况操作成本的凸目标函数，该框架实现了可调的风险规避。在水泥厂案例研究中的应用表明，该方法显著提高了对预测偏差的弹性，降低了成本波动性，并实现了更稳定的运营，为不确定性下的工业柔性规划提供了一种可扩展且风险感知的方法。", "keywords": "鲁棒优化, 能源调度, 工业柔性, 不确定性, 混合整数线性规划", "comments": "这篇论文的创新点在于提出了一个结合情景生成和鲁棒优化的两阶段MILP框架，有效地处理了工业能源调度中的不确定性。其凸目标函数设计允许灵活调整风险偏好，这在实际应用中非常重要。将模型应用于真实水泥厂案例并验证其在电价不确定性下的性能，增强了其实用性和说服力。该研究为工业能源管理提供了宝贵的工具，有助于提高运营效率和应对市场波动。"}}
{"id": "2506.10212", "title": "Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms", "authors": ["Sajjad Karimi", "Amit J. Shah", "Gari D. Clifford", "Reza Sameni"], "summary": "Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a\ncomprehensive, multimodal perspective on cardiac function by capturing the\nheart's electrical and mechanical activities, respectively. However, the\ndistinct and overlapping information content of these signals, as well as their\npotential for mutual reconstruction and biomarker extraction, remains\nincompletely understood, especially under varying physiological conditions and\nacross individuals.\n  In this study, we systematically investigate the common and exclusive\ncharacteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous\nECG-PCG recordings during rest and exercise. We employ a suite of linear and\nnonlinear machine learning models, including non-causal LSTM networks, to\nreconstruct each modality from the other and analyze the influence of\ncausality, physiological state, and cross-subject variability. Our results\ndemonstrate that nonlinear models, particularly non-causal LSTM, provide\nsuperior reconstruction performance, with reconstructing ECG from PCG proving\nmore tractable than the reverse. Exercise and cross-subject scenarios present\nsignificant challenges, but envelope-based modeling that utilizes instantaneous\namplitude features substantially improves cross-subject generalizability for\ncross-modal learning. Furthermore, we demonstrate that clinically relevant ECG\nbiomarkers, such as fiducial points and QT intervals, can be estimated from PCG\nin cross-subject settings.\n  These findings advance our understanding of the relationship between\nelectromechanical cardiac modalities, in terms of both waveform characteristics\nand the timing of cardiac events, with potential applications in novel\nmultimodal cardiac monitoring technologies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10212v1", "AI": {"title_translation": "心电图和心音图的交叉学习：探索双模态心电机械波形的共同和独有特征", "tldr": "本研究系统地探讨了心电图（ECG）和心音图（PCG）在静息和运动状态下的共同与独有特征，并使用非线性机器学习模型实现了模态间的相互重建和生物标志物估计。", "motivation": "同时记录的心电图（ECG）和心音图（PCG）能全面反映心脏功能，但这些信号中独特和重叠的信息内容、相互重建和生物标志物提取的潜力，特别是在不同生理条件和个体之间，仍未被完全理解。", "method": "本研究利用EPHNOGRAM数据集中的同步ECG-PCG记录，系统地调查了ECG和PCG的共同和独有特征。研究采用了包括非因果LSTM网络在内的线性与非线性机器学习模型，实现模态间的相互重建，并分析了因果关系、生理状态和跨受试者变异性的影响。此外，还采用了基于包络的模型，利用瞬时振幅特征来提高跨模态学习的跨受试者泛化能力。", "result": "非线性模型，特别是非因果LSTM，提供了卓越的重建性能，其中从PCG重建ECG比反向重建更可行。运动和跨受试者场景带来了显著挑战，但利用瞬时振幅特征的基于包络的模型显著提高了跨模态学习的跨受试者泛化能力。此外，临床相关的ECG生物标志物（如特征点和QT间期）可以在跨受试者设置中从PCG中估计出来。", "conclusion": "这些发现增进了我们对心电机械模态之间关系的理解，包括波形特征和心脏事件的时间，并有望应用于新型多模态心脏监测技术。", "translation": "同时心电图（ECG）和心音图（PCG）通过分别捕获心脏的电活动和机械活动，提供了对心脏功能的全面、多模态视角。然而，这些信号独特和重叠的信息内容，以及它们相互重建和生物标志物提取的潜力，在不同生理条件和个体之间仍然未被完全理解。\n在本研究中，我们系统地调查了ECG和PCG在静息和运动期间同步ECG-PCG记录的EPHNOGRAM数据集中的共同和独有特征。我们采用了一套线性和非线性机器学习模型，包括非因果LSTM网络，以从彼此重建每种模态，并分析因果关系、生理状态和跨受试者变异性的影响。我们的结果表明，非线性模型，特别是非因果LSTM，提供了卓越的重建性能，其中从PCG重建ECG比反向重建更易处理。运动和跨受试者场景带来了显著挑战，但利用瞬时振幅特征的基于包络的模型显著提高了跨模态学习的跨受试者泛化能力。此外，我们证明了临床相关的ECG生物标志物，如特征点和QT间期，可以在跨受试者设置中从PCG中估计出来。\n这些发现增进了我们对心电机械模态之间关系的理解，包括波形特征和心脏事件的时间，并有望应用于新型多模态心脏监测技术。", "summary": "本研究利用EPHNOGRAM数据集，系统地探索了心电图（ECG）和心音图（PCG）在静息和运动状态下的共同与独有特征。通过使用包括非因果LSTM在内的线性与非线性机器学习模型，论文成功实现了ECG和PCG之间的相互重建，并分析了生理状态和跨受试者变异性的影响。结果表明，非线性模型在重建性能上表现优越，尤其是在从PCG重建ECG方面。此外，研究还发现基于包络的模型能有效提升跨受试者泛化能力，并成功从PCG中估计出临床相关的ECG生物标志物。这些发现加深了对心电机械模态关系的理解，并为多模态心脏监测技术提供了新思路。", "keywords": "心电图, 心音图, 交叉学习, 生物标志物, 机器学习", "comments": "本研究通过深入探索ECG和PCG之间的交叉学习，为理解心脏电机械活动的复杂关系提供了重要见解。其创新之处在于利用先进的非线性机器学习模型（特别是非因果LSTM）进行模态间重建，并成功解决了跨受试者泛化和生物标志物估计的挑战。能够从PCG估计ECG生物标志物，为无创、便捷的心脏监测提供了新的可能性，具有重要的临床应用潜力。"}}
{"id": "2506.10958", "title": "Bias-Switchable Row-Column Array Imaging using Fast Orthogonal Row-Column Electronic Scanning (FORCES) Compared with Conventional Row-Column Array Imaging", "authors": ["Randy Palamar", "Mohammad Rahim Sobhani", "Darren Dahunsi", "Negar Majidi", "Afshin Kashani Ilkhechi", "Joy Wang", "Jeremy Brown", "Roger Zemp"], "summary": "Row-Column Arrays (RCAs) offer an attractive alternative to fully wired\n2D-arrays for 3D-ultrasound, due to their greatly simplified wiring. However,\nconventional RCAs face challenges related to their long elements. These include\nan inability to image beyond the shadow of the aperture and an inability to\nfocus in both transmit and receive for desired scan planes. To address these\nlimitations, we recently developed bias-switchable RCAs, also known as Top\nOrthogonal to Bottom Electrode (TOBE) arrays. These arrays provide novel\nopportunities to read out from every element of the array and achieve\nhigh-quality images. While TOBE arrays and their associated imaging schemes\nhave shown promise, they have not yet been directly compared experimentally to\nconventional RCA imaging techniques. This study aims to provide such a\ncomparison, demonstrating superior B-scan and volumetric images from two\nelectrostrictive relaxor TOBE arrays, using a method called Fast Orthogonal\nRow-Column Electronic scanning (FORCES), compared to conventional RCA imaging\nschemes, including Tilted Plane Wave (TPW) compounding and Virtual Line Source\n(VLS) imaging. The study quantifies resolution and Generalized Contrast to\nNoise Ratio (gCNR) in phantoms, and also demonstrates volumetric acquisitions\nin phantom and animal models.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10958v1", "AI": {"title_translation": "采用快速正交行-列电子扫描（FORCES）的偏置可切换行-列阵列成像与传统行-列阵列成像的比较", "tldr": "偏置可切换行-列阵列（TOBE）结合FORCES技术在超声成像中优于传统行-列阵列。", "motivation": "传统的行-列阵列（RCAs）在3D超声成像中存在局限性，如无法在孔径阴影之外成像以及无法在发射和接收时对所需扫描平面进行聚焦。", "method": "研究开发了偏置可切换行-列阵列（TOBE阵列），并使用快速正交行-列电子扫描（FORCES）方法，将其与传统的RCA成像方案（包括倾斜平面波（TPW）复合和虚拟线源（VLS）成像）进行了实验比较。研究在模型中量化了分辨率和广义对比度噪声比（gCNR），并演示了在模型和动物模型中的体积采集。", "result": "偏置可切换TOBE阵列结合FORCES技术相比传统RCA成像方案，在B扫描和体积图像上表现出卓越的性能。在模型中量化了更高的分辨率和广义对比度噪声比（gCNR）。", "conclusion": "偏置可切换行-列阵列（TOBE）结合FORCES技术是3D超声成像中传统行-列阵列的有效且优越的替代方案。", "translation": "行-列阵列（RCAs）由于其大大简化的布线，为3D超声的完全有线2D阵列提供了一个有吸引力的替代方案。然而，传统的RCA面临与其长元件相关的挑战。这些挑战包括无法在孔径阴影之外成像以及无法在发射和接收时对所需的扫描平面进行聚焦。为了解决这些局限性，我们最近开发了偏置可切换RCAs，也称为顶部正交到底部电极（TOBE）阵列。这些阵列提供了从阵列的每个元件中读取并获得高质量图像的新机会。尽管TOBE阵列及其相关的成像方案已显示出前景，但它们尚未与传统的RCA成像技术进行直接的实验比较。本研究旨在提供这种比较，展示了使用一种名为快速正交行-列电子扫描（FORCES）的方法，从两个电致伸缩弛豫TOBE阵列获得的卓越B扫描和体积图像，与传统的RCA成像方案（包括倾斜平面波（TPW）复合和虚拟线源（VLS）成像）相比。该研究量化了模型中的分辨率和广义对比度噪声比（gCNR），并演示了在模型和动物模型中的体积采集。", "summary": "本文旨在比较偏置可切换行-列阵列（TOBE）与传统行-列阵列在3D超声成像中的性能。TOBE阵列通过快速正交行-列电子扫描（FORCES）方法克服了传统RCAs在成像范围和聚焦能力上的局限性。实验结果表明，TOBE阵列结合FORCES技术在B扫描和体积图像质量方面显著优于传统的倾斜平面波和虚拟线源成像方案，并在模型中量化了更高的分辨率和广义对比度噪声比。", "keywords": "行-列阵列, 偏置可切换, FORCES, 3D超声, 成像质量", "comments": "这篇论文通过实验比较明确地展示了新型TOBE阵列结合FORCES技术在3D超声成像领域的显著优势。其创新点在于引入了偏置可切换的设计，有效解决了传统行-列阵列的成像盲区和聚焦限制。这种方法简化了布线，同时提高了图像质量，对于推进3D超声技术具有重要意义。"}}
{"id": "2506.10133", "title": "Provable Sim-to-Real Transfer via Offline Domain Randomization", "authors": ["Arnaud Fickinger", "Abderrahim Bendahi", "Stuart Russell"], "summary": "Reinforcement-learning agents often struggle when deployed from simulation to\nthe real-world. A dominant strategy for reducing the sim-to-real gap is domain\nrandomization (DR) which trains the policy across many simulators produced by\nsampling dynamics parameters, but standard DR ignores offline data already\navailable from the real system. We study offline domain randomization (ODR),\nwhich first fits a distribution over simulator parameters to an offline\ndataset. While a growing body of empirical work reports substantial gains with\nalgorithms such as DROPO, the theoretical foundations of ODR remain largely\nunexplored. In this work, we (i) formalize ODR as a maximum-likelihood\nestimation over a parametric simulator family, (ii) prove consistency of this\nestimator under mild regularity and identifiability conditions, showing it\nconverges to the true dynamics as the dataset grows, (iii) derive gap bounds\ndemonstrating ODRs sim-to-real error is up to an O(M) factor tighter than\nuniform DR in the finite-simulator case (and analogous gains in the continuous\nsetting), and (iv) introduce E-DROPO, a new version of DROPO which adds an\nentropy bonus to prevent variance collapse, yielding broader randomization and\nmore robust zero-shot transfer in practice.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10133v1", "AI": {"title_translation": "可证明的离线域随机化实现仿真到现实迁移", "tldr": "本文通过离线域随机化（ODR）解决了强化学习中仿真到现实的迁移问题，首次提供了其理论基础，证明了其优于传统域随机化，并提出了改进算法E-DROPO。", "motivation": "强化学习智能体从仿真到现实部署时常遇到困难，即存在“仿真到现实差距”。标准域随机化（DR）是解决此问题的主流策略，但它忽略了已有的离线真实系统数据。尽管离线域随机化（ODR）在经验上取得了显著成果，但其理论基础尚未得到充分探索。", "method": "本文研究了离线域随机化（ODR），该方法首先将模拟器参数的分布拟合到离线数据集。具体方法包括：(i) 将ODR形式化为参数化模拟器族上的最大似然估计。(ii) 在温和的正则性和可识别性条件下，证明了该估计器的一致性，表明其随数据集增长而收敛到真实动态。(iii) 推导了差距边界，证明在有限模拟器情况下，ODR的仿真到现实误差比均匀DR紧密高达O(M)倍（在连续设置中也有类似增益）。(iv) 引入了E-DROPO，一个DROPO的新版本，增加了熵奖励以防止方差崩溃，从而在实践中实现更广泛的随机化和更鲁棒的零样本迁移。", "result": "(i) ODR估计器在温和条件下具有一致性，并随数据集增长收敛到真实动态。(ii) ODR的仿真到现实误差比均匀DR在有限模拟器情况下紧密高达O(M)倍，在连续设置中也有类似增益。(iii) 引入的E-DROPO算法通过增加熵奖励，实现了更广泛的随机化和更鲁棒的零样本迁移。", "conclusion": "本文首次为离线域随机化（ODR）提供了坚实的理论基础，证明了其在缩小仿真到现实差距方面的优越性，并通过引入改进的E-DROPO算法，提升了实际应用中的迁移鲁棒性。", "translation": "强化学习智能体从仿真部署到现实世界时经常遇到困难。减少仿真到现实差距的主导策略是域随机化（DR），它通过采样动力学参数来训练策略，从而生成许多模拟器，但标准DR忽略了已有的真实系统离线数据。我们研究了离线域随机化（ODR），它首先将模拟器参数的分布拟合到离线数据集。尽管越来越多的实证工作报告称DROPO等算法取得了显著收益，但ODR的理论基础在很大程度上仍未被探索。在这项工作中，我们（i）将ODR形式化为参数化模拟器族上的最大似然估计，（ii）在温和的正则性和可识别性条件下证明了该估计器的一致性，表明它随数据集的增长而收敛到真实动态，（iii）推导了差距边界，证明ODR的仿真到现实误差在有限模拟器情况下比均匀DR紧密高达O(M)倍（在连续设置中也有类似增益），并且（iv）引入了E-DROPO，一个DROPO的新版本，增加了熵奖励以防止方差崩溃，从而在实践中实现更广泛的随机化和更鲁棒的零样本迁移。", "summary": "本文针对强化学习中仿真到现实迁移的挑战，提出了离线域随机化（ODR）方法，该方法利用离线真实数据拟合模拟器参数分布。与现有经验工作不同，本文首次为ODR提供了严格的理论基础，包括将其形式化为最大似然估计并证明其一致性。研究还推导了ODR相比传统均匀域随机化在缩小仿真到现实差距上的显著优势（误差紧密性提高O(M)倍）。此外，本文引入了改进的E-DROPO算法，通过熵奖励增强了随机化广度，从而实现了更鲁棒的零样本迁移。", "keywords": "离线域随机化, 仿真到现实迁移, 强化学习, 理论分析, E-DROPO", "comments": "这篇论文的创新点在于首次为离线域随机化（ODR）提供了坚实的理论证明，弥补了此前该领域经验成果缺乏理论支撑的空白。其证明了ODR估计器的一致性以及其在减小仿真到现实差距方面的显著优势，这对于理解和改进仿真到现实迁移方法具有重要意义。引入的E-DROPO算法也显示了在实践中提升鲁棒性的潜力。"}}
{"id": "2506.10209", "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games", "authors": ["Prakamya Mishra", "Jiang Liu", "Jialian Wu", "Xiaodong Yu", "Zicheng Liu", "Emad Barsoum"], "summary": "Large reasoning models (LRMs) have demonstrated impressive reasoning\ncapabilities across a broad range of tasks including Olympiad-level\nmathematical problems, indicating evidence of their complex reasoning\nabilities. While many reasoning benchmarks focus on the STEM domain, the\nability of LRMs to reason correctly in broader task domains remains\nunderexplored. In this work, we introduce \\textbf{TTT-Bench}, a new benchmark\nthat is designed to evaluate basic strategic, spatial, and logical reasoning\nabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games\nthat humans can effortlessly solve from a young age. We propose a simple yet\nscalable programmatic approach for generating verifiable two-player game\nproblems for TTT-Bench. Although these games are trivial for humans, they\nrequire reasoning about the intentions of the opponent, as well as the game\nboard's spatial configurations, to ensure a win. We evaluate a diverse set of\nstate-of-the-art LRMs, and \\textbf{discover that the models that excel at hard\nmath problems frequently fail at these simple reasoning games}. Further testing\nreveals that our evaluated reasoning models score on average $\\downarrow$ 41\\%\n\\& $\\downarrow$ 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024\nrespectively, with larger models achieving higher performance using shorter\nreasoning traces, where most of the models struggle on long-term strategic\nreasoning situations on simple and new TTT-Bench tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10209v1", "AI": {"title_translation": "TTT-Bench：一个用于评估简单新颖井字棋式游戏推理能力的基准", "tldr": "TTT-Bench是一个新基准，用于评估大型推理模型（LRMs）在简单井字棋式游戏中的基本战略、空间和逻辑推理能力。研究发现，即使是擅长数学难题的LRMs，在这些对人类而言简单的游戏中也表现不佳，表明它们在更广泛任务领域的推理能力仍有待探索。", "motivation": "虽然大型推理模型（LRMs）在STEM领域表现出强大的推理能力，但它们在更广泛任务领域（如基本战略、空间和逻辑推理）中的表现仍未得到充分探索。现有基准多集中于STEM领域。", "method": "本文引入了TTT-Bench，这是一个包含四种双人井字棋式游戏的基准，旨在评估LRMs的基本战略、空间和逻辑推理能力。作者提出了一种简单且可扩展的程序化方法来生成可验证的双人游戏问题。", "result": "研究发现，擅长解决数学难题的模型在这些简单的推理游戏中常常失败。评估的推理模型在TTT-Bench上的得分平均比MATH 500低41%，比AIME 2024低5%。较大的模型使用较短的推理轨迹能获得更高的性能，但大多数模型在简单和新的TTT-Bench任务上，在长期战略推理情境中表现不佳。", "conclusion": "即使是先进的大型推理模型，在对人类而言简单的、需要基本战略、空间和逻辑推理的新颖井字棋式游戏中也存在显著的推理能力缺陷，这表明它们在非STEM领域的泛化推理能力仍有待提高。", "translation": "大型推理模型（LRMs）在包括奥林匹克级别的数学问题在内的广泛任务中展示了令人印象深刻的推理能力，这表明它们具有复杂的推理能力。虽然许多推理基准侧重于STEM领域，但LRMs在更广泛任务领域中正确推理的能力仍未得到充分探索。在这项工作中，我们引入了TTT-Bench，这是一个新基准，旨在通过一套四种双人井字棋式游戏来评估LRMs的基本战略、空间和逻辑推理能力，这些游戏是人类从小就能轻松解决的。我们提出了一种简单但可扩展的程序化方法来为TTT-Bench生成可验证的双人游戏问题。尽管这些游戏对人类来说微不足道，但它们需要推理对手的意图以及棋盘的空间配置，以确保获胜。我们评估了各种最先进的LRMs，并发现那些擅长数学难题的模型在这些简单的推理游戏中经常失败。进一步测试表明，我们评估的推理模型在TTT-Bench上的平均得分分别比MATH 500和AIME 2024低41%和5%，其中较大的模型使用较短的推理轨迹实现了更高的性能，而大多数模型在简单和新的TTT-Bench任务中，在长期战略推理情境下表现挣扎。", "summary": "TTT-Bench是一个新颖的基准，用于评估大型推理模型（LRMs）在简单井字棋式游戏中的基本战略、空间和逻辑推理能力。该基准包含四种对人类而言简单的双人游戏，并通过程序化方法生成问题。研究发现，即使是擅长解决复杂数学问题的LRMs，在这些需要推理对手意图和空间配置的简单游戏中也表现不佳，得分远低于在传统数学基准上的表现。这表明LRMs在非STEM领域的泛化推理能力存在局限性，尤其是在长期战略推理方面。", "keywords": "大型推理模型, 基准测试, 井字棋, 战略推理, 逻辑推理", "comments": "TTT-Bench的创新之处在于它提供了一个独特且简单的基准，专门用于测试大型推理模型在非传统STEM领域的基本战略、空间和逻辑推理能力。这项工作的重要性在于揭示了当前LRMs的局限性，即使它们在复杂数学问题上表现出色，也可能在对人类而言微不足道的简单、新颖情境中失败。这为未来LRMs的研究指明了方向，即需要提升模型在更广泛、更具泛化性的推理任务中的表现，而不仅仅是依赖于大量数据训练的特定领域能力。"}}
{"id": "2506.10182", "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates", "authors": ["Fiona Ryan", "Josef Sivic", "Fabian Caba Heilbron", "Judy Hoffman", "James M. Rehg", "Bryan Russell"], "summary": "Personalized vision-language retrieval seeks to recognize new concepts (e.g.\n\"my dog Fido\") from only a few examples. This task is challenging because it\nrequires not only learning a new concept from a few images, but also\nintegrating the personal and general knowledge together to recognize the\nconcept in different contexts. In this paper, we show how to effectively adapt\nthe internal representation of a vision-language dual encoder model for\npersonalized vision-language retrieval. We find that regularized low-rank\nadaption of a small set of parameters in the language encoder's final layer\nserves as a highly effective alternative to textual inversion for recognizing\nthe personal concept while preserving general knowledge. Additionally, we\nexplore strategies for combining parameters of multiple learned personal\nconcepts, finding that parameter addition is effective. To evaluate how well\ngeneral knowledge is preserved in a finetuned representation, we introduce a\nmetric that measures image retrieval accuracy based on captions generated by a\nvision language model (VLM). Our approach achieves state-of-the-art accuracy on\ntwo benchmarks for personalized image retrieval with natural language queries -\nDeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal\nretrievals.", "comment": "CVPR 2025 Highlight. Code: http://github.com/adobe-research/polar-vl", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10182v1", "AI": {"title_translation": "采用正则化低秩参数更新改进个性化搜索", "tldr": "通过对视觉-语言双编码器模型进行正则化低秩参数更新，有效提升个性化视觉-语言检索性能，同时保持通用知识。", "motivation": "个性化视觉-语言检索任务（从少量示例中识别新概念）面临挑战，因为它不仅需要从少量图像中学习新概念，还需要将个人和通用知识结合起来以在不同上下文中识别概念。", "method": "1. 有效调整视觉-语言双编码器模型的内部表示。\n2. 具体方法是对语言编码器最后一层的一小部分参数进行正则化低秩自适应，作为文本反演的有效替代方案，同时保留通用知识。\n3. 探索结合多个已学习的个人概念参数的策略，发现参数相加是有效的。\n4. 引入一个衡量通用知识保留程度的指标，该指标基于视觉语言模型（VLM）生成的标题的图像检索准确性。", "result": "1. 在两个个性化图像检索基准（DeepFashion2和ConCon-Chi）上实现了最先进的准确性。\n2. 在个人检索方面，性能比现有技术提高了4%-22%。", "conclusion": "通过正则化低秩参数更新，可以有效改进个性化视觉-语言检索，同时成功地将个人和通用知识结合起来，并在相关基准上取得了显著的SOTA性能提升。", "translation": "个性化视觉-语言检索旨在仅从少量示例中识别新概念（例如“我的狗Fido”）。这项任务具有挑战性，因为它不仅需要从少量图像中学习新概念，还需要将个人和通用知识结合起来以在不同上下文中识别该概念。在本文中，我们展示了如何有效地调整视觉-语言双编码器模型的内部表示，用于个性化视觉-语言检索。我们发现，对语言编码器最后一层的一小部分参数进行正则化低秩自适应，可以作为文本反演的一种高效替代方案，用于识别个人概念，同时保留通用知识。此外，我们探索了组合多个已学习的个人概念参数的策略，发现参数相加是有效的。为了评估微调表示中通用知识的保留情况，我们引入了一个衡量基于视觉语言模型（VLM）生成的标题的图像检索准确性的指标。我们的方法在两个使用自然语言查询的个性化图像检索基准（DeepFashion2和ConCon-Chi）上取得了最先进的准确性，在个人检索方面比现有技术提高了4%-22%。", "summary": "本文提出了一种通过正则化低秩参数更新来改进个性化视觉-语言检索的方法。研究发现，对视觉-语言双编码器模型中语言编码器最后一层的少量参数进行正则化低秩自适应，能有效识别个人概念并保留通用知识，优于传统的文本反演。此外，文章还探讨了结合多个个人概念参数的策略，并引入了新的通用知识保留评估指标。该方法在DeepFashion2和ConCon-Chi基准上取得了最先进的性能，在个人检索方面有显著提升。", "keywords": "个性化检索, 视觉-语言模型, 低秩适应, 参数更新, 概念学习", "comments": "这篇论文通过引入正则化低秩参数更新，为个性化视觉-语言检索提供了一种新颖且高效的参数适应方法，解决了在学习新概念的同时保留通用知识的关键挑战。其创新之处在于利用低秩适应替代了计算成本较高的文本反演，并在参数组合方面进行了探索。引入新的通用知识评估指标也增强了研究的严谨性。"}}
{"id": "2506.10661", "title": "Alternating steepest descent methods for tensor completion with applications to spectromicroscopy", "authors": ["Oliver Townsend", "Sergey Dolgov", "Silvia Gazzola", "Misha Kilmer"], "summary": "In this paper we develop two new Tensor Alternating Steepest Descent\nalgorithms for tensor completion in the low-rank $\\star_{M}$-product format,\nwhereby we aim to reconstruct an entire low-rank tensor from a small number of\nmeasurements thereof. Both algorithms are rooted in the Alternating Steepest\nDescent (ASD) method for matrix completion, first proposed in [J. Tanner and K.\nWei, Appl. Comput. Harmon. Anal., 40 (2016), pp. 417-429]. In deriving the new\nmethods we target the X-ray spectromicroscopy undersampling problem, whereby\ndata are collected by scanning a specimen on a rectangular viewpoint with X-ray\nbeams of different energies. The recorded absorptions coefficients of the mixed\nspecimen materials are naturally stored in a third-order tensor, with spatial\nhorizontal and vertical axes, and an energy axis. To speed the X-ray\nspectromicroscopy measurement process up, only a fraction of tubes from (a\nreshaped version of) this tensor are fully scanned, leading to a tensor\ncompletion problem. In this framework we can apply any transform (such as the\nFourier transform) to the tensor tube by tube, providing a natural way to work\nwith the $\\star_{M}$-tensor algebra, and propose: (1) a tensor completion\nalgorithm that is essentially ASD reformulated in the $\\star_{M}$-induced\nmetric space and (2) a tensor completion algorithm that solves a set of\n(readily parallelizable) independent matrix completion problems for the frontal\nslices of the transformed tensor. The two new methods are tested on real X-ray\nspectromicroscopy data, demonstrating that they achieve the same reconstruction\nerror with fewer samples from the tensor compared to the matrix completion\nalgorithms applied to a flattened tensor.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10661v1", "AI": {"title_translation": "用于张量补全的交替最速下降方法及其在光谱显微镜中的应用", "tldr": "本文提出了两种基于张量交替最速下降算法，用于低秩张量补全，并应用于X射线光谱显微镜数据，证明了其在减少样本量的情况下能达到与矩阵补全算法相同的重建误差。", "motivation": "旨在从少量测量中重建整个低秩张量。具体针对X射线光谱显微镜欠采样问题，该问题导致数据仅部分扫描，形成张量补全问题。", "method": "开发了两种新的张量交替最速下降算法（Tensor Alternating Steepest Descent algorithms）用于低秩$\\\\star_{M}$-乘积格式的张量补全。两种算法都源自矩阵补全的交替最速下降（ASD）方法。方法1：ASD在$\\\\star_{M}$-诱导度量空间中的重构。方法2：求解一组（易于并行化）独立的矩阵补全问题，针对变换后张量的前切片。可以对张量按管（tube）应用任何变换（如傅里叶变换）。", "result": "这两种新方法在真实的X射线光谱显微镜数据上进行了测试，结果表明，与应用于扁平张量的矩阵补全算法相比，它们在从张量中获取更少样本的情况下，实现了相同的重建误差。", "conclusion": "本文提出的两种张量交替最速下降算法能有效解决低秩张量补全问题，特别是在光谱显微镜等应用中，可以在减少数据采集量的同时保持重建精度。", "translation": "本文提出了两种用于低秩$\\\\star_{M}$-乘积格式张量补全的新型张量交替最速下降算法，旨在从少量测量中重建整个低秩张量。这两种算法都源于[J. Tanner和K. Wei, Appl. Comput. Harmon. Anal., 40 (2016), pp. 417-429]首次提出的矩阵补全交替最速下降（ASD）方法。在推导新方法时，我们针对X射线光谱显微镜欠采样问题，即通过用不同能量的X射线束扫描矩形视场的样本来收集数据。混合样本材料的记录吸收系数自然地存储在三阶张量中，具有空间水平和垂直轴以及能量轴。为了加快X射线光谱显微镜测量过程，该张量（或其重塑版本）只有一小部分管被完全扫描，从而导致一个张量补全问题。在此框架中，我们可以逐管地对张量应用任何变换（例如傅里叶变换），提供了一种与$\\\\star_{M}$-张量代数自然协作的方式，并提出了：(1) 一种本质上是ASD在$\\\\star_{M}$-诱导度量空间中重新公式化的张量补全算法；(2) 一种通过解决变换张量前切片的一组（易于并行化）独立的矩阵补全问题来实现张量补全的算法。这两种新方法在真实的X射线光谱显微镜数据上进行了测试，证明与应用于扁平张量的矩阵补全算法相比，它们在从张量中获取更少样本的情况下，实现了相同的重建误差。", "summary": "本文提出了两种新颖的张量交替最速下降算法，用于解决低秩$\\\\star_{M}$-乘积格式下的张量补全问题。这些算法基于矩阵补全的ASD方法，并特别应用于X射线光谱显微镜中的数据欠采样问题。其中一种算法是在$\\\\star_{M}$-诱导度量空间中重构ASD，另一种则通过并行解决变换后张量前切片的独立矩阵补全问题。实验结果表明，与传统矩阵补全方法相比，这两种新方法在样本量更少的情况下能达到相同的重建精度。", "keywords": "张量补全, 交替最速下降, $\\\\star_{M}$-乘积, 光谱显微镜, 欠采样", "comments": "这项研究的创新之处在于将传统的矩阵补全ASD方法扩展到张量领域，并引入了$\\\\star_{M}$-乘积格式来处理张量数据，特别是针对X射线光谱显微镜的实际应用。通过提出两种不同的实现方式，提供了解决张量欠采样问题的有效途径，并且证明了其在减少数据采集量方面的优越性，这对于实际应用具有重要意义。该工作强调了张量方法在处理高维稀疏数据方面的潜力。"}}
{"id": "2506.10501", "title": "BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis", "authors": ["Surya Jasper", "Minh Luu", "Evan Pan", "Aakash Tyagi", "Michael Quinn", "Jiang Hu", "David Kebo Houngninou"], "summary": "Hardware complexity continues to strain verification resources, motivating\nthe adoption of machine learning (ML) methods to improve debug efficiency.\nHowever, ML-assisted debugging critically depends on diverse and scalable bug\ndatasets, which existing manual or automated bug insertion methods fail to\nreliably produce. We introduce BugGen, a first of its kind, fully autonomous,\nmulti-agent pipeline leveraging Large Language Models (LLMs) to systematically\ngenerate, insert, and validate realistic functional bugs in RTL. BugGen\npartitions modules, selects mutation targets via a closed-loop agentic\narchitecture, and employs iterative refinement and rollback mechanisms to\nensure syntactic correctness and functional detectability. Evaluated across\nfive OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional\naccuracy and achieved a throughput of 17.7 validated bugs per hour-over five\ntimes faster than typical manual expert insertion. Additionally, BugGen\nidentified 104 previously undetected bugs in OpenTitan regressions,\nhighlighting its utility in exposing verification coverage gaps. Compared\nagainst Certitude, BugGen demonstrated over twice the syntactic accuracy,\ndeeper exposure of testbench blind spots, and more functionally meaningful and\ncomplex bug scenarios. Furthermore, when these BugGen-generated datasets were\nemployed to train ML-based failure triage models, we achieved high\nclassification accuracy (88.1%-93.2%) across different IP blocks, confirming\nthe practical utility and realism of generated bugs. BugGen thus provides a\nscalable solution for generating high-quality bug datasets, significantly\nenhancing verification efficiency and ML-assisted debugging.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10501v1", "AI": {"title_translation": "BugGen：一种用于真实RTL错误合成的自校正多智能体LLM管道", "tldr": "BugGen是一个自校正多智能体LLM管道，用于自动化生成真实RTL错误，显著提高硬件验证效率和ML辅助调试。", "motivation": "硬件复杂性持续给验证资源带来压力，需要机器学习方法提高调试效率。然而，ML辅助调试关键依赖于多样化和可扩展的错误数据集，而现有的手动或自动化错误插入方法都无法可靠地生成此类数据集。", "method": "BugGen是一个全自主、多智能体管道，利用大型语言模型（LLM）系统性地生成、插入和验证RTL中的真实功能错误。它通过闭环智能体架构分区模块，选择突变目标，并采用迭代细化和回滚机制确保语法正确性和功能可检测性。", "result": "在五个OpenTitan IP块上，BugGen生成了500个独特错误，功能准确率达94%，吞吐量为每小时17.7个已验证错误（比典型手动插入快五倍以上）。它还识别了OpenTitan回归测试中104个先前未检测到的错误。与Certitude相比，BugGen的语法准确性提高一倍以上，更深入地暴露了测试台盲点，并生成了更具功能意义和复杂的错误场景。此外，生成的错误数据集用于训练ML故障分类模型，实现了88.1%-93.2%的高分类准确率。", "conclusion": "BugGen提供了一个可扩展的解决方案，用于生成高质量错误数据集，显著提高验证效率和ML辅助调试。", "translation": "硬件复杂性持续给验证资源带来压力，这促使人们采用机器学习（ML）方法来提高调试效率。然而，ML辅助调试关键依赖于多样化和可扩展的错误数据集，而现有的手动或自动化错误插入方法都无法可靠地生成此类数据集。我们引入了BugGen，这是首个利用大型语言模型（LLM）的完全自主、多智能体管道，用于系统地在RTL中生成、插入和验证真实的功能错误。BugGen通过闭环智能体架构对模块进行分区，选择变异目标，并采用迭代细化和回滚机制，以确保语法正确性和功能可检测性。在对五个OpenTitan IP块的评估中，BugGen生成了500个独特的错误，功能准确率达到94%，并实现了每小时17.7个已验证错误的吞吐量——比典型的人工专家插入快五倍以上。此外，BugGen在OpenTitan回归测试中识别出104个先前未检测到的错误，突显了其在暴露验证覆盖率空白方面的作用。与Certitude相比，BugGen的语法准确性提高了一倍以上，更深入地暴露了测试台盲点，并生成了更具功能意义和更复杂的错误场景。此外，当这些BugGen生成的数据集用于训练基于ML的故障分类模型时，我们在不同的IP块上实现了高分类准确率（88.1%-93.2%），证实了所生成错误的实用性和真实性。因此，BugGen提供了一个可扩展的解决方案，用于生成高质量的错误数据集，显著提高了验证效率和ML辅助调试。", "summary": "本文介绍了BugGen，一个首创的、基于LLM的自校正多智能体管道，旨在自动化生成和验证真实的RTL功能错误。为解决现有方法无法可靠生成多样化错误数据集的问题，BugGen通过迭代细化和回滚机制确保语法正确性和功能可检测性。实验表明，BugGen能高效生成高质量错误，显著提高硬件验证效率，并能有效训练ML辅助调试模型，其性能优于现有工具。", "keywords": "RTL错误合成, LLM, 多智能体, 硬件验证, 错误生成", "comments": "BugGen的创新之处在于其采用多智能体LLM管道进行自校正的错误合成，这解决了硬件验证中高质量、可扩展错误数据集的痛点。其自动化、高效率和高准确率的特点，以及在发现新错误和辅助ML调试方面的实用性，都显示了其在未来硬件验证领域的巨大潜力。"}}
{"id": "2506.10462", "title": "Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions", "authors": ["Ana Müller", "Sabina Jeschke", "Anja Richert"], "summary": "This paper investigates the impact of a group-adaptive conversation design in\ntwo socially interactive agents (SIAs) through two real-world studies. Both\nSIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped\nwith a conversational artificial intelligence (CAI) backend combining hybrid\nretrieval and generative models. The studies were carried out in an in-the-wild\nsetting with a total of $N = 188$ participants who interacted with the SIAs -\nin dyads, triads or larger groups - at a German museum. Although the results\ndid not reveal a significant effect of the group-sensitive conversation design\non perceived satisfaction, the findings provide valuable insights into the\nchallenges of adapting CAI for multi-party interactions and across different\nembodiments (robot vs.\\ virtual agent), highlighting the need for multimodal\nstrategies beyond linguistic pluralization. These insights contribute to the\nfields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and\nbroader Human-Machine Interaction (HMI), providing insights for future research\non effective dialogue adaptation in group settings.", "comment": "Accepted as a regular paper at the 2025 IEEE International Conference\n  on Robot and Human Interactive Communication (RO-MAN). \\c{opyright} IEEE.\n  This is the preprint version. The final version will appear in the IEEE\n  proceedings", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10462v1", "AI": {"title_translation": "我们是否在从例外中概括？一项关于人机交互中群组敏感对话设计的实地研究", "tldr": "本研究在真实环境中调查了群组自适应对话设计对社交智能体（机器人和虚拟代理）的影响。尽管对用户满意度无显著影响，但揭示了多方交互和跨实体适应对话式人工智能（CAI）的挑战，强调了超越语言的多模态策略的必要性。", "motivation": "本研究旨在调查群组自适应对话设计在两个社交互动智能体（SIAs）中的影响。", "method": "通过两项实地研究进行，共有188名参与者在德国一家博物馆的真实环境中与社交机器人Furhat和虚拟代理MetaHuman（均配备结合混合检索和生成模型的对话式人工智能后端）以两人、三人或更大规模的群组形式进行互动。", "result": "尽管群组敏感对话设计对感知满意度没有显著影响，但研究结果为在多方交互和不同实体（机器人与虚拟代理）中调整对话式人工智能的挑战提供了宝贵见解。", "conclusion": "研究强调了在群组设置中，超越语言复数化的多模态策略对于有效对话适应的需求，并为人机交互（HAI）、人机人交互（HRI）和更广泛的人机交互（HMI）领域未来关于有效对话适应的研究提供了见解。", "translation": "这篇论文通过两项真实世界的研究，调查了群组自适应对话设计在两个社交互动智能体（SIAs）中的影响。这两个社交互动智能体——社交机器人Furhat和虚拟代理MetaHuman——都配备了结合混合检索和生成模型的对话式人工智能（CAI）后端。这些研究是在德国一家博物馆的真实环境中进行的，共有188名参与者以两人、三人或更大规模的群组形式与SIAs进行了互动。尽管结果并未揭示群组敏感对话设计对感知满意度有显著影响，但这些发现为多方交互和跨不同实体（机器人与虚拟代理）调整CAI所面临的挑战提供了宝贵见解，强调了超越语言复数化的多模态策略的需求。这些见解有助于人机交互（HAI）、人机人交互（HRI）和更广泛的人机交互（HMI）领域，为未来在群组环境中有效对话适应的研究提供了启示。", "summary": "本研究在真实环境中，通过与社交机器人Furhat和虚拟代理MetaHuman（均搭载混合对话式人工智能）的互动，调查了群组自适应对话设计对用户满意度的影响。尽管未发现显著影响，但研究揭示了在多方和跨实体交互中调整对话式人工智能的复杂性，强调了多模态而非单一语言策略的重要性，为未来人机交互研究提供了方向。", "keywords": "人机交互, 群组敏感对话, 社交智能体, 对话式人工智能, 实地研究", "comments": "这项研究的创新之处在于其“in-the-wild”的实地研究设置，这比实验室研究更能反映真实世界的使用情况。尽管主要假设（群组敏感设计对满意度的影响）未被证实，但其负面结果同样重要，揭示了在复杂多方和多实体情境下设计对话式人工智能的挑战，并强调了多模态交互的重要性，这对于人机交互领域具有重要指导意义。"}}
{"id": "2506.10933", "title": "Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs", "authors": ["Ziwen Wang", "Yue Zhang", "Zhiqiang Zhang", "Sheng Quan Xie", "Alexander Lanzon", "William P. Heath", "Zhenhong Li"], "summary": "Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces\n(BCIs) can achieve high recognition accuracy with sufficient training data.\nTransfer learning presents a promising solution to alleviate data requirements\nfor the target subject by leveraging data from source subjects; however,\neffectively addressing individual variability among both target and source\nsubjects remains a challenge. This paper proposes a novel transfer learning\nframework, termed instance-based task-related component analysis (iTRCA), which\nleverages knowledge from source subjects while considering their individual\ncontributions. iTRCA extracts two types of features: (1) the subject-general\nfeature, capturing shared information between source and target subjects in a\ncommon latent space, and (2) the subject-specific feature, preserving the\nunique characteristics of the target subject. To mitigate negative transfer, we\nfurther design an enhanced framework, subject selection-based iTRCA (SS-iTRCA),\nwhich integrates a similarity-based subject selection strategy to identify\nappropriate source subjects for transfer based on their task-related components\n(TRCs). Comparative evaluations on the Benchmark, BETA, and a self-collected\ndataset demonstrate the effectiveness of the proposed iTRCA and SS-iTRCA\nframeworks. This study provides a potential solution for developing\nhigh-performance SSVEP-based BCIs with reduced target subject data.", "comment": "IEEE Journal of Biomedical and Health Informatics", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10933v1", "AI": {"title_translation": "基于实例的迁移学习，结合相似度感知的受试者选择用于跨受试者SSVEP-BCI", "tldr": "本研究提出了一种名为iTRCA的新型迁移学习框架及其增强版SS-iTRCA，通过提取通用和特定特征并结合相似度选择，有效减少了SSVEP-BCI对目标受试者数据的需求并提高了性能。", "motivation": "稳态视觉诱发电位（SSVEP）脑机接口（BCI）需要大量训练数据才能达到高识别精度。迁移学习有望通过利用源受试者数据来缓解目标受试者的数据需求，但如何有效解决目标和源受试者之间的个体差异仍然是一个挑战。", "method": "本文提出了一种名为实例基任务相关成分分析（iTRCA）的新型迁移学习框架，它从源受试者那里获取知识，同时考虑他们的个体贡献。iTRCA提取两种特征：主体通用特征（捕获源和目标受试者在共同潜在空间中的共享信息）和主体特定特征（保留目标受试者的独特特征）。为了减轻负迁移，进一步设计了增强框架——基于受试者选择的iTRCA（SS-iTRCA），它整合了基于相似度的受试者选择策略，根据任务相关成分（TRCs）识别合适的源受试者进行迁移。", "result": "在Benchmark、BETA和自收集数据集上的比较评估表明，所提出的iTRCA和SS-iTRCA框架是有效的。", "conclusion": "本研究为开发高性能、低目标受试者数据需求的SSVEP-BCI提供了一个潜在的解决方案。", "translation": "稳态视觉诱发电位（SSVEP）脑机接口（BCI）在拥有足够训练数据的情况下可以实现高识别精度。迁移学习提供了一个有前景的解决方案，通过利用源受试者的数据来缓解目标受试者的数据需求；然而，有效解决目标和源受试者之间的个体差异仍然是一个挑战。本文提出了一种名为实例基任务相关成分分析（iTRCA）的新型迁移学习框架，它从源受试者那里获取知识，同时考虑他们的个体贡献。iTRCA提取两种特征：(1)主体通用特征，捕获源和目标受试者在共同潜在空间中的共享信息，以及(2)主体特定特征，保留目标受试者的独特特征。为了减轻负迁移，我们进一步设计了一个增强框架——基于受试者选择的iTRCA（SS-iTRCA），它整合了基于相似度的受试者选择策略，根据任务相关成分（TRCs）识别合适的源受试者进行迁移。在Benchmark、BETA和自收集数据集上的比较评估表明，所提出的iTRCA和SS-iTRCA框架是有效的。本研究为开发高性能、低目标受试者数据需求的SSVEP-BCI提供了一个潜在的解决方案。", "summary": "本论文提出了一种新的迁移学习框架iTRCA及其增强版SS-iTRCA，旨在解决SSVEP-BCI中个体差异和数据需求过大的问题。iTRCA通过提取主体通用和主体特定特征来利用源受试者知识。SS-iTRCA进一步引入了基于相似度的受试者选择策略，以避免负迁移。实验结果表明，这些框架能够有效提高SSVEP-BCI的性能并减少对目标受试者数据的依赖。", "keywords": "迁移学习, SSVEP-BCI, 受试者选择, 个体差异, 任务相关成分分析", "comments": "该论文的创新点在于提出了iTRCA和SS-iTRCA两种迁移学习框架，特别是在SSVEP-BCI领域。通过区分主体通用和主体特定特征，并引入相似度感知的受试者选择机制，有效地解决了跨受试者迁移学习中个体差异带来的负迁移问题，并降低了对目标受试者数据量的要求，具有重要的实际应用价值。"}}
{"id": "2506.10889", "title": "Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning", "authors": ["Waylon Luo", "Jiapeng Zhao", "Tong Zhan", "Qiang Guan"], "summary": "Present-day quantum systems face critical bottlenecks, including limited\nqubit counts, brief coherence intervals, and high susceptibility to errors-all\nof which obstruct the execution of large and complex circuits. The advancement\nof quantum algorithms has outpaced the capabilities of existing quantum\nhardware, making it difficult to scale computations effectively. Additionally,\ninconsistencies in hardware performance and pervasive quantum noise undermine\nsystem stability and computational accuracy. To optimize quantum workloads\nunder these constraints, strategic approaches to task scheduling and resource\ncoordination are essential. These methods must aim to accelerate processing,\nretain operational fidelity, and reduce the communication burden inherent to\ndistributed setups. One of the persistent challenges in this domain is how to\nefficiently divide and execute large circuits across multiple quantum\nprocessors (QPUs), especially in error-prone environments. In response, we\nintroduce a simulation-based tool that supports distributed scheduling and\nconcurrent execution of quantum jobs on networked QPUs connected via real-time\nclassical channels. The tool models circuit decomposition for workloads that\nsurpass individual QPU limits, allowing for parallel execution through\ninter-processor communication. Using this simulation environment, we compare\nfour distinct scheduling techniques-among them, a model informed by\nreinforcement learning. These strategies are evaluated across multiple metrics,\nincluding runtime efficiency, fidelity preservation, and communication costs.\nOur analysis underscores the trade-offs inherent in each approach and\nhighlights how parallelized, noise-aware scheduling can meaningfully improve\ncomputational throughput in distributed quantum infrastructures.", "comment": "10 pages, 6 figures, ICPP 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10889v1", "AI": {"title_translation": "量子云中基于强化学习的自适应作业调度", "tldr": "本文提出一个模拟工具，用于在有噪声的分布式量子系统中进行量子作业调度，并比较了包括强化学习在内的多种调度技术，发现并行化、噪声感知的调度能提高吞吐量。", "motivation": "现有量子系统面临量子比特限制、相干时间短和高错误率等瓶颈，阻碍了大型复杂电路的执行。量子算法发展超越硬件能力，导致计算难以扩展，硬件性能不一致和量子噪声也损害系统稳定性。因此，需要在这些约束下优化量子工作负载的调度和资源协调，尤其是在错误环境中跨多个QPU高效划分和执行大型电路。", "method": "本文引入一个基于仿真的工具，支持通过实时经典通道连接的网络化QPU上的分布式调度和并发执行量子作业。该工具模拟电路分解，以处理超出单个QPU限制的工作负载，并通过处理器间通信实现并行执行。在模拟环境中，比较了四种不同的调度技术，其中一种是基于强化学习的模型，并根据运行时效率、保真度保留和通信成本等指标进行评估。", "result": "分析揭示了每种方法固有的权衡，并强调了并行化、噪声感知的调度如何能显著提高分布式量子基础设施中的计算吞吐量。", "conclusion": "通过并行化和噪声感知调度，可以有效提高分布式量子基础设施中的计算吞吐量，但需权衡不同调度方法的优劣。", "translation": "现有量子系统面临关键瓶颈，包括有限的量子比特数、短暂的相干时间以及对错误的高度敏感性——所有这些都阻碍了大型复杂电路的执行。量子算法的进步已经超越了现有量子硬件的能力，使得有效扩展计算变得困难。此外，硬件性能的不一致性和普遍存在的量子噪声损害了系统稳定性和计算精度。为了在这些约束下优化量子工作负载，战略性的任务调度和资源协调至关重要。这些方法必须旨在加速处理、保持操作保真度并减少分布式设置中固有的通信负担。该领域中一个持续存在的挑战是如何在多个量子处理器（QPU）上高效地划分和执行大型电路，尤其是在易出错的环境中。为此，我们引入了一个基于仿真的工具，支持通过实时经典通道连接的网络化QPU上的分布式调度和并发执行量子作业。该工具模拟了超出单个QPU限制的工作负载的电路分解，允许通过处理器间通信进行并行执行。使用此仿真环境，我们比较了四种不同的调度技术——其中包括一种由强化学习启发的模型。这些策略根据多项指标进行评估，包括运行时效率、保真度保留和通信成本。我们的分析强调了每种方法固有的权衡，并突出了并行化、噪声感知的调度如何能有意义地提高分布式量子基础设施中的计算吞吐量。", "summary": "本文针对当前量子系统在处理大型复杂电路时面临的量子比特限制、相干时间短和高错误率等挑战，提出了一种基于仿真的工具。该工具旨在优化分布式量子云中的作业调度和并发执行，通过模拟电路分解和处理器间通信实现并行化。研究比较了包括强化学习在内的四种调度策略，并评估了它们在运行时效率、保真度和通信成本方面的表现，结果表明并行化和噪声感知的调度能有效提升分布式量子计算的吞吐量。", "keywords": "量子调度, 强化学习, 分布式量子计算, 量子云, 噪声感知", "comments": "本文的创新之处在于提出了一个模拟工具来解决分布式量子计算中的作业调度挑战，特别是在考虑噪声和资源限制的情况下。引入强化学习作为调度策略之一，展现了其在复杂、动态量子环境中的潜力。这项工作对于未来量子云基础设施的实用化和扩展性具有重要意义，因为它直接解决了当前量子硬件的局限性，并为优化量子工作负载提供了实用的方法。"}}
{"id": "2506.10586", "title": "Size-adaptive Hypothesis Testing for Fairness", "authors": ["Antonio Ferrara", "Francesco Cozzi", "Alan Perotti", "André Panisson", "Francesco Bonchi"], "summary": "Determining whether an algorithmic decision-making system discriminates\nagainst a specific demographic typically involves comparing a single point\nestimate of a fairness metric against a predefined threshold. This practice is\nstatistically brittle: it ignores sampling error and treats small demographic\nsubgroups the same as large ones. The problem intensifies in intersectional\nanalyses, where multiple sensitive attributes are considered jointly, giving\nrise to a larger number of smaller groups. As these groups become more\ngranular, the data representing them becomes too sparse for reliable\nestimation, and fairness metrics yield excessively wide confidence intervals,\nprecluding meaningful conclusions about potential unfair treatments.\n  In this paper, we introduce a unified, size-adaptive, hypothesis-testing\nframework that turns fairness assessment into an evidence-based statistical\ndecision. Our contribution is twofold. (i) For sufficiently large subgroups, we\nprove a Central-Limit result for the statistical parity difference, leading to\nanalytic confidence intervals and a Wald test whose type-I (false positive)\nerror is guaranteed at level $\\alpha$. (ii) For the long tail of small\nintersectional groups, we derive a fully Bayesian Dirichlet-multinomial\nestimator; Monte-Carlo credible intervals are calibrated for any sample size\nand naturally converge to Wald intervals as more data becomes available. We\nvalidate our approach empirically on benchmark datasets, demonstrating how our\ntests provide interpretable, statistically rigorous decisions under varying\ndegrees of data availability and intersectionality.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10586v1", "AI": {"title_translation": "公平性的大小自适应假设检验", "tldr": "提出了一种大小自适应的假设检验框架，用于公平性评估，解决了现有方法在小群体数据稀疏时的统计脆性问题。", "motivation": "现有的算法决策系统公平性评估方法（如比较单一指标点估计与阈值）存在统计脆性，忽略抽样误差，且对大小群体一视同仁。在交叉分析中，当多个敏感属性被联合考虑时，小群体数据稀疏导致估计不可靠，置信区间过宽，难以对潜在的不公平待遇得出有意义的结论。", "method": "论文引入了一个统一的、大小自适应的假设检验框架。对于足够大的子群体，证明了统计平等的差异的中心极限定理，并提出了Wald检验，保证第一类错误率。对于小型交叉群体，推导了一个完全贝叶斯Dirichlet-multinomial估计器，使用蒙特卡洛可信区间，该区间可针对任何样本量进行校准。", "result": "该方法在基准数据集上得到了经验性验证，证明其在不同数据可用性和交叉性下能提供可解释的、统计学上严谨的决策。", "conclusion": "该框架将公平性评估转化为基于证据的统计决策，有效解决了现有方法在数据稀疏和交叉分析中的局限性，提供了更可靠的公平性判断。", "translation": "确定算法决策系统是否歧视特定人群通常涉及将公平性指标的单一估计值与预设阈值进行比较。这种做法在统计上是脆弱的：它忽略了抽样误差，并将小型人口子群体与大型子群体同等对待。在交叉分析中，当多个敏感属性被联合考虑时，问题会加剧，导致出现更多更小的群体。随着这些群体变得更加细化，代表它们的数据变得过于稀疏，无法进行可靠估计，公平性指标会产生过宽的置信区间，从而无法对潜在的不公平待遇得出有意义的结论。\n在本文中，我们引入了一个统一的、大小自适应的假设检验框架，将公平性评估转化为基于证据的统计决策。我们的贡献是双重的。(i) 对于足够大的子群体，我们证明了统计平等的差异的中心极限定理，从而得到了分析置信区间和一个Wald检验，其第一类（假阳性）错误率保证在$\\alpha$水平。(ii) 对于小型交叉群体的长尾部分，我们推导了一个完全贝叶斯Dirichlet-multinomial估计器；蒙特卡洛可信区间针对任何样本量进行校准，并随着更多数据的可用而自然地收敛到Wald区间。我们在基准数据集上通过实证验证了我们的方法，展示了我们的检验如何在不同程度的数据可用性和交叉性下提供可解释的、统计学上严谨的决策。", "summary": "本文针对算法公平性评估中，现有方法在数据稀疏，尤其是在交叉分析中对小型群体判断的统计脆性问题，提出了一种统一的、大小自适应的假设检验框架。该框架结合了针对大型群体的中心极限定理和Wald检验，以及针对小型交叉群体的贝叶斯Dirichlet-multinomial估计器和蒙特卡洛可信区间。实验证明，该方法能在不同数据量和交叉性下提供更可靠和可解释的公平性决策。", "keywords": "公平性评估, 假设检验, 统计平等等级, 贝叶斯方法, 交叉性", "comments": "这篇论文通过引入一个大小自适应的假设检验框架，解决了算法公平性评估中的一个关键挑战：如何准确评估数据稀疏的小型群体（尤其是交叉群体）的公平性。其创新之处在于结合了传统统计方法（中心极限定理和Wald检验）与贝叶斯方法（Dirichlet-multinomial估计器），为不同数据量的群体提供了量身定制的统计推断工具。这对于提高公平性评估的统计严谨性和决策的可靠性具有重要意义。"}}
{"id": "2506.10835", "title": "General Reference Frame Identification and Transformation in Unbalanced Power Systems", "authors": ["Francisco G. Montoya", "Santiago Sánchez Acevedo"], "summary": "Various domains such as power system stability analysis, electric machine\nmodeling, and control of power electronic converters have significantly\nbenefited from the application of coordinate transformations. One of the main\nbenefits is the dimensional reduction, which reduces the complexity of the\nproblems. This paper introduces a novel general transformation based on a\ngeometric framework that directly identifies the plane containing the locus for\nunbalanced quantities through bivector analysis using Geometric Algebra. The\nproposed method provides a direct transformation valid for any degree of\nunbalance in $n$-phase, $(n+1)$-wire sinusoidal systems. The transformation\nrequires only two measurements (voltage or current) taken at different time\ninstants, making it computationally efficient. Moreover, we demonstrate through\npure geometric reasoning that our approach is general and encompasses other\ntechniques, such as the classical Clarke transformation. Numerical simulations\nand experimental validation using a real-time digital simulator and a physical\nlaboratory setup demonstrate the effectiveness of the proposed method. This\ngeneralization to multi-dimensional systems, combined with the reduced\nmeasurement requirements, represents a significant advancement over existing\napproaches that are typically restricted to three-phase applications or suffer\nfrom computational limitations.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10835v1", "AI": {"title_translation": "不平衡电力系统中的通用参考系识别与变换", "tldr": "本文提出了一种基于几何代数的新型通用变换方法，用于不平衡电力系统中的参考系识别和变换，该方法适用于任意不平衡程度的n相、(n+1)线系统，计算效率高，且包含现有技术。", "motivation": "坐标变换在电力系统稳定性分析、电机建模和电力电子变换器控制等领域带来了显著益处，主要优点是降维，从而降低了问题复杂性。", "method": "本文提出了一种基于几何框架的新型通用变换方法，通过使用几何代数中的双向量分析，直接识别包含不平衡量轨迹的平面。", "result": "所提出的方法提供了一种直接变换，适用于n相、(n+1)线正弦系统中任意程度的不平衡。该变换仅需要两个不同时间点的测量值（电压或电流），计算效率高。此外，通过纯几何推理证明，该方法具有通用性，并包含了经典Clarke变换等其他技术。数值模拟和实验验证表明了该方法的有效性。", "conclusion": "将该方法推广到多维系统，并结合其减少的测量需求，代表了相对于现有方法的重大进步，现有方法通常仅限于三相应用或存在计算限制。", "translation": "各种领域，如电力系统稳定性分析、电机建模和电力电子变换器控制，都从坐标变换的应用中显著受益。其中一个主要好处是降维，这降低了问题的复杂性。本文引入了一种基于几何框架的新型通用变换，通过使用几何代数中的双向量分析直接识别包含不平衡量轨迹的平面。所提出的方法提供了一种直接变换，适用于n相、(n+1)线正弦系统中任意程度的不平衡。该变换仅需要两个不同时间点的测量值（电压或电流），使其计算效率高。此外，我们通过纯几何推理证明，我们的方法具有通用性，并包含了经典Clarke变换等其他技术。使用实时数字模拟器和物理实验室设置进行的数值模拟和实验验证证明了所提出方法的有效性。这种向多维系统的推广，结合减少的测量需求，代表了相对于现有方法的重大进步，现有方法通常仅限于三相应用或存在计算限制。", "summary": "本文提出了一种基于几何代数和双向量分析的新型通用变换方法，用于不平衡电力系统中的参考系识别和变换。该方法能够直接识别不平衡量轨迹所在的平面，并适用于任意程度不平衡的n相、(n+1)线系统。其仅需两次测量，计算效率高，且能推广到多维系统，包含并超越了如Clarke变换等现有技术。数值模拟和实验验证证实了其有效性和优越性。", "keywords": "通用变换, 不平衡电力系统, 几何代数, 参考系, 多维系统", "comments": "本文的创新之处在于提出了一种基于几何代数的新型通用变换，能够直接处理多维不平衡系统，且仅需少量测量数据，显著提高了计算效率。其通用性使其能涵盖并超越现有技术，如经典的Clarke变换，这代表了不平衡电力系统分析领域的一大进步。"}}
{"id": "2506.10628", "title": "Leveraging Low-rank Factorizations of Conditional Correlation Matrices in Graph Learning", "authors": ["Thu Ha Phi", "Alexandre Hippert-Ferrer", "Florent Bouchard", "Arnaud Breloy"], "summary": "This paper addresses the problem of learning an undirected graph from data\ngathered at each nodes. Within the graph signal processing framework, the\ntopology of such graph can be linked to the support of the conditional\ncorrelation matrix of the data. The corresponding graph learning problem then\nscales to the squares of the number of variables (nodes), which is usually\nproblematic at large dimension. To tackle this issue, we propose a graph\nlearning framework that leverages a low-rank factorization of the conditional\ncorrelation matrix. In order to solve for the resulting optimization problems,\nwe derive tools required to apply Riemannian optimization techniques for this\nparticular structure. The proposal is then particularized to a low-rank\nconstrained counterpart of the GLasso algorithm, i.e., the penalized maximum\nlikelihood estimation of a Gaussian graphical model. Experiments on synthetic\nand real data evidence that a very efficient dimension-versus-performance\ntrade-off can be achieved with this approach.", "comment": "11 pages, 5 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10628v1", "AI": {"title_translation": "图学习中利用条件相关矩阵的低秩分解", "tldr": "本文提出了一种利用条件相关矩阵低秩分解的图学习框架，通过黎曼优化解决大规模图学习问题，并在实验中取得了高效的维度-性能权衡。", "motivation": "现有的图学习问题在处理大规模数据时，其复杂度与变量（节点）数量的平方成正比，导致在大维度下出现问题。", "method": "提出了一种利用条件相关矩阵低秩分解的图学习框架。为了解决由此产生的优化问题，推导了应用黎曼优化技术所需的工具。该方法具体化为GLasso算法的低秩约束对应物，即高斯图模型的惩罚最大似然估计。", "result": "在合成数据和真实数据上的实验表明，该方法可以实现非常高效的维度-性能权衡。", "conclusion": "通过利用条件相关矩阵的低秩分解和黎曼优化，可以有效解决大规模图学习问题，并在维度和性能之间取得良好平衡。", "translation": "本文解决了从每个节点收集的数据中学习无向图的问题。在图信号处理框架内，这种图的拓扑结构可以与数据的条件相关矩阵的支持相关联。相应的图学习问题随后会随着变量（节点）数量的平方而扩展，这在维度较大时通常会成为问题。为了解决这个问题，我们提出了一种利用条件相关矩阵低秩分解的图学习框架。为了解决由此产生的优化问题，我们推导了将黎曼优化技术应用于这种特定结构所需的工具。该提议随后被具体化为GLasso算法的低秩约束对应物，即高斯图模型的惩罚最大似然估计。在合成数据和真实数据上的实验证明，这种方法可以实现非常高效的维度与性能权衡。", "summary": "本文针对从节点数据中学习无向图的问题，提出了一种创新的图学习框架。该框架通过利用条件相关矩阵的低秩分解来解决传统方法在大维度下复杂度过高的问题。为求解优化问题，文章引入了黎曼优化技术。具体应用上，该方法是GLasso算法的低秩约束变体。实验证明，该方法在处理大规模数据时，能在维度和性能之间取得高效的平衡。", "keywords": "图学习, 低秩分解, 条件相关矩阵, 黎曼优化, GLasso", "comments": "本文的创新点在于将低秩分解与黎曼优化技术相结合，有效解决了大规模图学习中条件相关矩阵维度过高的问题。这种方法为图学习在处理高维数据时提供了一个有前景的方向，特别是在图信号处理和高斯图模型领域。"}}
{"id": "2506.10481", "title": "OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics", "authors": ["Yaoming Zhu", "Junxin Wang", "Yiyang Li", "Lin Qiu", "ZongYu Wang", "Jun Xu", "Xuezhi Cao", "Yuhuai Wei", "Mingshi Wang", "Xunliang Cai", "Rong Ma"], "summary": "As models become increasingly sophisticated, conventional algorithm\nbenchmarks are increasingly saturated, underscoring the need for more\nchallenging benchmarks to guide future improvements in algorithmic reasoning.\nThis paper introduces OIBench, a high-quality, private, and challenging\nolympiad-level informatics dataset comprising 250 carefully curated original\nproblems. We detail the construction methodology of the benchmark, ensuring a\ncomprehensive assessment across various programming paradigms and complexities,\nand we demonstrate its contamination-resistant properties via experiments. We\npropose Time/Space Completion Curves for finer-grained efficiency analysis and\nenable direct human-model comparisons through high-level participant\nevaluations. Our experiments reveal that while open-source models lag behind\nclosed-source counterparts, current SOTA models already outperform most human\nparticipants in both correctness and efficiency, while still being suboptimal\ncompared to the canonical solutions. By releasing OIBench as a fully\nopen-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we\nhope this benchmark will contribute to advancing code reasoning capabilities\nfor future LLMs.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10481v1", "AI": {"title_translation": "OIBench：使用信息学奥赛基准测试强大的推理模型", "tldr": "OIBench是一个高质量、私有、具有挑战性的奥赛级别信息学数据集，用于评估和推动算法推理模型的进步。实验表明，当前SOTA模型在正确性和效率上已超越大多数人类参与者，但仍未达到最优解。", "motivation": "随着模型日益复杂，传统算法基准测试已趋于饱和，急需更具挑战性的基准来指导算法推理能力的未来改进。", "method": "本文介绍了OIBench，一个包含250个精心策划的原创问题的奥赛级别信息学数据集。详细阐述了基准的构建方法，以确保对各种编程范式和复杂性进行全面评估，并通过实验证明了其抗污染特性。提出了时间/空间完成曲线进行更细粒度的效率分析，并通过高级别参与者评估实现人机直接比较。", "result": "实验表明，开源模型落后于闭源模型，但当前最先进的模型在正确性和效率方面已超越大多数人类参与者，尽管与标准解决方案相比仍有不足。", "conclusion": "通过发布OIBench作为完全开源的资源，作者希望这个基准能够促进未来大型语言模型的代码推理能力的发展。", "translation": "随着模型日益复杂，传统算法基准测试已趋于饱和，这凸显了需要更具挑战性的基准来指导算法推理的未来改进。本文介绍了OIBench，一个高质量、私有、具有挑战性的奥赛级别信息学数据集，包含250个精心策划的原创问题。我们详细介绍了该基准的构建方法，确保对各种编程范式和复杂性进行全面评估，并通过实验证明了其抗污染特性。我们提出了时间/空间完成曲线用于更细粒度的效率分析，并通过高级别参与者评估实现人机直接比较。我们的实验表明，虽然开源模型落后于闭源模型，但当前最先进的模型在正确性和效率方面已超越大多数人类参与者，尽管与规范解决方案相比仍有不足。通过将OIBench作为一个完全开源的资源发布（https://huggingface.co/datasets/AGI-Eval/OIBench），我们希望这个基准能够促进未来大型语言模型的代码推理能力的发展。", "summary": "本文引入了OIBench，一个高质量、私有、具有挑战性的信息学奥赛级别数据集，旨在解决现有算法基准饱和的问题。该数据集包含250个原创问题，并详细介绍了其构建方法和抗污染特性。研究者提出了时间/空间完成曲线进行效率分析，并实现了人机性能比较。实验结果显示，当前最先进的模型在正确性和效率上已超越大多数人类，但仍未达到最优解，且开源模型表现不及闭源模型。OIBench的开源发布旨在推动未来大型语言模型的代码推理能力。", "keywords": "OIBench, 算法推理, 基准测试, 信息学奥赛, 大型语言模型", "comments": "OIBench的创新之处在于其奥赛级别的问题难度和抗污染特性，为评估和推动高级算法推理能力提供了更严格的基准。其引入的时间/空间完成曲线和人机对比评估方法也提供了更细致的分析维度。该工作对于LLM在复杂编程和算法领域的进展具有重要意义，有助于识别当前模型的局限性并指导未来的研究方向。"}}
{"id": "2506.10331", "title": "Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video", "authors": ["Fei Zhao", "Da Pan", "Zelu Qi", "Ping Shi"], "summary": "In response to the rising prominence of the Metaverse, omnidirectional videos\n(ODVs) have garnered notable interest, gradually shifting from\nprofessional-generated content (PGC) to user-generated content (UGC). However,\nthe study of audio-visual quality assessment (AVQA) within ODVs remains\nlimited. To address this, we construct a dataset of UGC omnidirectional audio\nand video (A/V) content. The videos are captured by five individuals using two\ndifferent types of omnidirectional cameras, shooting 300 videos covering 10\ndifferent scene types. A subjective AVQA experiment is conducted on the dataset\nto obtain the Mean Opinion Scores (MOSs) of the A/V sequences. After that, to\nfacilitate the development of UGC-ODV AVQA fields, we construct an effective\nAVQA baseline model on the proposed dataset, of which the baseline model\nconsists of video feature extraction module, audio feature extraction and\naudio-visual fusion module. The experimental results demonstrate that our model\nachieves optimal performance on the proposed dataset.", "comment": "Our paper has been accepted by ICME 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10331v1", "AI": {"title_translation": "用户生成全景视频音视频质量评估数据集与方法研究", "tldr": "该研究构建了一个用户生成全景视频的音视频质量评估数据集，并提出了一个有效的基线模型，以解决该领域研究不足的问题。", "motivation": "随着元宇宙的兴起，全景视频（ODVs）受到关注，并逐渐从专业生成内容（PGC）转向用户生成内容（UGC）。然而，全景视频中的音视频质量评估（AVQA）研究仍然有限。", "method": "本文构建了一个用户生成全景音视频内容数据集，包含由五个人使用两种不同类型全景相机拍摄的300个视频，涵盖10种场景类型。对该数据集进行了主观AVQA实验以获取平均意见得分（MOSs）。之后，构建了一个有效的AVQA基线模型，该模型包括视频特征提取模块、音频特征提取模块和音视频融合模块。", "result": "实验结果表明，所提出的模型在所构建的数据集上取得了最佳性能。", "conclusion": "本文构建了用户生成全景视频的音视频质量评估数据集，并提出了有效的基线模型，为UGC-ODV AVQA领域的发展提供了基础和促进。", "translation": "针对元宇宙日益突出的地位，全景视频（ODVs）引起了显著关注，并逐渐从专业生成内容（PGC）转向用户生成内容（UGC）。然而，全景视频中的音视频质量评估（AVQA）研究仍然有限。为了解决这个问题，我们构建了一个用户生成全景音视频（A/V）内容数据集。视频由五个人使用两种不同类型的全景相机拍摄，共300个视频，涵盖10种不同的场景类型。对数据集进行了主观AVQA实验，以获得音视频序列的平均意见得分（MOSs）。之后，为了促进UGC-ODV AVQA领域的发展，我们在所提出的数据集上构建了一个有效的AVQA基线模型，该模型由视频特征提取模块、音频特征提取和音视频融合模块组成。实验结果表明，我们的模型在所提出的数据集上取得了最佳性能。", "summary": "本文旨在解决用户生成全景视频（UGC-ODV）中音视频质量评估（AVQA）研究不足的问题。为此，研究团队构建了一个UGC全景音视频数据集，该数据集包含由五位用户使用两种不同相机拍摄的300个视频，涵盖10种场景类型，并通过主观实验获得了音视频序列的平均意见得分（MOSs）。在此基础上，本文进一步构建了一个由视频特征提取、音频特征提取和音视频融合模块组成的有效AVQA基线模型，并通过实验证明了该模型在所构建数据集上的最优性能。", "keywords": "用户生成全景视频, 音视频质量评估, 数据集, 基线模型, 平均意见得分", "comments": "本文的创新点在于首次构建了专门针对用户生成全景视频的音视频质量评估数据集，并在此基础上提出了一个基线模型，填补了该领域研究的空白。其重要性在于为未来UGC全景视频的质量评估研究提供了基础数据和方法参考。"}}
{"id": "2506.10137", "title": "Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning", "authors": ["Daniel Lawson", "Adriana Hugessen", "Charlotte Cloutier", "Glen Berseth", "Khimya Khetarpal"], "summary": "Behavioral cloning (BC) methods trained with supervised learning (SL) are an\neffective way to learn policies from human demonstrations in domains like\nrobotics. Goal-conditioning these policies enables a single generalist policy\nto capture diverse behaviors contained within an offline dataset. While\ngoal-conditioned behavior cloning (GCBC) methods can perform well on\nin-distribution training tasks, they do not necessarily generalize zero-shot to\ntasks that require conditioning on novel state-goal pairs, i.e. combinatorial\ngeneralization. In part, this limitation can be attributed to a lack of\ntemporal consistency in the state representation learned by BC; if temporally\nrelated states are encoded to similar latent representations, then the\nout-of-distribution gap for novel state-goal pairs would be reduced. Hence,\nencouraging this temporal consistency in the representation space should\nfacilitate combinatorial generalization. Successor representations, which\nencode the distribution of future states visited from the current state, nicely\nencapsulate this property. However, previous methods for learning successor\nrepresentations have relied on contrastive samples, temporal-difference (TD)\nlearning, or both. In this work, we propose a simple yet effective\nrepresentation learning objective, $\\text{BYOL-}\\gamma$ augmented GCBC, which\nis not only able to theoretically approximate the successor representation in\nthe finite MDP case without contrastive samples or TD learning, but also,\nresults in competitive empirical performance across a suite of challenging\ntasks requiring combinatorial generalization.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10137v1", "AI": {"title_translation": "行为克隆中组合泛化的自预测表示", "tldr": "现有的目标条件行为克隆（GCBC）在面对新的状态-目标对时，组合泛化能力不足，部分原因在于状态表示缺乏时间一致性。本文提出了一种名为BYOL-γ增强GCBC的简单有效表示学习方法，它无需对比样本或时序差分（TD）学习即可近似后继表示，并在需要组合泛化的任务中取得了有竞争力的经验性能。", "motivation": "目标条件行为克隆（GCBC）方法在分布内训练任务上表现良好，但在需要对新颖状态-目标对进行条件设置的任务（即组合泛化）时，不一定能实现零样本泛化。这种限制部分归因于行为克隆学习到的状态表示缺乏时间一致性。如果时间相关的状态被编码为相似的潜在表示，则可以减少新颖状态-目标对的分布外差距。因此，鼓励表示空间中的时间一致性应有助于组合泛化。", "method": "本文提出了一种简单而有效的表示学习目标，即BYOL-γ增强GCBC。该方法无需对比样本或时序差分（TD）学习，便能在有限MDP情况下理论上近似后继表示。", "result": "该方法在需要组合泛化的一系列具有挑战性的任务中，取得了有竞争力的经验性能。", "conclusion": "本文提出的BYOL-γ增强GCBC方法，通过学习自预测表示，有效解决了目标条件行为克隆在组合泛化方面的挑战，且无需依赖对比样本或时序差分学习。", "translation": "行为克隆（BC）方法通过监督学习（SL）训练，是学习机器人等领域人类演示策略的有效方式。对这些策略进行目标条件设置，使得单一通用策略能够捕获离线数据集中包含的各种行为。虽然目标条件行为克隆（GCBC）方法在分布内训练任务上表现良好，但它们不一定能零样本泛化到需要对新颖状态-目标对进行条件设置的任务，即组合泛化。部分原因在于BC学习到的状态表示缺乏时间一致性；如果时间相关的状态被编码为相似的潜在表示，那么新颖状态-目标对的分布外差距将会减小。因此，鼓励表示空间中的这种时间一致性应有助于组合泛化。后继表示，编码从当前状态访问的未来状态分布，很好地概括了这一特性。然而，之前学习后继表示的方法依赖于对比样本、时序差分（TD）学习或两者兼有。在这项工作中，我们提出了一种简单而有效的表示学习目标，即BYOL-γ增强GCBC，它不仅能够在有限MDP情况下理论上近似后继表示而无需对比样本或TD学习，而且在一系列需要组合泛化的具有挑战性任务中，也取得了有竞争力的经验性能。", "summary": "本文针对目标条件行为克隆（GCBC）在面对新颖状态-目标对时组合泛化能力不足的问题，提出了一种名为BYOL-γ增强GCBC的表示学习方法。该方法旨在通过鼓励状态表示的时间一致性来提升泛化能力，其核心在于无需对比样本或时序差分（TD）学习即可近似后继表示。实验结果表明，BYOL-γ增强GCBC在需要组合泛化的多项任务中展现出有竞争力的性能。", "keywords": "行为克隆, 组合泛化, 后继表示, 表示学习, 目标条件", "comments": "该论文的创新点在于提出了一种不依赖于对比学习或时序差分（TD）学习就能学习到近似后继表示的方法（BYOL-γ增强GCBC），这简化了表示学习过程，同时有效提升了行为克隆在面对新颖组合任务时的泛化能力。其理论证明和经验性能均显示了该方法的潜力和重要性。"}}
{"id": "2506.10488", "title": "Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation", "authors": ["Juan C. Martinez-Sevilla", "Joan Cerveto-Serrano", "Noelia Luna", "Greg Chapman", "Craig Sapp", "David Rizo", "Jorge Calvo-Zaragoza"], "summary": "In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six\nhundred and eighty-five pages specifically designed to benchmark Optical Music\nRecognition (OMR) research. SMB encompasses a diverse array of musical\ntextures, including monophony, pianoform, quartet, and others, all encoded in\nCommon Western Modern Notation using the Humdrum **kern format. Alongside SMB,\nwe introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored\nexplicitly for evaluating OMR performance. OMR-NED builds upon the widely-used\nSymbol Error Rate (SER), offering a fine-grained and detailed error analysis\nthat covers individual musical elements such as note heads, beams, pitches,\naccidentals, and other critical notation features. The resulting numeric score\nprovided by OMR-NED facilitates clear comparisons, enabling researchers and\nend-users alike to identify optimal OMR approaches. Our work thus addresses a\nlong-standing gap in OMR evaluation, and we support our contributions with\nbaseline experiments using standardized SMB dataset splits for training and\nassessing state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10488v1", "AI": {"title_translation": "乐谱基准：标准化光学音乐识别评估", "tldr": "引入了乐谱基准数据集（SMB）和OMR归一化编辑距离（OMR-NED）新度量，以标准化光学音乐识别（OMR）的评估。", "motivation": "解决光学音乐识别（OMR）评估中长期存在的空白，提供标准化的基准和更精细的评估指标。", "method": "本文引入了Sheet Music Benchmark (SMB) 数据集，包含685页多样音乐纹理（如单音、钢琴形式、四重奏等），并采用Humdrum **kern格式编码。同时，引入了OMR Normalized Edit Distance (OMR-NED) 新度量，该度量基于Symbol Error Rate (SER)，并能对音符头、符杠、音高、变音记号等单个音乐元素进行详细的错误分析。", "result": "OMR-NED提供的数值分数便于清晰比较，使研究人员和最终用户能够识别最佳的OMR方法。该工作填补了OMR评估的长期空白，并通过使用标准化的SMB数据集分割进行基线实验来支持其贡献。", "conclusion": "SMB数据集和OMR-NED度量的引入为OMR研究提供了一个急需的标准化评估框架，有助于推动该领域的发展。", "translation": "在这项工作中，我们引入了乐谱基准（SMB），这是一个包含685页的专门用于基准测试光学音乐识别（OMR）研究的数据集。SMB涵盖了各种音乐纹理，包括单音、钢琴形式、四重奏等，所有这些都使用Humdrum **kern格式编码为通用西方现代乐谱。除了SMB，我们还引入了OMR归一化编辑距离（OMR-NED），这是一种专门为评估OMR性能量身定制的新度量。OMR-NED建立在广泛使用的符号错误率（SER）之上，提供了细致入微的错误分析，涵盖了音符头、符杠、音高、变音记号和其他关键记谱特征等单个音乐元素。OMR-NED提供的最终数值分数有助于清晰的比较，使研究人员和最终用户都能识别最佳的OMR方法。因此，我们的工作解决了OMR评估中长期存在的空白，我们通过使用标准化的SMB数据集分割进行训练和评估最先进方法来支持我们的贡献。", "summary": "本文介绍了乐谱基准（SMB）数据集，旨在标准化光学音乐识别（OMR）研究的评估。SMB包含685页乐谱，涵盖多种音乐纹理。同时，论文还提出了OMR归一化编辑距离（OMR-NED），这是一个基于符号错误率（SER）的新评估指标，能对音符的各个元素进行精细的错误分析。通过SMB和OMR-NED，研究人员可以更有效地比较不同的OMR方法，从而填补了OMR评估领域的空白。", "keywords": "光学音乐识别, 数据集, 评估, 乐谱基准, 归一化编辑距离", "comments": "这项工作通过引入一个专门设计的数据集（SMB）和定制的评估指标（OMR-NED），解决了光学音乐识别（OMR）领域长期存在的标准化评估问题。OMR-NED能够进行细粒度的错误分析，这对于理解和改进OMR算法至关重要，是该领域的重要贡献。"}}
{"id": "2506.10231", "title": "Classifying Unreliable Narrators with Large Language Models", "authors": ["Anneliese Brei", "Katharine Henry", "Abhisheik Sharma", "Shashank Srivastava", "Snigdha Chaturvedi"], "summary": "Often when we interact with a first-person account of events, we consider\nwhether or not the narrator, the primary speaker of the text, is reliable. In\nthis paper, we propose using computational methods to identify unreliable\nnarrators, i.e. those who unintentionally misrepresent information. Borrowing\nliterary theory from narratology to define different types of unreliable\nnarrators based on a variety of textual phenomena, we present TUNa, a\nhuman-annotated dataset of narratives from multiple domains, including blog\nposts, subreddit posts, hotel reviews, and works of literature. We define\nclassification tasks for intra-narrational, inter-narrational, and\ninter-textual unreliabilities and analyze the performance of popular\nopen-weight and proprietary LLMs for each. We propose learning from literature\nto perform unreliable narrator classification on real-world text data. To this\nend, we experiment with few-shot, fine-tuning, and curriculum learning\nsettings. Our results show that this task is very challenging, and there is\npotential for using LLMs to identify unreliable narrators. We release our\nexpert-annotated dataset and code and invite future research in this area.", "comment": "ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10231v1", "AI": {"title_translation": "使用大型语言模型分类不可靠叙述者", "tldr": "本文提出使用大型语言模型（LLMs）来识别不可靠的叙述者。为此，作者创建了一个名为TUNa的人工标注数据集，并定义了分类任务。尽管任务极具挑战性，但研究结果显示LLMs在此领域具有潜力，并发布了数据集和代码以促进未来研究。", "motivation": "在与第一人称叙述互动时，判断叙述者是否可靠是一个常见问题。本文旨在提出计算方法来识别那些无意中歪曲信息的不可靠叙述者。", "method": "1. 借鉴叙事学文学理论定义了不同类型的不可靠叙述者。2. 构建并发布了TUNa，一个包含博客、Reddit帖子、酒店评论和文学作品等多种领域叙述的人工标注数据集。3. 定义了叙述内、叙述间和文本间不可靠性的分类任务。4. 分析了流行的开源和专有大型语言模型在这些任务上的表现。5. 实验了少样本、微调和课程学习设置。", "result": "识别不可靠叙述者这项任务非常具有挑战性。尽管如此，研究结果表明使用大型语言模型识别不可靠叙述者具有潜力。", "conclusion": "本文提出了一种使用大型语言模型分类不可靠叙述者的方法，并为此构建了新的专家标注数据集TUNa和相关代码。尽管该任务极具挑战性，但研究证实了LLMs在此领域的潜力，并鼓励未来研究。", "translation": "当我们与第一人称叙述的事件互动时，我们经常会考虑叙述者，即文本的主要讲述者，是否可靠。在本文中，我们提出使用计算方法来识别不可靠的叙述者，即那些无意中歪曲信息的人。我们借鉴叙事学中的文学理论，根据各种文本现象定义了不同类型的不可靠叙述者，并提出了TUNa，一个包含来自多个领域（包括博客文章、Reddit帖子、酒店评论和文学作品）叙述的人工标注数据集。我们定义了叙述内、叙述间和文本间不可靠性的分类任务，并分析了流行的开源和专有大型语言模型在每个任务上的表现。我们提出从文学中学习，以在真实世界的文本数据上执行不可靠叙述者分类。为此，我们尝试了少样本、微调和课程学习设置。我们的结果表明，这项任务非常具有挑战性，但使用大型语言模型识别不可靠叙述者具有潜力。我们发布了我们专家标注的数据集和代码，并邀请该领域的未来研究。", "summary": "本文提出了一种计算方法来识别第一人称叙述中的不可靠叙述者。研究借鉴叙事学理论定义了叙述者不可靠性的类型，并构建了一个名为TUNa的人工标注数据集，其中包含来自不同领域的文本。作者定义并评估了多种分类任务，并分析了大型语言模型在少样本、微调和课程学习设置下的表现。尽管任务难度高，但结果表明LLMs在识别不可靠叙述者方面具有潜力。论文还发布了数据集和代码，以促进该领域的进一步研究。", "keywords": "不可靠叙述者, 大型语言模型, 数据集, 自然语言处理, 叙事学", "comments": "该论文通过将文学理论与计算方法相结合，解决了一个新颖且具有挑战性的自然语言处理任务。创建了一个新的、经过专家标注的数据集（TUNa），这是对该领域的重要贡献，为未来研究识别不可靠叙述等细微文本现象提供了宝贵资源。对不同LLM训练范式（少样本、微调、课程学习）的探索是全面的，并且对任务难度的坦诚承认以及潜力的识别提供了现实的展望。"}}
{"id": "2506.10226", "title": "ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators", "authors": ["Parsa Rahimi", "Sebastien Marcel"], "summary": "In this paper, we propose ScoreMix, a novel yet simple data augmentation\nstrategy leveraging the score compositional properties of diffusion models to\nenhance discriminator performance, particularly under scenarios with limited\nlabeled data. By convexly mixing the scores from different class-conditioned\ntrajectories during diffusion sampling, we generate challenging synthetic\nsamples that significantly improve discriminative capabilities in all studied\nbenchmarks. We systematically investigate class-selection strategies for mixing\nand discover that greater performance gains arise when combining classes\ndistant in the discriminator's embedding space, rather than close in the\ngenerator's condition space. Moreover, we empirically show that, under standard\nmetrics, the correlation between the generator's learned condition space and\nthe discriminator's embedding space is minimal. Our approach achieves notable\nperformance improvements without extensive parameter searches, demonstrating\npractical advantages for training discriminative models while effectively\nmitigating problems regarding collections of large datasets. Paper website:\nhttps://parsa-ra.github.io/scoremix", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10226v1", "AI": {"title_translation": "ScoreMix：通过扩散生成器中的分数组合改进人脸识别", "tldr": "ScoreMix是一种新颖的数据增强策略，它利用扩散模型的得分组合特性生成具有挑战性的合成样本，从而显著提高判别器的性能，尤其是在标记数据有限的情况下。", "motivation": "在标记数据有限的情况下提高判别器性能，并有效缓解收集大型数据集以训练判别模型的问题。", "method": "提出了一种名为ScoreMix的数据增强策略。该方法通过在扩散采样过程中凸混合来自不同类别条件轨迹的分数，生成具有挑战性的合成样本。研究了混合的类别选择策略，发现结合判别器嵌入空间中距离较远的类别能带来更大的性能提升。", "result": "ScoreMix显著提高了所有研究基准中的判别能力。当结合判别器嵌入空间中距离较远的类别时，性能提升更大。经验性地表明，在标准指标下，生成器学习到的条件空间与判别器嵌入空间之间的相关性最小。该方法在没有大量参数搜索的情况下取得了显著的性能改进。", "conclusion": "ScoreMix是一种实用且有效的方法，用于训练判别模型，并能有效缓解大型数据集收集的问题，展现出显著的性能提升。", "translation": "在本文中，我们提出了ScoreMix，这是一种新颖而简单的数据增强策略，它利用扩散模型的得分组合特性来增强判别器性能，特别是在标记数据有限的情况下。通过在扩散采样过程中凸混合来自不同类别条件轨迹的分数，我们生成了具有挑战性的合成样本，这些样本显著提高了所有研究基准中的判别能力。我们系统地研究了混合的类别选择策略，发现当结合判别器嵌入空间中距离较远的类别时，而不是生成器条件空间中接近的类别时，会产生更大的性能增益。此外，我们通过经验证明，在标准指标下，生成器学习到的条件空间与判别器嵌入空间之间的相关性最小。我们的方法在没有大量参数搜索的情况下取得了显著的性能改进，展示了训练判别模型的实际优势，同时有效缓解了大型数据集收集的问题。论文网站：https://parsa-ra.github.io/scoremix", "summary": "ScoreMix是一种新颖的数据增强方法，通过利用扩散模型的得分组合特性，在扩散采样过程中混合不同类别的得分，生成具有挑战性的合成样本。该方法显著提升了判别器的性能，尤其在标记数据有限的场景下表现突出。研究发现，混合在判别器嵌入空间中距离较远的类别比在生成器条件空间中接近的类别能带来更大的性能提升。ScoreMix无需大量参数搜索即可实现显著性能改进，为训练判别模型提供了实用优势，并有效缓解了大数据集收集的难题。", "keywords": "ScoreMix, 扩散模型, 数据增强, 人脸识别, 判别器性能", "comments": "ScoreMix的创新点在于其利用扩散模型的得分组合特性进行数据增强，以生成更具挑战性的训练样本。特别值得注意的是，该研究发现混合在判别器嵌入空间中距离较远的类别能带来更大的性能提升，这揭示了生成器和判别器空间之间的潜在差异。其重要性在于，它提供了一种有效且无需大量标记数据即可提升判别模型性能的方法，对于资源受限或需要快速部署的场景具有实际应用价值。"}}
{"id": "2506.10416", "title": "Can Sound Replace Vision in LLaVA With Token Substitution?", "authors": ["Ali Vosoughi", "Jing Bi", "Pinxin Liu", "Yunlong Tang", "Chenliang Xu"], "summary": "While multimodal systems have achieved impressive advances, they typically\nrely on text-aligned representations rather than directly integrating audio and\nvisual inputs. This reliance can limit the use of acoustic information in tasks\nrequiring nuanced audio understanding. In response, SoundCLIP explores direct\naudio-visual integration within multimodal large language models (MLLMs) by\nsubstituting CLIP's visual tokens with audio representations and selecting\nsound-relevant patch tokens in models such as LLaVA. We investigate two\nconfigurations: (1) projecting audio features into CLIP's visual manifold via a\nmultilayer perceptron trained with InfoNCE on paired audio-video segments, and\n(2) preserving raw audio embeddings with minimal dimensional adjustments.\nExperiments with five state-of-the-art audio encoders reveal a fundamental\ntrade-off. While audio-to-video retrieval performance increases dramatically\n(up to 44 percentage points in Top-1 accuracy) when audio is projected into\nCLIP's space, text generation quality declines. Encoders pre-trained with text\nsupervision (CLAP, Whisper, ImageBind) maintain stronger generative\ncapabilities than those focused primarily on audiovisual alignment (Wav2CLIP,\nAudioCLIP), highlighting the value of language exposure for generation tasks.\nWe introduce WhisperCLIP, an architecture that fuses intermediate\nrepresentations from Whisper, as well as AudioVisual Event Evaluation (AVE-2),\na dataset of 580,147 three-second audiovisual clips with fine-grained alignment\nannotations. Our findings challenge the assumption that stronger cross-modal\nalignment necessarily benefits all multimodal tasks; instead, a Pareto frontier\nemerges wherein optimal performance depends on balancing retrieval accuracy\nwith text generation quality. Codes and datasets:\nhttps://github.com/ali-vosoughi/SoundCLIP.", "comment": "29 pages including references and appendices", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10416v1", "AI": {"title_translation": "声音能否通过令牌替换在LLaVA中取代视觉？", "tldr": "SoundCLIP通过令牌替换将音频集成到MLLM中，发现检索性能和文本生成质量之间存在权衡。", "motivation": "现有多模态系统通常依赖文本对齐表示而非直接集成音频和视觉输入，这限制了声学信息在需要细致音频理解的任务中的使用。SoundCLIP旨在解决这一问题，探索多模态大语言模型（MLLMs）中直接的音视集成。", "method": "引入SoundCLIP，通过将CLIP的视觉令牌替换为音频表示，并在LLaVA等模型中选择与声音相关的补丁令牌。研究了两种配置：(1) 将音频特征投影到CLIP的视觉流形中，(2) 保留原始音频嵌入。实验使用了五种最先进的音频编码器，并引入了WhisperCLIP架构和AudioVisual Event Evaluation (AVE-2) 数据集。", "result": "当音频投影到CLIP空间时，音视频检索性能显著提高（Top-1准确率提高高达44个百分点），但文本生成质量下降。经过文本监督预训练的编码器（CLAP, Whisper, ImageBind）比主要关注音视对齐的编码器（Wav2CLIP, AudioCLIP）保持更强的生成能力。发现更强的跨模态对齐不一定有利于所有多模态任务，而是存在一个帕累托前沿，最佳性能取决于平衡检索准确性和文本生成质量。", "conclusion": "跨模态对齐的增强不一定对所有多模态任务都有利；相反，最佳性能在于平衡检索准确性和文本生成质量之间存在一个帕累托前沿。经过文本监督预训练的编码器在生成任务中表现更好。", "translation": "多模态系统取得了令人印象深刻的进展，但它们通常依赖于文本对齐的表示，而不是直接集成音频和视觉输入。这种依赖性可能会限制声学信息在需要细致音频理解的任务中的使用。为此，SoundCLIP通过用音频表示替换CLIP的视觉令牌并在LLaVA等模型中选择与声音相关的补丁令牌，探索了多模态大语言模型（MLLM）中的直接音视集成。我们研究了两种配置：(1) 通过多层感知器（使用InfoNCE在配对音视频段上训练）将音频特征投影到CLIP的视觉流形中，以及 (2) 保留原始音频嵌入并进行最小的维度调整。对五种最先进的音频编码器的实验揭示了一个根本性的权衡。当音频投影到CLIP空间时，音视频检索性能显著提高（Top-1准确率提高高达44个百分点），但文本生成质量下降。经过文本监督预训练的编码器（CLAP、Whisper、ImageBind）比主要关注音视对齐的编码器（Wav2CLIP、AudioCLIP）保持更强的生成能力，这凸显了语言暴露对生成任务的价值。我们引入了WhisperCLIP，这是一种融合了Whisper中间表示的架构，以及AudioVisual Event Evaluation (AVE-2)，一个包含580,147个三秒音视频片段和细粒度对齐注释的数据集。我们的发现挑战了“更强的跨模态对齐必然有利于所有多模态任务”的假设；相反，出现了一个帕累托前沿，其中最佳性能取决于平衡检索准确性和文本生成质量。代码和数据集：https://github.com/ali-vosoughi/SoundCLIP。", "summary": "本研究引入了SoundCLIP，旨在解决多模态大语言模型中音频和视觉输入直接集成不足的问题。通过将CLIP的视觉令牌替换为音频表示，并在LLaVA等模型中探索两种配置，研究人员发现，虽然将音频投影到视觉空间能显著提升音视频检索性能，但却会损害文本生成质量。实验表明，经过文本监督预训练的音频编码器在生成任务中表现更优。研究结果挑战了跨模态对齐越强越好的假设，提出最佳性能存在于检索准确性和文本生成质量之间的权衡中。此外，论文还引入了WhisperCLIP架构和AVE-2数据集。", "keywords": "SoundCLIP, 多模态大语言模型, 音频-视觉集成, 令牌替换, 跨模态对齐", "comments": "这项研究具有创新性，因为它直接探索了在MLLM中用声音替代视觉的可能性，挑战了传统的多模态融合范式。其重要性在于揭示了在音视频多模态任务中，检索性能和生成质量之间存在一个帕累特定律式的权衡，这对于未来MLLM的设计具有指导意义。引入WhisperCLIP和AVE-2数据集也为后续研究提供了有价值的资源。"}}
{"id": "2506.10723", "title": "Semi-discrete moduli of smoothness and their applications in one- and two- sided error estimates", "authors": ["Danilo Costarelli", "Donato Lavella"], "summary": "In this paper, we introduce a new semi-discrete modulus of smoothness, which\ngeneralizes the definition given by Kolomoitsev and Lomako (KL) in 2023 (in the\npaper published in the J. Approx. Theory), and we establish very general one-\nand two- sided error estimates under non-restrictive assumptions. The proposed\nresults have been proved exploiting the regularization and approximation\nproperties of certain Steklov integrals introduced by Sendov and Popov in 1983,\nand differ from the ones given by Kolomoitsev and Lomako. In addition, the\nproof of the original KL approximation theorems were strictly related to the\napplication of certain classical results of the trigonometric best\napproximation, and thus, they are applicable only for operators of the\ntrigonometric type. By the definition of semi-discrete moduli of smoothness\nhere proposed, we are able to deduce applications also for operators that are\nnot necessarily of the trigonometric type, and can also be used to derive\nsharper estimates than those that can be achieved by the classical averaged\nmoduli of smoothness ($\\tau$-moduli). Furthermore, a Rathore-type theorem is\nestablished, and a new notion of K-functional is also introduced showing its\nequivalence with the semi-discrete modulus of smoothness and its realization.\nOne-sided estimates of approximation can be established for classical operators\non bounded domains, such as the Bernstein polynomials. In the case of\napproximation operators on the whole real line, one-sided estimates can be\nachieved, e.g., for the Shannon sampling (cardinal) series, as well as for the\nso-called generalized sampling operators. At the end of the paper, the case of\nalgebraic Lagrange approximation has been considered, showing the main open\nproblems in order to derive two-sided error estimates in this noteworthy case.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10723v1", "AI": {"title_translation": "半离散光滑模及其在单边和双边误差估计中的应用", "tldr": "本文引入了一种新的半离散光滑模，推广了现有定义，并在非限制性假设下建立了单边和双边误差估计，适用于更广泛的算子类型，并能获得更尖锐的估计。", "motivation": "推广Kolomoitsev和Lomako (KL)于2023年给出的半离散光滑模的定义，并在非限制性假设下建立更一般的单边和双边误差估计，解决KL定理仅适用于三角型算子的问题，使其适用于非三角型算子，并获得比经典平均光滑模（$\\tau$-模）更尖锐的估计。", "method": "引入了一种新的半离散光滑模；利用Sendov和Popov于1983年引入的某些Steklov积分的正则化和逼近性质；建立了Rathore型定理；引入了新的K-泛函概念并证明其与半离散光滑模及其实现等价性。", "result": "建立了非常一般的单边和双边误差估计，且在非限制性假设下；所提出的结果与Kolomoitsev和Lomako的结果不同；新的半离散光滑模定义能够应用于非三角型算子，并能推导出比经典平均光滑模更尖锐的估计；证明了新的K-泛函与半离散光滑模及其实现的等价性；在有界域上的经典算子（如Bernstein多项式）和整个实数线上的逼近算子（如Shannon采样级数、广义采样算子）均可建立单边逼近估计。", "conclusion": "本文引入了一种新的半离散光滑模，成功推广了现有定义，并在更广泛的条件下建立了通用的单边和双边误差估计，使其适用于更多类型的算子并提供更精确的估计。研究还考虑了代数拉格朗日逼近中双边误差估计的开放问题。", "translation": "本文引入了一种新的半离散光滑模，它推广了Kolomoitsev和Lomako（KL）于2023年（发表在《逼近理论杂志》上的论文中）给出的定义，并在非限制性假设下建立了非常普遍的单边和双边误差估计。所提出的结果是利用Sendov和Popov于1983年引入的某些Steklov积分的正则化和逼近性质证明的，并且与Kolomoitsev和Lomako给出的结果不同。此外，原始KL逼近定理的证明与三角最佳逼近的某些经典结果的应用密切相关，因此，它们仅适用于三角型算子。通过本文提出的半离散光滑模的定义，我们也能够推导出适用于不一定是三角型算子的应用，并且还可以用于推导出比经典平均光滑模（$\\tau$-模）更尖锐的估计。此外，还建立了一个Rathore型定理，并引入了一个新的K-泛函概念，显示了其与半离散光滑模及其实现的等价性。对于有界域上的经典算子，例如Bernstein多项式，可以建立单边逼近估计。在整个实数线上的逼近算子的情况下，可以实现单边估计，例如Shannon采样（基数）级数以及所谓的广义采样算子。在论文的最后，考虑了代数拉格朗日逼近的情况，展示了在该值得关注的情况下推导双边误差估计的主要开放问题。", "summary": "本文提出了一种新的半离散光滑模，推广了Kolomoitsev和Lomako的定义，并在非限制性条件下建立了更通用的单边和双边误差估计。研究利用Steklov积分的性质，解决了现有方法仅限于三角型算子的问题，使得新方法能应用于更广泛的算子类型，并能获得比传统方法更精确的估计。文中还建立了一个Rathore型定理，并引入了与半离散光滑模等价的K-泛函。新方法可用于经典算子如Bernstein多项式以及Shannon采样级数等，并讨论了代数拉格朗日逼近中双边误差估计的开放问题。", "keywords": "半离散光滑模, 误差估计, 逼近理论, K-泛函, Steklov积分", "comments": "该论文在逼近理论领域具有重要意义。它通过引入新的半离散光滑模，不仅推广了现有定义，还显著扩展了其适用范围，使其能够处理非三角型算子，这是对以往研究的一个重要突破。此外，能够获得更尖锐的误差估计也提升了理论的实用性。对K-泛函的引入和等价性证明也进一步丰富了理论基础。"}}
{"id": "2506.10323", "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space", "authors": ["Chuyang Chen", "Brendan Dolan-Gavitt", "Zhiqiang Lin"], "summary": "Generation-based fuzzing produces appropriate testing cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual efforts to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage and triggers up to 174.0% more artificially injected bugs. We also\nused ELFuzz to conduct a real-world fuzzing campaign on the newest version of\ncvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are\nexploitable). Moreover, we conducted an ablation study, which shows that the\nfuzzer space model, the key component of ELFuzz, contributes the most (up to\n62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers\nsynthesized by ELFuzz confirms that they catch interesting grammatical\nstructures and semantic constraints in a human-understandable way. The results\npresent the promising potential of ELFuzz for more automated, efficient, and\nextensible input generation for fuzzing.", "comment": "Accepted by USENIX Security'25 Cycle 2", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10323v1", "AI": {"title_translation": "ELFuzz：通过LLM驱动的模糊器空间合成实现高效输入生成", "tldr": "ELFuzz利用大型语言模型自动生成高效的模糊测试器，显著提高代码覆盖率并发现更多bug，包括0-day漏洞。", "motivation": "现有的基于生成的模糊测试方法需要大量手动工作来构建输入语法和语义约束规范。", "method": "本文提出了一种新方法ELFuzz（Evolution Through Large Language Models for Fuzzing），它通过在模糊器空间上进行LLM驱动的合成，自动为待测试系统（SUT）定制生成式模糊器。该方法从最小的种子模糊器开始，并通过LLM驱动的自动化演化（受覆盖率指导）来推动合成。", "result": "ELFuzz可以无缝扩展到实际规模的SUT（评估中高达1,791,104行代码），并合成能够以人类可理解的方式捕获语法结构和语义约束的高效模糊器。与现有方法相比，ELFuzz的代码覆盖率提高高达434.8%，触发的人工注入bug增加高达174.0%。在cvc5最新版本上进行了14天的真实世界模糊测试，发现了五个0-day bug（其中三个可被利用）。消融研究表明，模糊器空间模型对ELFuzz的有效性贡献最大（高达62.5%）。", "conclusion": "ELFuzz在实现更自动化、高效和可扩展的模糊测试输入生成方面展现出巨大的潜力。", "translation": "基于生成的模糊测试根据输入语法和语义约束的规范生成适当的测试用例来测试系统和软件。然而，这些规范的构建需要大量手动工作。本文提出了一种新方法ELFuzz（通过大型语言模型进行模糊测试的演化），它通过在模糊器空间上进行LLM驱动的合成，自动为待测试系统（SUT）定制生成式模糊器。总的来说，它从最小的种子模糊器开始，并通过完全自动化的LLM驱动演化和覆盖率指导来推动合成。与以前的方法相比，ELFuzz可以1）无缝扩展到实际规模的SUT——在我们的评估中高达1,791,104行代码——以及2）合成高效的模糊器，以人类可理解的方式捕获有趣的语法结构和语义约束。我们的评估将ELFuzz与领域专家手动编写的规范以及最先进方法合成的规范进行了比较。结果表明，ELFuzz的代码覆盖率提高了高达434.8%，触发的人工注入bug增加了高达174.0%。我们还使用ELFuzz对最新版本的cvc5进行了为期14天的真实世界模糊测试活动，令人鼓舞的是，它发现了五个0-day bug（其中三个是可利用的）。此外，我们进行了一项消融研究，结果表明模糊器空间模型作为ELFuzz的关键组件，对ELFuzz的有效性贡献最大（高达62.5%）。对ELFuzz合成的模糊器进行进一步分析证实，它们以人类可理解的方式捕获了有趣的语法结构和语义约束。这些结果展现了ELFuzz在模糊测试中实现更自动化、高效和可扩展的输入生成方面的巨大潜力。", "summary": "ELFuzz是一种新颖的模糊测试方法，它利用大型语言模型（LLM）在模糊器空间上自动合成定制的生成式模糊器，以解决传统方法中手动构建测试规范的繁重工作。该方法从最小的种子模糊器开始，通过LLM驱动的演化和覆盖率指导来优化模糊器。实验结果表明，ELFuzz能够高效地扩展到大型系统，显著提高代码覆盖率并发现更多bug，包括真实世界中的0-day漏洞。其关键组件——模糊器空间模型，对性能提升贡献显著。ELFuzz展现了实现自动化、高效和可扩展的模糊测试输入的巨大潜力。", "keywords": "模糊测试, LLM, 输入生成, 模糊器合成, 代码覆盖率", "comments": "ELFuzz的创新之处在于将LLM引入模糊测试的模糊器合成过程，通过自动化演化显著降低了手动构建测试规范的成本，并提高了模糊测试的效率和有效性。其在发现真实世界0-day漏洞方面的表现突出了其实用价值。"}}
{"id": "2506.10525", "title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "authors": ["Junhang Cheng", "Fang Liu", "Chengru Wu", "Li Zhang"], "summary": "While Large Language Models (LLMs) have significantly advanced code\ngeneration efficiency, they face inherent challenges in balancing performance\nand inference costs across diverse programming tasks. Dynamically selecting the\noptimal LLM based on task difficulty and resource constraints offers a\npromising approach to achieve an optimal balance between efficiency and\nperformance. However, existing model selection methods are resource-intensive\nand often neglect cost efficiency. Moreover, these approaches rely on\nhuman-annotated difficulty labels that are frequently inaccessible in\nreal-world settings and may not align with the LLM's own assessment of task\ndifficulty. In this paper, we introduce AdaptiveLLM, a framework that\ndynamically selects optimal LLMs for a given coding task by automatically\nassessing task difficulty. Our framework first estimates task difficulty using\nChain-of-Thought lengths generated by reasoning model, clusters these into\nthree difficulty levels via k-means, and fine-tunes CodeBERT to embed\ndifficulty-aware features. A trained XGBoost classifier then selects the best\nmodel for each problem, optimizing the performance-cost trade-off. Experimental\nresults show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score\nwhile reducing resource consumption by 88.9% compared to baseline method\nComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an\napproximately 15% accuracy improvement, while maintaining the same level of\ncost consumption. Apart from that, the difficulty assessment using CoT provides\nmore reliable selection criteria than human evaluation. Our replication package\nis available at https://github.com/cjhCoder7/AdaptiveLLM.", "comment": "Accepted by Internetware 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10525v1", "AI": {"title_translation": "AdaptiveLLM：一个基于CoT长度为代码生成选择最佳成本效益LLM的框架", "tldr": "AdaptiveLLM是一个根据Chain-of-Thought长度自动评估任务难度，并动态选择最佳成本效益大型语言模型（LLM）用于代码生成的框架，显著提高了性能并降低了资源消耗。", "motivation": "大型语言模型（LLMs）在代码生成方面表现出色，但面临性能与推理成本之间的平衡挑战。现有模型选择方法资源密集，忽视成本效益，且依赖难以获取或与LLM自身评估不符的人工标注难度标签。", "method": "AdaptiveLLM框架通过以下步骤动态选择最优LLM：首先，使用推理模型生成的Chain-of-Thought（CoT）长度来估计任务难度；其次，通过k-means将这些难度聚类为三个级别；然后，微调CodeBERT以嵌入难度感知特征；最后，训练一个XGBoost分类器来选择每个问题的最佳模型，以优化性能-成本权衡。", "result": "实验结果显示，与基线方法ComplexityNet相比，AdaptiveLLM的pass@1得分提高了7.86%，同时资源消耗降低了88.9%。与单个模型相比，AdaptiveLLM在保持相同成本水平的同时，准确性提高了约15%。此外，使用CoT进行难度评估提供了比人工评估更可靠的选择标准。", "conclusion": "AdaptiveLLM通过自动评估任务难度并动态选择最佳LLM，有效解决了代码生成中性能与成本的平衡问题，显著提升了效率和性能，并证明了CoT长度作为难度评估指标的可靠性。", "translation": "大型语言模型（LLMs）显著提升了代码生成效率，但在不同编程任务中面临平衡性能和推理成本的固有挑战。根据任务难度和资源限制动态选择最优LLM，为实现效率和性能的最佳平衡提供了一种有前景的方法。然而，现有的模型选择方法资源密集，且常常忽视成本效益。此外，这些方法依赖于人工标注的难度标签，这些标签在实际应用中往往难以获取，并且可能与LLM自身对任务难度的评估不符。本文介绍了一种名为AdaptiveLLM的框架，该框架通过自动评估任务难度，为给定的编码任务动态选择最优的LLM。我们的框架首先使用推理模型生成的思维链（Chain-of-Thought）长度来估计任务难度，然后通过k-means将这些长度聚类为三个难度级别，并微调CodeBERT以嵌入难度感知特征。训练好的XGBoost分类器随后为每个问题选择最佳模型，从而优化性能-成本权衡。实验结果表明，与基线方法ComplexityNet相比，AdaptiveLLM在pass@1得分上提高了7.86%，同时资源消耗降低了88.9%。与单一模型相比，AdaptiveLLM在保持相同成本水平的同时，准确性提高了约15%。除此之外，使用CoT进行的难度评估提供了比人工评估更可靠的选择标准。我们的复制包可在https://github.com/cjhCoder7/AdaptiveLLM获取。", "summary": "AdaptiveLLM是一个创新的框架，旨在解决大型语言模型在代码生成中性能与成本平衡的难题。它通过自动分析Chain-of-Thought长度来评估任务难度，并利用k-means聚类、CodeBERT微调和XGBoost分类器动态选择最适合的LLM。实验证明，AdaptiveLLM在显著降低资源消耗的同时，有效提升了代码生成的准确率，并提供了一种比人工评估更可靠的任务难度判断方法。", "keywords": "LLM, 代码生成, 成本效益, Chain-of-Thought, 任务难度评估", "comments": "AdaptiveLLM的创新之处在于其自动化的任务难度评估机制，特别是利用Chain-of-Thought长度作为评估指标，这避免了对人工标注的依赖，并与LLM自身的推理过程相契合。该框架在优化性能-成本权衡方面表现出色，为实际应用中的LLM部署提供了有价值的解决方案，尤其是在资源受限的环境下。其方法结合了深度学习（CodeBERT）和传统机器学习（XGBoost），显示了多模型协同的潜力。"}}
{"id": "2506.10600", "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence", "authors": ["Wang Xinjie", "Liu Liu", "Cao Yu", "Wu Ruiqi", "Qin Wenkang", "Wang Dehui", "Sui Wei", "Su Zhizhong"], "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10600v1", "AI": {"title_translation": "EmbodiedGen：迈向具身智能的生成式3D世界引擎", "tldr": "EmbodiedGen是一个生成式3D世界引擎，旨在低成本地为具身智能任务提供可扩展、高质量、可控且逼真的3D资产和交互式世界。", "motivation": "当前具身智能任务严重依赖手动创建和标注的传统3D计算机图形资产，这些资产生产成本高且真实性有限，严重阻碍了数据驱动方法的可扩展性，而构建物理真实且精确缩放的模拟3D世界对具身智能的训练和评估至关重要。", "method": "EmbodiedGen是一个交互式3D世界生成基础平台。它通过六个关键模块（图像到3D、文本到3D、纹理生成、关节对象生成、场景生成和布局生成）利用生成式AI，以低成本生成具有精确物理属性和真实世界尺度的URDF格式的高质量、可控、逼真的3D资产。这些资产可以直接导入各种物理仿真引擎。", "result": "EmbodiedGen实现了可扩展地生成高质量、可控、逼真的3D资产，这些资产具有精确的物理属性和真实世界尺度，并且可以生成多样化、交互式的3D世界。", "conclusion": "EmbodiedGen通过利用生成式AI解决了具身智能相关研究在泛化和评估方面对多样化、真实、低成本3D数据资产的需求，从而克服了传统3D资产的局限性。", "translation": "构建物理真实且精确缩放的模拟3D世界对于具身智能任务的训练和评估至关重要。3D数据资产的多样性、真实性、低成本可访问性和可负担性对于实现具身AI的泛化和可扩展性至关重要。然而，当前大多数具身智能任务仍然严重依赖手动创建和标注的传统3D计算机图形资产，这些资产存在生产成本高和真实性有限的问题。这些限制严重阻碍了数据驱动方法的可扩展性。我们提出了EmbodiedGen，一个用于交互式3D世界生成的基础平台。它能够以低成本可扩展地生成高质量、可控、逼真的3D资产，这些资产具有精确的物理属性和真实世界尺度，并采用统一机器人描述格式（URDF）。这些资产可以直接导入各种物理仿真引擎进行细粒度物理控制，支持下游的训练和评估任务。EmbodiedGen是一个易于使用、功能齐全的工具包，由六个关键模块组成：图像到3D、文本到3D、纹理生成、关节对象生成、场景生成和布局生成。EmbodiedGen利用生成式AI生成由生成式3D资产组成的多样化、交互式3D世界，以解决具身智能相关研究的泛化和评估挑战。", "summary": "EmbodiedGen是一个为具身智能设计的生成式3D世界引擎，旨在解决传统3D资产生产成本高和真实性有限的问题。它提供一个基础平台，利用生成式AI通过图像到3D、文本到3D等六个模块，低成本地生成高质量、可控、逼真且具有精确物理属性的URDF格式3D资产。这些资产可直接用于各种物理仿真引擎，支持具身智能的训练和评估，从而提升数据驱动方法的可扩展性、泛化能力和真实性。", "keywords": "具身智能, 3D世界生成, 生成式AI, 模拟环境, URDF", "comments": "EmbodiedGen的创新性在于它将生成式AI应用于3D世界和资产的创建，极大地降低了具身智能研究中高质量、多样化3D数据的获取成本和难度。其模块化的设计使其功能全面且易于使用，有望显著推动具身AI的训练和评估的规模化和真实性。"}}
{"id": "2506.10853", "title": "A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models", "authors": ["Yu Zhang", "Yang Hu", "De Wang"], "summary": "Human spatiotemporal behavior simulation is critical for urban planning\nresearch, yet traditional rule-based and statistical approaches suffer from\nhigh computational costs, limited generalizability, and poor scalability. While\nlarge language models (LLMs) show promise as \"world simulators,\" they face\nchallenges in spatiotemporal reasoning including limited spatial cognition,\nlack of physical constraint understanding, and group homogenization tendencies.\nThis paper introduces a framework integrating chain-of-thought (CoT) reasoning\nwith Model Context Protocol (MCP) to enhance LLMs' capability in simulating\nspatiotemporal behaviors that correspond with validation data patterns. The\nmethodology combines human-like progressive reasoning through a five-stage\ncognitive framework with comprehensive data processing via six specialized MCP\ntool categories: temporal management, spatial navigation, environmental\nperception, personal memory, social collaboration, and experience evaluation.\nExperiments in Shanghai's Lujiazui district validate the framework's\neffectiveness across 1,000 generated samples. Results demonstrate high\nsimilarity with real mobile signaling data, achieving generation quality scores\nof 7.86 to 8.36 across different base models. Parallel processing experiments\nshow efficiency improvements, with generation times decreasing from 1.30 to\n0.17 minutes per sample when scaling from 2 to 12 processes. This work\ncontributes to integrating CoT reasoning with MCP for urban behavior modeling,\nadvancing LLMs applications in urban computing and providing a practical\napproach for synthetic mobility data generation. The framework offers a\nfoundation for smart city planning, transportation forecasting, and\nparticipatory urban design applications.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10853v1", "AI": {"title_translation": "基于MCP增强型思维链大语言模型的个体时空活动生成方法研究", "tldr": "本文提出了一种结合思维链（CoT）和模型上下文协议（MCP）的框架，以增强大语言模型在生成与真实数据模式相符的个体时空活动方面的能力，并在上海陆家嘴地区验证了其有效性。", "motivation": "传统的时空行为模拟方法计算成本高、泛化能力和可扩展性差。大语言模型（LLMs）虽有潜力，但在时空推理方面存在空间认知有限、缺乏物理约束理解和群体同质化等挑战。", "method": "本文引入了一个将思维链（CoT）推理与模型上下文协议（MCP）相结合的框架，通过五阶段认知框架实现类人渐进推理，并利用六类专门的MCP工具（时间管理、空间导航、环境感知、个人记忆、社会协作、经验评估）进行数据处理。", "result": "在上海陆家嘴地区进行的实验中，该框架在1,000个生成样本上展现出有效性，与真实移动信令数据高度相似，生成质量分数达到7.86至8.36。并行处理实验显示效率提升，每个样本的生成时间从1.30分钟减少到0.17分钟。", "conclusion": "该工作将思维链推理与模型上下文协议整合用于城市行为建模，推动了大语言模型在城市计算中的应用，为合成出行数据生成提供了一种实用方法，并为智慧城市规划、交通预测和参与式城市设计奠定了基础。", "translation": "人类时空行为模拟对于城市规划研究至关重要，但传统的基于规则和统计的方法存在计算成本高、泛化能力有限和可扩展性差的问题。尽管大语言模型（LLMs）作为“世界模拟器”展现出潜力，但它们在时空推理方面面临挑战，包括空间认知有限、缺乏物理约束理解以及群体同质化倾向。本文引入了一个将思维链（CoT）推理与模型上下文协议（MCP）相结合的框架，以增强LLMs模拟与验证数据模式相符的时空行为的能力。该方法通过五阶段认知框架结合类人渐进推理，并通过六类专门的MCP工具进行全面的数据处理：时间管理、空间导航、环境感知、个人记忆、社会协作和经验评估。在上海陆家嘴地区的实验验证了该框架在1,000个生成样本上的有效性。结果表明，与真实的移动信令数据具有高度相似性，在不同基础模型上实现了7.86到8.36的生成质量分数。并行处理实验显示了效率的提升，当从2个进程扩展到12个进程时，每个样本的生成时间从1.30分钟减少到0.17分钟。这项工作有助于将CoT推理与MCP整合用于城市行为建模，推进LLMs在城市计算中的应用，并为合成出行数据生成提供了一种实用方法。该框架为智慧城市规划、交通预测和参与式城市设计应用奠定了基础。", "summary": "本文提出一种结合思维链（CoT）推理和模型上下文协议（MCP）的新框架，旨在解决传统方法和现有大语言模型在时空行为模拟中的局限性。该框架通过五阶段认知推理和六类MCP工具增强LLMs的空间认知和物理约束理解能力。实验结果表明，该方法能有效生成与真实数据高度相似的个体时空活动，并显著提升了生成效率，为城市计算和合成出行数据生成提供了实用方案，对智慧城市规划等领域具有重要意义。", "keywords": "时空活动生成, 大语言模型, 思维链, 模型上下文协议, 城市计算", "comments": "该研究通过将思维链推理与模型上下文协议结合，有效地解决了大语言模型在时空行为模拟中面临的空间认知和物理约束理解等挑战，其创新点在于为LLMs提供了更精细的上下文管理和认知框架，使其能够生成更符合现实模式的个体时空活动。这对于城市规划、交通预测等领域具有重要的实际应用价值，为合成出行数据生成提供了一条高效且高保真的路径。"}}
{"id": "2506.10866", "title": "Data-Driven Model Reduction by Moment Matching for Linear and Nonlinear Parametric Systems", "authors": ["Hanqing Zhang", "Junyu Mao", "Mohammad Fahim Shakib", "Giordano Scarciotti"], "summary": "Theory and methods to obtain parametric reduced-order models by moment\nmatching are presented. The definition of the parametric moment is introduced,\nand methods (model-based and data-driven) for the approximation of the\nparametric moment of linear and nonlinear parametric systems are proposed.\nThese approximations are exploited to construct families of parametric\nreduced-order models that match the approximate parametric moment of the system\nto be reduced and preserve key system properties such as asymptotic stability\nand dissipativity. The use of the model reduction methods is illustrated by\nmeans of a parametric benchmark model for the linear case and a large-scale\nwind farm model for the nonlinear case. In the illustration, a comparison of\nthe proposed approximation methods is drawn and their advantages/disadvantages\nare discussed.", "comment": "16 pages, 6 figures, submitted to IEEE Transactions on Automatic\n  Control", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10866v1", "AI": {"title_translation": "基于矩匹配的数据驱动模型降阶方法用于线性和非线性参数系统", "tldr": "本文提出了通过矩匹配获取参数降阶模型的理论和方法，包括参数矩的定义以及近似方法（基于模型和数据驱动），并利用这些近似构建了保持系统关键特性的参数降阶模型族。", "motivation": "本文旨在提出通过矩匹配获得参数降阶模型的理论和方法，以有效地处理线性和非线性参数系统，同时保留关键系统特性。", "method": "本文引入了参数矩的定义，并提出了基于模型和数据驱动的方法来近似线性和非线性参数系统的参数矩。这些近似被用于构建参数降阶模型族，这些模型能够匹配系统的近似参数矩并保持渐近稳定性和耗散性等关键系统属性。", "result": "本文展示了通过所提出的方法构建的参数降阶模型族能够匹配近似的参数矩并保留系统关键特性。通过线性参数基准模型和大型风电场非线性模型进行了方法的使用说明，并对提出的近似方法进行了比较和优缺点讨论。", "conclusion": "本文成功提出了通过矩匹配获取参数降阶模型的理论和方法，并证明了其在处理线性和非线性参数系统时的有效性，同时能够保持关键系统属性。", "translation": "本文提出了通过矩匹配获取参数降阶模型的理论和方法。文中介绍了参数矩的定义，并提出了近似线性和非线性参数系统参数矩的方法（基于模型和数据驱动）。这些近似被用于构建参数降阶模型族，这些模型能够匹配待降阶系统的近似参数矩，并保留渐近稳定性和耗散性等关键系统属性。通过一个线性案例的参数基准模型和一个非线性案例的大型风电场模型，说明了模型降阶方法的使用。在说明中，对所提出的近似方法进行了比较，并讨论了它们的优缺点。", "summary": "本文提出了一套通过矩匹配实现线性和非线性参数系统数据驱动模型降阶的理论和方法。研究引入了参数矩的概念，并开发了基于模型和数据驱动的近似方法。这些方法用于构建能匹配系统近似参数矩并保持渐近稳定性和耗散性等关键属性的参数降阶模型。通过线性基准模型和非线性风电场模型验证了方法的有效性，并对不同近似方法的优缺点进行了比较和讨论。", "keywords": "模型降阶, 矩匹配, 参数系统, 数据驱动, 非线性系统", "comments": "本文的创新点在于提出了参数矩的定义以及基于数据驱动的参数矩近似方法，这为复杂参数系统的模型降阶提供了一种新颖且实用的途径。该方法能够有效保留系统关键特性，对于工程应用具有重要意义。"}}
{"id": "2506.10521", "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": ["Yuhao Zhou", "Yiheng Wang", "Xuming He", "Ruoyao Xiao", "Zhiwei Li", "Qiantai Feng", "Zijie Guo", "Yuejin Yang", "Hao Wu", "Wenxuan Huang", "Jiaqi Wei", "Dan Si", "Xiuqi Yao", "Jia Bu", "Haiwen Huang", "Tianfan Fu", "Shixiang Tang", "Ben Fei", "Dongzhan Zhou", "Fenghua Ling", "Yan Lu", "Siqi Sun", "Chenhui Li", "Guanjie Zheng", "Jiancheng Lv", "Wenlong Zhang", "Lei Bai"], "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.", "comment": "82 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10521v1", "AI": {"title_translation": "科学家初考：通过感知、理解和推理探究多模态大语言模型（MLLM）的认知能力", "tldr": "本文提出了SFE基准，用于评估MLLM在科学领域中的感知、理解和推理能力，结果显示现有SOTA模型表现不佳，仍有巨大提升空间。", "motivation": "现有科学基准主要侧重于评估多模态大语言模型（MLLM）的知识理解能力，导致对其感知和推理能力的评估不足。为了弥补这一空白，本文提出了一个新的基准。", "method": "本文提出了“科学家初考”（SFE）基准，旨在通过科学信号感知、科学属性理解和科学比较推理三个相互关联的层面来评估MLLM的科学认知能力。SFE包含830对专家验证的VQA（视觉问答）对，涵盖三种问题类型，涉及五个高价值学科的66个多模态任务。", "result": "广泛的实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的得分分别仅为34.08%和26.52%，这突出表明MLLM在科学领域仍有巨大的改进空间。", "conclusion": "SFE基准揭示了当前MLLM在科学感知、理解和推理能力方面的不足，预示着未来AI增强科学发现的巨大潜力，并希望其提供的见解能促进进一步发展。", "translation": "科学发现越来越依赖于基于信息密集型科学数据和领域专业知识的复杂多模态推理。在专家级科学基准的加持下，科学多模态大语言模型（MLLM）有潜力在现实工作流程中显著增强这一发现过程。然而，当前的科学基准大多侧重于评估MLLM的知识理解能力，导致对其感知和推理能力的评估不足。为了弥补这一空白，我们提出了“科学家初考”（SFE）基准，旨在通过科学信号感知、科学属性理解、科学比较推理三个相互关联的层面来评估MLLM的科学认知能力。具体而言，SFE包含830对专家验证的VQA（视觉问答）对，涵盖三种问题类型，涉及五个高价值学科的66个多模态任务。广泛的实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的得分分别仅为34.08%和26.52%，这突出表明MLLM在科学领域仍有巨大的改进空间。我们希望SFE中获得的见解能促进AI增强科学发现的进一步发展。", "summary": "本文针对现有科学基准未能充分评估多模态大语言模型（MLLM）感知和推理能力的问题，提出了“科学家初考”（SFE）基准。SFE通过科学信号感知、科学属性理解和科学比较推理三个层面，包含830对专家验证的VQA对，涵盖66个多模态任务和五个学科，全面评估MLLM的科学认知能力。实验结果显示，当前最先进的MLLM模型表现不佳，表明其在科学领域仍有显著提升空间，该基准有望推动AI增强科学发现的进一步发展。", "keywords": "MLLM, 科学基准, 认知能力, 感知, 推理", "comments": "该论文的创新之处在于提出了一个专门用于评估多模态大语言模型（MLLM）在科学领域中感知、理解和推理能力的全新基准——SFE，弥补了现有基准主要关注知识理解的不足。其重要性在于为未来AI增强科学发现提供了更全面的评估工具，并明确指出了当前SOTA模型在该领域的局限性，为后续研究指明了方向。通过揭示现有MLLM在复杂科学认知任务上的不足，该工作为推动AI在科学研究中的应用奠定了基础。"}}
{"id": "2506.10453", "title": "Rethinking Generative Human Video Coding with Implicit Motion Transformation", "authors": ["Bolin Chen", "Ru-Ling Liao", "Jie Chen", "Yan Ye"], "summary": "Beyond traditional hybrid-based video codec, generative video codec could\nachieve promising compression performance by evolving high-dimensional signals\ninto compact feature representations for bitstream compactness at the encoder\nside and developing explicit motion fields as intermediate supervision for\nhigh-quality reconstruction at the decoder side. This paradigm has achieved\nsignificant success in face video compression. However, compared to facial\nvideos, human body videos pose greater challenges due to their more complex and\ndiverse motion patterns, i.e., when using explicit motion guidance for\nGenerative Human Video Coding (GHVC), the reconstruction results could suffer\nsevere distortions and inaccurate motion. As such, this paper highlights the\nlimitations of explicit motion-based approaches for human body video\ncompression and investigates the GHVC performance improvement with the aid of\nImplicit Motion Transformation, namely IMT. In particular, we propose to\ncharacterize complex human body signal into compact visual features and\ntransform these features into implicit motion guidance for signal\nreconstruction. Experimental results demonstrate the effectiveness of the\nproposed IMT paradigm, which can facilitate GHVC to achieve high-efficiency\ncompression and high-fidelity synthesis.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10453v1", "AI": {"title_translation": "重新思考基于隐式运动变换的生成式人体视频编码", "tldr": "本文提出了一种名为隐式运动变换（IMT）的新范式，用于生成式人体视频编码（GHVC），以解决显式运动指导在处理复杂人体运动时导致的失真和不准确问题，实现了高效压缩和高质量合成。", "motivation": "传统的生成式视频编解码器在人脸视频压缩方面取得了成功，但对于具有更复杂和多样化运动模式的人体视频，使用显式运动指导会导致严重的失真和不准确的运动重建。因此，本文旨在解决显式运动方法在人体视频压缩中的局限性。", "method": "本文提出了一种名为隐式运动变换（IMT）的新范式。具体来说，该方法将复杂的人体信号特征化为紧凑的视觉特征，并将这些特征转换为隐式运动指导，用于信号重建。", "result": "实验结果表明，所提出的IMT范式是有效的，能够促进生成式人体视频编码（GHVC）实现高效率压缩和高保真度合成。", "conclusion": "通过引入隐式运动变换（IMT），可以显著提升生成式人体视频编码（GHVC）的性能，克服传统显式运动指导在处理复杂人体运动时的局限性，实现高效和高质量的视频压缩。", "translation": "超越传统的混合式视频编解码器，生成式视频编解码器通过将高维信号演变为紧凑的特征表示以实现编码器端的比特流紧凑性，以及开发显式运动场作为解码器端高质量重建的中间监督，从而实现了有前景的压缩性能。这种范式在人脸视频压缩中取得了显著成功。然而，与人脸视频相比，人体视频由于其更复杂多样的运动模式，带来了更大的挑战，即当使用显式运动指导进行生成式人体视频编码（GHVC）时，重建结果可能会遭受严重的失真和不准确的运动。因此，本文强调了基于显式运动方法在人体视频压缩中的局限性，并借助隐式运动变换（IMT）研究了GHVC性能的提升。特别是，我们提出将复杂的人体信号特征化为紧凑的视觉特征，并将这些特征转换为隐式运动指导以进行信号重建。实验结果证明了所提出的IMT范式的有效性，它能够促进GHVC实现高效率压缩和高保真度合成。", "summary": "本文针对生成式人体视频编码（GHVC）中显式运动指导在处理复杂人体运动时导致的重建失真和不准确问题，提出了一种名为隐式运动变换（IMT）的新范式。该方法将复杂人体信号转化为紧凑视觉特征，并将其作为隐式运动指导进行信号重建。实验证明，IMT能有效提升GHVC的压缩效率和合成质量。", "keywords": "生成式视频编码, 人体视频, 隐式运动变换, 视频压缩, 运动估计", "comments": "该论文的创新点在于提出了隐式运动变换（IMT）来解决生成式人体视频编码中显式运动指导的局限性，特别是在处理复杂人体运动时。这对于提高人体视频压缩的效率和质量具有重要意义，可能为未来生成式视频编解码器的发展提供新思路。"}}
{"id": "2506.10138", "title": "Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban", "authors": ["Mohammad Taufeeque", "Aaron David Tucker", "Adam Gleave", "Adrià Garriga-Alonso"], "summary": "We partially reverse-engineer a convolutional recurrent neural network (RNN)\ntrained to play the puzzle game Sokoban with model-free reinforcement learning.\nPrior work found that this network solves more levels with more test-time\ncompute. Our analysis reveals several mechanisms analogous to components of\nclassic bidirectional search. For each square, the RNN represents its plan in\nthe activations of channels associated with specific directions. These\nstate-action activations are analogous to a value function - their magnitudes\ndetermine when to backtrack and which plan branch survives pruning. Specialized\nkernels extend these activations (containing plan and value) forward and\nbackward to create paths, forming a transition model. The algorithm is also\nunlike classical search in some ways. State representation is not unified;\ninstead, the network considers each box separately. Each layer has its own plan\nrepresentation and value function, increasing search depth. Far from being\ninscrutable, the mechanisms leveraging test-time compute learned in this\nnetwork by model-free training can be understood in familiar terms.", "comment": "33 pages, 22 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10138v1", "AI": {"title_translation": "解释学习到的搜索：在玩推箱子游戏的RNN中找到一个转移模型和值函数", "tldr": "本文逆向工程了一个玩推箱子游戏的RNN，发现它利用了类似双向搜索、值函数和转移模型的机制，表明其学习到的行为是可解释的，尽管是无模型训练。", "motivation": "先前的研究发现该网络在测试时计算量增加的情况下能解决更多关卡，但其内部机制尚不清楚。本文的动机是理解这些机制，并解释其学习到的搜索行为。", "method": "部分逆向工程一个通过无模型强化学习训练来玩推箱子游戏的卷积循环神经网络（RNN）。分析其内部机制，特别是它如何表示计划、值函数和转移模型。", "result": "该RNN利用了类似于经典双向搜索的机制；它在与特定方向相关的通道激活中表示其计划；这些状态-动作激活类似于值函数，决定何时回溯和哪个计划分支得以保留；专用核将这些激活（包含计划和值）向前和向后扩展以创建路径，形成一个转移模型。与经典搜索不同的是，状态表示不是统一的，而是分别考虑每个箱子；每个层都有自己的计划表示和值函数，增加了搜索深度。最终，通过无模型训练在该网络中学习到的利用测试时计算的机制可以用熟悉的术语来理解。", "conclusion": "通过无模型训练学习到的、利用测试时计算来玩推箱子的RNN的机制是可解释的，并且类似于经典搜索组件，例如值函数和转移模型，这表明此类复杂行为并非不可理解。", "translation": "我们部分逆向工程了一个通过无模型强化学习训练来玩推箱子益智游戏的卷积循环神经网络（RNN）。之前的工作发现，这个网络在测试时计算量增加的情况下能解决更多关卡。我们的分析揭示了几个类似于经典双向搜索组件的机制。对于每个方格，RNN在与特定方向相关的通道激活中表示其计划。这些状态-动作激活类似于一个值函数——它们的幅度决定了何时回溯以及哪个计划分支在剪枝中幸存。专用核将这些激活（包含计划和值）向前和向后扩展以创建路径，形成一个转移模型。该算法在某些方面也与经典搜索不同。状态表示不是统一的；相反，网络分别考虑每个箱子。每个层都有自己的计划表示和值函数，增加了搜索深度。通过无模型训练在该网络中学习到的利用测试时计算的机制，远非不可理解，而是可以用熟悉的术语来理解。", "summary": "本文对一个通过无模型强化学习训练来玩推箱子游戏的卷积循环神经网络（RNN）进行了部分逆向工程。分析揭示，该网络采用了类似于经典双向搜索的机制，包括作为值函数的状态-动作激活以及形成转移模型的专用核。尽管它在某些方面与经典搜索不同，例如非统一的状态表示和分层的计划表示，但研究表明，网络学习到的复杂行为是可解释的，并且可以用熟悉的术语来理解。", "keywords": "RNN, 推箱子, 强化学习, 搜索解释, 值函数", "comments": "本文的创新之处在于揭示了无模型训练的RNN的内部工作机制，表明复杂的学习行为可以用经典的AI概念（如搜索、值函数和转移模型）来解释。这对于提高深度强化学习模型的信任度和理解至关重要。一个局限性是逆向工程只是“部分”的。"}}
{"id": "2506.10728", "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims", "authors": ["Priyanka Kargupta", "Runchu Tian", "Jiawei Han"], "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be\nclearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with\nscientific and political claims. However, a claim (e.g., \"vaccine A is better\nthan vaccine B\") can be dissected into its integral aspects and sub-aspects\n(e.g., efficacy, safety, distribution), which are individually easier to\nvalidate. This enables a more comprehensive, structured response that provides\na well-rounded perspective on a given problem while also allowing the reader to\nprioritize specific angles of interest within the claim (e.g., safety towards\nchildren). Thus, we propose ClaimSpect, a retrieval-augmented generation-based\nframework for automatically constructing a hierarchy of aspects typically\nconsidered when addressing a claim and enriching them with corpus-specific\nperspectives. This structure hierarchically partitions an input corpus to\nretrieve relevant segments, which assist in discovering new sub-aspects.\nMoreover, these segments enable the discovery of varying perspectives towards\nan aspect of the claim (e.g., support, neutral, or oppose) and their respective\nprevalence (e.g., \"how many biomedical papers believe vaccine A is more\ntransportable than B?\"). We apply ClaimSpect to a wide variety of real-world\nscientific and political claims featured in our constructed dataset, showcasing\nits robustness and accuracy in deconstructing a nuanced claim and representing\nperspectives within a corpus. Through real-world case studies and human\nevaluation, we validate its effectiveness over multiple baselines.", "comment": "Accepted to ACL 2025 Main Conference. Code available at:\n  https://github.com/pkargupta/claimspect", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10728v1", "AI": {"title_translation": "超越真假：检索增强的细微主张层次分析", "tldr": "ClaimSpect是一个检索增强的框架，用于自动构建和丰富对细微主张的层次分析，以提供多角度的结构化回应。", "motivation": "现有方法在处理无法简单标记为“真”或“假”的细微主张（如科学和政治主张）时存在局限性。这些主张需要更全面的、结构化的回应，能够分解成可单独验证的方面和子方面。", "method": "本文提出了ClaimSpect，一个基于检索增强生成（RAG）的框架。它自动构建分析主张时通常考虑的方面层次结构，并用语料库特定的视角丰富它们。该框架通过层次化划分输入语料库来检索相关片段，从而发现新的子方面和针对主张某方面（如支持、中立、反对）的不同观点及其流行程度。", "result": "ClaimSpect被应用于各种现实世界的科学和政治主张数据集，展示了其在解构细微主张和表示语料库中观点方面的鲁棒性和准确性。通过现实世界案例研究和人工评估，验证了其优于多个基线的有效性。", "conclusion": "ClaimSpect提供了一种有效且鲁棒的方法来分析和表示细微主张，超越了简单的真假判断，通过层次化地揭示和量化语料库中的多重观点。", "translation": "个人或实体提出的主张往往是细微的，不能被明确地标记为完全“真”或“假”——科学和政治主张经常如此。然而，一个主张（例如，“疫苗A比疫苗B更好”）可以分解成其组成部分和子部分（例如，功效、安全性、分发），这些部分更容易单独验证。这使得能够提供更全面、结构化的回应，为给定问题提供一个全面的视角，同时允许读者优先关注主张中感兴趣的特定角度（例如，对儿童的安全性）。因此，我们提出了ClaimSpect，这是一个基于检索增强生成（RAG）的框架，用于自动构建在处理主张时通常考虑的方面层次结构，并用语料库特定的视角丰富它们。这种结构层次化地划分输入语料库以检索相关片段，这些片段有助于发现新的子方面。此外，这些片段能够发现针对主张某方面（例如，支持、中立或反对）的不同观点及其各自的流行程度（例如，“有多少生物医学论文认为疫苗A比B更易于运输？”）。我们将ClaimSpect应用于我们构建的数据集中各种现实世界的科学和政治主张，展示了其在解构细微主张和表示语料库中观点方面的鲁棒性和准确性。通过现实世界案例研究和人工评估，我们验证了其优于多个基线的有效性。", "summary": "本文提出了ClaimSpect，一个检索增强生成框架，旨在解决细微主张（如科学和政治主张）难以简单判断真假的问题。ClaimSpect通过自动构建主张的层次化方面结构，并利用语料库检索相关信息和不同视角（支持、中立、反对）及其流行程度来丰富这些方面。该框架在现实世界的科学和政治主张数据集上进行了验证，展示了其在解构细微主张和呈现多角度观点方面的鲁棒性和准确性。", "keywords": "细微主张, 检索增强生成, 层次分析, 观点表示, ClaimSpect", "comments": "ClaimSpect的创新之处在于其超越了传统二元真假判断，引入了层次化分析和多视角呈现的概念，这对于处理复杂、细微的主张（尤其是在科学和政治领域）至关重要。其结合检索增强生成（RAG）的框架，使得系统能够从大量语料中自动发现和组织相关信息及不同观点，大大提升了主张分析的深度和广度。该方法对于提升信息透明度、辅助批判性思维和应对虚假信息具有重要意义。"}}
{"id": "2506.10245", "title": "ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese", "authors": ["Iago Alves Brito", "Julia Soares Dollis", "Fernanda Bufon Färber", "Diogo Fernandes Costa Silva", "Arlindo Rodrigues Galvão Filho"], "summary": "We present ToxSyn-PT, the first large-scale Portuguese corpus that enables\nfine-grained hate-speech classification across nine legally protected minority\ngroups. The dataset contains 53,274 synthetic sentences equally distributed\nbetween minorities groups and toxicity labels. ToxSyn-PT is created through a\nnovel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot\nexpansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and\n(4) enrichment, plus additional neutral texts to curb overfitting to\ngroup-specific cues. The resulting corpus is class-balanced, stylistically\ndiverse, and free from the social-media domain that dominate existing\nPortuguese datasets. Despite domain differences with traditional benchmarks,\nexperiments on both binary and multi-label classification on the corpus yields\nstrong results across five public Portuguese hate-speech datasets,\ndemonstrating robust generalization even across domain boundaries. The dataset\nis publicly released to advance research on synthetic data and hate-speech\ndetection in low-resource settings.", "comment": "8 pages, 5 tables, 1 figure", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10245v1", "AI": {"title_translation": "ToxSyn-PT：一个用于葡萄牙语仇恨言论检测的大规模合成数据集", "tldr": "ToxSyn-PT是首个大规模葡萄牙语合成数据集，包含53,274个句子，用于细粒度仇恨言论分类，通过四阶段管道创建，并在现有基准测试中表现出强大的泛化能力，已公开发布以促进低资源环境下的仇恨言论检测研究。", "motivation": "现有的葡萄牙语仇恨言论数据集主要集中在社交媒体领域，且缺乏大规模、细粒度的分类能力。本研究旨在创建首个大规模葡萄牙语语料库，以支持针对九个受法律保护的少数群体的细粒度仇恨言论分类，特别是在低资源环境下推进合成数据和仇恨言论检测的研究。", "method": "ToxSyn-PT通过一个新颖的四阶段管道创建：(1) 一个紧凑的手动策划种子；(2) 使用指令微调的LLM进行小样本扩展；(3) 基于释义的增强；(4) 富集，并添加额外的中性文本以抑制对特定群体线索的过拟合。该数据集包含53,274个合成句子，在少数群体和毒性标签之间均匀分布，并且是类别平衡、风格多样化且不依赖于社交媒体领域。", "result": "尽管与传统基准测试存在领域差异，但对该语料库进行的二元和多标签分类实验在五个公开的葡萄牙语仇恨言论数据集上均取得了强大的结果，甚至在跨领域边界时也表现出鲁棒的泛化能力。", "conclusion": "ToxSyn-PT数据集已公开发布，旨在推进合成数据和低资源环境下仇恨言论检测的研究。", "translation": "我们推出了ToxSyn-PT，这是首个大规模葡萄牙语语料库，能够对九个受法律保护的少数群体进行细粒度仇恨言论分类。该数据集包含53,274个合成句子，在少数群体和毒性标签之间均匀分布。ToxSyn-PT通过一个新颖的四阶段管道创建：(1) 一个紧凑的手动策划种子；(2) 使用指令微调的LLM进行小样本扩展；(3) 基于释义的增强；(4) 富集，并添加额外的中性文本以抑制对特定群体线索的过拟合。由此产生的语料库是类别平衡的、风格多样化的，并且摆脱了现有葡萄牙语数据集中主导的社交媒体领域。尽管与传统基准测试存在领域差异，但对该语料库进行的二元和多标签分类实验在五个公开的葡萄牙语仇恨言论数据集上均取得了强大的结果，即使跨领域边界也表现出鲁棒的泛化能力。该数据集已公开发布，旨在推进合成数据和低资源环境下仇恨言论检测的研究。", "summary": "本论文介绍了ToxSyn-PT，一个大规模的葡萄牙语合成数据集，专为细粒度仇恨言论分类设计，涵盖九个受保护的少数群体。该数据集包含53,274个合成句子，通过一个四阶段管道生成，确保了类别平衡和风格多样性，并避免了现有数据集常见的社交媒体领域偏向。实验证明，ToxSyn-PT在多个公共葡萄牙语仇恨言论数据集上表现出强大的泛化能力，即使面对领域差异也能保持鲁棒性。该数据集的公开发布旨在促进低资源环境下的合成数据和仇恨言论检测研究。", "keywords": "仇恨言论检测, 合成数据集, 葡萄牙语, 低资源语言, 细粒度分类", "comments": "这篇论文的创新点在于提出了一个新颖的四阶段管道来生成大规模合成数据集，有效地解决了葡萄牙语仇恨言论检测领域资源稀缺的问题。通过合成数据而非依赖于社交媒体语料，该数据集不仅扩大了规模，还提升了领域多样性和风格丰富性，有助于模型在更广泛的场景下进行泛化。其在多个现有基准上的强大表现证明了该方法的有效性，对于低资源语言的自然语言处理研究具有重要意义。数据集的公开发布将极大地推动该领域的发展。"}}
{"id": "2506.10228", "title": "California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops", "authors": ["Hamid Kamangir", "Mona Hajiesmaeeli", "Mason Earles"], "summary": "California is a global leader in agricultural production, contributing 12.5%\nof the United States total output and ranking as the fifth-largest food and\ncotton supplier in the world. Despite the availability of extensive historical\nyield data from the USDA National Agricultural Statistics Service, accurate and\ntimely crop yield forecasting remains a challenge due to the complex interplay\nof environmental, climatic, and soil-related factors. In this study, we\nintroduce a comprehensive crop yield benchmark dataset covering over 70 crops\nacross all California counties from 2008 to 2022. The benchmark integrates\ndiverse data sources, including Landsat satellite imagery, daily climate\nrecords, monthly evapotranspiration, and high-resolution soil properties. To\neffectively learn from these heterogeneous inputs, we develop a multi-modal\ndeep learning model tailored for county-level, crop-specific yield forecasting.\nThe model employs stratified feature extraction and a timeseries encoder to\ncapture spatial and temporal dynamics during the growing season. Static inputs\nsuch as soil characteristics and crop identity inform long-term variability.\nOur approach achieves an overall R2 score of 0.76 across all crops of unseen\ntest dataset, highlighting strong predictive performance across California\ndiverse agricultural regions. This benchmark and modeling framework offer a\nvaluable foundation for advancing agricultural forecasting, climate adaptation,\nand precision farming. The full dataset and codebase are publicly available at\nour GitHub repository.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10228v1", "AI": {"title_translation": "加州作物产量基准：结合卫星图像、气候、蒸散量和土壤数据层进行70多种作物县级产量预测", "tldr": "该研究引入了一个全面的加州作物产量基准数据集，并开发了一个多模态深度学习模型，用于超过70种作物的县级产量预测，实现了0.76的R2分数。", "motivation": "尽管有大量的历史产量数据，但由于环境、气候和土壤相关因素的复杂相互作用，准确及时的作物产量预测仍然是一个挑战。", "method": "研究构建了一个涵盖2008年至2022年加州所有县70多种作物的综合作物产量基准数据集。该数据集整合了Landsat卫星图像、每日气候记录、月度蒸散量和高分辨率土壤特性等多种数据源。同时，开发了一个多模态深度学习模型，该模型采用分层特征提取和时间序列编码器来捕获生长季节的空间和时间动态，并利用土壤特性和作物身份等静态输入信息。", "result": "所提出的方法在未见过的测试数据集上，所有作物的总体R2分数达到0.76，表明在加州多样化的农业区域具有强大的预测性能。", "conclusion": "该基准数据集和建模框架为推进农业预测、气候适应和精准农业提供了宝贵的基础。完整的数据集和代码库已公开可用。", "translation": "加利福尼亚州是全球农业生产的领导者，贡献了美国总产量的12.5%，并位居世界第五大食品和棉花供应国。尽管美国农业部国家农业统计局提供了大量的历史产量数据，但由于环境、气候和土壤相关因素的复杂相互作用，准确及时的作物产量预测仍然是一个挑战。在本研究中，我们引入了一个全面的作物产量基准数据集，涵盖了2008年至2022年加州所有县的70多种作物。该基准整合了多种数据源，包括Landsat卫星图像、每日气候记录、月度蒸散量和高分辨率土壤特性。为了有效地从这些异构输入中学习，我们开发了一个多模态深度学习模型，专门用于县级、特定作物的产量预测。该模型采用分层特征提取和时间序列编码器，以捕获生长季节的空间和时间动态。土壤特性和作物身份等静态输入提供了长期变异性信息。我们的方法在未见过的测试数据集上，所有作物的总体R2分数达到0.76，突出了加州多样化农业区域的强大预测性能。该基准和建模框架为推进农业预测、气候适应和精准农业提供了宝贵的基础。完整的数据集和代码库可在我们的GitHub存储库中公开获取。", "summary": "本研究针对加州作物产量预测的挑战，构建了一个包含卫星图像、气候、蒸散量和土壤数据的综合基准数据集，涵盖2008-2022年间超过70种作物。在此基础上，开发了一个多模态深度学习模型，利用分层特征提取和时间序列编码器处理异构数据，实现了0.76的R2分数，为农业预测提供了新的工具和公开资源。", "keywords": "作物产量预测, 深度学习, 卫星图像, 加州农业, 基准数据集", "comments": "该论文的创新点在于构建了一个大规模、多源集成的作物产量预测基准数据集，并提出了一个针对异构数据优化的多模态深度学习模型。其重要性在于为加州这一全球重要农业区域提供了高精度的县级作物产量预测能力，对农业生产规划、气候适应和精准农业具有实际应用价值。数据集和代码的公开性也促进了相关领域的研究。"}}
{"id": "2506.10574", "title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation", "authors": ["Qing Wang", "Xiaohang Yang", "Yilan Dong", "Naveen Raj Govindaraj", "Gregory Slabaugh", "Shanxin Yuan"], "summary": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.", "comment": "check demos at https://dancechat.github.io/anon/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10574v1", "AI": {"title_translation": "舞动聊天：大语言模型引导的音乐到舞蹈生成", "tldr": "DanceChat利用大语言模型（LLM）作为编舞者，通过文本指令引导音乐到舞蹈的生成，解决了音乐与舞蹈间的语义鸿沟和数据稀缺问题，实现了更具多样性和音乐对齐的舞蹈生成。", "motivation": "音乐到舞蹈生成存在挑战，包括音乐与舞蹈运动之间的语义鸿沟（音乐提供抽象线索而非具体动作）、多对一映射（同一音乐可有多种舞蹈解释，需要额外指导以生成多样性）、以及配对音乐和舞蹈数据稀缺。", "method": "本文提出了DanceChat，一个由大语言模型（LLM）引导的音乐到舞蹈生成方法。LLM作为编舞者提供文本动作指令，提供明确的高级指导。该方法包含三个组件：1) 基于LLM的伪指令生成模块，根据音乐风格和结构生成文本舞蹈指导；2) 多模态特征提取与融合模块，整合音乐、节奏和文本指导到共享表示；3) 基于扩散的运动合成模块，结合多模态对齐损失，确保生成舞蹈与音乐和文本线索对齐。", "result": "在AIST++数据集上进行广泛实验和人类评估表明，DanceChat在定性和定量上均优于现有最先进的方法。", "conclusion": "DanceChat通过引入LLM作为编舞者，有效解决了音乐到舞蹈生成中的语义鸿沟和多样性挑战，显著提升了生成舞蹈的质量和对齐度。", "translation": "音乐到舞蹈生成旨在根据音乐输入合成人类舞蹈动作。尽管最近取得了进展，但由于音乐和舞蹈动作之间的语义鸿沟，仍然存在重大挑战，因为音乐只提供抽象线索，如旋律、律动和情感，而没有明确指定身体动作。此外，一首音乐可以产生多种合理的舞蹈解释。这种一对多的映射需要额外的指导，因为仅凭音乐为生成多样化的舞蹈动作提供的信息有限。配对音乐和舞蹈数据的稀缺进一步加剧了这一挑战，这限制了模型学习多样化舞蹈模式的能力。在本文中，我们引入了DanceChat，一种由大语言模型（LLM）引导的音乐到舞蹈生成方法。我们使用LLM作为编舞者，提供文本动作指令，为舞蹈生成提供明确的高级指导。这种方法超越了仅从音乐中进行隐式学习，使模型能够生成既多样化又与音乐风格更好地对齐的舞蹈。我们的方法包括三个组件：（1）一个基于LLM的伪指令生成模块，根据音乐风格和结构生成文本舞蹈指导；（2）一个多模态特征提取和融合模块，将音乐、节奏和文本指导整合到共享表示中；（3）一个基于扩散的运动合成模块，结合多模态对齐损失，确保生成的舞蹈与音乐和文本线索都对齐。在AIST++上的广泛实验和人类评估表明，DanceChat在定性和定量上都优于现有最先进的方法。", "summary": "本文提出了DanceChat，一种创新的大语言模型（LLM）引导的音乐到舞蹈生成方法。该方法通过将LLM用作“编舞者”，生成文本动作指令，有效弥补了音乐与舞蹈之间的语义鸿沟，并解决了单一音乐对应多种舞蹈解释的挑战。DanceChat包含LLM伪指令生成、多模态特征融合以及基于扩散的运动合成模块，确保生成的舞蹈既多样化又与音乐和文本指导高度对齐。实验证明，DanceChat在性能上超越了现有最先进的方法。", "keywords": "音乐到舞蹈生成, 大语言模型, 文本指导, 舞蹈合成, 多模态", "comments": "该论文的创新点在于首次将大语言模型（LLM）引入音乐到舞蹈生成任务，利用其理解和生成文本指令的能力，作为“编舞者”来弥补音乐与舞蹈之间的语义鸿沟。这提供了一种显式、高级的指导方式，克服了传统方法仅依赖音乐隐式学习的局限性，从而能生成更具多样性和更符合音乐风格的舞蹈。这种方法也部分缓解了配对数据稀缺的问题，为该领域带来了新的视角和解决方案。"}}
{"id": "2506.10763", "title": "Reduced-Order Time Splitting for Navier-Stokes with Open Boundaries", "authors": ["Mejdi Azaïez", "Tomás Chacón Rebollo", "Carlos Núñez Fernández", "Samuele Rubino"], "summary": "In this work, we propose a Proper Orthogonal Decomposition-Reduced Order\nModel (POD-ROM) applied to time-splitting schemes for solving the Navier-Stokes\nequations with open boundary conditions. In this method, we combine three\nstrategies to reduce the computing time to solve NSE: time splitting, reduction\nof the computational domain through non-standard treatment of open boundary\nconditions and reduced order modelling. To make the work self-contained, we\nfirst present the formulation of the time-splitting scheme applied to the\nNavier-Stokes equations with open boundary conditions, employing a first-order\nEuler time discretization and deriving the non-standard boundary condition for\npressure. Then, we construct a Galerkin projection-based ROM using POD with two\ndifferent treatments of the pressure boundary condition on the outlet. We\npropose a comparative performance analysis between the standard\nprojection-based POD-ROM (fully intrusive) and a hybrid POD-ROM that combines a\nprojection-based approach (intrusive) with a data-driven technique\n(non-intrusive) using Radial Basis Functions (RBF). We illustrate this\ncomparison through two different numerical tests: the flow in a bifurcated tube\nand the benchmark numerical test of the flow past cylinder, numerically\ninvestigating the efficiency and accuracy of both ROMs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10763v1", "AI": {"title_translation": "针对开放边界Navier-Stokes方程的降阶时间分裂法", "tldr": "本文提出了一种结合时间分裂、开放边界条件非标准处理和降阶模型（POD-ROM）的方法，旨在显著减少Navier-Stokes方程的求解时间，并通过数值实验比较了全侵入式和混合式POD-ROM的效率和精度。", "motivation": "旨在通过结合时间分裂、计算域缩减（通过非标准开放边界条件处理）和降阶建模（POD-ROM）三种策略，减少Navier-Stokes方程求解的计算时间。", "method": "提出了一种应用于Navier-Stokes方程开放边界条件的时间分裂方案，采用一阶欧拉时间离散化并推导了非标准压力边界条件。构建了基于Galerkin投影的POD-ROM，并对出口压力边界条件进行了两种不同处理。比较了标准的全侵入式投影POD-ROM和结合了基于投影（侵入式）与数据驱动（非侵入式，使用径向基函数RBF）技术的混合POD-ROM的性能。通过分叉管内流动和绕圆柱体流动两个数值算例进行验证。", "result": "论文通过数值算例比较了标准全侵入式POD-ROM与混合POD-ROM在效率和精度方面的表现，并进行了数值调查。", "conclusion": "论文成功地提出了结合时间分裂、开放边界处理和降阶建模的Navier-Stokes方程求解方法，并通过数值实验比较了全侵入式和混合POD-ROM的效率和精度。", "translation": "在这项工作中，我们提出了一种应用于Navier-Stokes方程开放边界条件下时间分裂方案的本征正交分解降阶模型（POD-ROM）。在该方法中，我们结合了三种策略来减少求解Navier-Stokes方程的计算时间：时间分裂、通过开放边界条件的非标准处理来缩小计算域以及降阶建模。为了使工作自洽，我们首先介绍了应用于Navier-Stokes方程开放边界条件的时间分裂方案的公式，采用了首次欧拉时间离散化并推导了压力的非标准边界条件。然后，我们构建了一个基于Galerkin投影的ROM，该ROM使用POD，并在出口处对压力边界条件进行了两种不同的处理。我们提出了一种比较标准基于投影的POD-ROM（完全侵入式）和结合了基于投影方法（侵入式）与数据驱动技术（非侵入式）使用径向基函数（RBF）的混合POD-ROM的性能分析。我们通过两个不同的数值测试来阐明这种比较：分叉管中的流动和绕圆柱体流动的基准数值测试，数值研究了两种ROM的效率和精度。", "summary": "本文提出了一种针对开放边界Navier-Stokes方程的降阶时间分裂方法，旨在显著减少计算时间。该方法融合了时间分裂、非标准开放边界条件处理和本征正交分解降阶模型（POD-ROM）。文章详细阐述了时间分裂方案的公式化，并构建了基于Galerkin投影的POD-ROM。特别地，研究对比了标准的全侵入式POD-ROM与结合了数据驱动技术的混合POD-ROM的性能，并通过分叉管流动和绕圆柱体流动等数值算例验证了这两种模型的效率和精度。", "keywords": "Navier-Stokes方程, 降阶模型, 时间分裂, 开放边界条件, POD-ROM", "comments": "论文通过结合多种现有技术（时间分裂、降阶模型、非标准边界条件）来解决Navier-Stokes方程的计算效率问题，具有一定的创新性。特别是对全侵入式和混合式POD-ROM的比较分析，对于理解和选择合适的降阶模型在实际应用中具有指导意义。"}}
{"id": "2506.10327", "title": "A Comprehensive Survey of Unmanned Aerial Systems' Risks and Mitigation Strategies", "authors": ["Sharad Shrestha", "Mohammed Ababneh", "Satyajayant Misra", "Henry M. Cathey, Jr.", "Roopa Vishwanathan", "Matt Jansen", "Jinhong Choi", "Rakesh Bobba", "Yeongjin Jang"], "summary": "In the last decade, the rapid growth of Unmanned Aircraft Systems (UAS) and\nUnmanned Aircraft Vehicles (UAV) in communication, defense, and transportation\nhas increased. The application of UAS will continue to increase rapidly. This\nhas led researchers to examine security vulnerabilities in various facets of\nUAS infrastructure and UAVs, which form a part of the UAS system to reinforce\nthese critical systems. This survey summarizes the cybersecurity\nvulnerabilities in several phases of UAV deployment, the likelihood of each\nvulnerability's occurrence, the impact of attacks, and mitigation strategies\nthat could be applied. We go beyond the state-of-the-art by taking a\ncomprehensive approach to enhancing UAS security by performing an analysis of\nboth UAS-specific and non-UAS-specific mitigation strategies that are\napplicable within the UAS domain to define the lessons learned. We also present\nrelevant cybersecurity standards and their recommendations in the UAS context.\nDespite the significant literature in UAS security and the relevance of\ncyberphysical and networked systems security approaches from the past, which we\nidentify in the survey, we find several critical research gaps that require\nfurther investigation. These form part of our discussions and recommendations\nfor the future exploration by our research community.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10327v1", "AI": {"title_translation": "无人机系统风险与缓解策略综合调查", "tldr": "该调查总结了无人机系统在部署各阶段的网络安全漏洞、发生可能性、攻击影响以及可应用的缓解策略，并提出了未来的研究方向。", "motivation": "过去十年中，无人机系统（UAS）在通信、国防和交通领域的快速增长，以及未来应用将持续增加的趋势，促使研究人员关注无人机系统基础设施和无人机（UAV）的安全漏洞，以加强这些关键系统。", "method": "本研究通过对无人机系统部署各个阶段的网络安全漏洞、漏洞发生的可能性、攻击影响以及可应用的缓解策略进行总结，并对无人机系统特有和非特有的缓解策略进行了综合分析。此外，还介绍了相关的网络安全标准及其在无人机系统背景下的建议。", "result": "该调查总结了无人机系统部署各阶段的网络安全漏洞、发生可能性、攻击影响和缓解策略。它超越现有技术水平，全面分析了适用于无人机领域的特有和非特有缓解策略，并提出了经验教训。研究还介绍了相关的网络安全标准及其建议。尽管现有文献丰富，但仍发现了一些需要进一步调查的关键研究空白。", "conclusion": "该调查发现，尽管无人机系统安全领域文献丰富，且过去的网络物理和网络系统安全方法具有相关性，但仍存在几个需要进一步研究的关键空白。这些空白构成了未来研究社区探索的讨论和建议的一部分。", "translation": "在过去十年中，无人机系统（UAS）和无人机（UAV）在通信、国防和交通领域的快速增长。无人机系统的应用将继续迅速增加。这促使研究人员检查无人机系统基础设施和无人机（作为无人机系统一部分）各个方面的安全漏洞，以加强这些关键系统。本调查总结了无人机部署几个阶段的网络安全漏洞、每个漏洞发生的可能性、攻击的影响以及可以应用的缓解策略。我们通过对无人机系统特有和非无人机系统特有的缓解策略进行分析，全面提升无人机系统安全性，超越了现有技术水平，从而定义了经验教训。我们还提出了无人机系统背景下的相关网络安全标准及其建议。尽管无人机系统安全领域有大量文献，并且我们在此次调查中识别出过去网络物理和网络系统安全方法的关联性，但我们发现了一些需要进一步调查的关键研究空白。这些构成了我们研究社区未来探索的讨论和建议的一部分。", "summary": "本调查旨在全面分析无人机系统（UAS）面临的网络安全风险和可行的缓解策略。文章总结了无人机部署各阶段的网络安全漏洞、其发生可能性、攻击影响，并提出了相应的缓解措施。研究不仅涵盖了无人机系统特有的策略，也探讨了非特有但适用于无人机领域的缓解方法，并提炼了经验教训。此外，调查还介绍了无人机系统相关的网络安全标准和建议，并指出了当前研究中存在的关键空白，为未来的研究方向提供了指导。", "keywords": "无人机系统, 网络安全, 风险缓解, 漏洞, 调查", "comments": "这是一项重要的调查研究，它系统地梳理了无人机系统领域的网络安全风险与缓解策略，并超越了现有技术水平，对特有和非特有策略进行了全面分析。其创新之处在于识别并明确指出了未来的研究空白，为该领域的发展提供了清晰的方向。这对于无人机系统的安全保障和可持续发展具有重要意义。"}}
{"id": "2506.10686", "title": "An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists", "authors": ["Andreas Mueller"], "summary": "Optimal control in general, and flatness-based control in particular, of\nrobotic arms necessitate to compute the first and second time derivatives of\nthe joint torques/forces required to achieve a desired motion. In view of the\nrequired computational efficiency, recursive $O(n)$-algorithms were proposed to\nthis end. Aiming at compact yet efficient formulations, a Lie group formulation\nwas recently proposed, making use of body-fixed and hybrid representation of\ntwists and wrenches. In this paper a formulation is introduced using the\nspatial representation. The second-order inverse dynamics algorithm is\naccompanied by a fourth-order forward and inverse kinematics algorithm. An\nadvantage of all Lie group formulations is that they can be parameterized in\nterms of vectorial quantities that are readily available. The method is\ndemonstrated for the 7 DOF Franka Emika Panda robot.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10686v1", "AI": {"title_translation": "一种使用空间旋量表示的串联机械臂高阶运动学和逆动力学O(n)算法", "tldr": "该论文提出了一种使用空间旋量表示的串联机械臂高阶运动学和逆动力学O(n)算法，以支持机器人臂的优化控制。", "motivation": "机器人臂的优化控制（特别是基于平坦度的控制）需要计算实现期望运动所需的关节扭矩/力的第一和第二时间导数。为了满足计算效率的需求，之前提出了递归的O(n)算法和基于李群的紧凑高效公式（使用固连和混合表示的旋量和力偶）。本文旨在引入一种使用空间表示的公式。", "method": "本文引入了一种使用空间表示的旋量的公式。该方法包括一个二阶逆动力学算法，以及一个四阶正向和逆运动学算法。该方法利用了所有李群公式的优点，即它们可以用易于获得的矢量量进行参数化。该方法在7自由度Franka Emika Panda机器人上进行了演示。", "result": "本文提出了一种用于高阶运动学和逆动力学的O(n)算法。具体来说，开发了一个二阶逆动力学算法和一个四阶正向和逆运动学算法。该方法在7自由度机器人上得到了验证。", "conclusion": "本文成功引入了一种使用空间表示的O(n)算法，用于串联机械臂的高阶运动学和逆动力学计算。李群公式的优势在于它们可以由易于获得的矢量量进行参数化，这对于机器人控制应用至关重要。", "translation": "机器人臂的优化控制，特别是基于平坦度的控制，需要计算实现期望运动所需的关节扭矩/力的第一和第二时间导数。为了满足所需的计算效率，为此提出了递归的O(n)算法。为了实现紧凑而高效的公式，最近提出了一种李群公式，利用固连和混合表示的旋量和力偶。本文引入了一种使用空间表示的公式。二阶逆动力学算法伴随着一个四阶正向和逆运动学算法。所有李群公式的一个优点是它们可以用易于获得的矢量量进行参数化。该方法在7自由度Franka Emika Panda机器人上进行了演示。", "summary": "本文提出了一种用于串联机械臂高阶运动学和逆动力学的O(n)算法。在最近的李群公式基础上，该研究引入了一种使用空间旋量表示的新方法。所提出的方法包含一个二阶逆动力学算法和一个四阶正向和逆运动学算法，这对于需要关节扭矩导数的优化控制应用至关重要。论文强调了李群公式的优势在于其可以由易于获得的矢量量进行参数化，并在7自由度Franka Emika Panda机器人上进行了演示。", "keywords": "运动学, 逆动力学, 串联机械臂, O(n)算法, 空间表示", "comments": "该论文通过提供计算高效的机器人高阶运动学和逆动力学算法，为机器人控制领域做出了贡献。在李群框架内使用空间表示提供了一种紧凑且可能更直观的公式，与之前的固连或混合表示相比。O(n)复杂度对于实时应用至关重要，提升了算法的实用性。"}}
{"id": "2506.10034", "title": "Impacts between multibody systems and deformable structures", "authors": ["Lipinski Krzysztof"], "summary": "Collisions and impacts are the principal reasons for impulsive motions, which\nwe frequently see in dynamic responses of systems. Precise modelling of impacts\nis a challenging problem due to the lack of the accurate and commonly accepted\nconstitutive law that governs their mechanics. Rigid-body approach and soft\ncontact methods are discussed in this paper and examined in the presented\nnumerical examples. The main focus is set to impacts in systems with multiple\nunilateral contacts and collisions with elastic elements of the reference.\nParameters of interconnecting unilateral springs are under discussion.", "comment": "20 pages, 11 figures, submitted to Virtual Conference Proceeding of\n  12th ECCOMAS Thematic Conference on Multibody Dynamics - Innsbruck July\n  13-18, 2025 and to the journal of Multibody System Dynamics", "cate": "physics.class-ph", "url": "http://arxiv.org/abs/2506.10034v1", "AI": {"title_translation": "多体系统与可变形结构之间的冲击", "tldr": "本文讨论了多体系统与可变形结构之间冲击的精确建模问题，并探讨了刚体方法和软接触方法，重点关注多边接触和弹性碰撞。", "motivation": "冲击的精确建模是一个挑战性问题，因为缺乏准确和普遍接受的本构律来描述其力学行为。", "method": "论文讨论并检验了刚体方法和软接触方法，并通过数值例子进行验证。主要关注点是具有多个单边接触的系统中的冲击以及与参考系弹性元件的碰撞，并讨论了互连单边弹簧的参数。", "result": "论文讨论并检验了刚体方法和软接触方法，并在数值例子中进行了验证。互连单边弹簧的参数正在讨论中。具体的量化结果未在摘要中提及。", "conclusion": "本文讨论了多体系统与可变形结构之间冲击的精确建模问题，并探讨了刚体方法和软接触方法在处理多边单边接触和弹性元件碰撞方面的应用。", "translation": "碰撞和冲击是系统动态响应中常见脉冲运动的主要原因。由于缺乏准确且普遍接受的本构律来描述其力学行为，冲击的精确建模是一个具有挑战性的问题。本文讨论了刚体方法和软接触方法，并在所提供的数值示例中进行了检验。主要关注点是具有多个单边接触的系统中的冲击以及与参考系弹性元件的碰撞。互连单边弹簧的参数正在讨论中。", "summary": "本文旨在解决多体系统与可变形结构之间冲击的精确建模难题，指出其主要挑战在于缺乏统一的本构律。文章讨论并检验了刚体方法和软接触方法，并通过数值例子进行验证。研究重点在于处理具有多个单边接触的系统中的冲击以及与弹性元件的碰撞，并对互连单边弹簧的参数进行了深入探讨。", "keywords": "冲击, 多体系统, 可变形结构, 刚体方法, 软接触方法", "comments": "这篇论文探讨了冲击建模这一复杂且重要的工程动力学问题。其重要性在于冲击在许多系统动态响应中扮演关键角色。论文的贡献可能在于对刚体和软接触方法在处理多体系统与可变形结构之间复杂冲击（特别是多边单边接触和弹性碰撞）方面的比较和应用。局限性可能在于摘要中未明确指出具体的创新点或量化结果。"}}
{"id": "2506.10527", "title": "LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs", "authors": ["Yanan Cai", "Ahmed Salem", "Besmira Nushi", "Mark Russinovich"], "summary": "We introduce LogiPlan, a novel benchmark designed to evaluate the\ncapabilities of large language models (LLMs) in logical planning and reasoning\nover complex relational structures. Logical relational reasoning is important\nfor applications that may rely on LLMs to generate and query structured graphs\nof relations such as network infrastructure, knowledge bases, or business\nprocess schema. Our framework allows for dynamic variation of task complexity\nby controlling the number of objects, relations, and the minimum depth of\nrelational chains, providing a fine-grained assessment of model performance\nacross difficulty levels. LogiPlan encompasses three complementary tasks: (1)\nPlan Generation, where models must construct valid directed relational graphs\nmeeting specified structural constraints; (2) Consistency Detection, testing\nmodels' ability to identify inconsistencies in relational structures; and (3)\nComparison Question, evaluating models' capacity to determine the validity of\nqueried relationships within a given graph. Additionally, we assess models'\nself-correction capabilities by prompting them to verify and refine their\ninitial solutions. We evaluate state-of-the-art models including DeepSeek R1,\nGemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,\nO3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant\nperformance gaps that correlate with model scale and architecture. Our analysis\ndemonstrates that while recent reasoning-enhanced models show promising results\non simpler instances, they struggle with more complex configurations requiring\ndeeper logical planning.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10527v1", "AI": {"title_translation": "LogiPlan：一个用于大型语言模型逻辑规划和关系推理的结构化基准", "tldr": "LogiPlan是一个评估大型语言模型逻辑规划和关系推理能力的新基准。它包含生成、一致性检测和比较任务，并揭示了当前LLM在复杂逻辑规划方面存在显著性能差距。", "motivation": "逻辑关系推理对于依赖大型语言模型（LLMs）生成和查询结构化关系图（如网络基础设施、知识库或业务流程模式）的应用至关重要，因此需要一个专门的基准来评估LLM在这些方面的能力。", "method": "本文引入了LogiPlan基准，通过控制对象数量、关系和关系链的最小深度来动态调整任务复杂性。LogiPlan包含三个互补任务：计划生成（构建有效的有向关系图）、一致性检测（识别关系结构中的不一致性）和比较问题（评估查询关系的有效性）。此外，还通过提示模型验证和完善其初始解决方案来评估其自我纠正能力。该研究评估了包括DeepSeek R1、Gemini 2.0 Pro、Gemini 2 Flash Thinking、GPT-4.5、GPT-4o、Llama 3.1 405B、O3-mini、O1和Claude 3.7 Sonnet在内的先进模型。", "result": "评估结果显示，模型性能存在显著差距，且与模型规模和架构相关。尽管近期增强推理能力的模型在简单实例上表现出有希望的结果，但它们在需要更深层次逻辑规划的复杂配置上表现不佳。", "conclusion": "当前最先进的大型语言模型虽然在简单任务上表现出潜力，但在需要深度逻辑规划的复杂场景中仍面临显著挑战，这表明在该领域仍有很大的改进空间。", "translation": "我们引入了LogiPlan，一个旨在评估大型语言模型（LLMs）在逻辑规划和复杂关系结构推理方面能力的新型基准。逻辑关系推理对于可能依赖LLMs生成和查询结构化关系图（如网络基础设施、知识库或业务流程模式）的应用至关重要。我们的框架允许通过控制对象数量、关系和关系链的最小深度来动态改变任务复杂性，从而对模型在不同难度级别下的性能进行细致评估。LogiPlan包含三个互补任务：(1) 计划生成，模型必须构建满足指定结构约束的有效有向关系图；(2) 一致性检测，测试模型识别关系结构中不一致性的能力；以及(3) 比较问题，评估模型确定给定图中查询关系有效性的能力。此外，我们通过提示模型验证和完善其初始解决方案来评估模型的自我纠正能力。我们评估了包括DeepSeek R1、Gemini 2.0 Pro、Gemini 2 Flash Thinking、GPT-4.5、GPT-4o、Llama 3.1 405B、O3-mini、O1和Claude 3.7 Sonnet在内的先进模型在这些任务上的表现，揭示了与模型规模和架构相关的显著性能差距。我们的分析表明，尽管近期增强推理能力的模型在简单实例上表现出有希望的结果，但它们在需要更深层次逻辑规划的复杂配置上表现不佳。", "summary": "LogiPlan是一个新的基准，旨在评估大型语言模型（LLMs）在逻辑规划和复杂关系推理方面的能力。该基准通过动态调整任务复杂度，涵盖计划生成、一致性检测和比较问题三个核心任务，并评估了模型的自我纠正能力。对包括Gemini和GPT系列在内的SOTA模型进行的评估显示，模型性能存在显著差距，且与模型规模和架构相关。研究发现，尽管现有模型在简单任务上表现良好，但在需要深度逻辑规划的复杂场景中仍面临挑战。", "keywords": "逻辑规划, 关系推理, 大型语言模型, 基准, 结构化图", "comments": "LogiPlan的创新之处在于其结构化的设计和动态调整复杂度的能力，这为评估LLM的逻辑规划和关系推理能力提供了一个精细的工具。该基准的重要性在于它揭示了当前LLM在处理复杂逻辑任务时的局限性，为未来研究指明了方向。论文强调了模型规模和架构对性能的影响，并指出即使是先进模型也难以应对深度逻辑规划，这表明LLM在真正掌握复杂推理方面仍有很长的路要走。"}}
{"id": "2506.10459", "title": "Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance", "authors": ["Chun Liu", "Bingqian Zhu", "Tao Xu", "Zheng Zheng", "Zheng Li", "Wei Yang", "Zhigang Han", "Jiayao Wang"], "summary": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose\nsecurity challenges to hyperspectral image (HSI) classification technologies\nbased on DNNs. In the domain of natural images, numerous transfer-based\nadversarial attack methods have been studied. However, HSIs differ from natural\nimages due to their high-dimensional and rich spectral information. Current\nresearch on HSI adversarial examples remains limited and faces challenges in\nfully utilizing the structural and feature information of images. To address\nthese issues, this paper proposes a novel method to enhance the transferability\nof the adversarial examples for HSI classification models. First, while keeping\nthe image structure unchanged, the proposed method randomly divides the image\ninto blocks in both spatial and spectral dimensions. Then, various\ntransformations are applied on a block by block basis to increase input\ndiversity and mitigate overfitting. Second, a feature distancing loss targeting\nintermediate layers is designed, which measures the distance between the\namplified features of the original examples and the features of the adversarial\nexamples as the primary loss, while the output layer prediction serves as the\nauxiliary loss. This guides the perturbation to disrupt the features of the\ntrue class in adversarial examples, effectively enhancing transferability.\nExtensive experiments demonstrate that the adversarial examples generated by\nthe proposed method achieve effective transferability to black-box models on\ntwo public HSI datasets. Furthermore, the method maintains robust attack\nperformance even under defense strategies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10459v1", "AI": {"title_translation": "提升高光谱图像分类对抗样本可迁移性：利用三维结构不变变换和中间特征距离", "tldr": "本文提出一种针对高光谱图像分类的对抗攻击方法，通过三维结构不变变换增加输入多样性，并利用中间特征距离损失增强对抗样本的可迁移性，对黑盒模型攻击有效且在防御下保持鲁棒。", "motivation": "深度神经网络（DNN）容易受到对抗攻击，对基于DNN的高光谱图像（HSI）分类技术构成安全挑战。自然图像领域的许多迁移攻击方法不适用于高光谱图像，因为高光谱图像具有高维丰富的光谱信息。当前针对HSI对抗样本的研究有限，并且在充分利用图像结构和特征信息方面面临挑战。", "method": "该方法通过以下两部分增强高光谱图像分类模型对抗样本的可迁移性：1. 三维结构不变变换：在保持图像结构不变的情况下，将图像在空间和光谱维度上随机分块，然后对每个块应用各种变换，以增加输入多样性并减轻过拟合。2. 中间特征距离：设计了一种针对中间层的特征距离损失，将原始样本的放大特征与对抗样本的特征之间的距离作为主要损失，同时将输出层预测作为辅助损失。这引导扰动破坏对抗样本中真实类别的特征，有效增强可迁移性。", "result": "实验表明，该方法生成的对抗样本在两个公共高光谱图像数据集上对黑盒模型实现了有效的可迁移性。此外，即使在防御策略下，该方法也能保持鲁棒的攻击性能。", "conclusion": "该论文提出了一种新的方法，通过结合三维结构不变变换和中间特征距离损失，有效提升了高光谱图像分类模型对抗样本的可迁移性及其在防御策略下的鲁棒攻击性能。", "translation": "深度神经网络（DNN）容易受到对抗攻击，这对基于DNN的高光谱图像（HSI）分类技术构成了安全挑战。在自然图像领域，已经有大量基于迁移的对抗攻击方法被研究。然而，高光谱图像由于其高维和丰富的光谱信息，与自然图像不同。当前关于高光谱图像对抗样本的研究仍然有限，并且在充分利用图像结构和特征信息方面面临挑战。为了解决这些问题，本文提出了一种新颖的方法，以增强高光谱图像分类模型对抗样本的可迁移性。首先，在保持图像结构不变的同时，所提出的方法在空间和光谱维度上将图像随机分块。然后，对每个块应用各种变换，以增加输入多样性并减轻过拟合。其次，设计了一种针对中间层的特征距离损失，将原始样本的放大特征与对抗样本的特征之间的距离作为主要损失，而输出层预测作为辅助损失。这引导扰动破坏对抗样本中真实类别的特征，有效增强可迁移性。大量的实验表明，所提出的方法生成的对抗样本在两个公共高光谱图像数据集上对黑盒模型实现了有效的可迁移性。此外，即使在防御策略下，该方法也能保持鲁棒的攻击性能。", "summary": "本论文提出了一种新的方法来增强高光谱图像（HSI）分类模型对抗样本的可迁移性，以应对DNN在HSI分类中的安全挑战。该方法通过引入三维结构不变变换，在空间和光谱维度上对图像进行分块变换以增加输入多样性，并设计了中间特征距离损失来引导扰动破坏真实类别特征。实验证明，该方法生成的对抗样本对黑盒模型具有有效的可迁移性，并在防御策略下仍保持鲁棒性能。", "keywords": "对抗攻击, 高光谱图像分类, 可迁移性, 结构不变变换, 中间特征距离", "comments": "创新点在于结合了三维结构不变变换和中间特征距离损失来解决高光谱图像对抗样本的迁移性问题。该方法考虑了高光谱图像的特殊性（高维光谱信息），通过分块变换增加了扰动的多样性，并通过特征距离损失有效引导了攻击，提升了对黑盒模型的攻击效果和在防御下的鲁棒性。"}}
{"id": "2506.10140", "title": "Survival Analysis as Imprecise Classification with Trainable Kernels", "authors": ["Andrei V. Konstantinov", "Vlada A. Efremenko", "Lev V. Utkin"], "summary": "Survival analysis is a fundamental tool for modeling time-to-event data in\nhealthcare, engineering, and finance, where censored observations pose\nsignificant challenges. While traditional methods like the Beran estimator\noffer nonparametric solutions, they often struggle with the complex data\nstructures and heavy censoring. This paper introduces three novel survival\nmodels, iSurvM (the imprecise Survival model based on Mean likelihood\nfunctions), iSurvQ (the imprecise Survival model based on the Quantiles of\nlikelihood functions), and iSurvJ (the imprecise Survival model based on the\nJoint learning), that combine imprecise probability theory with attention\nmechanisms to handle censored data without parametric assumptions. The first\nidea behind the models is to represent censored observations by interval-valued\nprobability distributions for each instance over time intervals between events\nmoments. The second idea is to employ the kernel-based Nadaraya-Watson\nregression with trainable attention weights for computing the imprecise\nprobability distribution over time intervals for the entire dataset. The third\nidea is to consider three decision strategies for training, which correspond to\nthe proposed three models. Experiments on synthetic and real datasets\ndemonstrate that the proposed models, especially iSurvJ, consistently\noutperform the Beran estimator from the accuracy and computational complexity\npoints of view. Codes implementing the proposed models are publicly available.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10140v1", "AI": {"title_translation": "生存分析作为可训练核的模糊分类", "tldr": "本文提出了三种新的生存模型（iSurvM, iSurvQ, iSurvJ），它们结合了模糊概率理论和注意力机制来处理删失数据，并在准确性和计算复杂度上优于传统方法。", "motivation": "生存分析在处理删失观测时面临挑战，传统方法如Beran估计器在处理复杂数据结构和严重删失时表现不佳。", "method": "本文引入了iSurvM、iSurvQ和iSurvJ三种新型生存模型，它们将模糊概率理论与注意力机制相结合，以无参数假设的方式处理删失数据。核心思想包括：将删失观测表示为时间间隔上的区间值概率分布；采用带有可训练注意力权重的基于核的Nadaraya-Watson回归来计算整个数据集在时间间隔上的模糊概率分布；以及为训练考虑三种决策策略。", "result": "在合成数据集和真实数据集上的实验表明，所提出的模型，特别是iSurvJ，在准确性和计算复杂性方面始终优于Beran估计器。", "conclusion": "所提出的iSurvM、iSurvQ和iSurvJ模型，特别是iSurvJ，能够有效且高效地处理生存分析中的删失数据，并取得了比传统方法更好的性能。", "translation": "生存分析是医疗保健、工程和金融领域中建模事件发生时间数据的基本工具，其中删失观测带来了重大挑战。虽然像Beran估计器这样的传统方法提供了非参数解决方案，但它们通常难以处理复杂的数据结构和严重的删失。本文介绍了三种新颖的生存模型：iSurvM（基于均值似然函数的模糊生存模型）、iSurvQ（基于似然函数分位数的模糊生存模型）和iSurvJ（基于联合学习的模糊生存模型），它们将模糊概率理论与注意力机制相结合，以在没有参数假设的情况下处理删失数据。这些模型背后的第一个想法是，将每个实例在事件发生时刻之间的时间间隔上的删失观测表示为区间值概率分布。第二个想法是，采用带有可训练注意力权重的基于核的Nadaraya-Watson回归来计算整个数据集在时间间隔上的模糊概率分布。第三个想法是，考虑三种训练决策策略，这与所提出的三种模型相对应。在合成数据集和真实数据集上的实验表明，所提出的模型，特别是iSurvJ，从准确性和计算复杂性的角度来看，始终优于Beran估计器。实现所提出模型的代码已公开可用。", "summary": "本文提出iSurvM、iSurvQ和iSurvJ三种新型生存分析模型，它们融合了模糊概率理论与注意力机制，旨在克服传统方法在处理复杂和重删失时间-事件数据时的局限性。这些模型通过将删失观测表示为区间值概率分布，并利用带可训练权重的核回归来计算模糊概率分布。实验证明，特别是iSurvJ模型，在准确性和计算效率上均优于传统的Beran估计器，为生存分析提供了有效且无参数的解决方案。", "keywords": "生存分析, 模糊概率, 注意力机制, 删失数据, 核回归", "comments": "本文的创新点在于将模糊概率理论与注意力机制引入生存分析，以无参数方式处理删失数据，这为传统生存分析方法面临的挑战提供了新的视角。通过将删失观测建模为区间值概率分布并结合可训练的核回归，该方法能够更灵活地适应复杂数据结构。其重要性体现在实验结果表明提出的模型，尤其是iSurvJ，在准确性和计算效率上均优于现有方法，具有实际应用价值。公开代码也促进了研究的复现和进一步发展。"}}
{"id": "2506.10737", "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora", "authors": ["Priyanka Kargupta", "Nan Zhang", "Yunyi Zhang", "Rui Zhang", "Prasenjit Mitra", "Jiawei Han"], "summary": "The rapid evolution of scientific fields introduces challenges in organizing\nand retrieving scientific literature. While expert-curated taxonomies have\ntraditionally addressed this need, the process is time-consuming and expensive.\nFurthermore, recent automatic taxonomy construction methods either (1)\nover-rely on a specific corpus, sacrificing generalizability, or (2) depend\nheavily on the general knowledge of large language models (LLMs) contained\nwithin their pre-training datasets, often overlooking the dynamic nature of\nevolving scientific domains. Additionally, these approaches fail to account for\nthe multi-faceted nature of scientific literature, where a single research\npaper may contribute to multiple dimensions (e.g., methodology, new tasks,\nevaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a\nframework that dynamically adapts an LLM-generated taxonomy to a given corpus\nacross multiple dimensions. TaxoAdapt performs iterative hierarchical\nclassification, expanding both the taxonomy width and depth based on corpus'\ntopical distribution. We demonstrate its state-of-the-art performance across a\ndiverse set of computer science conferences over the years to showcase its\nability to structure and capture the evolution of scientific fields. As a\nmultidimensional method, TaxoAdapt generates taxonomies that are 26.51% more\ngranularity-preserving and 50.41% more coherent than the most competitive\nbaselines judged by LLMs.", "comment": "Accepted to ACL 2025 Main Conference. Code available at:\n  https://github.com/pkargupta/taxoadapt", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10737v1", "AI": {"title_translation": "TaxoAdapt：将基于LLM的多维分类法构建与演进中的研究语料库对齐", "tldr": "TaxoAdapt是一个框架，能动态调整LLM生成的分类法，使其适应不断演变的多维科学语料库，并表现出最先进的性能。", "motivation": "现有方法在组织和检索科学文献方面存在挑战，专家构建耗时昂贵；自动构建方法过度依赖特定语料或LLM的预训练知识，忽视领域动态性；且未能考虑科学文献的多维度性。", "method": "提出TaxoAdapt框架，动态地将LLM生成的分类法适应到给定语料库的多维结构中。它通过迭代分层分类，根据语料库的主题分布扩展分类法的宽度和深度。", "result": "在多年计算机科学会议数据集上展现了最先进的性能，能够构建和捕捉科学领域的发展。作为多维方法，TaxoAdapt生成的分类法在粒度保留方面比最具竞争力的基线高26.51%，在连贯性方面高50.41%（由LLM判断）。", "conclusion": "TaxoAdapt成功解决了现有分类法构建方法的不足，特别是针对不断演变的科学领域和文献的多维度性，提供了更精确和连贯的分类结果。", "translation": "科学领域的快速发展给组织和检索科学文献带来了挑战。尽管专家策划的分类法传统上满足了这一需求，但其过程耗时且昂贵。此外，最近的自动分类法构建方法要么 (1) 过度依赖特定语料库，牺牲了通用性，要么 (2) 严重依赖大型语言模型 (LLM) 预训练数据中包含的通用知识，常常忽视不断演变的科学领域的动态性质。此外，这些方法未能考虑到科学文献的多面性，即一篇研究论文可能涉及多个维度（例如，方法论、新任务、评估指标、基准）。为了解决这些空白，我们提出了 TaxoAdapt，一个能够动态地将 LLM 生成的分类法适应到给定语料库的多维度的框架。TaxoAdapt 执行迭代分层分类，根据语料库的主题分布扩展分类法的宽度和深度。我们通过在多年来各种计算机科学会议数据集上的最先进性能来展示其能力，以展示其构建和捕捉科学领域演变的能力。作为一种多维方法，TaxoAdapt 生成的分类法在粒度保留方面比最具竞争力的基线高 26.51%，在连贯性方面高 50.41%（由 LLM 判断）。", "summary": "TaxoAdapt是一个创新框架，旨在解决科学文献组织和检索中的挑战，特别是针对快速发展的多维科学领域。它通过动态调整基于LLM生成的分类法，使其适应特定语料库，并利用迭代分层分类扩展分类法的宽度和深度。实验证明，TaxoAdapt在计算机科学领域表现出最先进的性能，其生成的多维分类法在粒度保留和连贯性方面显著优于现有基线。", "keywords": "分类法构建, 大型语言模型, 科学文献, 多维度, 动态适应", "comments": "TaxoAdapt的创新之处在于其动态适应LLM生成分类法以匹配特定语料库的能力，并解决了现有方法在通用性、动态性和多维度性方面的不足。通过迭代分层分类，它能有效捕捉科学领域的演变。其在粒度保留和连贯性方面的显著提升表明了其在实际应用中的重要潜力，尤其是在快速迭代的科研领域。"}}
{"id": "2506.10268", "title": "Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models", "authors": ["Andrea Yaoyun Cui", "Pengfei Yu"], "summary": "Language models are essentially probability distributions over token\nsequences. Auto-regressive models generate sentences by iteratively computing\nand sampling from the distribution of the next token. This iterative sampling\nintroduces stochasticity, leading to the assumption that language models make\nprobabilistic decisions, similar to sampling from unknown distributions.\nBuilding on this assumption, prior research has used simulated Gibbs sampling,\ninspired by experiments designed to elicit human priors, to infer the priors of\nlanguage models. In this paper, we revisit a critical question: Do language\nmodels possess Bayesian brains? Our findings show that under certain\nconditions, language models can exhibit near-deterministic decision-making,\nsuch as producing maximum likelihood estimations, even with a non-zero sampling\ntemperature. This challenges the sampling assumption and undermines previous\nmethods for eliciting human-like priors. Furthermore, we demonstrate that\nwithout proper scrutiny, a system with deterministic behavior undergoing\nsimulated Gibbs sampling can converge to a \"false prior.\" To address this, we\npropose a straightforward approach to distinguish between stochastic and\ndeterministic decision patterns in Gibbs sampling, helping to prevent the\ninference of misleading language model priors. We experiment on a variety of\nlarge language models to identify their decision patterns under various\ncircumstances. Our results provide key insights in understanding decision\nmaking of large language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10268v1", "AI": {"title_translation": "语言模型拥有贝叶斯大脑吗？区分大型语言模型中的随机性和确定性决策模式", "tldr": "本文挑战了语言模型总是进行概率性决策的假设，发现它们在特定条件下可以表现出接近确定性的行为，并提出了区分随机性和确定性决策模式的方法，以避免推断出错误的先验。", "motivation": "先前的研究假设语言模型通过迭代采样进行概率性决策，并使用模拟吉布斯采样来推断其先验。本文旨在重新审视一个关键问题：语言模型是否拥有贝叶斯大脑？特别是，它挑战了语言模型总是进行概率性决策的假设，并指出现有方法可能推断出错误的先验。", "method": "作者重新审视了语言模型决策模式的问题。他们通过实验证明，在特定条件下，即使采样温度非零，语言模型也能表现出接近确定性的决策行为（如最大似然估计）。他们进一步展示了确定性系统在模拟吉布斯采样下可能收敛到“错误先验”。为解决此问题，他们提出了一种直接的方法来区分吉布斯采样中的随机性和确定性决策模式。他们还在多种大型语言模型上进行了实验，以识别它们在各种情况下的决策模式。", "result": "研究发现，在特定条件下，语言模型可以表现出接近确定性的决策行为，例如产生最大似然估计，即使采样温度非零。这挑战了采样假设，并削弱了先前用于推断类似人类先验的方法。此外，研究表明，在缺乏适当审查的情况下，具有确定性行为的系统在模拟吉布斯采样下会收敛到“错误先验”。", "conclusion": "语言模型并非总是进行概率性决策，在特定条件下它们可以表现出接近确定性的行为。这挑战了现有关于语言模型决策的假设和用于推断其先验的方法。区分随机性和确定性决策模式对于准确理解大型语言模型的决策过程至关重要。", "translation": "语言模型本质上是令牌序列上的概率分布。自回归模型通过迭代计算并从下一个令牌的分布中采样来生成句子。这种迭代采样引入了随机性，导致人们假设语言模型做出概率性决策，类似于从未知分布中采样。基于这一假设，先前的研究利用模拟吉布斯采样（受旨在引发人类先验的实验启发）来推断语言模型的先验。在本文中，我们重新审视了一个关键问题：语言模型是否拥有贝叶斯大脑？我们的研究结果表明，在某些条件下，即使采样温度非零，语言模型也能表现出接近确定性的决策行为，例如产生最大似然估计。这挑战了采样假设，并削弱了先前用于推断类似人类先验的方法。此外，我们证明，在缺乏适当审查的情况下，具有确定性行为的系统在模拟吉布斯采样下会收敛到“错误先验”。为了解决这个问题，我们提出了一种直接的方法来区分吉布斯采样中的随机性和确定性决策模式，有助于防止推断出误导性的语言模型先验。我们对各种大型语言模型进行了实验，以识别它们在各种情况下的决策模式。我们的结果为理解大型语言模型的决策提供了关键见解。", "summary": "本文探讨了大型语言模型（LLMs）的决策模式，质疑了它们总是进行概率性采样的普遍假设。研究发现，在特定条件下，LLMs即使在非零采样温度下也能表现出接近确定性的决策行为，例如最大似然估计。这挑战了先前基于采样假设推断LLM先验的方法，并指出确定性行为可能导致“错误先验”的推断。为解决此问题，作者提出了一种区分吉布斯采样中随机性和确定性决策模式的简单方法，并对多种LLMs进行了实验，以提供关于其决策过程的关键见解。", "keywords": "语言模型, 贝叶斯大脑, 随机性, 确定性, 吉布斯采样, 先验", "comments": "本文创新性地挑战了语言模型决策行为的传统观点，即它们总是进行概率性采样。通过揭示语言模型在特定条件下可能表现出确定性行为，该研究对当前推断语言模型先验的方法提出了重要质疑。提出的区分随机性和确定性决策模式的方法对于避免误导性结论具有实际意义，对深入理解大型语言模型的内部运作机制及其认知特性具有重要价值。"}}
{"id": "2506.10242", "title": "DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hong Cai", "Fatih Porikli"], "summary": "Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most\nimportant perception tasks in autonomous driving. Earlier methods rely on dense\nBEV features, which are costly to construct. More recent works explore sparse\nquery-based detection. However, they still require a large number of queries\nand can become expensive to run when more video frames are used. In this paper,\nwe propose DySS, a novel method that employs state-space learning and dynamic\nqueries. More specifically, DySS leverages a state-space model (SSM) to\nsequentially process the sampled features over time steps. In order to\nencourage the model to better capture the underlying motion and correspondence\ninformation, we introduce auxiliary tasks of future prediction and masked\nreconstruction to better train the SSM. The state of the SSM then provides an\ninformative yet efficient summarization of the scene. Based on the state-space\nlearned features, we dynamically update the queries via merge, remove, and\nsplit operations, which help maintain a useful, lean set of detection queries\nthroughout the network. Our proposed DySS achieves both superior detection\nperformance and efficient inference. Specifically, on the nuScenes test split,\nDySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the\nart. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a\nreal-time inference speed of 33 FPS.", "comment": "CVPR 2025 Workshop on Autonomous Driving", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10242v1", "AI": {"title_translation": "DySS：用于多摄像头视频高效3D目标检测的动态查询与状态空间学习", "tldr": "DySS提出了一种结合状态空间学习和动态查询的新方法，实现了多摄像头视频中高效且高性能的3D目标检测。", "motivation": "现有的基于BEV的相机3D目标检测方法要么依赖于昂贵的密集BEV特征，要么虽然探索了稀疏查询，但仍需要大量查询，并且在处理更多视频帧时成本高昂。", "method": "本文提出了DySS，一种采用状态空间学习和动态查询的新方法。DySS利用状态空间模型（SSM）顺序处理随时间采样的特征，并引入未来预测和掩码重建辅助任务来训练SSM，以更好地捕获运动和对应信息。SSM的状态提供了场景的信息丰富且高效的总结。基于状态空间学习到的特征，DySS通过合并、移除和分割操作动态更新查询，以在整个网络中保持有用且精简的检测查询集。", "result": "在nuScenes测试集上，DySS实现了65.31 NDS和57.4 mAP，优于最新的SOTA方法。在验证集上，DySS实现了56.2 NDS和46.2 mAP，并达到33 FPS的实时推理速度。", "conclusion": "DySS在多摄像头视频3D目标检测中实现了卓越的检测性能和高效的推理速度。", "translation": "基于摄像头的鸟瞰图（BEV）3D目标检测是自动驾驶中最重要的感知任务之一。早期方法依赖于密集的BEV特征，其构建成本高昂。近期工作探索了基于稀疏查询的检测。然而，它们仍然需要大量的查询，并且在使用更多视频帧时运行成本会变得很高。在本文中，我们提出了DySS，一种采用状态空间学习和动态查询的新方法。更具体地说，DySS利用状态空间模型（SSM）随时间步长顺序处理采样的特征。为了鼓励模型更好地捕获底层的运动和对应信息，我们引入了未来预测和掩码重建的辅助任务来更好地训练SSM。SSM的状态随后提供了场景的信息丰富且高效的总结。基于状态空间学习到的特征，我们通过合并、移除和分割操作动态更新查询，这有助于在整个网络中保持一组有用且精简的检测查询。我们提出的DySS实现了卓越的检测性能和高效的推理。具体来说，在nuScenes测试集上，DySS实现了65.31 NDS和57.4 mAP，超越了最新的最先进水平。在验证集上，DySS实现了56.2 NDS和46.2 mAP，以及33 FPS的实时推理速度。", "summary": "DySS是一种针对多摄像头视频3D目标检测的新方法，旨在解决现有方法在效率上的不足。它创新性地结合了状态空间模型（SSM）来有效总结时序特征，并通过动态查询机制（合并、移除、分割）来维持高效且精简的检测查询集。通过引入辅助任务训练SSM以捕获运动信息，DySS在nuScenes数据集上展示了领先的检测性能（65.31 NDS，57.4 mAP）和实时推理速度（33 FPS），超越了现有技术水平。", "keywords": "3D目标检测, 多摄像头, 状态空间学习, 动态查询, 自动驾驶", "comments": "DySS的创新点在于将状态空间学习引入到3D目标检测中，以有效处理时序信息并对场景进行高效总结。同时，动态查询机制的引入进一步优化了检测效率，使得模型能够适应多摄像头视频流的复杂性。该方法在性能和推理速度上的显著提升，对于自动驾驶等实时性要求高的应用具有重要意义。"}}
{"id": "2506.10820", "title": "A Combined Parallel-in-time Direct Inverse (ParaDIn)-Parareal Method for Nonlinear Differential Equations", "authors": ["Subhash Paudel", "Nail K. Yamaleev"], "summary": "As has been shown in our previous work, the parallel-in-time direct inverse\n(ParaDIn) method introduced by Yamaleev and Paudel in (arXiv: 2406.00878v1,\n2024) imposes some constraint on the maximum number of time levels, $N_t$, that\ncan be integrated in parallel. To circumvent this problem and further increase\nthe speedup, we combine the ParaDIn method with the Parareal algorithm to\nefficiently parallelize the first-order time derivative term in nonlinear\npartial differential equations discretized by the method of lines. The main\nidea of the proposed approach is to use a block-Jacobi preconditioner, so that\neach block is solved by using the ParaDIn method. To accelerate the convergence\nof Jacobi iterations, we use the Parareal method which can be interpreted as a\ntwo-level multigrid method in time. In contrast to the conventional Parareal\nalgorithm whose coarse grid correction step is performed sequentially, both the\ncoarse- and fine-grid propagators in the proposed approach are implemented in\nparallel by using the ParaDIn method, thus significantly increasing the\nparallel performance of the combined algorithm. Numerical results show that the\nnew combined ParaDIn-Parareal method provides the speedup of up to 124 on 480\ncomputing cores as compared with the sequential first-order implicit backward\ndifference (BDF1) scheme for the 2-D nonlinear heat and Burgers equations with\nboth smooth and discontinuous solutions.", "comment": "24 pages. arXiv admin note: text overlap with arXiv:2406.00878", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10820v1", "AI": {"title_translation": "一种用于非线性微分方程的并行时间直接逆（ParaDIn）-Parareal组合方法", "tldr": "为了克服ParaDIn方法在并行时间层数上的限制并进一步提高加速比，本文提出了一种将ParaDIn与Parareal算法结合的新方法。该方法通过ParaDIn并行化Parareal的粗细网格传播器，在非线性偏微分方程的求解中实现了显著的并行性能提升，最高加速比达124倍。", "motivation": "ParaDIn方法在并行时间层数（$N_t$）上存在最大限制，阻碍了其进一步的加速能力。", "method": "本文提出了一种结合ParaDIn和Parareal算法的方法。核心思想是使用一个块-雅可比预处理器，其中每个块都通过ParaDIn方法求解。为了加速雅可比迭代的收敛，使用Parareal方法（可解释为时间上的两级多重网格方法）。与传统Parareal不同，新方法中粗网格和细网格的传播器都通过ParaDIn方法并行实现。", "result": "数值结果表明，与顺序一阶隐式后向差分（BDF1）方案相比，新的ParaDIn-Parareal组合方法在480个计算核心上，对于二维非线性热方程和Burgers方程（包括光滑和不连续解），提供了高达124倍的加速。", "conclusion": "结合ParaDIn和Parareal算法能够有效克服ParaDIn的时间层数限制，并通过并行化Parareal的粗细网格传播器，显著提升非线性微分方程并行求解的性能。", "translation": "正如我们之前的工作所示，由Yamaleev和Paudel在（arXiv: 2406.00878v1, 2024）中引入的并行时间直接逆（ParaDIn）方法对可以并行积分的最大时间层数$N_t$施加了一些限制。为了规避这个问题并进一步提高加速比，我们将ParaDIn方法与Parareal算法结合起来，以有效地并行化通过线法离散的非线性偏微分方程中的一阶时间导数项。所提出方法的主要思想是使用块-雅可比预处理器，以便每个块都通过ParaDIn方法求解。为了加速雅可比迭代的收敛，我们使用Parareal方法，该方法可以解释为时间上的两级多重网格方法。与传统的Parareal算法（其粗网格校正步骤是顺序执行的）不同，所提出方法中的粗网格和细网格传播器都通过ParaDIn方法并行实现，从而显著提高了组合算法的并行性能。数值结果表明，与顺序一阶隐式后向差分（BDF1）方案相比，新的ParaDIn-Parareal组合方法在480个计算核心上，对于二维非线性热方程和Burgers方程（包括光滑和不连续解），提供了高达124倍的加速。", "summary": "本文提出了一种结合并行时间直接逆（ParaDIn）和Parareal算法的新方法，旨在克服ParaDIn方法在并行时间层数上的限制并提高加速比。该方法采用块-雅可比预处理器，并利用ParaDIn求解每个块；同时，通过将Parareal解释为时间上的两级多重网格方法来加速收敛。与传统Parareal不同，本方法将粗细网格传播器均并行化。数值结果显示，新方法在480个核心上，对于2D非线性热方程和Burgers方程，相较于顺序BDF1方案，实现了高达124倍的加速。", "keywords": "并行时间积分, ParaDIn, Parareal, 非线性微分方程, 加速比", "comments": "该论文的创新点在于巧妙地结合了ParaDIn和Parareal两种并行时间积分方法，有效地解决了ParaDIn在时间层数上的限制。尤其值得注意的是，通过ParaDIn并行化Parareal的粗细网格传播器，显著提升了传统Parareal算法的并行度，这对于大规模非线性微分方程的并行求解具有重要意义。"}}
{"id": "2506.10338", "title": "Adaptive Chosen-Ciphertext Security of Distributed Broadcast Encryption", "authors": ["Kwangsu Lee"], "summary": "Distributed broadcast encryption (DBE) is a specific kind of broadcast\nencryption (BE) where users independently generate their own public and private\nkeys, and a sender can efficiently create a ciphertext for a subset of users by\nusing the public keys of the subset users. Previously proposed DBE schemes have\nbeen proven in the adaptive chosen-plaintext attack (CPA) security model and\nhave the disadvantage of requiring linear number of pairing operations when\nverifying the public key of a user. In this paper, we propose an efficient DBE\nscheme in bilinear groups and prove adaptive chosen-ciphertext attack (CCA)\nsecurity for the first time. To do this, we first propose a semi-static CCA\nsecure DBE scheme and prove the security under the $q$-Type assumption. Then,\nby modifying the generic transformation of Gentry and Waters that converts a\nsemi-static CPA secure DBE scheme into an adaptive CPA secure DBE scheme to be\napplied to CCA secure DBE schemes, we propose an adaptive CCA secure DBE scheme\nand prove its adaptive CCA security. Our proposed DBE scheme is efficient\nbecause it requires constant size ciphertexts, constant size private keys, and\nlinear size public keys, and the public key verification requires only a\nconstant number of pairing operations and efficient group membership checks.", "comment": "arXiv admin note: text overlap with arXiv:2505.17527", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10338v1", "AI": {"title_translation": "分布式广播加密的自适应选择密文安全性", "tldr": "本文首次提出了一种高效的分布式广播加密方案，实现了自适应选择密文攻击（CCA）安全性。", "motivation": "以前的分布式广播加密（DBE）方案仅在自适应选择明文攻击（CPA）安全模型中被证明，并且在验证用户公钥时需要线性数量的配对操作，效率较低。", "method": "本文首先提出了一个半静态CCA安全的DBE方案，并在q-Type假设下证明了其安全性。然后，通过修改Gentry和Waters的通用转换（该转换将半静态CPA安全DBE方案转换为自适应CPA安全DBE方案），使其适用于CCA安全的DBE方案，从而提出了一个自适应CCA安全的DBE方案并证明了其自适应CCA安全性。", "result": "所提出的DBE方案是高效的，因为它需要常量大小的密文、常量大小的私钥和线性大小的公钥，并且公钥验证仅需要常量数量的配对操作和高效的组成员检查。", "conclusion": "本文首次成功提出并证明了一个高效的、具有自适应选择密文攻击（CCA）安全性的分布式广播加密（DBE）方案。", "translation": "分布式广播加密（DBE）是一种特定类型的广播加密（BE），其中用户独立生成自己的公钥和私钥，发送方可以通过使用子集用户的公钥高效地为用户子集创建密文。以前提出的DBE方案已在自适应选择明文攻击（CPA）安全模型中被证明，并且在验证用户公钥时具有需要线性数量配对操作的缺点。在本文中，我们首次在双线性群中提出了一种高效的DBE方案，并证明了其自适应选择密文攻击（CCA）安全性。为此，我们首先提出了一个半静态CCA安全的DBE方案，并在q-Type假设下证明了其安全性。然后，通过修改Gentry和Waters的通用转换（该转换将半静态CPA安全DBE方案转换为自适应CPA安全DBE方案），使其应用于CCA安全的DBE方案，我们提出了一个自适应CCA安全的DBE方案并证明了其自适应CCA安全性。我们提出的DBE方案是高效的，因为它需要常量大小的密文、常量大小的私钥和线性大小的公钥，并且公钥验证仅需要常量数量的配对操作和高效的组成员检查。", "summary": "本文针对现有分布式广播加密（DBE）方案在安全性（仅CPA安全）和效率（公钥验证需要线性配对操作）上的不足，首次提出了一种在双线性群中实现自适应选择密文攻击（CCA）安全的高效DBE方案。该方案首先构建了一个半静态CCA安全的DBE方案，随后通过改进现有通用转换方法，将其扩展为自适应CCA安全。新方案在密文、私钥和公钥大小方面高效，且公钥验证仅需常量级配对操作，显著提升了实用性。", "keywords": "分布式广播加密, 自适应选择密文安全, 双线性群, q-Type假设", "comments": "本文的创新点在于首次实现了分布式广播加密方案的自适应选择密文攻击（CCA）安全性，这是密码学领域安全性证明的一个重要进展。同时，它解决了现有方案在公钥验证效率上的痛点，通过将线性配对操作降至常量级别，显著提升了方案的实用性。这是一个理论和实践价值兼具的工作。"}}
{"id": "2506.10654", "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub", "authors": ["Abir Bouraffa", "Carolin Brandt", "Andy Zaidmann", "Walid Maalej"], "summary": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10654v1", "AI": {"title_translation": "并非一统天下：从GitHub挖掘有意义的代码审查顺序", "tldr": "研究发现GitHub代码审查中，近一半的开发者不按字母顺序审查文件，而是采用多种有意义的复杂策略，如大改动优先、与标题相关性优先或测试优先，这表明需要更好的工具支持。", "motivation": "现有的代码审查工具（如GitHub）通常按字母顺序展示文件，但这与审查者的实际导航顺序不符。本研究旨在调查开发者在审查代码时所遵循的不同导航顺序。", "method": "研究从GitHub上100个流行的Java和Python仓库的23,241个拉取请求中挖掘了代码审查评论，分析了审查者对提交更改进行评论的顺序。", "result": "44.6%的拉取请求中，审查者以非字母顺序进行评论。在这些非字母顺序评论中，识别出多种有意义的替代顺序：20.6%遵循“最大差异优先”顺序，17.6%遵循文件与拉取请求标题和描述的相似性顺序，29%（包含生产和测试文件的拉取请求）遵循“测试优先”顺序。非字母顺序审查中，已审查文件占总提交文件的比例显著更高。非字母顺序审查平均获得的批准略少。", "conclusion": "研究结果强调了代码审查中需要额外的支持，特别是对于大型拉取请求，因为审查者更可能采用复杂的策略，而非遵循单一预定义顺序。", "translation": "开发者使用GitHub拉取请求等工具进行代码审查、讨论提议的更改并请求修改。虽然更改的文件通常按字母顺序呈现，但这不一定与审查者偏好的导航顺序一致。本研究调查了开发者在评论拉取请求中提交的更改时所遵循的不同导航顺序。我们从GitHub上100个流行的Java和Python仓库的23,241个拉取请求中挖掘了代码审查评论，以分析审查者对提交更改进行评论的顺序。我们的分析显示，对于44.6%的拉取请求，审查者以非字母顺序进行评论。在这些拉取请求中，我们发现了替代的有意义的顺序：20.6%（2,134个）遵循“最大差异优先”顺序，17.6%（1,827个）按照文件与拉取请求标题和描述的相似性顺序进行评论，以及29%（1,188个）包含生产和测试文件更改的拉取请求遵循“测试优先”顺序。我们还观察到，在非字母顺序审查中，已审查文件占总提交文件的比例显著更高，这些审查平均获得的批准也略少。我们的发现强调了代码审查中需要额外的支持，特别是对于大型拉取请求，因为审查者更可能采用复杂的策略，而非遵循单一预定义顺序。", "summary": "本文通过分析GitHub上23,241个拉取请求的代码审查评论，发现近一半的审查者不按字母顺序审查文件，而是采用“最大差异优先”、“与标题相似度优先”或“测试优先”等多种有意义的导航策略。研究指出，非字母顺序审查的覆盖率更高但批准率略低。这些发现表明，现有工具在代码审查中存在不足，尤其是在处理大型拉取请求时，需要提供更多支持以适应审查者复杂的审查习惯。", "keywords": "代码审查, GitHub, 导航顺序, 拉取请求, 开发者行为", "comments": "这项研究通过大规模数据分析揭示了代码审查中普遍存在的非线性、非字母顺序的审查行为，挑战了当前工具默认的文件呈现方式。其创新之处在于识别并量化了多种有意义的审查策略，如“最大差异优先”和“测试优先”，这对于理解开发者行为模式具有重要意义。研究结果强调了改进代码审查工具的必要性，以更好地支持审查者的实际工作流，从而可能提高审查效率和质量。"}}
{"id": "2506.10756", "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding", "authors": ["Yuhang Zhang", "Haosheng Yu", "Jiaping Xiao", "Mir Feroskhan"], "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in\nautonomous robotics, aiming to empower agents with the ability to follow human\ninstructions while navigating complex environments. Two key bottlenecks remain\nin this field: generalization to out-of-distribution environments and reliance\non fixed discrete action spaces. To address these challenges, we propose\nVision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles\n(UAVs) to execute language-guided flight. Without the requirement for\nlocalization or active ranging sensors, VLFly outputs continuous velocity\ncommands purely from egocentric observations captured by an onboard monocular\ncamera. The VLFly integrates three modules: an instruction encoder based on a\nlarge language model (LLM) that reformulates high-level language into\nstructured prompts, a goal retriever powered by a vision-language model (VLM)\nthat matches these prompts to goal images via vision-language similarity, and a\nwaypoint planner that generates executable trajectories for real-time UAV\ncontrol. VLFly is evaluated across diverse simulation environments without\nadditional fine-tuning and consistently outperforms all baselines. Moreover,\nreal-world VLN tasks in indoor and outdoor environments under direct and\nindirect instructions demonstrate that VLFly achieves robust open-vocabulary\ngoal understanding and generalized navigation capabilities, even in the\npresence of abstract language input.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10756v1", "AI": {"title_translation": "基于开放词汇目标理解的无人机接地视觉-语言导航", "tldr": "VLFly是一个为无人机设计的视觉-语言导航框架，仅通过单目摄像头输入即可理解语言指令并输出连续速度命令，在模拟和真实世界环境中均表现出色。", "motivation": "视觉-语言导航（VLN）领域存在两个关键瓶颈：对分布外环境的泛化能力不足以及对固定离散动作空间的依赖。", "method": "本文提出了VLFly框架，专为无人机执行语言引导飞行。它无需定位或主动测距传感器，纯粹通过机载单目摄像头捕获的自我中心观测输出连续速度命令。VLFly集成了三个模块：基于大型语言模型（LLM）的指令编码器、由视觉-语言模型（VLM）驱动的目标检索器以及一个航点规划器。", "result": "VLFly在没有额外微调的情况下，在各种模拟环境中始终优于所有基线。在室内和室外环境中的真实世界VLN任务表明，VLFly实现了鲁棒的开放词汇目标理解和泛化导航能力，即使在存在抽象语言输入的情况下也是如此。", "conclusion": "VLFly框架为无人机实现了鲁棒的开放词汇目标理解和泛化导航能力，有效解决了视觉-语言导航领域的关键瓶颈。", "translation": "视觉-语言导航（VLN）是自主机器人领域的一个长期挑战，旨在赋予智能体在复杂环境中遵循人类指令进行导航的能力。该领域仍存在两个关键瓶颈：对分布外环境的泛化能力以及对固定离散动作空间的依赖。为了解决这些挑战，我们提出了Vision-Language Fly（VLFly），一个专为无人机（UAV）执行语言引导飞行的框架。VLFly无需定位或主动测距传感器，仅凭机载单目摄像头捕获的自我中心观测，即可输出连续的速度指令。VLFly集成了三个模块：一个基于大型语言模型（LLM）的指令编码器，将高级语言重构为结构化提示；一个由视觉-语言模型（VLM）驱动的目标检索器，通过视觉-语言相似性将这些提示与目标图像匹配；以及一个生成可执行轨迹用于实时无人机控制的航点规划器。VLFly在没有额外微调的情况下，在各种模拟环境中进行了评估，并始终优于所有基线。此外，在室内和室外环境中的直接和间接指令下的真实世界VLN任务表明，VLFly即使在存在抽象语言输入的情况下，也能实现鲁棒的开放词汇目标理解和泛化导航能力。", "summary": "本文提出了VLFly框架，旨在解决视觉-语言导航（VLN）中无人机泛化能力和固定动作空间的问题。VLFly仅通过机载单目摄像头输入，输出连续速度指令，包含LLM指令编码器、VLM目标检索器和航点规划器。实验表明，VLFly在模拟和真实世界环境中均表现出色，实现了鲁棒的开放词汇目标理解和泛化导航能力。", "keywords": "视觉-语言导航, 无人机, 开放词汇, 连续控制, LLM-VLM", "comments": "VLFly的创新之处在于其无需定位或主动测距传感器，仅通过单目相机实现连续速度控制，并整合了LLM和VLM以实现开放词汇理解和泛化导航。这对于无人机在复杂环境中的自主操作具有重要意义，尤其是在没有GPS或预设地图的场景下。"}}
{"id": "2506.10813", "title": "Unsupervised Deformable Image Registration with Structural Nonparametric Smoothing", "authors": ["Hang Zhang", "Xiang Chen", "Renjiu Hu", "Rongguang Wang", "Jinwei Zhang", "Min Liu", "Yaonan Wang", "Gaolei Li", "Xinxing Cheng", "Jinming Duan"], "summary": "Learning-based deformable image registration (DIR) accelerates alignment by\namortizing traditional optimization via neural networks. Label supervision\nfurther enhances accuracy, enabling efficient and precise nonlinear alignment\nof unseen scans. However, images with sparse features amid large smooth\nregions, such as retinal vessels, introduce aperture and large-displacement\nchallenges that unsupervised DIR methods struggle to address. This limitation\noccurs because neural networks predict deformation fields in a single forward\npass, leaving fields unconstrained post-training and shifting the\nregularization burden entirely to network weights. To address these issues, we\nintroduce SmoothProper, a plug-and-play neural module enforcing smoothness and\npromoting message passing within the network's forward pass. By integrating a\nduality-based optimization layer with tailored interaction terms, SmoothProper\nefficiently propagates flow signals across spatial locations, enforces\nsmoothness, and preserves structural consistency. It is model-agnostic,\nseamlessly integrates into existing registration frameworks with minimal\nparameter overhead, and eliminates regularizer hyperparameter tuning.\nPreliminary results on a retinal vessel dataset exhibiting aperture and\nlarge-displacement challenges demonstrate our method reduces registration error\nto 1.88 pixels on 2912x2912 images, marking the first unsupervised DIR approach\nto effectively address both challenges. The source code will be available at\nhttps://github.com/tinymilky/SmoothProper.", "comment": "Accepted for publication at Information Processing in Medical Imaging\n  (IPMI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10813v1", "AI": {"title_translation": "无监督可变形图像配准与结构非参数平滑", "tldr": "本文提出了SmoothProper模块，用于无监督可变形图像配准，通过在网络前向传播中强制平滑和信息传递，解决了稀疏特征和大位移挑战，尤其在视网膜血管图像上表现出色。", "motivation": "现有的无监督可变形图像配准（DIR）方法难以处理具有稀疏特征和大位移的图像（如视网膜血管），因为神经网络在训练后预测的形变场缺乏约束，导致正则化负担完全落在网络权重上。", "method": "本文引入了SmoothProper，一个即插即用的神经网络模块，它在网络的前向传播中强制执行平滑并促进消息传递。通过集成一个基于对偶优化的层和定制的交互项，SmoothProper有效地在空间位置上传播流信号，强制执行平滑并保持结构一致性。该模块是模型无关的，可以无缝集成到现有配准框架中，参数开销极小，并消除了正则化超参数调优。", "result": "在具有孔径和大位移挑战的视网膜血管数据集上，该方法将2912x2912图像上的配准误差降低到1.88像素，是第一个有效解决这两个挑战的无监督可变形图像配准方法。", "conclusion": "SmoothProper模块成功解决了无监督可变形图像配准在处理稀疏特征和大位移图像时的挑战，通过在网络前向传播中引入结构平滑和信息传递，显著提高了配准精度。", "translation": "基于学习的可变形图像配准（DIR）通过神经网络分摊传统优化，从而加速对齐。标签监督进一步提高了准确性，实现了对未知扫描的高效精确非线性对齐。然而，具有稀疏特征和大平滑区域的图像，例如视网膜血管，引入了孔径和大位移挑战，这是无监督DIR方法难以解决的。这种限制的发生是因为神经网络在单次前向传播中预测形变场，导致训练后的场不受约束，并将正则化负担完全转移到网络权重上。为了解决这些问题，我们引入了SmoothProper，一个即插即用的神经模块，它在网络的前向传播中强制执行平滑并促进消息传递。通过集成一个基于对偶优化的层和定制的交互项，SmoothProper有效地在空间位置上传播流信号，强制执行平滑并保持结构一致性。它是模型无关的，可以无缝集成到现有配准框架中，参数开销极小，并消除了正则化超参数调优。在展示孔径和大位移挑战的视网膜血管数据集上的初步结果表明，我们的方法将2912x2912图像上的配准误差降低到1.88像素，标志着第一个有效解决这两个挑战的无监督DIR方法。源代码将可在https://github.com/tinymilky/SmoothProper获取。", "summary": "本文提出了一种名为SmoothProper的即插即用神经网络模块，用于解决无监督可变形图像配准（DIR）在处理具有稀疏特征和大位移的图像时遇到的挑战。该模块通过在网络前向传播中引入基于对偶优化的层，强制执行结构平滑并促进信息传递，从而有效约束形变场，减轻了网络权重上的正则化负担。实验结果表明，SmoothProper在视网膜血管数据集上显著降低了配准误差，是首个成功应对孔径和大位移挑战的无监督DIR方法。", "keywords": "可变形图像配准, 无监督学习, 平滑, 视网膜血管, 神经网络", "comments": "这篇论文的创新点在于提出了一个模型无关的即插即用模块SmoothProper，它通过在神经网络前向传播中引入结构平滑和消息传递，巧妙地解决了无监督DIR在处理稀疏特征和大位移图像时的局限性。这种方法将部分正则化负担从网络权重转移到优化层，简化了超参数调优，并显著提升了在特定挑战性数据集上的性能，具有重要的实际应用价值。"}}
{"id": "2506.10585", "title": "Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning", "authors": ["Mohd Anwar Jamal Faiz"], "summary": "This paper introduces the Primender sequence, a novel integer sequence\ndefined by a hybrid rule that combines classical primality with modular\ndigit-based conditions. Specifically, a number n is included in the sequence if\nit is prime or ends with a prime number of unit digit or any length. In other\nwords, numbers which are primes or have at least one prime suffix. The\nresulting sequence exhibits a deterministic yet non-trivial structure, blending\nnumber-theoretic properties with symbolic patterning. We propose the Primender\nsequence as a benchmark for evaluating the symbolic reasoning capabilities of\nLarge Language Models (LLMs). The study is motivated by the need for\ninterpretable, rule-based testbeds that can assess an LLM's ability to infer\nhidden rules, validate mathematical hypotheses, and generalize symbolic logic\nat scale. A key hypothesis explored is: Whenever a number in the Primender\nsequence is exactly one more than the largest prime less than or equal to it,\nthe difference between it and the previous number in the sequence is also 1. We\ndesign a structured prompt and evaluation framework to test this hypothesis\nacross multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,\nGemini, Grok, and LLaMA. The models are tasked with identifying the underlying\nrule, validating the hypothesis, and generating the next 100,000 terms of the\nsequence. Comparative metrics such as rule inference accuracy, hypothesis\nevaluation, sequence validity, and symbolic explanation quality are used to\nassess model performance. This work contributes a novel mathematical construct\nand a reproducible methodology for benchmarking LLMs in symbolic reasoning,\nhypothesis testing, and scalable pattern generalization - bridging the domains\nof number theory, artificial intelligence, and software engineering.", "comment": "9 pages, 7 figures, 2 tables, 3 codes, oeis sequence A384735", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10585v1", "AI": {"title_translation": "Primender 序列：一种用于测试符号推理和AI推理的新型数学结构", "tldr": "本文引入了 Primender 序列，这是一种新型整数序列，用作评估大型语言模型（LLMs）符号推理能力的基准。", "motivation": "出于对可解释、基于规则的测试平台的需求，以评估LLM推断隐藏规则、验证数学假设和大规模泛化符号逻辑的能力。", "method": "定义了 Primender 序列，提出并测试了一个关于序列的数学假设。设计了一个结构化提示和评估框架，使用包括ChatGPT、Copilot、DeepSeek、Gemini、Grok和LLaMA在内的多个最先进的LLM进行测试，并使用规则推理准确性、假设评估、序列有效性和符号解释质量等指标评估模型性能。", "result": "Not mentioned in abstract", "conclusion": "这项工作贡献了一种新颖的数学结构和可重复的方法，用于基准测试LLM在符号推理、假设检验和可扩展模式泛化方面的能力，连接了数论、人工智能和软件工程领域。", "translation": "本文介绍了 Primender 序列，这是一种通过结合经典素数性与基于模数字条件定义的整数序列。具体来说，如果一个数字 n 是素数，或者其单位数字或任意长度的结尾是一个素数，则它被包含在该序列中。换句话说，是素数或至少有一个素数后缀的数字。由此产生的序列表现出确定性但非平凡的结构，融合了数论性质与符号模式。我们建议将 Primender 序列作为评估大型语言模型（LLM）符号推理能力的基准。这项研究的动机在于需要可解释的、基于规则的测试平台，以评估LLM推断隐藏规则、验证数学假设以及大规模泛化符号逻辑的能力。探讨的一个关键假设是：当 Primender 序列中的一个数字恰好比小于或等于它的最大素数大一时，该数字与序列中前一个数字之间的差值也为1。我们设计了一个结构化提示和评估框架，用于在包括 ChatGPT、Copilot、DeepSeek、Gemini、Grok 和 LLaMA 在内的多个最先进的 LLM 上测试这一假设。模型的任务是识别底层规则、验证假设并生成序列的接下来的 100,000 个项。使用规则推理准确性、假设评估、序列有效性和符号解释质量等比较指标来评估模型性能。这项工作贡献了一种新颖的数学结构和可重复的方法，用于基准测试 LLM 在符号推理、假设检验和可扩展模式泛化方面的能力——桥接了数论、人工智能和软件工程领域。", "summary": "本文引入了 Primender 序列，这是一种结合素数性和数字后缀条件的新型整数序列，旨在作为评估大型语言模型（LLM）符号推理能力的基准。研究提出并测试了一个关于该序列的数学假设，通过设计结构化提示和评估框架，使用多种先进LLM（如ChatGPT、Gemini）进行测试，以评估它们在规则推断、假设验证和大规模序列生成方面的表现。这项工作提供了一个用于LLM符号推理基准测试的新颖数学工具和可重复方法。", "keywords": "Primender 序列, 符号推理, LLM, 数学基准, 假设检验", "comments": "这篇论文的创新点在于提出了一个新颖的数学序列（Primender 序列）作为评估LLM符号推理能力的独特基准。它结合了数论的确定性与符号模式的复杂性，为测试LLM在推断隐藏规则、验证数学假设和泛化符号逻辑方面的能力提供了一个可解释且基于规则的测试平台。其重要性在于为LLM评估提供了一个新的视角，超越了传统语言任务，深入到更深层次的逻辑和数学推理能力。"}}
{"id": "2506.10463", "title": "Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization", "authors": ["Stone Yun", "Alexander Wong"], "summary": "Deep neural network (DNN) quantization for fast, efficient inference has been\nan important tool in limiting the cost of machine learning (ML) model\ninference. Quantization-specific model development techniques such as\nregularization, quantization-aware training, and quantization-robustness\npenalties have served to greatly boost the accuracy and robustness of modern\nDNNs. However, very little exploration has been done on improving the initial\nconditions of DNN training for quantization. Just as random weight\ninitialization has been shown to significantly impact test accuracy of floating\npoint models, it would make sense that different weight initialization methods\nimpact quantization robustness of trained models. We present an extensive study\nexamining the effects of different weight initializations on a variety of CNN\nbuilding blocks commonly used in efficient CNNs. This analysis reveals that\neven with varying CNN architectures, the choice of random weight initializer\ncan significantly affect final quantization robustness. Next, we explore a new\nmethod for quantization-robust CNN initialization -- using Graph Hypernetworks\n(GHN) to predict parameters of quantized DNNs. Besides showing that\nGHN-predicted parameters are quantization-robust after regular float32\npretraining (of the GHN), we find that finetuning GHNs to predict parameters\nfor quantized graphs (which we call GHN-QAT) can further improve quantized\naccuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for\neven 4-bit quantization and better-than-random accuracy for 2-bits. To the best\nof our knowledge, this is the first in-depth study on quantization-aware DNN\nweight initialization. GHN-QAT offers a novel approach to quantized DNN model\ndesign. Future investigations, such as using GHN-QAT-initialized parameters for\nquantization-aware training, can further streamline the DNN quantization\nprocess.", "comment": "Portions of this article have been presented as extended abstracts at\n  the ICCV 2023 Workshop on Low Bit Quantized Neural Networks (ICCVW-LBQNN\n  2023) and the 2020 Conference on Vision and Intelligent Systems (CVIS 2020).\n  arXiv admin note: text overlap with arXiv:2011.14578, arXiv:2208.12489,\n  arXiv:2309.13773", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10463v1", "AI": {"title_translation": "起始位置很重要：神经网络量化中更好的权重初始化研究", "tldr": "本研究探讨了神经网络量化中权重初始化的重要性，并提出了一种新的基于图超网络（GHN-QAT）的方法来提高量化模型的准确性，尤其在低比特量化下表现显著。", "motivation": "深度神经网络（DNN）量化是限制机器学习模型推理成本的重要工具。尽管量化感知训练等技术已大大提升了DNN的准确性和鲁棒性，但很少有研究关注改善DNN训练的初始条件以实现量化。正如随机权重初始化对浮点模型测试准确率有显著影响一样，作者认为不同的权重初始化方法也会影响训练模型的量化鲁棒性。", "method": "作者首先进行了一项广泛研究，检查了不同权重初始化对各种常用高效CNN构建块的影响。接着，他们探索了一种新的量化鲁棒CNN初始化方法——使用图超网络（GHN）来预测量化DNN的参数。此外，他们还对GHN进行了微调，使其预测量化图的参数（称为GHN-QAT）。", "result": "研究发现，即使在不同的CNN架构中，随机权重初始化器的选择也能显著影响最终的量化鲁棒性。GHN预测的参数在常规float32预训练后具有量化鲁棒性。GHN-QAT能进一步提高CNN的量化准确性，尤其在4比特量化下显示出显著的准确性提升，并在2比特量化下优于随机初始化。", "conclusion": "这是首次深入研究量化感知DNN权重初始化。GHN-QAT为量化DNN模型设计提供了一种新颖的方法。未来的研究，如将GHN-QAT初始化的参数用于量化感知训练，可以进一步简化DNN量化过程。", "translation": "深度神经网络（DNN）量化用于快速、高效的推理，一直是限制机器学习（ML）模型推理成本的重要工具。量化特定的模型开发技术，如正则化、量化感知训练和量化鲁棒性惩罚，极大地提升了现代DNN的准确性和鲁棒性。然而，在改进DNN训练的初始条件以实现量化方面，探索甚少。正如随机权重初始化已被证明能显著影响浮点模型的测试准确率一样，不同的权重初始化方法很可能也会影响训练模型的量化鲁棒性。我们进行了一项广泛研究，考察了不同权重初始化对高效CNN中常用各种CNN构建块的影响。这项分析表明，即使在不同的CNN架构中，随机权重初始化器的选择也能显著影响最终的量化鲁棒性。接下来，我们探索了一种新的量化鲁棒CNN初始化方法——使用图超网络（GHN）来预测量化DNN的参数。除了表明GHN预测的参数在常规float32预训练（GHN的）后具有量化鲁棒性外，我们发现微调GHN以预测量化图的参数（我们称之为GHN-QAT）可以进一步提高CNN的量化准确性。值得注意的是，GHN-QAT甚至在4比特量化下也显示出显著的准确性提升，并且在2比特下优于随机初始化。据我们所知，这是首次对量化感知DNN权重初始化进行深入研究。GHN-QAT为量化DNN模型设计提供了一种新颖的方法。未来的研究，例如使用GHN-QAT初始化的参数进行量化感知训练，可以进一步简化DNN量化过程。", "summary": "本研究探讨了深度神经网络量化中权重初始化的重要性，指出其对模型量化鲁棒性的显著影响。论文通过广泛实验证实了不同初始化方法对最终量化准确率的影响，并提出了一种基于图超网络（GHN）的新型量化鲁棒初始化方法GHN-QAT。实验结果表明，GHN-QAT能够有效提高量化模型的准确性，尤其在低比特量化（如4比特和2比特）下表现出显著优势。这项工作是首次深入研究量化感知DNN权重初始化，为量化DNN模型设计提供了新颖途径。", "keywords": "神经网络量化, 权重初始化, 图超网络, 量化鲁棒性, 低比特量化", "comments": "这项研究的创新点在于首次深入探讨了神经网络量化中权重初始化的重要性，并提出了一种基于图超网络（GHN-QAT）的创新方法来解决这一问题。其重要性体现在能够显著提高量化模型的准确性，特别是在极低比特量化下仍能保持较好性能，这对于资源受限的部署场景具有重要意义。该方法为未来量化DNN模型设计提供了新的思路，有望进一步简化和优化量化流程。"}}
{"id": "2506.10144", "title": "Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities", "authors": ["Yaowen Zhang", "Libera Fresiello", "Peter H. Veltink", "Dirk W. Donker", "Ying Wang"], "summary": "Heart failure (HF) poses a significant global health challenge, with early\ndetection offering opportunities for improved outcomes. Abnormalities in heart\nrate (HR), particularly during daily activities, may serve as early indicators\nof HF risk. However, existing HR monitoring tools for HF detection are limited\nby their reliability on population-based averages. The estimation of\nindividualized HR serves as a dynamic digital twin, enabling precise tracking\nof cardiac health biomarkers. Current HR estimation methods, categorized into\nphysiologically-driven and purely data-driven models, struggle with efficiency\nand interpretability. This study introduces a novel physiological-model-based\nneural network (PMB-NN) framework for HR estimation based on oxygen uptake\n(VO2) data during daily physical activities. The framework was trained and\ntested on individual datasets from 12 participants engaged in activities\nincluding resting, cycling, and running. By embedding physiological\nconstraints, which were derived from our proposed simplified human movement\nphysiological model (PM), into the neural network training process, the PMB-NN\nmodel adheres to human physiological principles while achieving high estimation\naccuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative\nstatistical analysis demonstrates that the PMB-NN achieves performance on par\nwith the benchmark neural network model while significantly outperforming\ntraditional physiological model (p=0.002). In addition, our PMB-NN is adept at\nidentifying personalized parameters of the PM, enabling the PM to generate\nreasonable HR estimation. The proposed framework with a precise VO2 estimation\nsystem derived from body movements enables the future possibilities of\npersonalized and real-time cardiac monitoring during daily life physical\nactivities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10144v1", "AI": {"title_translation": "基于生理模型的神经网络用于日常体育活动中心率估计", "tldr": "该研究提出了一种新颖的基于生理模型的神经网络（PMB-NN）框架，用于根据日常体育活动中的摄氧量（VO2）数据估计心率，实现了高估计精度，并优于传统生理模型。", "motivation": "心力衰竭（HF）是一个重大的全球健康挑战，早期发现可以改善预后。日常活动中心率异常可能是HF风险的早期指标。然而，现有用于HF检测的心率监测工具受限于其对基于人群平均值的依赖性，且现有心率估计方法（生理驱动型和纯数据驱动型）在效率和可解释性方面存在不足。该研究旨在实现个性化心率估计，作为动态数字孪生，精确追踪心脏健康生物标志物。", "method": "本研究引入了一种新颖的基于生理模型的神经网络（PMB-NN）框架，用于根据日常体育活动中的摄氧量（VO2）数据估计心率。该框架在来自12名参与者的个体数据集上进行训练和测试，这些参与者进行了包括休息、骑自行车和跑步在内的活动。通过将从简化的“人体运动生理模型”（PM）中导出的生理约束嵌入到神经网络训练过程中，PMB-NN模型在遵循人类生理原理的同时实现了高估计精度。", "result": "PMB-NN模型实现了高估计精度，中位数R^2分数为0.8，RMSE为8.3 bpm。比较统计分析表明，PMB-NN的性能与基准神经网络模型相当，同时显著优于传统生理模型（p=0.002）。此外，PMB-NN能够识别PM的个性化参数，使PM能够生成合理的心率估计。", "conclusion": "提出的PMB-NN框架结合精确的VO2估计系统，为未来在日常生活中进行个性化和实时心脏监测提供了可能性。", "translation": "心力衰竭（HF）构成重大的全球健康挑战，早期发现可改善预后。心率（HR）异常，特别是在日常活动期间，可能作为HF风险的早期指标。然而，现有用于HF检测的心率监测工具受限于其对基于人群平均值的可靠性。个性化心率估计作为动态数字孪生，能够精确跟踪心脏健康生物标志物。当前的心率估计方法，分为生理驱动型和纯数据驱动型模型，在效率和可解释性方面存在困难。本研究引入了一种新颖的基于生理模型的神经网络（PMB-NN）框架，用于在日常体育活动中根据摄氧量（VO2）数据估计心率。该框架在来自12名参与者的个体数据集上进行训练和测试，这些参与者进行了包括休息、骑自行车和跑步在内的活动。通过将从我们提出的简化人体运动生理模型（PM）中导出的生理约束嵌入到神经网络训练过程中，PMB-NN模型在遵循人类生理原理的同时实现了高估计精度，中位数R^2分数为0.8，RMSE为8.3 bpm。比较统计分析表明，PMB-NN的性能与基准神经网络模型相当，同时显著优于传统生理模型（p=0.002）。此外，我们的PMB-NN善于识别PM的个性化参数，使PM能够生成合理的心率估计。所提出的框架与来自身体运动的精确VO2估计系统相结合，为未来在日常生活中进行个性化和实时心脏监测提供了可能性。", "summary": "本研究提出了一种新颖的基于生理模型的神经网络（PMB-NN）框架，旨在解决现有心率估计方法在心力衰竭早期检测中面临的效率和可解释性挑战。该框架将简化的生理模型约束嵌入到神经网络训练中，利用日常体育活动中的摄氧量数据进行心率估计。在12名参与者的个体数据集上进行训练和测试后，PMB-NN模型展现出高精度（R^2=0.8，RMSE=8.3 bpm），性能与基准神经网络相当，并显著优于传统生理模型。该方法能够识别个性化生理参数，为未来实现个性化、实时心脏健康监测奠定基础。", "keywords": "心率估计, 生理模型, 神经网络, 摄氧量, 心力衰竭", "comments": "该论文的创新之处在于将生理模型（PM）的约束嵌入到神经网络（NN）的训练过程中，形成了一种结合生理学原理和数据驱动优势的混合模型（PMB-NN）。这不仅提高了心率估计的准确性，还增强了模型的可解释性和个性化能力。通过识别个性化参数，该模型有望为心脏健康监测提供更精确、更具针对性的解决方案，特别是在心力衰竭的早期预警方面具有重要意义。其局限性可能在于目前仅在12名参与者的数据集上进行了测试，未来需要更大规模、更多样化的数据集进行验证。"}}
{"id": "2506.10844", "title": "CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training", "authors": ["Alireza Salemi", "Mukta Maddipatla", "Hamed Zamani"], "summary": "This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)\nframework composed of specialized agents for subtasks such as planning,\nsearching, reasoning, and coordination. Our system uses a self-training\nparadigm with reward-guided trajectory sampling to optimize inter-agent\ncollaboration and enhance response generation. Evaluated on DataMorgana-derived\ndatasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms\nconventional RAG baselines. We further analyze competition outcomes and\nshowcase the framework's strengths with case studies, demonstrating its\nefficacy for complex, real-world RAG tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10844v1", "AI": {"title_translation": "CIIR@LiveRAG 2025: 通过自训练优化多智能体检索增强生成", "tldr": "论文提出了mRAG，一个多智能体RAG框架，通过自训练和奖励引导优化代理协作，在LiveRAG竞赛中表现优于传统RAG基线。", "motivation": "本研究旨在通过优化多智能体协作来增强响应生成，并有效处理复杂的真实世界RAG任务。", "method": "本文提出了mRAG，一个多智能体检索增强生成（RAG）框架，由专门负责规划、搜索、推理和协调等子任务的智能体组成。该系统采用自训练范式，通过奖励引导的轨迹采样来优化智能体间的协作。", "result": "在SIGIR 2025 LiveRAG竞赛期间，mRAG在DataMorgana衍生数据集上进行了评估，其性能优于传统的RAG基线。竞赛结果分析和案例研究进一步展示了该框架的优势。", "conclusion": "mRAG框架通过自训练和优化多智能体协作，有效解决了复杂的真实世界检索增强生成（RAG）任务。", "translation": "本文提出了mRAG，一个由规划、搜索、推理和协调等子任务的专业智能体组成的多智能体检索增强生成（RAG）框架。我们的系统采用自训练范式，通过奖励引导的轨迹采样来优化智能体间的协作并增强响应生成。在SIGIR 2025 LiveRAG竞赛期间，mRAG在DataMorgana衍生数据集上进行了评估，其性能优于传统的RAG基线。我们进一步分析了竞赛结果，并通过案例研究展示了该框架的优势，证明了其在复杂、真实世界RAG任务中的有效性。", "summary": "本文介绍了mRAG，一个创新的多智能体检索增强生成（RAG）框架。该框架包含多个专业代理，负责规划、搜索、推理和协调等任务。mRAG利用自训练范式和奖励引导的轨迹采样来优化代理间的协作，从而提升生成响应的质量。在SIGIR 2025 LiveRAG竞赛的DataMorgana数据集上，mRAG的性能超越了传统的RAG基线，并通过案例研究证明了其在处理复杂真实世界RAG任务方面的有效性。", "keywords": "多智能体系统, 检索增强生成, 自训练, 奖励引导, LiveRAG", "comments": "这篇论文的创新点在于提出了一个多智能体RAG框架，并通过自训练和奖励引导的轨迹采样来优化智能体间的协作，这为提升RAG系统在复杂任务中的表现提供了一个有前景的方向。其在LiveRAG竞赛中的优异表现也证明了其有效性。"}}
{"id": "2506.10288", "title": "ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs", "authors": ["Zige Wang", "Qi Zhu", "Fei Mi", "Minghui Xu", "Ruochun Jin", "Wenjing Yang"], "summary": "Gradient-based data influence approximation has been leveraged to select\nuseful data samples in the supervised fine-tuning of large language models.\nHowever, the computation of gradients throughout the fine-tuning process\nrequires too many resources to be feasible in practice. In this paper, we\npropose an efficient gradient-based data selection framework with clustering\nand a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition\nthat data samples with similar gradient features will have similar influences,\nwe first perform clustering on the training data pool. Then, we frame the\ninter-cluster data selection as a constrained computing budget allocation\nproblem and consider it a multi-armed bandit problem. A modified UCB algorithm\nis leveraged to solve this problem. Specifically, during the iterative sampling\nprocess, historical data influence information is recorded to directly estimate\nthe distributions of each cluster, and a cold start is adopted to balance\nexploration and exploitation. Experimental results on various benchmarks show\nthat our proposed framework, ClusterUCB, can achieve comparable results to the\noriginal gradient-based data selection methods while greatly reducing computing\nconsumption.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10288v1", "AI": {"title_translation": "ClusterUCB：面向LLM定向微调的高效基于梯度的数据选择方法", "tldr": "ClusterUCB提出了一种高效的基于梯度的LLM数据选择框架，通过聚类和改进的UCB算法，在大幅减少计算开销的同时，实现了与原始方法相当的性能。", "motivation": "现有的基于梯度的LLM数据选择方法计算资源消耗过大，难以在实际中应用。", "method": "本文提出了ClusterUCB框架。首先，基于梯度特征相似的数据样本影响相似的直觉，对训练数据池进行聚类。然后，将簇间数据选择视为受限计算预算分配问题，并将其建模为多臂老虎机问题，通过改进的UCB算法解决。该算法记录历史数据影响信息以估计每个簇的分布，并采用冷启动平衡探索与利用。", "result": "实验结果表明，ClusterUCB框架在各种基准测试上取得了与原始基于梯度的LLM数据选择方法相当的结果，同时显著降低了计算消耗。", "conclusion": "ClusterUCB通过创新的聚类和UCB算法有效解决了基于梯度的数据选择在LLM微调中计算成本过高的问题，使其在实际应用中更具可行性。", "translation": "基于梯度的数据影响力近似已被用于在大语言模型的监督微调中选择有用数据样本。然而，在整个微调过程中计算梯度需要过多的资源，在实践中不可行。在本文中，我们提出了一种高效的基于梯度的、结合聚类和改进的Upper Confidence Bound（UCB）算法的数据选择框架。基于梯度特征相似的数据样本将具有相似影响的直觉，我们首先对训练数据池执行聚类。然后，我们将簇间数据选择构建为一个受限计算预算分配问题，并将其视为一个多臂老虎机问题。利用改进的UCB算法来解决这个问题。具体来说，在迭代采样过程中，记录历史数据影响信息以直接估计每个簇的分布，并采用冷启动来平衡探索和利用。在各种基准测试上的实验结果表明，我们提出的框架ClusterUCB可以实现与原始基于梯度的LLM数据选择方法相当的结果，同时大大减少了计算消耗。", "summary": "ClusterUCB是一种高效的LLM微调数据选择框架，旨在解决传统基于梯度方法计算成本高昂的问题。它通过对训练数据进行聚类，并将簇间选择建模为多臂老虎机问题，利用改进的UCB算法进行优化。该方法在保证性能的同时，显著降低了计算资源消耗，使其在实际应用中更具可行性。", "keywords": "LLMs, 数据选择, 梯度, 微调, ClusterUCB", "comments": "该论文的创新点在于将聚类思想与多臂老虎机（UCB）算法相结合，以优化基于梯度的LLM数据选择过程。这种方法有效地解决了梯度计算成本高昂的实际问题，使得基于影响力的数据选择在LLM微调中更加实用。其贡献在于提供了一种高效且性能相当的替代方案。"}}
{"id": "2506.10286", "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models", "authors": ["Eunkyu Park", "Minyeong Kim", "Gunhee Kim"], "summary": "Hallucinations pose a significant challenge to the reliability of large\nvision-language models, making their detection essential for ensuring accuracy\nin critical applications. Current detection methods often rely on\ncomputationally intensive models, leading to high latency and resource demands.\nTheir definitive outcomes also fail to account for real-world scenarios where\nthe line between hallucinated and truthful information is unclear. To address\nthese issues, we propose HalLoc, a dataset designed for efficient,\nprobabilistic hallucination detection. It features 150K token-level annotated\nsamples, including hallucination types, across Visual Question Answering (VQA),\ninstruction-following, and image captioning tasks. This dataset facilitates the\ndevelopment of models that detect hallucinations with graded confidence,\nenabling more informed user interactions. Additionally, we introduce a baseline\nmodel trained on HalLoc, offering low-overhead, concurrent hallucination\ndetection during generation. The model can be seamlessly integrated into\nexisting VLMs, improving reliability while preserving efficiency. The prospect\nof a robust plug-and-play hallucination detection module opens new avenues for\nenhancing the trustworthiness of vision-language models in real-world\napplications. The HalLoc dataset and code are publicly available at:\nhttps://github.com/dbsltm/cvpr25_halloc.", "comment": "CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10286v1", "AI": {"title_translation": "HalLoc: 视觉语言模型幻觉的Token级定位", "tldr": "提出了HalLoc数据集，用于高效、概率性地检测视觉语言模型的幻觉，并提供了一个低开销的基线模型。", "motivation": "幻觉严重影响大型视觉语言模型的可靠性，现有检测方法计算成本高、延迟大、资源消耗多，且无法处理幻觉与真实信息界限模糊的场景。", "method": "提出了HalLoc数据集，包含15万个Token级标注样本（包括幻觉类型），涵盖视觉问答（VQA）、指令遵循和图像字幕任务，旨在实现高效、概率性幻觉检测。同时，引入了一个基于HalLoc训练的基线模型，可在生成过程中进行低开销、并发的幻觉检测。", "result": "HalLoc数据集促进了能够以分级置信度检测幻觉的模型的开发，实现了更明智的用户交互。基线模型可以无缝集成到现有VLM中，在保持效率的同时提高可靠性。", "conclusion": "鲁棒的即插即用幻觉检测模块有望增强视觉语言模型在实际应用中的可信度。", "translation": "幻觉对大型视觉语言模型的可靠性构成了重大挑战，因此，检测幻觉对于确保关键应用中的准确性至关重要。当前的检测方法通常依赖于计算密集型模型，导致高延迟和资源需求。它们的确定性结果也未能考虑到幻觉信息与真实信息界限不明确的实际场景。为了解决这些问题，我们提出了HalLoc，一个旨在实现高效、概率性幻觉检测的数据集。它包含15万个Token级标注样本，涵盖视觉问答（VQA）、指令遵循和图像字幕任务中的幻觉类型。该数据集有助于开发能够以分级置信度检测幻觉的模型，从而实现更明智的用户交互。此外，我们引入了一个在HalLoc上训练的基线模型，可在生成过程中提供低开销、并发的幻觉检测。该模型可以无缝集成到现有VLM中，在保持效率的同时提高可靠性。这种强大的即插即用幻觉检测模块的前景为增强视觉语言模型在实际应用中的可信度开辟了新途径。HalLoc数据集和代码已公开：https://github.com/dbsltm/cvpr25_halloc。", "summary": "本文提出了HalLoc数据集，旨在解决大型视觉语言模型中幻觉检测效率低下和确定性不足的问题。HalLoc包含15万个Token级标注样本，支持VQA、指令遵循和图像字幕任务， enabling模型以概率性方式检测幻觉。研究还提供了一个基于HalLoc训练的低开销基线模型，可无缝集成到现有VLM中，以提高其可靠性。该工作为开发可信赖的视觉语言模型提供了新的即插即用解决方案。", "keywords": "幻觉检测, 视觉语言模型, 数据集, Token级标注, 可靠性", "comments": "HalLoc数据集的创新之处在于其Token级的幻觉标注和对概率性检测的支持，这比传统的确定性检测方法更贴近实际应用。其低开销的即插即用基线模型也大大降低了集成难度，有望推动VLM在实际部署中的可靠性提升。"}}
{"id": "2506.10894", "title": "Numerical approximation of a PDE-constrained Optimization problem that appears in Data-Driven Computational Mechanics", "authors": ["Pedro B. Bazon", "Cristian G. Gebhardt", "Gustavo C. Buscaglia", "Roberto F. Ausas"], "summary": "We investigate an optimization problem that arises when working within the\nparadigm of Data-Driven Computational Mechanics. In the context of the\ndiffusion-reaction problem, such an optimization problem seeks for the\ncontinuous primal fields (gradient and flux) that are closest to some\npredefined discrete fields taken from a material data set. The optimization is\nperformed over primal fields that satisfy the physical conservation law and the\ngeometrical compatibility. We consider a reaction term in the conservation law,\nwhich has the effect of coupling all the optimality conditions. We first\nestablish the well-posedness in the continuous setting. Then, we propose stable\nfinite element discretizations that consistently approximate the continuous\nformulation, preserving its saddle-point structure and allowing for equal-order\ninterpolation of all fields. Finally, we demonstrate the effectiveness of the\nproposed methods through a set of numerical examples.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10894v1", "AI": {"title_translation": "数据驱动计算力学中偏微分方程约束优化问题的数值近似", "tldr": "本文研究了数据驱动计算力学中的一个PDE约束优化问题，提出了稳定的有限元离散化方法，并验证了其有效性。", "motivation": "在数据驱动计算力学范式中，需要寻找最接近给定材料数据集离散场的连续原始场（梯度和通量），该优化问题是受物理守恒定律和几何兼容性约束的。", "method": "首先，在连续设置中建立了问题的适定性。然后，提出了稳定的有限元离散化方法，该方法能够一致地近似连续公式，保留其鞍点结构，并允许所有场进行等阶插值。", "result": "通过一系列数值示例，证明了所提出方法的有效性。", "conclusion": "所提出的数值方法能够有效近似数据驱动计算力学中的偏微分方程约束优化问题。", "translation": "我们研究了在数据驱动计算力学范式中出现的一个优化问题。在扩散-反应问题的背景下，这种优化问题旨在寻找最接近从材料数据集中获取的某些预定义离散场的连续原始场（梯度和通量）。优化是在满足物理守恒定律和几何兼容性的原始场上进行的。我们在守恒定律中考虑了一个反应项，这会耦合所有的最优性条件。我们首先在连续设置中建立了适定性。然后，我们提出了稳定的有限元离散化方法，这些方法能够一致地近似连续公式，保留其鞍点结构并允许所有场进行等阶插值。最后，我们通过一系列数值示例证明了所提出方法的有效性。", "summary": "本文研究了数据驱动计算力学中出现的偏微分方程约束优化问题。该问题旨在寻找最接近预定义离散材料数据集的连续原始场，同时满足物理守恒定律和几何兼容性。文章首先建立了连续设置下的适定性，然后提出了稳定的有限元离散化方法，该方法保留了连续公式的鞍点结构并允许等阶插值。最后，通过数值示例验证了方法的有效性。", "keywords": "数据驱动计算力学, 偏微分方程约束优化, 有限元离散化, 鞍点结构, 适定性", "comments": "本文的创新之处在于提出了保持鞍点结构并允许所有场进行等阶插值的稳定有限元离散化方法，这对于处理数据驱动计算力学中的PDE约束优化问题具有重要意义。"}}
{"id": "2506.10399", "title": "FicGCN: Unveiling the Homomorphic Encryption Efficiency from Irregular Graph Convolutional Networks", "authors": ["Zhaoxuan Kan", "Husheng Han", "Shangyi Shi", "Tenghui Hua", "Hang Lu", "Xiaowei Li", "Jianan Mu", "Xing Hu"], "summary": "Graph Convolutional Neural Networks (GCNs) have gained widespread popularity\nin various fields like personal healthcare and financial systems, due to their\nremarkable performance. Despite the growing demand for cloud-based GCN\nservices, privacy concerns over sensitive graph data remain significant.\nHomomorphic Encryption (HE) facilitates Privacy-Preserving Machine Learning\n(PPML) by allowing computations to be performed on encrypted data. However, HE\nintroduces substantial computational overhead, particularly for GCN operations\nthat require rotations and multiplications in matrix products. The sparsity of\nGCNs offers significant performance potential, but their irregularity\nintroduces additional operations that reduce practical gains. In this paper, we\npropose FicGCN, a HE-based framework specifically designed to harness the\nsparse characteristics of GCNs and strike a globally optimal balance between\naggregation and combination operations. FicGCN employs a latency-aware packing\nscheme, a Sparse Intra-Ciphertext Aggregation (SpIntra-CA) method to minimize\nrotation overhead, and a region-based data reordering driven by local adjacency\nstructure. We evaluated FicGCN on several popular datasets, and the results\nshow that FicGCN achieved the best performance across all tested datasets, with\nup to a 4.10x improvement over the latest design.", "comment": "Accepted by ICML 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10399v1", "AI": {"title_translation": "FicGCN：揭示不规则图卷积网络中同态加密的效率", "tldr": "FicGCN是一个基于同态加密的框架，旨在提高云端图卷积网络（GCN）的隐私保护效率，通过利用GCN的稀疏性并优化聚合和组合操作，显著降低了计算开销，性能比现有设计提升高达4.10倍。", "motivation": "图卷积网络（GCNs）在个人医疗和金融系统等领域广泛应用，但基于云的GCN服务存在敏感图数据隐私泄露的风险。同态加密（HE）虽能实现隐私保护机器学习（PPML），但其计算开销巨大，尤其是在GCN所需的旋转和乘法操作中。GCN的稀疏性虽有性能潜力，但不规则性引入了额外的操作，降低了实际收益。因此，需要一个能有效利用GCN稀疏性并优化HE操作的框架。", "method": "本文提出了FicGCN，一个基于同态加密的框架，专门设计用于利用GCN的稀疏特性，并在聚合和组合操作之间实现全局最优平衡。FicGCN采用了延迟感知打包方案、稀疏密文内聚合（SpIntra-CA）方法以最小化旋转开销，以及由局部邻接结构驱动的基于区域的数据重排序。", "result": "在多个流行数据集上进行评估，结果表明FicGCN在所有测试数据集上均取得了最佳性能，比最新设计提升高达4.10倍。", "conclusion": "FicGCN通过有效利用图卷积网络的稀疏性并优化同态加密操作，显著提高了隐私保护下GCN的计算效率，实现了优于现有设计的性能，证明了其在云端隐私保护GCN服务中的潜力。", "translation": "图卷积神经网络（GCNs）因其卓越的性能在个人医疗和金融系统等各个领域获得了广泛的普及。尽管对基于云的GCN服务需求不断增长，但对敏感图数据的隐私担忧仍然显著。同态加密（HE）通过允许在加密数据上执行计算，促进了隐私保护机器学习（PPML）。然而，HE引入了巨大的计算开销，特别是对于矩阵乘法中需要旋转和乘法操作的GCN。GCN的稀疏性提供了显著的性能潜力，但其不规则性引入了额外的操作，降低了实际收益。在本文中，我们提出了FicGCN，一个基于HE的框架，专门设计用于利用GCN的稀疏特性，并在聚合和组合操作之间实现全局最优平衡。FicGCN采用了延迟感知打包方案、稀疏密文内聚合（SpIntra-CA）方法以最小化旋转开销，以及由局部邻接结构驱动的基于区域的数据重排序。我们在几个流行数据集上评估了FicGCN，结果表明FicGCN在所有测试数据集上均取得了最佳性能，比最新设计提升高达4.10倍。", "summary": "该论文提出了FicGCN，一个基于同态加密（HE）的框架，旨在解决云端图卷积网络（GCN）中隐私保护与计算效率之间的矛盾。FicGCN通过利用GCN的稀疏性，并采用延迟感知打包、稀疏密文内聚合（SpIntra-CA）和基于区域的数据重排序等技术，优化了HE操作中的聚合和组合过程，显著降低了计算开销。实验结果表明，FicGCN在多个数据集上均表现出最佳性能，相较于现有设计实现了高达4.10倍的性能提升。", "keywords": "同态加密, 图卷积网络, 隐私保护, 稀疏性, 计算效率", "comments": "FicGCN的创新点在于其将同态加密与不规则GCN的稀疏特性相结合，通过精巧的设计如SpIntra-CA和数据重排序，有效解决了HE在GCN中引入的巨大计算开销问题。该工作对于推动隐私保护机器学习，尤其是在敏感数据领域的GCN应用具有重要意义。其提出的优化策略为未来HE在复杂图结构数据上的应用提供了宝贵的经验。"}}
{"id": "2506.10704", "title": "Formalising Software Requirements using Large Language Models", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "summary": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.", "comment": "Accepted and presented as a poster in ADAPT Annual Conference\n  (AACS2025) on 15th of May, 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10704v1", "AI": {"title_translation": "使用大型语言模型形式化软件需求", "tldr": "本文介绍了VERIFAI项目，旨在通过自然语言处理、本体论、软件复用、大型语言模型和人工智能等方法，自动化自然语言需求的形式化规范生成和可追溯性，以解决形式化规范的可追溯性和验证挑战。", "motivation": "解决自然语言需求的形式化规范在可追溯性和验证方面的挑战。", "method": "探索的方法包括自然语言处理（NLP）、使用本体论描述软件系统领域、复用现有软件构件（基于相似性的复用）、使用大型语言模型识别和声明规范，以及利用人工智能指导整个过程。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "本文简要介绍了我们最近启动的一个名为VERIFAI的项目：自然语言需求的可追溯性和验证。该项目通过支持形式化规范的自动生成以及从初始软件设计阶段到系统实施和验证的整个过程中需求的可追溯性，来解决形式化规范可追溯性和验证方面的挑战。该项目探索的方法包括自然语言处理、使用本体论描述软件系统领域、复用来自类似系统的现有软件构件（即通过基于相似性的复用），以及使用大型语言模型识别和声明规范，同时利用人工智能指导整个过程。", "summary": "本文介绍了VERIFAI项目，旨在解决自然语言需求形式化规范的可追溯性和验证挑战。该项目通过自动化形式化规范的生成和需求的全生命周期可追溯性来实现，主要方法包括自然语言处理、本体论、软件构件复用、大型语言模型以及人工智能指导。", "keywords": "形式化规范, 需求可追溯性, 大型语言模型, 自然语言处理, 软件工程", "comments": "本文介绍了VERIFAI项目，其创新点在于利用大型语言模型和人工智能来形式化软件需求，并提高其可追溯性和验证。这对于解决软件工程中形式化规范的挑战具有重要意义。由于是项目初期介绍，因此未提及具体局限性或已取得的成果。"}}
{"id": "2506.10787", "title": "In-Hand Object Pose Estimation via Visual-Tactile Fusion", "authors": ["Felix Nonnengießer", "Alap Kshirsagar", "Boris Belousov", "Jan Peters"], "summary": "Accurate in-hand pose estimation is crucial for robotic object manipulation,\nbut visual occlusion remains a major challenge for vision-based approaches.\nThis paper presents an approach to robotic in-hand object pose estimation,\ncombining visual and tactile information to accurately determine the position\nand orientation of objects grasped by a robotic hand. We address the challenge\nof visual occlusion by fusing visual information from a wrist-mounted RGB-D\ncamera with tactile information from vision-based tactile sensors mounted on\nthe fingertips of a robotic gripper. Our approach employs a weighting and\nsensor fusion module to combine point clouds from heterogeneous sensor types\nand control each modality's contribution to the pose estimation process. We use\nan augmented Iterative Closest Point (ICP) algorithm adapted for weighted point\nclouds to estimate the 6D object pose. Our experiments show that incorporating\ntactile information significantly improves pose estimation accuracy,\nparticularly when occlusion is high. Our method achieves an average pose\nestimation error of 7.5 mm and 16.7 degrees, outperforming vision-only\nbaselines by up to 20%. We also demonstrate the ability of our method to\nperform precise object manipulation in a real-world insertion task.", "comment": "8 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10787v1", "AI": {"title_translation": "手持物体姿态估计通过视觉-触觉融合", "tldr": "本文提出一种结合视觉和触觉信息的方法，用于准确估计机器人手持物体的姿态，尤其在视觉遮挡严重时表现更优。", "motivation": "准确的手持物体姿态估计对机器人操作至关重要，但视觉遮挡是基于视觉方法的主要挑战。", "method": "结合腕部RGB-D相机视觉信息和指尖视觉触觉传感器的触觉信息。采用加权传感器融合模块组合异构传感器点云，并使用适应加权点云的增强型迭代最近点（ICP）算法估计6D物体姿态。", "result": "结合触觉信息显著提高了姿态估计精度，尤其在遮挡严重时。平均姿态估计误差为7.5毫米和16.7度，优于纯视觉基线高达20%。并在实际插入任务中展示了精确物体操作能力。", "conclusion": "结合视觉和触觉信息能够有效解决机器人手持物体姿态估计中的视觉遮挡问题，显著提高精度，并支持实际操作。", "translation": "准确的手持姿态估计对于机器人物体操作至关重要，但视觉遮挡仍然是基于视觉方法的主要挑战。本文提出一种机器人手持物体姿态估计方法，结合视觉和触觉信息，以准确确定机器人手抓取物体的姿态和方向。我们通过融合腕部安装的RGB-D相机的视觉信息与机器人夹持器指尖上基于视觉的触觉传感器的触觉信息来解决视觉遮挡的挑战。我们的方法采用加权和传感器融合模块来组合来自异构传感器类型的点云，并控制每种模态对姿态估计过程的贡献。我们使用适应加权点云的增强型迭代最近点（ICP）算法来估计6D物体姿态。我们的实验表明，结合触觉信息显著提高了姿态估计精度，尤其是在遮挡严重时。我们的方法实现了7.5毫米和16.7度的平均姿态估计误差，性能优于纯视觉基线高达20%。我们还展示了我们的方法在真实世界插入任务中执行精确物体操作的能力。", "summary": "本文提出一种创新的机器人手持物体姿态估计方法，通过融合腕部RGB-D相机的视觉数据和指尖触觉传感器的触觉数据来克服视觉遮挡问题。该方法利用加权传感器融合模块和增强型ICP算法处理异构点云。实验证明，该方法在视觉遮挡高的情况下显著提高了姿态估计精度，误差低，并成功应用于实际操作任务。", "keywords": "姿态估计, 视觉-触觉融合, 机器人操作, 视觉遮挡, ICP算法", "comments": "本文的创新点在于有效地结合了视觉和触觉信息来解决机器人手持物体姿态估计中常见的视觉遮挡问题。通过加权融合异构传感器数据和改进ICP算法，显著提升了姿态估计的鲁棒性和精度，并在实际操作中得到了验证，具有重要的应用价值。"}}
{"id": "2506.10167", "title": "Wasserstein Barycenter Soft Actor-Critic", "authors": ["Zahra Shahrooei", "Ali Baheri"], "summary": "Deep off-policy actor-critic algorithms have emerged as the leading framework\nfor reinforcement learning in continuous control domains. However, most of\nthese algorithms suffer from poor sample efficiency, especially in environments\nwith sparse rewards. In this paper, we take a step towards addressing this\nissue by providing a principled directed exploration strategy. We propose\nWasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from\na pessimistic actor for temporal difference learning and an optimistic actor to\npromote exploration. This is achieved by using the Wasserstein barycenter of\nthe pessimistic and optimistic policies as the exploration policy and adjusting\nthe degree of exploration throughout the learning process. We compare WBSAC\nwith state-of-the-art off-policy actor-critic algorithms and show that WBSAC is\nmore sample-efficient on MuJoCo continuous control tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10167v1", "AI": {"title_translation": "Wasserstein 质心软演员-评论家算法", "tldr": "本文提出了 Wasserstein 质心软演员-评论家 (WBSAC) 算法，通过结合悲观和乐观的策略，并利用 Wasserstein 质心进行有向探索，提高了深度离策略演员-评论家算法在连续控制任务中的样本效率。", "motivation": "深度离策略演员-评论家算法在连续控制领域中表现出色，但其样本效率低下，尤其是在稀疏奖励环境中。", "method": "提出 Wasserstein 质心软演员-评论家 (WBSAC) 算法。该算法利用一个悲观的演员进行时序差分学习，并利用一个乐观的演员来促进探索。通过使用悲观和乐观策略的 Wasserstein 质心作为探索策略，并在学习过程中调整探索程度来实现。", "result": "与最先进的离策略演员-评论家算法相比，WBSAC 在 MuJoCo 连续控制任务上具有更高的样本效率。", "conclusion": "WBSAC 算法通过提供一种有原则的有向探索策略，显著提高了深度离策略演员-评论家算法在连续控制任务中的样本效率。", "translation": "深度离策略演员-评论家算法已成为连续控制领域强化学习的主导框架。然而，大多数这些算法都存在样本效率低下的问题，尤其是在稀疏奖励环境中。在本文中，我们通过提供一种有原则的有向探索策略，向解决此问题迈出了一步。我们提出了 Wasserstein 质心软演员-评论家 (WBSAC) 算法，该算法受益于用于时序差分学习的悲观演员和用于促进探索的乐观演员。这是通过使用悲观和乐观策略的 Wasserstein 质心作为探索策略，并在整个学习过程中调整探索程度来实现的。我们将 WBSAC 与最先进的离策略演员-评论家算法进行了比较，结果表明 WBSAC 在 MuJoCo 连续控制任务上具有更高的样本效率。", "summary": "WBSAC 算法旨在解决深度离策略演员-评论家算法在连续控制任务中样本效率低下的问题，尤其是在稀疏奖励环境下。它通过结合一个用于时序差分学习的悲观演员和一个用于促进探索的乐观演员，并以它们的 Wasserstein 质心作为自适应探索策略来实现。实验结果表明，WBSAC 在 MuJoCo 连续控制任务上比现有最先进的算法更具样本效率。", "keywords": "深度强化学习, 离策略, 演员-评论家, Wasserstein 质心, 样本效率", "comments": "本文的创新之处在于利用 Wasserstein 质心融合悲观和乐观策略，从而实现有原则的有向探索。这种方法有效地解决了离策略强化学习中样本效率低下的关键挑战，对于提高连续控制任务的学习效率具有重要意义。"}}
{"id": "2506.10613", "title": "Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information", "authors": ["Henrik Sebastian Steude", "Alexander Diedrich", "Ingo Pill", "Lukas Moddemann", "Daniel Vranješ", "Oliver Niggemann"], "summary": "Diagnostic processes for complex cyber-physical systems often require\nextensive prior knowledge in the form of detailed system models or\ncomprehensive training data. However, obtaining such information poses a\nsignificant challenge. To address this issue, we present a new diagnostic\napproach that operates with minimal prior knowledge, requiring only a basic\nunderstanding of subsystem relationships and data from nominal operations. Our\nmethod combines a neural network-based symptom generator, which employs\nsubsystem-level anomaly detection, with a new graph diagnosis algorithm that\nleverages minimal causal relationship information between\nsubsystems-information that is typically available in practice. Our experiments\nwith fully controllable simulated datasets show that our method includes the\ntrue causal component in its diagnosis set for 82 p.c. of all cases while\neffectively reducing the search space in 73 p.c. of the scenarios. Additional\ntests on the real-world Secure Water Treatment dataset showcase the approach's\npotential for practical scenarios. Our results thus highlight our approach's\npotential for practical applications with large and complex cyber-physical\nsystems where limited prior knowledge is available.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10613v1", "AI": {"title_translation": "针对大型信息物理系统在先验信息最少情况下的数据驱动诊断", "tldr": "提出一种新的数据驱动诊断方法，针对大型信息物理系统，仅需少量先验信息，结合神经网络和图诊断算法，并在模拟和实际数据集上表现良好。", "motivation": "诊断复杂信息物理系统通常需要大量的先验知识（如详细系统模型或全面的训练数据），但获取这些信息非常困难。", "method": "该方法结合了一个基于神经网络的症状生成器（采用子系统级异常检测）和一个新的图诊断算法（利用子系统间最小的因果关系信息），仅需对子系统关系的基本理解和正常操作数据。", "result": "在完全可控的模拟数据集上，该方法在82%的案例中诊断集中包含真实的因果成分，并在73%的场景中有效减少了搜索空间。在真实世界的安全水处理数据集上的额外测试也展示了其在实际场景中的潜力。", "conclusion": "该方法在先验知识有限的大型复杂信息物理系统实际应用中具有巨大潜力。", "translation": "复杂信息物理系统的诊断过程通常需要大量的先验知识，形式为详细的系统模型或全面的训练数据。然而，获取此类信息提出了重大挑战。为了解决这个问题，我们提出了一种新的诊断方法，该方法在最少的先验知识下运行，仅需要对子系统关系的基本理解和来自正常操作的数据。我们的方法结合了一个基于神经网络的症状生成器，该生成器采用子系统级异常检测，以及一种新的图诊断算法，该算法利用子系统之间最小的因果关系信息——这些信息在实践中通常是可用的。我们对完全可控的模拟数据集进行的实验表明，我们的方法在82%的所有案例中，其诊断集包含真实的因果成分，同时在73%的场景中有效减少了搜索空间。在真实世界的安全水处理数据集上的额外测试展示了该方法在实际场景中的潜力。因此，我们的结果突出了该方法在先验知识有限的大型复杂信息物理系统实际应用中的潜力。", "summary": "本文提出了一种针对大型复杂信息物理系统的数据驱动诊断新方法，旨在解决传统诊断方法对大量先验知识的依赖。该方法仅需对子系统关系的基本理解和正常操作数据，通过结合基于神经网络的症状生成器（进行子系统级异常检测）和新的图诊断算法（利用最小因果关系信息）实现。实验证明，该方法在模拟数据集上能有效识别真实因果成分并减少搜索空间，并在实际数据集上展现出应用潜力，凸显了其在先验知识受限的复杂系统诊断中的实用性。", "keywords": "数据驱动诊断, 信息物理系统, 最小先验信息, 神经网络, 图诊断", "comments": "该论文的创新点在于其能够在先验信息极少的情况下对大型复杂信息物理系统进行诊断，这对于实际应用中数据和模型获取困难的场景具有重要意义。结合神经网络进行症状生成和图算法进行诊断，是解决这一挑战的有效途径。其在模拟和真实数据集上的验证也增强了方法的说服力。"}}
{"id": "2506.10713", "title": "Deep Learning-based Multi Project InP Wafer Simulation for Unsupervised Surface Defect Detection", "authors": ["Emílio Dolgener Cantú", "Rolf Klemens Wittmann", "Oliver Abdeen", "Patrick Wagner", "Wojciech Samek", "Moritz Baier", "Sebastian Lapuschkin"], "summary": "Quality management in semiconductor manufacturing often relies on template\nmatching with known golden standards. For Indium-Phosphide (InP) multi-project\nwafer manufacturing, low production scale and high design variability lead to\nsuch golden standards being typically unavailable. Defect detection, in turn,\nis manual and labor-intensive. This work addresses this challenge by proposing\na methodology to generate a synthetic golden standard using Deep Neural\nNetworks, trained to simulate photo-realistic InP wafer images from CAD data.\nWe evaluate various training objectives and assess the quality of the simulated\nimages on both synthetic data and InP wafer photographs. Our\ndeep-learning-based method outperforms a baseline decision-tree-based approach,\nenabling the use of a 'simulated golden die' from CAD plans in any user-defined\nregion of a wafer for more efficient defect detection. We apply our method to a\ntemplate matching procedure, to demonstrate its practical utility in surface\ndefect detection.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10713v1", "AI": {"title_translation": "基于深度学习的多项目InP晶圆模拟用于无监督表面缺陷检测", "tldr": "本文提出了一种基于深度学习的方法，通过从CAD数据生成合成的黄金标准图像来解决InP多项目晶圆制造中缺乏黄金标准导致的手动缺陷检测问题，并证明其在表面缺陷检测中的有效性。", "motivation": "在半导体制造中，特别是对于磷化铟（InP）多项目晶圆制造，由于生产规模小和设计变异性高，通常无法获得用于质量管理的“黄金标准”模板。这导致缺陷检测过程依赖于手动和劳动密集型操作，效率低下。", "method": "本文提出了一种使用深度神经网络生成合成黄金标准的方法。该网络通过CAD数据训练，以模拟逼真的InP晶圆图像。研究评估了不同的训练目标，并在合成数据和InP晶圆照片上评估了模拟图像的质量。该方法被应用于模板匹配过程，以实现缺陷检测。", "result": "本文提出的基于深度学习的方法优于基线决策树方法。它能够利用CAD计划中的“模拟黄金芯片”在晶圆的任何用户定义区域进行更高效的缺陷检测，并被证明在表面缺陷检测的模板匹配程序中具有实用性。", "conclusion": "通过使用深度学习生成合成的黄金标准，本文成功解决了InP多项目晶圆制造中缺乏真实黄金标准的问题，显著提高了缺陷检测的效率和自动化水平，为半导体质量管理提供了一种有效的解决方案。", "translation": "半导体制造中的质量管理通常依赖于与已知黄金标准的模板匹配。对于磷化铟（InP）多项目晶圆制造，低生产规模和高设计变异性导致通常无法获得此类黄金标准。因此，缺陷检测是手动且劳动密集型的。这项工作通过提出一种生成合成黄金标准的方法来解决这一挑战，该方法使用深度神经网络训练，以从CAD数据模拟逼真的InP晶圆图像。我们评估了各种训练目标，并在合成数据和InP晶圆照片上评估了模拟图像的质量。我们基于深度学习的方法优于基线决策树方法，使得能够利用CAD计划中的“模拟黄金芯片”在晶圆的任何用户定义区域进行更高效的缺陷检测。我们将我们的方法应用于模板匹配程序，以证明其在表面缺陷检测中的实际效用。", "summary": "本文针对InP多项目晶圆制造中缺乏“黄金标准”导致缺陷检测手动且劳动密集的问题，提出了一种基于深度学习的方法。该方法通过深度神经网络从CAD数据生成逼真的InP晶圆图像作为合成黄金标准。实验证明，该方法优于传统基线方法，能够利用模拟的黄金标准实现高效的无监督表面缺陷检测，并在模板匹配中展现出实用性。", "keywords": "深度学习, InP晶圆, 缺陷检测, 模拟黄金标准, 半导体制造", "comments": "本文的创新之处在于利用深度学习生成合成的“黄金标准”，解决了InP多项目晶圆制造中因生产规模小和高设计变异性而难以获取真实黄金标准的问题。这使得原本依赖手动和劳动密集型的缺陷检测过程得以自动化和高效化，对于提升半导体制造的质量管理具有重要意义。该方法通过模拟逼真图像，为无监督缺陷检测提供了新的途径，具有较好的泛化潜力。"}}
{"id": "2506.10146", "title": "Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors", "authors": ["Tejaswi Kasarla", "Max van Spengler", "Pascal Mettes"], "summary": "Out-of-distribution recognition forms an important and well-studied problem\nin deep learning, with the goal to filter out samples that do not belong to the\ndistribution on which a network has been trained. The conclusion of this paper\nis simple: a good hierarchical hyperbolic embedding is preferred for\ndiscriminating in- and out-of-distribution samples. We introduce Balanced\nHyperbolic Learning. We outline a hyperbolic class embedding algorithm that\njointly optimizes for hierarchical distortion and balancing between shallow and\nwide subhierarchies. We then use the class embeddings as hyperbolic prototypes\nfor classification on in-distribution data. We outline how to generalize\nexisting out-of-distribution scoring functions to operate with hyperbolic\nprototypes. Empirical evaluations across 13 datasets and 13 scoring functions\nshow that our hyperbolic embeddings outperform existing out-of-distribution\napproaches when trained on the same data with the same backbones. We also show\nthat our hyperbolic embeddings outperform other hyperbolic approaches, beat\nstate-of-the-art contrastive methods, and natively enable hierarchical\nout-of-distribution generalization.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10146v1", "AI": {"title_translation": "平衡双曲嵌入是天然的分布外检测器", "tldr": "本文提出平衡双曲学习，利用双曲嵌入作为原型，在多个数据集上显著优于现有分布外检测方法，并支持分层泛化。", "motivation": "深度学习中，过滤不属于训练数据分布的样本（分布外识别）是一个重要且被充分研究的问题。", "method": "引入平衡双曲学习，提出一种双曲类别嵌入算法，该算法联合优化分层失真和浅层与宽子层次之间的平衡。然后使用这些类别嵌入作为双曲原型进行分类，并推广现有分布外评分函数以与双曲原型一起操作。", "result": "在13个数据集和13个评分函数上的实证评估表明，当使用相同数据和相同骨干网络进行训练时，我们的双曲嵌入优于现有的分布外方法。它还优于其他双曲方法，击败了最先进的对比方法，并原生支持分层分布外泛化。", "conclusion": "一个好的分层双曲嵌入更适合区分分布内和分布外样本。", "translation": "分布外识别是深度学习中一个重要且被充分研究的问题，其目标是过滤掉不属于网络训练数据分布的样本。本文的结论很简单：一个好的分层双曲嵌入更适合区分分布内和分布外样本。我们引入了平衡双曲学习。我们概述了一种双曲类别嵌入算法，该算法联合优化分层失真以及浅层和宽子层次之间的平衡。然后，我们使用这些类别嵌入作为双曲原型，对分布内数据进行分类。我们概述了如何推广现有分布外评分函数以与双曲原型一起操作。在13个数据集和13个评分函数上的实证评估表明，当使用相同数据和相同骨干网络进行训练时，我们的双曲嵌入优于现有的分布外方法。我们还表明，我们的双曲嵌入优于其他双曲方法，击败了最先进的对比方法，并原生支持分层分布外泛化。", "summary": "本文提出了一种名为“平衡双曲学习”的新方法，用于分布外（OOD）识别。该方法利用双曲嵌入作为分类原型，并设计了一种双曲类别嵌入算法来优化分层结构。通过在多个数据集和评分函数上的广泛实验，证明了该方法在OOD检测方面优于现有方法、其他双曲方法以及最先进的对比方法，并且能够实现分层OOD泛化。", "keywords": "分布外检测, 双曲嵌入, 平衡双曲学习, 分层结构, 异常检测", "comments": "本文的创新点在于将双曲几何引入分布外检测领域，并提出了平衡双曲学习算法，有效地利用了双曲空间的层级结构特性。其重要性在于提供了一种新的、表现优异的OOD检测范式，尤其在处理具有层级关系的数据时具有天然优势。文章的结论清晰，实验结果支持其主张，展现了双曲嵌入在OOD问题上的巨大潜力。"}}
{"id": "2506.10960", "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark", "authors": ["Kangwei Liu", "Siyuan Cheng", "Bozhong Tian", "Xiaozhuan Liang", "Yuyang Yin", "Meng Han", "Ningyu Zhang", "Bryan Hooi", "Xi Chen", "Shumin Deng"], "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.", "comment": "Work in progress", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10960v1", "AI": {"title_translation": "中文有害内容检测基准：ChineseHarm-Bench", "tldr": "提出了一个名为ChineseHarm-Bench的中文有害内容检测基准，包含专业标注数据和知识规则库，并引入知识增强基线使小模型也能达到SOTA LLM性能。", "motivation": "现有有害内容检测资源主要集中在英文，中文数据集稀缺且范围有限，而大型语言模型在有害内容检测中的应用日益增加，因此需要一个全面的中文有害内容检测基准。", "method": "1. 构建了一个全面的、专业标注的中文有害内容检测基准，涵盖六个代表性类别，并完全基于真实世界数据。2. 通过标注过程，得到了一个知识规则库，提供明确的专家知识以辅助LLM进行中文有害内容检测。3. 提出了一个知识增强基线，该基线整合了人工标注的知识规则和来自大型语言模型的隐式知识。", "result": "提出的知识增强基线能够使较小的模型达到与最先进的大型语言模型相当的性能。", "conclusion": "ChineseHarm-Bench及其伴随的知识规则库和知识增强基线显著提升了中文有害内容检测的能力，特别是使得较小模型也能达到高性能，弥补了中文资源稀缺的空白。", "translation": "大型语言模型（LLMs）越来越多地应用于自动化有害内容检测任务，协助审核员识别政策违规并提高内容审查的整体效率和准确性。然而，现有有害内容检测资源主要集中在英文，中文数据集仍然稀缺且范围有限。我们提出了一个全面的、专业标注的中文内容危害检测基准，该基准涵盖了六个代表性类别，并且完全由真实世界数据构建。我们的标注过程进一步产生了一个知识规则库，该规则库提供了明确的专家知识，以协助LLMs进行中文有害内容检测。此外，我们提出了一个知识增强基线，该基线整合了人工标注的知识规则和来自大型语言模型的隐式知识，使较小的模型能够达到与最先进的大型语言模型相当的性能。代码和数据可在https://github.com/zjunlp/ChineseHarm-bench获取。", "summary": "本文推出了ChineseHarm-Bench，一个专门用于中文有害内容检测的综合基准数据集。该基准包含六个类别的真实世界数据，并伴随一个由专业标注产生的知识规则库。研究者还提出了一种知识增强基线方法，该方法结合了人工规则和LLM的隐式知识，使得较小模型也能实现与顶尖LLM相媲美的有害内容检测性能，有效弥补了中文有害内容检测资源的不足。", "keywords": "中文有害内容检测, 基准数据集, 知识规则库, 大型语言模型, 知识增强", "comments": "这篇论文通过构建一个高质量的中文有害内容检测基准和知识规则库，有效填补了中文领域资源的空白。其创新之处在于提出了知识增强基线，使得小型模型也能达到与大型模型相当的性能，这对于资源受限的场景具有重要意义，降低了部署高性能有害内容检测系统的门槛。"}}
{"id": "2506.10292", "title": "Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages", "authors": ["Ali Almutairi", "Abdullah Alsuhaibani", "Shoaib Jameel", "Usman Naseem", "Gelareh Mohammadi", "Imran Razzak"], "summary": "Training deep learning networks with minimal supervision has gained\nsignificant research attention due to its potential to reduce reliance on\nextensive labelled data. While self-training methods have proven effective in\nsemi-supervised learning, they remain vulnerable to errors from noisy pseudo\nlabels. Moreover, most recent approaches to the few-label classification\nproblem are either designed for resource-rich languages such as English or\ninvolve complex cascading models that are prone to overfitting. To address the\npersistent challenge of few-label text classification in truly low-resource\nlinguistic contexts, where existing methods often struggle with noisy\npseudo-labels and domain adaptation, we propose Flick. Unlike prior methods\nthat rely on generic multi-cluster pseudo-labelling or complex cascading\narchitectures, Flick leverages the fundamental insight that distilling\nhigh-confidence pseudo-labels from a broader set of initial clusters can\ndramatically improve pseudo-label quality, particularly for linguistically\ndiverse, low-resource settings. Flick introduces a novel pseudo-label\nrefinement component, a departure from traditional pseudo-labelling strategies\nby identifying and leveraging top-performing pseudo-label clusters. This\ncomponent specifically learns to distil highly reliable pseudo-labels from an\ninitial broad set by focusing on single-cluster cohesion and leveraging an\nadaptive top-k selection mechanism. This targeted refinement process is crucial\nfor mitigating the propagation of errors inherent in low-resource data,\nallowing for robust fine-tuning of pre-trained language models with only a\nhandful of true labels. We demonstrate Flick's efficacy across 14 diverse\ndatasets, encompassing challenging low-resource languages such as Arabic, Urdu,\nand Setswana, alongside English, showcasing its superior performance and\nadaptability.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10292v1", "AI": {"title_translation": "Flick：在多任务低资源语言中使用 K 感知中间学习的少量标签文本分类", "tldr": "Flick 提出了一种新的伪标签提炼方法，用于解决低资源语言中少量标签文本分类的挑战，通过识别和利用表现最佳的伪标签簇来提高伪标签质量。", "motivation": "现有方法在低资源语言的少量标签文本分类中表现不佳，原因在于易受嘈杂伪标签的错误影响、主要为资源丰富的语言设计或涉及易过拟合的复杂级联模型。", "method": "Flick 提出了一种新颖的伪标签提炼组件，通过关注单簇凝聚力和利用自适应的 top-k 选择机制，从初始广泛的簇中提炼出高置信度的伪标签。这种有针对性的提炼过程有助于减轻低资源数据中固有的错误传播，从而能够使用少量真实标签对预训练语言模型进行鲁棒的微调。", "result": "Flick 在包括阿拉伯语、乌尔都语和茨瓦纳语等具有挑战性的低资源语言以及英语在内的 14 个不同数据集上展示了其有效性，证明了其卓越的性能和适应性。", "conclusion": "Flick 通过改进伪标签质量的新型提炼过程，有效解决了低资源语言中少量标签文本分类的挑战，从而实现了鲁棒的性能。", "translation": "训练深度学习网络以最少的监督获得了显著的研究关注，因为它有可能减少对大量标记数据的依赖。虽然自训练方法在半监督学习中被证明是有效的，但它们仍然容易受到嘈杂伪标签的错误影响。此外，大多数最近解决少量标签分类问题的方法要么是为英语等资源丰富的语言设计的，要么涉及容易过拟合的复杂级联模型。为了解决在真正的低资源语言环境中少量标签文本分类的持续挑战，现有方法在嘈杂伪标签和领域适应方面常常面临困难，我们提出了 Flick。与依赖通用多簇伪标签或复杂级联架构的现有方法不同，Flick 利用了基本见解：从更广泛的初始簇中提炼高置信度伪标签可以显著提高伪标签质量，特别是在语言多样性、低资源的设置中。Flick 引入了一个新颖的伪标签提炼组件，通过识别和利用表现最佳的伪标签簇，这与传统的伪标签策略不同。该组件专门学习通过关注单簇凝聚力和利用自适应的 top-k 选择机制，从最初的广泛集合中提炼出高度可靠的伪标签。这种有针对性的提炼过程对于减轻低资源数据中固有的错误传播至关重要，从而能够使用少量真实标签对预训练语言模型进行鲁棒的微调。我们在 14 个不同数据集上展示了 Flick 的功效，这些数据集涵盖了挑战性的低资源语言，如阿拉伯语、乌尔都语和茨瓦纳语，以及英语，展示了其卓越的性能和适应性。", "summary": "Flick 提出了一种新颖的伪标签提炼组件，旨在解决低资源语言中少量标签文本分类的挑战。该方法通过关注单簇凝聚力和自适应的 top-k 选择机制，从初始广泛的簇中提炼出高置信度的伪标签，从而显著提高伪标签质量并减轻错误传播。这使得能够使用少量真实标签对预训练语言模型进行鲁棒的微调。Flick 在包括阿拉伯语、乌尔都语和茨瓦纳语在内的 14 个多样化数据集上表现出卓越的性能和适应性。", "keywords": "少量标签文本分类, 低资源语言, 伪标签提炼, K 感知学习, 半监督学习", "comments": "该论文的创新点在于其针对低资源语言的伪标签提炼方法，通过 K 感知中间学习和识别高置信度伪标签簇，有效解决了现有方法在嘈杂伪标签和过拟合方面的局限性，对低资源自然语言处理领域具有重要意义。"}}
{"id": "2506.10302", "title": "Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation", "authors": ["Hamzeh Asgharnezhad", "Pegah Tabarisaadi", "Abbas Khosravi", "Roohallah Alizadehsani", "U. Rajendra Acharya"], "summary": "Accurate and reliable skin cancer diagnosis is critical for early treatment\nand improved patient outcomes. Deep learning (DL) models have shown promise in\nautomating skin cancer classification, but their performance can be limited by\ndata scarcity and a lack of uncertainty awareness. In this study, we present a\ncomprehensive evaluation of DL-based skin lesion classification using transfer\nlearning and uncertainty quantification (UQ) on the HAM10000 dataset. In the\nfirst phase, we benchmarked several pre-trained feature extractors-including\nContrastive Language-Image Pretraining (CLIP) variants, Residual Network-50\n(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual\nGeometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range\nof traditional classifiers such as Support Vector Machine (SVM), eXtreme\nGradient Boosting (XGBoost), and logistic regression. Our results show that\nCLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,\ndeliver the highest classification performance. In the second phase, we\nincorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte\nCarlo Dropout (EMCD) to assess not only prediction accuracy but also the\nreliability of model outputs. We evaluated these models using uncertainty-aware\nmetrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),\nuncertainty specificity(USpe), and uncertainty precision(UPre). The results\ndemonstrate that ensemble methods offer a good trade-off between accuracy and\nuncertainty handling, while EMCD is more sensitive to uncertain predictions.\nThis study highlights the importance of integrating UQ into DL-based medical\ndiagnosis to enhance both performance and trustworthiness in real-world\nclinical applications.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10302v1", "AI": {"title_translation": "不确定性感知深度学习在自动化皮肤癌分类中的应用：一项综合评估", "tldr": "本研究对基于深度学习的皮肤癌分类进行了全面评估，结合迁移学习和不确定性量化，发现CLIP-based模型表现最佳，集成方法在准确性和不确定性处理之间取得良好平衡。", "motivation": "尽管深度学习模型在自动化皮肤癌分类中显示出前景，但其性能受限于数据稀缺和缺乏不确定性感知。", "method": "本研究分两阶段进行：第一阶段，基准测试了多种预训练特征提取器（如CLIP变体、ResNet50等）与传统分类器（如SVM、XGBoost等）的组合，用于皮肤病变分类。第二阶段，引入不确定性量化（UQ），使用蒙特卡洛Dropout (MCD)、集成方法和集成蒙特卡洛Dropout (EMCD) 来评估预测准确性和模型输出的可靠性，并使用不确定性感知指标进行评估。", "result": "第一阶段结果显示，基于CLIP的视觉Transformer，特别是LAION CLIP ViT-H/14与SVM结合，提供了最高的分类性能。第二阶段结果表明，集成方法在准确性和不确定性处理之间提供了良好的权衡，而EMCD对不确定预测更敏感。", "conclusion": "本研究强调了将不确定性量化整合到基于深度学习的医学诊断中的重要性，以提高实际临床应用中的性能和可信度。", "translation": "准确可靠的皮肤癌诊断对于早期治疗和改善患者预后至关重要。深度学习（DL）模型在自动化皮肤癌分类方面已显示出前景，但其性能可能受到数据稀缺和缺乏不确定性感知的限制。在本研究中，我们利用迁移学习和不确定性量化（UQ）在HAM10000数据集上对基于DL的皮肤病变分类进行了全面评估。在第一阶段，我们对几种预训练的特征提取器进行了基准测试——包括对比语言-图像预训练（CLIP）变体、残差网络-50（ResNet50）、密集连接卷积网络（DenseNet121）、视觉几何组网络（VGG16）和EfficientNet-V2-Large——并结合了一系列传统分类器，如支持向量机（SVM）、极端梯度提升（XGBoost）和逻辑回归。我们的结果表明，基于CLIP的视觉Transformer，特别是LAION CLIP ViT-H/14与SVM结合，提供了最高的分类性能。在第二阶段，我们使用蒙特卡洛Dropout（MCD）、集成方法和集成蒙特卡洛Dropout（EMCD）引入了UQ，不仅评估了预测准确性，还评估了模型输出的可靠性。我们使用不确定性感知指标评估了这些模型，例如不确定性准确率（UAcc）、不确定性敏感度（USen）、不确定性特异度（USpe）和不确定性精度（UPre）。结果表明，集成方法在准确性和不确定性处理之间提供了良好的权衡，而EMCD对不确定预测更敏感。本研究强调了将UQ整合到基于DL的医学诊断中的重要性，以提高实际临床应用中的性能和可信度。", "summary": "本研究对基于深度学习的皮肤癌分类进行了全面评估，旨在解决数据稀缺和不确定性感知不足的问题。研究分两阶段进行：第一阶段基准测试了多种预训练特征提取器和传统分类器，发现CLIP-based视觉Transformer表现最佳；第二阶段引入了不确定性量化方法（MCD、Ensemble、EMCD），以评估模型可靠性。结果表明，集成方法在准确性和不确定性处理之间取得良好平衡，而EMCD对不确定预测更敏感。研究强调了将不确定性量化整合到深度学习医学诊断中的重要性，以增强其在临床应用中的性能和可信度。", "keywords": "皮肤癌分类, 深度学习, 不确定性量化, 迁移学习, CLIP", "comments": "这项研究的创新之处在于其对不确定性感知深度学习在皮肤癌分类中的全面评估，特别是结合了迁移学习和不确定性量化。它不仅比较了多种先进的特征提取器，还深入探讨了模型预测的可靠性，这对于医疗诊断应用至关重要。强调UQ在提高模型可信度方面的作用，使其在实际临床应用中更具价值。"}}
{"id": "2506.10935", "title": "Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials", "authors": ["Ekaterina Grishina", "Matvey Smirnov", "Maxim Rakhuba"], "summary": "The problem of computing optimal orthogonal approximation to a given matrix\nhas attracted growing interest in machine learning. Notable applications\ninclude the recent Muon optimizer or Riemannian optimization on the Stiefel\nmanifold. Among existing approaches, the Newton-Schulz iteration has emerged as\na particularly effective solution, as it relies solely on matrix\nmultiplications and thus achieves high computational efficiency on GPU\nhardware. Despite its efficiency, the method has inherent limitations - its\ncoefficients are fixed and thus not optimized for a given matrix. In this paper\nwe address this issue by proposing a Chebyshev-optimized version of\nNewton-Schulz (CANS). Based on the Chebyshev's alternance theorem, we\ntheoretically derive optimal coefficients for the 3-rd order Newton-Schulz\niteration and apply a Remez algorithm to compute optimal higher-degree\npolynomials. We leverage these polynomials to construct controlled approximate\northogonalization schemes, which is of interest in deep learning applications.\nPractically, we demonstrate the method's effectiveness in two key applications:\northogonalization in the Muon optimizer, and providing an efficient retraction\nalternative for Riemannian optimization on the Stiefel manifold.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10935v1", "AI": {"title_translation": "通过切比雪夫型多项式加速牛顿-舒尔茨正交迭代", "tldr": "本文提出了一种基于切比雪夫多项式优化的牛顿-舒尔茨迭代（CANS），解决了其系数固定不优化的限制，并在机器学习应用中展示了其有效性。", "motivation": "现有牛顿-舒尔茨迭代在矩阵正交近似计算中虽然高效，但其系数是固定的，未针对特定矩阵进行优化，限制了其性能。", "method": "论文提出了切比雪夫优化版的牛顿-舒尔茨（CANS）方法，通过切比雪夫交错定理理论推导了三阶牛顿-舒尔茨迭代的最优系数，并利用Remez算法计算更高阶的最优多项式，从而构建受控的近似正交化方案。", "result": "CANS方法在Muon优化器中的正交化以及作为Stiefel流形上黎曼优化的有效收缩替代方案中，均表现出实际的有效性。", "conclusion": "本文成功提出并验证了CANS方法，通过优化牛顿-舒尔茨迭代的系数，显著提升了矩阵正交近似计算的效率和灵活性，为深度学习和黎曼优化提供了新的高效工具。", "translation": "矩阵最优正交近似计算问题在机器学习领域引起了日益增长的兴趣。显著的应用包括最近的Muon优化器或Stiefel流形上的黎曼优化。在现有方法中，牛顿-舒尔茨迭代已成为一种特别有效的解决方案，因为它仅依赖于矩阵乘法，因此在GPU硬件上实现了高计算效率。尽管效率高，但该方法存在固有的局限性——其系数是固定的，因此未针对给定矩阵进行优化。在本文中，我们通过提出切比雪夫优化版牛顿-舒尔茨（CANS）来解决这个问题。基于切比雪夫交错定理，我们理论推导了三阶牛顿-舒尔茨迭代的最优系数，并应用Remez算法计算最优的高阶多项式。我们利用这些多项式来构建受控的近似正交化方案，这在深度学习应用中具有重要意义。实际上，我们在两个关键应用中展示了该方法的有效性：Muon优化器中的正交化，以及为Stiefel流形上的黎曼优化提供高效的收缩替代方案。", "summary": "本文针对牛顿-舒尔茨迭代在矩阵正交近似计算中系数固定、非最优的问题，提出了一种切比雪夫优化版的牛顿-舒尔茨（CANS）方法。该方法利用切比雪夫交错定理推导最优系数，并通过Remez算法计算高阶最优多项式，从而构建高效且受控的近似正交化方案。实验证明，CANS在Muon优化器和Stiefel流形上的黎曼优化中均表现出显著的有效性。", "keywords": "牛顿-舒尔茨迭代, 正交化, 切比雪夫多项式, 矩阵近似, 机器学习", "comments": "本文的创新点在于将切比雪夫多项式引入牛顿-舒尔茨迭代，通过优化迭代系数克服了传统方法的固有局限性。这种方法不仅提升了计算效率，还为深度学习和黎曼优化等领域提供了更灵活、更精确的正交化工具，具有重要的理论和实践意义。"}}
{"id": "2506.10424", "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks", "authors": ["Kaiyuan Zhang", "Siyuan Cheng", "Hanxi Guo", "Yuetian Chen", "Zian Su", "Shengwei An", "Yuntao Du", "Charles Fleming", "Ashish Kundu", "Xiangyu Zhang", "Ninghui Li"], "summary": "Large language models (LLMs) have achieved remarkable success and are widely\nadopted for diverse applications. However, fine-tuning these models often\ninvolves private or sensitive information, raising critical privacy concerns.\nIn this work, we conduct the first comprehensive study evaluating the\nvulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our\nempirical analysis demonstrates that MIAs exploit the loss reduction during\nfine-tuning, making them highly effective in revealing membership information.\nThese findings motivate the development of our defense. We propose SOFT\n(\\textbf{S}elective data \\textbf{O}bfuscation in LLM\n\\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates\nprivacy leakage by leveraging influential data selection with an adjustable\nparameter to balance utility preservation and privacy protection. Our extensive\nexperiments span six diverse domains and multiple LLM architectures and scales.\nResults show that SOFT effectively reduces privacy risks while maintaining\ncompetitive model performance, offering a practical and scalable solution to\nsafeguard sensitive information in fine-tuned LLMs.", "comment": "Accepted by the 34th USENIX Security Symposium 2025. Code is\n  available at https://github.com/KaiyuanZh/SOFT", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10424v1", "AI": {"title_translation": "SOFT：选择性数据混淆以保护LLM微调免受成员推断攻击", "tldr": "本文首次全面评估了微调LLM对成员推断攻击（MIA）的脆弱性，并提出了SOFT，一种通过选择性数据混淆来有效降低隐私风险同时保持模型性能的防御技术。", "motivation": "大型语言模型（LLM）在微调过程中经常涉及私人或敏感信息，这引发了关键的隐私问题。本文旨在评估微调LLM对成员推断攻击（MIA）的脆弱性，并开发一种防御机制来缓解隐私泄露。", "method": "本文提出了SOFT（LLM微调中的选择性数据混淆），这是一种新颖的防御技术。它通过利用有影响力的数据选择和可调参数来平衡效用保持和隐私保护，从而减轻隐私泄露。", "result": "实验结果表明，SOFT在有效降低隐私风险的同时，保持了具有竞争力的模型性能。它在六个不同领域和多种LLM架构及规模上进行了广泛的实验。", "conclusion": "SOFT提供了一种实用且可扩展的解决方案，用于保护微调LLM中的敏感信息，有效降低了隐私风险并保持了模型性能。", "translation": "大型语言模型（LLM）取得了显著成功，并被广泛应用于各种应用。然而，微调这些模型通常涉及私人或敏感信息，引发了关键的隐私担忧。在这项工作中，我们首次进行了全面研究，评估了微调LLM对成员推断攻击（MIA）的脆弱性。我们的实证分析表明，MIA利用微调过程中的损失降低，使其在揭示成员信息方面非常有效。这些发现促使我们开发防御措施。我们提出了SOFT（LLM微调中的选择性数据混淆），这是一种新颖的防御技术，通过利用有影响力的数据选择和可调参数来平衡效用保持和隐私保护，从而减轻隐私泄露。我们广泛的实验涵盖了六个不同领域和多种LLM架构和规模。结果表明，SOFT在有效降低隐私风险的同时，保持了具有竞争力的模型性能，为保护微调LLM中的敏感信息提供了一种实用且可扩展的解决方案。", "summary": "本文首次全面研究了微调大型语言模型（LLM）对成员推断攻击（MIA）的脆弱性。研究发现MIA利用微调过程中的损失降低来有效揭示成员信息。为应对此问题，作者提出了SOFT（LLM微调中的选择性数据混淆），这是一种新颖的防御技术，通过选择有影响力的数据并调节参数来平衡隐私保护和模型效用。广泛的实验证明，SOFT能够有效降低隐私风险，同时保持模型性能，为保护微调LLM中的敏感数据提供了一个实用且可扩展的解决方案。", "keywords": "成员推断攻击, LLM微调, 隐私保护, 数据混淆, SOFT", "comments": "这项工作首次全面评估了微调LLM对成员推断攻击的脆弱性，填补了现有研究空白。其提出的SOFT方法通过选择性数据混淆，巧妙地在隐私保护和模型效用之间取得平衡，具有实际应用价值和可扩展性。这项研究对于提高LLM在敏感数据场景下的安全性至关重要。"}}
{"id": "2506.10770", "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models", "authors": ["Joran Leest", "Claudia Raibulet", "Patricia Lago", "Ilias Gerostathopoulos"], "summary": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10770v1", "AI": {"title_translation": "从茶叶到系统地图：操作机器学习模型监控中的上下文感知", "tldr": "生产中的ML模型失败是由于上下文错位而非统计异常。现有监控缺乏上下文理解。本文通过系统性综述，提出了C-SAR框架和20种模式，将ML监控从统计观察提升为系统地图管理，以实现可靠监控。", "motivation": "生产环境中的机器学习模型失败并非源于输入数据的统计异常，而是由于环境与训练假设偏离导致的上下文错位。尽管ML监控研究广泛，但缺乏如何有效利用上下文信息的共享理解，导致难以从统计漂移检测转向有意义的警报和系统性根本原因分析。", "method": "本文进行了一项系统性综述，分析了数据挖掘、数据库、软件工程和机器学习领域94项主要研究，旨在表征和构建该领域中各种类型的上下文信息。在此基础上，提出了一个概念模型——上下文系统-方面-表示（C-SAR）框架。", "result": "识别并提出了上下文系统-方面-表示（C-SAR）框架，这是一个综合研究发现的概念模型。此外，还识别了20种重复出现且可能可重用的特定系统、方面和表示组合模式，并将它们映射到所支持的监控活动。", "conclusion": "该研究为ML监控提供了一个新视角：从解释观测统计数据的“茶叶”转变为构建和管理“系统地图”，从而实现系统化、可靠的ML监控实践。", "translation": "生产中的机器学习（ML）模型并非因输入数据中的统计异常而失败；它们是由于上下文错位而失败——当其环境偏离训练假设时，导致不可靠的预测。有效的ML监控需要丰富的上下文信息，才能超越统计偏移检测，转向有意义的警报和系统性的根本原因分析。然而，令人惊讶的是，尽管在ML监控及相关学科（漂移检测、数据验证、域外检测）进行了广泛研究，但对于如何使用上下文信息却没有共享的理解——考虑到监控涉及对信息在上下文中的解释，这一点令人震惊。为此，本文提出了一项系统性综述，旨在表征和构建该领域中各种类型的上下文信息。我们的分析考察了数据挖掘、数据库、软件工程和ML领域的94项主要研究。我们引入了上下文系统-方面-表示（C-SAR）框架，这是一个综合我们研究结果的概念模型。我们还识别了20种重复出现且可能可重用的特定系统、方面和表示组合模式，并将它们映射到它们支持的监控活动。这项研究为ML监控提供了一个新视角：从解释观测统计数据的“茶叶”转变为构建和管理“系统地图”，从而实现系统化、可靠的ML监控实践。", "summary": "本文通过对94项研究的系统性综述，探讨了生产环境中机器学习模型失败的根本原因在于上下文错位而非统计异常。为解决ML监控中缺乏上下文信息共享理解的问题，作者提出了上下文系统-方面-表示（C-SAR）框架，并识别了20种可重用的上下文模式，旨在将ML监控从简单的统计漂移检测提升为基于系统地图的、更具系统性和可靠性的实践。", "keywords": "机器学习监控, 上下文感知, 系统性综述, C-SAR框架, 模型失败", "comments": "这篇论文的创新点在于它明确指出了生产ML模型失败的核心问题是“上下文错位”，而非传统关注的统计异常。通过系统性综述，它提出了一个全新的概念框架（C-SAR）和具体的模式，为ML监控提供了一种更全面、更具解释性的方法，超越了单纯的漂移检测，有助于实现更可靠的根本原因分析。这对于MLOps实践具有重要指导意义。"}}
{"id": "2506.10826", "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System", "authors": ["Wenxuan Song", "Jiayi Chen", "Wenxue Li", "Xu He", "Han Zhao", "Pengxiang Ding Shiyan Su", "Feilong Tang", "Xuelian Cheng", "Donglin Wang", "Zongyuan Ge", "Xinhu Zheng", "Zhe Liu", "Hesheng Wang", "Yunhui Liu", "Haoang Li"], "summary": "A fundamental requirement for real-world robotic deployment is the ability to\nunderstand and respond to natural language instructions. Existing\nlanguage-conditioned manipulation tasks typically assume that instructions are\nperfectly aligned with the environment. This assumption limits robustness and\ngeneralization in realistic scenarios where instructions may be ambiguous,\nirrelevant, or infeasible. To address this problem, we introduce RAtional\nMAnipulation (RAMA), a new benchmark that challenges models with both unseen\nexecutable instructions and defective ones that should be rejected. In RAMA, we\nconstruct a dataset with over 14,000 samples, including diverse defective\ninstructions spanning six dimensions: visual, physical, semantic, motion,\nsafety, and out-of-context. We further propose the Rational\nVision-Language-Action model (RationalVLA). It is a dual system for robotic\narms that integrates the high-level vision-language model with the low-level\nmanipulation policy by introducing learnable latent space embeddings. This\ndesign enables RationalVLA to reason over instructions, reject infeasible\ncommands, and execute manipulation effectively. Experiments demonstrate that\nRationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher\nsuccess rate and 0.94 average task length, while maintaining competitive\nperformance on standard manipulation tasks. Real-world trials further validate\nits effectiveness and robustness in practical applications. Our project page is\nhttps://irpn-eai.github.io/rationalvla.", "comment": "14 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10826v1", "AI": {"title_translation": "RationalVLA: 一种双系统理性视觉-语言-动作模型", "tldr": "RationalVLA提出了一种双系统视觉-语言-动作模型，通过引入RAMA基准测试来处理机器人操作中模糊、不相关或不可行的指令，并在该基准上表现优异，提高了成功率和任务效率。", "motivation": "现有语言条件下的操作任务通常假设指令与环境完美对齐，这限制了机器人在指令可能模糊、不相关或不可行等现实场景中的鲁棒性和泛化能力。", "method": "为解决现有模型的局限性，作者提出了RAtional MAnipulation (RAMA) 基准测试，其中包含超过14,000个样本，包括可执行指令和六个维度（视觉、物理、语义、运动、安全、上下文无关）的缺陷指令。在此基础上，提出了Rational Vision-Language-Action (RationalVLA) 模型，它是一个用于机械臂的双系统，通过引入可学习的潜在空间嵌入，将高级视觉-语言模型与低级操作策略相结合。这种设计使RationalVLA能够对指令进行推理，拒绝不可行的命令，并有效执行操作。", "result": "实验证明，RationalVLA在RAMA基准测试上的成功率比现有最先进的基线模型高出14.5%，平均任务长度为0.94，同时在标准操作任务上保持了有竞争力的性能。实际世界试验进一步验证了其在实际应用中的有效性和鲁棒性。", "conclusion": "RationalVLA模型通过其双系统设计和处理缺陷指令的能力，显著提高了机器人在复杂、不确定环境中的语言理解和操作鲁棒性。", "translation": "在现实世界中部署机器人的一项基本要求是理解和响应自然语言指令的能力。现有语言条件下的操作任务通常假设指令与环境完美对齐。这一假设限制了在指令可能模糊、不相关或不可行等现实场景中的鲁棒性和泛化能力。为了解决这个问题，我们引入了理性操作（RAtional MAnipulation, RAMA），这是一个新的基准测试，它挑战模型处理未见的、可执行的指令以及应该被拒绝的有缺陷的指令。在RAMA中，我们构建了一个包含14,000多个样本的数据集，其中包括跨越六个维度（视觉、物理、语义、运动、安全和上下文无关）的各种缺陷指令。我们进一步提出了理性视觉-语言-动作模型（Rational Vision-Language-Action model, RationalVLA）。它是一个用于机械臂的双系统，通过引入可学习的潜在空间嵌入，将高级视觉-语言模型与低级操作策略相结合。这种设计使RationalVLA能够对指令进行推理，拒绝不可行的命令，并有效执行操作。实验表明，RationalVLA在RAMA上的成功率比现有最先进的基线模型高出14.5%，平均任务长度为0.94，同时在标准操作任务上保持了有竞争力的性能。实际世界试验进一步验证了其在实际应用中的有效性和鲁棒性。我们的项目页面是https://irpn-eai.github.io/rationalvla。", "summary": "本文提出RationalVLA，一个双系统视觉-语言-动作模型，旨在解决机器人操作中自然语言指令的模糊性和不可行性问题。为此，引入了RAMA基准测试，包含大量缺陷指令。RationalVLA通过整合高层视觉-语言模型和低层操作策略，实现对指令的推理、拒绝不可行命令和有效操作。实验证明，RationalVLA在RAMA基准测试上显著优于现有SOTA模型，并在实际应用中展现出鲁棒性。", "keywords": "视觉-语言-动作, 双系统, 机器人操作, 指令理解, RAMA基准", "comments": "本文的创新点在于提出了RAMA基准测试，专门用于评估模型处理缺陷指令的能力，这弥补了现有基准的不足。同时，RationalVLA的双系统架构，尤其是其对指令进行推理和拒绝不可行命令的能力，对于提高机器人在复杂现实环境中的鲁棒性和泛化能力具有重要意义。该研究为实现更智能、更可靠的机器人部署奠定了基础。"}}
{"id": "2506.10674", "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving", "authors": ["Vincenzo Colle", "Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Fadhel Ayed", "Merouane Debbah"], "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.", "comment": "6 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10674v1", "AI": {"title_translation": "TeleMath: 电信数学问题解决中大型语言模型的基准", "tldr": "本文介绍了TeleMath，这是首个用于评估大型语言模型（LLMs）在电信领域数学问题解决能力的基准数据集（包含500个问答对）。研究发现，专门为数学或逻辑推理设计的LLMs表现最佳，而通用LLMs则表现不佳。数据集和评估代码已发布。", "motivation": "尽管大型语言模型（LLMs）在一般数学推理方面有所进步，但它们在电信领域（如信号处理、网络优化、性能分析）等专业且数学密集型任务中的有效性尚未得到充分探索。目前缺乏专门的基准来评估LLMs在这些领域解决数学问题的能力。", "method": "作者引入了TeleMath，这是一个包含500个问答对（QnA）的基准数据集，由主题专家精心设计，旨在评估LLMs在电信领域解决数值数学问题的性能。论文详细介绍了QnA的生成流程。随后，他们使用该基准评估了多种开源大型语言模型。", "result": "评估结果显示，专门为数学或逻辑推理设计的最新大型语言模型在TeleMath基准上表现最佳。相比之下，即使是参数量庞大的通用大型语言模型，在面对这些电信领域的专业数学挑战时也常常表现不佳。", "conclusion": "专门为数学或逻辑推理优化的大型语言模型在解决电信领域特定数学问题上的表现优于通用大型语言模型。TeleMath基准的发布将有助于推动该领域的未来研究。", "translation": "随着人工智能在电信领域的日益普及，人们对大型语言模型（LLMs）解决领域特定、数学密集型任务的能力越来越感兴趣。尽管最近的进展提高了LLMs在一般数学推理方面的性能，但它们在信号处理、网络优化和性能分析等专业领域中的有效性在很大程度上仍未被探索。为了弥补这一空白，我们引入了TeleMath，这是第一个专门设计用于评估LLMs在电信领域解决数值数学问题的性能的基准数据集。TeleMath包含500个问答（QnA）对，涵盖了电信领域的广泛主题。本文概述了所提出的QnA生成管道，该管道从由主题专家精心制作的精选问题种子开始。对各种开源LLMs的评估表明，在TeleMath上表现最佳的是那些明确为数学或逻辑推理而设计的最新模型。相比之下，通用模型，即使参数数量庞大，也常常难以应对这些挑战。我们已经发布了数据集和评估代码，以方便结果复现并支持未来的研究。", "summary": "本文介绍了TeleMath，这是首个专门用于评估大型语言模型（LLMs）在电信领域解决数学问题能力的基准数据集。该研究旨在弥补LLMs在信号处理、网络优化等专业数学密集型电信任务中性能评估的空白。TeleMath包含500个由主题专家构建的问答对。对多种开源LLMs的评估表明，专门为数学或逻辑推理设计的模型表现优异，而通用模型则面临挑战。为促进研究和复现性，数据集和评估代码已公开发布。", "keywords": "大型语言模型, 电信, 数学推理, 基准, 数据集", "comments": "该论文通过创建TeleMath基准数据集，填补了大型语言模型在电信领域特定数学问题解决能力评估方面的关键空白，具有重要意义。这一创新性贡献为评估LLMs在专业技术领域的实际应用提供了一个标准化工具。研究结果表明，针对数学或逻辑推理进行优化的模型表现更佳，这强调了在实际AI部署中对领域特定微调或架构的需求。数据集和代码的发布进一步增强了其影响力，促进了研究的可复现性和合作。"}}
{"id": "2506.10159", "title": "Probabilistic Variational Contrastive Learning", "authors": ["Minoh Jeong", "Seonho Kim", "Alfred Hero"], "summary": "Deterministic embeddings learned by contrastive learning (CL) methods such as\nSimCLR and SupCon achieve state-of-the-art performance but lack a principled\nmechanism for uncertainty quantification. We propose Variational Contrastive\nLearning (VCL), a decoder-free framework that maximizes the evidence lower\nbound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction\nterm and adding a KL divergence regularizer to a uniform prior on the unit\nhypersphere. We model the approximate posterior $q_\\theta(z|x)$ as a projected\nnormal distribution, enabling the sampling of probabilistic embeddings. Our two\ninstantiations--VSimCLR and VSupCon--replace deterministic embeddings with\nsamples from $q_\\theta(z|x)$ and incorporate a normalized KL term into the\nloss. Experiments on multiple benchmarks demonstrate that VCL mitigates\ndimensional collapse, enhances mutual information with class labels, and\nmatches or outperforms deterministic baselines in classification accuracy, all\nthe while providing meaningful uncertainty estimates through the posterior\nmodel. VCL thus equips contrastive learning with a probabilistic foundation,\nserving as a new basis for contrastive approaches.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10159v1", "AI": {"title_translation": "概率变分对比学习", "tldr": "VCL为对比学习引入了概率基础，解决了确定性嵌入缺乏不确定性量化的问题，并通过最大化ELBO实现，在多个基准测试中表现良好并提供有意义的不确定性估计。", "motivation": "现有的对比学习（CL）方法（如SimCLR和SupCon）学习到的确定性嵌入达到了最先进的性能，但缺乏一种原则性的机制来进行不确定性量化。", "method": "本文提出了变分对比学习（VCL），一个无解码器框架，通过将InfoNCE损失解释为替代重建项，并向单位超球面上的均匀先验添加KL散度正则化器来最大化证据下界（ELBO）。VCL将近似后验$q_\\theta(z|x)$建模为投影正态分布，从而能够采样概率嵌入。其两个实例化——VSimCLR和VSupCon——用来自$q_\\theta(z|x)$的样本替换确定性嵌入，并将归一化的KL项纳入损失函数。", "result": "实验表明，VCL减轻了维度坍塌，增强了与类别标签的互信息，并在分类准确性方面与确定性基线持平或超越，同时通过后验模型提供了有意义的不确定性估计。", "conclusion": "VCL为对比学习提供了概率基础，可作为对比方法的新基础。", "translation": "对比学习（CL）方法如SimCLR和SupCon学习到的确定性嵌入达到了最先进的性能，但缺乏一种原则性的机制来进行不确定性量化。我们提出了变分对比学习（VCL），一个无解码器框架，通过将InfoNCE损失解释为替代重建项，并向单位超球面上的均匀先验添加KL散度正则化器来最大化证据下界（ELBO）。我们将近似后验$q_\\theta(z|x)$建模为投影正态分布，从而能够采样概率嵌入。我们的两个实例化——VSimCLR和VSupCon——用来自$q_\\theta(z|x)$的样本替换确定性嵌入，并将归一化的KL项纳入损失函数。在多个基准测试上的实验表明，VCL减轻了维度坍塌，增强了与类别标签的互信息，并在分类准确性方面与确定性基线持平或超越，同时通过后验模型提供了有意义的不确定性估计。因此，VCL为对比学习配备了概率基础，可作为对比方法的新基础。", "summary": "本文提出了一种名为变分对比学习（VCL）的新框架，旨在解决现有对比学习方法在确定性嵌入中缺乏不确定性量化的问题。VCL通过最大化证据下界（ELBO）并引入KL散度正则化来建模概率嵌入，将InfoNCE损失视为重建项。通过将近似后验建模为投影正态分布，VCL能够采样概率嵌入。实验结果表明，VCL有效缓解了维度坍塌，提高了与类别标签的互信息，并在分类准确性上与现有确定性方法相当甚至更优，同时提供了可靠的不确定性估计，为对比学习奠定了概率基础。", "keywords": "对比学习, 变分推断, 不确定性量化, 概率嵌入, 维度坍塌", "comments": "VCL的创新之处在于将变分推断引入对比学习，为确定性嵌入提供了急需的不确定性量化能力。这不仅解决了现有方法的一个关键局限性，还通过实验证明了其在缓解维度坍塌、增强互信息和保持分类准确性方面的有效性。VCL为未来基于概率的对比学习研究开辟了新方向。"}}
{"id": "2506.10328", "title": "Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework", "authors": ["Sadia Kamal", "Tim Oates", "Joy Wan"], "summary": "Skin carcinoma is the most prevalent form of cancer globally, accounting for\nover $8 billion in annual healthcare expenditures. In clinical settings,\nphysicians document patient visits using detailed SOAP (Subjective, Objective,\nAssessment, and Plan) notes. However, manually generating these notes is\nlabor-intensive and contributes to clinician burnout. In this work, we propose\na weakly supervised multimodal framework to generate clinically structured SOAP\nnotes from limited inputs, including lesion images and sparse clinical text.\nOur approach reduces reliance on manual annotations, enabling scalable,\nclinically grounded documentation while alleviating clinician burden and\nreducing the need for large annotated data. Our method achieves performance\ncomparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical\nrelevance metrics. To evaluate clinical quality, we introduce two novel metrics\nMedConceptEval and Clinical Coherence Score (CCS) which assess semantic\nalignment with expert medical concepts and input features, respectively.", "comment": "Accepted at IEEE/CVF Computer Society Conference on Computer Vision\n  and Pattern Recognition Workshops (CVPRW)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10328v1", "AI": {"title_translation": "迈向可扩展的SOAP笔记生成：一个弱监督多模态框架", "tldr": "本文提出了一个弱监督多模态框架，用于从有限输入（图像和稀疏文本）生成临床结构化SOAP笔记，旨在减轻医生负担并减少对大量标注数据的依赖，其性能可与GPT-4o等模型媲美。", "motivation": "皮肤癌是全球最常见的癌症，每年医疗开支超过80亿美元。医生手动生成详细的SOAP笔记费时费力，导致临床医生倦怠。因此，需要一种可扩展、自动化生成SOAP笔记的方法来减轻医生负担。", "method": "本文提出了一个弱监督多模态框架，能够从有限的输入（包括病变图像和稀疏临床文本）生成临床结构化的SOAP笔记。该方法减少了对手动标注的依赖，并引入了两个新的评估指标：MedConceptEval和Clinical Coherence Score (CCS)，分别评估与专家医学概念的语义对齐和与输入特征的对齐。", "result": "该方法在关键临床相关性指标上的性能与GPT-4o、Claude和DeepSeek Janus Pro相当。引入了MedConceptEval和Clinical Coherence Score (CCS)两个新指标来评估临床质量。", "conclusion": "该弱监督多模态框架能够有效地从有限输入生成临床结构化SOAP笔记，减轻了临床医生负担，减少了对大量标注数据的需求，并取得了与现有先进模型相当的性能。", "translation": "皮肤癌是全球最常见的癌症形式，每年导致超过80亿美元的医疗保健支出。在临床环境中，医生使用详细的SOAP（主观、客观、评估和计划）笔记记录患者就诊情况。然而，手动生成这些笔记费时费力，导致临床医生倦怠。在这项工作中，我们提出了一个弱监督多模态框架，用于从有限的输入（包括病变图像和稀疏临床文本）生成临床结构化的SOAP笔记。我们的方法减少了对手动标注的依赖，实现了可扩展、临床依据的文档生成，同时减轻了临床医生负担并减少了对大量标注数据的需求。我们的方法在关键临床相关性指标上的性能与GPT-4o、Claude和DeepSeek Janus Pro相当。为了评估临床质量，我们引入了两个新的指标MedConceptEval和Clinical Coherence Score (CCS)，它们分别评估与专家医学概念的语义对齐和与输入特征的对齐。", "summary": "本文提出了一种弱监督多模态框架，旨在从有限的病变图像和稀疏临床文本输入中自动生成结构化的SOAP笔记，以应对皮肤癌诊疗中医生手动记录负担重的问题。该框架减少了对大量人工标注数据的依赖，实现了可扩展的临床文档生成，并引入了MedConceptEval和Clinical Coherence Score (CCS)两个新指标来评估生成笔记的临床质量。实验结果表明，该方法在关键临床相关性指标上表现出与GPT-4o等先进模型相当的性能。", "keywords": "SOAP笔记生成, 弱监督, 多模态, 皮肤癌, 临床文档", "comments": "该论文的创新点在于提出了一个弱监督的多模态框架，解决了传统SOAP笔记生成中对大量手动标注数据的高度依赖问题，从而提高了可扩展性。其通过结合图像和文本输入，并引入新的评估指标，为临床文档自动化提供了一条有前景的路径，对于减轻医生工作量具有重要意义。"}}
{"id": "2506.10467", "title": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications", "authors": ["Felix Härer"], "summary": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10467v1", "AI": {"title_translation": "多智能体LLM系统规范与评估——原型与网络安全应用", "tldr": "本文探讨了多智能体LLM系统的规范与评估，并提出了一个原型系统，通过网络安全任务验证了其可行性。", "motivation": "尽管大型语言模型（LLMs）在文本生成之外的特定领域具有应用潜力，并且多智能体LLM方法可以解决复杂任务，但目前对这些系统的联合规范和综合应用缺乏深入探索。为了系统地评估LLM、推理技术及相关方面，需要明确定义多智能体LLM系统的规范。", "method": "本文报告了一项探索性研究的结果，旨在通过一个多智能体系统来规范和评估这些方面。研究扩展了先前的系统架构和原型，并引入了多智能体系统的规范。", "result": "涉及网络安全任务的测试案例表明了所提出的架构和评估方法的可行性。具体而言，结果显示使用OpenAI和DeepSeek的LLM代理正确完成了问答、服务器安全和网络安全任务的评估。", "conclusion": "研究表明，通过引入明确的规范和原型系统，可以有效地对多智能体LLM系统进行规范和评估，并在网络安全等复杂应用领域展现出可行性。", "translation": "大型语言模型（LLM）的最新进展预示着新型应用的潜力，例如通过最新OpenAI和DeepSeek模型的推理能力。为了在文本生成之外的特定领域应用这些模型，可以利用基于LLM的多智能体方法，通过结合推理技术、代码生成和软件执行来解决复杂任务。应用可能利用这些能力和专业LLM代理的知识。然而，尽管对LLM、推理技术和应用程序进行了单独的许多评估，但它们的联合规范和组合应用尚未得到很好的探索。需要为多智能体LLM系统定义规范，以探索其潜力及其对特定应用的适用性，从而允许对LLM、推理技术和相关方面进行系统评估。本文报告了通过多智能体系统规范和评估这些方面的探索性研究结果。系统架构和原型是从先前的研究中扩展而来的，并引入了多智能体系统的规范。涉及网络安全任务的测试案例表明了该架构和评估方法的可行性。特别是，结果显示了由OpenAI和DeepSeek的LLM代理正确完成的问答、服务器安全和网络安全任务的评估。", "summary": "本研究旨在解决多智能体大型语言模型（LLM）系统在联合规范和综合应用方面的不足。论文提出了一种新的规范，并扩展了现有的多智能体系统原型，以实现对LLM、推理技术和相关方面的系统评估。通过在网络安全任务（包括问答、服务器安全和网络安全）上的测试，验证了所提出的架构和评估方法的可行性，并展示了使用OpenAI和DeepSeek LLM代理的有效性。", "keywords": "多智能体系统, LLM, 规范, 评估, 网络安全", "comments": "本文的创新之处在于解决了多智能体LLM系统在联合规范和系统评估方面的空白，这对于将LLM应用于复杂领域至关重要。通过引入明确的规范和原型，为未来的多智能体LLM系统开发和部署提供了重要基础。其在网络安全领域的应用案例进一步证明了其在实际复杂任务中的潜力。"}}
{"id": "2506.10785", "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "comment": "12 pages, 6 figures, 5 tables", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10785v1", "AI": {"title_translation": "用户看重和批评什么：AI驱动移动应用用户反馈的大规模分析", "tldr": "通过大规模分析AI驱动移动应用的用户反馈，发现用户关注生产力、可靠性、个性化，并抱怨技术故障、价格和语言支持。", "motivation": "AI驱动功能在移动应用中迅速普及，但用户如何感知、评估和批评这些AI功能仍未被充分探索，这主要是由于用户反馈量巨大。", "method": "本研究对Google Play上292个AI驱动应用（涵盖14个类别，89.4万条AI相关评论）的用户反馈进行了首次全面、大规模研究。开发并验证了一个多阶段分析管道，包括人工标注基准、评估大型语言模型（LLMs）和提示策略，以及评论分类、方面情感提取和聚类。该管道实现了对用户反馈的可扩展、高精度分析。", "result": "管道提取了超过一百万个方面情感对，聚类为18个积极用户主题和15个消极用户主题。分析显示，用户积极评论侧重于生产力、可靠性和个性化帮助，而负面反馈突出技术故障（如扫描和识别）、价格问题和语言支持限制。该管道能识别同一评论中对不同功能的满意和不满，这些细粒度、共存的情感常被传统方法忽略。类别感知分析揭示了普遍的满意驱动因素和特定领域的挫败感。", "conclusion": "本研究提出的方法能更忠实地反映用户在AI驱动应用中的真实体验，揭示了用户对AI功能普遍的关注点和特定领域的痛点。", "translation": "人工智能（AI）驱动的功能已迅速普及到各种领域的移动应用中，包括生产力、教育、娱乐和创造力。然而，用户如何感知、评估和批评这些AI功能在很大程度上仍未被探索，这主要是由于用户反馈量巨大。在这项工作中，我们首次对AI驱动移动应用的用户反馈进行了全面、大规模的研究，利用了来自Google Play的292个AI驱动应用（涵盖14个类别）的89.4万条AI特定评论的精选数据集。我们开发并验证了一个多阶段分析管道，该管道始于人工标注基准，并系统地评估大型语言模型（LLMs）和提示策略。每个阶段，包括评论分类、方面情感提取和聚类，都经过了准确性和一致性验证。我们的管道实现了用户反馈的可扩展、高精度分析，提取了超过一百万个方面情感对，这些情感对被聚类为18个积极用户主题和15个消极用户主题。我们的分析表明，用户始终关注一组狭窄的主题：积极评论强调生产力、可靠性和个性化帮助，而负面反馈则突出技术故障（例如，扫描和识别）、价格问题和语言支持的限制。我们的管道能识别同一评论中对某个功能的满意和对另一个功能的不满。这些细粒度、共存的情感常常被传统方法忽略，因为传统方法将积极和消极反馈孤立处理或依赖粗粒度分析。为此，我们的方法更忠实地反映了AI驱动应用中真实的现实世界用户体验。类别感知分析进一步揭示了普遍的满意驱动因素和特定领域的挫败感。", "summary": "本研究对AI驱动移动应用的用户反馈进行了首次大规模分析，利用Google Play上近90万条评论构建数据集并开发了多阶段分析管道。该管道能高精度提取细粒度方面情感对，揭示了用户对AI功能的普遍关注点（如生产力、可靠性）和主要痛点（如技术故障、价格、语言支持），并能识别同一评论中的正负面共存情感，比传统方法更全面地反映了用户体验。", "keywords": "AI驱动应用, 用户反馈, 大规模分析, 情感分析, 大型语言模型", "comments": "这项研究的创新之处在于其大规模的用户反馈分析方法，特别是构建了包含近90万条AI特定评论的数据集，并开发了一个能处理细粒度、共存情感的多阶段分析管道。它揭示了AI驱动应用中用户价值和批评的具体方面，为AI产品设计和改进提供了宝贵的见解。其重要性在于填补了对AI功能用户感知空白，并提供了一个可扩展的分析框架。"}}
{"id": "2506.10850", "title": "Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements", "authors": ["Derek Benham", "Easton Potokar", "Joshua G. Mangelson"], "summary": "Autonomous surface vessels (ASVs) are increasingly vital for marine science,\noffering robust platforms for underwater mapping and inspection. Accurate state\nestimation, particularly of vehicle pose, is paramount for precise seafloor\nmapping, as even small surface deviations can have significant consequences\nwhen sensing the seafloor below. To address this challenge, we propose an\nInvariant Extended Kalman Filter (InEKF) framework designed to integrate\npartial orientation measurements. While conventional estimation often relies on\nrelative position measurements to fixed landmarks, open ocean ASVs primarily\nobserve a receding horizon. We leverage forward-facing monocular cameras to\nestimate roll and pitch with respect to this horizon, which provides\nyaw-ambiguous partial orientation information. To effectively utilize these\nmeasurements within the InEKF, we introduce a novel framework for incorporating\nsuch partial orientation data. This approach contrasts with traditional InEKF\nimplementations that assume full orientation measurements and is particularly\nrelevant for planar vehicle motion constrained to a \"seafaring plane.\" This\npaper details the developed InEKF framework; its integration with horizon-based\nroll/pitch observations and dual-antenna GPS heading measurements for ASV state\nestimation; and provides a comparative analysis against the InEKF using full\norientation and a Multiplicative EKF (MEKF). Our results demonstrate the\nefficacy and robustness of the proposed partial orientation measurements for\naccurate ASV state estimation in open ocean environments.", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics. 8 pages,\n  4 figures, 2 tables", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10850v1", "AI": {"title_translation": "用于自主水面船舶的部分姿态测量不变扩展卡尔曼滤波器", "tldr": "本文提出了一种不变扩展卡尔曼滤波器（InEKF）框架，用于整合部分姿态测量，以提高开放海洋环境中自主水面船舶（ASV）的精确状态估计。", "motivation": "自主水面船舶（ASV）在海洋科学中日益重要，但精确的状态估计，特别是车辆姿态估计，对于精确的海底测绘至关重要，因为即使是小的表面偏差也会对海底传感产生重大影响。为了解决这一挑战，本文提出了一种集成部分姿态测量的方法。", "method": "本文提出了一种不变扩展卡尔曼滤波器（InEKF）框架，旨在整合部分姿态测量。利用前视单目摄像头估计相对于地平线的横摇和俯仰，这提供了偏航模糊的部分姿态信息。为了有效利用这些测量，引入了一种新颖的框架来整合此类部分姿态数据。该方法与假设完整姿态测量的传统InEKF实现形成对比，并且特别适用于受限于“航海平面”的平面车辆运动。本文详细介绍了开发的InEKF框架，其与基于地平线的横摇/俯仰观测以及双天线GPS航向测量的集成，用于ASV状态估计。", "result": "结果表明，所提出的部分姿态测量对于开放海洋环境中精确的ASV状态估计是有效且鲁棒的。", "conclusion": "本文提出的用于整合部分姿态测量的不变扩展卡尔曼滤波器（InEKF）框架，有效解决了自主水面船舶在开放海洋环境中精确状态估计的挑战，并通过实验证明了其有效性和鲁棒性。", "translation": "自主水面船舶（ASV）在海洋科学中日益重要，为水下测绘和检查提供了强大的平台。精确的状态估计，特别是车辆姿态估计，对于精确的海底测绘至关重要，因为即使是小的表面偏差也可能对下方的海底传感产生重大影响。为了解决这一挑战，我们提出了一种不变扩展卡尔曼滤波器（InEKF）框架，旨在整合部分姿态测量。虽然传统估计通常依赖于相对于固定地标的相对位置测量，但开放海洋ASV主要观察到一个后退的地平线。我们利用前视单目摄像头估计相对于该地平线的横摇和俯仰，这提供了偏航模糊的部分姿态信息。为了有效利用InEKF中的这些测量，我们引入了一种新颖的框架来整合此类部分姿态数据。这种方法与假设完整姿态测量的传统InEKF实现形成对比，并且特别适用于受限于“航海平面”的平面车辆运动。本文详细介绍了开发的InEKF框架；其与基于地平线的横摇/俯仰观测和双天线GPS航向测量的集成，用于ASV状态估计；并提供了与使用完整姿态的InEKF和乘法扩展卡尔曼滤波器（MEKF）的比较分析。我们的结果证明了所提出的部分姿态测量在开放海洋环境中精确ASV状态估计的有效性和鲁棒性。", "summary": "本文针对自主水面船舶（ASV）在开放海洋环境中精确状态估计的挑战，提出了一种不变扩展卡尔曼滤波器（InEKF）框架。该框架创新性地整合了来自前视单目摄像头和双天线GPS的部分姿态测量（横摇、俯仰和航向），解决了传统InEKF对完整姿态测量的依赖。通过与现有方法的比较分析，研究结果验证了所提出方法在提高ASV状态估计精度和鲁棒性方面的有效性。", "keywords": "不变扩展卡尔曼滤波器,自主水面船舶,部分姿态测量,状态估计,地平线观测", "comments": "本文的创新点在于提出了一个新颖的InEKF框架，能够有效地整合偏航模糊的部分姿态测量，这对于在开放海洋这种缺乏固定地标的环境中运行的ASV至关重要。它通过利用地平线观测和GPS航向数据，克服了传统InEKF对完整姿态的假设，显著提升了ASV在复杂环境下的状态估计能力。其重要性在于为未来ASV在海洋科学和水下测绘中的应用提供了更精确、更鲁棒的导航和定位解决方案。"}}
{"id": "2506.10247", "title": "Optimal Voltage Control Using Online Exponential Barrier Method", "authors": ["Peng Zhang", "Baosen Zhang"], "summary": "This paper address the optimal voltage control problem of distribution\nsystems with high penetration of inverter-based renewable energy resources,\nunder inaccurate model information. We propose the online exponential barrier\nmethod that explicitly leverages the online feedback from grids to enhance the\nrobustness to model inaccuracy and incorporates the voltage constraints to\nmaintain the safety requirements. We provide analytical results on the optimal\nbarrier parameter selection and sufficient conditions for the safety guarantee\nof converged voltages. We also establish theoretical results on the exponential\nconvergence rate with proper step-size. The effectiveness of the proposed\nframework is validated on a 56-bus radial network, where we significantly\nimprove the robustness against model inaccuracy compared to existing methods.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10247v1", "AI": {"title_translation": "使用在线指数障碍法的最优电压控制", "tldr": "本文提出了一种在线指数障碍法，用于在模型信息不准确的情况下，对高渗透率逆变器型可再生能源的配电系统进行最优电压控制，并通过实验验证了其对模型不准确的鲁棒性显著提高。", "motivation": "解决配电系统在高渗透率逆变器型可再生能源和模型信息不准确情况下的最优电压控制问题。", "method": "提出在线指数障碍法，利用电网在线反馈增强对模型不准确的鲁棒性，并纳入电压约束以满足安全要求。提供了关于最优障碍参数选择和收敛电压安全保证的充分条件的分析结果。建立了适当步长下指数收敛速度的理论结果。", "result": "在56节点辐射状网络上验证了所提出框架的有效性，与现有方法相比，显著提高了对模型不准确的鲁棒性。", "conclusion": "所提出的在线指数障碍法能有效解决高渗透率可再生能源配电系统的最优电压控制问题，并在模型不准确的情况下表现出更高的鲁棒性。", "translation": "本文解决了在模型信息不准确的情况下，具有高渗透率逆变器型可再生能源的配电系统的最优电压控制问题。我们提出了在线指数障碍法，该方法明确利用来自电网的在线反馈来增强对模型不准确的鲁棒性，并结合电压约束以保持安全要求。我们提供了关于最优障碍参数选择和收敛电压安全保证的充分条件的分析结果。我们还建立了在适当步长下指数收敛速度的理论结果。所提出的框架在56节点辐射状网络上得到了验证，与现有方法相比，我们显著提高了对模型不准确的鲁棒性。", "summary": "本文提出了一种在线指数障碍法来解决高渗透率逆变器型可再生能源配电系统在模型信息不准确下的最优电压控制问题。该方法利用在线反馈提高鲁棒性，并包含电压约束确保安全。研究提供了最优参数选择和安全保证的分析结果，并证明了指数收敛速度。在56节点网络上的验证显示，该方法在应对模型不准确方面显著优于现有技术。", "keywords": "最优电压控制, 在线指数障碍法, 配电系统, 模型不准确, 鲁棒性", "comments": "本文提出了一种新颖的在线指数障碍法，其创新之处在于能够有效应对配电系统中日益增长的可再生能源渗透率带来的电压控制挑战，尤其是在模型信息不准确的现实条件下。通过结合在线反馈和理论分析，该方法不仅提高了控制的鲁棒性，还提供了安全保证和收敛性证明，具有较高的理论和实际应用价值。其对模型不准确的显著改进是其重要性的一大体现。"}}
{"id": "2506.10855", "title": "Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models", "authors": ["Michele Gubian", "Ioana Krehan", "Oli Liu", "James Kirby", "Sharon Goldwater"], "summary": "Analyses of self-supervised speech models have begun to reveal where and how\nthey represent different types of information. However, almost all analyses\nhave focused on English. Here, we examine how wav2vec2 models trained on four\ndifferent languages encode both language-matched and non-matched speech. We use\nprobing classifiers and geometric analyses to examine how phones, lexical\ntones, and speaker information are represented. We show that for all\npretraining and test languages, the subspaces encoding phones, tones, and\nspeakers are largely orthogonal, and that layerwise patterns of probing\naccuracy are similar, with a relatively small advantage for matched-language\nphone and tone (but not speaker) probes in the later layers. Our findings\nsuggest that the structure of representations learned by wav2vec2 is largely\nindependent of the speech material used during pretraining.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10855v1", "AI": {"title_translation": "分析自监督语音模型中预训练语言、语音、音调和说话人信息之间的关系", "tldr": "本文分析了wav2vec2模型如何在不同的预训练语言中编码语音、音调和说话人信息，发现这些表示在很大程度上是正交的，并且独立于预训练语音材料。", "motivation": "此前对自监督语音模型的分析几乎都集中在英语上。本文旨在探究在不同语言上训练的wav2vec2模型如何编码与语言匹配和不匹配的语音。", "method": "作者使用了在四种不同语言上训练的wav2vec2模型。他们采用探针分类器和几何分析来检查语音、词汇音调和说话人信息的表示。", "result": "研究发现，对于所有预训练和测试语言，编码语音、音调和说话人的子空间大体上是正交的。探针准确率的层级模式相似，在后期层中，匹配语言的语音和音调（但不是说话人）探针具有相对较小的优势。", "conclusion": "研究结果表明，wav2vec2学习到的表示结构在很大程度上独立于预训练期间使用的语音材料。", "translation": "对自监督语音模型的分析已经开始揭示它们在哪里以及如何表示不同类型的信息。然而，几乎所有分析都集中在英语上。在此，我们研究了在四种不同语言上训练的wav2vec2模型如何编码与语言匹配和不匹配的语音。我们使用探针分类器和几何分析来检查语音、词汇音调和说话人信息是如何表示的。我们表明，对于所有预训练和测试语言，编码语音、音调和说话人的子空间大体上是正交的，并且探针准确率的层级模式相似，在后期层中，匹配语言的语音和音调（但不是说话人）探针具有相对较小的优势。我们的研究结果表明，wav2vec2学习到的表示结构在很大程度上独立于预训练期间使用的语音材料。", "summary": "本文研究了wav2vec2自监督语音模型在多种预训练语言中对语音、音调和说话人信息的表示。通过探针分类器和几何分析，研究揭示了这些不同类型的信息被编码在很大程度上正交的子空间中，与预训练语言无关。探针准确率的层级模式一致，仅在后期层中，匹配语言的语音和音调探针具有微小优势，这表明所学习的表示结构在很大程度上独立于特定的预训练语音材料。", "keywords": "自监督语音模型, wav2vec2, 语言表示, 语音信息, 音调信息, 说话人信息", "comments": "这篇论文为wav2vec2等自监督语音模型的跨语言泛化能力提供了宝贵的见解。不同类型的语音信息（语音、音调、说话人）在很大程度上独立于预训练语言，以正交子空间的形式表示，这一发现意义重大。它表明这些模型中存在一个鲁棒且可能是通用的底层结构，这可以简化多语言语音处理并减少对特定语言预训练数据的需求。"}}
{"id": "2506.10678", "title": "Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL", "authors": ["Tom Westermann", "Aljosha Köcher", "Felix Gehlhoff"], "summary": "AutomationML (AML) enables standardized data exchange in engineering, yet\nexisting recommendations for proper AML modeling are typically formulated as\ninformal and textual constraints. These constraints cannot be validated\nautomatically within AML itself. This work-in-progress paper introduces a\npipeline to formalize and verify such constraints. First, AML models are mapped\nto OWL ontologies via RML and SPARQL. In addition, a Large Language Model\ntranslates textual rules into SHACL constraints, which are then validated\nagainst the previously generated AML ontology. Finally, SHACL validation\nresults are automatically interpreted in natural language. The approach is\ndemonstrated on a sample AML recommendation. Results show that even complex\nmodeling rules can be semi-automatically checked -- without requiring users to\nunderstand formal methods or ontology technologies.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10678v1", "AI": {"title_translation": "通过大型语言模型和SHACL对AutomationML中的文本约束进行自动化验证", "tldr": "论文提出了一种通过大型语言模型（LLM）和SHACL自动验证AutomationML中非正式文本约束的方法。", "motivation": "AutomationML (AML) 的建模建议通常是非正式的文本约束，无法在AML内部自动验证，这阻碍了AML模型的标准化和质量保证。", "method": "该论文引入了一个验证流程：首先，通过RML和SPARQL将AML模型映射到OWL本体；其次，大型语言模型将文本规则翻译成SHACL约束；然后，SHACL约束针对生成的AML本体进行验证；最后，SHACL验证结果被自动解释为自然语言。", "result": "该方法在一个示例AML建议上得到了演示，结果表明即使复杂的建模规则也可以半自动检查，且用户无需理解形式化方法或本体技术。", "conclusion": "该论文提出的方法成功实现了AutomationML中文本约束的自动化验证，有效降低了用户门槛，有助于提升AML模型的标准化和可靠性。", "translation": "AutomationML (AML) 实现了工程领域中数据的标准化交换，然而，现有的关于正确AML建模的建议通常以非正式的文本约束形式制定。这些约束无法在AML内部自动验证。这篇正在进行中的论文介绍了一个用于形式化和验证此类约束的流程。首先，AML模型通过RML和SPARQL映射到OWL本体。此外，大型语言模型将文本规则翻译成SHACL约束，然后针对先前生成的AML本体进行验证。最后，SHACL验证结果会自动以自然语言进行解释。该方法在一个示例AML建议上进行了演示。结果表明，即使复杂的建模规则也可以半自动检查——而无需用户理解形式化方法或本体技术。", "summary": "本文提出了一种自动化验证AutomationML (AML) 中非正式文本约束的流程。该流程首先将AML模型通过RML和SPARQL映射到OWL本体，随后利用大型语言模型将文本规则转换为SHACL约束，并针对生成的本体进行验证。最终，验证结果以自然语言形式呈现。实验证明，该方法能够半自动地检查复杂的建模规则，且用户无需具备形式化方法或本体技术的专业知识。", "keywords": "AutomationML, 文本约束, 验证, 大型语言模型, SHACL", "comments": "该论文的创新点在于巧妙地结合了大型语言模型（LLM）的自然语言理解能力与SHACL的形式化验证能力，解决了AutomationML中非正式文本约束的自动化验证难题。其重要性在于显著降低了用户对复杂形式化方法的学习成本，提高了工程数据交换中模型的一致性和可靠性，对提升工业自动化领域的数据质量和标准化具有积极意义。"}}
{"id": "2506.10321", "title": "Fast Ramanujan--type Series for Logarithms. Part II", "authors": ["Jorge Zuniga"], "summary": "This work extends the results of the preprint Ramanujan type Series for\nLogarithms, Part I, arXiv:2506.08245, which introduced single hypergeometric\ntype identities for the efficient computing of $\\log(p)$, where\n$p\\in\\mathbb{Z}_{>1}$. We present novel formulas for arctangents and methods\nfor a very fast multiseries evaluation of logarithms. Building upon a\n$\\mathcal{O}((p-1)^{6})$ Ramanujan type series asymptotic approximation for\n$\\log(p)$ as $p\\rightarrow1$, formulas for computing $n$ simultaneous\nlogarithms are developed. These formulas are derived by solving an integer\nprogramming problem to identify optimal variable values within a finite lattice\n$\\mathbb{Z}^{n}$. This approach yields linear combinations of series that\nprovide: (i) highly efficient formulas for single logarithms of natural numbers\nand (ii) the fastest known hypergeometric formulas for multivalued logarithms\nof $n$ selected integers in $\\mathbb{Z}_{>1}$. An application of these results\nwas to extend the number of decimal places known for log(10) up to\n2.0$\\cdot$10$^{12}$ digits (June 06 2025).", "comment": "17 pages, 1 table. TeX file must be downloaded, PARI GP program is\n  embedded as a large comment there", "cate": "math.NT", "url": "http://arxiv.org/abs/2506.10321v1", "AI": {"title_translation": "快速拉马努金型对数级数。第二部分", "tldr": "本文扩展了拉马努金型对数级数的研究，提出了反正切的新公式和对数快速多级数评估方法，并通过整数规划优化了级数组合，实现了单一对数和多值对数的高效计算，并将log(10)的精度扩展至2.0·10^12位。", "motivation": "该研究旨在扩展先前关于拉马努金型级数在对数高效计算方面的成果，开发反正切的新公式和对数非常快速的多级数评估方法，特别是用于同步计算多个对数。", "method": "该方法基于$\text{log}(p)$在$p\rightarrow1$时的$\text{O}((p-1)^6)$拉马努金型级数渐近逼近。通过解决一个整数规划问题，在有限格$\text{Z}^n$中识别最优变量值，从而推导出计算$\text{n}$个同步对数的公式。这种方法产生了级数的线性组合。", "result": "该方法提供了：(i) 自然数单一对数的高效公式；(ii) $\text{Z}_{>1}$中$\text{n}$个选定整数多值对数的最快已知超几何公式。这些成果的一个应用是将log(10)的已知小数位数扩展到2.0·10^12位。", "conclusion": "通过结合整数规划和拉马努金型级数，本文开发的方法显著提高了单一对数和多个对数计算的效率和速度，并为特定常数如log(10)的计算达到了创纪录的精度。", "translation": "这项工作扩展了预印本《拉马努金型对数级数，第一部分》（arXiv:2506.08245）的成果，该预印本介绍了用于有效计算$\text{log}(p)$（其中$p\text{∈Z}_{>1}$）的单一超几何型恒等式。我们提出了反正切的新公式以及对数非常快速的多级数评估方法。在$\text{log}(p)$在$p\rightarrow1$时的一个$\text{O}((p-1)^6)$拉马努金型级数渐近逼近的基础上，开发了计算$\text{n}$个同步对数的公式。这些公式是通过解决一个整数规划问题来推导的，以在有限格$\text{Z}^n$中识别最优变量值。这种方法产生了级数的线性组合，提供了：(i) 自然数单一对数的高效公式；(ii) $\text{Z}_{>1}$中$\text{n}$个选定整数多值对数的最快已知超几何公式。这些成果的一个应用是将log(10)的已知小数位数扩展到2.0·10^12位（2025年6月6日）。", "summary": "本文《快速拉马努金型对数级数。第二部分》扩展了先前工作，引入了反正切新公式和对数快速多级数评估方法。通过解决整数规划问题来优化级数组合，开发了同步对数计算公式。这产生了高效的单一对数公式和已知最快的超几何多值对数公式，并成功将log(10)的计算精度扩展至2.0·10^12位。", "keywords": "拉马努金级数, 对数, 超几何公式, 整数规划, 数值计算", "comments": "该论文的创新之处在于应用整数规划来优化拉马努金型级数中变量值的选择，从而为对数计算提供了高效且创纪录的方法。其重要性在于推动了数值常数计算的界限。"}}
{"id": "2506.10502", "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks", "authors": ["Junhua Lin", "Marc Juarez"], "summary": "We present a novel attack specifically designed against Tree-Ring, a\nwatermarking technique for diffusion models known for its high imperceptibility\nand robustness against removal attacks. Unlike previous removal attacks, which\nrely on strong assumptions about attacker capabilities, our attack only\nrequires access to the variational autoencoder that was used to train the\ntarget diffusion model, a component that is often publicly available. By\nleveraging this variational autoencoder, the attacker can approximate the\nmodel's intermediate latent space, enabling more effective surrogate-based\nattacks. Our evaluation shows that this approach leads to a dramatic reduction\nin the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to\n0.153 and from 0.994 to 0.385, respectively, while maintaining high image\nquality. Notably, our attacks outperform existing methods that assume full\naccess to the diffusion model. These findings highlight the risk of reusing\npublic autoencoders to train diffusion models -- a threat not considered by\ncurrent industry practices. Furthermore, the results suggest that the Tree-Ring\ndetector's precision, a metric that has been overlooked by previous\nevaluations, falls short of the requirements for real-world deployment.", "comment": "18 pages, to be published in the 34th USENIX Security Symposium", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10502v1", "AI": {"title_translation": "树皮中的裂缝：利用公共知识去除Tree-Ring水印", "tldr": "一种新的攻击方法利用公开的变分自编码器有效去除Tree-Ring水印，揭示了公共模型重用带来的安全风险。", "motivation": "Tree-Ring是一种针对扩散模型的水印技术，以其高不可感知性和对移除攻击的鲁棒性而闻名。然而，现有的移除攻击方法对攻击者能力有很强的假设。本文旨在开发一种更实际且有效的攻击方法来移除Tree-Ring水印，特别是针对其未被充分考虑的漏洞。", "method": "本文提出了一种新颖的攻击方法，它不依赖于对攻击者能力的强假设，而仅需要访问用于训练目标扩散模型的变分自编码器（VAE），该组件通常是公开可用的。通过利用这个VAE，攻击者可以近似模型的中间潜在空间，从而实现更有效的基于代理的攻击。", "result": "评估显示，这种方法导致Tree-Ring检测器ROC和PR曲线的AUC显著下降，分别从0.993降至0.153和从0.994降至0.385，同时保持了高图像质量。值得注意的是，该攻击方法优于假设完全访问扩散模型的现有方法。", "conclusion": "本文的发现强调了重用公共自编码器训练扩散模型的风险，这是当前行业实践中未考虑到的威胁。此外，结果表明Tree-Ring检测器的精度（一个被忽视的指标）未能满足实际部署的要求。", "translation": "我们提出了一种专门针对Tree-Ring的新型攻击，Tree-Ring是一种用于扩散模型的水印技术，以其高不可感知性和对移除攻击的鲁棒性而闻名。与以往依赖于对攻击者能力强假设的移除攻击不同，我们的攻击仅需要访问用于训练目标扩散模型的变分自编码器，这是一个通常公开可用的组件。通过利用这个变分自编码器，攻击者可以近似模型的中间潜在空间，从而实现更有效的基于代理的攻击。我们的评估表明，这种方法导致Tree-Ring检测器ROC和PR曲线的AUC显著下降，分别从0.993降至0.153和从0.994降至0.385，同时保持了高图像质量。值得注意的是，我们的攻击优于假设完全访问扩散模型的现有方法。这些发现凸显了重用公共自编码器来训练扩散模型的风险——这是当前行业实践中未考虑到的威胁。此外，结果表明，Tree-Ring检测器的精度（一个被以往评估忽略的指标）未能满足实际部署的要求。", "summary": "本文介绍了一种针对Tree-Ring水印技术的新型攻击。Tree-Ring是一种针对扩散模型的水印，以其鲁棒性著称。与现有方法不同，该攻击仅利用公开可用的变分自编码器来近似扩散模型的潜在空间，从而实现有效的代理攻击。实验结果表明，该方法能显著降低Tree-Ring检测器的性能（AUC大幅下降），同时保持图像质量，并优于其他需要更多访问权限的方法。研究强调了重用公共自编码器训练扩散模型的安全风险，并指出Tree-Ring检测器在实际应用中的精度不足。", "keywords": "水印移除, 扩散模型, 变分自编码器, Tree-Ring, 安全风险", "comments": "这篇论文通过利用公开可用的组件（VAE）来攻击Tree-Ring水印，展示了一种新颖且实用的攻击方法。其创新之处在于识别并利用了公共模型组件的潜在安全漏洞，这在现有研究中可能被忽视。论文的重要性在于揭示了扩散模型训练中重用公共自编码器所带来的未被充分认识的风险，为水印技术和模型安全领域提供了重要的警示。"}}
{"id": "2506.10803", "title": "Solving Package Management via Hypergraph Dependency Resolution", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Michael Winston Dales", "Mark Elvers", "Thomas Gazagnaire", "Sadiq Jaffer", "Thomas Leonard", "Jon Ludlam", "Anil Madhavapeddy"], "summary": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.", "comment": "Submitted to SPLASH 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10803v1", "AI": {"title_translation": "通过超图依赖解析解决包管理问题", "tldr": "HyperRes是一个使用超图的正式系统，旨在解决不同包管理器生态系统之间缺乏互操作性的问题，允许跨语言项目精确表达和解决依赖关系，而无需用户更换现有包管理器。", "motivation": "现有包管理器之间缺乏互操作性，导致多语言项目无法精确表达跨语言生态系统的依赖关系，且外部系统和硬件依赖通常是隐式且未版本化的。", "method": "定义了HyperRes，一个使用超图描述版本化依赖解析的正式系统。该系统足够表达和建模多种生态系统，并能解决跨生态系统的依赖约束。此外，还定义了从数十个现有包管理器到HyperRes的转换。", "result": "HyperRes能够建模许多现有生态系统并解决跨它们存在的依赖约束。研究表明，依赖解析可以在当前独立的生态系统之间进行。HyperRes允许在生态系统之间转换打包元数据，并可以为特定部署环境进行精确的专业化解决。", "conclusion": "HyperRes提供了一种统一且可互操作的包管理解决方案，能够跨不同生态系统精确解决依赖关系，同时不强制用户改变其现有包管理器的选择。", "translation": "包管理器无处不在，几乎每种语言和操作系统都实现了自己的解决方案。这些系统之间缺乏互操作性，意味着多语言项目无法在语言生态系统之间表达精确的依赖关系，并且外部系统和硬件依赖通常是隐式且未版本化的。我们定义了HyperRes，一个用于描述版本化依赖解析的正式系统，它使用超图，其表达能力足以建模许多生态系统并解决它们之间的依赖约束。我们定义了从数十个现有包管理器到HyperRes的转换，并全面证明了依赖解析可以在当前独立的生态系统之间工作。这不需要用户改变他们选择的包管理器；相反，HyperRes允许在生态系统之间转换打包元数据，并可以为特定的部署环境进行精确的专业化解决。", "summary": "该研究提出了HyperRes，一个基于超图的正式系统，旨在解决现有包管理器之间缺乏互操作性的问题。通过将数十种现有包管理器转换为HyperRes模型，该系统能够精确建模并解决跨多个语言和操作系统生态系统的版本化依赖关系。其核心创新在于，它允许跨生态系统进行依赖解析和元数据转换，同时无需用户切换其偏好的包管理器，从而为多语言和复杂项目提供了统一且可定制的依赖管理能力。", "keywords": "包管理, 依赖解析, 超图, 互操作性, 跨生态系统", "comments": "本文的创新之处在于提出了一种基于超图的统一形式系统HyperRes来解决跨生态系统的包依赖问题，这在现有各自为政的包管理领域具有重要意义。它通过提供一种非侵入性的解决方案（无需用户更换包管理器）来增强互操作性，为多语言和复杂软件项目的依赖管理提供了强大的支持。其贡献在于理论框架的构建和实际可行性的证明。"}}
{"id": "2506.10875", "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material", "authors": ["Guanjin Wang", "Xiangxue Zhao", "Shapour Azarm", "Balakumar Balachandran"], "summary": "An alternative data-driven modeling approach has been proposed and employed\nto gain fundamental insights into robot motion interaction with granular\nterrain at certain length scales. The approach is based on an integration of\ndimension reduction (Sequentially Truncated Higher-Order Singular Value\nDecomposition), surrogate modeling (Gaussian Process), and data assimilation\ntechniques (Reduced Order Particle Filter). This approach can be used online\nand is based on offline data, obtained from the offline collection of\nhigh-fidelity simulation data and a set of sparse experimental data. The\nresults have shown that orders of magnitude reduction in computational time can\nbe obtained from the proposed data-driven modeling approach compared with\nphysics-based high-fidelity simulations. With only simulation data as input,\nthe data-driven prediction technique can generate predictions that have\ncomparable accuracy as simulations. With both simulation data and sparse\nphysical experimental measurement as input, the data-driven approach with its\nembedded data assimilation techniques has the potential in outperforming only\nhigh-fidelity simulations for the long-horizon predictions. In addition, it is\ndemonstrated that the data-driven modeling approach can also reproduce the\nscaling relationship recovered by physics-based simulations for maximum\nresistive forces, which may indicate its general predictability beyond a\ncase-by-case basis. The results are expected to help robot navigation and\nexploration in unknown and complex terrains during both online and offline\nphases.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10875v1", "AI": {"title_translation": "机器人附肢与颗粒材料动态相互作用的数据驱动预测", "tldr": "本文提出了一种新的数据驱动建模方法，用于预测机器人与颗粒材料的动态相互作用，该方法结合了降维、代理建模和数据同化技术，显著减少了计算时间，并能实现高精度预测，有望助力机器人在复杂地形中的导航与探索。", "motivation": "为了深入理解机器人在特定尺度下与颗粒地形的运动交互，并实现对其动态相互作用的预测。", "method": "提出并采用了一种数据驱动建模方法，该方法整合了：1) 降维（序贯截断高阶奇异值分解），2) 代理建模（高斯过程），以及 3) 数据同化技术（降阶粒子滤波器）。该方法基于离线收集的高保真模拟数据和少量实验数据，可用于在线预测。", "result": "与基于物理的高保真模拟相比，所提出的数据驱动建模方法可将计算时间减少数个数量级。仅使用模拟数据作为输入时，预测精度与模拟结果相当。结合模拟数据和稀疏物理实验测量数据时，该数据驱动方法在长期预测方面有望超越纯高保真模拟。此外，该方法还能再现物理模拟所揭示的最大阻力标度关系，表明其具有超越个案的普遍预测能力。", "conclusion": "该研究结果有望在线和离线阶段帮助机器人在未知和复杂地形中的导航和探索。", "translation": "本文提出并采用了一种替代的数据驱动建模方法，以获取机器人在特定长度尺度下与颗粒地形运动交互的基本洞察。该方法基于降维（序贯截断高阶奇异值分解）、代理建模（高斯过程）和数据同化技术（降阶粒子滤波器）的集成。该方法可在线使用，并基于离线数据，这些数据通过离线收集高保真模拟数据和一组稀疏实验数据获得。结果表明，与基于物理的高保真模拟相比，所提出的数据驱动建模方法可以将计算时间减少数个数量级。仅以模拟数据作为输入时，数据驱动预测技术可以生成与模拟具有可比精度的预测。当以模拟数据和稀疏物理实验测量数据作为输入时，嵌入数据同化技术的数据驱动方法在长期预测方面有可能超越仅使用高保真模拟。此外，研究表明，数据驱动建模方法还可以重现基于物理模拟所恢复的最大阻力标度关系，这可能表明其具有超越个例的普遍可预测性。这些结果有望在线和离线阶段帮助机器人在未知和复杂地形中的导航和探索。", "summary": "本文提出了一种创新的数据驱动建模方法，旨在预测机器人附肢与颗粒材料的动态相互作用。该方法通过整合序贯截断高阶奇异值分解、高斯过程和降阶粒子滤波器，利用离线高保真模拟数据和稀疏实验数据进行训练，实现在线预测。研究结果显示，与传统物理模拟相比，该方法能显著缩短计算时间，并在仅使用模拟数据时达到相当的预测精度。结合少量实验数据后，其长期预测性能有望超越纯高保真模拟。此外，该方法还能准确复现物理模拟揭示的力学标度关系，预示其具有广泛适用性，对未来机器人在复杂未知地形中的导航与探索具有重要意义。", "keywords": "数据驱动, 机器人交互, 颗粒材料, 预测, 建模", "comments": "该论文提出了一种结合多种先进数据科学技术（降维、代理建模、数据同化）的创新数据驱动建模范式，用于解决机器人与复杂颗粒介质交互这一难题。其核心创新在于通过数据驱动的方法，在保持预测精度的同时，大幅度降低了计算成本，并展现了超越特定案例的泛化能力。这将极大地推动机器人领域在实际复杂环境（如行星探测、灾后救援）中应用。其结合模拟数据和稀疏实验数据的策略，也为数据量有限的实际应用提供了有效途径。"}}
{"id": "2506.10708", "title": "System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers", "authors": ["Michael Bartholomew", "Joohyung Lee"], "summary": "Answer Set Programming Modulo Theories (ASPMT) is an approach to combining\nanswer set programming and satisfiability modulo theories based on the\nfunctional stable model semantics. It is shown that the tight fragment of ASPMT\nprograms can be turned into SMT instances, thereby allowing SMT solvers to\ncompute stable models of ASPMT programs. In this paper we present a compiler\ncalled {\\sc aspsmt2smt}, which implements this translation. The system uses ASP\ngrounder {\\sc gringo} and SMT solver {\\sc z3}. {\\sc gringo} partially grounds\ninput programs while leaving some variables to be processed by {\\sc z3}. We\ndemonstrate that the system can effectively handle real number computations for\nreasoning about continuous changes.", "comment": "In Proceedings of the 14th European Conference on Logics in\n  Artificial Intelligence (JELIA 2014)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10708v1", "AI": {"title_translation": "系统ASPMT2SMT：使用SMT求解器计算ASPMT理论", "tldr": "ASPMT2SMT系统将ASPMT程序转换为SMT实例，使SMT求解器能够计算ASPMT稳定模型，特别适用于实数计算。", "motivation": "该研究旨在通过将ASPMT程序转换为SMT实例，从而使SMT求解器能够计算ASPMT程序的稳定模型。", "method": "本文提出了一个名为ASPMT2SMT的编译器，该编译器实现了将ASPMT程序的紧密片段转换为SMT实例。该系统利用ASP推导器Gringo进行部分推导，并使用SMT求解器Z3处理剩余变量。", "result": "该系统能够有效地处理实数计算，用于推理连续变化。", "conclusion": "ASPMT2SMT系统能够有效地处理实数计算，用于推理关于连续变化的问题。", "translation": "理论回答集编程（ASPMT）是一种结合回答集编程和可满足性模理论的方法，基于函数稳定模型语义。研究表明，ASPMT程序的紧密片段可以转换为SMT实例，从而允许SMT求解器计算ASPMT程序的稳定模型。在本文中，我们提出了一个名为ASPMT2SMT的编译器，它实现了这种转换。该系统使用ASP推导器Gringo和SMT求解器Z3。Gringo部分推导输入程序，同时留下一些变量由Z3处理。我们证明该系统可以有效地处理实数计算，用于推理连续变化。", "summary": "本文介绍了ASPMT2SMT，一个将理论回答集编程（ASPMT）程序转换为可满足性模理论（SMT）实例的编译器。这种转换使得SMT求解器（如Z3）能够计算ASPMT程序的稳定模型。该系统利用Gringo进行部分推导，并被证明能有效处理实数计算，以推理连续变化。", "keywords": "ASPMT, SMT求解器, 编译器, 稳定模型, 实数计算", "comments": "这项工作的创新在于提供了一个实用的系统ASPMT2SMT，它成功地连接了理论回答集编程（ASPMT）和可满足性模理论（SMT）。这使得强大的SMT求解器能够应用于ASPMT问题，特别是在处理连续域方面，如其在实数计算能力上的展示，具有重要的实际意义。"}}
{"id": "2506.10343", "title": "Code Execution as Grounded Supervision for LLM Reasoning", "authors": ["Dongwon Jung", "Wenxuan Zhou", "Muhao Chen"], "summary": "Training large language models (LLMs) with chain-of-thought (CoT) supervision\nhas proven effective for enhancing their reasoning abilities. However,\nobtaining reliable and accurate reasoning supervision remains a significant\nchallenge. We propose a scalable method for generating a high-quality CoT\nsupervision dataset by leveraging the determinism of program execution. Unlike\nexisting reasoning dataset generation methods that rely on costly human\nannotations or error-prone LLM-generated CoT, our approach extracts verifiable,\nstep-by-step reasoning traces from code execution and transforms them into a\nnatural language CoT reasoning. Experiments on reasoning benchmarks across\nvarious domains show that our method effectively equips LLMs with transferable\nreasoning abilities across diverse tasks. Furthermore, the ablation studies\nvalidate that our method produces highly accurate reasoning data and reduces\noverall token length during inference by reducing meaningless repetition and\noverthinking.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10343v1", "AI": {"title_translation": "代码执行作为LLM推理的基础监督", "tldr": "该论文提出了一种利用代码执行生成高质量、可验证的思维链（CoT）监督数据的方法，以提高大型语言模型（LLM）的推理能力，并减少推理时的token长度。", "motivation": "目前，为大型语言模型（LLM）获取可靠准确的思维链（CoT）推理监督存在挑战，现有方法要么成本高昂（人工标注），要么容易出错（LLM生成CoT）。", "method": "该方法通过利用程序执行的确定性来生成高质量的CoT监督数据集。它从代码执行中提取可验证的、分步的推理轨迹，并将其转换为自然语言的CoT推理。", "result": "实验表明，该方法能有效赋予LLM跨不同任务的可迁移推理能力。消融研究验证了该方法能产生高度准确的推理数据，并通过减少无意义的重复和过度思考，缩短了推理过程中的总token长度。", "conclusion": "该论文提出了一种利用代码执行生成高质量思维链（CoT）监督的可扩展且有效的方法，从而提升了大型语言模型（LLM）的推理能力并提高了推理效率。", "translation": "训练大型语言模型（LLM）与思维链（CoT）监督已被证明能有效增强其推理能力。然而，获取可靠准确的推理监督仍然是一个重大挑战。我们提出了一种可扩展的方法，通过利用程序执行的确定性来生成高质量的CoT监督数据集。与现有依赖昂贵的人工标注或易出错的LLM生成CoT的推理数据集生成方法不同，我们的方法从代码执行中提取可验证的、逐步的推理轨迹，并将其转化为自然语言CoT推理。在各种领域的推理基准测试中进行的实验表明，我们的方法有效地使LLM具备了跨不同任务的可迁移推理能力。此外，消融研究验证了我们的方法能产生高度准确的推理数据，并通过减少无意义的重复和过度思考，减少了推理过程中的总token长度。", "summary": "该论文提出了一种可扩展的方法，通过利用程序执行的确定性来为大型语言模型（LLM）生成高质量的思维链（CoT）监督。与依赖人工标注或易出错的LLM生成CoT的方法不同，该方法从代码执行中提取可验证的、分步的推理轨迹，并将其转化为自然语言CoT。实验证明，该方法能有效提升LLM在各种任务中的可迁移推理能力，并通过减少冗余推理来缩短推理时的token长度。", "keywords": "大型语言模型, 思维链, 代码执行, 推理监督, 程序执行", "comments": "该论文提出了一种创新的方法，利用代码执行来生成高质量的思维链（CoT）监督数据，为传统方法提供了一种可扩展且可验证的替代方案。其关注确定性以及通过更准确的推理来减少推理token长度，对提高LLM的效率和可靠性具有重要意义。"}}
{"id": "2506.10334", "title": "Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions", "authors": ["Deliang Wang", "Chao Yang", "Gaowei Chen"], "summary": "Students' academic emotions significantly influence their social behavior and\nlearning performance. Traditional approaches to automatically and accurately\nanalyze these emotions have predominantly relied on supervised machine learning\nalgorithms. However, these models often struggle to generalize across different\ncontexts, necessitating repeated cycles of data collection, annotation, and\ntraining. The emergence of Vision-Language Models (VLMs) offers a promising\nalternative, enabling generalization across visual recognition tasks through\nzero-shot prompting without requiring fine-tuning. This study investigates the\npotential of VLMs to analyze students' academic emotions via facial expressions\nin an online learning environment. We employed two VLMs,\nLlama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000\nimages depicting confused, distracted, happy, neutral, and tired expressions\nusing zero-shot prompting. Preliminary results indicate that both models\ndemonstrate moderate performance in academic facial expression recognition,\nwith Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.\nNotably, both models excel in identifying students' happy emotions but fail to\ndetect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits\nrelatively high performance in recognizing students' confused expressions,\nhighlighting its potential for practical applications in identifying content\nthat causes student confusion.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10334v1", "AI": {"title_translation": "使用视觉语言模型通过面部表情检测学生的学习情感", "tldr": "本研究探讨了视觉语言模型（VLMs）在通过面部表情检测学生学习情感方面的潜力，发现VLMs表现适中，其中Qwen2.5-VL-7B-Instruct优于Llama-3.2-11B-Vision-Instruct，尤其擅长识别快乐和困惑，但在检测分心行为上存在不足。", "motivation": "学生的学习情感显著影响他们的社交行为和学习表现。传统的监督机器学习方法在自动准确分析这些情感时，泛化能力差，需要反复的数据收集、标注和训练。视觉语言模型（VLMs）的出现提供了一种无需微调即可实现泛化视觉识别任务的有前景的替代方案。", "method": "本研究利用Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct两种视觉语言模型，通过零样本提示（zero-shot prompting）分析了5000张描绘困惑、分心、快乐、中性、疲惫表情的图像，以检测在线学习环境中的学生学习情感。", "result": "初步结果显示，两种模型在学术面部表情识别方面表现适中。Qwen2.5-VL-7B-Instruct的表现优于Llama-3.2-11B-Vision-Instruct。两者在识别学生的快乐情感方面表现出色，但未能检测到分心行为。Qwen2.5-VL-7B-Instruct在识别学生的困惑表情方面表现出相对较高的性能。", "conclusion": "视觉语言模型，特别是Qwen2.5-VL-7B-Instruct，在通过面部表情识别学生学习情感方面具有潜力，尤其是在识别导致学生困惑的内容方面。", "translation": "学生的学习情感显著影响他们的社交行为和学习表现。传统上，自动准确分析这些情感的方法主要依赖于监督机器学习算法。然而，这些模型通常难以在不同情境下泛化，需要反复进行数据收集、标注和训练。视觉语言模型（VLMs）的出现提供了一种有前景的替代方案，它们通过零样本提示（zero-shot prompting）实现了视觉识别任务的泛化，而无需进行微调。本研究旨在探讨VLMs在在线学习环境中通过面部表情分析学生学习情感的潜力。我们使用了Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct两种VLM，通过零样本提示分析了5000张描绘困惑、分心、快乐、中性、疲惫表情的图像。初步结果表明，这两种模型在学术面部表情识别方面表现适中，其中Qwen2.5-VL-7B-Instruct的表现优于Llama-3.2-11B-Vision-Instruct。值得注意的是，两种模型在识别学生的快乐情感方面表现出色，但未能检测到分心行为。此外，Qwen2.5-VL-7B-Instruct在识别学生的困惑表情方面表现出相对较高的性能，这突显了其在识别导致学生困惑的内容方面的实际应用潜力。", "summary": "本研究探讨了视觉语言模型（VLMs）在通过面部表情检测学生学习情感方面的应用潜力，以克服传统监督学习方法的泛化难题。研究使用Llama-3.2和Qwen2.5两种VLM，通过零样本提示分析了5000张学生表情图像。结果显示，VLMs在学术面部表情识别上表现适中，其中Qwen2.5优于Llama-3.2，尤其擅长识别快乐和困惑，但在检测分心行为上存在不足。研究认为VLMs，特别是Qwen2.5，在识别学生困惑情感方面具有实际应用价值。", "keywords": "视觉语言模型, 学习情感, 面部表情识别, 零样本学习, 在线学习", "comments": "这项研究创新性地探索了视觉语言模型在教育领域情感识别的应用，特别是其零样本泛化能力，避免了传统方法的繁琐数据标注和训练。研究指出了VLMs在特定情感（如快乐和困惑）识别上的潜力，但也揭示了其在识别某些复杂情感（如分心）上的局限性，为未来研究提供了方向。"}}
{"id": "2506.10506", "title": "On a mean-field Pontryagin minimum principle for stochastic optimal control", "authors": ["Manfred Opper", "Sebastian Reich"], "summary": "This papers outlines a novel extension of the classical Pontryagin minimum\n(maximum) principle to stochastic optimal control problems. Contrary to the\nwell-known stochastic Pontryagin minimum principle involving forward-backward\nstochastic differential equations, the proposed formulation is deterministic\nand of mean-field type. The Hamiltonian structure of the proposed Pontryagin\nminimum principle is achieved via the introduction of an appropriate gauge\nvariable. The gauge freedom can be used to decouple the forward and reverse\ntime equations; hence simplifying the solution of the underlying boundary value\nproblem. We also consider infinite horizon discounted cost optimal control\nproblems. In this case, the mean-field formulation allows converting the\ncomputation of the desired optimal control law into solving a pair of forward\nmean-field ordinary differential equations. The proposed mean-field formulation\nof the Pontryagin minimum principle is tested numerically for a controlled\ninverted pendulum and a controlled Lorenz-63 system.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10506v1", "AI": {"title_translation": "随机最优控制的平均场庞特里亚金最小原理", "tldr": "本文提出了一种新的确定性平均场庞特里亚金最小原理，用于随机最优控制，通过引入规范变量简化了方程求解，并进行了数值验证。", "motivation": "现有的随机庞特里亚金最小原理涉及复杂的前向-后向随机微分方程，本文旨在提供一种确定性的平均场方法来简化随机最优控制问题的求解。", "method": "本文提出了一种确定性的平均场庞特里亚金最小原理，通过引入适当的规范变量来构建哈密顿结构，并利用规范自由度解耦前向和反向时间方程。对于无限时域问题，将最优控制律的计算转换为求解一对前向平均场常微分方程。", "result": "所提出的方法能够解耦前向和反向时间方程，从而简化了边值问题的求解。对于无限时域问题，可以将最优控制律的计算转化为求解一对前向平均场常微分方程。该方法已在受控倒立摆和受控Lorenz-63系统上进行了数值测试。", "conclusion": "本文成功地将经典的庞特里亚金最小原理扩展到随机最优控制问题，提出了一种确定性平均场方法，通过规范变量简化了求解过程，并展示了其在实际系统中的有效性。", "translation": "本文概述了经典庞特里亚金最小（最大）原理在随机最优控制问题上的新颖扩展。与涉及前向-后向随机微分方程的众所周知的随机庞特里亚金最小原理相反，所提出的公式是确定性的，并且是平均场类型的。所提出的庞特里亚金最小原理的哈密顿结构是通过引入适当的规范变量实现的。规范自由度可用于解耦前向和反向时间方程；从而简化了底层边值问题的求解。我们还考虑了无限时域折现成本最优控制问题。在这种情况下，平均场公式允许将所需最优控制律的计算转换为求解一对前向平均场常微分方程。所提出的庞特里亚金最小原理的平均场公式在受控倒立摆和受控Lorenz-63系统上进行了数值测试。", "summary": "本文提出了一种新颖的、确定性的平均场庞特里亚金最小原理，用于解决随机最优控制问题。该方法通过引入规范变量构建哈密顿结构，并利用规范自由度解耦了前向和反向时间方程，从而简化了边值问题的求解。对于无限时域问题，该方法将最优控制律的计算转化为求解前向平均场常微分方程。该原理已在受控倒立摆和Lorenz-63系统上进行了数值验证。", "keywords": "庞特里亚金最小原理, 随机最优控制, 平均场, 规范变量, 确定性控制", "comments": "这篇论文通过引入“平均场”和“规范变量”的概念，为随机最优控制提供了一种创新的、确定性的方法，避免了传统随机庞特里亚金原理中复杂的前向-后向随机微分方程。这种方法的创新之处在于其确定性性质和简化求解过程的能力，对于实际应用中的计算效率和可实现性具有重要意义。"}}
{"id": "2506.10597", "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models", "authors": ["Xunguang Wang", "Zhenlan Ji", "Wenxuan Wang", "Zongjie Li", "Daoyuan Wu", "Shuai Wang"], "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10597v1", "AI": {"title_translation": "知识系统化：评估大型语言模型的越狱护栏", "tldr": "本文对大型语言模型的越狱护栏进行了首次全面的知识系统化分析，提出了一个新的多维度分类法和安全-效率-实用性评估框架，以指导未来研究和部署。", "motivation": "大型语言模型（LLMs）在部署中暴露了关键漏洞，特别是越狱攻击绕过了安全机制。虽然护栏被认为是解决方案，但当前LLM护栏领域碎片化，缺乏统一的分类法和全面的评估框架。", "method": "本文作为一篇知识系统化（SoK）论文，对LLM的越狱护栏进行了首次整体分析。研究者提出了一个新颖的多维度分类法，将护栏沿六个关键维度进行分类，并引入了一个安全-效率-实用性评估框架来评估其实用有效性。通过广泛的分析和实验进行研究。", "result": "识别了现有护栏方法的优点和局限性，探讨了它们在不同攻击类型上的普适性，并为优化防御组合提供了见解。", "conclusion": "本文为未来的研究和开发提供了结构化基础，旨在指导稳健的LLM护栏的原则性推进和部署。", "translation": "大型语言模型（LLM）取得了显著进展，但其部署暴露了关键漏洞，特别是绕过安全机制的越狱攻击。护栏——作为监控和控制LLM交互的外部防御机制——已成为一种有前景的解决方案。然而，当前LLM护栏领域碎片化，缺乏统一的分类法和全面的评估框架。在这篇知识系统化（SoK）论文中，我们首次对LLM的越狱护栏进行了整体分析。我们提出了一个新颖的多维度分类法，将护栏沿六个关键维度进行分类，并引入了一个安全-效率-实用性评估框架来评估其实用有效性。通过广泛的分析和实验，我们识别了现有护栏方法的优点和局限性，探讨了它们在不同攻击类型上的普适性，并为优化防御组合提供了见解。我们的工作为未来的研究和开发提供了结构化基础，旨在指导稳健的LLM护栏的原则性推进和部署。代码可在https://github.com/xunguangwang/SoK4JailbreakGuardrails 获取。", "summary": "本文是一篇关于大型语言模型（LLMs）越狱护栏的知识系统化（SoK）论文。鉴于LLMs在部署中面临越狱攻击的漏洞以及现有护栏领域的碎片化，作者提出了首个对越狱护栏的整体分析。研究引入了一个新颖的多维度分类法（涵盖六个关键维度）和一个安全-效率-实用性评估框架，用于评估护栏的实际效果。通过广泛的分析和实验，论文揭示了现有护栏方法的优缺点、其对不同攻击类型的普适性，并提供了优化防御组合的策略。这项工作为未来研究和开发提供了结构化基础，旨在推动健壮LLM护栏的规范化发展和部署。", "keywords": "大型语言模型, 越狱攻击, 安全护栏, 知识系统化, 评估框架", "comments": "这篇SoK论文非常及时且重要，因为它解决了LLM安全领域的一个核心痛点——越狱攻击及其防御机制的混乱现状。通过提出统一的分类法和评估框架，它为该领域的未来研究奠定了坚实的基础，有助于规范术语和比较不同方法的有效性。其“安全-效率-实用性”的评估框架也考虑了实际部署中的关键因素。"}}
{"id": "2506.10833", "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "authors": ["Fabian C. Peña", "Steffen Herbold"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10833v1", "AI": {"title_translation": "评估大型语言模型在非代码软件工程任务中的表现", "tldr": "本文提出了SELU，首个全面评估LLM在17项非代码软件工程任务中表现的基准，结果显示中等规模的解码器模型表现最佳。", "motivation": "大型语言模型（LLMs）在代码理解和生成方面表现出色，但它们在非代码软件工程（SE）任务中的有效性尚未得到充分探索。", "method": "本文提出了首个综合基准测试集SELU，用于评估LLM在17项非代码任务中的表现，这些任务涵盖了从识别需求功能性到估算待办事项工作量和复杂性。SELU包含分类、回归、命名实体识别（NER）和掩码语言建模（MLM）等目标，数据来源于代码仓库、问题跟踪系统和开发者论坛。研究人员对22个开源LLM进行了微调，提示了2个专有LLM，并训练了2个基线模型。性能通过F1-macro、SMAPE、F1-micro和准确率等指标进行衡量，并采用贝叶斯符号秩检验进行比较。", "result": "结果表明，中等规模的仅解码器模型持续表现出顶尖水平，具有高平均性能和低跨任务方差，而通过代码领域预训练进行的领域适应可能只会带来适度的改进。", "conclusion": "这些见解为非代码软件工程工作流中的模型选择提供了指导，并指出了将SELU扩展到生成式和设计导向场景的方向。", "translation": "大型语言模型（LLMs）在代码理解和生成方面表现出卓越的能力；然而，它们在非代码软件工程（SE）任务中的有效性仍未得到充分探索。我们提出了第一个综合基准测试集，命名为“软件工程语言理解”（SELU），用于评估LLMs在17项非代码任务上的表现，这些任务涵盖了从识别需求是功能性还是非功能性到估算待办事项的工作量和复杂性。SELU涵盖分类、回归、命名实体识别（NER）和掩码语言建模（MLM）目标，数据来源于代码仓库、问题跟踪系统和开发者论坛等多样化来源。我们对22个开源LLMs进行了微调，提示了两个专有替代方案，并训练了两个基线模型。性能通过F1-macro、SMAPE、F1-micro和准确率等指标进行衡量，并通过贝叶斯符号秩检验进行比较。我们的结果表明，中等规模的仅解码器模型始终构成顶级水平，表现出高平均性能和低跨任务方差，而通过代码领域预训练进行的领域适应可能只会带来适度的改进。这些见解指导了非代码SE工作流中的模型选择，并突出了将SELU扩展到生成式和设计导向场景的方向。", "summary": "本文提出了首个用于评估大型语言模型（LLMs）在17项不同非代码软件工程（SE）任务上表现的综合基准测试集SELU。SELU涵盖分类、回归、命名实体识别（NER）和掩码语言建模（MLM）等任务类型，数据来源于多种渠道。研究人员对22个开源LLM进行了微调，并评估了2个专有LLM和2个基线模型，通过多种指标衡量性能。结果显示，中等规模的仅解码器模型表现出卓越且稳定的一致性，而通过代码领域预训练进行的领域适应仅带来了有限的提升。这些发现为非代码SE工作流中的模型选择提供了指导，并指明了SELU未来扩展的方向。", "keywords": "大型语言模型, 软件工程, 非代码任务, 基准测试, SELU", "comments": "本文的主要创新在于创建了SELU，这是首个专门针对非代码软件工程任务的综合基准测试集。这填补了LLM评估领域的一个重要空白，因为此前大多数工作都集中在代码相关任务上。研究发现中等规模的仅解码器模型表现出色，而代码领域预训练对非代码任务的益处有限，这一洞察对从业者和研究人员都至关重要，它能指导SE领域更高效的模型开发和选择。"}}
{"id": "2506.10884", "title": "Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies", "authors": ["Dong Hae Mangalindan", "Karthik Kandikonda", "Ericka Rovira", "Vaibhav Srivastava"], "summary": "With increasing efficiency and reliability, autonomous systems are becoming\nvaluable assistants to humans in various tasks. In the context of\nrobot-assisted delivery, we investigate how robot performance and trust repair\nstrategies impact human trust. In this task, while handling a secondary task,\nhumans can choose to either send the robot to deliver autonomously or manually\ncontrol it. The trust repair strategies examined include short and long\nexplanations, apology and promise, and denial.\n  Using data from human participants, we model human behavior using an\nInput-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and\nhuman action probabilities. Our findings indicate that humans are more likely\nto deploy the robot autonomously when their trust is high. Furthermore, state\ntransition estimates show that long explanations are the most effective at\nrepairing trust following a failure, while denial is most effective at\npreventing trust loss.\n  We also demonstrate that the trust estimates generated by our model are\nisomorphic to self-reported trust values, making them interpretable. This model\nlays the groundwork for developing optimal policies that facilitate real-time\nadjustment of human trust in autonomous systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10884v1", "AI": {"title_translation": "机器人辅助递送中信任动态建模：信任修复策略的影响", "tldr": "本文研究了在机器人辅助递送中，机器人性能和信任修复策略如何影响人类信任。通过使用IOHMM建模人类行为，发现高信任度促使自主部署，长解释最能修复故障后的信任，而否认最能防止信任损失。", "motivation": "随着自主系统在效率和可靠性方面的提升，它们在各种任务中成为人类的重要助手。本研究旨在机器人辅助递送的背景下，探究机器人性能和信任修复策略如何影响人类对机器人的信任。", "method": "研究收集了人类参与者的数据，并使用输入-输出隐马尔可夫模型（IOHMM）来建模人类行为，以捕捉信任的动态和人类行动的概率。考察的信任修复策略包括简短和冗长的解释、道歉和承诺以及否认。", "result": "研究结果表明，当人类信任度高时，他们更倾向于自主部署机器人。状态转换估计显示，长解释在故障后修复信任方面最有效，而否认在防止信任损失方面最有效。此外，模型生成的信任估计与自我报告的信任值同构，具有可解释性。", "conclusion": "该模型为未来开发优化策略奠定了基础，这些策略有助于实时调整人类对自主系统的信任。", "translation": "随着效率和可靠性的不断提高，自主系统正成为人类在各种任务中的宝贵助手。在机器人辅助递送的背景下，我们研究了机器人性能和信任修复策略如何影响人类信任。在此任务中，人类在处理次要任务的同时，可以选择让机器人自主递送或手动控制它。所考察的信任修复策略包括简短和冗长的解释、道歉和承诺以及否认。\n我们利用人类参与者的数据，使用输入-输出隐马尔可夫模型（IOHMM）来建模人类行为，以捕捉信任的动态和人类行动的概率。我们的研究结果表明，当信任度高时，人类更倾向于自主部署机器人。此外，状态转换估计表明，冗长的解释在故障后修复信任方面最有效，而否认在防止信任损失方面最有效。\n我们还证明，模型生成的信任估计与自我报告的信任值同构，使其具有可解释性。该模型为开发优化策略奠定了基础，这些策略有助于实时调整人类对自主系统的信任。", "summary": "本文在机器人辅助递送情境下，探究了机器人性能和不同信任修复策略（如解释、道歉、承诺和否认）对人类信任的影响。研究通过收集人类参与者数据，并利用输入-输出隐马尔可夫模型（IOHMM）对人类行为进行建模，以捕捉信任动态。主要发现包括：人类在高信任度下更倾向于自主部署机器人；长解释在故障后修复信任最有效；而否认在防止信任损失方面最有效。此外，模型生成的信任估计与自我报告的信任值一致，具有良好的可解释性。该模型为未来开发能够实时调整人类对自主系统信任的优化策略奠定了基础。", "keywords": "机器人辅助递送, 信任动态, 信任修复策略, 输入-输出隐马尔可夫模型, 人机交互", "comments": "这篇论文通过引入IOHMM来建模人类对机器人的信任动态，并评估了不同信任修复策略的效果，具有创新性。其发现对于设计更有效的人机协作系统，尤其是在自主系统出现故障时如何维护和修复人类信任具有重要指导意义。模型的可解释性也增强了其在实际应用中的潜力。"}}
{"id": "2506.10753", "title": "Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering", "authors": ["Adam Ishay", "Zhun Yang", "Joohyung Lee", "Ilgu Kang", "Dongjae Lim"], "summary": "Causal and temporal reasoning about video dynamics is a challenging problem.\nWhile neuro-symbolic models that combine symbolic reasoning with neural-based\nperception and prediction have shown promise, they exhibit limitations,\nespecially in answering counterfactual questions. This paper introduces a\nmethod to enhance a neuro-symbolic model for counterfactual reasoning,\nleveraging symbolic reasoning about causal relations among events. We define\nthe notion of a causal graph to represent such relations and use Answer Set\nProgramming (ASP), a declarative logic programming method, to find how to\ncoordinate perception and simulation modules. We validate the effectiveness of\nour approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves\nstate-of-the-art performance on the CLEVRER challenge, significantly\noutperforming existing models. In the case of the CRAFT benchmark, we leverage\na large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a\ndynamics simulator. Our findings show that this method can further improve its\nperformance on counterfactual questions by providing alternative prompts\ninstructed by symbolic causal reasoning.", "comment": "In Proceedings the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2024)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10753v1", "AI": {"title_translation": "先思考再模拟：符号推理协调神经计算以进行反事实问答", "tldr": "论文提出一种通过符号推理（因果图和ASP）增强神经符号模型的方法，用于反事实问答，在CLEVRER上达到SOTA，并在CRAFT上结合大型语言模型进一步提升性能。", "motivation": "视频动态的因果和时间推理是一个挑战性问题。现有的神经符号模型在结合符号推理与神经感知和预测方面表现出潜力，但在回答反事实问题时存在局限性，尤其是在反事实问答方面。", "method": "本文引入一种方法，通过利用事件间的因果关系进行符号推理，来增强神经符号模型处理反事实推理的能力。具体而言，定义了因果图来表示这些关系，并使用答案集编程（ASP）来协调感知和模拟模块。在CRAFT基准测试中，还利用了大型预训练语言模型（如GPT-3.5和GPT-4）作为动态模拟器的代理，并通过符号因果推理指导的替代提示来改进性能。", "result": "该增强方法在CLEVRER挑战赛上实现了最先进的性能，显著优于现有模型。在CRAFT基准测试中，结合大型预训练语言模型并提供由符号因果推理指导的替代提示，可以进一步提高反事实问题的性能。", "conclusion": "通过符号推理（特别是因果图和ASP）协调神经计算，可以有效增强神经符号模型在处理反事实问答方面的能力，并在不同基准测试中展现出卓越性能，甚至能与大型语言模型结合进一步提升效果。", "translation": "视频动态的因果和时间推理是一个具有挑战性的问题。虽然结合了符号推理与基于神经网络的感知和预测的神经符号模型已显示出前景，但它们存在局限性，尤其是在回答反事实问题方面。本文介绍了一种增强神经符号模型反事实推理能力的方法，该方法利用事件间因果关系的符号推理。我们定义了因果图的概念来表示此类关系，并使用答案集编程（ASP）这一声明式逻辑编程方法来寻找如何协调感知和模拟模块。我们在CLEVRER和CRAFT两个基准测试上验证了我们方法的有效性。我们的增强在CLEVRER挑战赛上取得了最先进的性能，显著优于现有模型。在CRAFT基准测试中，我们利用大型预训练语言模型（例如GPT-3.5和GPT-4）作为动态模拟器的代理。我们的研究结果表明，通过符号因果推理指导的替代提示，该方法可以进一步提高其在反事实问题上的性能。", "summary": "本文提出了一种增强神经符号模型反事实问答能力的新方法。通过引入因果图来表示事件间的因果关系，并利用答案集编程（ASP）协调感知和模拟模块，该方法在CLEVRER基准测试上达到了最先进的性能。此外，在CRAFT基准测试中，结合大型预训练语言模型（如GPT-3.5/GPT-4）并利用符号因果推理生成提示，进一步提升了反事实问题的回答效果。这表明符号推理在复杂推理任务中能有效指导神经计算。", "keywords": "神经符号模型, 反事实问答, 符号推理, 因果图, 答案集编程", "comments": "这篇论文的创新点在于将符号推理（特别是因果图和ASP）深度整合到神经符号模型中，以解决反事实问答这一复杂问题。通过“先思考再模拟”的理念，利用符号逻辑指导神经模块的感知和模拟，显著提升了模型性能。在与大型语言模型结合时，符号推理也能提供有价值的指导，这为未来神经符号AI的发展提供了新的方向。该方法在处理因果推理和反事实情景方面展现出强大潜力。"}}
{"id": "2506.10177", "title": "Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models", "authors": ["Defang Chen", "Zhenyu Zhou", "Can Wang", "Siwei Lyu"], "summary": "Diffusion-based generative models employ stochastic differential equations\n(SDEs) and their equivalent probability flow ordinary differential equations\n(ODEs) to establish a smooth transformation between complex high-dimensional\ndata distributions and tractable prior distributions. In this paper, we reveal\na striking geometric regularity in the deterministic sampling dynamics: each\nsimulated sampling trajectory lies within an extremely low-dimensional\nsubspace, and all trajectories exhibit an almost identical ''boomerang'' shape,\nregardless of the model architecture, applied conditions, or generated content.\nWe characterize several intriguing properties of these trajectories,\nparticularly under closed-form solutions based on kernel-estimated data\nmodeling. We also demonstrate a practical application of the discovered\ntrajectory regularity by proposing a dynamic programming-based scheme to better\nalign the sampling time schedule with the underlying trajectory structure. This\nsimple strategy requires minimal modification to existing ODE-based numerical\nsolvers, incurs negligible computational overhead, and achieves superior image\ngeneration performance, especially in regions with only $5 \\sim 10$ function\nevaluations.", "comment": "50 pages. The short version appeared in ICML 2024. arXiv admin note:\n  substantial text overlap with arXiv:2405.11326", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10177v1", "AI": {"title_translation": "扩散生成模型确定性采样中的几何规律性", "tldr": "扩散模型的确定性采样轨迹具有低维几何规律性（“回旋镖”形状），利用此规律可提高图像生成性能。", "motivation": "扩散模型在复杂数据分布和可处理先验分布之间建立平滑变换，但其确定性采样动力学中的几何特性尚未被充分揭示和利用。", "method": "揭示并表征了确定性采样轨迹的几何规律性，特别是在基于核估计数据建模的闭合形式解下。提出了一种基于动态规划的方案，将采样时间表与底层轨迹结构更好地对齐。", "result": "发现每个模拟采样轨迹都位于一个极低维子空间内，所有轨迹都呈现几乎相同的“回旋镖”形状，不受模型架构、应用条件或生成内容的影响。提出的动态规划方案只需对现有基于ODE的数值求解器进行最小修改，计算开销可忽略不计，并在图像生成性能上取得卓越表现，尤其是在只有5到10次函数评估的区域。", "conclusion": "扩散模型的确定性采样轨迹存在显著的几何规律性，利用这一规律可以有效优化采样时间调度，从而在低函数评估次数下显著提升图像生成质量。", "translation": "扩散生成模型利用随机微分方程（SDEs）及其等效的概率流常微分方程（ODEs）在复杂高维数据分布和可处理的先验分布之间建立平滑变换。在本文中，我们揭示了确定性采样动力学中一个显著的几何规律性：每个模拟的采样轨迹都位于一个极低维子空间内，并且所有轨迹都呈现出几乎相同的“回旋镖”形状，无论模型架构、应用条件或生成内容如何。我们描述了这些轨迹的几个有趣特性，特别是在基于核估计数据建模的闭合形式解下。我们还展示了所发现轨迹规律性的一种实际应用，即提出了一种基于动态规划的方案，以更好地将采样时间表与底层轨迹结构对齐。这种简单的策略只需对现有基于ODE的数值求解器进行最小修改，计算开销可忽略不计，并在图像生成性能上取得卓越表现，尤其是在只有5到10次函数评估的区域。", "summary": "本文揭示了扩散模型确定性采样轨迹中显著的几何规律性：所有轨迹都位于极低维子空间，并呈现一致的“回旋镖”形状。基于此发现，提出了一种利用动态规划优化采样时间表的方案，该方案计算开销小，能显著提升图像生成性能，尤其是在低函数评估次数下。", "keywords": "扩散模型, 确定性采样, 几何规律性, 动态规划, 图像生成", "comments": "这项研究揭示了扩散模型确定性采样过程中的一个基本几何特性，为理解和优化扩散模型提供了新的视角。通过利用这一规律性，论文提出了一种高效的采样策略，显著提升了生成性能，尤其是在计算资源有限或需要快速生成的情况下，具有重要的实际应用价值。其创新点在于从几何角度深入分析了采样动力学，并成功将其转化为实际的性能提升。"}}
{"id": "2506.10335", "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting", "authors": ["Lintao Xiang", "Hongpei Zheng", "Yating Huang", "Qijun Yang", "Hujun Yin"], "summary": "3D Gaussian splatting (3DGS) is an innovative rendering technique that\nsurpasses the neural radiance field (NeRF) in both rendering speed and visual\nquality by leveraging an explicit 3D scene representation. Existing 3DGS\napproaches require a large number of calibrated views to generate a consistent\nand complete scene representation. When input views are limited, 3DGS tends to\noverfit the training views, leading to noticeable degradation in rendering\nquality. To address this limitation, we propose a Point-wise Feature-Aware\nGaussian Splatting framework that enables real-time, high-quality rendering\nfrom sparse training views. Specifically, we first employ the latest stereo\nfoundation model to estimate accurate camera poses and reconstruct a dense\npoint cloud for Gaussian initialization. We then encode the colour attributes\nof each 3D Gaussian by sampling and aggregating multiscale 2D appearance\nfeatures from sparse inputs. To enhance point-wise appearance representation,\nwe design a point interaction network based on a self-attention mechanism,\nallowing each Gaussian point to interact with its nearest neighbors. These\nenriched features are subsequently decoded into Gaussian parameters through two\nlightweight multi-layer perceptrons (MLPs) for final rendering. Extensive\nexperiments on diverse benchmarks demonstrate that our method significantly\noutperforms NeRF-based approaches and achieves competitive performance under\nfew-shot settings compared to the state-of-the-art 3DGS methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10335v1", "AI": {"title_translation": "PointGS：基于点注意力感知高斯泼溅的稀疏视图合成", "tldr": "PointGS 提出了一种新的高斯泼溅框架，通过点注意力机制和多尺度特征聚合，实现了从稀疏视图进行高质量实时渲染，解决了传统 3DGS 在稀疏视图下过拟合的问题。", "motivation": "现有的 3D Gaussian Splatting (3DGS) 方法需要大量校准视图来生成一致且完整的场景表示。当输入视图有限时，3DGS 倾向于过拟合训练视图，导致渲染质量显著下降。", "method": "PointGS 框架包括：1. 采用最新的立体基础模型估计精确相机姿态并重建密集点云用于高斯初始化。2. 通过采样和聚合稀疏输入的多尺度 2D 外观特征来编码每个 3D 高斯点的颜色属性。3. 设计一个基于自注意力机制的点交互网络，使每个高斯点与其最近邻点进行交互，以增强点级外观表示。4. 将丰富后的特征通过两个轻量级多层感知器 (MLP) 解码为高斯参数进行最终渲染。", "result": "在各种基准测试中，PointGS 显著优于基于 NeRF 的方法，并且在少量视图设置下与最先进的 3DGS 方法相比，取得了具有竞争力的性能。", "conclusion": "PointGS 成功解决了 3DGS 在稀疏视图下渲染质量下降的问题，通过点注意力感知框架实现了高质量的实时渲染，并展现出优于 NeRF 和与 SOTA 3DGS 相当的性能。", "translation": "3D Gaussian splatting (3DGS) 是一种创新的渲染技术，通过利用显式的 3D 场景表示，在渲染速度和视觉质量上都超越了神经辐射场 (NeRF)。现有的 3DGS 方法需要大量校准视图才能生成一致且完整的场景表示。当输入视图有限时，3DGS 倾向于过拟合训练视图，导致渲染质量显著下降。为了解决这一限制，我们提出了一种点式特征感知高斯泼溅框架，该框架能够从稀疏训练视图进行实时、高质量渲染。具体来说，我们首先采用最新的立体基础模型来估计精确的相机姿态并重建密集点云用于高斯初始化。然后，我们通过采样和聚合稀疏输入的多尺度 2D 外观特征来编码每个 3D 高斯点的颜色属性。为了增强点式外观表示，我们设计了一个基于自注意力机制的点交互网络，允许每个高斯点与其最近邻点进行交互。这些丰富后的特征随后通过两个轻量级多层感知器 (MLP) 解码为高斯参数进行最终渲染。在各种基准测试上进行的广泛实验表明，我们的方法显著优于基于 NeRF 的方法，并且在少量视图设置下与最先进的 3DGS 方法相比，取得了具有竞争力的性能。", "summary": "本文提出了 PointGS，一个点式特征感知的高斯泼溅框架，旨在解决 3DGS 在稀疏视图下渲染质量下降的问题。PointGS 首先利用立体基础模型进行相机姿态估计和点云初始化，然后通过聚合多尺度 2D 特征编码高斯点的颜色属性。其核心创新在于引入了一个基于自注意力机制的点交互网络，以增强点级外观表示。实验证明，PointGS 在稀疏视图设置下实现了高质量的实时渲染，性能优于 NeRF，并与最先进的 3DGS 方法相当。", "keywords": "3D Gaussian Splatting, 稀疏视图合成, 点注意力, 实时渲染, 神经辐射场", "comments": "该论文的创新点在于提出了一个点式特征感知框架，通过引入自注意力机制的点交互网络，有效地增强了高斯点的外观表示，从而解决了 3DGS 在稀疏视图下过拟合的痛点。这对于实际应用中数据采集受限的场景具有重要意义，提升了 3DGS 在低资源条件下的实用性和渲染质量。"}}
{"id": "2506.10620", "title": "Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation", "authors": ["Stefano Longari", "Paolo Cerracchio", "Michele Carminati", "Stefano Zanero"], "summary": "The security of modern vehicles has become increasingly important, with the\ncontroller area network (CAN) bus serving as a critical communication backbone\nfor various Electronic Control Units (ECUs). The absence of robust security\nmeasures in CAN, coupled with the increasing connectivity of vehicles, makes\nthem susceptible to cyberattacks. While intrusion detection systems (IDSs) have\nbeen developed to counter such threats, they are not foolproof. Adversarial\nattacks, particularly evasion attacks, can manipulate inputs to bypass\ndetection by IDSs. This paper extends our previous work by investigating the\nfeasibility and impact of gradient-based adversarial attacks performed with\ndifferent degrees of knowledge against automotive IDSs. We consider three\nscenarios: white-box (attacker with full system knowledge), grey-box (partial\nsystem knowledge), and the more realistic black-box (no knowledge of the IDS'\ninternal workings or data). We evaluate the effectiveness of the proposed\nattacks against state-of-the-art IDSs on two publicly available datasets.\nAdditionally, we study effect of the adversarial perturbation on the attack\nimpact and evaluate real-time feasibility by precomputing evasive payloads for\ntimed injection based on bus traffic. Our results demonstrate that, besides\nattacks being challenging due to the automotive domain constraints, their\neffectiveness is strongly dependent on the dataset quality, the target IDS, and\nthe attacker's degree of knowledge.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10620v1", "AI": {"title_translation": "评估车载入侵检测系统对对抗性操纵的弹性", "tldr": "本文研究了梯度对抗性攻击对车载入侵检测系统的影响，考虑了白盒、灰盒和黑盒场景，并评估了攻击有效性及其对数据集质量、IDS和攻击者知识程度的依赖性。", "motivation": "现代车辆的安全性日益重要，而控制器局域网（CAN）总线缺乏强大的安全措施，加上车辆互联性增加，使其容易受到网络攻击。尽管已开发出入侵检测系统（IDS）来应对此类威胁，但它们并非万无一失，对抗性攻击，特别是规避攻击，可以操纵输入以绕过IDS的检测。", "method": "本文扩展了之前的工作，调查了在不同知识程度下进行的基于梯度的对抗性攻击对车载IDS的可行性和影响。研究考虑了三种场景：白盒（攻击者拥有完整的系统知识）、灰盒（部分系统知识）和更现实的黑盒（对IDS内部工作或数据一无所知）。研究在两个公开可用的数据集上评估了所提出攻击对最先进IDS的有效性。此外，还研究了对抗性扰动对攻击影响的作用，并通过预计算规避性有效载荷以基于总线流量进行定时注入来评估实时可行性。", "result": "结果表明，除了由于汽车领域限制导致攻击具有挑战性之外，其有效性强烈依赖于数据集质量、目标IDS和攻击者的知识程度。", "conclusion": "Not mentioned in abstract", "translation": "现代车辆的安全性日益重要，控制器局域网（CAN）总线作为各种电子控制单元（ECU）的关键通信骨干。CAN总线缺乏强大的安全措施，加上车辆互联性增加，使其容易受到网络攻击。尽管已开发出入侵检测系统（IDS）来应对此类威胁，但它们并非万无一失。对抗性攻击，特别是规避攻击，可以操纵输入以绕过IDS的检测。本文扩展了我们之前的工作，调查了在不同知识程度下进行的基于梯度的对抗性攻击对车载IDS的可行性和影响。我们考虑了三种场景：白盒（攻击者拥有完整的系统知识）、灰盒（部分系统知识）和更现实的黑盒（对IDS内部工作或数据一无所知）。我们在两个公开可用的数据集上评估了所提出攻击对最先进IDS的有效性。此外，我们研究了对抗性扰动对攻击影响的作用，并通过预计算规避性有效载荷以基于总线流量进行定时注入来评估实时可行性。我们的结果表明，除了由于汽车领域限制导致攻击具有挑战性之外，其有效性强烈依赖于数据集质量、目标IDS和攻击者的知识程度。", "summary": "本文探讨了针对车载入侵检测系统（IDS）的梯度对抗性攻击的可行性和影响，旨在评估其对规避攻击的弹性。研究考虑了白盒、灰盒和黑盒三种攻击场景，并在公开数据集上评估了攻击对现有IDS的有效性。此外，文章还分析了对抗性扰动对攻击效果的影响以及实时注入规避性有效载荷的可行性。研究发现，攻击的有效性受限于汽车领域约束，并强烈依赖于数据集质量、目标IDS和攻击者的知识程度。", "keywords": "车载IDS, 对抗性攻击, CAN总线安全, 规避攻击, 梯度攻击", "comments": "该研究通过全面考虑不同知识程度的攻击场景，深入探讨了车载IDS在面对对抗性攻击时的脆弱性，具有重要的实践意义。它强调了汽车领域特有的挑战，并指出了未来IDS设计需要考虑的关键因素，如数据集质量和对抗性鲁棒性。其创新点在于评估了实时注入规避性有效载荷的可行性。"}}
{"id": "2506.10869", "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework", "authors": ["Quinn Thibeault", "Giulia Pedrielli"], "summary": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10869v1", "AI": {"title_translation": "MultiCoSim：一个基于Python的多精度协同仿真框架", "tldr": "MultiCoSim是一个基于Python的框架，旨在为复杂的网络物理系统提供灵活、分布式、多精度的协同仿真，解决了现有工具的局限性。", "motivation": "现有用于网络物理系统（CPS）的仿真工具配置僵化、缺乏自动化支持，并且在可移植性和模块化方面存在障碍，这阻碍了对日益复杂的CPS进行快速、准确的分析以及可重用基准的开发。", "method": "本文引入了MultiCoSim，一个基于Python的仿真框架，允许用户以编程方式定义、组合和配置仿真组件。MultiCoSim支持分布式、基于组件的协同仿真，并允许无缝替换和重新配置组件。", "result": "通过涉及自定义基于自动机的控制器以及与PX4无人机自动驾驶仪等现成平台集成的案例研究，展示了MultiCoCoSim的灵活性。这些示例突出了MultiCoSim简化CPS研究和开发仿真流程的能力。", "conclusion": "MultiCoSim有效地解决了现有仿真工具的挑战，通过提供一个灵活、编程化和模块化的框架，用于复杂网络物理系统的多精度协同仿真，这已通过案例研究得到证明。", "translation": "仿真分析是网络物理系统（CPS）分析和测试的基础工具，支撑着算法开发、运行时监控和系统验证等活动。随着CPS复杂性和规模的增长，特别是在安全关键和支持学习的环境中，精确的分析和合成越来越依赖于仿真实验的快速使用。由于CPS本质上集成了硬件、软件和物理过程，仿真平台必须支持异构组件在不同精度级别的协同仿真。尽管硬件、固件和物理的高精度建模最近取得了进展，但在多样化环境中的协同仿真仍然具有挑战性。这些限制阻碍了可重用基准的开发，并阻碍了仿真在自动化和比较评估中的使用。\n现有仿真工具通常依赖于僵化的配置，缺乏自动化支持，并对可移植性和模块化造成障碍。许多工具通过静态文本文件进行配置，或对仿真组件的表示和连接方式施加限制，使得灵活地组合系统或跨平台集成组件变得困难。\n为了解决这些挑战，我们引入了MultiCoSim，一个基于Python的仿真框架，使用户能够以编程方式定义、组合和配置仿真组件。MultiCoSim支持分布式、基于组件的协同仿真，并允许组件的无缝替换和重新配置。我们通过案例研究展示了MultiCoSim的灵活性，其中包括涉及自定义基于自动机的控制器以及与用于空中机器人的PX4自动驾驶仪等现成平台的协同仿真。这些示例突出了MultiCoSim简化CPS研究和开发仿真流程的能力。", "summary": "MultiCoSim是一个基于Python的协同仿真框架，旨在解决现有网络物理系统（CPS）仿真工具的局限性。它支持分布式、多精度、组件化的协同仿真，允许用户通过编程方式定义、组合和配置仿真组件，实现无缝替换和重新配置。通过与PX4自动驾驶仪等平台的集成以及自定义控制器的案例研究，MultiCoSim展示了其在简化CPS研究和开发仿真流程方面的灵活性和有效性。", "keywords": "多精度, 协同仿真, 网络物理系统, Python, 仿真框架", "comments": "MultiCoSim的创新之处在于其Python基础和编程接口，大大提高了仿真配置的灵活性、模块化和自动化程度，这对于复杂且安全关键的网络物理系统至关重要。它解决了传统工具的刚性配置问题，有助于推动可重用基准和自动化评估的发展。"}}
{"id": "2506.10923", "title": "Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations", "authors": ["Xili Yi", "Nima Fazeli"], "summary": "We introduce Vib2Move, a novel approach for in-hand object reconfiguration\nthat uses fingertip micro-vibrations and gravity to precisely reposition planar\nobjects. Our framework comprises three key innovations. First, we design a\nvibration-based actuator that dynamically modulates the effective finger-object\nfriction coefficient, effectively emulating changes in gripping force. Second,\nwe derive a sliding motion model for objects clamped in a parallel gripper with\ntwo symmetric, variable-friction contact patches. Third, we propose a motion\nplanner that coordinates end-effector finger trajectories and fingertip\nvibrations to achieve the desired object pose. In real-world trials, Vib2Move\nconsistently yields final positioning errors below 6 mm, demonstrating\nreliable, high-precision manipulation across a variety of planar objects. For\nmore results and information, please visit https://vib2move.github.io.", "comment": "11 pages, 12 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10923v1", "AI": {"title_translation": "Vib2Move：通过指尖微振动实现手内物体重构", "tldr": "Vib2Move是一种利用指尖微振动和重力精确重新定位平面物体的手内物体重构新方法，实现了可靠的高精度操作。", "motivation": "该研究旨在通过一种新颖的方法，即利用指尖微振动和重力，实现手内物体的精确重新定位。", "method": "该方法包含三项关键创新：设计了一种动态调节有效手指-物体摩擦系数的振动执行器；推导了在具有两个对称、可变摩擦接触点的平行夹持器中夹持物体的滑动运动模型；提出了一个运动规划器，用于协调末端执行器手指轨迹和指尖振动以实现所需的物体姿态。", "result": "在实际试验中，Vib2Move的最终定位误差始终低于6毫米，表明其在各种平面物体上实现了可靠的高精度操作。", "conclusion": "Vib2Move通过利用指尖微振动和重力，能够可靠且高精度地实现手内物体的重构。", "translation": "我们引入了Vib2Move，这是一种利用指尖微振动和重力精确重新定位平面物体的手内物体重构新方法。我们的框架包含三项关键创新。首先，我们设计了一种基于振动的执行器，可以动态调节有效的手指-物体摩擦系数，有效地模拟抓取力的变化。其次，我们推导了一个在具有两个对称、可变摩擦接触点的平行夹持器中夹持物体的滑动运动模型。第三，我们提出了一个运动规划器，用于协调末端执行器手指轨迹和指尖振动，以实现所需的物体姿态。在实际试验中，Vib2Move的最终定位误差始终低于6毫米，表明其在各种平面物体上实现了可靠的高精度操作。欲了解更多结果和信息，请访问 https://vib2move.github.io。", "summary": "Vib2Move提出了一种新颖的手内物体重构方法，该方法利用指尖微振动和重力来精确重新定位平面物体。其核心创新包括一个能动态调节摩擦系数的振动执行器、一个用于平行夹持器的滑动运动模型以及一个协调手指轨迹和振动的运动规划器。实验结果显示，Vib2Move的最终定位误差小于6毫米，证明了其在多种平面物体操作中的高精度和可靠性。", "keywords": "手内重构, 微振动, 摩擦调节, 运动规划, 平面物体操作", "comments": "Vib2Move的创新之处在于利用指尖微振动来动态调节摩擦力，从而实现对物体精细的在手操作，这为机器人抓取和操纵领域提供了一个新颖且高效的解决方案。其在实际应用中展现出的高精度和可靠性具有重要意义，尤其是在需要精细调整物体姿态的场景。"}}
{"id": "2506.10764", "title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems", "authors": ["Xiaozhe Li", "Jixuan Chen", "Xinyu Fang", "Shengyuan Ding", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "summary": "Large Language Models (LLMs) have shown remarkable capabilities in solving\ndiverse tasks. However, their proficiency in iteratively optimizing complex\nsolutions through learning from previous feedback remains insufficiently\nexplored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark\ndesigned to evaluate LLM agents on large-scale search space optimization\nproblems. OPT-BENCH includes 20 real-world machine learning tasks sourced from\nKaggle and 10 classical NP problems, offering a diverse and challenging\nenvironment for assessing LLM agents on iterative reasoning and solution\nrefinement. To enable rigorous evaluation, we introduce OPT-Agent, an\nend-to-end optimization framework that emulates human reasoning when tackling\ncomplex problems by generating, validating, and iteratively improving solutions\nthrough leveraging historical feedback. Through extensive experiments on 9\nstate-of-the-art LLMs from 6 model families, we analyze the effects of\noptimization iterations, temperature settings, and model architectures on\nsolution quality and convergence. Our results demonstrate that incorporating\nhistorical context significantly enhances optimization performance across both\nML and NP tasks. All datasets, code, and evaluation tools are open-sourced to\npromote further research in advancing LLM-driven optimization and iterative\nreasoning. Project page:\n\\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10764v1", "AI": {"title_translation": "OPT-BENCH：评估大型语言模型智能体在大型搜索空间优化问题上的表现", "tldr": "OPT-BENCH是一个新基准，用于评估LLM智能体在大型搜索空间优化问题上的迭代优化能力，并发现结合历史上下文能显著提升性能。", "motivation": "大型语言模型（LLMs）在解决各种任务方面表现出色，但其通过从历史反馈中学习来迭代优化复杂解决方案的能力尚未得到充分探索。", "method": "本研究提出了OPT-BENCH，一个综合基准，用于评估LLM智能体在大型搜索空间优化问题上的表现，包含20个Kaggle真实世界机器学习任务和10个经典NP问题。同时引入了OPT-Agent，一个端到端优化框架，通过生成、验证和迭代改进解决方案来模拟人类推理。研究对来自6个模型家族的9个最先进的LLM进行了广泛实验，分析了优化迭代次数、温度设置和模型架构对解决方案质量和收敛性的影响。", "result": "实验结果表明，结合历史上下文显著增强了LLM在机器学习任务和NP任务上的优化性能。", "conclusion": "结合历史上下文能够显著提升大型语言模型在迭代优化任务中的表现。所有数据集、代码和评估工具均已开源，以促进LLM驱动优化和迭代推理领域的进一步研究。", "translation": "大型语言模型（LLMs）在解决各种任务方面表现出卓越的能力。然而，它们通过从历史反馈中学习来迭代优化复杂解决方案的能力仍未得到充分探索。为了弥补这一空白，我们提出了OPT-BENCH，一个综合基准，旨在评估大型语言模型智能体在大型搜索空间优化问题上的表现。OPT-BENCH包含来自Kaggle的20个真实世界机器学习任务和10个经典NP问题，为评估大型语言模型智能体在迭代推理和解决方案优化方面的能力提供了多样化和具有挑战性的环境。为了实现严格评估，我们引入了OPT-Agent，一个端到端的优化框架，它通过利用历史反馈生成、验证和迭代改进解决方案来模拟人类解决复杂问题时的推理过程。通过对来自6个模型家族的9个最先进大型语言模型的广泛实验，我们分析了优化迭代次数、温度设置和模型架构对解决方案质量和收敛性的影响。我们的结果表明，结合历史上下文显著增强了机器学习任务和NP任务的优化性能。所有数据集、代码和评估工具都已开源，以促进大型语言模型驱动优化和迭代推理领域的进一步研究。项目页面：https://github.com/OliverLeeXZ/OPT-BENCH。", "summary": "本研究提出了OPT-BENCH，一个旨在评估大型语言模型（LLM）智能体在大型搜索空间优化问题上迭代优化能力的综合基准。OPT-BENCH包含了来自Kaggle的20个真实世界机器学习任务和10个经典NP问题。为实现严格评估，研究引入了OPT-Agent框架，该框架通过生成、验证和迭代改进解决方案来模拟人类推理。通过对9个最先进LLM的广泛实验，研究发现结合历史上下文能显著提升LLM在优化任务中的表现。所有相关资源均已开源，以促进该领域的进一步研究。", "keywords": "大型语言模型, 优化, 迭代推理, 基准测试, 智能体", "comments": "OPT-BENCH的创新之处在于它首次系统地评估了LLM智能体在大型搜索空间优化问题上的迭代优化能力，填补了现有研究的空白。通过引入OPT-Agent框架，该研究模拟了人类在复杂问题解决中的迭代推理过程，为LLM的优化行为提供了新的评估视角。其开源的数据集、代码和工具将极大地促进LLM驱动优化和迭代推理领域的未来研究。"}}
{"id": "2506.10180", "title": "A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes", "authors": ["Mowafaq Salem Alzboon", "Mohammad Al-Batah", "Muhyeeddin Alqaraleh", "Ahmad Abuashour", "Ahmad Fuad Bader"], "summary": "In many nations, diabetes is becoming a significant health problem, and early\nidentification and control are crucial. Using machine learning algorithms to\npredict diabetes has yielded encouraging results. Using the Pima Indians\nDiabetes dataset, this study attempts to evaluate the efficacy of several\nmachine-learning methods for diabetes prediction. The collection includes\ninformation on 768 patients, such as their ages, BMIs, and glucose levels. The\ntechniques assessed are Logistic Regression, Decision Tree, Random Forest,\nk-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,\nand Neural Network. The findings indicate that the Neural Network algorithm\nperformed the best, with an accuracy of 78.57 percent, followed by the Random\nForest method, with an accuracy of 76.30 percent. The study implies that\nmachine learning algorithms can aid diabetes prediction and be an efficient\nearly detection tool.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10180v1", "AI": {"title_translation": "机器学习技术在糖尿病早期预测中的比较研究", "tldr": "本研究比较了多种机器学习算法在Pima印第安人糖尿病数据集上进行糖尿病早期预测的有效性，结果显示神经网络表现最佳。", "motivation": "糖尿病在全球范围内日益成为严重的健康问题，早期识别和控制至关重要。利用机器学习算法进行糖尿病预测已显示出积极效果，本研究旨在评估不同机器学习方法在糖尿病预测中的有效性。", "method": "本研究使用Pima印第安人糖尿病数据集，该数据集包含768名患者的年龄、BMI和血糖水平等信息。评估了多种机器学习技术，包括逻辑回归、决策树、随机森林、k-最近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络。", "result": "研究结果表明，神经网络算法表现最佳，准确率为78.57%；其次是随机森林方法，准确率为76.30%。", "conclusion": "本研究表明，机器学习算法可以辅助糖尿病预测，并作为一种有效的早期检测工具。", "translation": "在许多国家，糖尿病正成为一个重大的健康问题，早期识别和控制至关重要。利用机器学习算法预测糖尿病已取得令人鼓舞的成果。本研究旨在利用Pima印第安人糖尿病数据集，评估多种机器学习方法在糖尿病预测中的有效性。该数据集包含768名患者的信息，如年龄、体重指数（BMI）和血糖水平。评估的技术包括逻辑回归、决策树、随机森林、k-最近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络。研究结果表明，神经网络算法表现最佳，准确率为78.57%，其次是随机森林方法，准确率为76.30%。该研究表明，机器学习算法可以帮助糖尿病预测，并成为一种有效的早期检测工具。", "summary": "本研究旨在比较不同机器学习技术在糖尿病早期预测中的性能。研究使用了包含768名患者数据的Pima印第安人糖尿病数据集，并评估了逻辑回归、决策树、随机森林、k-最近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络等算法。结果显示，神经网络以78.57%的准确率表现最佳，其次是随机森林。研究强调了机器学习算法在糖尿病早期检测中的潜力。", "keywords": "机器学习, 糖尿病预测, 早期检测, 比较研究, 神经网络", "comments": "该研究通过比较多种主流机器学习算法在特定数据集上的性能，为糖尿病早期预测提供了有价值的参考。其创新点在于对多种方法的系统性评估，并明确指出了表现最优的算法。然而，研究的局限性可能在于数据集的单一性，未来的工作可以考虑在更广泛、更多样化的数据集上进行验证。"}}
{"id": "2506.10406", "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier", "authors": ["Yuhua Jiang", "Yuwen Xiong", "Yufeng Yuan", "Chao Xin", "Wenyuan Xu", "Yu Yue", "Qianchuan Zhao", "Lin Yan"], "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks, yet they still struggle to reliably verify the\ncorrectness of their own outputs. Existing solutions to this verification\nchallenge often depend on separate verifier models or require multi-stage\nself-correction training pipelines, which limit scalability. In this paper, we\npropose Policy as Generative Verifier (PAG), a simple and effective framework\nthat empowers LLMs to self-correct by alternating between policy and verifier\nroles within a unified multi-turn reinforcement learning (RL) paradigm.\nDistinct from prior approaches that always generate a second attempt regardless\nof model confidence, PAG introduces a selective revision mechanism: the model\nrevises its answer only when its own generative verification step detects an\nerror. This verify-then-revise workflow not only alleviates model collapse but\nalso jointly enhances both reasoning and verification abilities. Extensive\nexperiments across diverse reasoning benchmarks highlight PAG's dual\nadvancements: as a policy, it enhances direct generation and self-correction\naccuracy; as a verifier, its self-verification outperforms self-consistency.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10406v1", "AI": {"title_translation": "PAG：基于生成式验证器的多轮强化LLM自我纠正", "tldr": "PAG是一个通过多轮强化学习实现LLM自我纠正的框架，它让LLM在策略和验证器角色间切换，并在检测到错误时选择性地进行修正，从而提高推理和验证能力。", "motivation": "现有大型语言模型（LLMs）在复杂推理任务中表现出色，但难以可靠地验证自身输出的正确性。当前解决方案依赖独立的验证器或多阶段训练，限制了可扩展性。", "method": "论文提出了PAG（Policy as Generative Verifier）框架，通过在统一的多轮强化学习范式中，让LLM在策略和验证器角色之间交替进行自我纠正。PAG引入了选择性修正机制，即模型仅在其生成式验证步骤检测到错误时才修正答案，与以往总是生成第二次尝试的方法不同。", "result": "广泛的实验表明，PAG在两个方面都有所改进：作为策略，它提高了直接生成和自我纠正的准确性；作为验证器，其自我验证能力优于自我一致性。", "conclusion": "PAG框架通过其独特的生成式验证和选择性修正机制，有效提升了LLM的推理和验证能力，解决了现有自验证方案的局限性。", "translation": "大型语言模型（LLMs）在复杂推理任务中展现出令人印象深刻的能力，但它们仍然难以可靠地验证自身输出的正确性。现有解决这一验证挑战的方案通常依赖独立的验证器模型或需要多阶段的自我纠正训练流程，这限制了可扩展性。在本文中，我们提出了策略即生成式验证器（PAG），这是一个简单而有效的框架，它通过在统一的多轮强化学习（RL）范式中，让LLMs在策略和验证器角色之间交替，从而赋能LLMs进行自我纠正。与以往无论模型置信度如何总是生成第二次尝试的方法不同，PAG引入了一种选择性修正机制：模型仅在其自身的生成式验证步骤检测到错误时才修正其答案。这种“验证-然后-修正”的工作流程不仅缓解了模型崩溃，而且共同增强了推理和验证能力。在各种推理基准上的广泛实验突出了PAG的双重进步：作为策略，它提高了直接生成和自我纠正的准确性；作为验证器，其自我验证能力优于自我一致性。", "summary": "本文提出了PAG（Policy as Generative Verifier）框架，旨在解决大型语言模型（LLMs）在自我验证输出方面的挑战。PAG通过在多轮强化学习中让LLM扮演策略和生成式验证器两种角色，实现自我纠正。其核心创新在于引入了选择性修正机制，即仅在LLM自身检测到错误时才进行修正，这不同于以往的无条件修正方法。实验证明，PAG不仅提升了LLM的推理和自我纠正准确性，其自我验证能力也优于传统的自我一致性方法，有效缓解了模型崩溃问题。", "keywords": "大型语言模型, 自我纠正, 强化学习, 生成式验证, 选择性修正", "comments": "PAG的创新之处在于其将策略和生成式验证器角色统一到多轮强化学习框架中，并引入了选择性修正机制。这种“验证-然后-修正”的流程不仅提高了效率，避免了不必要的重复生成，还解决了模型崩溃的问题，并同时提升了推理和验证能力，具有重要的实践意义和理论价值。"}}
{"id": "2506.10337", "title": "GeoCAD: Local Geometry-Controllable CAD Generation", "authors": ["Zhanwei Zhang", "Kaiyuan Liu", "Junjie Liu", "Wenxiao Wang", "Binbin Lin", "Liang Xie", "Chen Shen", "Deng Cai"], "summary": "Local geometry-controllable computer-aided design (CAD) generation aims to\nmodify local parts of CAD models automatically, enhancing design efficiency. It\nalso ensures that the shapes of newly generated local parts follow\nuser-specific geometric instructions (e.g., an isosceles right triangle or a\nrectangle with one corner cut off). However, existing methods encounter\nchallenges in achieving this goal. Specifically, they either lack the ability\nto follow textual instructions or are unable to focus on the local parts. To\naddress this limitation, we introduce GeoCAD, a user-friendly and local\ngeometry-controllable CAD generation method. Specifically, we first propose a\ncomplementary captioning strategy to generate geometric instructions for local\nparts. This strategy involves vertex-based and VLLM-based captioning for\nsystematically annotating simple and complex parts, respectively. In this way,\nwe caption $\\sim$221k different local parts in total. In the training stage,\ngiven a CAD model, we randomly mask a local part. Then, using its geometric\ninstruction and the remaining parts as input, we prompt large language models\n(LLMs) to predict the masked part. During inference, users can specify any\nlocal part for modification while adhering to a variety of predefined geometric\ninstructions. Extensive experiments demonstrate the effectiveness of GeoCAD in\ngeneration quality, validity and text-to-CAD consistency. Code will be\navailable at https://github.com/Zhanwei-Z/GeoCAD.", "comment": "18 pages, 12 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10337v1", "AI": {"title_translation": "GeoCAD：局部几何可控的CAD生成", "tldr": "GeoCAD是一种新的CAD生成方法，它允许用户通过文本指令精确控制CAD模型的局部几何形状，解决了现有方法无法遵循文本指令或关注局部区域的问题，并通过LLM实现了高效的局部部件生成。", "motivation": "现有的局部几何可控CAD生成方法在遵循文本指令或聚焦于CAD模型的局部区域方面存在不足，导致设计效率受限。", "method": "GeoCAD首先提出了一种互补的标注策略，结合基于顶点的标注和基于VLLM的标注，为约22.1万个不同的局部部件生成几何指令。在训练阶段，给定一个CAD模型，随机遮盖一个局部部件，然后利用其几何指令和剩余部件作为输入，提示大型语言模型（LLMs）预测被遮盖的部件。在推理阶段，用户可以指定任何局部部件进行修改，并遵循预定义的几何指令。", "result": "大量实验证明了GeoCAD在生成质量、有效性和文本到CAD一致性方面的有效性。", "conclusion": "GeoCAD通过其创新的标注策略和LLM驱动的生成方法，成功实现了局部几何可控的CAD生成，显著提升了设计效率，并解决了现有方法面临的挑战。", "translation": "局部几何可控的计算机辅助设计（CAD）生成旨在自动修改CAD模型的局部部分，从而提高设计效率。它还确保新生成的局部部分的形状遵循用户特定的几何指令（例如，等腰直角三角形或一个角被切掉的矩形）。然而，现有方法在实现这一目标时遇到了挑战。具体来说，它们要么缺乏遵循文本指令的能力，要么无法专注于局部部分。为了解决这一限制，我们引入了GeoCAD，一种用户友好且局部几何可控的CAD生成方法。具体而言，我们首先提出了一种互补的标注策略，用于生成局部部件的几何指令。该策略包括基于顶点的标注和基于VLLM的标注，分别用于系统地标注简单和复杂的部件。通过这种方式，我们总共标注了约22.1万个不同的局部部件。在训练阶段，给定一个CAD模型，我们随机遮盖一个局部部件。然后，使用其几何指令和剩余部件作为输入，我们提示大型语言模型（LLMs）预测被遮盖的部分。在推理过程中，用户可以指定任何局部部件进行修改，同时遵循各种预定义的几何指令。大量的实验证明了GeoCAD在生成质量、有效性和文本到CAD一致性方面的有效性。代码将在https://github.com/Zhanwei-Z/GeoCAD上提供。", "summary": "GeoCAD是一种新型的局部几何可控CAD生成方法，旨在提高设计效率并允许用户通过文本指令精确控制CAD模型的局部修改。该方法通过引入互补的标注策略（结合顶点和VLLM标注）为CAD局部部件生成几何指令，并利用大型语言模型（LLMs）根据这些指令和上下文生成或修改局部部件。实验结果表明，GeoCAD在生成质量、有效性和文本到CAD一致性方面表现出色，有效解决了现有方法无法遵循文本指令或专注于局部区域的局限性。", "keywords": "CAD生成, 局部几何控制, 大型语言模型, 几何指令, 文本到CAD", "comments": "GeoCAD的创新之处在于其提出的互补标注策略，将顶点级标注与VLLM（视觉-语言大型模型）相结合，系统地为CAD局部部件生成几何指令，从而能够处理简单和复杂的几何形状。此外，利用LLM进行局部部件的预测和生成，使得文本指令能够直接转化为CAD模型的几何修改，显著提升了用户对设计过程的控制力。该方法在解决现有CAD生成工具在局部控制和文本指令遵循方面的不足具有重要意义。"}}
{"id": "2506.10638", "title": "CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment", "authors": ["Stefano Longari", "Alessandro Pozone", "Jessica Leoni", "Mario Polino", "Michele Carminati", "Mara Tanelli", "Stefano Zanero"], "summary": "In the last decades, Cyber-physical Systems (CPSs) have experienced a\nsignificant technological evolution and increased connectivity, at the cost of\ngreater exposure to cyber-attacks. Since many CPS are used in safety-critical\nsystems, such attacks entail high risks and potential safety harms. Although\nseveral defense strategies have been proposed, they rarely exploit the\ncyber-physical nature of the system. In this work, we exploit the nature of CPS\nby proposing CyFence, a novel architecture that improves the resilience of\nclosed-loop control systems against cyber-attacks by adding a semantic check,\nused to confirm that the system is behaving as expected. To ensure the security\nof the semantic check code, we use the Trusted Execution Environment\nimplemented by modern processors. We evaluate CyFence considering a real-world\napplication, consisting of an active braking digital controller, demonstrating\nthat it can mitigate different types of attacks with a negligible computation\noverhead.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10638v1", "AI": {"title_translation": "CyFence：通过可信执行环境保护信息物理控制器", "tldr": "CyFence是一种利用可信执行环境为信息物理系统（CPS）控制器提供语义检查以抵御网络攻击的新型架构，其在实际应用中表现出低开销且能有效缓解攻击。", "motivation": "信息物理系统（CPS）的互联性增加导致其更容易受到网络攻击，尤其是在安全关键系统中，此类攻击可能带来高风险和潜在的安全危害。现有防御策略很少利用系统的信息物理特性。", "method": "提出了一种名为CyFence的新型架构，通过添加语义检查来提高闭环控制系统抵御网络攻击的弹性，该语义检查用于确认系统行为是否符合预期。为确保语义检查代码的安全性，使用了现代处理器实现的可信执行环境（TEE）。", "result": "通过一个由主动制动数字控制器组成的真实世界应用进行评估，结果表明CyFence能够以可忽略的计算开销缓解不同类型的攻击。", "conclusion": "CyFence通过利用可信执行环境中的语义检查，成功提高了信息物理系统控制器抵御网络攻击的弹性，且计算开销极低。", "translation": "在过去的几十年里，信息物理系统（CPS）经历了显著的技术演进和连接性增加，但代价是更容易受到网络攻击。由于许多CPS用于安全关键系统，此类攻击会带来高风险和潜在的安全危害。尽管已经提出了几种防御策略，但它们很少利用系统的信息物理特性。在这项工作中，我们利用CPS的特性，提出了CyFence，这是一种新颖的架构，通过添加语义检查来提高闭环控制系统抵御网络攻击的弹性，该检查用于确认系统行为是否符合预期。为了确保语义检查代码的安全性，我们使用了现代处理器实现的可信执行环境。我们通过一个由主动制动数字控制器组成的真实世界应用来评估CyFence，结果表明它能够以可忽略的计算开销缓解不同类型的攻击。", "summary": "本文提出了一种名为CyFence的新型架构，旨在通过利用可信执行环境（TEE）实现语义检查，从而增强信息物理系统（CPS）闭环控制器抵御网络攻击的能力。CyFence通过验证系统行为是否符合预期来提高弹性，并利用TEE确保检查代码的安全性。在主动制动数字控制器的实际应用中进行的评估表明，CyFence能够有效缓解多种攻击，且计算开销可忽略不计。", "keywords": "信息物理系统, 可信执行环境, 网络安全, 语义检查, 控制器安全", "comments": "该论文的创新点在于其利用了信息物理系统的固有特性，通过引入语义检查来提升系统对网络攻击的弹性，并且巧妙地结合了可信执行环境（TEE）来保障检查代码的完整性和安全性。这种方法对于保护安全关键的CPS系统具有重要意义，尤其是在当前网络攻击日益增多的背景下。其在实际应用中展示的低计算开销特性也增加了其实用性。"}}
{"id": "2506.10954", "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks", "authors": ["Lianghong Guo", "Yanlin Wang", "Caihua Li", "Pengyu Yang", "Jiachi Chen", "Wei Tao", "Yingtian Zou", "Duyu Tang", "Zibin Zheng"], "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10954v1", "AI": {"title_translation": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂", "tldr": "SWE-Factory是一个自动化管道，用于高效、准确地构建大规模GitHub问题解决数据集，解决了传统方法中环境设置、结果评分和任务验证的挑战。", "motivation": "构建用于训练和评估大型语言模型（LLMs）软件工程能力的大规模GitHub问题解决数据集至关重要，但传统的数据集创建过程（特别是评估环境设置、测试结果评分和任务实例验证阶段）具有挑战性且劳动密集。", "method": "本文提出了SWE-Factory，一个自动化管道，集成了三个核心自动化组件：1. SWE-Builder：一个多代理系统，自动化评估环境构建，通过四个专业代理的协作迭代循环和环境内存池提高效率。2. 标准化的、基于退出代码的评分方法，无需手动编写自定义解析器。3. 自动化fail2pass验证过程，利用可靠的退出代码信号。", "result": "实验表明，该管道可以有效构建有效的任务实例，例如：使用GPT-4.1-mini，SWE-Builder能以每个实例0.045美元的成本构建269个有效实例；使用Gemini-2.5-flash，能以每个实例0.024美元的最低成本实现类似性能。基于退出代码的评分与手动检查相比达到100%的准确率，自动化fail2pass验证的精确度达到0.92，召回率达到1.00。", "conclusion": "SWE-Factory自动化管道有望加速大规模、高质量GitHub问题解决数据集的收集，以用于训练和评估。", "translation": "构建用于GitHub问题解决任务的大规模数据集对于训练和评估大型语言模型（LLMs）的软件工程能力至关重要。然而，创建此类基准的传统过程以其挑战性和劳动密集性而闻名，特别是在设置评估环境、对测试结果进行评分和验证任务实例的阶段。在本文中，我们提出了SWE-Factory，一个旨在解决这些挑战的自动化管道。为了解决这些问题，我们的管道集成了三个核心自动化组件。首先，我们引入了SWE-Builder，一个多代理系统，它自动化评估环境的构建，该系统采用四个专门的代理，以协作、迭代的方式工作，并利用环境内存池来提高效率。其次，我们引入了一种标准化的、基于退出代码的评分方法，该方法消除了手动编写自定义解析器的需要。最后，我们使用这些可靠的退出代码信号自动化了fail2pass验证过程。对四种编程语言的671个问题的实验表明，我们的管道可以有效地构建有效的任务实例；例如，使用GPT-4.1-mini，我们的SWE-Builder以每个实例0.045美元的成本构建了269个有效实例，而使用Gemini-2.5-flash，它以每个实例0.024美元的最低成本实现了可比较的性能。我们还证明了我们基于退出代码的评分与手动检查相比达到了100%的准确率，并且我们的自动化fail2pass验证达到了0.92的精确度和1.00的召回率。我们希望我们的自动化管道将加速大规模、高质量GitHub问题解决数据集的收集，以用于训练和评估。我们的代码和数据集已发布在https://github.com/DeepSoftwareAnalytics/swe-factory。", "summary": "SWE-Factory是一个创新的自动化管道，旨在解决构建大规模GitHub问题解决训练和评估数据集的挑战。它通过整合SWE-Builder（一个自动化环境构建的多代理系统）、基于退出代码的标准化评分方法以及自动化的fail2pass验证过程，显著提高了数据集构建的效率和准确性。实验证明了其在生成有效任务实例方面的能力，并在成本效益和准确性方面表现出色，有望加速高质量软件工程数据集的收集。", "keywords": "自动化管道, GitHub问题解决, 数据集构建, 大型语言模型, 软件工程", "comments": "该论文提出了一种高度创新且实用的解决方案，通过自动化关键环节，极大地降低了构建高质量GitHub问题解决数据集的成本和复杂性。SWE-Builder的多代理系统设计，结合基于退出代码的评分和自动化验证，是其核心创新点。这对于加速大型语言模型在软件工程领域的训练和评估具有重要意义，解决了当前数据稀缺和标注成本高昂的痛点。其开源代码和数据集的发布也体现了研究的开放性和可复现性。"}}
{"id": "2506.10966", "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10966v1", "AI": {"title_translation": "GENMANIP：LLM驱动的通用指令遵循操纵模拟", "tldr": "该论文介绍了GenManip，一个由LLM驱动的模拟平台和基准（GenManip-Bench），用于研究机器人操纵策略的泛化能力，发现带有基础模型的模块化系统比扩展的端到端方法具有更好的泛化能力。", "motivation": "现有模拟平台缺乏足够的支持来探索机器人策略如何适应不同的指令和场景，尤其是在LLM等指令遵循基础模型日益增长的背景下。机器人操纵在现实世界中的鲁棒泛化能力仍然是一个挑战，且LLM在机器人适应性方面的潜力尚未得到充分探索。", "method": "1. GenManip平台：一个为策略泛化研究量身定制的真实桌面模拟平台。2. 自动化管道：通过LLM驱动的任务导向场景图，利用10K带注释的3D对象资产合成大规模、多样化的任务。3. GenManip-Bench：一个通过人工循环校正精炼的200个场景基准，用于系统评估泛化能力。4. 评估：评估了两种策略类型：（1）集成基础模型用于感知、推理和规划的模块化操纵系统；（2）通过可扩展数据收集训练的端到端策略。", "result": "数据扩展有利于端到端方法，但增强了基础模型的模块化系统在不同场景中泛化能力更强。", "conclusion": "该平台有望为在现实条件下推进策略泛化提供关键见解。", "translation": "机器人操纵在现实世界中仍然具有挑战性，尤其是在鲁棒泛化方面。现有模拟平台缺乏足够的支持来探索策略如何适应不同的指令和场景。因此，它们落后于人们对LLM等指令遵循基础模型日益增长的兴趣，这些模型的适应性至关重要，但在公平比较中仍未得到充分探索。为了弥补这一差距，我们引入了GenManip，一个为策略泛化研究量身定制的真实桌面模拟平台。它通过LLM驱动的任务导向场景图的自动化管道，利用10K带注释的3D对象资产合成大规模、多样化的任务。为了系统地评估泛化能力，我们提出了GenManip-Bench，这是一个通过人工循环校正精炼的200个场景的基准。我们评估了两种策略类型：（1）集成基础模型用于感知、推理和规划的模块化操纵系统，以及（2）通过可扩展数据收集训练的端到端策略。结果表明，虽然数据扩展有利于端到端方法，但增强了基础模型的模块化系统在不同场景中泛化能力更强。我们预计该平台将为在现实条件下推进策略泛化提供关键见解。项目页面：https://genmanip.axi404.top/。", "summary": "本文介绍了GenManip，一个由LLM驱动的真实桌面模拟平台及其配套基准GenManip-Bench，旨在解决机器人操纵中鲁棒泛化的挑战。GenManip利用自动化管道和10K带注释的3D资产来合成多样化的任务。GenManip-Bench包含200个人工精炼的场景，用于系统评估。该研究评估了集成基础模型的模块化操纵系统和端到端策略，结果表明，尽管数据扩展有助于端到端方法，但带有基础模型的模块化系统在不同场景中表现出更优越的泛化能力。该平台旨在为现实条件下的策略泛化提供更深入的见解。", "keywords": "机器人操纵, LLM, 模拟, 泛化, 基础模型", "comments": "本文通过整合LLM生成多样化任务，弥补了机器人模拟中的一个关键空白，具有创新性。GenManip和GenManip-Bench的引入为机器人操纵中的泛化能力系统评估提供了急需的工具，尤其是在基础模型的背景下。研究发现带有基础模型的模块化系统比扩展的端到端方法具有更好的泛化能力，这对未来的研究方向具有重要意义。"}}
{"id": "2506.10434", "title": "System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters", "authors": ["Nart Gashi", "Panagiotis Kakosimos", "George Papafotiou"], "summary": "Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for\ninterpretable and efficient system identification in dynamic systems. By\nleveraging the Kolmogorov-Arnold representation theorem, KANs enable function\napproximation through learnable activation functions, offering improved\nscalability, accuracy, and interpretability compared to traditional neural\nnetworks. This paper investigates the application of KANs to model and analyze\nthe dynamics of a buck converter system, focusing on state-space parameter\nestimation along with discovering the system equations. Using simulation data,\nthe methodology involves approximating state derivatives with KANs,\nconstructing interpretable state-space representations, and validating these\nmodels through numerical experiments. The results demonstrate the ability of\nKANs to accurately identify system dynamics, verify model consistency, and\ndetect parameter changes, providing valuable insights into their applicability\nfor system identification in modern industrial systems.", "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10434v1", "AI": {"title_translation": "使用Kolmogorov-Arnold网络进行系统辨识：降压转换器案例研究", "tldr": "本研究探讨了使用Kolmogorov-Arnold网络（KANs）对降压转换器进行系统辨识，证明了KANs在准确识别系统动态、验证模型一致性及检测参数变化方面的有效性，并强调了其在工业系统中的应用潜力。", "motivation": "Kolmogorov-Arnold网络（KANs）作为一种强大的框架，在新兴的动态系统可解释和高效系统辨识领域中崭露头角。与传统神经网络相比，KANs通过可学习的激活函数实现函数逼近，提供了更好的可扩展性、准确性和可解释性。本研究旨在探讨KANs在降压转换器系统动态建模和分析中的应用。", "method": "本研究将Kolmogorov-Arnold网络应用于降压转换器系统，重点关注状态空间参数估计和系统方程的发现。方法包括使用仿真数据，通过KANs逼近状态导数，构建可解释的状态空间表示，并通过数值实验验证这些模型。", "result": "研究结果表明，KANs能够准确识别系统动态，验证模型一致性，并检测参数变化。这些结果为KANs在现代工业系统中的系统辨识应用提供了宝贵的见解。", "conclusion": "本研究的结论是，Kolmogorov-Arnold网络在系统辨识中表现出强大的能力，特别是在准确识别系统动态、验证模型一致性和检测参数变化方面，证明了其在现代工业系统中的适用性。", "translation": "Kolmogorov-Arnold网络（KANs）正在成为动态系统中可解释和高效系统辨识的强大框架。通过利用Kolmogorov-Arnold表示定理，KANs通过可学习的激活函数实现函数逼近，与传统神经网络相比，提供了改进的可扩展性、准确性和可解释性。本文研究了KANs在对降压转换器系统动态进行建模和分析中的应用，重点关注状态空间参数估计以及系统方程的发现。使用仿真数据，该方法包括利用KANs逼近状态导数，构建可解释的状态空间表示，并通过数值实验验证这些模型。结果证明了KANs准确识别系统动态、验证模型一致性和检测参数变化的能力，为其在现代工业系统中的系统辨识应用提供了宝贵的见解。", "summary": "本论文探讨了Kolmogorov-Arnold网络（KANs）在降压转换器系统辨识中的应用。KANs利用其独特的可学习激活函数，在可解释性、效率和准确性方面优于传统神经网络。研究方法涉及使用KANs对状态导数进行逼近，构建可解释的状态空间模型，并利用仿真数据进行验证。实验结果表明，KANs能够有效识别系统动态，验证模型一致性，并检测参数变化，这突显了其在现代工业系统辨识中的巨大潜力。", "keywords": "Kolmogorov-Arnold网络, 系统辨识, 降压转换器, 可解释性, 状态空间模型", "comments": "该论文的创新点在于将新兴的Kolmogorov-Arnold网络应用于系统辨识领域，特别强调了其在提升模型可解释性、效率和准确性方面的优势，这些是传统神经网络的痛点。选择降压转换器作为案例研究，具体展示了KANs在实际工程问题中的应用潜力。这项工作为将KANs推广到更广泛的工业控制和监控系统辨识任务奠定了基础，具有重要的实践意义。"}}
{"id": "2506.10184", "title": "Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition", "authors": ["Mohammad Subhi Al-Batah", "Mowafaq Salem Alzboon", "Muhyeeddin Alqaraleh"], "summary": "This study conducts an empirical examination of MLP networks investigated\nthrough a rigorous methodical experimentation process involving three diverse\ndatasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes\nthree key methods: a) a baseline training using the default settings for the\nMulti-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)\nbased refinement c) Principal Component Analysis (PCA) based dimension\nreduction. The results show important information on how such techniques affect\nperformance. While PCA had showed benefits in low-dimensional and noise-free\ndatasets GA consistently increased accuracy in complex datasets by accurately\nidentifying critical features. Comparison reveals that feature selection and\ndimensionality reduction play interdependent roles in enhancing MLP\nperformance. The study contributes to the literature on feature engineering and\nneural network parameter optimization, offering practical guidelines for a wide\nrange of machine learning tasks", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10184v1", "AI": {"title_translation": "使用多层感知器网络优化遗传算法以增强微小人脸识别", "tldr": "本研究通过结合遗传算法和多层感知器网络，并在不同数据集上进行评估，旨在优化特征工程和神经网络参数，以提高模型性能，特别是在复杂数据集上遗传算法表现优异。", "motivation": "本研究旨在通过实证检验多层感知器（MLP）网络，探讨特征选择和降维技术（如遗传算法和主成分分析）如何影响模型性能，以优化特征工程和神经网络参数。", "method": "研究采用了三种方法：a) 使用默认设置训练多层感知器（MLP）作为基线；b) 使用遗传算法（GA）进行特征选择；c) 使用主成分分析（PCA）进行降维。实验在TinyFace、Heart Disease和Iris三个数据集上进行。", "result": "结果表明，这些技术对性能有重要影响。PCA在低维和无噪声数据集上表现出优势，而遗传算法（GA）通过准确识别关键特征，在复杂数据集上持续提高准确性。特征选择和降维在提升MLP性能方面发挥着相互依赖的作用。", "conclusion": "特征选择和降维在提升MLP性能方面发挥着相互依赖的作用。该研究为特征工程和神经网络参数优化提供了实践指导，适用于广泛的机器学习任务。", "translation": "本研究通过涉及TinyFace、心脏病和Iris三个不同数据集的严格方法学实验过程，对MLP网络进行了实证检验。研究概述：本研究包括三个关键方法：a) 使用多层感知器（MLP）的默认设置进行基线训练，b) 使用基于遗传算法（GA）的特征选择进行优化，c) 基于主成分分析（PCA）的降维。结果显示了这些技术如何影响性能的重要信息。虽然PCA在低维和无噪声数据集中显示出优势，但GA通过准确识别关键特征，在复杂数据集中持续提高了准确性。比较揭示了特征选择和降维在增强MLP性能中发挥着相互依赖的作用。该研究为特征工程和神经网络参数优化文献做出了贡献，为广泛的机器学习任务提供了实用指导。", "summary": "本研究通过在TinyFace、心脏病和Iris数据集上对多层感知器（MLP）网络进行实证检验，探讨了遗传算法（GA）进行特征选择和主成分分析（PCA）进行降维对模型性能的影响。结果表明，GA在复杂数据集中通过精准识别关键特征显著提升了准确性，而PCA在低维无噪声数据集中有效。研究强调了特征选择和降维在提升MLP性能中的相互依赖性，并为特征工程和神经网络参数优化提供了实践指导。", "keywords": "遗传算法, 多层感知器, 特征选择, 降维, 机器学习", "comments": "该研究通过比较遗传算法和主成分分析在提升MLP性能中的作用，深入探讨了特征工程的重要性。其创新点在于强调了GA在复杂数据集上进行特征选择的有效性，为实际应用提供了有价值的指导。然而，抽象中未明确指出具体优化了遗传算法的哪些方面，这可能是一个限制。"}}
{"id": "2506.10415", "title": "Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?", "authors": ["Yingjin Song", "Yupei Du", "Denis Paperno", "Albert Gatt"], "summary": "This paper introduces the TempVS benchmark, which focuses on temporal\ngrounding and reasoning capabilities of Multimodal Large Language Models\n(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event\nrelation inference, sentence ordering and image ordering), each accompanied\nwith a basic grounding test. TempVS requires MLLMs to rely on both visual and\nlinguistic modalities to understand the temporal order of events. We evaluate\n38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,\nwith a substantial performance gap compared to human capabilities. We also\nprovide fine-grained insights that suggest promising directions for future\nresearch. Our TempVS benchmark data and code are available at\nhttps://github.com/yjsong22/TempVS.", "comment": "27 pages, 14 figures. Accepted to ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10415v1", "AI": {"title_translation": "阅后即焚：多模态大型语言模型真的能捕捉图像序列中的事件顺序吗？", "tldr": "新基准TempVS发现，多模态大型语言模型在理解图像序列中的事件时间顺序方面表现不佳，与人类能力存在显著差距。", "motivation": "探索并评估多模态大型语言模型（MLLMs）在图像序列中时间定位和推理能力方面的不足。", "method": "本文引入了TempVS基准，该基准包含事件关系推理、句子排序和图像排序三项主要测试，并评估了38个最先进的多模态大型语言模型在这些任务上的表现。", "result": "评估结果显示，多模态大型语言模型在解决TempVS基准任务时表现不佳，与人类能力相比存在显著的性能差距。", "conclusion": "多模态大型语言模型在理解图像序列中的事件时间顺序和进行时间推理方面存在明显不足，需要未来的研究来改进。", "translation": "本文介绍了TempVS基准，该基准专注于评估多模态大型语言模型（MLLMs）在图像序列中的时间定位和推理能力。TempVS包含三个主要测试（即事件关系推理、句子排序和图像排序），每个测试都伴随着一个基本的定位测试。TempVS要求MLLMs依靠视觉和语言两种模态来理解事件的时间顺序。我们评估了38个最先进的MLLMs，结果表明模型在解决TempVS时表现不佳，与人类能力相比存在显著的性能差距。我们还提供了细致的见解，为未来的研究指明了有希望的方向。我们的TempVS基准数据和代码可在https://github.com/yjsong22/TempVS获取。", "summary": "本文提出了TempVS基准，旨在评估多模态大型语言模型（MLLMs）在图像序列中理解事件时间顺序的能力。该基准包含事件关系推理、句子排序和图像排序等任务，要求MLLMs结合视觉和语言信息进行推理。对38个主流MLLMs的评估发现，它们在这些任务上表现远低于人类水平，揭示了当前MLLMs在时间顺序理解方面存在的显著不足，并为未来研究提供了方向。", "keywords": "多模态大型语言模型, 时间推理, 图像序列, 基准测试, TempVS", "comments": "这篇论文通过引入专门的TempVS基准，系统性地揭示了当前多模态大型语言模型在处理图像序列时间顺序方面的局限性。其创新之处在于设计了多维度的时间推理任务，并量化了模型与人类表现的差距，为未来多模态AI研究指明了关键的改进方向，具有重要的诊断和指导意义。"}}
{"id": "2506.10342", "title": "UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models", "authors": ["Jun Yin", "Jing Zhong", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Ran Luo", "Shuai Lu"], "summary": "Urban cultures and architectural styles vary significantly across cities due\nto geographical, chronological, historical, and socio-political factors.\nUnderstanding these differences is essential for anticipating how cities may\nevolve in the future. As representative cases of historical continuity and\nmodern innovation in China, Beijing and Shenzhen offer valuable perspectives\nfor exploring the transformation of urban streetscapes. However, conventional\napproaches to urban cultural studies often rely on expert interpretation and\nhistorical documentation, which are difficult to standardize across different\ncontexts. To address this, we propose a multimodal research framework based on\nvision-language models, enabling automated and scalable analysis of urban\nstreetscape style differences. This approach enhances the objectivity and\ndata-driven nature of urban form research. The contributions of this study are\nas follows: First, we construct UrbanDiffBench, a curated dataset of urban\nstreetscapes containing architectural images from different periods and\nregions. Second, we develop UrbanSense, the first vision-language-model-based\nframework for urban streetscape analysis, enabling the quantitative generation\nand comparison of urban style representations. Third, experimental results show\nthat Over 80% of generated descriptions pass the t-test (p less than 0.05).\nHigh Phi scores (0.912 for cities, 0.833 for periods) from subjective\nevaluations confirm the method's ability to capture subtle stylistic\ndifferences. These results highlight the method's potential to quantify and\ninterpret urban style evolution, offering a scientifically grounded lens for\nfuture design.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10342v1", "AI": {"title_translation": "UrbanSense：一个利用视觉大语言模型对城市街景进行定量分析的框架", "tldr": "UrbanSense利用视觉大语言模型框架，通过构建数据集和开发分析工具，实现对城市街景风格的自动化、可扩展的定量分析，并验证了其捕捉细微风格差异的能力。", "motivation": "理解城市文化和建筑风格差异对于预测城市未来演变至关重要。然而，传统的城市文化研究方法依赖专家解读和历史文献，难以标准化。为了解决这一问题，本研究提出了一个多模态研究框架。", "method": "本研究提出了一个基于视觉语言模型的多模态研究框架，用于自动化和可扩展地分析城市街景风格差异。具体贡献包括：1. 构建了UrbanDiffBench数据集，包含不同时期和地区的建筑图像。2. 开发了UrbanSense，第一个基于视觉语言模型的城市街景分析框架，用于定量生成和比较城市风格表示。", "result": "实验结果显示，超过80%的生成描述通过t检验（p < 0.05）。主观评估的高Phi分数（城市为0.912，时期为0.833）证实了该方法捕捉细微风格差异的能力。这些结果表明该方法在量化和解释城市风格演变方面的潜力。", "conclusion": "本研究提出的UrbanSense框架为城市街景的定量分析提供了一个科学基础，能够有效捕捉和解释城市风格的演变，为未来的城市设计提供了数据驱动的视角。", "translation": "城市文化和建筑风格因地理、年代、历史和社会政治因素而在不同城市间差异显著。理解这些差异对于预测城市未来演变至关重要。作为中国历史延续性和现代创新的代表案例，北京和深圳为探索城市街景的转型提供了宝贵的视角。然而，传统的城市文化研究方法往往依赖于专家解读和历史文献，这在不同语境下难以标准化。为解决此问题，我们提出了一个基于视觉语言模型的多模态研究框架，以实现城市街景风格差异的自动化和可扩展分析。这种方法增强了城市形态研究的客观性和数据驱动性。本研究的贡献如下：首先，我们构建了UrbanDiffBench，一个精选的城市街景数据集，包含来自不同时期和地区的建筑图像。其次，我们开发了UrbanSense，第一个基于视觉语言模型的城市街景分析框架，能够定量生成和比较城市风格表示。第三，实验结果表明，超过80%的生成描述通过t检验（p小于0.05）。主观评估的高Phi分数（城市为0.912，时期为0.833）证实了该方法捕捉细微风格差异的能力。这些结果突出了该方法量化和解释城市风格演变潜力，为未来设计提供了科学基础的视角。", "summary": "本研究提出UrbanSense，一个基于视觉大语言模型的多模态框架，旨在解决传统城市文化研究中街景风格分析标准化困难的问题。通过构建UrbanDiffBench数据集和开发UrbanSense框架，实现了对城市街景风格差异的自动化和定量分析。实验结果验证了该方法在捕捉细微风格差异方面的有效性，为理解和预测城市演变提供了客观、数据驱动的新方法。", "keywords": "城市街景, 视觉大语言模型, 定量分析, 城市风格, UrbanSense", "comments": "本论文创新性地将视觉大语言模型应用于城市街景的定量分析，克服了传统方法主观性强、难以标准化的局限。通过构建专门的数据集和开发分析框架，为城市形态研究提供了新的数据驱动工具，在城市规划和设计领域具有重要应用潜力。"}}
{"id": "2506.10645", "title": "From IOCs to Group Profiles: On the Specificity of Threat Group Behaviors in CTI Knowledge Bases", "authors": ["Aakanksha Saha", "Martina Lindorfer", "Juan Caballero"], "summary": "Indicators of Compromise (IOCs) such as IP addresses, file hashes, and domain\nnames are commonly used for threat detection and attribution. However, IOCs\ntend to be short-lived as they are easy to change. As a result, the\ncybersecurity community is shifting focus towards more persistent behavioral\nprofiles, such as the Tactics, Techniques, and Procedures (TTPs) and the\nsoftware used by a threat group. However, the distinctiveness and completeness\nof such behavioral profiles remain largely unexplored. In this work, we\nsystematically analyze threat group profiles built from two open cyber threat\nintelligence (CTI) knowledge bases: MITRE ATT&CK and Malpedia. We first\ninvestigate what fraction of threat groups have group-specific behaviors, i.e.,\nbehaviors used exclusively by a single group. We find that only 34% of threat\ngroups in ATT&CK have group-specific techniques. The software used by a threat\ngroup proves to be more distinctive, with 73% of ATT&CK groups using\ngroup-specific software. However, this percentage drops to 24% in the broader\nMalpedia dataset. Next, we evaluate how group profiles improve when data from\nboth sources are combined. While coverage improves modestly, the proportion of\ngroups with group-specific behaviors remains under 30%. We then enhance\nprofiles by adding exploited vulnerabilities and additional techniques\nextracted from more threat reports. Despite the additional information, 64% of\ngroups still lack any group-specific behavior. Our findings raise concerns on\nthe belief that behavioral profiles can replace IOCs in threat group\nattribution.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10645v1", "AI": {"title_translation": "从IOCs到群组画像：关于CTI知识库中威胁群组行为特异性的研究", "tldr": "研究发现，尽管网络安全界正转向行为画像，但威胁群组的行为画像（如TTPs和使用的软件）的特异性远低于预期，这引发了对它们替代IOCs进行归因能力的担忧。", "motivation": "传统上用于威胁检测和归因的妥协指标（IOCs）生命周期短且易于更改。网络安全界正将重点转向更持久的行为画像（如TTPs和软件），但这些行为画像的独特性和完整性尚未得到充分探索。", "method": "本研究系统分析了从MITRE ATT&CK和Malpedia两个开放网络威胁情报（CTI）知识库构建的威胁群组画像。研究首先调查了有多少威胁群组具有群组特异性行为（即仅由单个群组使用的行为），然后评估了结合两个来源数据后群组画像的改进情况，并进一步通过添加利用的漏洞和从更多威胁报告中提取的额外技术来增强画像。", "result": "研究发现，ATT&CK中只有34%的威胁群组具有群组特异性技术。威胁群组使用的软件更具独特性，ATT&CK中73%的群组使用群组特异性软件，但在更广泛的Malpedia数据集中，这一比例降至24%。即使结合了两个来源的数据，具有群组特异性行为的群组比例仍低于30%。在添加了额外信息后，仍有64%的群组缺乏任何群组特异性行为。", "conclusion": "研究结果对“行为画像可以取代IOCs进行威胁群组归因”的观点提出了担忧。", "translation": "妥协指标（IOCs）如IP地址、文件哈希和域名常用于威胁检测和归因。然而，IOCs往往是短命的，因为它们易于更改。因此，网络安全社区正将重点转向更持久的行为画像，如威胁群组的战术、技术和程序（TTPs）以及其使用的软件。然而，这些行为画像的独特性和完整性在很大程度上仍未被探索。在这项工作中，我们系统地分析了从两个开放网络威胁情报（CTI）知识库构建的威胁群组画像：MITRE ATT&CK和Malpedia。我们首先调查了有多少威胁群组具有群组特异性行为，即仅由单个群组使用的行为。我们发现，ATT&CK中只有34%的威胁群组具有群组特异性技术。威胁群组使用的软件被证明更具独特性，ATT&CK中73%的群组使用群组特异性软件。然而，在更广泛的Malpedia数据集中，这一比例降至24%。接下来，我们评估了当结合两个来源的数据时，群组画像如何得到改善。虽然覆盖率略有提高，但具有群组特异性行为的群组比例仍低于30%。然后，我们通过添加被利用的漏洞和从更多威胁报告中提取的额外技术来增强画像。尽管增加了额外信息，仍有64%的群组缺乏任何群组特异性行为。我们的发现对“行为画像可以取代IOCs进行威胁群组归因”的信念提出了担忧。", "summary": "该研究系统分析了来自MITRE ATT&CK和Malpedia的威胁群组行为画像，旨在评估其特异性和完整性。研究发现，相较于易变的IOCs，TTPs和软件等行为画像的群组特异性远低于预期。具体而言，ATT&CK中仅34%的群组具有特异性技术，软件特异性虽高（ATT&CK中73%），但在Malpedia中降至24%。即使结合多源数据并补充信息，仍有高达64%的群组缺乏任何特异性行为。这些发现对行为画像在威胁群组归因中替代IOCs的有效性提出了质疑。", "keywords": "威胁情报, 行为画像, IOCs, 群组归因, MITRE ATT&CK", "comments": "这篇论文揭示了当前威胁情报知识库中威胁群组行为画像的局限性，特别是其在区分不同群组方面的不足。研究结果挑战了业界对行为画像能够完全替代IOCs进行威胁归因的普遍看法，强调了在构建和利用行为情报时需要更加谨慎，并可能需要更丰富、更精细的数据来提高归因的准确性。其创新之处在于首次系统性地量化了这种“特异性”问题。"}}
{"id": "2506.10968", "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop", "authors": ["Justin Kerr", "Kush Hari", "Ethan Weber", "Chung Min Kim", "Brent Yi", "Tyler Bonnen", "Ken Goldberg", "Angjoo Kanazawa"], "summary": "Humans do not passively observe the visual world -- we actively look in order\nto act. Motivated by this principle, we introduce EyeRobot, a robotic system\nwith gaze behavior that emerges from the need to complete real-world tasks. We\ndevelop a mechanical eyeball that can freely rotate to observe its surroundings\nand train a gaze policy to control it using reinforcement learning. We\naccomplish this by first collecting teleoperated demonstrations paired with a\n360 camera. This data is imported into a simulation environment that supports\nrendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze\non top of robot demonstrations. We then introduce a BC-RL loop to train the\nhand and eye jointly: the hand (BC) agent is trained from rendered eye\nobservations, and the eye (RL) agent is rewarded when the hand produces correct\naction predictions. In this way, hand-eye coordination emerges as the eye looks\ntowards regions which allow the hand to complete the task. EyeRobot implements\na foveal-inspired policy architecture allowing high resolution with a small\ncompute budget, which we find also leads to the emergence of more stable\nfixation as well as improved ability to track objects and ignore distractors.\nWe evaluate EyeRobot on five panoramic workspace manipulation tasks requiring\nmanipulation in an arc surrounding the robot arm. Our experiments suggest\nEyeRobot exhibits hand-eye coordination behaviors which effectively facilitate\nmanipulation over large workspaces with a single camera. See project site for\nvideos: https://www.eyerobot.net/", "comment": "Project page: https://www.eyerobot.net/", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10968v1", "AI": {"title_translation": "眼，机器人：通过BC-RL感知-动作循环学习观察以行动", "tldr": "本文介绍了EyeRobot，一个通过BC-RL循环学习凝视行为的机器人系统，使其能够有效地进行大范围操作，模拟人类主动观察以行动的方式。", "motivation": "受人类主动观察世界以采取行动的启发，本文旨在开发一个机器人系统，使其凝视行为能够从完成现实世界任务的需求中自然产生。", "method": "研究开发了一个可自由旋转的机械眼球，并使用强化学习训练其凝视策略。首先收集带有360度摄像头的遥操作演示数据，然后将其导入支持任意眼球视点渲染的仿真环境。接着引入一个BC-RL（行为克隆-强化学习）循环来联合训练手和眼：手（BC）代理从渲染的眼部观察中训练，而眼（RL）代理则在手产生正确动作预测时获得奖励。此外，EyeRobot采用了一种受中央凹启发的策略架构，以在小计算预算下实现高分辨率感知。", "result": "EyeRobot实现了手眼协调行为，眼部会看向有助于手完成任务的区域。该系统在小计算预算下实现了高分辨率，并带来了更稳定的注视以及更好的物体追踪和干扰物忽略能力。实验表明，EyeRobot能够有效地促进使用单个摄像头在大工作空间内的操作。", "conclusion": "EyeRobot系统通过模仿人类主动观察以行动的方式，成功地实现了机器人手眼协调，使其能够高效地在广阔的工作空间内完成操作任务，并展现出稳定的凝视和优异的物体追踪能力。", "translation": "人类并非被动地观察视觉世界——我们积极地观察以便行动。受此原理启发，我们引入了EyeRobot，一个具有凝视行为的机器人系统，其行为源于完成现实世界任务的需求。我们开发了一个可以自由旋转以观察周围环境的机械眼球，并使用强化学习训练其凝视策略来控制它。我们首先收集了与360度摄像头配对的遥操作演示数据。这些数据被导入到一个支持渲染任意眼球视点的仿真环境中，从而可以在机器人演示的基础上进行眼球凝视的片段回放。然后，我们引入了一个BC-RL循环来联合训练手和眼：手（BC）代理从渲染的眼部观察中进行训练，而眼（RL）代理在手产生正确动作预测时获得奖励。通过这种方式，手眼协调得以产生，因为眼部会看向允许手完成任务的区域。EyeRobot实现了一种受中央凹启发的策略架构，在小计算预算下实现高分辨率，我们发现这也导致了更稳定的注视以及更好的物体追踪和忽略干扰物的能力。我们在五个需要机器臂周围弧形操作的全景工作空间操作任务上评估了EyeRobot。我们的实验表明，EyeRobot展现了手眼协调行为，有效地促进了使用单个摄像头在大工作空间内的操作。请访问项目网站观看视频：https://www.eyerobot.net/", "summary": "EyeRobot是一个受人类主动观察行为启发的机器人系统，它通过一个可旋转的机械眼球和基于强化学习的凝视策略实现手眼协调。系统采用BC-RL循环联合训练手和眼，其中手的行为克隆代理从眼的观察中学习，而眼的强化学习代理则根据手的正确预测获得奖励。此外，其受中央凹启发的架构在计算效率下提供了高分辨率感知。实验证明，EyeRobot能有效进行大范围操作，展现出稳定的凝视和卓越的物体追踪能力。", "keywords": "手眼协调, 凝视控制, 强化学习, 机器人操作, 中央凹感知", "comments": "本文的创新点在于其提出的BC-RL循环，实现了手眼系统的联合训练，使得机器人能够学习到类似人类的主动凝视行为，以高效完成任务。受中央凹启发的感知架构在保证高分辨率的同时，有效控制了计算成本。这对于提高机器人操作在复杂、大范围环境中的鲁棒性和效率具有重要意义。"}}
{"id": "2506.10897", "title": "GenPlanX. Generation of Plans and Execution", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tomás de la Rosa", "Alfredo Garrachón", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10897v1", "AI": {"title_translation": "GenPlanX：计划生成与执行", "tldr": "GenPlanX集成大语言模型与经典AI规划技术，实现自然语言描述的规划任务，并辅助执行和监控。", "motivation": "经典的AI规划技术无法理解自然语言描述的规划任务，而大语言模型（LLMs）在解释人类意图方面表现出色，因此需要一种能整合LLMs以处理自然语言规划任务的系统。", "method": "本文介绍了GenPlanX，它将大语言模型（LLMs）用于自然语言描述的规划任务，与经典的AI规划引擎以及一个执行和监控框架相结合。", "result": "GenPlanX在协助用户处理办公室相关任务方面展示了其效用，突出了其通过无缝人机协作简化工作流程和提高生产力的潜力。", "conclusion": "GenPlanX通过整合LLMs和经典AI规划，能够简化工作流程并提高生产力，实现更好的人机协作。", "translation": "经典的AI规划技术能够为复杂的任务生成一系列动作。然而，它们缺乏理解自然语言提供的规划任务的能力。大语言模型（LLMs）的出现为人机交互带来了新的能力。在规划任务的背景下，LLMs在解释人类意图方面表现出色。本文介绍了GenPlanX，它将LLMs用于自然语言描述的规划任务，与经典的AI规划引擎以及一个执行和监控框架相结合。我们展示了GenPlanX在协助用户处理办公室相关任务方面的效用，突出了其通过无缝人机协作简化工作流程和提高生产力的潜力。", "summary": "GenPlanX是一个结合大语言模型和经典AI规划引擎的系统，旨在解决传统AI规划在理解自然语言规划任务方面的不足。该系统还包含一个执行和监控框架，并已在办公室任务中验证了其有效性，展示了通过人机协作提高生产力和优化工作流程的潜力。", "keywords": "AI规划, 大语言模型, 自然语言处理, 任务执行, 人机协作", "comments": "GenPlanX的创新之处在于它弥合了经典AI规划的结构化推理能力与大语言模型强大的自然语言理解能力之间的鸿沟。这对于实现更自然、更直观的人机交互式规划系统具有重要意义，尤其是在需要用户以日常语言表达复杂意图的场景中。其在办公室任务中的应用展示了实际价值。"}}
{"id": "2506.10186", "title": "Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment", "authors": ["Yuhui Ding", "Thomas Hofmann"], "summary": "Equivariant diffusion models have achieved impressive performance in 3D\nmolecule generation. These models incorporate Euclidean symmetries of 3D\nmolecules by utilizing an SE(3)-equivariant denoising network. However,\nspecialized equivariant architectures limit the scalability and efficiency of\ndiffusion models. In this paper, we propose an approach that relaxes such\nequivariance constraints. Specifically, our approach learns a sample-dependent\nSO(3) transformation for each molecule to construct an aligned latent space. A\nnon-equivariant diffusion model is then trained over the aligned\nrepresentations. Experimental results demonstrate that our approach performs\nsignificantly better than previously reported non-equivariant models. It yields\nsample quality comparable to state-of-the-art equivariant diffusion models and\noffers improved training and sampling efficiency. Our code is available at\nhttps://github.com/skeletondyh/RADM", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10186v1", "AI": {"title_translation": "通过旋转对齐实现可扩展的非等变三维分子生成", "tldr": "提出一种通过旋转对齐构建对齐潜在空间，然后训练非等变扩散模型生成3D分子的方法，解决了等变模型的扩展性和效率问题，性能与最先进模型相当且更高效。", "motivation": "现有的等变扩散模型在3D分子生成方面表现出色，但其专门的等变架构限制了扩散模型的可扩展性和效率。", "method": "提出一种放松等变约束的方法。具体来说，该方法为每个分子学习一个样本依赖的SO(3)变换，以构建一个对齐的潜在空间。然后，在这个对齐的表示上训练一个非等变扩散模型。", "result": "实验结果表明，该方法显著优于先前报道的非等变模型。它产生了与最先进的等变扩散模型相当的样本质量，并提供了改进的训练和采样效率。", "conclusion": "通过旋转对齐的非等变扩散模型可以有效生成高质量3D分子，同时解决等变模型的扩展性和效率问题。", "translation": "等变扩散模型在三维分子生成方面取得了令人印象深刻的性能。这些模型通过利用SE(3)等变去噪网络来整合三维分子的欧几里得对称性。然而，专门的等变架构限制了扩散模型的可扩展性和效率。在本文中，我们提出了一种放松这种等变约束的方法。具体来说，我们的方法为每个分子学习一个样本依赖的SO(3)变换，以构建一个对齐的潜在空间。然后，在这个对齐的表示上训练一个非等变扩散模型。实验结果表明，我们的方法比先前报道的非等变模型表现显著更好。它产生了与最先进的等变扩散模型相当的样本质量，并提供了改进的训练和采样效率。我们的代码可在https://github.com/skeletondyh/RADM获取。", "summary": "本文提出了一种可扩展的非等变三维分子生成方法，通过为每个分子学习样本依赖的SO(3)变换来构建对齐的潜在空间，并在此空间上训练非等变扩散模型。实验证明，该方法在性能上优于现有非等变模型，并能达到与最先进等变模型相当的样本质量，同时显著提升了训练和采样的效率，解决了等变模型的可扩展性限制。", "keywords": "3D分子生成, 非等变模型, 扩散模型, 旋转对齐, 可扩展性", "comments": "这项工作通过引入旋转对齐来规避SE(3)等变网络的复杂性，从而在保持生成质量的同时显著提高模型的可扩展性和效率，这是一个重要的创新。它展示了在某些应用中，通过巧妙的设计来放松严格的对称性约束，可以带来实际的性能优势。"}}
{"id": "2506.10421", "title": "Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting", "authors": ["Avneet Kaur", "Arnav Arora"], "summary": "Framing used by news media, especially in times of conflict, can have\nsubstantial impact on readers' opinion, potentially aggravating the conflict\nitself. Current studies on the topic of conflict framing have limited insights\ndue to their qualitative nature or only look at surface level generic frames\nwithout going deeper. In this work, we identify indicators of war and peace\njournalism, as outlined by prior work in conflict studies, in a corpus of news\narticles reporting on the Israel-Palestine war. For our analysis, we use\ncomputational approaches, using a combination of frame semantics and large\nlanguage models to identify both communicative framing and its connection to\nlinguistic framing. Our analysis reveals a higher focus on war based reporting\nrather than peace based. We also show substantial differences in reporting\nacross the US, UK, and Middle Eastern news outlets in framing who the assailant\nand victims of the conflict are, surfacing biases within the media.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10421v1", "AI": {"title_translation": "超越战场：冲突报道中媒体报道的框架分析", "tldr": "本研究利用计算方法分析了以色列-巴勒斯坦战争新闻报道中的战争与和平新闻框架，发现媒体更侧重战争报道，且不同地区媒体在受害者和袭击者框架上存在显著偏差。", "motivation": "新闻媒体在冲突报道中使用的框架对读者观点影响巨大，可能加剧冲突。现有冲突框架研究因定性性质或仅停留在表面通用框架而缺乏深入见解，因此本研究旨在弥补这一不足。", "method": "本研究在一个关于以色列-巴勒斯坦战争的新闻文章语料库中，识别了战争与和平新闻的指标。分析方法采用计算方法，结合框架语义学和大型语言模型，以识别交际框架及其与语言框架的联系。", "result": "分析显示，新闻报道更侧重于基于战争的报道而非基于和平的报道。研究还表明，美国、英国和中东新闻媒体在冲突中的袭击者和受害者框架上存在显著差异，揭示了媒体内部的偏见。", "conclusion": "媒体在冲突报道中存在明显的战争导向偏见，且不同地区媒体在塑造冲突叙事方面存在显著差异，这可能影响公众对冲突的认知并加剧冲突。", "translation": "新闻媒体使用的框架，尤其是在冲突时期，可能对读者的观点产生重大影响，并可能加剧冲突本身。当前关于冲突框架主题的研究由于其定性性质或仅关注表面通用框架而缺乏深入见解。在这项工作中，我们在一个报道以色列-巴勒斯坦战争的新闻文章语料库中，识别了冲突研究中先前工作概述的战争与和平新闻指标。在我们的分析中，我们使用计算方法，结合框架语义学和大型语言模型来识别交际框架及其与语言框架的联系。我们的分析揭示了对基于战争的报道的更高关注，而不是基于和平的报道。我们还表明，美国、英国和中东新闻媒体在构建冲突中的袭击者和受害者方面存在实质性差异，揭示了媒体内部的偏见。", "summary": "本研究旨在克服现有冲突框架研究的局限性，通过计算方法分析以色列-巴勒斯坦战争新闻报道中的媒体框架。研究利用框架语义学和大型语言模型，在一个新闻文章语料库中识别战争与和平新闻指标，并探讨交际框架与语言框架的联系。结果表明，媒体更侧重于战争报道，而非和平报道，并且美国、英国和中东新闻媒体在冲突中袭击者和受害者的框架上存在显著差异，揭示了媒体偏见。", "keywords": "冲突报道, 媒体框架, 战争与和平新闻, 计算方法, 偏见", "comments": "该研究的创新之处在于结合了计算方法、框架语义学和大型语言模型来深入分析冲突报道中的媒体框架，超越了以往定性或表面化的研究。其重要性在于揭示了新闻媒体在冲突报道中的潜在偏见和其对公众认知的影响，尤其是在敏感的以色列-巴勒斯坦冲突背景下。研究结果对理解媒体在塑造冲突叙事中的作用具有重要意义。"}}
{"id": "2506.10344", "title": "RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration", "authors": ["Mina C. Moghadam", "Alan Q. Wang", "Omer Taub", "Martin R. Prince", "Mert R. Sabuncu"], "summary": "Many real-world settings require registration of a pair of medical images\nthat differ in spatial resolution, which may arise from differences in image\nacquisition parameters like pixel spacing, slice thickness, and field-of-view.\nHowever, all previous machine learning-based registration techniques resample\nimages onto a fixed resolution. This is suboptimal because resampling can\nintroduce artifacts due to interpolation. To address this, we present\nRealKeyMorph (RKM), a resolution-agnostic method for image registration. RKM is\nan extension of KeyMorph, a registration framework which works by training a\nnetwork to learn corresponding keypoints for a given pair of images, after\nwhich a closed-form keypoint matching step is used to derive the transformation\nthat aligns them. To avoid resampling and enable operating on the raw data, RKM\noutputs keypoints in real-world coordinates of the scanner. To do this, we\nleverage the affine matrix produced by the scanner (e.g., MRI machine) that\nencodes the mapping from voxel coordinates to real world coordinates. By\ntransforming keypoints into real-world space and integrating this into the\ntraining process, RKM effectively enables the extracted keypoints to be\nresolution-agnostic. In our experiments, we demonstrate the advantages of RKM\non the registration task for orthogonal 2D stacks of abdominal MRIs, as well as\n3D volumes with varying resolutions in brain datasets.", "comment": "23 pages, 8 figures, to be submitted to MELBA", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10344v1", "AI": {"title_translation": "RealKeyMorph：用于分辨率无关图像配准的真实世界坐标关键点", "tldr": "RealKeyMorph (RKM) 是一种新的图像配准方法，通过在真实世界坐标中输出关键点来避免图像重采样，从而实现分辨率无关的配准，解决了现有机器学习方法在不同分辨率医学图像配准中引入伪影的问题。", "motivation": "许多实际场景需要对空间分辨率不同的医学图像进行配准，这可能由图像采集参数差异引起。然而，所有先前的基于机器学习的配准技术都会将图像重采样到固定分辨率，这会因插值引入伪影，导致次优结果。", "method": "本文提出了RealKeyMorph (RKM)，它是KeyMorph的扩展。RKM通过训练网络学习图像对的对应关键点，然后使用闭合形式的关键点匹配步骤来导出对齐它们的变换。为了避免重采样并对原始数据进行操作，RKM利用扫描仪（例如MRI机器）生成的仿射矩阵，将关键点输出到扫描仪的真实世界坐标中，从而使提取的关键点与分辨率无关。", "result": "实验证明了RKM在腹部MRI的正交2D堆栈配准任务以及具有不同分辨率的脑数据集3D体配准任务中的优势。", "conclusion": "RealKeyMorph (RKM) 提供了一种分辨率无关的图像配准方法，通过在真实世界坐标中操作关键点来避免重采样引入的伪影，从而解决了医学图像配准中不同分辨率图像的挑战。", "translation": "许多实际场景需要对空间分辨率不同的医学图像进行配准，这可能由像素间距、切片厚度和视野等图像采集参数的差异引起。然而，所有先前的基于机器学习的配准技术都会将图像重采样到固定分辨率。这是次优的，因为重采样会因插值引入伪影。为了解决这个问题，我们提出了RealKeyMorph (RKM)，这是一种用于图像配准的分辨率无关方法。RKM是KeyMorph的扩展，KeyMorph是一种通过训练网络学习给定图像对的对应关键点，然后使用闭合形式的关键点匹配步骤来导出对齐它们的变换的配准框架。为了避免重采样并能够在原始数据上操作，RKM以扫描仪的真实世界坐标输出关键点。为此，我们利用扫描仪（例如MRI机器）生成的仿射矩阵，该矩阵编码了从体素坐标到真实世界坐标的映射。通过将关键点转换为真实世界空间并将此集成到训练过程中，RKM有效地使提取的关键点与分辨率无关。在我们的实验中，我们展示了RKM在腹部MRI的正交2D堆栈配准任务以及具有不同分辨率的脑数据集3D体配准任务中的优势。", "summary": "RealKeyMorph (RKM) 是一种针对不同分辨率医学图像配准的新型机器学习方法。与传统方法通过重采样引入伪影不同，RKM是KeyMorph的扩展，它利用扫描仪的仿射矩阵将关键点直接输出到真实世界坐标中，从而实现分辨率无关的配准并避免重采样。实验证明RKM在腹部MRI和脑部数据集的配准任务中表现出优势。", "keywords": "图像配准, 分辨率无关, 关键点, 真实世界坐标, 医学图像", "comments": "该论文的创新点在于提出了RealKeyMorph (RKM) 方法，其核心是通过在真实世界坐标中处理关键点，从而避免了传统图像配准中常见的重采样步骤。这一创新有效地解决了医学图像在不同分辨率下配准时可能引入的插值伪影问题，显著提高了配准的准确性和鲁棒性。其重要性在于为处理真实世界中分辨率差异的医学图像配准提供了一个更优的解决方案，对于临床应用具有潜在价值。"}}
{"id": "2506.10924", "title": "A space-time interface-fitted method for moving-subdomain distributed control problems with energy regularization", "authors": ["Quang Huy Nguyen", "Phuong Cuc Hoang", "Van Chien Le", "Thi Thanh Mai Ta"], "summary": "This paper investigates a space-time interface-fitted approximation of a\nmoving-interface optimal control problem with energy regularization. We\nreformulate the optimality conditions into a variational problem involving both\nthe state and adjoint. This problem is shown to be equivalent to our optimal\ncontrol problem. Based on fully unstructured, space-time interface-fitted\nmeshes, we propose and analyze a Petrov-Galerkin approximation of the problem.\nAn optimal error estimate with respect to a discrete norm is established under\na specific regularity assumption on the state and adjoint. Several numerical\nresults are presented to corroborate our theoretical results.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10924v1", "AI": {"title_translation": "具有能量正则化的移动子域分布式控制问题的时空界面拟合方法", "tldr": "本文研究了具有能量正则化的移动界面最优控制问题，提出并分析了一种时空界面拟合的Petrov-Galerkin近似方法，建立了最佳误差估计，并用数值结果进行了验证。", "motivation": "本文旨在研究具有能量正则化的移动界面最优控制问题的时空界面拟合近似方法。", "method": "将最优性条件重新表述为一个涉及状态和伴随变量的变分问题，并证明其等同于最优控制问题。基于完全非结构化的时空界面拟合网格，提出并分析了该问题的Petrov-Galerkin近似。在状态和伴随变量的特定正则性假设下，建立了关于离散范数的最佳误差估计。", "result": "建立了关于离散范数的最佳误差估计。提出了几个数值结果来证实理论结果。", "conclusion": "本文提出的时空界面拟合Petrov-Galerkin近似方法对于具有能量正则化的移动界面最优控制问题是有效的，并得到了理论分析（最佳误差估计）和数值结果的支持。", "translation": "本文研究了具有能量正则化的移动界面最优控制问题的时空界面拟合近似。我们将最优性条件重新表述为一个涉及状态和伴随变量的变分问题。该问题被证明等同于我们的最优控制问题。基于完全非结构化的时空界面拟合网格，我们提出并分析了该问题的Petrov-Galerkin近似。在状态和伴随变量的特定正则性假设下，建立了关于离散范数的最佳误差估计。提出了几个数值结果来证实我们的理论结果。", "summary": "本文针对具有能量正则化的移动界面最优控制问题，将其最优性条件转化为一个等价的变分问题。在此基础上，提出并分析了一种基于非结构化时空界面拟合网格的Petrov-Galerkin近似方法，并在特定正则性假设下建立了最佳误差估计。数值结果验证了理论分析的正确性。", "keywords": "最优控制, 移动界面, 时空方法, Petrov-Galerkin, 能量正则化", "comments": "本文为处理涉及移动界面的最优控制问题提供了一种新颖且严谨的数值方法。时空界面拟合网格和Petrov-Galerkin方法的结合，加上详尽的误差分析，显著提升了对此类复杂问题的求解能力，具有重要的理论和应用价值。"}}
{"id": "2506.10665", "title": "GOLIATH: A Decentralized Framework for Data Collection in Intelligent Transportation Systems", "authors": ["Davide Maffiola", "Stefano Longari", "Michele Carminati", "Mara Tanelli", "Stefano Zanero"], "summary": "Intelligent Transportation Systems (ITSs) technology has advanced during the\npast years, and it is now used for several applications that require vehicles\nto exchange real-time data, such as in traffic information management.\nTraditionally, road traffic information has been collected using on-site\nsensors. However, crowd-sourcing traffic information from onboard sensors or\nsmartphones has become a viable alternative. State-of-the-art solutions\ncurrently follow a centralized model where only the service provider has\ncomplete access to the collected traffic data and represent a single point of\nfailure and trust. In this paper, we propose GOLIATH, a blockchain-based\ndecentralized framework that runs on the In-Vehicle Infotainment (IVI) system\nto collect real-time information exchanged between the network's participants.\nOur approach mitigates the limitations of existing crowd-sourcing centralized\nsolutions by guaranteeing trusted information collection and exchange, fully\nexploiting the intrinsic distributed nature of vehicles. We demonstrate its\nfeasibility in the context of vehicle positioning and traffic information\nmanagement. Each vehicle participating in the decentralized network shares its\nposition and neighbors' ones in the form of a transaction recorded on the\nledger, which uses a novel consensus mechanism to validate it. We design the\nconsensus mechanism resilient against a realistic set of adversaries that aim\nto tamper or disable the communication. We evaluate the proposed framework in a\nsimulated (but realistic) environment, which considers different threats and\nallows showing its robustness and safety properties.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10665v1", "AI": {"title_translation": "GOLIATH：智能交通系统中数据收集的去中心化框架", "tldr": "GOLIATH是一个基于区块链的去中心化框架，用于智能交通系统（ITS）中的实时数据收集，旨在克服现有集中式众包解决方案的局限性，确保可信的信息交换。", "motivation": "智能交通系统（ITS）需要车辆实时交换数据，例如交通信息管理。传统的现场传感器方法已面临挑战，而众包成为替代方案。然而，当前的众包解决方案普遍采用集中式模型，导致单点故障和信任问题，且数据访问受限。", "method": "本文提出了GOLIATH，一个基于区块链的去中心化框架，运行在车载信息娱乐（IVI）系统上，用于收集网络参与者之间交换的实时信息。该方法通过在分类账上记录车辆位置和邻居位置的交易，并使用一种新颖的共识机制进行验证。该共识机制被设计为能够抵御旨在篡改或禁用通信的对手。该框架在一个模拟环境中进行了评估。", "result": "GOLIATH在车辆定位和交通信息管理方面展示了其可行性。模拟评估结果表明，该框架具有鲁棒性和安全性。", "conclusion": "GOLIATH是一个基于区块链的去中心化框架，通过利用车辆固有的分布式特性，有效缓解了现有众包集中式解决方案的局限性，保障了可信的信息收集与交换，并在模拟环境中证明了其鲁棒性和安全性。", "translation": "智能交通系统（ITS）技术在过去几年中取得了进步，现在它被用于需要车辆交换实时数据的多种应用，例如交通信息管理。传统上，道路交通信息是通过现场传感器收集的。然而，从车载传感器或智能手机众包交通信息已成为一种可行的替代方案。目前最先进的解决方案遵循集中式模型，其中只有服务提供商才能完全访问收集到的交通数据，这代表了单点故障和信任问题。在本文中，我们提出了GOLIATH，一个基于区块链的去中心化框架，运行在车载信息娱乐（IVI）系统上，用于收集网络参与者之间交换的实时信息。我们的方法通过保证可信的信息收集和交换，充分利用车辆固有的分布式特性，从而减轻了现有众包集中式解决方案的局限性。我们在车辆定位和交通信息管理的背景下展示了其可行性。参与去中心化网络的每辆车都以交易的形式共享其位置和邻居的位置，并记录在分类账上，该分类账使用一种新颖的共识机制来验证它。我们设计的共识机制能够抵抗一组旨在篡改或禁用通信的真实对手。我们在一个模拟（但真实）环境中评估了所提出的框架，该环境考虑了不同的威胁，并展示了其鲁棒性和安全属性。", "summary": "GOLIATH是一个基于区块链的去中心化框架，旨在解决智能交通系统（ITS）中传统集中式数据收集的局限性。它运行在车载信息娱乐（IVI）系统上，使车辆能够安全地交换实时数据。该框架通过记录车辆位置和邻居位置的交易到分类账上，并采用一种新颖且抗攻击的共识机制来确保信息的可信度和完整性。在模拟环境中的评估表明，GOLIATH在数据收集方面具有良好的鲁棒性和安全性。", "keywords": "智能交通系统, 去中心化框架, 区块链, 数据收集, 众包", "comments": "GOLIATH的创新之处在于将区块链技术应用于智能交通系统的数据收集，有效解决了传统集中式方案的单点故障和信任问题。其提出的新型共识机制增强了数据交换的安全性与鲁棒性，对于构建更可靠、去中心化的交通数据生态系统具有重要意义。"}}
{"id": "2506.10912", "title": "Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?", "authors": ["Fei Lin", "Ziyang Gong", "Cong Wang", "Yonglin Tian", "Tengchao Zhang", "Xue Yang", "Gen Luo", "Fei-Yue Wang"], "summary": "Toxicity remains a leading cause of early-stage drug development failure.\nDespite advances in molecular design and property prediction, the task of\nmolecular toxicity repair - generating structurally valid molecular\nalternatives with reduced toxicity - has not yet been systematically defined or\nbenchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task\nfor general-purpose Multimodal Large Language Models (MLLMs) focused on\nmolecular toxicity repair. We construct a standardized dataset covering 11\nprimary tasks and 560 representative toxic molecules spanning diverse\nmechanisms and granularities. We design a prompt annotation pipeline with\nmechanism-aware and task-adaptive capabilities, informed by expert\ntoxicological knowledge. In parallel, we propose an automated evaluation\nframework, ToxiEval, which integrates toxicity endpoint prediction, synthetic\naccessibility, drug-likeness, and structural similarity into a high-throughput\nevaluation chain for repair success. We systematically assess nearly 30\nmainstream general-purpose MLLMs and design multiple ablation studies to\nanalyze key factors such as evaluation criteria, candidate diversity, and\nfailure attribution. Experimental results show that although current MLLMs\nstill face significant challenges on this task, they begin to demonstrate\npromising capabilities in toxicity understanding, semantic constraint\nadherence, and structure-aware molecule editing.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10912v1", "AI": {"title_translation": "打破有害分子：多模态大语言模型（MLLMs）已准备好进行结构级分子解毒了吗？", "tldr": "本文提出了ToxiMol，首个用于评估多模态大语言模型（MLLMs）在分子毒性修复方面的基准任务，并设计了ToxiEval评估框架。研究发现当前MLLMs在此任务上仍面临挑战，但已展现出初步潜力。", "motivation": "毒性是早期药物开发失败的主要原因。尽管分子设计和性质预测取得了进展，但分子毒性修复（生成毒性降低的结构有效分子替代品）的任务尚未被系统定义或基准测试。", "method": "研究引入了ToxiMol，这是首个针对通用多模态大语言模型（MLLMs）的分子毒性修复基准任务。构建了一个涵盖11个主要任务和560个有代表性毒性分子的标准化数据集。设计了一个由专家毒理学知识指导的、机制感知和任务自适应的提示标注流程。同时，提出了一个自动化评估框架ToxiEval，该框架整合了毒性终点预测、合成可及性、药物相似性和结构相似性，形成高通量评估链。系统评估了近30个主流通用MLLMs，并设计了多项消融研究来分析关键因素。", "result": "实验结果表明，尽管当前的多模态大语言模型（MLLMs）在此任务上仍面临重大挑战，但它们已开始在毒性理解、语义约束遵守和结构感知分子编辑方面展现出有前景的能力。", "conclusion": "尽管当前的多模态大语言模型在分子毒性修复任务中面临挑战，但它们已显示出在毒性理解和分子编辑方面的初步潜力，为未来的研究奠定了基础。", "translation": "毒性仍然是早期药物开发失败的主要原因。尽管分子设计和性质预测取得了进展，但分子毒性修复——生成毒性降低的结构有效分子替代品——的任务尚未被系统定义或基准测试。为了弥补这一空白，我们引入了ToxiMol，这是首个针对通用多模态大语言模型（MLLMs）的分子毒性修复基准任务。我们构建了一个标准化数据集，涵盖11个主要任务和560个具有代表性的毒性分子，涉及多种机制和粒度。我们设计了一个由专家毒理学知识指导的、机制感知和任务自适应的提示标注流程。同时，我们提出了一个自动化评估框架ToxiEval，该框架整合了毒性终点预测、合成可及性、药物相似性和结构相似性，形成高通量评估链，用于评估修复成功率。我们系统评估了近30个主流通用多模态大语言模型，并设计了多项消融研究来分析评估标准、候选多样性和失败归因等关键因素。实验结果表明，尽管当前的多模态大语言模型在此任务上仍面临重大挑战，但它们已开始在毒性理解、语义约束遵守和结构感知分子编辑方面展现出有前景的能力。", "summary": "本文针对早期药物开发中分子毒性修复的挑战，首次提出了一个专门用于评估多模态大语言模型（MLLMs）在该任务上的基准——ToxiMol。ToxiMol包含一个涵盖多任务和毒性分子的标准化数据集，并结合了专家知识的提示标注流程。为实现自动化评估，研究还开发了ToxiEval框架，该框架综合考虑了毒性预测、合成可及性、药物相似性和结构相似性。通过对近30个主流MLLMs的系统评估，研究发现尽管现有模型仍面临挑战，但已展现出在毒性理解和分子结构编辑方面的初步潜力。", "keywords": "分子毒性修复, 多模态大语言模型, 基准测试, 药物开发, ToxiMol", "comments": "该论文的创新之处在于首次系统地定义了分子毒性修复任务，并为多模态大语言模型（MLLMs）构建了第一个专用基准ToxiMol。其提出的ToxiEval评估框架整合了多项关键指标，为评估分子修复的成功率提供了全面的视角。这项工作对于推动MLLMs在药物发现领域的应用具有重要意义，尤其是在解决药物毒性问题上。尽管当前MLLMs的表现仍有提升空间，但该研究为未来开发更高效的分子解毒模型奠定了坚实的基础。"}}
{"id": "2506.10189", "title": "Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction", "authors": ["Mohammad Subhi Al-Batah", "Muhyeeddin Alqaraleh", "Mowafaq Salem Alzboon"], "summary": "Oral cancer presents a formidable challenge in oncology, necessitating early\ndiagnosis and accurate prognosis to enhance patient survival rates. Recent\nadvancements in machine learning and data mining have revolutionized\ntraditional diagnostic methodologies, providing sophisticated and automated\ntools for differentiating between benign and malignant oral lesions. This study\npresents a comprehensive review of cutting-edge data mining methodologies,\nincluding Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines\n(SVM), and ensemble learning techniques, specifically applied to the diagnosis\nand prognosis of oral cancer. Through a rigorous comparative analysis, our\nfindings reveal that Neural Networks surpass other models, achieving an\nimpressive classification accuracy of 93,6 % in predicting oral cancer.\nFurthermore, we underscore the potential benefits of integrating feature\nselection and dimensionality reduction techniques to enhance model performance.\nThese insights underscore the significant promise of advanced data mining\ntechniques in bolstering early detection, optimizing treatment strategies, and\nultimately improving patient outcomes in the realm of oral oncology.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10189v1", "AI": {"title_translation": "机器学习和降维技术改善口腔癌预后", "tldr": "本研究回顾并比较了多种机器学习方法在口腔癌诊断和预后中的应用，发现神经网络在预测口腔癌方面表现最佳，准确率达到93.6%，并强调了特征选择和降维的重要性。", "motivation": "口腔癌的早期诊断和准确预后对于提高患者生存率至关重要，而机器学习和数据挖掘的最新进展为区分良恶性口腔病变提供了先进的自动化工具，因此需要对这些技术在口腔癌领域的应用进行全面审查。", "method": "本研究全面回顾了包括神经网络、K-近邻（KNN）、支持向量机（SVM）和集成学习技术在内的尖端数据挖掘方法，并对其在口腔癌诊断和预后中的应用进行了严格的比较分析。", "result": "研究发现神经网络在预测口腔癌方面表现优于其他模型，分类准确率达到93.6%。此外，研究强调了整合特征选择和降维技术以提高模型性能的潜在益处。", "conclusion": "先进的数据挖掘技术在加强早期检测、优化治疗策略以及最终改善口腔肿瘤学领域患者预后方面具有显著前景。", "translation": "口腔癌在肿瘤学中是一个严峻的挑战，需要早期诊断和准确预后来提高患者生存率。机器学习和数据挖掘的最新进展彻底改变了传统的诊断方法，为区分良性和恶性口腔病变提供了复杂和自动化的工具。本研究全面回顾了尖端数据挖掘方法，包括神经网络、K-近邻（KNN）、支持向量机（SVM）和集成学习技术，并专门将其应用于口腔癌的诊断和预后。通过严格的比较分析，我们的研究结果表明，神经网络超越了其他模型，在预测口腔癌方面取得了93.6%的惊人分类准确率。此外，我们强调了整合特征选择和降维技术以提高模型性能的潜在益处。这些见解突显了先进数据挖掘技术在加强早期检测、优化治疗策略以及最终改善口腔肿瘤学领域患者预后方面的巨大前景。", "summary": "本研究回顾并比较了多种机器学习和数据挖掘方法在口腔癌诊断和预后中的应用。通过对神经网络、KNN、SVM和集成学习等技术进行严格分析，发现神经网络在预测口腔癌方面表现最佳，分类准确率高达93.6%。研究还强调了特征选择和降维技术对提升模型性能的重要性，最终指出先进数据挖掘技术在改善口腔癌患者预后方面的巨大潜力。", "keywords": "口腔癌, 机器学习, 神经网络, 降维, 诊断, 预后", "comments": "本文的创新之处在于系统比较了多种机器学习模型在口腔癌诊断和预后中的性能，并明确指出神经网络的优越性。其重要性在于为口腔癌的早期检测和治疗策略优化提供了数据驱动的见解，有望通过自动化和高精度的诊断工具改善患者预后。局限性可能在于未详细说明所使用的数据集规模、来源以及具体的特征工程过程，也未深入探讨模型的泛化能力和临床转化潜力。"}}
{"id": "2506.10446", "title": "Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty", "authors": ["Zehui Ling", "Deshu Chen", "Hongwei Zhang", "Yifeng Jiao", "Xin Guo", "Yuan Cheng"], "summary": "Large language models (LLMs) have demonstrated significant advancements in\nreasoning capabilities, performing well on various challenging benchmarks.\nTechniques like Chain-of-Thought prompting have been introduced to further\nimprove reasoning. However, these approaches frequently generate longer\noutputs, which in turn increase computational latency. Although some methods\nuse reinforcement learning to shorten reasoning, they often apply uniform\npenalties without considering the problem's complexity, leading to suboptimal\noutcomes. In this study, we seek to enhance the efficiency of LLM reasoning by\npromoting conciseness for simpler problems while preserving sufficient\nreasoning for more complex ones for accuracy, thus improving the model's\noverall performance. Specifically, we manage the model's reasoning efficiency\nby dividing the reward function and including a novel penalty for output\nlength. Our approach has yielded impressive outcomes in benchmark evaluations\nacross three datasets: GSM8K, MATH500, and AIME2024. For the comparatively\nsimpler datasets GSM8K and MATH500, our method has effectively shortened output\nlengths while preserving or enhancing accuracy. On the more demanding AIME2024\ndataset, our approach has resulted in improved accuracy.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10446v1", "AI": {"title_translation": "易题快解，难题深思：通过强化长度惩罚实现高效推理", "tldr": "本研究提出一种新的奖励函数和输出长度惩罚机制，以提高大型语言模型在不同难度问题上的推理效率和准确性。", "motivation": "大型语言模型在推理方面取得了显著进展，但目前的推理技术（如思维链）常导致输出过长，增加计算延迟。现有的缩短推理方法通常采用统一惩罚，未考虑问题复杂性，导致次优结果。", "method": "本研究通过划分奖励函数并引入一种新颖的输出长度惩罚来管理模型的推理效率。该方法旨在针对简单问题促进简洁性，同时为复杂问题保留足够的推理以确保准确性。", "result": "在GSM8K和MATH500等相对简单的基准数据集上，该方法有效缩短了输出长度，同时保持或提高了准确性。在更具挑战性的AIME2024数据集上，该方法提高了准确性。", "conclusion": "通过引入分段奖励函数和新颖的输出长度惩罚，本研究提出的方法能够根据问题复杂度自适应地调整大型语言模型的推理过程，从而在保持或提高准确性的同时显著提高推理效率。", "translation": "大型语言模型（LLMs）在推理能力方面取得了显著进步，在各种具有挑战性的基准测试中表现出色。诸如思维链提示等技术已被引入以进一步提高推理能力。然而，这些方法经常生成更长的输出，这反过来增加了计算延迟。尽管一些方法使用强化学习来缩短推理，但它们通常施加统一的惩罚，而没有考虑问题的复杂性，导致次优结果。在本研究中，我们旨在通过促进简单问题的简洁性，同时为更复杂的问题保留足够的推理以确保准确性，从而提高LLM推理的效率，进而提高模型的整体性能。具体来说，我们通过划分奖励函数并包含一个新颖的输出长度惩罚来管理模型的推理效率。我们的方法在GSM8K、MATH500和AIME2024三个数据集的基准评估中取得了令人印象深刻的结果。对于相对简单的GSM8K和MATH500数据集，我们的方法有效缩短了输出长度，同时保持或提高了准确性。在要求更高的AIME2024数据集上，我们的方法提高了准确性。", "summary": "本研究旨在解决大型语言模型推理效率低下的问题，尤其是在生成过长输出时增加计算延迟。作者提出了一种新颖的方法，通过划分奖励函数并引入自适应的输出长度惩罚，使模型能够根据问题难度调整推理长度：对简单问题追求简洁，对复杂问题保持深度。该方法在GSM8K、MATH500和AIME2024等数据集上进行了评估，结果表明其在简单数据集上能有效缩短输出长度并保持准确性，在复杂数据集上则能提高准确性。", "keywords": "大型语言模型, 推理效率, 长度惩罚, 奖励函数, 自适应推理", "comments": "这项研究的创新之处在于其提出的自适应长度惩罚机制，它突破了现有方法统一惩罚的局限性，能够根据问题复杂度智能地调整LLM的推理过程。这种“易题快解，难题深思”的策略对于提升LLM在实际应用中的效率和性能具有重要意义，尤其是在需要快速响应或资源受限的场景下。"}}
{"id": "2506.10353", "title": "Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation", "authors": ["Runqi Ouyang", "Haoyun Li", "Zhenyuan Zhang", "Xiaofeng Wang", "Zheng Zhu", "Guan Huang", "Xingang Wang"], "summary": "Recent advances in large language models, especially in natural language\nunderstanding and reasoning, have opened new possibilities for text-to-motion\ngeneration. Although existing approaches have made notable progress in semantic\nalignment and motion synthesis, they often rely on end-to-end mapping\nstrategies that fail to capture deep linguistic structures and logical\nreasoning. Consequently, generated motions tend to lack controllability,\nconsistency, and diversity. To address these limitations, we propose Motion-R1,\na unified motion-language modeling framework that integrates a Chain-of-Thought\nmechanism. By explicitly decomposing complex textual instructions into\nlogically structured action paths, Motion-R1 provides high-level semantic\nguidance for motion generation, significantly enhancing the model's ability to\ninterpret and execute multi-step, long-horizon, and compositionally rich\ncommands. To train our model, we adopt Group Relative Policy Optimization, a\nreinforcement learning algorithm designed for large models, which leverages\nmotion quality feedback to optimize reasoning chains and motion synthesis\njointly. Extensive experiments across multiple benchmark datasets demonstrate\nthat Motion-R1 achieves competitive or superior performance compared to\nstate-of-the-art methods, particularly in scenarios requiring nuanced semantic\nunderstanding and long-term temporal coherence. The code, model and data will\nbe publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10353v1", "AI": {"title_translation": "Motion-R1：链式思考推理与强化学习在人体动作生成中的应用", "tldr": "Motion-R1结合链式思考和强化学习，解决了现有文本到动作生成中缺乏控制性、一致性和多样性的问题。", "motivation": "现有文本到动作生成方法依赖端到端映射策略，未能捕捉深层语言结构和逻辑推理，导致生成动作缺乏可控性、一致性和多样性。", "method": "提出Motion-R1，一个统一的动作-语言建模框架，集成了链式思考（Chain-of-Thought）机制，将复杂文本指令分解为逻辑结构化的动作路径，提供高级语义指导。采用群相对策略优化（Group Relative Policy Optimization），一种为大型模型设计的强化学习算法，利用动作质量反馈联合优化推理链和动作合成。", "result": "在多个基准数据集上的广泛实验表明，Motion-R1与最先进的方法相比，实现了有竞争力或更优的性能，特别是在需要细致语义理解和长期时间连贯性的场景中。", "conclusion": "Motion-R1通过结合链式思考和强化学习，有效解决了文本到动作生成中现有方法的局限性，显著提高了生成动作的可控性、一致性和多样性，并在复杂指令下表现出优越性能。", "translation": "最近大语言模型在自然语言理解和推理方面的进展，为文本到动作生成开辟了新的可能性。尽管现有方法在语义对齐和动作合成方面取得了显著进展，但它们通常依赖端到端映射策略，未能捕捉深层语言结构和逻辑推理。因此，生成的动作往往缺乏可控性、一致性和多样性。为了解决这些局限性，我们提出了Motion-R1，一个统一的动作-语言建模框架，它集成了链式思考机制。通过将复杂的文本指令明确分解为逻辑结构化的动作路径，Motion-R1为动作生成提供了高层语义指导，显著增强了模型解释和执行多步骤、长时程和组合丰富命令的能力。为了训练我们的模型，我们采用了群相对策略优化，这是一种为大型模型设计的强化学习算法，它利用动作质量反馈来联合优化推理链和动作合成。在多个基准数据集上的广泛实验表明，Motion-R1与最先进的方法相比，实现了有竞争力或更优的性能，特别是在需要细致语义理解和长期时间连贯性的场景中。代码、模型和数据将公开可用。", "summary": "本文提出了Motion-R1，一个结合链式思考（Chain-of-Thought）机制和强化学习（Group Relative Policy Optimization）的统一动作-语言建模框架，旨在解决现有文本到动作生成方法在可控性、一致性和多样性方面的不足。Motion-R1通过分解复杂指令为逻辑动作路径，增强了模型对多步和复杂命令的理解与执行能力。实验证明，Motion-R1在多项基准测试中表现出与SOTA方法相当或更优的性能，尤其是在需要精细语义理解和长期时间连贯性的任务上。", "keywords": "文本到动作生成, 链式思考, 强化学习, 人体动作生成, 语义理解", "comments": "Motion-R1的创新点在于将大语言模型的链式思考能力引入到文本到动作生成领域，通过显式地分解复杂指令，提升了生成动作的语义理解和逻辑一致性。结合强化学习进行联合优化，进一步增强了模型的性能和生成质量，为长时程、复杂动作序列的生成提供了新的范式。"}}
{"id": "2506.10973", "title": "Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning", "authors": ["Julius Berner", "Miguel Liu-Schiaffini", "Jean Kossaifi", "Valentin Duruisseaux", "Boris Bonev", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "summary": "A wide range of scientific problems, such as those described by\ncontinuous-time dynamical systems and partial differential equations (PDEs),\nare naturally formulated on function spaces. While function spaces are\ntypically infinite-dimensional, deep learning has predominantly advanced\nthrough applications in computer vision and natural language processing that\nfocus on mappings between finite-dimensional spaces. Such fundamental\ndisparities in the nature of the data have limited neural networks from\nachieving a comparable level of success in scientific applications as seen in\nother fields. Neural operators are a principled way to generalize neural\nnetworks to mappings between function spaces, offering a pathway to replicate\ndeep learning's transformative impact on scientific problems. For instance,\nneural operators can learn solution operators for entire classes of PDEs, e.g.,\nphysical systems with different boundary conditions, coefficient functions, and\ngeometries. A key factor in deep learning's success has been the careful\nengineering of neural architectures through extensive empirical testing.\nTranslating these neural architectures into neural operators allows operator\nlearning to enjoy these same empirical optimizations. However, prior neural\noperator architectures have often been introduced as standalone models, not\ndirectly derived as extensions of existing neural network architectures. In\nthis paper, we identify and distill the key principles for constructing\npractical implementations of mappings between infinite-dimensional function\nspaces. Using these principles, we propose a recipe for converting several\npopular neural architectures into neural operators with minimal modifications.\nThis paper aims to guide practitioners through this process and details the\nsteps to make neural operators work in practice. Our code can be found at\nhttps://github.com/neuraloperator/NNs-to-NOs", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10973v1", "AI": {"title_translation": "将神经网络架构扩展到函数空间进行算子学习的原则性方法", "tldr": "本文提出了将现有神经网络架构转换为神经算子的原则和方法，以更好地解决科学领域中的函数空间问题。", "motivation": "传统的深度学习主要应用于有限维空间的数据，但在科学领域（如连续时间动力学系统和偏微分方程）中，问题自然地在函数空间上表述，这些空间通常是无限维的。这种数据性质的根本差异限制了神经网络在科学应用中取得与计算机视觉和自然语言处理领域相当的成功。虽然神经算子是泛化神经网络到函数空间映射的原则性方法，但之前的神经算子架构通常是独立引入的，并未直接从现有神经网络架构扩展而来，这限制了它们享受深度学习中经过经验测试的架构优化。", "method": "本文识别并提炼了构建无限维函数空间映射实用实现的关键原则。利用这些原则，作者提出了一种将几种流行的神经网络架构以最小修改转换为神经算子的“食谱”或方法。", "result": "作者提供了一个将流行神经网络架构转换为神经算子的“食谱”，并详细介绍了使神经算子在实践中工作的步骤。他们还提供了相关代码。", "conclusion": "本文旨在通过提供将现有神经网络架构转换为神经算子的原则和实用方法，指导实践者更好地应用神经算子解决科学问题，从而使算子学习能够利用深度学习中经过验证的架构优化。", "translation": "许多科学问题，例如由连续时间动力学系统和偏微分方程（PDEs）描述的问题，自然地在函数空间上被公式化。虽然函数空间通常是无限维的，但深度学习主要通过计算机视觉和自然语言处理等应用取得了进展，这些应用侧重于有限维空间之间的映射。数据性质上的这种根本差异限制了神经网络在科学应用中取得与其他领域相当的成功。神经算子是将神经网络泛化到函数空间之间映射的一种原则性方法，为复制深度学习对科学问题的变革性影响提供了途径。例如，神经算子可以学习整类偏微分方程的解算子，例如具有不同边界条件、系数函数和几何形状的物理系统。深度学习成功的一个关键因素是通过广泛的经验测试精心设计神经网络架构。将这些神经网络架构转化为神经算子可以使算子学习享受相同的经验优化。然而，以前的神经算子架构通常是作为独立模型引入的，并非直接作为现有神经网络架构的扩展而导出。在本文中，我们识别并提炼了构建无限维函数空间之间映射的实用实现的关键原则。利用这些原则，我们提出了一种将几种流行的神经网络架构以最小修改转换为神经算子的“食谱”。本文旨在指导实践者完成这一过程，并详细说明了使神经算子在实践中工作的步骤。我们的代码可以在 https://github.com/neuraloperator/NNs-to-NOs 找到。", "summary": "本文针对深度学习在处理科学领域无限维函数空间问题上的局限性，提出了将现有神经网络架构扩展到神经算子的原则性方法。作者识别了构建无限维函数空间映射的关键原则，并基于此提出了一种“食谱”，可以将多种流行神经网络架构以最小修改转换为神经算子。这使得算子学习能够利用深度学习中已验证的架构优化，从而更好地解决偏微分方程等科学问题，并提供了实现代码和实践指导。", "keywords": "神经算子, 函数空间, 神经网络架构, 算子学习, 科学计算", "comments": "本文的创新之处在于其系统性地提出了将现有、成熟的神经网络架构转换为神经算子的原则和方法，而非像以往那样将神经算子视为独立模型。这极大地降低了将深度学习的成功经验迁移到科学计算领域的门槛，使得研究人员和工程师能够更便捷地利用已有的架构优化。其提出的“食谱”具有很强的实用性，有助于推动神经算子在实际科学问题中的应用。该工作对于弥合深度学习与科学计算之间的差距具有重要意义。"}}
{"id": "2506.10721", "title": "Commitment Schemes for Multi-Party Computation", "authors": ["Ioan Ionescu", "Ruxandra F. Olimid"], "summary": "The paper presents an analysis of Commitment Schemes (CSs) used in\nMulti-Party Computation (MPC) protocols. While the individual properties of CSs\nand the guarantees offered by MPC have been widely studied in isolation, their\ninterrelation in concrete protocols and applications remains mostly\nunderexplored. This paper presents the relation between the two, with an\nemphasis on (security) properties and their impact on the upper layer MPC. In\nparticular, we investigate how different types of CSs contribute to various MPC\nconstructions and their relation to real-life applications of MPC. The paper\ncan also serve as a tutorial for understanding the cryptographic interplay\nbetween CS and MPC, making it accessible to both researchers and practitioners.\nOur findings emphasize the importance of carefully selecting CS to meet the\nadversarial and functional requirements of MPC, thereby aiming for more robust\nand privacy-preserving cryptographic applications", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10721v1", "AI": {"title_translation": "多方计算中的承诺方案", "tldr": "本文分析了承诺方案（CSs）在多方计算（MPC）协议中的应用，强调了CSs的选择对MPC安全性和功能需求的重要性，旨在提升加密应用的健壮性和隐私性。", "motivation": "尽管承诺方案（CSs）的独立属性和多方计算（MPC）提供的保障已被广泛研究，但它们在具体协议和应用中的相互关系仍未得到充分探索。本文旨在填补这一空白。", "method": "本文分析了承诺方案（CSs）与多方计算（MPC）之间的关系，重点关注安全属性及其对上层MPC的影响。具体来说，研究了不同类型的CSs如何影响各种MPC构造及其与MPC实际应用的关联。该论文也可用作理解CS和MPC之间密码学相互作用的教程。", "result": "研究结果强调了仔细选择承诺方案（CS）以满足多方计算（MPC）的对抗性和功能需求的重要性。", "conclusion": "通过仔细选择承诺方案，可以实现更健壮和隐私保护的密码学应用。", "translation": "本文分析了多方计算（MPC）协议中使用的承诺方案（CSs）。尽管CSs的个体属性和MPC提供的保障已被广泛独立研究，但它们在具体协议和应用中的相互关系在很大程度上仍未被充分探索。本文介绍了两者之间的关系，重点强调（安全）属性及其对上层MPC的影响。特别是，我们研究了不同类型的CSs如何促进各种MPC构造及其与MPC实际应用的关联。该论文也可以作为理解CS和MPC之间密码学相互作用的教程，使其对研究人员和从业者都易于理解。我们的发现强调了仔细选择CS以满足MPC对抗性和功能要求的重要性，从而旨在实现更健壮和隐私保护的密码学应用。", "summary": "本文深入分析了承诺方案（CSs）在多方计算（MPC）协议中的应用，填补了CSs与MPC在实际协议和应用中相互关系研究的空白。论文详细探讨了不同类型CSs的安全属性及其对MPC构造和实际应用的影响，旨在为研究人员和从业者提供理解CS与MPC密码学交互的教程。研究强调，为满足MPC的对抗性和功能要求，精心选择CS至关重要，这有助于构建更强大、更注重隐私的加密应用。", "keywords": "承诺方案, 多方计算, 安全属性, 密码学应用, 隐私保护", "comments": "本文的创新之处在于其对承诺方案（CSs）与多方计算（MPC）之间相互关系的深入分析，填补了该领域研究的空白。其重要性体现在它不仅为研究人员提供了理论指导，还为从业者提供了实用的教程，有助于推动更健壮和隐私保护的密码学应用。该论文在实践和理论之间搭建了桥梁。"}}
{"id": "2506.10947", "title": "Spurious Rewards: Rethinking Training Signals in RLVR", "authors": ["Rulin Shao", "Shuyue Stella Li", "Rui Xin", "Scott Geng", "Yiping Wang", "Sewoong Oh", "Simon Shaolei Du", "Nathan Lambert", "Sewon Min", "Ranjay Krishna", "Yulia Tsvetkov", "Hannaneh Hajishirzi", "Pang Wei Koh", "Luke Zettlemoyer"], "summary": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit\nstrong mathematical reasoning in certain models even with spurious rewards that\nhave little, no, or even negative correlation with the correct answer. For\nexample, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute\npoints by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect\nlabel), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the\n29.1% gained with ground truth rewards. However, the spurious rewards that work\nfor Qwen often fail to yield gains with other model families like Llama3 or\nOLMo2. In particular, we find code reasoning -- thinking in code without actual\ncode execution -- to be a distinctive Qwen2.5-Math behavior that becomes\nsignificantly more frequent after RLVR, from 65% to over 90%, even with\nspurious rewards. Overall, we hypothesize that, given the lack of useful reward\nsignal, RLVR must somehow be surfacing useful reasoning representations learned\nduring pretraining, although the exact mechanism remains a topic for future\nwork. We suggest that future RLVR research should possibly be validated on\ndiverse models rather than a single de facto choice, as we show that it is easy\nto get significant performance gains on Qwen models even with completely\nspurious reward signals.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10947v1", "AI": {"title_translation": "虚假奖励：重新思考RLVR中的训练信号", "tldr": "研究表明，即使使用虚假奖励，RLVR也能在某些模型（如Qwen）中激发强大的数学推理能力，这可能是通过激活预训练的推理能力实现的，但这种效果并非适用于所有模型。", "motivation": "该研究旨在重新思考可验证奖励强化学习（RLVR）中的训练信号，探究即使奖励与正确答案相关性很低、没有甚至为负，RLVR是否仍能有效激发模型的数学推理能力。", "method": "研究人员使用不同类型的“虚假奖励”（如随机奖励、格式奖励、错误标签、单次RL和多数投票）对Qwen2.5-Math-7B模型进行了可验证奖励强化学习（RLVR），并在MATH-500数据集上评估了其性能。同时，他们也测试了Llama3和OLMo2等其他模型家族，并分析了Qwen模型在RLVR后“代码推理”行为的变化。", "result": "Qwen2.5-Math-7B在MATH-500上的性能在绝对点数上提升显著：随机奖励提升21.4%，格式奖励提升13.8%，错误标签提升24.1%，单次RL提升26.0%，多数投票提升27.1%，几乎与使用真实奖励获得的29.1%提升相当。然而，对Qwen有效的虚假奖励未能使Llama3或OLMo2等其他模型家族获得收益。研究发现，即使使用虚假奖励，Qwen2.5-Math在RLVR后，“代码推理”的频率从65%显著增加到90%以上。", "conclusion": "研究认为，即使奖励信号缺乏有用性，RLVR仍然能够有效地激发某些模型（特别是Qwen）中在预训练期间学到的有用推理表示。未来的RLVR研究应在多样化的模型上进行验证，而非仅限于单一模型，因为在Qwen模型上即使使用完全虚假的奖励信号也很容易获得显著的性能提升。", "translation": "我们展示了可验证奖励强化学习（RLVR）即使在奖励与正确答案相关性很低、没有甚至为负的虚假奖励下，也能在某些模型中激发强大的数学推理能力。例如，RLVR使Qwen2.5-Math-7B在MATH-500上的性能绝对点数提升了21.4%（随机奖励）、13.8%（格式奖励）、24.1%（错误标签）、26.0%（单次RL）和27.1%（多数投票），几乎与使用真实奖励获得的29.1%提升相当。然而，对Qwen有效的虚假奖励往往未能使Llama3或OLMo2等其他模型家族获得收益。特别是，我们发现代码推理——在没有实际代码执行的情况下进行代码思考——是Qwen2.5-Math的一个独特行为，在RLVR后，即使使用虚假奖励，其频率也从65%显著增加到90%以上。总的来说，我们推测，鉴于缺乏有用的奖励信号，RLVR必定以某种方式激活了预训练期间学到的有用推理表示，尽管确切机制仍是未来工作的课题。我们建议未来的RLVR研究应可能在多样化的模型上进行验证，而非单一的实际选择，因为我们发现即使使用完全虚假的奖励信号也很容易在Qwen模型上获得显著的性能提升。", "summary": "该论文探讨了可验证奖励强化学习（RLVR）在“虚假奖励”下的有效性，即奖励与正确答案相关性很低或没有。研究表明，RLVR能显著提升Qwen2.5-Math-7B等模型的数学推理能力，其性能提升与真实奖励相当，即使使用随机或不正确的奖励信号。然而，这些虚假奖励对Llama3和OLMo2等其他模型家族无效。研究观察到，RLVR后Qwen模型中的“代码推理”行为显著增加，这表明RLVR可能激活了模型预训练中已有的推理能力。作者建议未来的RLVR研究应在多样化模型上进行验证。", "keywords": "RLVR, 虚假奖励, 数学推理, Qwen, 代码推理", "comments": "该论文挑战了RLVR中奖励信号的传统认知，创新性地证明了即使是高度不可靠的虚假奖励也能在特定模型（如Qwen）中带来显著的性能提升。这暗示RLVR可能不仅通过学习显式信号，更是作为一种机制来解锁模型预先存在的推理能力。然而，其局限性在于结果的模型特异性，虚假奖励仅对Qwen有效，这凸显了在更广泛模型上进行验证的重要性。"}}
{"id": "2506.10200", "title": "DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection", "authors": ["Tina Behrouzi", "Sana Tonekaboni", "Rahul G. Krishnan", "Anna Goldenberg"], "summary": "Real-world observational data often contain existing or emerging\nheterogeneous subpopulations that deviate from global patterns. The majority of\nmodels tend to overlook these underrepresented groups, leading to inaccurate or\neven harmful predictions. Existing solutions often rely on detecting these\nsamples as Out-of-domain (OOD) rather than adapting the model to new emerging\npatterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational\nAutoencoder framework that jointly performs representation learning and\nadaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with\nthe data by dynamically updating its latent structure to capture new trends. It\nleverages a novel non-parametric clustering mechanism, inspired by Gaussian\nMixture Models, to discover and model latent subgroups based on embedding\nsimilarity. Extensive experiments show that DynaSubVAE achieves competitive\nperformance in both near-OOD and far-OOD detection, and excels in class-OOD\nscenarios where an entire class is missing during training. We further\nillustrate that our dynamic subgrouping mechanism outperforms standalone\nclustering methods such as GMM and KMeans++ in terms of both OOD accuracy and\nregret precision.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10200v1", "AI": {"title_translation": "DynaSubVAE：可扩展和鲁棒OOD检测的自适应子分组", "tldr": "DynaSubVAE是一个动态子分组变分自编码器框架，通过动态更新其潜在结构来同时进行表示学习和自适应OOD检测，在各种OOD检测场景中表现出色。", "motivation": "现实世界数据常包含异质子群体，现有模型易忽略这些未被充分代表的群体，导致不准确预测。现有解决方案多将这些样本视为域外（OOD）而非适应新模式。", "method": "本文提出了DynaSubVAE，一个动态子分组变分自编码器框架，它联合执行表示学习和自适应OOD检测。DynaSubVAE通过动态更新其潜在结构来捕捉新趋势，利用一种受高斯混合模型启发的非参数聚类机制，根据嵌入相似性发现和建模潜在子群体。", "result": "DynaSubVAE在近OOD和远OOD检测中均取得了竞争性表现，尤其在训练期间缺失整个类别的类OOD场景中表现出色。动态子分组机制在OOD准确性和后悔精度方面均优于独立聚类方法（如GMM和KMeans++）。", "conclusion": "DynaSubVAE通过其动态子分组机制，成功地解决了异质数据中的OOD检测问题，并在多种OOD场景下展现出优越的性能，特别是对于未见类别。", "translation": "现实世界中的观测数据通常包含现有或新兴的异质子群体，这些群体偏离了全局模式。大多数模型往往忽视这些代表性不足的群体，导致不准确甚至有害的预测。现有解决方案通常依赖于将这些样本检测为域外（OOD），而不是使模型适应新的新兴模式。我们引入了DynaSubVAE，一个动态子分组变分自编码器框架，它联合执行表示学习和自适应OOD检测。与传统方法不同，DynaSubVAE通过动态更新其潜在结构以捕捉新趋势，与数据一同演进。它利用一种受高斯混合模型启发的、新颖的非参数聚类机制，根据嵌入相似性发现和建模潜在子群体。大量实验表明，DynaSubVAE在近OOD和远OOD检测中均取得了竞争性表现，并在训练期间缺少整个类别的类OOD场景中表现出色。我们进一步说明，我们的动态子分组机制在OOD准确性和后悔精度方面均优于独立的聚类方法，如GMM和KMeans++。", "summary": "DynaSubVAE是一种新颖的动态子分组变分自编码器框架，旨在解决现实世界数据中异质子群体的OOD检测问题。它通过动态更新潜在结构和利用非参数聚类机制，实现了表示学习和自适应OOD检测的联合进行。实验证明，DynaSubVAE在多种OOD场景，特别是类OOD检测中表现卓越，且其动态子分组机制优于传统聚类方法。", "keywords": "OOD检测, 变分自编码器, 动态子分组, 异质数据, 聚类", "comments": "DynaSubVAE的创新之处在于其动态子分组机制，使其能够随着数据演进并适应新的模式，而非仅仅将新样本标记为OOD。这对于处理现实世界中不断变化的异质数据流具有重要意义，尤其是在处理未见类别或新兴群体时，提高了模型的鲁棒性和预测准确性。"}}
{"id": "2506.10486", "title": "Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers", "authors": ["Xanh Ho", "Sunisth Kumar", "Yun-Ang Wu", "Florian Boudin", "Atsuhiro Takasu", "Akiko Aizawa"], "summary": "Scientific claim verification against tables typically requires predicting\nwhether a claim is supported or refuted given a table. However, we argue that\npredicting the final label alone is insufficient: it reveals little about the\nmodel's reasoning and offers limited interpretability. To address this, we\nreframe table-text alignment as an explanation task, requiring models to\nidentify the table cells essential for claim verification. We build a new\ndataset by extending the SciTab benchmark with human-annotated cell-level\nrationales. Annotators verify the claim label and highlight the minimal set of\ncells needed to support their decision. After the annotation process, we\nutilize the collected information and propose a taxonomy for handling ambiguous\ncases. Our experiments show that (i) incorporating table alignment information\nimproves claim verification performance, and (ii) most LLMs, while often\npredicting correct labels, fail to recover human-aligned rationales, suggesting\nthat their predictions do not stem from faithful reasoning.", "comment": "8 pages; code and data are available at\n  https://github.com/Alab-NII/SciTabAlign", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10486v1", "AI": {"title_translation": "表格-文本对齐：解释科学论文中针对表格的声明验证", "tldr": "本文将表格文本对齐重构为解释任务，以提高科学声明验证的可解释性。通过构建一个新数据集并进行实验，发现表格对齐信息能提高验证性能，但大型语言模型在预测正确标签的同时，未能恢复人类对齐的理由。", "motivation": "传统的科学声明验证方法（即预测支持或反驳标签）不足以揭示模型推理过程，且可解释性有限。为了解决这一问题，需要一种方法来识别对声明验证至关重要的表格单元格，从而提高模型的可解释性。", "method": "将表格-文本对齐重构为一个解释任务，要求模型识别对声明验证至关重要的表格单元格。通过扩展SciTab基准，构建了一个新数据集，其中包含人工标注的单元格级别的理由。标注者验证声明标签并突出显示支持其决策所需的最小单元格集。基于收集到的信息，提出了一种处理模糊情况的分类法。", "result": "实验表明：(i) 结合表格对齐信息可以提高声明验证性能；(ii) 大多数大型语言模型（LLMs）虽然经常预测正确的标签，但未能恢复人类对齐的理由，这表明它们的预测并非源于忠实的推理。", "conclusion": "将表格-文本对齐视为解释任务，并通过提供单元格级别的理由，可以提高科学声明验证的可解释性和性能。然而，大型语言模型在生成可信赖的解释方面仍有不足，其预测可能并非基于人类认可的推理过程。", "translation": "科学声明针对表格的验证通常需要预测声明是否被表格支持或反驳。然而，我们认为仅仅预测最终标签是不够的：它几乎没有揭示模型的推理过程，并且提供了有限的可解释性。为了解决这个问题，我们将表格-文本对齐重构为一个解释任务，要求模型识别对声明验证至关重要的表格单元格。我们通过扩展SciTab基准，构建了一个新数据集，其中包含人工标注的单元格级别理由。标注者验证声明标签并突出显示支持其决策所需的最小单元格集。在标注过程之后，我们利用收集到的信息，并提出了一种处理模糊情况的分类法。我们的实验表明：(i) 结合表格对齐信息可以提高声明验证性能；(ii) 大多数大型语言模型（LLMs）虽然经常预测正确的标签，但未能恢复人类对齐的理由，这表明它们的预测并非源于忠实的推理。", "summary": "本文将科学论文中针对表格的声明验证任务重新定义为表格-文本对齐的解释任务，旨在提高模型的可解释性。通过扩展现有基准数据集SciTab，引入了人工标注的单元格级别理由，以识别支持声明验证的关键表格单元格。研究提出了一种处理模糊情况的分类法。实验结果显示，整合表格对齐信息能提升声明验证性能，但大型语言模型在预测准确标签的同时，未能有效捕捉人类认定的解释依据，表明其推理过程可能缺乏忠实性。", "keywords": "表格-文本对齐, 声明验证, 可解释性AI, 大型语言模型, 科学论文", "comments": "本文的创新之处在于将传统的声明验证任务转化为一个解释性任务，通过要求模型识别关键表格单元格来增强可解释性，这对于理解模型决策过程至关重要。其构建带有单元格级理由的新数据集对后续研究有重要价值。此外，对LLMs在解释性方面不足的发现，揭示了当前LLMs虽然在预测准确率上表现良好，但在推理的忠实性上仍有待提高，为未来的LLM研究指明了方向。"}}
{"id": "2506.10361", "title": "FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device", "authors": ["Novendra Setyawan", "Chi-Chia Sun", "Mao-Hsiu Hsu", "Wen-Kai Kuo", "Jun-Wei Hsieh"], "summary": "This paper introduces FaceLiVT, a lightweight yet powerful face recognition\nmodel that integrates a hybrid Convolution Neural Network (CNN)-Transformer\narchitecture with an innovative and lightweight Multi-Head Linear Attention\n(MHLA) mechanism. By combining MHLA alongside a reparameterized token mixer,\nFaceLiVT effectively reduces computational complexity while preserving\ncompetitive accuracy. Extensive evaluations on challenging benchmarks;\nincluding LFW, CFP-FP, AgeDB-30, IJB-B, and IJB-C; highlight its superior\nperformance compared to state-of-the-art lightweight models. MHLA notably\nimproves inference speed, allowing FaceLiVT to deliver high accuracy with lower\nlatency on mobile devices. Specifically, FaceLiVT is 8.6 faster than EdgeFace,\na recent hybrid CNN-Transformer model optimized for edge devices, and 21.2\nfaster than a pure ViT-Based model. With its balanced design, FaceLiVT offers\nan efficient and practical solution for real-time face recognition on\nresource-constrained platforms.", "comment": "2025 ICIP", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10361v1", "AI": {"title_translation": "FaceLiVT：用于移动设备的结构重参数化线性视觉Transformer人脸识别", "tldr": "FaceLiVT是一种轻量级混合CNN-Transformer人脸识别模型，通过线性注意力机制和重参数化降低了计算复杂度，同时在移动设备上实现了高精度和低延迟，性能优于现有轻量级模型。", "motivation": "在移动设备上实现高效、准确的实时人脸识别面临计算复杂度和准确性的挑战，需要一个既轻量级又强大的解决方案。", "method": "引入FaceLiVT模型，它结合了混合卷积神经网络（CNN）-Transformer架构与创新的轻量级多头线性注意力（MHLA）机制。通过将MHLA与重参数化令牌混合器结合，有效降低计算复杂度。", "result": "在LFW、CFP-FP、AgeDB-30、IJB-B和IJB-C等挑战性基准测试中表现出优越性能，超越了最先进的轻量级模型。MHLA显著提高了推理速度，FaceLiVT比EdgeFace快8.6倍，比纯ViT模型快21.2倍，实现了高精度和低延迟。", "conclusion": "FaceLiVT为资源受限平台上的实时人脸识别提供了一个高效实用的解决方案。", "translation": "本文介绍了FaceLiVT，这是一种轻量级但功能强大的人脸识别模型，它集成了混合卷积神经网络（CNN）-Transformer架构与创新且轻量级的多头线性注意力（MHLA）机制。通过将MHLA与重参数化令牌混合器结合，FaceLiVT有效降低了计算复杂度，同时保持了具有竞争力的准确性。在LFW、CFP-FP、AgeDB-30、IJB-B和IJB-C等挑战性基准测试上的广泛评估突出显示了其与最先进轻量级模型相比的卓越性能。MHLA显著提高了推理速度，使FaceLiVT能够在移动设备上以较低延迟提供高精度。具体而言，FaceLiVT比EdgeFace（一种为边缘设备优化的近期混合CNN-Transformer模型）快8.6倍，比纯ViT模型快21.2倍。凭借其均衡的设计，FaceLiVT为资源受限平台上的实时人脸识别提供了一个高效实用的解决方案。", "summary": "FaceLiVT是一种专为移动设备设计的人脸识别模型，它融合了CNN和Transformer架构，并引入了创新的多头线性注意力（MHLA）机制和重参数化令牌混合器。该模型在降低计算复杂度的同时保持了高准确性，并在多个基准测试中展现出优于现有轻量级模型的性能，尤其在推理速度上显著提升。FaceLiVT为资源受限环境下的实时人脸识别提供了一个高效且实用的解决方案。", "keywords": "人脸识别, 线性视觉Transformer, 移动设备, 结构重参数化, 轻量级模型", "comments": "FaceLiVT的创新之处在于其混合CNN-Transformer架构与轻量级多头线性注意力机制的结合，以及结构重参数化。这使其能够在移动设备上实现高效的人脸识别，兼顾了速度和精度，是边缘计算领域的一项重要进展。"}}
{"id": "2506.10722", "title": "TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks", "authors": ["Xiaoxing Mo", "Yuxuan Cheng", "Nan Sun", "Leo Yu Zhang", "Wei Luo", "Shang Gao"], "summary": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where\nattackers implant hidden triggers during training to maliciously control model\nbehavior. Topological Evolution Dynamics (TED) has recently emerged as a\npowerful tool for detecting backdoor attacks in DNNs. However, TED can be\nvulnerable to backdoor attacks that adaptively distort topological\nrepresentation distributions across network layers. To address this limitation,\nwe propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow\nrelease, and Target mapping attack strategies), a novel defense strategy that\nenhances TED's robustness against adaptive attacks. TED-LaST introduces two key\ninnovations: label-supervised dynamics tracking and adaptive layer emphasis.\nThese enhancements enable the identification of stealthy threats that evade\ntraditional TED-based defenses, even in cases of inseparability in topological\nspace and subtle topological perturbations. We review and classify data\npoisoning tricks in state-of-the-art adaptive attacks and propose enhanced\nadaptive attack with target mapping, which can dynamically shift malicious\ntasks and fully leverage the stealthiness that adaptive attacks possess. Our\ncomprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and\nImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST\neffectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,\nand the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for\nrobust backdoor detection, substantially enhancing DNN security against\nevolving threats.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10722v1", "AI": {"title_translation": "TED-LaST：迈向抵御自适应攻击的鲁棒后门防御", "tldr": "TED-LaST是一种新颖的防御策略，通过引入标签监督动力学跟踪和自适应层强调，增强了拓扑演化动力学（TED）对自适应后门攻击的鲁棒性，有效对抗复杂的后门威胁。", "motivation": "深度神经网络（DNN）容易受到后门攻击，攻击者在训练期间植入隐藏触发器以恶意控制模型行为。现有基于拓扑演化动力学（TED）的防御方法可能容易受到自适应攻击，这些攻击会扭曲跨网络层的拓扑表示分布。", "method": "本文提出了TED-LaST（Topological Evolution Dynamics against Laundry, Slow release, and Target mapping attack strategies），一种新的防御策略，通过引入标签监督动力学跟踪和自适应层强调，增强了TED对自适应攻击的鲁棒性。此外，本文还回顾并分类了最先进自适应攻击中的数据中毒技巧，并提出了一种带有目标映射的增强型自适应攻击。", "result": "在多个数据集（CIFAR-10、GTSRB和ImageNet100）和模型架构（ResNet20、ResNet101）上的综合实验表明，TED-LaST有效地对抗了复杂的后门，如Adap-Blend、Adapt-Patch和本文提出的增强型自适应攻击。", "conclusion": "TED-LaST为鲁棒后门检测树立了新基准，显著增强了DNN抵御不断演变威胁的安全性。", "translation": "深度神经网络（DNN）容易受到后门攻击，攻击者在训练期间植入隐藏触发器以恶意控制模型行为。拓扑演化动力学（TED）最近已成为检测DNN中后门攻击的强大工具。然而，TED可能容易受到自适应地扭曲跨网络层拓扑表示分布的后门攻击。为了解决这一限制，我们提出了TED-LaST（Topological Evolution Dynamics against Laundry, Slow release, and Target mapping attack strategies），一种新颖的防御策略，增强了TED抵御自适应攻击的鲁棒性。TED-LaST引入了两项关键创新：标签监督动力学跟踪和自适应层强调。这些增强功能能够识别即使在拓扑空间中不可分离和细微拓扑扰动的情况下也能规避传统TED防御的隐蔽威胁。我们回顾并分类了最先进自适应攻击中的数据中毒技巧，并提出了一种带有目标映射的增强型自适应攻击，该攻击可以动态转移恶意任务并充分利用自适应攻击所具有的隐蔽性。我们在多个数据集（CIFAR-10、GTSRB和ImageNet100）和模型架构（ResNet20、ResNet101）上的综合实验表明，TED-LaST有效地对抗了复杂的后门，如Adap-Blend、Adapt-Patch和本文提出的增强型自适应攻击。TED-LaST为鲁棒后门检测树立了新基准，显著增强了DNN抵御不断演变威胁的安全性。", "summary": "本文提出了TED-LaST，一种增强拓扑演化动力学（TED）对自适应后门攻击鲁棒性的新型防御策略。针对现有TED方法在面对自适应攻击时可能失效的问题，TED-LaST引入了标签监督动力学跟踪和自适应层强调，使其能够识别传统方法难以发现的隐蔽威胁。研究还分类了现有的数据中毒技巧并提出了增强型自适应攻击。在多个数据集和模型上的实验证明，TED-LaST能有效对抗多种复杂的后门攻击，为深度神经网络的安全防御设立了新标准。", "keywords": "后门攻击, 深度神经网络, 拓扑演化动力学, 自适应攻击, 防御", "comments": "本文的创新点在于提出了TED-LaST防御策略，通过引入标签监督动力学跟踪和自适应层强调，显著提升了现有拓扑演化动力学（TED）方法在应对自适应后门攻击时的鲁棒性。这对于提高深度神经网络在面对不断演变威胁时的安全性具有重要意义。"}}
{"id": "2506.10234", "title": "Playing in the Sandbox: A Study on the Usability of Seccomp", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "Sandboxing restricts what applications do, and prevents exploited processes\nbeing abused; yet relatively few applications get sandboxed: why? We report a\nusability trial with 7 experienced Seccomp developers exploring how they\napproached sandboxing an application and the difficulties they faced. The\ndevelopers each approached sandboxing the application differently and each came\nto different solutions. We highlight many challenges of using Seccomp, the\nsandboxing designs by the participants, and what developers think would make it\neasier for them to sandbox applications effectively.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.10234v1", "AI": {"title_translation": "在沙箱中玩耍：Seccomp可用性研究", "tldr": "研究了7名经验丰富的Seccomp开发者对沙箱化应用的可用性挑战，发现他们的方法和解决方案各不相同，并指出了Seccomp的使用难点。", "motivation": "尽管沙箱技术能够限制应用程序行为并防止被利用的进程被滥用，但实际应用中很少有程序被沙箱化，本研究旨在探究其原因。", "method": "对7名有经验的Seccomp开发者进行了一项可用性试验，观察他们如何对应用程序进行沙箱化以及他们面临的困难。", "result": "开发者们对沙箱化应用程序采取了不同的方法，并得出了不同的解决方案。研究揭示了使用Seccomp的诸多挑战、参与者的沙箱设计，以及开发者认为能有效简化沙箱化应用的改进建议。", "conclusion": "本研究揭示了Seccomp在实际应用中面临的可用性挑战，并提出了开发者对改进沙箱化工具的期望，这对于未来提升沙箱技术的普及和效率具有指导意义。", "translation": "沙箱技术限制了应用程序的行为，并防止被利用的进程被滥用；然而，相对较少的应用程序被沙箱化：为什么？我们报告了一项针对7名经验丰富的Seccomp开发者进行的可用性试验，探讨了他们如何进行应用程序沙箱化以及他们面临的困难。开发者们各自以不同的方式进行应用程序沙箱化，并得出了不同的解决方案。我们强调了使用Seccomp的许多挑战、参与者的沙箱设计，以及开发者认为什么能让他们更有效地沙箱化应用程序。", "summary": "本研究通过对7名经验丰富的Seccomp开发者进行可用性试验，探究了应用程序沙箱化过程中遇到的挑战。结果显示，开发者们在沙箱化方法和解决方案上存在显著差异，研究识别了Seccomp的诸多使用难题，并收集了开发者关于如何提高沙箱化效率的建议。", "keywords": "Seccomp, 沙箱, 可用性, 开发者, 挑战", "comments": "这项研究通过用户研究方法直接探究了Seccomp沙箱技术的实际可用性问题，而非仅仅关注技术本身。其创新之处在于揭示了开发者在实际操作中面临的具体困难，这对于改进沙箱工具的设计和推广具有重要意义。研究结果表明，即使是经验丰富的开发者也面临挑战，暗示了该技术在普及方面存在可用性障碍。"}}
{"id": "2504.15777", "title": "Tina: Tiny Reasoning Models via LoRA", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Willie Neiswanger"], "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2504.15777v1", "AI": {"title_translation": "Tina：基于LoRA的微型推理模型", "tldr": "Tina通过在微型基础模型上使用LoRA进行强化学习，以极低的成本实现了强大的推理能力，性能与现有SOTA模型相当甚至超越。", "motivation": "本文旨在解决如何在语言模型中以经济高效的方式实现强大的推理能力这一根本问题。", "method": "研究提出了Tina，一种通过在已有的1.5B参数微型基础模型上，利用低秩适应（LoRA）在强化学习（RL）过程中进行参数高效更新，从而开发出微型推理模型的方法。", "result": "Tina模型在极低的计算后训练成本下（例如，最佳模型在AIME24上实现了超过20%的推理性能提升和43.33%的Pass@1准确率，成本仅为9美元，估计成本降低了260倍），实现了与SOTA RL推理模型相当甚至超越的推理性能。研究在多个开源推理数据集和消融设置中验证了其有效性。", "conclusion": "研究揭示了通过LoRA进行高效强化学习推理的惊人有效性。这种方法能够使模型快速适应强化学习奖励的推理结构格式，同时很大程度上保留了基础模型的底层知识。", "translation": "论文标题：Tina：基于LoRA的微型推理模型\n\n论文摘要：在语言模型中，如何以成本效益高的方式实现强大的推理能力？受这一基本问题的驱动，我们提出了Tina，一个以高成本效益实现的微型推理模型家族。值得注意的是，Tina通过在强化学习（RL）过程中，使用低秩适应（LoRA）对一个仅有1.5B参数的微型基础模型进行参数高效更新，证明了仅用最少资源即可开发出显著的推理性能。这种极简主义方法产生的模型，其推理性能与基于相同基础模型构建的SOTA RL推理模型相比具有竞争力，有时甚至超越。关键在于，这是以现有SOTA模型所采用的计算后训练成本的极小一部分实现的。事实上，最佳的Tina模型在AIME24上实现了超过20%的推理性能提升和43.33%的Pass@1准确率，而其后训练和评估成本仅为9美元（即估计成本降低了260倍）。我们的工作揭示了通过LoRA进行高效RL推理的惊人有效性。我们从一组固定的超参数开始，在多个开源推理数据集和各种消融设置中验证了这一点。此外，我们推测这种有效性和效率源于LoRA能够快速使模型适应RL奖励的推理结构格式，同时很大程度上保留了基础模型的底层知识。为了促进可访问性和开放研究，我们完全开源了所有代码、训练日志以及模型权重和检查点。", "summary": "本文介绍了Tina，一个通过在微型语言模型上应用LoRA进行强化学习，以极低成本实现强大推理能力的模型家族。Tina证明了仅需少量资源即可达到与现有SOTA模型相当甚至超越的推理性能，显著降低了计算成本。研究推测其高效性源于LoRA能快速适应推理结构，同时保留基础模型知识。", "keywords": "微型推理模型, LoRA, 强化学习, 成本效益, 参数高效微调", "comments": "Tina模型在低资源下实现高性能推理是一个重要的创新，特别是在模型部署和可访问性方面。它证明了参数高效微调在强化学习中的潜力，为构建更经济、更环保的AI模型提供了新思路。其主要贡献在于通过LoRA结合RL实现了显著的成本效益和竞争力。"}}
{"id": "2506.10205", "title": "AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent", "authors": ["Jing Liu", "Toshiaki Koike-Akino", "Ye Wang", "Hassan Mansour", "Matthew Brand"], "summary": "To address the enormous size of Large Language Models (LLMs), model\ncompression methods, such as quantization and pruning, are often deployed,\nespecially on edge devices. In this work, we focus on layer-wise post-training\nquantization and pruning. Drawing connections between activation-aware weight\npruning and sparse approximation problems, and motivated by the success of\nIterative Hard Thresholding (IHT), we propose a unified method for\nActivation-aware Weight pruning and quantization via Projected gradient descent\n(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM\npruning and quantization methods. Theoretical convergence guarantees of the\nproposed method for pruning are also provided.", "comment": "ICML 2025 workshop on Efficient Systems for Foundation Models", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10205v1", "AI": {"title_translation": "AWP：基于投影梯度下降的激活感知权重剪枝与量化", "tldr": "AWP是一种新的统一方法，用于在LLM上进行激活感知权重剪枝和量化，它通过投影梯度下降实现，并优于现有方法。", "motivation": "解决大型语言模型（LLMs）体积庞大，尤其是在边缘设备上部署时，需要进行模型压缩（如量化和剪枝）的问题。", "method": "提出了一种名为AWP（Activation-aware Weight pruning and quantization via Projected gradient descent）的统一方法，该方法将激活感知权重剪枝与稀疏近似问题联系起来，并受到迭代硬阈值（IHT）成功的启发。", "result": "实验证明AWP优于最先进的LLM剪枝和量化方法。该方法还提供了剪枝的理论收敛保证。", "conclusion": "AWP提供了一种有效且统一的LLM压缩方法，通过激活感知剪枝和量化，实现了优于现有技术的性能和理论保证。", "translation": "为了解决大型语言模型（LLMs）的巨大体积问题，模型压缩方法，如量化和剪枝，经常被部署，尤其是在边缘设备上。在这项工作中，我们专注于逐层训练后量化和剪枝。通过将激活感知权重剪枝与稀疏近似问题联系起来，并受迭代硬阈值（IHT）成功的启发，我们提出了一种通过投影梯度下降实现激活感知权重剪枝和量化（AWP）的统一方法。我们的实验表明，AWP优于最先进的LLM剪枝和量化方法。本文还提供了所提出剪枝方法的理论收敛保证。", "summary": "本文提出了一种名为AWP的统一模型压缩方法，用于大型语言模型（LLMs）的逐层训练后量化和剪枝。AWP将激活感知权重剪枝与稀疏近似问题相结合，并借鉴了迭代硬阈值（IHT）的成功经验，通过投影梯度下降实现。实验结果表明，AWP在LLM剪枝和量化方面优于现有最先进的方法，并且提供了剪枝的理论收敛保证。", "keywords": "大型语言模型, 模型压缩, 剪枝, 量化, 投影梯度下降", "comments": "AWP的创新之处在于其将激活感知权重剪枝与稀疏近似问题相结合，并利用投影梯度下降实现统一的剪枝和量化框架。其在LLM压缩上的SOTA性能和理论收敛保证，使其在边缘设备部署大型模型方面具有重要意义。"}}
{"id": "2506.10491", "title": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models", "authors": ["Aleksandra Sorokovikova", "Pavel Chizhov", "Iuliia Eremenko", "Ivan P. Yamshchikov"], "summary": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10491v1", "AI": {"title_translation": "表面公平，深层偏见：语言模型偏见的比较研究", "tldr": "语言模型在某些基准测试中表现出表面公平性，但在评估用户答案或提供建议时会显示出显著偏见，这在模型个性化趋势下尤为突出。", "motivation": "现代语言模型在大量数据上训练，这些数据不可避免地包含有争议和带有刻板印象的内容，导致模型表达偏见观点或根据分配的人格或用户的人格产生不同结果。本文旨在调查大型语言模型（LLMs）中偏见的各种代理度量。", "method": "本文调查了大型语言模型（LLMs）中偏见的各种代理度量。具体通过以下方式进行：1) 在多学科基准（MMLU）上使用预设人格评估模型。2) 重新设计任务，要求模型对用户答案进行评分。3) 要求模型提供薪资谈判建议。", "result": "1) 使用预设人格在MMLU基准上评估模型时，分数差异微不足道且大多随机。2) 当任务重新设计为要求模型评分用户答案时，显示出更显著的偏见迹象。3) 当要求模型提供薪资谈判建议时，答案中表现出明显的偏见。", "conclusion": "虽然在某些评估下语言模型可能表现出表面公平，但更深入的互动（如评估用户答案或提供专业建议）会揭示显著的偏见。随着LLM助手记忆和个性化趋势的兴起，这些偏见问题将从新的角度出现，因为模型可能已了解用户的社会人口学信息。", "translation": "现代语言模型在大量数据上训练。这些数据不可避免地包含有争议和带有刻板印象的内容，其中包含与性别、出身、年龄等相关的各种偏见。因此，模型会表达偏见观点或根据分配的人格或用户的个性产生不同的结果。在本文中，我们调查了大型语言模型（LLMs）中偏见的各种代理度量。我们发现，在多学科基准（MMLU）上使用预设人格评估模型会导致分数差异微不足道且大多随机。然而，如果我们重新设计任务，要求模型对用户的答案进行评分，这会显示出更显著的偏见迹象。最后，如果我们要求模型提供薪资谈判建议，我们会在答案中看到明显的偏见。随着LLM助手记忆和个性化的最新趋势，这些问题从一个不同的角度出现：现代LLM用户不需要预先提示他们的人格描述，因为模型已经了解他们的社会人口学信息。", "summary": "本文研究了大型语言模型（LLMs）中存在的偏见问题。研究发现，尽管在一些表面评估（如MMLU基准上的预设人格测试）中，模型表现出微小的偏见差异，但在更深层次的互动中，如评估用户答案或提供薪资谈判建议时，模型会表现出显著的偏见。随着语言模型个性化和记忆功能的普及，这些深层偏见将变得更加突出，因为模型将直接了解用户的社会人口学信息，可能导致更隐蔽的偏见输出。", "keywords": "语言模型, 偏见, 公平性, 个性化, MMLU", "comments": "这项研究揭示了语言模型偏见的复杂性。其创新之处在于，它不仅关注了表面公平性，还深入探讨了在不同交互场景下（特别是涉及用户评估和建议时）偏见的表现。研究结果强调，随着LLM个性化功能的普及，模型内部的深层偏见将构成更大的挑战，因为模型能够直接获取用户数据，可能在用户无感知的情况下传递偏见。这项工作的局限性可能在于其偏见测量方法的普适性，但它为未来LLM偏见研究和缓解策略提供了重要方向。"}}
{"id": "2506.10366", "title": "FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui", "Yuhan Lyu"], "summary": "The infrared and visible images fusion (IVIF) is receiving increasing\nattention from both the research community and industry due to its excellent\nresults in downstream applications. Existing deep learning approaches often\nutilize convolutional neural networks to extract image features. However, the\ninherently capacity of convolution operations to capture global context can\nlead to information loss, thereby restricting fusion performance. To address\nthis limitation, we propose an end-to-end fusion network named the\nFrequency-Spatial Attention Transformer Fusion Network (FSATFusion). The\nFSATFusion contains a frequency-spatial attention Transformer (FSAT) module\ndesigned to effectively capture discriminate features from source images. This\nFSAT module includes a frequency-spatial attention mechanism (FSAM) capable of\nextracting significant features from feature maps. Additionally, we propose an\nimproved Transformer module (ITM) to enhance the ability to extract global\ncontext information of vanilla Transformer. We conducted both qualitative and\nquantitative comparative experiments, demonstrating the superior fusion quality\nand efficiency of FSATFusion compared to other state-of-the-art methods.\nFurthermore, our network was tested on two additional tasks without any\nmodifications, to verify the excellent generalization capability of FSATFusion.\nFinally, the object detection experiment demonstrated the superiority of\nFSATFusion in downstream visual tasks. Our code is available at\nhttps://github.com/Lmmh058/FSATFusion.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10366v1", "AI": {"title_translation": "FSATFusion：用于红外和可见光图像融合的频率-空间注意力Transformer", "tldr": "FSATFusion是一个新的基于Transformer的网络，用于红外和可见光图像融合，解决了CNN捕获全局上下文的局限性，并表现出卓越的融合性能和泛化能力。", "motivation": "现有深度学习方法在红外和可见光图像融合（IVIF）中常使用卷积神经网络（CNN）提取特征，但CNN捕获全局上下文的能力有限，这会导致信息丢失，从而限制融合性能。", "method": "提出了一种名为频率-空间注意力Transformer融合网络（FSATFusion）的端到端融合网络。FSATFusion包含一个频率-空间注意力Transformer（FSAT）模块，旨在有效捕获源图像的区分性特征。该FSAT模块包括一个频率-空间注意力机制（FSAM），能够从特征图中提取重要特征。此外，还提出了一个改进的Transformer模块（ITM）以增强原始Transformer提取全局上下文信息的能力。", "result": "通过定性和定量比较实验，FSATFusion展示了优于其他最先进方法的融合质量和效率。此外，该网络在未经任何修改的情况下，在两项额外任务上进行了测试，验证了FSATFusion出色的泛化能力。最后，目标检测实验证明了FSATFusion在下游视觉任务中的优越性。", "conclusion": "FSATFusion通过其新颖的频率-空间注意力Transformer设计，有效解决了现有方法在红外和可见光图像融合中全局上下文捕获的局限性，并展现了卓越的融合性能、效率、泛化能力以及在下游任务中的应用潜力。", "translation": "红外和可见光图像融合（IVIF）因其在下游应用中的出色表现而受到研究界和工业界日益增长的关注。现有的深度学习方法通常利用卷积神经网络来提取图像特征。然而，卷积操作固有的捕获全局上下文的能力不足，可能导致信息丢失，从而限制融合性能。为了解决这一局限性，我们提出了一种名为频率-空间注意力Transformer融合网络（FSATFusion）的端到端融合网络。FSATFusion包含一个频率-空间注意力Transformer（FSAT）模块，旨在有效捕获源图像的区分性特征。该FSAT模块包括一个频率-空间注意力机制（FSAM），能够从特征图中提取重要特征。此外，我们提出了一种改进的Transformer模块（ITM）来增强原始Transformer提取全局上下文信息的能力。我们进行了定性和定量比较实验，证明了FSATFusion与其他最先进方法相比具有卓越的融合质量和效率。此外，我们的网络在未经任何修改的情况下，在两项额外任务上进行了测试，以验证FSATFusion出色的泛化能力。最后，目标检测实验证明了FSATFusion在下游视觉任务中的优越性。我们的代码可在https://github.com/Lmmh058/FSATFusion获取。", "summary": "FSATFusion是一种新型的端到端频率-空间注意力Transformer网络，专为红外和可见光图像融合（IVIF）设计，旨在克服现有基于CNN方法在捕获全局上下文信息方面的不足。该网络包含一个频率-空间注意力Transformer（FSAT）模块，结合了频率-空间注意力机制（FSAM）和改进的Transformer模块（ITM），以高效提取判别性特征和增强全局上下文理解。实验结果表明，FSATFusion在融合质量、效率、泛化能力和下游视觉任务表现上均优于最先进方法。", "keywords": "红外和可见光图像融合, Transformer, 频率-空间注意力, 图像融合, 深度学习", "comments": "该论文提出了一种创新的Transformer架构FSATFusion，将频率和空间注意力机制结合，有效解决了传统CNN在图像融合中全局上下文信息捕获不足的问题。其在多种任务和下游应用中的优异表现，特别是泛化能力，展现了该方法的潜力和重要性。"}}
{"id": "2506.10744", "title": "ObfusBFA: A Holistic Approach to Safeguarding DNNs from Different Types of Bit-Flip Attacks", "authors": ["Xiaobei Yan", "Han Qiu", "Tianwei Zhang"], "summary": "Bit-flip attacks (BFAs) represent a serious threat to Deep Neural Networks\n(DNNs), where flipping a small number of bits in the model parameters or binary\ncode can significantly degrade the model accuracy or mislead the model\nprediction in a desired way. Existing defenses exclusively focus on protecting\nmodels for specific attacks and platforms, while lacking effectiveness for\nother scenarios. We propose ObfusBFA, an efficient and holistic methodology to\nmitigate BFAs targeting both the high-level model weights and low-level\ncodebase (executables or shared libraries). The key idea of ObfusBFA is to\nintroduce random dummy operations during the model inference, which effectively\ntransforms the delicate attacks into random bit flips, making it much harder\nfor attackers to pinpoint and exploit vulnerable bits. We design novel\nalgorithms to identify critical bits and insert obfuscation operations. We\nevaluate ObfusBFA against different types of attacks, including the adaptive\nscenarios where the attacker increases the flip bit budget to attempt to\ncircumvent our defense. The results show that ObfusBFA can consistently\npreserve the model accuracy across various datasets and DNN architectures while\nsignificantly reducing the attack success rates. Additionally, it introduces\nminimal latency and storage overhead, making it a practical solution for\nreal-world applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10744v1", "AI": {"title_translation": "ObfusBFA: 一种保护深度神经网络免受不同类型位翻转攻击的整体方法", "tldr": "ObfusBFA通过引入随机虚拟操作来防御针对DNNs的位翻转攻击，有效保持模型精度并降低攻击成功率。", "motivation": "位翻转攻击（BFAs）对深度神经网络（DNNs）构成严重威胁，现有防御措施仅针对特定攻击和平台，缺乏普适性，无法有效应对其他场景。", "method": "本文提出了ObfusBFA，一种高效且整体的方法来缓解针对高层模型权重和低层代码库（可执行文件或共享库）的位翻转攻击。其核心思想是在模型推理期间引入随机虚拟操作，将精确的攻击转化为随机位翻转，从而使攻击者难以定位和利用脆弱位。为此，论文设计了新颖的算法来识别关键位并插入混淆操作。", "result": "ObfusBBA在不同数据集和DNN架构上都能持续保持模型精度，同时显著降低攻击成功率。此外，它引入了最小的延迟和存储开销。", "conclusion": "ObfusBFA是一种高效、实用且全面的解决方案，能够有效防御针对深度神经网络的位翻转攻击，适用于实际应用。", "translation": "位翻转攻击（BFAs）对深度神经网络（DNNs）构成严重威胁，其中翻转模型参数或二进制代码中少量位就可能显著降低模型精度或以期望的方式误导模型预测。现有防御措施仅专注于保护特定攻击和平台下的模型，而对其他场景缺乏有效性。我们提出了ObfusBFA，一种高效且整体的方法来缓解针对高层模型权重和低层代码库（可执行文件或共享库）的BFAs。ObfusBFA的关键思想是在模型推理期间引入随机虚拟操作，这有效地将精细攻击转化为随机位翻转，使攻击者更难定位和利用脆弱位。我们设计了新颖的算法来识别关键位并插入混淆操作。我们评估了ObfusBFA针对不同类型攻击的防御能力，包括攻击者增加翻转位预算以试图规避我们防御的自适应场景。结果表明，ObfusBFA在各种数据集和DNN架构上都能持续保持模型精度，同时显著降低攻击成功率。此外，它引入了最小的延迟和存储开销，使其成为实际应用的实用解决方案。", "summary": "本文提出ObfusBFA，一种针对深度神经网络（DNNs）位翻转攻击（BFAs）的整体防御方法。该方法通过在模型推理时引入随机虚拟操作，将精确攻击转化为随机位翻转，从而有效保护DNNs免受针对模型权重和代码库的BFAs。实验证明ObfusBFA在保持模型精度的同时显著降低攻击成功率，且开销极低，具有很高的实用性。", "keywords": "位翻转攻击, 深度神经网络, 模型安全, 混淆, 鲁棒性", "comments": "本文的创新点在于提出了一个“整体（holistic）”的防御方法ObfusBFA，能够同时对抗针对高层模型权重和低层代码库的位翻转攻击，弥补了现有防御方法仅专注于特定攻击和平台的不足。其核心思想——通过引入随机虚拟操作来“模糊化”攻击，将精确攻击转化为随机位翻转——非常巧妙且高效。该方法在实际应用中具有潜在价值，因为它提供了广泛的保护，同时保持了较低的性能开销。"}}
{"id": "2506.09967", "title": "Resa: Transparent Reasoning Models via SAEs", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09967v1", "AI": {"title_translation": "Resa：通过稀疏自编码器实现透明推理模型", "tldr": "Resa使用一种新颖的SAE-Tuning方法，以极低的成本和时间在语言模型中高效地诱导强大的推理能力，并证明了其通用性和模块性。", "motivation": "研究如何在语言模型中通过利用其底层表示，以成本效益高的方式激发强大的推理能力。", "method": "本文提出了Resa，一个1.5B推理模型家族，采用新颖高效的稀疏自编码器调优（SAE-Tuning）过程。该方法首先训练稀疏自编码器（SAE）从源模型中捕获推理能力，然后利用训练好的SAE指导标准的监督微调过程，在目标模型中激发这些能力，全程仅使用经过验证的问答数据，无需推理痕迹。", "result": "在RL后训练之前应用于某些基础模型时，SAE-Tuning保留了其RL训练对应模型超过97%的推理性能，同时将训练成本降低了2000倍以上（约1美元），训练时间缩短了450倍以上（约20分钟）。应用于轻度RL训练模型时，它能以大约1美元的额外成本实现如AIME24上43.33% Pass@1和AMC23上90% Pass@1的推理性能。通过SAE提取的推理能力具有潜在的通用性和模块性，即能从一个数据集泛化到更大的语料库，并可无需重新训练地附加到其他模型上产生可比的增益。", "conclusion": "通过SAE-Tuning，可以极大地提高在语言模型中激发推理能力的成本效益，并且这些能力表现出良好的通用性和模块性。", "translation": "**标题**：Resa：通过稀疏自编码器实现透明推理模型\n\n**摘要**：我们如何才能通过利用语言模型的底层表示，以经济高效的方式在其中激发强大的推理能力？我们用Resa回答了这个问题，Resa是一个1.5B推理模型家族，通过一种新颖高效的稀疏自编码器调优（SAE-Tuning）过程进行训练。该方法首先训练一个SAE从源模型中捕获推理能力，然后利用训练好的SAE指导标准的监督微调过程，在目标模型中激发这些能力，所有这些都使用经过验证的问答数据，而无需任何推理痕迹。值得注意的是，当在进一步RL后训练之前应用于某些基础模型时，SAE-Tuning保留了其RL训练对应模型超过97%的推理性能，同时将训练成本降低了2000倍以上，降至约1美元，训练时间缩短了450倍以上，降至约20分钟。此外，当应用于轻度RL训练的模型（例如，在2个GPU上训练1小时内），它能以大约1美元的额外成本实现如AIME24上43.33% Pass@1和AMC23上90% Pass@1的推理性能。令人惊讶的是，通过SAE提取的推理能力可能既具有通用性又具有模块性。通用性意味着从一个数据集提取的能力仍然能提升在更大、重叠语料库上的性能。模块性意味着从Qwen或Qwen-Math提取的能力可以在测试时无需任何重新训练地附加到R1-Distill模型上，并产生可比的增益。广泛的消融实验验证了这些发现，并且所有工件都已完全开源。", "summary": "本文介绍了Resa，一个1.5B的推理模型家族，它通过创新的稀疏自编码器调优（SAE-Tuning）方法，以极低的成本和时间高效地在语言模型中诱导强大的推理能力。SAE-Tuning通过训练SAE捕获源模型的推理能力，并将其引导至目标模型。实验证明，该方法能以极小的训练开销（约1美元，20分钟）实现与RL训练模型相当的推理性能，甚至在轻度RL训练模型上取得显著成果。此外，通过SAE提取的推理能力展现出良好的通用性和模块性，即能在不同数据集上泛化并可作为模块附加到其他模型而无需重新训练。所有相关成果均已开源。", "keywords": "Resa, 稀疏自编码器, 推理模型, SAE-Tuning, 语言模型", "comments": "这篇论文通过引入SAE-Tuning，提供了一种成本效益极高且高效的方法来在语言模型中激发推理能力。其创新点在于利用稀疏自编码器来捕获和转移推理能力，显著降低了训练成本和时间。通用性和模块性的发现尤其重要，这表明提取的推理能力可能具有更广泛的应用前景，并为构建可组合的AI系统提供了新的思路。这项工作对资源受限的AI研究和应用具有重要意义。"}}
{"id": "2506.10504", "title": "Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models", "authors": ["Sangmin Song", "Juhwan Choi", "JungMin Yun", "YoungBin Kim"], "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nzero-shot dialogue state tracking (DST), reducing the need for task-specific\ntraining. However, conventional DST benchmarks primarily focus on structured\nuser-agent conversations, failing to capture the complexities of real-world\nmulti-user interactions. In this study, we assess the robustness of LLMs in\nmulti-user DST while minimizing dataset construction costs. Inspired by recent\nadvances in LLM-based data annotation, we extend an existing DST dataset by\ngenerating utterances of a second user based on speech act theory. Our\nmethodology systematically incorporates a second user's utterances into\nconversations, enabling a controlled evaluation of LLMs in multi-user settings.\nExperimental results reveal a significant performance drop compared to\nsingle-user DST, highlighting the limitations of current LLMs in extracting and\ntracking dialogue states amidst multiple speakers. Our findings emphasize the\nneed for future research to enhance LLMs for multi-user DST scenarios, paving\nthe way for more realistic and robust DST models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10504v1", "AI": {"title_translation": "超越单用户对话：评估大型语言模型在多用户对话状态跟踪中的能力", "tldr": "本研究评估了大型语言模型在多用户对话状态跟踪中的能力，发现与单用户设置相比，性能显著下降，表明当前LLM在多说话人环境下存在局限性。", "motivation": "尽管大型语言模型在零样本对话状态跟踪（DST）中表现出色，但传统的DST基准测试主要关注结构化的用户-代理对话，未能捕捉真实世界多用户交互的复杂性。因此，本研究旨在评估LLM在多用户DST中的鲁棒性。", "method": "研究人员通过基于言语行为理论生成第二个用户的语句，扩展了一个现有的DST数据集。这种方法系统地将第二个用户的语句整合到对话中，从而在多用户设置中实现了对LLM的受控评估。", "result": "实验结果显示，与单用户DST相比，性能显著下降。这突出表明了当前大型语言模型在从多个说话人中提取和跟踪对话状态方面的局限性。", "conclusion": "研究结果强调，未来的研究需要增强大型语言模型以适应多用户DST场景，从而为更真实、更强大的DST模型铺平道路。", "translation": "大型语言模型（LLMs）在零样本对话状态跟踪（DST）中展现出卓越的性能，减少了对特定任务训练的需求。然而，传统的DST基准测试主要关注结构化的用户-代理对话，未能捕捉真实世界多用户交互的复杂性。在本研究中，我们评估了LLMs在多用户DST中的鲁棒性，同时最大限度地降低了数据集构建成本。受LLM驱动的数据标注最新进展的启发，我们通过基于言语行为理论生成第二个用户的语句，扩展了一个现有的DST数据集。我们的方法系统地将第二个用户的语句整合到对话中，从而在多用户设置中实现了对LLMs的受控评估。实验结果显示，与单用户DST相比，性能显著下降，突出表明了当前LLMs在多个说话人之间提取和跟踪对话状态方面的局限性。我们的发现强调，未来的研究需要增强LLMs以适应多用户DST场景，从而为更真实、更强大的DST模型铺平道路。", "summary": "本研究评估了大型语言模型（LLMs）在多用户对话状态跟踪（DST）中的能力。为解决传统DST基准未能捕捉多用户复杂性的问题，研究人员基于言语行为理论，通过生成第二个用户的语句来扩展现有DST数据集，从而在受控环境中评估LLMs。实验结果表明，与单用户DST相比，LLMs在多用户DST中的性能显著下降，揭示了当前LLMs在处理多说话人对话状态跟踪方面的局限性。研究强调未来需加强LLMs以适应多用户DST场景。", "keywords": "多用户对话状态跟踪, 大型语言模型, 对话状态跟踪, 零样本学习, 数据集扩展", "comments": "本研究的创新之处在于其通过系统性地引入第二用户语句来构建多用户对话数据集的方法，这为评估LLM在复杂交互环境中的能力提供了一个受控且成本效益高的新途径。其重要性在于揭示了当前LLM在多用户DST方面存在的显著性能瓶颈，明确指出了未来研究的方向。论文的局限性在于当前LLM在多说话人对话状态跟踪方面的表现不佳，这需要进一步的算法和模型改进。"}}
{"id": "2506.10371", "title": "Revisiting Transformers with Insights from Image Filtering", "authors": ["Laziz U. Abdullaev", "Maksim Tkachenko", "Tan M. Nguyen"], "summary": "The self-attention mechanism, a cornerstone of Transformer-based\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\nfundamentally challenging to interpret. Establishing a robust theoretical\nfoundation to explain its remarkable success and limitations has therefore\nbecome an increasingly prominent focus in recent research. Some notable\ndirections have explored understanding self-attention through the lens of image\ndenoising and nonparametric regression. While promising, existing frameworks\nstill lack a deeper mechanistic interpretation of various architectural\ncomponents that enhance self-attention, both in its original formulation and\nsubsequent variants. In this work, we aim to advance this understanding by\ndeveloping a unifying image processing framework, capable of explaining not\nonly the self-attention computation itself but also the role of components such\nas positional encoding and residual connections, including numerous later\nvariants. We also pinpoint potential distinctions between the two concepts\nbuilding upon our framework, and make effort to close this gap. We introduce\ntwo independent architectural modifications within transformers. While our\nprimary objective is interpretability, we empirically observe that image\nprocessing-inspired modifications can also lead to notably improved accuracy\nand robustness against data contamination and adversaries across language and\nvision tasks as well as better long sequence understanding.", "comment": "12 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10371v1", "AI": {"title_translation": "借鉴图像滤波的Transformer再探", "tldr": "本文旨在通过开发一个统一的图像处理框架，深入解释Transformer中自注意力机制及其组件（如位置编码、残差连接）的运作原理，并提出了两种受图像处理启发的Transformer架构修改，经验证可提高准确性和鲁棒性。", "motivation": "Transformer中的自注意力机制是启发式的且难以解释，缺乏一个鲁棒的理论基础来解释其成功和局限性。现有框架未能深入解释增强自注意力机制的各种架构组件（如位置编码、残差连接及其变体）的机理。", "method": "开发一个统一的图像处理框架，用于解释自注意力计算、位置编码和残差连接的作用，并提出了两种独立的、受图像处理启发的Transformer架构修改。", "result": "经验证明，受图像处理启发的修改可以显著提高Transformer在语言和视觉任务中的准确性和对数据污染及对抗性攻击的鲁棒性，并改善长序列理解能力。", "conclusion": "通过引入一个统一的图像处理框架和相关的架构修改，不仅提升了对Transformer内部机制的理解，还带来了实际的性能提升，特别是在准确性和鲁棒性方面。", "translation": "自注意力机制作为Transformer这一最先进深度学习架构的基石，很大程度上是启发式驱动的，并且从根本上难以解释。因此，建立一个强大的理论基础来解释其显著的成功和局限性已成为近期研究中日益突出的焦点。一些引人注目的方向已经探索了通过图像去噪和非参数回归的角度来理解自注意力。尽管前景光明，但现有框架仍然缺乏对增强自注意力机制的各种架构组件（无论是其原始公式还是后续变体）的更深层次的机械解释。在这项工作中，我们旨在通过开发一个统一的图像处理框架来推进这种理解，该框架不仅能够解释自注意力计算本身，还能解释位置编码和残差连接等组件的作用，包括许多后续变体。我们还基于我们的框架，指出这两个概念之间潜在的区别，并努力弥合这一差距。我们在Transformer中引入了两种独立的架构修改。虽然我们的主要目标是可解释性，但我们凭经验观察到，受图像处理启发的修改还可以显著提高在语言和视觉任务中的准确性和对数据污染和对抗性攻击的鲁棒性，以及更好的长序列理解能力。", "summary": "这项工作通过引入一个统一的图像处理框架，旨在深入理解Transformer中自注意力机制及其核心组件（如位置编码和残差连接）的运作原理。该研究不仅提供了更深层次的机制解释，还提出了两种受图像处理启发的Transformer架构修改。经验结果表明，这些修改不仅有助于提高可解释性，还能显著提升模型在语言和视觉任务中的准确性、对数据污染和对抗性攻击的鲁棒性，并改善长序列理解能力。", "keywords": "Transformer, 自注意力, 图像滤波, 可解释性, 鲁棒性", "comments": "这篇论文通过将Transformer的自注意力机制与图像处理（特别是图像滤波）联系起来，提供了一个新颖的视角来解释其内部运作原理，具有创新性。它不仅关注理论可解释性，还通过实际的架构修改证明了这种跨领域见解的实用价值，即在提高模型性能和鲁棒性方面。"}}
{"id": "2506.10755", "title": "Quantifying Azure RBAC Wildcard Overreach", "authors": ["Christophe Parisel"], "summary": "Azure RBAC leverages wildcard permissions to simplify policy authoring, but\nthis abstraction often obscures the actual set of allowed operations and\nundermines least-privilege guarantees. We introduce Belshazaar, a two-stage\nframework that targets both the effective permission set problem and the\nevaluation of wildcards permissions spread. First, we formalize Azure action\nsyntax via a context free grammar and implement a compiler that expands any\nwildcard into its explicit action set. Second, we define an ultrametric\ndiameter metric to quantify semantic overreach in wildcard scenarios. Applied\nto Microsoft s official catalog of 15481 actions, Belshazaar reveals that about\n39 percent of actions admit a cross Resource Provider reach when associated\nwith non obvious wildcards, and that effective permissions sets are effectively\ncomputable. These findings demonstrate that wildcard patterns can introduce\nsubstantial privilege bloat, and that our approach offers a scalable, semantics\ndriven path toward tighter, least-privilege RBAC policies in Azure\nenvironments.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10755v1", "AI": {"title_translation": "量化 Azure RBAC 通配符过度授权", "tldr": "Belshazaar 框架量化了 Azure RBAC 通配符权限的过度授权问题，发现约39%的操作存在跨资源提供商的过度授权，并证明了有效权限集的可计算性，有助于实现更严格的最小权限策略。", "motivation": "Azure RBAC 中的通配符权限简化了策略编写，但它模糊了实际允许的操作集，并破坏了最小权限原则。因此，需要一种方法来量化和理解这些通配符权限的实际影响。", "method": "论文介绍了 Belshazaar 框架，该框架分两阶段进行：1. 通过上下文无关文法形式化 Azure 操作语法，并实现一个编译器将任何通配符扩展为显式操作集。2. 定义一个超度量直径指标来量化通配符场景中的语义过度授权。", "result": "将 Belshazaar 应用于微软官方的15481个操作目录，结果显示约39%的操作在与不明显的通配符关联时，会产生跨资源提供商的权限覆盖，并且有效权限集是可计算的。", "conclusion": "研究结果表明，通配符模式会引入大量的权限膨胀，并且所提出的方法为在 Azure 环境中实现更严格、最小权限的 RBAC 策略提供了一条可扩展、语义驱动的途径。", "translation": "Azure RBAC 利用通配符权限来简化策略编写，但这种抽象常常模糊了实际允许的操作集，并破坏了最小权限保证。我们引入了 Belshazaar，一个两阶段框架，旨在解决有效权限集问题和评估通配符权限的扩散。首先，我们通过上下文无关文法形式化 Azure 操作语法，并实现了一个编译器，将任何通配符扩展为其显式操作集。其次，我们定义了一个超度量直径指标来量化通配符场景中的语义过度授权。将 Belshazaar 应用于微软官方的15481个操作目录，结果显示约39%的操作在与不明显的通配符关联时，会产生跨资源提供商的权限覆盖，并且有效权限集是可计算的。这些发现表明，通配符模式会引入大量的权限膨胀，并且我们的方法为在 Azure 环境中实现更严格、最小权限的 RBAC 策略提供了一条可扩展、语义驱动的途径。", "summary": "本研究提出了 Belshazaar 框架，旨在量化 Azure RBAC 中通配符权限的过度授权问题。该框架首先通过形式化语法和编译器将通配符扩展为显式操作，然后使用超度量直径指标量化语义过度授权。实验结果表明，近四成的 Azure 操作因通配符而存在跨资源提供商的权限膨胀，验证了有效权限集的可计算性。这证明了所提方法能有效识别并帮助收紧 Azure 环境中的最小权限 RBAC 策略。", "keywords": "Azure RBAC, 通配符, 权限管理, 最小权限, 云安全", "comments": "Belshazaar 框架通过结合形式化语法分析和量化指标，为理解和管理云环境中复杂的通配符权限提供了一种新颖且实用的方法。其创新之处在于能够将模糊的通配符权限具体化并量化其“越界”程度，对于提升云安全治理和实现最小权限原则具有重要意义。"}}
{"id": "2506.10001", "title": "Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure", "authors": ["Yuxuan Li", "Sheng Jinag", "Bizhu Wang"], "summary": "With technology advancing and the pursuit of new audiovisual experiences\nstrengthening, the metaverse has gained surging enthusiasm. However, it faces\npractical hurdles as substantial data like high-resolution virtual scenes must\nbe transmitted between cloud platforms and VR devices. Specifically, the VR\ndevice's wireless transmission hampered by insufficient bandwidth, causes speed\nand delay problems. Meanwhile, poor channel quality leads to data errors and\nworsens user experience. To solve this, we've proposed the Semantic\nCommunication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service\n(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic\ntransmission, video synthesis, and 3D virtual scene reconstruction. By\ndeploying semantic modules on VR devices and edge servers and sending key\nsemantic info instead of focusing on bit-level reconstruction, it can cut\nlatency, resolve the resource-bandwidth conflict, and better withstand channel\ninterference. Also, the cloud deploys video synthesis and 3D scene\nreconstruction preprocessing, while edge devices host 3D reconstruction\nrendering modules, all for immersive services. Verified on Meta Quest Pro, the\nSC-CEE-Meta can reduce wireless transmission delay by 96.05\\% and boost image\nquality by 43.99\\% under poor channel condition.", "comment": "arXiv admin note: text overlap with arXiv:2407.13764 by other authors", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10001v1", "AI": {"title_translation": "语义通信赋能的云边端协同元宇宙服务架构", "tldr": "元宇宙服务面临带宽和延迟问题，本文提出SC-CEE-Meta架构，通过语义通信和云边端协同显著降低延迟并提升图像质量。", "motivation": "随着技术进步和对新视听体验的追求，元宇宙获得了巨大的热情。然而，元宇宙服务面临实际障碍，如高分辨率虚拟场景等大量数据需要在云平台和VR设备之间传输。具体而言，VR设备的无线传输受到带宽不足的阻碍，导致速度和延迟问题。同时，较差的信道质量导致数据错误并恶化用户体验。", "method": "本文提出了一种语义通信赋能的云边端协同沉浸式元宇宙服务（SC-CEE-Meta）架构，包含VR视频语义传输、视频合成和3D虚拟场景重建三个模块。通过在VR设备和边缘服务器上部署语义模块，发送关键语义信息而非位级重建，以减少延迟、解决资源带宽冲突并增强抗信道干扰能力。此外，云端负责视频合成和3D场景重建预处理，边缘设备负责3D重建渲染模块。", "result": "在Meta Quest Pro上验证，SC-CEE-Meta可以将无线传输延迟降低96.05%，并在信道条件较差的情况下将图像质量提高43.99%。", "conclusion": "SC-CEE-Meta架构通过语义通信和云边端协同，有效解决了元宇宙服务中无线传输的带宽、延迟和信道干扰问题，显著提升了用户体验。", "translation": "随着技术进步和对新视听体验追求的加强，元宇宙获得了高涨的热情。然而，它面临实际障碍，因为高分辨率虚拟场景等大量数据必须在云平台和VR设备之间传输。具体而言，VR设备的无线传输受到带宽不足的阻碍，导致速度和延迟问题。同时，较差的信道质量导致数据错误并恶化用户体验。为了解决这个问题，我们提出了语义通信赋能的云边端协同沉浸式元宇宙服务（SC-CEE-Meta）架构，其中包括三个模块：VR视频语义传输、视频合成和3D虚拟场景重建。通过在VR设备和边缘服务器上部署语义模块，并发送关键语义信息而不是关注位级重建，它可以减少延迟，解决资源带宽冲突，并更好地抵抗信道干扰。此外，云端部署视频合成和3D场景重建预处理，而边缘设备托管3D重建渲染模块，所有这些都用于沉浸式服务。在Meta Quest Pro上验证，SC-CEE-Meta可以将无线传输延迟降低96.05%，并在信道条件较差的情况下将图像质量提高43.99%。", "summary": "本文针对元宇宙服务中VR设备无线传输带宽不足、延迟高和信道质量差导致的用户体验问题，提出了一种语义通信赋能的云边端协同沉浸式元宇宙服务（SC-CEE-Meta）架构。该架构通过在VR设备和边缘服务器部署语义模块，传输关键语义信息，并结合云边端协同处理，有效降低了无线传输延迟，提升了图像质量，解决了资源带宽冲突，并增强了对信道干扰的抵抗能力。实验结果显示其在延迟和图像质量方面有显著提升。", "keywords": "语义通信, 元宇宙, 云边端协同, VR, 传输延迟", "comments": "这篇论文的创新点在于将语义通信引入到云边端协同的元宇宙服务架构中，解决了传统位级传输在高带宽、低延迟要求下的局限性。通过传输关键语义信息而非原始比特流，有效地缓解了VR设备无线传输的瓶颈问题，尤其是在恶劣信道条件下仍能保持较好的性能，这对于提升元宇宙用户体验具有重要意义。"}}
{"id": "2506.10508", "title": "Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs", "authors": ["Yilin Xiao", "Chuang Zhou", "Qinggang Zhang", "Bo Li", "Qing Li", "Xiao Huang"], "summary": "Large language models (LLMs) often struggle with knowledge-intensive tasks\ndue to a lack of background knowledge and a tendency to hallucinate. To address\nthese limitations, integrating knowledge graphs (KGs) with LLMs has been\nintensively studied. Existing KG-enhanced LLMs focus on supplementary factual\nknowledge, but still struggle with solving complex questions. We argue that\nrefining the relationships among facts and organizing them into a logically\nconsistent reasoning path is equally important as factual knowledge itself.\nDespite their potential, extracting reliable reasoning paths from KGs poses the\nfollowing challenges: the complexity of graph structures and the existence of\nmultiple generated paths, making it difficult to distinguish between useful and\nredundant ones. To tackle these challenges, we propose the RRP framework to\nmine the knowledge graph, which combines the semantic strengths of LLMs with\nstructural information obtained through relation embedding and bidirectional\ndistribution learning. Additionally, we introduce a rethinking module that\nevaluates and refines reasoning paths according to their significance.\nExperimental results on two public datasets show that RRP achieves\nstate-of-the-art performance compared to existing baseline methods. Moreover,\nRRP can be easily integrated into various LLMs to enhance their reasoning\nabilities in a plug-and-play manner. By generating high-quality reasoning paths\ntailored to specific questions, RRP distills effective guidance for LLM\nreasoning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10508v1", "AI": {"title_translation": "可靠推理路径：利用知识图谱为大型语言模型推理提炼有效指导", "tldr": "RRP框架利用知识图谱为大型语言模型提供可靠的推理路径，通过结合语义和结构信息，并引入反思模块，解决了LLM在知识密集型任务中的挑战，实现了最先进的性能。", "motivation": "大型语言模型（LLMs）在知识密集型任务中常因缺乏背景知识和幻觉问题而表现不佳。现有结合知识图谱（KGs）的LLMs侧重于补充事实知识，但在解决复杂问题时仍有困难。本文认为，提炼事实间的关系并组织成逻辑一致的推理路径与事实知识本身同等重要。从KGs中提取可靠推理路径面临图结构复杂性和多路径选择的挑战。", "method": "提出了RRP（Reliable Reasoning Path）框架来挖掘知识图谱，该框架结合了LLM的语义优势以及通过关系嵌入和双向分布学习获得的结构信息。此外，引入了一个反思模块，根据推理路径的重要性对其进行评估和优化。", "result": "在两个公共数据集上的实验结果表明，RRP与现有基线方法相比，实现了最先进的性能。此外，RRP可以方便地以即插即用的方式集成到各种LLM中，以增强它们的推理能力。", "conclusion": "RRP通过生成针对特定问题量身定制的高质量推理路径，为大型语言模型的推理提炼了有效的指导。", "translation": "大型语言模型（LLMs）由于缺乏背景知识和容易产生幻觉，在知识密集型任务中常常表现不佳。为了解决这些限制，将知识图谱（KGs）与LLMs集成已得到深入研究。现有的KG增强型LLMs侧重于补充事实知识，但仍然难以解决复杂问题。我们认为，提炼事实之间的关系并将其组织成逻辑一致的推理路径与事实知识本身同样重要。尽管它们具有潜力，但从KGs中提取可靠的推理路径面临以下挑战：图结构的复杂性以及存在多个生成的路径，这使得区分有用路径和冗余路径变得困难。为了应对这些挑战，我们提出了RRP框架来挖掘知识图谱，该框架结合了LLMs的语义优势和通过关系嵌入和双向分布学习获得的结构信息。此外，我们引入了一个反思模块，根据推理路径的重要性对其进行评估和优化。在两个公共数据集上的实验结果表明，RRP与现有基线方法相比，实现了最先进的性能。此外，RRP可以方便地以即插即用的方式集成到各种LLMs中，以即插即用的方式增强它们的推理能力。通过生成针对特定问题量身定制的高质量推理路径，RRP为LLM推理提炼了有效的指导。", "summary": "本文提出了RRP框架，旨在通过从知识图谱中提取可靠的推理路径来提升大型语言模型（LLMs）在知识密集型任务中的推理能力。针对现有方法仅关注事实知识而忽略推理路径的不足，RRP结合了LLM的语义理解与知识图谱的结构信息（通过关系嵌入和双向分布学习），并引入了一个反思模块来评估和优化路径。实验证明，RRP在公共数据集上达到了最先进的性能，并能以即插即用的方式集成到不同LLM中，有效指导其推理。", "keywords": "大型语言模型, 知识图谱, 推理路径, RRP, 知识蒸馏", "comments": "该论文的创新点在于将LLM与知识图谱结合的重点从单纯的事实知识补充转向了“推理路径”的提炼和优化。RRP框架通过结合语义和结构信息，并引入独到的“反思模块”来评估和精炼推理路径，有效解决了从复杂知识图谱中提取可靠推理路径的挑战。其“即插即用”的特性也大大增强了其实用性和影响力。"}}
{"id": "2506.10386", "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial", "authors": ["Jerry Yan", "Chinmay Talegaonkar", "Nicholas Antipa", "Eric Terrill", "Sophia Merrifield"], "summary": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10386v1", "AI": {"title_translation": "利用六自由度姿态基础模型进行海洋沉积物埋藏测绘", "tldr": "本文介绍了一种名为PoseIDON的计算机视觉流程，结合深度基础模型特征和多视图摄影测量技术，从ROV视频中估计海底物体的六自由度姿态和周围海底的方位，从而推断埋藏深度。该方法实现了约10厘米的平均埋藏深度误差，并能解析反映沉积物输运过程的空间埋藏模式。", "motivation": "海底人为物体埋藏状态的准确估计对于了解局部沉积动力学、评估生态风险、潜在污染物输运以及有害物质（如弹药）的回收或缓解策略的可行性至关重要。然而，由于部分遮挡、能见度差和物体降解，通过遥感图像准确估计埋藏深度仍然很困难。", "method": "本文引入了一种名为PoseIDON的计算机视觉流程，该流程将深度基础模型特征与多视图摄影测量相结合，用于从ROV视频中估计物体的六自由度姿态和周围海底的方位。通过将物体的CAD模型与观测图像对齐，并拟合海底的局部平面近似来推断埋藏深度。", "result": "该模型在圣佩德罗盆地历史海洋倾倒场记录的54个物体（包括桶和弹药）的视频片段上进行了验证。该模型实现了约10厘米的平均埋藏深度误差，并解析了反映底层沉积物输运过程的空间埋藏模式。", "conclusion": "该方法实现了海底埋藏的可扩展、非侵入式测绘，并支持对受污染场地的环境评估。", "translation": "海底人为物体的埋藏状态提供了对局部沉积动力学的深入了解，对于评估生态风险、潜在污染物输运以及弹药等有害物质的回收或缓解策略的可行性也至关重要。由于部分遮挡、能见度差和物体降解，通过遥感图像准确估计准确的埋藏深度仍然很困难。这项工作引入了一个名为PoseIDON的计算机视觉流程，它结合了深度基础模型特征与多视图摄影测量，以从ROV视频中估计六自由度物体姿态和周围海底的方位。埋藏深度通过将物体的CAD模型与观测图像对齐，并拟合海底的局部平面近似来推断。该方法使用在圣佩德罗盆地一个历史海洋倾倒场记录的54个物体（包括桶和弹药）的视频片段进行了验证。该模型实现了大约10厘米的平均埋藏深度误差，并解析了反映底层沉积物输运过程的空间埋藏模式。这种方法实现了海底埋藏的可扩展、非侵入式测绘，并支持对受污染场地的环境评估。", "summary": "本文提出了一种名为PoseIDON的计算机视觉管道，旨在解决海底人为物体埋藏深度估计的难题。该方法通过结合深度基础模型特征和多视图摄影测量技术，从ROV视频中精确估计物体的六自由度姿态和周围海底的方位。通过将CAD模型与观测图像对齐并拟合局部海底平面，系统能够推断出埋藏深度。实验结果表明，该模型在实际海洋倾倒场数据上实现了约10厘米的平均埋藏深度误差，并能有效揭示与沉积物输运相关的空间埋藏模式。此方法为海底埋藏物的非侵入式、可扩展测绘提供了有效工具，有助于污染场地的环境评估。", "keywords": "海底埋藏, 六自由度姿态, 计算机视觉, 深度学习, 海洋测绘", "comments": "这项工作通过结合前沿的深度基础模型和多视图摄影测量技术，为水下物体埋藏深度估计提供了一个创新的解决方案。其亮点在于能够从低质量的ROV视频中推断出精确的六自由度姿态和埋藏深度，并且在实际场景中验证了其有效性（平均误差10厘米）。这种非侵入式、可扩展的测绘方法对于海洋环境监测和污染场地管理具有重要意义。"}}
{"id": "2506.10776", "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement", "authors": ["Feiyu Yang", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "summary": "The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10776v1", "AI": {"title_translation": "ME：触发元素组合后门攻击在版权侵权中的应用", "tldr": "本文提出了ME攻击方法，通过增加中毒样本中的视觉-文本元素数量并引入DCT来增强对生成扩散模型的版权侵权攻击能力，并在新数据集上取得了优于SBD的攻击效果，尤其在低采样率下表现更佳。", "motivation": "现有的针对生成扩散模型（如Stable Diffusion）的版权侵权攻击方法（如SilentBadDiffusion, SBD）存在数据资源有限、数据集适用性差以及在少量中毒样本下攻击性能不佳等问题，促使研究者寻找更有效、更隐蔽的攻击方法。", "method": "本文提出了Multi-Element (ME) 攻击方法，该方法基于SBD，通过增加每个中毒样本中的中毒视觉-文本元素数量来增强攻击能力。同时，为了保持隐蔽性，引入了离散余弦变换（DCT）对中毒样本进行处理。此外，研究者还创建了新的数据集以供相关研究使用。", "result": "在两个新数据集上，ME攻击方法的版权侵权率（CIR）/首次攻击周期（FAE）分别为16.78% / 39.50和51.20% / 23.60，接近或优于基准的Pokemon和Mijourney数据集。在低采样率（5%中毒样本，即6个中毒样本）条件下，MESI和DCT的CIR / FAE分别为0.23% / 84.00和12.73% / 65.50，均优于完全无法攻击的原始SBD。", "conclusion": "本文提出的ME攻击方法有效解决了现有版权侵权攻击方法在数据资源和攻击效率方面的局限性，尤其在低中毒样本率下表现出显著的攻击能力提升和隐蔽性，证明了其在增强对生成扩散模型的版权侵权攻击方面的有效性。", "translation": "生成扩散模型（DMs）如Stable Diffusion（SD）复制训练数据的能力可能被攻击者利用，通过复制中毒的图像-文本对来发起版权侵权攻击。SilentBadDiffusion（SBD）是最近提出的一种方法，在文本到图像任务中攻击SD表现出色。然而，该领域可用的数据资源仍然有限，其中一些由于版权所有权或不当内容等问题甚至受到限制或禁止；并且当前数据集中并非所有图像都适用于所提出的攻击方法；此外，当少量生成的投毒样本可用于攻击时，SBD的最新（SoTA）性能远非理想。在本文中，我们提出了可用于研究SBD等攻击的新数据集，并基于SBD提出了Multi-Element（ME）攻击方法，通过增加每个投毒样本中投毒视觉-文本元素的数量来增强攻击能力，同时为投毒样本引入离散余弦变换（DCT）以保持隐蔽性。我们在两个新数据集上获得的版权侵权率（CIR）/首次攻击周期（FAE）分别为16.78% / 39.50和51.20% / 23.60，分别接近或甚至超越了基准Pokemon和Mijourney数据集。在低采样率（5%，6个投毒样本）条件下，MESI和DCT的CIR / FAE分别为0.23% / 84.00和12.73% / 65.50，均优于完全无法攻击的原始SBD。", "summary": "本文针对生成扩散模型（如Stable Diffusion）面临的版权侵权攻击问题，指出现有方法（如SBD）在数据资源和低样本量攻击性能上的不足。为此，作者提出了Multi-Element (ME) 攻击方法，通过增加中毒样本中的视觉-文本元素数量来增强攻击效果，并引入离散余弦变换（DCT）以保持隐蔽性。实验结果表明，ME方法在新建数据集上取得了优于或接近基准数据集的攻击性能，尤其在极低中毒样本量（如5%采样率）条件下，ME方法显著提升了攻击成功率，而SBD则完全失效。", "keywords": "版权侵权攻击, 生成扩散模型, 后门攻击, Multi-Element, 离散余弦变换", "comments": "本文的创新点在于提出了Multi-Element (ME) 攻击方法，通过增加中毒样本中的元素数量来增强攻击能力，并引入DCT来保持攻击的隐蔽性，有效解决了现有版权侵权攻击方法在数据资源和效率方面的局限性。其重要性在于揭示了生成扩散模型潜在的版权侵权风险，并提供了一种更高效、更隐蔽的攻击手段，对模型安全性和版权保护领域具有警示意义。尤其在低中毒样本量下的显著提升，展现了该方法的实用性和威胁性。"}}
{"id": "2506.10002", "title": "EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis", "authors": ["Jianwu Fang", "Lei-Lei Li", "Zhedong Zheng", "Hongkai Yu", "Jianru Xue", "Zhengguo Li", "Tat-Seng Chua"], "summary": "Traffic Accident Anticipation (TAA) in traffic scenes is a challenging\nproblem for achieving zero fatalities in the future. Current approaches\ntypically treat TAA as a supervised learning task needing the laborious\nannotation of accident occurrence duration. However, the inherent long-tailed,\nuncertain, and fast-evolving nature of traffic scenes has the problem that real\ncausal parts of accidents are difficult to identify and are easily dominated by\ndata bias, resulting in a background confounding issue. Thus, we propose an\nAttentive Video Diffusion (AVD) model that synthesizes additional accident\nvideo clips by generating the causal part in dashcam videos, i.e., from normal\nclips to accident clips. AVD aims to generate causal video frames based on\naccident or accident-free text prompts while preserving the style and content\nof frames for TAA after video generation. This approach can be trained using\ndatasets collected from various driving scenes without any extra annotations.\nAdditionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant\ntriple loss for an anchor accident-free video clip, along with the generated\npair of contrastive pseudo-normal and pseudo-accident clips. Extensive\nexperiments have been conducted to evaluate the performance of AVD and EQ-TAA,\nand competitive performance compared to state-of-the-art methods has been\nobtained.", "comment": "Accepted by IEEE-TMM", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10002v1", "AI": {"title_translation": "EQ-TAA：基于扩散的事故视频合成的等变交通事故预测", "tldr": "本文提出EQ-TAA，通过Attentive Video Diffusion (AVD) 模型合成事故视频片段，以解决交通事故预测中数据标注困难和数据偏见问题，并取得了具有竞争力的性能。", "motivation": "交通场景中的交通事故预测（TAA）是一个具有挑战性的问题，旨在实现未来零死亡。当前方法通常将TAA视为监督学习任务，需要对事故发生持续时间进行费力的标注。然而，交通场景固有的长尾、不确定和快速演变特性使得事故的真实因果部分难以识别，并且容易受到数据偏见的影响，导致背景混淆问题。", "method": "我们提出了一种Attentive Video Diffusion (AVD) 模型，通过在行车记录仪视频中生成因果部分（即从正常片段到事故片段）来合成额外的事故视频片段。AVD旨在根据事故或无事故文本提示生成因果视频帧，同时在视频生成后保留TAA的帧风格和内容。这种方法可以使用从各种驾驶场景收集的数据集进行训练，无需任何额外标注。此外，AVD通过等变三重损失促进了Equivariant TAA (EQ-TAA)，该损失针对一个锚定无事故视频片段以及生成的一对对比伪正常和伪事故片段。", "result": "对AVD和EQ-TAA的性能进行了广泛的实验评估，并取得了与最先进方法相比具有竞争力的表现。", "conclusion": "本文提出的AVD模型能够有效地合成交通事故视频，解决了数据稀缺和标注困难的问题。在此基础上，EQ-TAA结合等变三重损失，进一步提升了交通事故预测的性能，并在实验中取得了与现有SOTA方法相当的竞争性表现，为未来实现零死亡目标提供了新的途径。", "translation": "交通事故预测（TAA）在交通场景中是一个具有挑战性的问题，旨在未来实现零死亡。当前方法通常将TAA视为监督学习任务，需要对事故发生持续时间进行费力的标注。然而，交通场景固有的长尾、不确定和快速演变特性使得事故的真实因果部分难以识别，并且容易受到数据偏见的影响，导致背景混淆问题。因此，我们提出了一种Attentive Video Diffusion (AVD) 模型，通过在行车记录仪视频中生成因果部分（即从正常片段到事故片段）来合成额外的事故视频片段。AVD旨在根据事故或无事故文本提示生成因果视频帧，同时在视频生成后保留TAA的帧风格和内容。这种方法可以使用从各种驾驶场景收集的数据集进行训练，无需任何额外标注。此外，AVD通过等变三重损失促进了Equivariant TAA (EQ-TAA)，该损失针对一个锚定无事故视频片段以及生成的一对对比伪正常和伪事故片段。对AVD和EQ-TAA的性能进行了广泛的实验评估，并取得了与最先进方法相比具有竞争力的表现。", "summary": "本文针对交通事故预测（TAA）中数据标注困难和数据偏见问题，提出了一种名为Attentive Video Diffusion (AVD) 的模型。AVD通过从正常视频片段生成事故视频片段，合成额外的事故视频数据，从而无需人工标注即可训练。该模型能够根据文本提示生成因果视频帧，并保持原始帧的风格和内容。在此基础上，本文进一步提出了Equivariant TAA (EQ-TAA)，利用等变三重损失，结合锚定无事故视频和生成的伪正常/伪事故片段对进行对比学习。实验结果表明，AVD和EQ-TAA在交通事故预测任务上取得了与现有最先进方法相当的竞争性性能。", "keywords": "交通事故预测, 扩散模型, 视频合成, 等变学习, 数据增强", "comments": "该论文创新性地将扩散模型应用于交通事故预测领域，通过合成因果事故视频解决了该领域数据稀缺和标注困难的痛点。其无需额外标注的训练方式显著降低了数据准备成本。此外，等变三重损失的设计有效应对了交通场景的复杂性和数据偏见问题，提升了模型的鲁棒性。这为交通事故预测提供了一种新颖且高效的解决方案，具有重要的实际应用价值。"}}
{"id": "2506.10244", "title": "A new type of federated clustering: A non-model-sharing approach", "authors": ["Yuji Kawamata", "Kaoru Kamijo", "Maki Kihira", "Akihiro Toyoda", "Tomoru Nakayama", "Akira Imakura", "Tetsuya Sakurai", "Yukihiko Okada"], "summary": "In recent years, the growing need to leverage sensitive data across\ninstitutions has led to increased attention on federated learning (FL), a\ndecentralized machine learning paradigm that enables model training without\nsharing raw data. However, existing FL-based clustering methods, known as\nfederated clustering, typically assume simple data partitioning scenarios such\nas horizontal or vertical splits, and cannot handle more complex distributed\nstructures. This study proposes data collaboration clustering (DC-Clustering),\na novel federated clustering method that supports clustering over complex data\npartitioning scenarios where horizontal and vertical splits coexist. In\nDC-Clustering, each institution shares only intermediate representations\ninstead of raw data, ensuring privacy preservation while enabling collaborative\nclustering. The method allows flexible selection between k-means and spectral\nclustering, and achieves final results with a single round of communication\nwith the central server. We conducted extensive experiments using synthetic and\nopen benchmark datasets. The results show that our method achieves clustering\nperformance comparable to centralized clustering where all data are pooled.\nDC-Clustering addresses an important gap in current FL research by enabling\neffective knowledge discovery from distributed heterogeneous data. Its\npractical properties -- privacy preservation, communication efficiency, and\nflexibility -- make it a promising tool for privacy-sensitive domains such as\nhealthcare and finance.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10244v1", "AI": {"title_translation": "一种新型联邦聚类：非模型共享方法", "tldr": "提出了一种名为DC-Clustering的新型联邦聚类方法，它支持复杂的数据划分场景，通过共享中间表示而非原始数据实现隐私保护和高效协作聚类，性能可媲美集中式聚类。", "motivation": "现有的联邦学习（FL）聚类方法通常假设简单的数据划分场景（如水平或垂直分割），无法处理更复杂的分布式数据结构，例如水平和垂直分割并存的情况。", "method": "本研究提出数据协作聚类（DC-Clustering），这是一种新型联邦聚类方法，支持在水平和垂直分割共存的复杂数据划分场景下进行聚类。DC-Clustering中，每个机构只共享中间表示而非原始数据，以确保隐私保护。该方法允许在k-means和谱聚类之间灵活选择，并通过与中央服务器的单轮通信即可获得最终结果。", "result": "使用合成数据集和开放基准数据集进行了广泛实验，结果表明该方法实现的聚类性能与所有数据汇集在一起的集中式聚类相当。", "conclusion": "DC-Clustering通过实现对分布式异构数据的有效知识发现，弥补了当前联邦学习研究中的一个重要空白。其隐私保护、通信效率和灵活性等实用特性使其成为医疗保健和金融等隐私敏感领域的一个有前景的工具。", "translation": "近年来，利用跨机构敏感数据的日益增长的需求，使得联邦学习（FL）受到越来越多的关注。联邦学习是一种去中心化的机器学习范式，它可以在不共享原始数据的情况下进行模型训练。然而，现有的基于FL的聚类方法，即联邦聚类，通常假设简单的数据划分场景，如水平或垂直分割，并且无法处理更复杂的分布式结构。本研究提出数据协作聚类（DC-Clustering），这是一种新型联邦聚类方法，支持在水平和垂直分割并存的复杂数据划分场景下进行聚类。在DC-Clustering中，每个机构只共享中间表示而不是原始数据，确保了隐私保护，同时实现了协作聚类。该方法允许在k-means和谱聚类之间灵活选择，并通过与中央服务器的单轮通信即可获得最终结果。我们使用合成数据集和开放基准数据集进行了广泛的实验。结果表明，我们的方法实现的聚类性能与所有数据汇集在一起的集中式聚类相当。DC-Clustering通过实现从分布式异构数据中有效发现知识，解决了当前FL研究中的一个重要空白。其隐私保护、通信效率和灵活性等实用特性使其成为医疗保健和金融等隐私敏感领域的一个有前景的工具。", "summary": "该论文提出了一种名为数据协作聚类（DC-Clustering）的新型联邦聚类方法，旨在解决现有联邦聚类方法无法处理复杂数据划分场景（如水平与垂直分割共存）的问题。DC-Clustering通过仅共享中间表示而非原始数据来保护隐私，并允许在k-means和谱聚类之间灵活选择，只需单轮通信即可完成聚类。实验证明，其性能与集中式聚类相当，为隐私敏感领域分布式异构数据的知识发现提供了高效且实用的解决方案。", "keywords": "联邦聚类, 数据协作聚类, 隐私保护, 复杂数据划分, 中间表示", "comments": "该论文的创新点在于提出了DC-Clustering，这是一种支持复杂数据划分（水平和垂直分割共存）的联邦聚类方法，这在现有联邦聚类研究中是一个重要的进步。其非模型共享的特性，通过共享中间表示而非原始数据，显著增强了隐私保护。此外，单轮通信的效率和对多种聚类算法的兼容性也增加了其实用价值，特别适用于医疗和金融等对隐私要求极高的领域。"}}
{"id": "2506.10614", "title": "Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search", "authors": ["Promise Dodzi Kpoglu"], "summary": "We propose an unsupervised method for the reconstruction of protoforms i.e.,\nancestral word forms from which modern language forms are derived. While prior\nwork has primarily relied on probabilistic models of phonological edits to\ninfer protoforms from cognate sets, such approaches are limited by their\npredominantly data-driven nature. In contrast, our model integrates data-driven\ninference with rule-based heuristics within an evolutionary optimization\nframework. This hybrid approach leverages on both statistical patterns and\nlinguistically motivated constraints to guide the reconstruction process. We\nevaluate our method on the task of reconstructing Latin protoforms using a\ndataset of cognates from five Romance languages. Experimental results\ndemonstrate substantial improvements over established baselines across both\ncharacter-level accuracy and phonological plausibility metrics.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10614v1", "AI": {"title_translation": "无监督原始词形重建：基于简约规则引导启发式和演化搜索", "tldr": "本文提出一种无监督方法，结合数据驱动推理和规则启发式，通过演化优化框架重建原始词形，并在拉丁语重建任务上表现出显著提升。", "motivation": "现有方法主要依赖于语音编辑的概率模型，但其数据驱动性质限制了其应用。因此，需要一种结合数据驱动推理和语言学约束的方法来改进原始词形重建。", "method": "该方法是一种无监督的混合方法，它将数据驱动推理与基于规则的启发式方法集成到演化优化框架中。它利用统计模式和语言学约束来指导重建过程。", "result": "在重建拉丁语原始词形的任务中，该方法在字符级准确性和语音合理性指标上都显著优于现有基线。", "conclusion": "该研究表明，结合数据驱动推理和规则启发式的演化优化框架能够有效且显著地改进原始词形重建的性能。", "translation": "我们提出了一种无监督方法，用于重建原始词形，即现代语言形式所源自的祖先词形。虽然先前的研究主要依赖于语音编辑的概率模型来从同源词集推断原始词形，但这些方法受限于其主要由数据驱动的性质。相比之下，我们的模型将数据驱动推理与基于规则的启发式方法集成到演化优化框架中。这种混合方法利用统计模式和语言学驱动的约束来指导重建过程。我们使用来自五种罗曼语的同源词数据集，在重建拉丁语原始词形的任务上评估了我们的方法。实验结果表明，在字符级准确性和语音合理性指标上，该方法均比现有基线有显著改进。", "summary": "本文提出了一种创新的无监督原始词形重建方法，该方法通过将数据驱动推理与基于规则的启发式方法融合到演化优化框架中，克服了以往纯数据驱动模型的局限性。通过在拉丁语原始词形重建任务上进行评估，结果显示其在准确性和语音合理性方面均显著优于现有基线。", "keywords": "原始词形重建, 无监督学习, 演化优化, 规则启发式, 计算语言学", "comments": "该论文的创新之处在于其混合方法，结合了数据驱动的统计推断和语言学规则引导的启发式，并通过演化搜索进行优化。这克服了纯数据驱动模型的局限性，为原始词形重建提供了一个更鲁棒和语言学上更合理的框架。其在拉丁语重建上的显著提升证明了该方法的有效性。"}}
{"id": "2506.10390", "title": "DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba", "authors": ["Shicheng Yin", "Kaixuan Yin", "Yang Liu", "Weixing Chen", "Liang Lin"], "summary": "Recently, non-convolutional models such as the Vision Transformer (ViT) and\nVision Mamba (Vim) have achieved remarkable performance in computer vision\ntasks. However, their reliance on fixed-size patches often results in excessive\nencoding of background regions and omission of critical local details,\nespecially when informative objects are sparsely distributed. To address this,\nwe introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART),\nwhich adaptively partitions images into content-dependent patches of varying\nsizes. DART combines learnable region scores with piecewise differentiable\nquantile operations to allocate denser tokens to information-rich areas.\nDespite introducing only approximately 1 million (1M) additional parameters,\nDART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that\nuniformly increase token density to capture fine-grained details, DART offers a\nmore efficient alternative, achieving 45% FLOPs reduction with superior\nperformance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that\nDART consistently enhances accuracy while incurring minimal or even reduced\ncomputational overhead. Code is available at\nhttps://github.com/HCPLab-SYSU/DART.", "comment": "Code is available at https://github.com/HCPLab-SYSU/DART", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10390v1", "AI": {"title_translation": "DART：用于视觉Transformer和Mamba的可微分动态自适应区域分词器", "tldr": "DART是一种新的可微分自适应区域分词器，它能根据图像内容动态调整分词大小，有效解决了现有视觉模型中固定大小分块导致的问题，提高了准确性并降低了计算成本。", "motivation": "现有的视觉Transformer和Mamba模型依赖固定大小的分块，这导致背景区域编码过多，并遗漏了关键的局部细节，尤其是在信息丰富的对象稀疏分布时。", "method": "本文引入了一种完全可微分的动态自适应区域分词器（DART），它能够自适应地将图像划分为内容依赖的、大小可变的图像块。DART结合了可学习的区域分数和分段可微分的分位数操作，将更密集的token分配给信息丰富的区域。", "result": "DART仅增加了约100万额外参数，却在DeiT（ImageNet-1K）上将准确率提高了2.1%。与那些统一增加token密度以捕获细粒度细节的方法不同，DART提供了一种更高效的替代方案，在性能更优的同时实现了45%的FLOPs降低。在DeiT、Vim和VideoMamba上的广泛实验证实，DART在提高准确性的同时，计算开销极小甚至有所降低。", "conclusion": "DART通过动态自适应区域分词，有效解决了现有视觉模型固定分块的缺陷，显著提升了模型性能并优化了计算效率，是视觉Transformer和Mamba的有效改进方案。", "translation": "近年来，视觉Transformer（ViT）和视觉Mamba（Vim）等非卷积模型在计算机视觉任务中取得了显著性能。然而，它们对固定大小图像块的依赖常常导致背景区域的过度编码以及关键局部细节的遗漏，尤其是在信息丰富的对象稀疏分布时。为解决此问题，我们引入了一种完全可微分的动态自适应区域分词器（DART），它能自适应地将图像划分为内容依赖的、大小可变的图像块。DART结合了可学习的区域分数和分段可微分的分位数操作，将更密集的token分配给信息丰富的区域。尽管仅引入了大约100万（1M）额外参数，DART在DeiT（ImageNet-1K）上的准确率提高了2.1%。与那些统一增加token密度以捕获细粒度细节的方法不同，DART提供了一种更高效的替代方案，在性能更优的同时实现了45%的FLOPs降低。在DeiT、Vim和VideoMamba上的广泛实验证实，DART在提高准确性的同时，计算开销极小甚至有所降低。代码可在https://github.com/HCPLab-SYSU/DART获取。", "summary": "本文提出了一种名为DART的可微分动态自适应区域分词器，旨在解决现有视觉Transformer和Mamba模型中固定大小图像块导致的冗余编码和细节丢失问题。DART能够根据图像内容自适应地划分不同大小的区域，并通过将更多token分配给信息密集区域来优化表示。实验证明，DART在仅增加少量参数的情况下，显著提升了DeiT等模型的准确率，并实现了计算效率的提升（例如FLOPs降低45%），验证了其在视觉任务中的有效性和高效性。", "keywords": "DART, 视觉Transformer, Mamba, 自适应分词, 图像表示", "comments": "DART的创新点在于其可微分的动态自适应区域分词机制，这使得模型能够智能地关注图像中的重要区域，避免了固定分块的局限性。这种方法不仅提升了准确性，还显著降低了计算成本，为视觉Transformer和Mamba的效率优化提供了有价值的解决方案。其通用性体现在对多种非卷积模型的适用性。"}}
{"id": "2506.10949", "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors", "authors": ["Chen Yueh-Han", "Nitish Joshi", "Yulin Chen", "Maksym Andriushchenko", "Rico Angell", "He He"], "summary": "Current LLM safety defenses fail under decomposition attacks, where a\nmalicious goal is decomposed into benign subtasks that circumvent refusals. The\nchallenge lies in the existing shallow safety alignment techniques: they only\ndetect harm in the immediate prompt and do not reason about long-range intent,\nleaving them blind to malicious intent that emerges over a sequence of\nseemingly benign instructions. We therefore propose adding an external monitor\nthat observes the conversation at a higher granularity. To facilitate our study\nof monitoring decomposition attacks, we curate the largest and most diverse\ndataset to date, including question-answering, text-to-image, and agentic\ntasks. We verify our datasets by testing them on frontier LLMs and show an 87%\nattack success rate on average on GPT-4o. This confirms that decomposition\nattack is broadly effective. Additionally, we find that random tasks can be\ninjected into the decomposed subtasks to further obfuscate malicious intents.\nTo defend in real time, we propose a lightweight sequential monitoring\nframework that cumulatively evaluates each subtask. We show that a carefully\nprompt engineered lightweight monitor achieves a 93% defense success rate,\nbeating reasoning models like o3 mini as a monitor. Moreover, it remains robust\nagainst random task injection and cuts cost by 90% and latency by 50%. Our\nfindings suggest that lightweight sequential monitors are highly effective in\nmitigating decomposition attacks and are viable in deployment.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10949v1", "AI": {"title_translation": "使用轻量级序列监控器监控大型语言模型中的分解攻击", "tldr": "LLM安全防御在分解攻击下失效，本研究提出一种轻量级序列监控框架，通过累积评估子任务，有效防御分解攻击，并显著降低成本和延迟。", "motivation": "当前LLM的安全防御机制无法抵御分解攻击，因为它们仅能检测即时提示中的危害，而无法推理长程意图，导致对通过一系列看似无害指令实现的恶意意图视而不见。", "method": "1. 提出了一个外部监控器，以更高粒度观察对话。2. 整理了迄今为止最大、最多样化的数据集，包括问答、文本到图像和代理任务，用于研究分解攻击。3. 提出了一个轻量级序列监控框架，通过累积评估每个子任务来实时防御。", "result": "1. 在前沿LLM上测试数据集，显示在GPT-4o上平均攻击成功率为87%，证实分解攻击普遍有效。2. 发现随机任务可以注入分解的子任务中以进一步混淆恶意意图。3. 精心设计的轻量级监控器实现了93%的防御成功率，优于o3 mini等推理模型。4. 对随机任务注入保持鲁棒性，并将成本降低90%，延迟降低50%。", "conclusion": "轻量级序列监控器在缓解分解攻击方面非常有效，并且在部署中是可行的。", "translation": "当前大型语言模型（LLM）的安全防御在分解攻击下失效，在这种攻击中，恶意目标被分解为看似良性的子任务，从而规避了拒绝机制。挑战在于现有的浅层安全对齐技术：它们只检测即时提示中的危害，而无法推理长程意图，导致它们对通过一系列看似良性指令出现的恶意意图视而不见。因此，我们建议添加一个外部监控器，以更高的粒度观察对话。为了促进我们对分解攻击的研宄，我们策划了迄今为止最大、最多样化的数据集，包括问答、文本到图像和代理任务。我们通过在前沿LLM上测试来验证我们的数据集，并显示在GPT-4o上平均攻击成功率为87%。这证实了分解攻击普遍有效。此外，我们发现随机任务可以注入分解的子任务中，以进一步混淆恶意意图。为了实时防御，我们提出了一种轻量级序列监控框架，该框架累积评估每个子任务。我们表明，一个经过精心提示工程的轻量级监控器实现了93%的防御成功率，击败了像o3 mini这样的推理模型作为监控器。此外，它对随机任务注入保持鲁棒性，并将成本降低90%，延迟降低50%。我们的发现表明，轻量级序列监控器在缓解分解攻击方面非常有效，并且在部署中是可行的。", "summary": "本研究针对大型语言模型（LLM）在分解攻击下现有安全防御失效的问题，提出了一种外部轻量级序列监控框架。该框架通过累积评估对话中的每个子任务，能够检测并防御恶意意图被分解为看似良性子任务以规避安全机制的攻击。研究团队构建了迄今为止最大的分解攻击数据集，并在前沿LLM上验证了分解攻击的有效性（GPT-4o上87%的成功率）。实验结果表明，该轻量级监控器实现了93%的防御成功率，优于其他推理模型，同时显著降低了成本和延迟，并对随机任务注入保持鲁棒性，证明了其在实际部署中的可行性。", "keywords": "分解攻击, LLM安全, 序列监控, 轻量级监控, 安全防御", "comments": "这项研究的创新之处在于提出了一个外部的、轻量级且序列化的监控方法来解决LLM分解攻击这一新兴且难以防御的安全漏洞。它突破了现有浅层安全对齐技术仅关注即时提示的局限性，通过累积评估实现对长程恶意意图的识别。其提出的数据集对于后续研究具有重要价值。此外，该方法在防御效果、成本和延迟方面的显著优化，使其在实际部署中具有高度可行性和实用性。"}}
{"id": "2506.10259", "title": "Meta-learning Representations for Learning from Multiple Annotators", "authors": ["Atsutoshi Kumagai", "Tomoharu Iwata", "Taishi Nishiyama", "Yasutoshi Ida", "Yasuhiro Fujiwara"], "summary": "We propose a meta-learning method for learning from multiple noisy\nannotators. In many applications such as crowdsourcing services, labels for\nsupervised learning are given by multiple annotators. Since the annotators have\ndifferent skills or biases, given labels can be noisy. To learn accurate\nclassifiers, existing methods require many noisy annotated data. However,\nsufficient data might be unavailable in practice. To overcome the lack of data,\nthe proposed method uses labeled data obtained in different but related tasks.\nThe proposed method embeds each example in tasks to a latent space by using a\nneural network and constructs a probabilistic model for learning a\ntask-specific classifier while estimating annotators' abilities on the latent\nspace. This neural network is meta-learned to improve the expected test\nclassification performance when the classifier is adapted to a given small\namount of annotated data. This classifier adaptation is performed by maximizing\nthe posterior probability via the expectation-maximization (EM) algorithm.\nSince each step in the EM algorithm is easily computed as a closed-form and is\ndifferentiable, the proposed method can efficiently backpropagate the loss\nthrough the EM algorithm to meta-learn the neural network. We show the\neffectiveness of our method with real-world datasets with synthetic noise and\nreal-world crowdsourcing datasets.", "comment": "24 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10259v1", "AI": {"title_translation": "元学习表示用于从多标注者学习", "tldr": "本文提出了一种元学习方法，用于从多位噪声标注者提供的有限数据中学习准确的分类器，通过利用相关任务和基于EM的适应过程。", "motivation": "在许多应用中，如众包服务，监督学习的标签由多位标注者提供，但由于标注者技能和偏见不同，标签可能存在噪声。现有方法需要大量带噪声的标注数据来学习准确分类器，但在实践中数据可能不足。", "method": "本文提出一种元学习方法，利用不同但相关的任务中获得的标注数据。该方法使用神经网络将任务中的每个示例嵌入到潜在空间中，并构建一个概率模型，用于学习特定任务的分类器，同时在潜在空间中估计标注者的能力。该神经网络通过元学习来提高当分类器适应少量标注数据时的预期测试分类性能。分类器适应通过期望最大化（EM）算法最大化后验概率来执行。由于EM算法中的每一步都易于计算为闭合形式且可微分，因此该方法可以有效地通过EM算法反向传播损失以元学习神经网络。", "result": "该方法在具有合成噪声的真实世界数据集和真实世界众包数据集上显示出有效性。", "conclusion": "本文提出的元学习方法通过利用相关任务和高效的基于EM的适应过程，有效解决了从有限、有噪声的多标注者数据中学习的挑战。", "translation": "我们提出了一种元学习方法，用于从多个有噪声的标注者那里学习。在许多应用中，例如众包服务，监督学习的标签由多个标注者提供。由于标注者具有不同的技能或偏见，给定的标签可能带有噪声。为了学习准确的分类器，现有方法需要大量带有噪声的标注数据。然而，在实践中可能无法获得足够的数据。为了克服数据不足的问题，所提出的方法使用了在不同但相关任务中获得的标注数据。所提出的方法通过使用神经网络将任务中的每个示例嵌入到潜在空间中，并构建一个概率模型，用于学习任务特定分类器，同时在潜在空间中估计标注者的能力。这个神经网络经过元学习，以提高当分类器适应给定少量标注数据时的预期测试分类性能。这种分类器适应通过期望最大化（EM）算法最大化后验概率来执行。由于EM算法中的每一步都可以很容易地计算为闭合形式且可微分，因此所提出的方法可以有效地通过EM算法反向传播损失以元学习神经网络。我们通过带有合成噪声的真实世界数据集和真实世界众包数据集展示了我们方法的有效性。", "summary": "本文提出了一种元学习方法，用于从多位噪声标注者提供的有限数据中学习。针对现有方法需要大量数据而实际数据不足的问题，该方法利用相关任务的数据。它通过元学习的神经网络将样本嵌入潜在空间，并构建概率模型来学习任务特定分类器并估计标注者能力。分类器适应通过可微分的EM算法实现，使得损失能高效反向传播以优化神经网络。该方法在合成噪声和真实众包数据集上均显示出有效性。", "keywords": "元学习, 多标注者, 噪声标签, 众包, 期望最大化", "comments": "该论文的创新之处在于将元学习应用于从噪声标注者数据中学习的问题，特别通过利用相关任务来解决数据稀缺性。使用可微分的EM算法进行高效的反向传播也是一个关键的技术贡献。"}}
{"id": "2506.10622", "title": "SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Petr Motlicek"], "summary": "The advancement of conversational AI systems relies on the availability of\nhigh-quality, flexible, and reproducible synthetic dialogues for training,\nevaluation, and benchmarking. SDialog is a modular, extensible Python toolkit\ndesigned to address the challenges of synthetic dialogue generation and\nanalysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog\nprovides abstractions for personas, orchestration, and scenario management,\nenabling the creation of realistic, diverse, and controllable conversational\ndata for research and development. SDialog supports workflows such as\nmulti-agent simulation and scenario-driven generation, and represents a step\nforward in the standardization of tools and frameworks for synthetic data\ngeneration, a crucial advancement for ensuring reproducibility in today's\nfast-evolving research landscape.", "comment": "https://github.com/idiap/sdialog", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10622v1", "AI": {"title_translation": "SDialog：一个用于合成对话生成和分析的Python工具包", "tldr": "SDialog是一个Python工具包，利用指令微调的大型语言模型生成高质量、可控的合成对话，以支持会话式AI系统的训练、评估和基准测试，并促进数据生成的标准化和可复现性。", "motivation": "会话式AI系统的发展依赖于高质量、灵活且可复现的合成对话数据，用于训练、评估和基准测试。然而，合成对话的生成和分析面临挑战。", "method": "SDialog是一个模块化、可扩展的Python工具包，通过利用指令微调的大型语言模型（LLMs），提供角色（personas）、编排（orchestration）和场景管理（scenario management）的抽象，以生成逼真、多样和可控的对话数据。它支持多智能体模拟和场景驱动生成等工作流程。", "result": "SDialog能够创建逼真、多样且可控的会话数据，支持多智能体模拟和场景驱动生成等工作流程。它代表了合成数据生成工具和框架标准化的进步。", "conclusion": "SDialog是合成对话生成和分析领域的重要一步，通过标准化工具和框架，为确保当今快速发展的研究领域中的可复现性做出了关键贡献。", "translation": "会话式AI系统的进步依赖于高质量、灵活且可复现的合成对话，用于训练、评估和基准测试。SDialog是一个模块化、可扩展的Python工具包，旨在解决合成对话生成和分析的挑战。通过利用指令微调的大型语言模型（LLMs），SDialog为角色、编排和场景管理提供了抽象，从而能够为研究和开发创建逼真、多样且可控的会话数据。SDialog支持多智能体模拟和场景驱动生成等工作流程，代表了合成数据生成工具和框架标准化方面的进步，这是确保当今快速发展的研究领域中可复现性的关键进展。", "summary": "SDialog是一个Python工具包，旨在解决合成对话生成和分析的挑战。它利用指令微调的大型语言模型，提供抽象层以管理角色、编排和场景，从而生成逼真、多样且可控的合成对话数据。该工具包支持多智能体模拟和场景驱动生成等工作流程，并有助于推动合成数据生成工具和框架的标准化，对于提高会话式AI研究的可复现性至关重要。", "keywords": "合成对话, Python工具包, 大型语言模型, 会话式AI, 数据生成", "comments": "SDialog通过利用LLMs来生成合成对话，解决了高质量、可复现数据稀缺的问题，这对于会话式AI的发展至关重要。其创新点在于提供角色、编排和场景管理的抽象，使得生成的数据更具真实性和可控性。此外，它对标准化和可复现性的强调，在当前AI研究快速迭代的环境中，具有重要的实践意义。"}}
{"id": "2506.10391", "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion", "authors": ["Yuanyi Song", "Pumeng Lyu", "Ben Fei", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "summary": "Accurate reconstruction of ocean is essential for reflecting global climate\ndynamics and supporting marine meteorological research. Conventional methods\nface challenges due to sparse data, algorithmic complexity, and high\ncomputational costs, while increasing usage of machine learning (ML) method\nremains limited to reconstruction problems at the sea surface and local\nregions, struggling with issues like cloud occlusion. To address these\nlimitations, this paper proposes ReconMOST, a data-driven guided diffusion\nmodel framework for multi-layer sea temperature reconstruction. Specifically,\nwe first pre-train an unconditional diffusion model using a large collection of\nhistorical numerical simulation data, enabling the model to attain physically\nconsistent distribution patterns of ocean temperature fields. During the\ngeneration phase, sparse yet high-accuracy in-situ observational data are\nutilized as guidance points for the reverse diffusion process, generating\naccurate reconstruction results. Importantly, in regions lacking direct\nobservational data, the physically consistent spatial distribution patterns\nlearned during pre-training enable implicitly guided and physically plausible\nreconstructions. Our method extends ML-based SST reconstruction to a global,\nmulti-layer setting, handling over 92.5% missing data while maintaining\nreconstruction accuracy, spatial resolution, and superior generalization\ncapability. We pre-train our model on CMIP6 numerical simulation data and\nconduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The\nresults of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on\nreconstruction, and 0.633 on total, respectively, demonstrating the\neffectiveness and robustness of the proposed framework. Our source code is\navailable at https://github.com/norsheep/ReconMOST.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10391v1", "AI": {"title_translation": "ReconMOST：基于观测引导扩散的多层海温重建", "tldr": "ReconMOST提出一种观测引导的扩散模型，用于多层海温重建，解决了稀疏数据和传统ML方法局限性，实现了高精度、物理一致的全球多层重建。", "motivation": "准确的海洋重建对反映全球气候动态和支持海洋气象研究至关重要。传统方法面临数据稀疏、算法复杂、计算成本高的问题。现有机器学习方法局限于海表和局部区域，且受云遮挡等问题困扰。", "method": "论文提出ReconMOST，一个数据驱动的引导扩散模型框架。首先，使用大量历史数值模拟数据预训练一个无条件扩散模型，使其学习到物理一致的海洋温度场分布模式。在生成阶段，利用稀疏但高精度的原位观测数据作为反向扩散过程的引导点，生成准确的重建结果。在缺乏直接观测数据的区域，预训练学到的物理一致空间分布模式能够隐式引导并实现物理上合理的重建。", "result": "该方法将基于机器学习的海表温度重建扩展到全球、多层设置，在保持重建精度、空间分辨率和优异泛化能力的同时，处理了超过92.5%的缺失数据。模型在CMIP6数值模拟数据上进行预训练，并在CMIP6和EN4分析数据上进行了引导重建实验。均方误差（MSE）值在引导上达到0.049，重建上达到0.680，总计达到0.633，证明了所提框架的有效性和鲁棒性。", "conclusion": "ReconMOST框架能够有效且鲁棒地实现全球多层海温重建，克服了传统方法和现有ML方法的局限性，并处理了大量的缺失数据，同时保持了高精度和物理一致性。", "translation": "准确的海洋重建对于反映全球气候动态和支持海洋气象研究至关重要。传统方法面临数据稀疏、算法复杂和计算成本高昂的挑战，而机器学习（ML）方法的日益使用仍局限于海表和局部区域的重建问题，并受云遮挡等问题困扰。为解决这些局限性，本文提出了ReconMOST，一个数据驱动的引导扩散模型框架，用于多层海温重建。具体而言，我们首先使用大量历史数值模拟数据预训练一个无条件扩散模型，使模型能够获得海洋温度场物理一致的分布模式。在生成阶段，利用稀疏但高精度的原位观测数据作为反向扩散过程的引导点，生成准确的重建结果。重要的是，在缺乏直接观测数据的区域，预训练学到的物理一致空间分布模式能够实现隐式引导和物理上合理的重建。我们的方法将基于机器学习的海表温度重建扩展到全球、多层设置，在保持重建精度、空间分辨率和优异泛化能力的同时，处理了超过92.5%的缺失数据。我们在CMIP6数值模拟数据上预训练了我们的模型，并在CMIP6和EN4分析数据上进行了引导重建实验。均方误差（MSE）值在引导上分别达到0.049，在重建上达到0.680，总计达到0.633，证明了所提框架的有效性和鲁棒性。我们的源代码可在https://github.com/norsheep/ReconMOST 获取。", "summary": "本文提出了ReconMOST，一个基于观测引导扩散的数据驱动模型框架，旨在解决传统方法和现有机器学习方法在海洋温度重建中面临的数据稀疏、计算复杂和区域局限性等挑战。ReconMOST通过预训练无条件扩散模型学习物理一致的温度场分布，并利用稀疏高精度观测数据引导反向扩散过程，实现了全球、多层海温的高精度重建。该方法能够处理超过92.5%的缺失数据，并在实验中展现出优异的重建精度、空间分辨率和泛化能力。", "keywords": "海温重建, 扩散模型, 观测引导, 多层, 缺失数据", "comments": "这篇论文通过引入观测引导的扩散模型，创新性地解决了多层海温重建中数据稀疏和物理一致性难题。其将扩散模型应用于地球科学领域，并结合物理先验知识进行预训练，再通过观测数据进行引导，有效克服了传统ML方法在处理大范围、多层数据时的局限性。处理高达92.5%的缺失数据能力和良好的泛化性是其重要亮点，对全球气候研究和海洋气象预报具有重要意义。"}}
{"id": "2506.10269", "title": "Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification", "authors": ["Ryota Ueda", "Takami Sato", "Ken Kobayashi", "Kazuhide Nakata"], "summary": "Semidefinite programming (SDP) relaxation has emerged as a promising approach\nfor neural network verification, offering tighter bounds than other convex\nrelaxation methods for deep neural networks (DNNs) with ReLU activations.\nHowever, we identify a critical limitation in the SDP relaxation when applied\nto deep networks: interior-point vanishing, which leads to the loss of strict\nfeasibility -- a crucial condition for the numerical stability and optimality\nof SDP. Through rigorous theoretical and empirical analysis, we demonstrate\nthat as the depth of DNNs increases, the strict feasibility is likely to be\nlost, creating a fundamental barrier to scaling SDP-based verification. To\naddress the interior-point vanishing, we design and investigate five solutions\nto enhance the feasibility conditions of the verification problem. Our methods\ncan successfully solve 88% of the problems that could not be solved by existing\nmethods, accounting for 41% of the total. Our analysis also reveals that the\nvalid constraints for the lower and upper bounds for each ReLU unit are\ntraditionally inherited from prior work without solid reasons, but are actually\nnot only unbeneficial but also even harmful to the problem's feasibility. This\nwork provides valuable insights into the fundamental challenges of SDP-based\nDNN verification and offers practical solutions to improve its applicability to\ndeeper neural networks, contributing to the development of more reliable and\nsecure systems with DNNs.", "comment": "17 pages, 2 figures. Version revised after ICML 2025 reviews", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10269v1", "AI": {"title_translation": "神经网络验证中半定松弛的内点消失问题", "tldr": "半定规划 (SDP) 松弛在深度神经网络验证中存在内点消失问题，导致严格可行性丧失，难以扩展。本文提出了五种解决方案，显著提高了问题解决率，并发现传统 ReLU 约束可能有害。", "motivation": "半定规划 (SDP) 松弛是神经网络验证的一种有前景的方法，但当应用于深度网络时，会遇到内点消失问题，导致严格可行性丧失，从而影响数值稳定性和最优性，并成为基于 SDP 的验证方法扩展到更深网络的根本障碍。", "method": "通过严格的理论和实证分析，研究了内点消失问题，并设计和研究了五种解决方案来增强验证问题的可行性条件。此外，还分析了传统上继承的 ReLU 单元上下界约束，发现它们对问题可行性有害。", "result": "所提出的方法成功解决了现有方法无法解决的 88% 的问题（占总数的 41%）。分析还揭示，传统上用于 ReLU 单元的上下界约束不仅无益，甚至对问题的可行性有害。", "conclusion": "本研究深入探讨了基于 SDP 的深度神经网络验证的根本挑战，并提供了实用的解决方案来提高其对更深神经网络的适用性，从而有助于开发更可靠、更安全的深度神经网络系统。", "translation": "半定规划 (SDP) 松弛已成为神经网络验证的一种有前景的方法，与用于 ReLU 激活的深度神经网络 (DNN) 的其他凸松弛方法相比，它提供了更紧密的边界。然而，我们发现 SDP 松弛应用于深度网络时存在一个关键限制：内点消失，这导致严格可行性的丧失——这是 SDP 数值稳定性和最优性的关键条件。通过严格的理论和实证分析，我们证明了随着 DNN 深度增加，严格可行性很可能丧失，这为基于 SDP 的验证的扩展制造了根本障碍。为了解决内点消失问题，我们设计并研究了五种解决方案以增强验证问题的可行性条件。我们的方法成功解决了现有方法无法解决的 88% 的问题，占总数的 41%。我们的分析还揭示，传统上为每个 ReLU 单元设置的上下界有效约束是从先前工作中继承而来，没有充分的理由，但实际上不仅无益，甚至对问题的可行性有害。这项工作为基于 SDP 的 DNN 验证的根本挑战提供了宝贵的见解，并提供了实用的解决方案以提高其对更深神经网络的适用性，从而有助于开发更可靠和安全的 DNN 系统。", "summary": "本研究关注神经网络验证中半定规划 (SDP) 松弛的内点消失问题，该问题导致深度神经网络验证时严格可行性丧失，影响数值稳定性和可扩展性。通过理论和实证分析，作者提出了五种增强可行性的解决方案，成功解决了现有方法无法解决的多数问题。研究还发现，传统的 ReLU 单元上下界约束实际上可能对问题可行性有害。这项工作为 SDP-based DNN 验证提供了新见解和实用改进方案。", "keywords": "神经网络验证, 半定规划, 内点消失, 严格可行性, 深度学习", "comments": "这项工作识别并解决了基于 SDP 的神经网络验证中一个关键的、此前未被充分认识到的“内点消失”问题，这对于该方法的数值稳定性和扩展性至关重要。其创新点在于不仅指出了问题，还提供了具体的解决方案，并且挑战了传统上对 ReLU 单元约束的认识。这项研究对于推动深度神经网络验证的实用性和可靠性具有重要意义。"}}
{"id": "2506.10627", "title": "NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors", "authors": ["Numaan Naeem", "Sarfraz Ahmad", "Momina Ahsan", "Hasan Iqbal"], "summary": "This paper presents our system for Track 1: Mistake Identification in the BEA\n2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The\ntask involves evaluating whether a tutor's response correctly identifies a\nmistake in a student's mathematical reasoning. We explore four approaches: (1)\nan ensemble of machine learning models over pooled token embeddings from\nmultiple pretrained language models (LMs); (2) a frozen sentence-transformer\nusing [CLS] embeddings with an MLP classifier; (3) a history-aware model with\nmulti-head attention between token-level history and response embeddings; and\n(4) a retrieval-augmented few-shot prompting system with a large language model\n(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,\nconstructs structured prompts, and uses schema-guided output parsing to produce\ninterpretable predictions. It outperforms all baselines, demonstrating the\neffectiveness of combining example-driven prompting with LLM reasoning for\npedagogical feedback assessment. Our code is available at\nhttps://github.com/NaumanNaeem/BEA_2025.", "comment": "6 pages, 2 figures, 1 table", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10627v1", "AI": {"title_translation": "NeuralNexus 在 BEA 2025 共享任务中：用于 AI 导师错误识别的检索增强型提示", "tldr": "本文提出了一个用于AI导师错误识别的检索增强型提示系统，该系统在BEA 2025共享任务中表现优于所有基线。", "motivation": "该研究旨在评估AI驱动导师的教学能力，特别是其在识别学生数学推理错误方面的准确性。", "method": "研究探索了四种方法，最终采用了一个基于GPT-4o的检索增强型少样本提示系统。该系统通过检索语义相似的例子、构建结构化提示和使用模式引导的输出解析来生成可解释的预测。", "result": "最终系统超越了所有基线。", "conclusion": "结合示例驱动的提示与大型语言模型推理对于教学反馈评估是有效的。", "translation": "本文介绍了我们为 BEA 2025 AI 驱动导师教学能力评估共享任务中赛道 1：错误识别而开发的系统。该任务涉及评估导师的回答是否正确识别了学生数学推理中的错误。我们探索了四种方法：(1) 对来自多个预训练语言模型 (LM) 的池化 token 嵌入进行机器学习模型集成；(2) 使用 [CLS] 嵌入和 MLP 分类器的冻结句子转换器；(3) 具有 token 级历史和响应嵌入之间多头注意力的历史感知模型；以及 (4) 一个结合大型语言模型 (LLM)，即 GPT 4o 的检索增强型少样本提示系统。我们的最终系统检索语义相似的示例，构建结构化提示，并使用模式引导的输出解析来生成可解释的预测。它超越了所有基线，证明了将示例驱动的提示与 LLM 推理相结合对于教学反馈评估的有效性。我们的代码可在 https://github.com/NaumanNaeem/BEA_2025 获取。", "summary": "本文介绍了 NeuralNexus 团队为 BEA 2025 共享任务中 AI 导师错误识别赛道开发的系统。该系统旨在评估 AI 导师能否正确识别学生数学推理中的错误。研究探索了四种方法，最终采用了一个基于 GPT-4o 的检索增强型少样本提示系统，该系统通过检索相似示例、构建结构化提示和模式引导解析来生成预测。实验结果表明，该系统优于所有基线，证明了将示例驱动提示与 LLM 推理结合在教学反馈评估中的有效性。", "keywords": "AI 导师, 错误识别, 检索增强, 大型语言模型, 教学评估", "comments": "本文的创新点在于结合了检索增强技术和大型语言模型（LLM）的少样本提示能力，用于复杂的教学反馈评估任务。这种方法不仅提高了错误识别的准确性，还通过生成可解释的预测增强了系统的实用性。对于AI教育领域，尤其是在开发更智能、更准确的AI导师方面，这项工作具有重要意义。"}}
{"id": "2506.10395", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "comment": "Unified image understanding and generation model", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10395v1", "AI": {"title_translation": "Pisces：一种用于图像理解和生成的自回归基础模型", "tldr": "Pisces是一个自回归多模态基础模型，通过解耦视觉编码和定制训练，在图像理解和生成任务上均达到有竞争力的性能。", "motivation": "现有统一多模态模型在图像理解和生成任务上的表现通常不如专门模型，主要挑战在于两任务对视觉特征和训练过程的不同需求。", "method": "引入Pisces，一个自回归多模态基础模型，采用新颖的解耦视觉编码架构和为多模态生成优化的定制训练技术。结合精心的数据整理、预训练和微调。", "result": "Pisces在20多个图像理解公开基准测试中表现出色，并在GenEval图像生成基准测试中展现出强大的生成能力，在两项任务上均达到有竞争力的性能。", "conclusion": "图像理解和生成之间存在协同关系，使用单独的视觉编码器是有益的，这推动了统一多模态模型领域的发展。", "translation": "大型语言模型（LLMs）的最新进展使得多模态基础模型能够在统一框架内处理图像理解和生成任务。尽管取得了这些进步，但统一模型在任一任务上的表现往往不如专门模型。开发统一模型的一个关键挑战在于图像理解和生成所需的视觉特征之间存在固有的差异，以及每种模态所需的独特训练过程。\n在这项工作中，我们引入了Pisces，一个自回归多模态基础模型，它通过一种新颖的解耦视觉编码架构和为多模态生成优化的定制训练技术来解决这一挑战。结合精心的数据整理、预训练和微调，Pisces在图像理解和图像生成方面均取得了有竞争力的性能。\n我们在超过20个用于图像理解的公共基准测试中评估了Pisces，它在广泛的任务中表现出强大的性能。此外，在广泛采用的图像生成基准GenEval上，Pisces展现出强大的生成能力。我们广泛的分析揭示了图像理解和生成之间的协同关系，以及使用单独视觉编码器的好处，从而推动了统一多模态模型领域的发展。", "summary": "Pisces是一个自回归多模态基础模型，旨在解决现有统一模型在图像理解和生成方面表现不佳的问题。它通过采用解耦视觉编码架构和定制训练技术，并在大量基准测试中验证，实现了在两类任务上的竞争力表现，并揭示了图像理解与生成之间的协同效应以及独立视觉编码器的优势。", "keywords": "自回归模型, 多模态基础模型, 图像理解, 图像生成, 解耦视觉编码", "comments": "这项工作通过引入解耦视觉编码架构和定制训练技术，有效解决了统一多模态模型在图像理解和生成任务上性能不足的挑战。其创新之处在于认识到两类任务对视觉特征的不同需求，并提出相应的解决方案，这对于推动统一多模态模型的发展具有重要意义。"}}
{"id": "2506.10364", "title": "Can We Infer Confidential Properties of Training Data from LLMs?", "authors": ["Penguin Huang", "Chhavi Yadav", "Ruihan Wu", "Kamalika Chaudhuri"], "summary": "Large language models (LLMs) are increasingly fine-tuned on domain-specific\ndatasets to support applications in fields such as healthcare, finance, and\nlaw. These fine-tuning datasets often have sensitive and confidential\ndataset-level properties -- such as patient demographics or disease prevalence\n-- that are not intended to be revealed. While prior work has studied property\ninference attacks on discriminative models (e.g., image classification models)\nand generative models (e.g., GANs for image data), it remains unclear if such\nattacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark\ntask for evaluating property inference in LLMs under two fine-tuning paradigms:\nquestion-answering and chat-completion. Built on the ChatDoctor dataset, our\nbenchmark includes a range of property types and task configurations. We\nfurther propose two tailored attacks: a prompt-based generation attack and a\nshadow-model attack leveraging word frequency signals. Empirical evaluations\nacross multiple pretrained LLMs show the success of our attacks, revealing a\npreviously unrecognized vulnerability in LLMs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10364v1", "AI": {"title_translation": "我们能否从大型语言模型中推断出训练数据的机密属性？", "tldr": "大型语言模型在敏感数据上微调后，可能泄露训练数据的机密属性。本文介绍了PropInfer基准测试和两种攻击方法，揭示了LLM的这一漏洞。", "motivation": "大型语言模型（LLM）越来越多地在包含敏感和机密数据集级属性的特定领域数据集上进行微调。然而，目前尚不清楚针对判别模型和生成模型的属性推断攻击是否也适用于LLM，这引发了对LLM泄露机密训练数据属性的担忧。", "method": "本文引入了PropInfer，一个用于评估LLM在问答和聊天补全两种微调范式下属性推断的基准任务。该基准基于ChatDoctor数据集构建，涵盖多种属性类型和任务配置。此外，本文提出了两种定制攻击：一种是基于提示的生成攻击，另一种是利用词频信号的影子模型攻击。", "result": "对多个预训练LLM的实证评估表明，本文提出的攻击取得了成功，揭示了LLM中一个以前未被识别的漏洞。", "conclusion": "大型语言模型容易受到属性推断攻击，其训练数据中的机密属性可以被推断出来。", "translation": "大型语言模型（LLM）越来越多地在特定领域的数据集上进行微调，以支持医疗、金融和法律等领域的应用。这些微调数据集通常具有敏感和机密的数据集级属性——例如患者人口统计数据或疾病流行率——这些属性不打算被泄露。虽然先前的工作已经研究了对判别模型（例如图像分类模型）和生成模型（例如图像数据的GAN）的属性推断攻击，但尚不清楚此类攻击是否适用于LLM。在这项工作中，我们引入了PropInfer，一个用于评估LLM在两种微调范式下（问答和聊天补全）属性推断的基准任务。我们的基准基于ChatDoctor数据集构建，包含一系列属性类型和任务配置。我们进一步提出了两种定制的攻击：一种基于提示的生成攻击和一种利用词频信号的影子模型攻击。对多个预训练LLM的实证评估显示了我们攻击的成功，揭示了LLM中一个以前未被识别的漏洞。", "summary": "本文研究了能否从在敏感领域特定数据集上微调的大型语言模型（LLM）中推断出训练数据的机密属性。为此，论文引入了PropInfer，一个基于ChatDoctor数据集构建的基准测试，用于评估问答和聊天补全范式下的属性推断。论文提出了两种新的攻击方法：一种是基于提示的生成攻击，另一种是影子模型攻击。实证结果证明了这些攻击的成功性，揭示了LLM在敏感训练数据属性泄露方面存在的新漏洞。", "keywords": "大型语言模型, 属性推断, 数据隐私, 漏洞, 微调", "comments": "这篇论文解决了LLM在敏感领域部署时一个关键的隐私问题。引入专用基准（PropInfer）和定制攻击是创新且重要的，有助于理解和缓解数据泄露风险。发现“以前未被识别的漏洞”凸显了这项研究的重要性。"}}
{"id": "2506.10006", "title": "HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction", "authors": ["Jie Qin", "Wei Yang", "Yan Su", "Yiran Zhu", "Weizhen Li", "Yunyue Pan", "Chengchang Pan", "Honggang Qi"], "summary": "Current HER2 assessment models for breast cancer predominantly analyze H&E or\nIHC images in isolation,despite clinical reliance on their synergistic\ninterpretation. However, concurrent acquisition of both modalities is often\nhindered by workflow complexity and cost constraints. We propose an adaptive\nbimodal framework enabling flexible single-/dual-modality HER2 prediction\nthrough three innovations: 1) A dynamic branch selector that activates either\nsingle-modality reconstruction or dual-modality joint inference based on input\ncompleteness; 2) A bidirectional cross-modal GAN performing context-aware\nfeature-space reconstruction of missing modalities; 3) A hybrid training\nprotocol integrating adversarial learning and multi-task optimization. This\narchitecture elevates single-modality H&E prediction accuracy from 71.44% to\n94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%\nreliability with sole IHC inputs. The framework's \"dual-preferred,\nsingle-compatible\" design delivers near-bimodal performance without requiring\nsynchronized acquisition, particularly benefiting resource-limited settings\nthrough IHC infrastructure cost reduction. Experimental validation confirms\n22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with\ncross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251\n(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or\nnative fusion pathways, the system mitigates performance degradation from\nmissing data while preserving computational efficiency (78.55% parameter\nreduction in lightweight variant). This elastic architecture demonstrates\nsignificant potential for democratizing precise HER2 assessment across diverse\nhealthcare settings.", "comment": "7 pages,5 figures,3 tables,submitted to the 33rd ACM International\n  Conference on Multimedia(ACM MM 2025)", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10006v1", "AI": {"title_translation": "基于动态双向重建的HER2表达预测灵活多模态输入", "tldr": "本研究提出了一个自适应双模态框架，通过动态分支选择器、双向跨模态GAN和混合训练协议，实现了灵活的单/双模态HER2表达预测，显著提高了预测准确性，尤其在数据不完整的情况下，同时降低了成本。", "motivation": "目前的乳腺癌HER2评估模型主要孤立地分析H&E或IHC图像，尽管临床上依赖它们的协同解释。然而，同时获取这两种模态常因工作流程复杂性和成本限制而受阻。", "method": "提出一个自适应双模态框架，实现灵活的单/双模态HER2预测。该框架包含三项创新：1) 动态分支选择器，根据输入完整性激活单模态重建或双模态联合推断；2) 双向跨模态GAN，执行缺失模态的上下文感知特征空间重建；3) 结合对抗学习和多任务优化的混合训练协议。", "result": "单模态H&E预测准确率从71.44%提升至94.25%，双模态准确率达到95.09%，仅使用IHC输入时仍保持90.28%的可靠性。相较于H&E/IHC基线，准确率分别提高了22.81%/12.90%。跨模态重建将F1分数提升至0.9609（HE到IHC）和0.9251（IHC到HE）。轻量级变体参数减少了78.55%。", "conclusion": "该弹性架构通过动态路由输入，减轻了数据缺失导致的性能下降，同时保持了计算效率，在不同医疗环境中普及精准HER2评估方面显示出巨大潜力。", "translation": "当前用于乳腺癌的HER2评估模型主要孤立地分析H&E或IHC图像，尽管临床上依赖它们的协同解释。然而，同时获取这两种模态常因工作流程复杂性和成本限制而受阻。我们提出了一个自适应双模态框架，通过三项创新实现灵活的单/双模态HER2预测：1) 一个动态分支选择器，根据输入完整性激活单模态重建或双模态联合推断；2) 一个双向跨模态GAN，执行缺失模态的上下文感知特征空间重建；3) 一个整合对抗学习和多任务优化的混合训练协议。该架构将单模态H&E预测准确率从71.44%提升至94.25%，同时实现95.09%的双模态准确率，并保持仅IHC输入时90.28%的可靠性。该框架的“双模态优先，单模态兼容”设计在无需同步采集的情况下提供了接近双模态的性能，尤其通过降低IHC基础设施成本而使资源受限的环境受益。实验验证证实，相较于H&E/IHC基线，准确率分别提高了22.81%/12.90%，跨模态重建将F1分数提升至0.9609（HE到IHC）和0.9251（IHC到HE）。通过动态路由输入通过重建增强或原生融合路径，该系统减轻了数据缺失导致的性能下降，同时保持了计算效率（轻量级变体参数减少78.55%）。这种弹性架构在不同医疗环境中普及精准HER2评估方面显示出巨大潜力。", "summary": "本论文提出了一种名为“动态双向重建”的自适应双模态框架，用于乳腺癌HER2表达预测。该框架解决了现有模型孤立处理H&E和IHC图像的局限性以及双模态数据获取的困难。通过引入动态分支选择器、双向跨模态生成对抗网络（GAN）进行缺失模态重建以及混合训练协议，实现了单模态和双模态输入的灵活预测。实验结果表明，该方法显著提高了单模态H&E预测准确率至94.25%，双模态准确率达到95.09%，并在仅有IHC输入时保持高可靠性，同时有效降低了计算成本和对同步数据采集的需求，有望在资源受限的环境中推广精准HER2评估。", "keywords": "HER2预测, 多模态输入, 动态重建, 跨模态GAN, 乳腺癌", "comments": "该论文的创新点在于其提出的“双模态优先，单模态兼容”设计，通过动态分支选择和跨模态重建，有效解决了临床实践中多模态数据获取不完整的问题，同时显著提升了预测性能。这对于资源有限的医疗环境具有重要意义，能够降低成本并推广精准医疗。其结合GAN进行特征空间重建以及多任务优化的训练策略也值得关注。"}}
{"id": "2506.10282", "title": "Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning", "authors": ["Jiajin Liu", "Dongzhe Fan", "Jiacheng Shen", "Chuanhao Ji", "Daochen Zha", "Qiaoyu Tan"], "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in representing and understanding diverse modalities. However,\nthey typically focus on modality alignment in a pairwise manner while\noverlooking structural relationships across data points. Integrating\nmultimodality with structured graph information (i.e., multimodal graphs, MMGs)\nis essential for real-world applications such as social networks, healthcare,\nand recommendation systems. Existing MMG learning methods fall into three\nparadigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.\nMLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via\nmultimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in\nlanguage or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor\ntreats MLLMs as standalone reasoners with in-context learning or fine-tuning.\nDespite their advances, the MMG field lacks a unified benchmark to fairly\nevaluate across these approaches, making it unclear what progress has been\nmade. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for\nmultimodal graph learning by systematically evaluating these three paradigms\nacross six datasets with different domains. Through extensive experiments, we\nobserve that jointly considering the visual and textual attributes of the nodes\nbenefits graph learning, even when using pre-trained text-to-image alignment\nmodels (e.g., CLIP) as encoders. We also find that converting visual attributes\ninto textual descriptions further improves performance compared to directly\nusing visual inputs. Moreover, we observe that fine-tuning MLLMs on specific\nMMGs can achieve state-of-the-art results in most scenarios, even without\nexplicit graph structure information. We hope that our open-sourced library\nwill facilitate rapid, equitable evaluation and inspire further innovative\nresearch in this field.", "comment": "16 pages, 4 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10282v1", "AI": {"title_translation": "Graph-MLLM：利用多模态大语言模型进行多模态图学习", "tldr": "本文提出了Graph-MLLM，一个用于多模态图学习的综合基准，旨在解决现有MLLM忽略图结构以及MMG领域缺乏统一评估标准的问题。通过系统评估MLLM作为编码器、对齐器和预测器的三种范式，研究发现联合考虑视觉和文本属性、将视觉属性转换为文本描述，以及对MLLM进行微调能有效提升性能。", "motivation": "现有多模态大语言模型（MLLMs）通常侧重于成对模态对齐，忽略了数据点之间的结构关系。将多模态与结构化图信息（多模态图，MMGs）整合对于现实世界应用至关重要。然而，现有MMG学习方法缺乏统一的基准来公平评估不同方法，导致进展不明确。", "method": "本文提出了Graph-MLLM，一个针对多模态图学习的综合基准。该基准系统评估了基于MLLM的三种范式：MLLM-as-Encoder、MLLM-as-Aligner和MLLM-as-Predictor，并在六个不同领域的数据集上进行了广泛实验。", "result": "1. 联合考虑节点的视觉和文本属性有利于图学习，即使使用预训练的文本到图像对齐模型（如CLIP）作为编码器。2. 将视觉属性转换为文本描述比直接使用视觉输入能进一步提高性能。3. 在特定MMG上对MLLM进行微调，即使没有明确的图结构信息，也能在大多数场景中达到最先进的结果。", "conclusion": "Graph-MLLM基准及其发现促进了多模态图学习领域的快速、公平评估，并有望激发该领域进一步的创新研究。", "translation": "多模态大语言模型（MLLMs）在表示和理解不同模态方面展现出卓越能力。然而，它们通常侧重于成对的模态对齐，而忽略了数据点之间的结构关系。将多模态与结构化图信息（即多模态图，MMGs）整合对于社交网络、医疗保健和推荐系统等现实应用至关重要。现有的MMG学习方法根据其利用MLLM的方式分为三种范式：编码器、对齐器和预测器。MLLM-as-Encoder 侧重于通过多模态特征融合增强图神经网络（GNNs）；MLLM-as-Aligner 在语言或隐藏空间中对齐多模态属性，以实现基于LLM的图推理；MLLM-as-Predictor 将MLLMs视为独立的推理器，通过上下文学习或微调进行操作。尽管取得了进展，MMG领域缺乏一个统一的基准来公平评估这些方法，使得进展不明确。为了弥补这一差距，我们提出了Graph-MLLM，这是一个针对多模态图学习的综合基准，通过系统地评估这三种范式在六个不同领域的数据集上的表现。通过大量实验，我们观察到联合考虑节点的视觉和文本属性有利于图学习，即使使用预训练的文本到图像对齐模型（例如CLIP）作为编码器。我们还发现，将视觉属性转换为文本描述比直接使用视觉输入能进一步提高性能。此外，我们观察到在特定MMG上对MLLM进行微调，即使没有明确的图结构信息，也能在大多数场景中达到最先进的结果。我们希望我们的开源库将促进快速、公平的评估，并激发该领域进一步的创新研究。", "summary": "本文提出了Graph-MLLM，一个用于多模态图学习的综合基准，旨在解决现有MLLM忽略图结构以及MMG领域缺乏统一评估标准的问题。该基准系统评估了MLLM作为编码器、对齐器和预测器的三种范式在六个数据集上的表现。研究发现，联合考虑视觉和文本属性、将视觉属性转换为文本描述，以及对MLLM进行微调，都能有效提升多模态图学习的性能，甚至在缺乏明确图结构时也能达到SOTA结果。该开源库旨在促进公平评估和未来研究。", "keywords": "多模态图学习, 多模态大语言模型, 基准测试, 图神经网络, 模态对齐", "comments": "这篇论文通过引入Graph-MLLM基准，填补了多模态图学习领域在公平评估方面的空白，具有重要的实践意义。其创新之处在于系统性地对比了MLLM在MMG中的三种应用范式，并提出了有价值的性能提升策略，如视觉到文本的转换和MLLM微调，这为未来的研究指明了方向。"}}
{"id": "2506.10641", "title": "Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters", "authors": ["Tatsuya Hiraoka", "Kentaro Inui"], "summary": "Large language models (LLMs) can spell out tokens character by character with\nhigh accuracy, yet they struggle with more complex character-level tasks, such\nas identifying compositional subcomponents within tokens. In this work, we\ninvestigate how LLMs internally represent and utilize character-level\ninformation during the spelling-out process. Our analysis reveals that,\nalthough spelling out is a simple task for humans, it is not handled in a\nstraightforward manner by LLMs. Specifically, we show that the embedding layer\ndoes not fully encode character-level information, particularly beyond the\nfirst character. As a result, LLMs rely on intermediate and higher Transformer\nlayers to reconstruct character-level knowledge, where we observe a distinct\n\"breakthrough\" in their spelling behavior. We validate this mechanism through\nthree complementary analyses: probing classifiers, identification of knowledge\nneurons, and inspection of attention weights.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10641v1", "AI": {"title_translation": "拼写输出并非直截了当：大型语言模型从词元到字符的分词能力", "tldr": "大型语言模型在字符级任务上表现挣扎，本研究揭示了它们在拼写输出过程中如何内部表示和利用字符级信息，发现嵌入层未能完全编码字符信息，LLMs依赖中间和更高层的Transformer层来重建字符级知识。", "motivation": "虽然大型语言模型（LLMs）可以高精度地逐字符拼写出词元，但它们在更复杂的字符级任务（如识别词元内的组成子组件）上却表现困难。本研究旨在调查LLMs在拼写输出过程中如何内部表示和利用字符级信息。", "method": "通过三种互补的分析方法：探测分类器（probing classifiers）、知识神经元识别（identification of knowledge neurons）和注意力权重检查（inspection of attention weights）来验证机制。", "result": "分析显示，尽管拼写输出对人类来说是一个简单的任务，但LLMs处理起来并非直截了当。具体而言，嵌入层未能完全编码字符级信息，尤其是在第一个字符之后。因此，LLMs依赖中间和更高层的Transformer层来重建字符级知识，并在此过程中观察到其拼写行为的明显“突破”。", "conclusion": "本研究得出结论，对于大型语言模型而言，拼写输出并非一个简单的过程，它们需要通过中间和更高层的Transformer层来重建字符级知识，而不是在嵌入层就完全编码。", "translation": "大型语言模型（LLMs）可以高精度地逐字符拼写出词元，但它们在更复杂的字符级任务（例如识别词元内的组成子组件）上却表现困难。在这项工作中，我们调查了LLMs在拼写输出过程中如何内部表示和利用字符级信息。我们的分析揭示，尽管拼写输出对人类来说是一个简单的任务，但LLMs处理起来并非直截了当。具体而言，我们发现嵌入层未能完全编码字符级信息，尤其是在第一个字符之后。因此，LLMs依赖中间和更高层的Transformer层来重建字符级知识，我们观察到它们拼写行为中明显的“突破”。我们通过三种互补的分析验证了这一机制：探测分类器、知识神经元识别和注意力权重检查。", "summary": "本研究探讨了大型语言模型（LLMs）在执行字符级拼写输出任务时内部处理信息的方式。尽管LLMs能高精度地拼写，但在更复杂的字符级任务上存在困难。研究发现，LLMs的嵌入层未能完全编码字符级信息，特别是首字符之后的部分。相反，LLMs需要依赖中间和更高层的Transformer层来重建和利用字符级知识，这在它们的拼写行为中表现出显著的“突破”。该机制通过探测分类器、知识神经元识别和注意力权重检查三种方法得到验证。", "keywords": "LLMs, 分词, 字符级, 拼写输出, Transformer层", "comments": "这项工作创新性地揭示了LLMs在处理字符级信息时的内部机制，特别是指出了嵌入层的局限性以及Transformer高层在重建字符知识中的关键作用。这对于理解LLMs的内部工作原理及其在细粒度文本处理上的能力边界具有重要意义。"}}
{"id": "2506.10425", "title": "It's Not the Target, It's the Background: Rethinking Infrared Small Target Detection via Deep Patch-Free Low-Rank Representations", "authors": ["Guoyi Zhang", "Guangsheng Xu", "Siyang Chen", "Han Wang", "Xiaohu Zhang"], "summary": "Infrared small target detection (IRSTD) remains a long-standing challenge in\ncomplex backgrounds due to low signal-to-clutter ratios (SCR), diverse target\nmorphologies, and the absence of distinctive visual cues. While recent deep\nlearning approaches aim to learn discriminative representations, the intrinsic\nvariability and weak priors of small targets often lead to unstable\nperformance. In this paper, we propose a novel end-to-end IRSTD framework,\ntermed LRRNet, which leverages the low-rank property of infrared image\nbackgrounds. Inspired by the physical compressibility of cluttered scenes, our\napproach adopts a compression--reconstruction--subtraction (CRS) paradigm to\ndirectly model structure-aware low-rank background representations in the image\ndomain, without relying on patch-based processing or explicit matrix\ndecomposition. To the best of our knowledge, this is the first work to directly\nlearn low-rank background structures using deep neural networks in an\nend-to-end manner. Extensive experiments on multiple public datasets\ndemonstrate that LRRNet outperforms 38 state-of-the-art methods in terms of\ndetection accuracy, robustness, and computational efficiency. Remarkably, it\nachieves real-time performance with an average speed of 82.34 FPS. Evaluations\non the challenging NoisySIRST dataset further confirm the model's resilience to\nsensor noise. The source code will be made publicly available upon acceptance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10425v1", "AI": {"title_translation": "不是目标，而是背景：通过深度无块低秩表示重新思考红外小目标检测", "tldr": "本文提出了一种名为LRRNet的端到端深度学习框架，利用红外图像背景的低秩特性，在不依赖块处理或显式矩阵分解的情况下，实现了对红外小目标的高效、鲁棒和实时的检测。", "motivation": "红外小目标检测（IRSTD）在复杂背景下由于信杂比低、目标形态多样以及缺乏独特的视觉线索而面临长期挑战。现有的深度学习方法虽然旨在学习判别性表示，但小目标的内在变异性和弱先验往往导致性能不稳定。", "method": "本文提出了一种名为LRRNet的端到端IRSTD框架，利用红外图像背景的低秩特性。该方法采用压缩-重建-减法（CRS）范式，直接在图像域中建模结构感知的低秩背景表示，不依赖于基于块的处理或显式矩阵分解。这是首次通过深度神经网络以端到端方式直接学习低秩背景结构。", "result": "LRRNet在多个公共数据集上优于38种最先进的方法，在检测精度、鲁棒性和计算效率方面表现出色。它实现了实时性能，平均速度达到82.34 FPS。在具有挑战性的NoisySIRST数据集上的评估进一步证实了模型对传感器噪声的弹性。", "conclusion": "LRRNet通过利用红外图像背景的低秩特性，并采用创新的压缩-重建-减法范式，有效解决了红外小目标检测的挑战，并在多个方面超越了现有技术，实现了实时、高精度和鲁棒的检测。", "translation": "红外小目标检测（IRSTD）由于信杂比（SCR）低、目标形态多样以及缺乏独特的视觉线索，在复杂背景下仍然是一个长期存在的挑战。尽管最近的深度学习方法旨在学习判别性表示，但小目标的内在变异性和弱先验往往导致性能不稳定。在本文中，我们提出了一种新颖的端到端IRSTD框架，命名为LRRNet，它利用了红外图像背景的低秩特性。受杂乱场景物理可压缩性的启发，我们的方法采用了一种压缩-重建-减法（CRS）范式，直接在图像域中建模结构感知的低秩背景表示，而无需依赖基于块的处理或显式矩阵分解。据我们所知，这是首次通过深度神经网络以端到端方式直接学习低秩背景结构。在多个公共数据集上进行的大量实验表明，LRRNet在检测精度、鲁棒性和计算效率方面优于38种最先进的方法。值得注意的是，它实现了实时性能，平均速度达到82.34 FPS。对具有挑战性的NoisySIRST数据集的评估进一步证实了模型对传感器噪声的弹性。源代码将在接受后公开提供。", "summary": "本文提出LRRNet，一个新颖的端到端红外小目标检测框架。该框架利用红外背景的低秩特性，通过压缩-重建-减法（CRS）范式直接学习结构感知的低秩背景表示，无需传统块处理。实验证明，LRRNet在检测精度、鲁棒性和计算效率上超越了38种SOTA方法，并能实现实时检测（82.34 FPS），对传感器噪声也表现出良好弹性。", "keywords": "红外小目标检测, 低秩表示, 深度学习, 背景建模, 实时检测", "comments": "该论文的创新点在于首次将深度学习与红外图像背景的低秩特性相结合，并提出了一种新颖的无块、端到端学习低秩背景结构的方法。这种方法避免了传统低秩分解的复杂性，并有效解决了小目标检测中背景建模的难题。其在实时性、精度和鲁棒性上的显著提升，使其在实际应用中具有重要价值。"}}
{"id": "2506.10685", "title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework", "authors": ["Xia Du", "Xiaoyuan Liu", "Jizhe Zhou", "Zheng Lin", "Chi-man Pun", "Zhe Chen", "Wei Ni", "Jun Luo"], "summary": "With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10685v1", "AI": {"title_translation": "无源对抗性验证码：一种双阶段对抗性验证码框架", "tldr": "提出了一种名为UAC的新型验证码框架，它能生成高质量的对抗性验证码，以抵御深度学习攻击，并支持有目标和无目标的攻击。", "motivation": "传统的验证码方案在深度学习的快速发展下，越来越容易受到深度神经网络（DNNs）驱动的自动化攻击。现有对抗性攻击方法依赖原始图像特征，导致图像失真，阻碍人类识别，且在缺乏初始输入图像的场景中应用受限。", "method": "我们提出了无源对抗性验证码（UAC），一个通过攻击者指定文本提示生成高保真对抗性示例的新颖框架。UAC利用大型语言模型（LLM）增强验证码多样性，并支持有目标和无目标的攻击。对于有目标攻击，EDICT方法优化扩散模型中的双重潜在变量以获得卓越的图像质量。在无目标攻击中，特别是针对黑盒场景，我们引入了双路径无源对抗性验证码（BP-UAC），这是一种采用多模态梯度和双路径优化的两步优化策略，以实现高效的错误分类。", "result": "实验表明，BP-UAC在各种系统中均实现了高攻击成功率，生成了人类和DNN都无法区分的自然验证码。", "conclusion": "UAC框架，特别是BP-UAC，能够生成高质量、难以区分的对抗性验证码，有效抵御深度学习攻击，解决了现有方法在图像质量和应用场景上的限制。", "translation": "随着深度学习的快速发展，传统的验证码方案越来越容易受到深度神经网络（DNNs）驱动的自动化攻击。现有对抗性攻击方法通常依赖原始图像特征，导致图像失真，阻碍人类识别，并限制了在缺乏初始输入图像场景中的适用性。为了解决这些挑战，我们提出了无源对抗性验证码（UAC），这是一种通过攻击者指定文本提示生成高保真对抗性示例的新颖框架。UAC利用大型语言模型（LLM）增强验证码多样性，并支持有目标和无目标的攻击。对于有目标攻击，EDICT方法优化扩散模型中的双重潜在变量，以获得卓越的图像质量。在无目标攻击中，特别是针对黑盒场景，我们引入了双路径无源对抗性验证码（BP-UAC），这是一种采用多模态梯度和双路径优化的两步优化策略，以实现高效的错误分类。实验表明，BP-UAC在各种系统中均实现了高攻击成功率，生成了人类和DNN都无法区分的自然验证码。", "summary": "本文提出了无源对抗性验证码（UAC），一个旨在应对深度学习对传统验证码攻击的新型框架。UAC利用大型语言模型和攻击者指定的文本提示生成高保真对抗性验证码。它支持有目标攻击（通过EDICT方法优化扩散模型）和无目标攻击（通过双路径无源对抗性验证码BP-UAC）。BP-UAC采用多模态梯度和双路径优化策略，在实验中表现出高攻击成功率，并能生成人类和DNN都难以区分的自然验证码，有效解决了现有方法的局限性。", "keywords": "对抗性验证码, 深度学习攻击, 大型语言模型, 扩散模型, 黑盒攻击", "comments": "该论文提出了一种创新的“无源”对抗性验证码生成方法，解决了传统对抗性攻击依赖原始图像和导致图像失真的问题。其结合LLM和扩散模型生成高保真、难以区分的对抗性示例，特别是在黑盒无目标攻击中引入的BP-UAC策略，具有重要的理论和实际意义，为提升验证码的安全性提供了新思路。"}}
{"id": "2506.10007", "title": "Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space", "authors": ["Kangwei Liu", "Junwu Liu", "Xiaowei Yi", "Jinlin Guo", "Yun Cao"], "summary": "Audio-driven emotional 3D facial animation encounters two significant\nchallenges: (1) reliance on single-modal control signals (videos, text, or\nemotion labels) without leveraging their complementary strengths for\ncomprehensive emotion manipulation, and (2) deterministic regression-based\nmapping that constrains the stochastic nature of emotional expressions and\nnon-verbal behaviors, limiting the expressiveness of synthesized animations. To\naddress these challenges, we present a diffusion-based framework for\ncontrollable expressive 3D facial animation. Our approach introduces two key\ninnovations: (1) a FLAME-centered multimodal emotion binding strategy that\naligns diverse modalities (text, audio, and emotion labels) through contrastive\nlearning, enabling flexible emotion control from multiple signal sources, and\n(2) an attention-based latent diffusion model with content-aware attention and\nemotion-guided layers, which enriches motion diversity while maintaining\ntemporal coherence and natural facial dynamics. Extensive experiments\ndemonstrate that our method outperforms existing approaches across most\nmetrics, achieving a 21.6\\% improvement in emotion similarity while preserving\nphysiologically plausible facial dynamics. Project Page:\nhttps://kangweiiliu.github.io/Control_3D_Animation.", "comment": "Accepted by ICME2025", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10007v1", "AI": {"title_translation": "可控的富有表现力的统一多模态空间扩散三维面部动画", "tldr": "提出一个基于扩散的多模态框架，用于可控的三维面部动画，解决了单模态控制和确定性映射的限制，显著提升了情感相似度。", "motivation": "现有音频驱动的情感三维面部动画面临两大挑战：1) 依赖单一模态控制信号（视频、文本或情感标签），未能利用其互补优势进行全面的情感操控；2) 基于确定性回归的映射限制了情感表达和非语言行为的随机性，从而限制了合成动画的表现力。", "method": "提出一个基于扩散的框架，用于可控的富有表现力的三维面部动画。该方法引入了两项关键创新：1) 以FLAME为中心的多模态情感绑定策略，通过对比学习对齐不同模态（文本、音频和情感标签），实现从多个信号源进行灵活的情感控制；2) 一个具有内容感知注意力和情感引导层的基于注意力的潜在扩散模型，它在保持时间连贯性和自然面部动态的同时，丰富了运动多样性。", "result": "在大多数指标上优于现有方法，情感相似度提高了21.6%，同时保留了生理上合理的面部动态。", "conclusion": "该方法通过结合多模态控制和扩散模型，显著提升了三维面部动画的表现力和情感相似度，解决了现有方法的局限性。", "translation": "音频驱动的情感三维面部动画面临两大显著挑战：(1) 依赖单一模态控制信号（视频、文本或情感标签），未能利用其互补优势进行全面的情感操控；(2) 基于确定性回归的映射限制了情感表达和非语言行为的随机性，从而限制了合成动画的表现力。为了解决这些挑战，我们提出了一种基于扩散的可控表现力三维面部动画框架。我们的方法引入了两项关键创新：(1) 一种以FLAME为中心的多模态情感绑定策略，通过对比学习对齐不同模态（文本、音频和情感标签），从而实现从多个信号源进行灵活的情感控制；(2) 一个具有内容感知注意力和情感引导层的基于注意力的潜在扩散模型，它在保持时间连贯性和自然面部动态的同时，丰富了运动多样性。大量实验表明，我们的方法在大多数指标上优于现有方法，在保持生理上合理的面部动态的同时，情感相似度提高了21.6%。项目页面：https://kangweiiliu.github.io/Control_3D_Animation。", "summary": "本文提出了一种基于扩散的框架，用于生成可控且富有表现力的三维面部动画，旨在解决现有方法中单模态控制的局限性以及确定性映射对表达随机性的限制。该框架引入了FLAME中心的多模态情感绑定策略，通过对比学习整合文本、音频和情感标签等多种模态，实现灵活的情感控制。同时，一个带有内容感知注意力和情感引导层的潜在扩散模型被用于增加运动多样性并保持动画的自然性。实验结果表明，该方法在情感相似度方面有显著提升（21.6%），并优于现有技术。", "keywords": "3D Facial Animation, Diffusion Model, Multimodal Control, Emotion Synthesis, FLAME", "comments": "这项工作通过引入多模态融合和扩散模型，为三维面部动画领域带来了创新。其多模态情感绑定策略有效解决了单一控制信号的局限性，而扩散模型则克服了传统确定性方法在表现力上的不足，使生成的动画更具随机性和自然性。情感相似度的大幅提升是其重要贡献。"}}
{"id": "2506.10313", "title": "Collaborative Min-Max Regret in Grouped Multi-Armed Bandits", "authors": ["Moïse Blanchard", "Vineet Goyal"], "summary": "We study the impact of sharing exploration in multi-armed bandits in a\ngrouped setting where a set of groups have overlapping feasible action sets\n[Baek and Farias '24]. In this grouped bandit setting, groups share reward\nobservations, and the objective is to minimize the collaborative regret,\ndefined as the maximum regret across groups. This naturally captures\napplications in which one aims to balance the exploration burden between groups\nor populations -- it is known that standard algorithms can lead to\nsignificantly imbalanced exploration cost between groups. We address this\nproblem by introducing an algorithm Col-UCB that dynamically coordinates\nexploration across groups. We show that Col-UCB achieves both optimal minimax\nand instance-dependent collaborative regret up to logarithmic factors. These\nbounds are adaptive to the structure of shared action sets between groups,\nproviding insights into when collaboration yields significant benefits over\neach group learning their best action independently.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10313v1", "AI": {"title_translation": "分组多臂老虎机中的协作最小-最大遗憾", "tldr": "本文研究了在分组多臂老虎机中共享探索的影响，提出了Col-UCB算法，该算法在协作遗憾方面实现了最优性能，并揭示了协作的益处。", "motivation": "在分组多臂老虎机设置中，标准算法可能导致各组之间探索成本的显著不平衡，目标是平衡各组之间的探索负担，并最小化协作遗憾（定义为各组中的最大遗憾）。", "method": "引入了Col-UCB算法，该算法动态协调各组之间的探索。该算法通过共享奖励观测来解决问题。", "result": "Col-UCB算法实现了最优的最小-最大和依赖于实例的协作遗憾，误差仅为对数因子。这些界限适应于各组之间共享行动集的结构。", "conclusion": "协作探索在分组多臂老虎机中可以带来显著益处，Col-UCB算法能够有效地平衡各组间的探索负担并达到最优性能，尤其是在共享行动集结构下。", "translation": "我们研究了在分组设置中多臂老虎机共享探索的影响，其中一组组具有重叠的可行行动集[Baek and Farias '24]。在这种分组老虎机设置中，各组共享奖励观测，目标是最小化协作遗憾，协作遗憾定义为各组中的最大遗憾。这自然地捕捉了旨在平衡各组或人群之间探索负担的应用——已知标准算法可能导致各组之间探索成本的显著不平衡。我们通过引入一种动态协调各组之间探索的算法Col-UCB来解决这个问题。我们表明Col-UCB实现了最优的最小-最大和依赖于实例的协作遗憾，误差仅为对数因子。这些界限适应于各组之间共享行动集的结构，提供了关于何时协作比各组独立学习其最佳行动产生显著益处的见解。", "summary": "本文研究了在具有重叠可行行动集的分组多臂老虎机中共享探索的问题。为解决标准算法可能导致的探索成本不平衡问题，提出了Col-UCB算法。该算法通过动态协调各组间的探索，旨在最小化协作遗憾（即各组中的最大遗憾）。研究结果表明，Col-UCB在协作遗憾方面达到了最优的最小-最大和依赖于实例的性能，且误差仅为对数因子。这些发现还揭示了在不同共享行动集结构下，协作如何比独立学习带来显著优势。", "keywords": "多臂老虎机, 协作学习, 最小-最大遗憾, 分组设置, 探索-利用", "comments": "这篇论文的创新点在于提出了Col-UCB算法来解决分组多臂老虎机中探索成本不平衡的问题，并以最小-最大遗憾为目标。其重要性在于提供了一种理论上最优的方法来平衡各组的探索负担，并提供了关于协作何时能带来显著收益的见解。"}}
{"id": "2506.10687", "title": "Large Language Models for Detection of Life-Threatening Texts", "authors": ["Thanh Thi Nguyen", "Campbell Wilson", "Janis Dalins"], "summary": "Detecting life-threatening language is essential for safeguarding individuals\nin distress, promoting mental health and well-being, and preventing potential\nharm and loss of life. This paper presents an effective approach to identifying\nlife-threatening texts using large language models (LLMs) and compares them\nwith traditional methods such as bag of words, word embedding, topic modeling,\nand Bidirectional Encoder Representations from Transformers. We fine-tune three\nopen-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter\nvariants on different datasets, which are constructed with class balance,\nimbalance, and extreme imbalance scenarios. Experimental results demonstrate a\nstrong performance of LLMs against traditional methods. More specifically,\nMistral and Llama-2 models are top performers in both balanced and imbalanced\ndata scenarios while Gemma is slightly behind. We employ the upsampling\ntechnique to deal with the imbalanced data scenarios and demonstrate that while\nthis method benefits traditional approaches, it does not have as much impact on\nLLMs. This study demonstrates a great potential of LLMs for real-world\nlife-threatening language detection problems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10687v1", "AI": {"title_translation": "大型语言模型用于生命威胁文本检测", "tldr": "本文展示了大型语言模型（LLMs）在检测生命威胁文本方面的卓越性能，超越了传统方法。", "motivation": "检测生命威胁语言对于保障处于困境中的个体、促进心理健康和福祉以及预防潜在的伤害和生命损失至关重要。", "method": "本文提出了一种使用大型语言模型（LLMs）识别生命威胁文本的有效方法。研究者微调了Gemma、Mistral和Llama-2（7B参数变体）三种开源LLMs，并在不同类别平衡（平衡、不平衡、极端不平衡）的数据集上进行实验。同时，将LLMs与词袋模型、词嵌入、主题模型和BERT等传统方法进行了比较。对于不平衡数据场景，采用了上采样技术。", "result": "实验结果表明，LLMs的表现优于传统方法。具体来说，Mistral和Llama-2模型在平衡和不平衡数据场景中表现最佳，而Gemma略逊一筹。上采样技术对传统方法有益，但对LLMs的影响不大。", "conclusion": "这项研究表明大型语言模型在解决现实世界中生命威胁语言检测问题方面具有巨大潜力。", "translation": "检测生命威胁语言对于保障处于困境中的个体、促进心理健康和福祉以及预防潜在的伤害和生命损失至关重要。本文提出了一种使用大型语言模型（LLMs）识别生命威胁文本的有效方法，并将其与词袋模型、词嵌入、主题模型和双向编码器表示转换器（BERT）等传统方法进行了比较。我们使用Gemma、Mistral和Llama-2三种开源LLM的7B参数变体，在构建了类别平衡、不平衡和极端不平衡场景的不同数据集上进行了微调。实验结果表明，LLMs相对于传统方法表现出强大的性能。更具体地说，Mistral和Llama-2模型在平衡和不平衡数据场景中均表现最佳，而Gemma略逊一筹。我们采用上采样技术来处理不平衡数据场景，并证明虽然这种方法有利于传统方法，但对LLMs的影响不大。这项研究表明大型语言模型在解决现实世界中生命威胁语言检测问题方面具有巨大潜力。", "summary": "本文研究了大型语言模型（LLMs）在检测生命威胁文本方面的应用，并与传统方法进行了比较。通过在不同类别平衡的数据集上微调Gemma、Mistral和Llama-2等开源LLMs，实验证明LLMs的性能优于传统方法，其中Mistral和Llama-2表现最佳。研究还发现上采样技术对LLMs处理不平衡数据的影响较小。该研究突出了LLMs在生命威胁语言检测领域的巨大潜力。", "keywords": "大型语言模型,生命威胁文本检测,文本分类,机器学习,心理健康", "comments": "本文创新性地将大型语言模型应用于生命威胁文本检测这一关键领域，并系统地与传统方法进行对比，明确展示了LLMs的优越性。特别指出不同LLM模型的性能差异以及上采样技术对LLM影响较小，为未来实际应用提供了宝贵的指导。这项研究对于心理健康支持和危机干预具有重要意义。"}}
{"id": "2506.10430", "title": "MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment", "authors": ["Shuo wang", "Jihao Zhang"], "summary": "The rapid proliferation of online video content necessitates effective video\nsummarization techniques. Traditional methods, often relying on a single\nmodality (typically visual), struggle to capture the full semantic richness of\nvideos. This paper introduces MF2Summ, a novel video summarization model based\non multimodal content understanding, integrating both visual and auditory\ninformation. MF2Summ employs a five-stage process: feature extraction,\ncross-modal attention interaction, feature fusion, segment prediction, and key\nshot selection. Visual features are extracted using a pre-trained GoogLeNet\nmodel, while auditory features are derived using SoundNet. The core of our\nfusion mechanism involves a cross-modal Transformer and an alignment-guided\nself-attention Transformer, designed to effectively model inter-modal\ndependencies and temporal correspondences. Segment importance, location, and\ncenter-ness are predicted, followed by key shot selection using Non-Maximum\nSuppression (NMS) and the Kernel Temporal Segmentation (KTS) algorithm.\nExperimental results on the SumMe and TVSum datasets demonstrate that MF2Summ\nachieves competitive performance, notably improving F1-scores by 1.9\\% and\n0.6\\% respectively over the DSNet model, and performing favorably against other\nstate-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10430v1", "AI": {"title_translation": "MF2Summ：基于时间对齐的多模态融合视频摘要", "tldr": "MF2Summ是一个新的视频摘要模型，它通过融合视觉和听觉信息来解决传统单模态方法的不足，并在两个数据集上取得了竞争性的SOTA性能。", "motivation": "在线视频内容的迅速增长，需要有效的视频摘要技术。传统方法通常依赖单一模态（主要是视觉），难以捕捉视频的全部语义丰富性。", "method": "MF2Summ采用五阶段流程：特征提取（GoogLeNet用于视觉，SoundNet用于听觉）、跨模态注意力交互、特征融合（使用跨模态Transformer和对齐引导的自注意力Transformer）、片段预测（重要性、位置和中心性）和关键镜头选择（使用NMS和KTS算法）。", "result": "在SumMe和TVSum数据集上，MF2Summ的F1分数分别比DSNet模型提高了1.9%和0.6%，并且优于其他SOTA方法。", "conclusion": "MF2Summ通过有效融合视觉和听觉信息，显著提升了视频摘要的性能，证明了多模态内容理解在视频摘要中的优越性。", "translation": "在线视频内容的迅速增长，使得有效的视频摘要技术成为必需。传统方法通常依赖单一模态（通常是视觉），难以捕捉视频的全部语义丰富性。本文介绍了MF2Summ，这是一种基于多模态内容理解的新型视频摘要模型，它整合了视觉和听觉信息。MF2Summ采用五阶段流程：特征提取、跨模态注意力交互、特征融合、片段预测和关键镜头选择。视觉特征使用预训练的GoogLeNet模型提取，而听觉特征则使用SoundNet获取。我们融合机制的核心包括一个跨模态Transformer和一个对齐引导的自注意力Transformer，旨在有效建模模态间依赖和时间对应关系。模型预测片段的重要性、位置和中心性，然后使用非极大值抑制（NMS）和核时间分割（KTS）算法选择关键镜头。在SumMe和TVSum数据集上的实验结果表明，MF2Summ取得了竞争性的性能，F1分数分别比DSNet模型显著提高了1.9%和0.6%，并且与其他最先进的方法相比表现出色。", "summary": "MF2Summ是一个创新的视频摘要模型，通过融合视觉和听觉多模态信息，克服了传统单模态方法的局限性。该模型采用五阶段处理流程，包括特征提取、跨模态交互、特征融合（利用Transformer结构进行模态间依赖和时间对齐）、片段预测和关键镜头选择。实验证明，MF2Summ在SumMe和TVSum数据集上取得了优于现有先进方法的性能，显著提升了F1分数。", "keywords": "视频摘要, 多模态融合, 时间对齐, Transformer, MF2Summ", "comments": "MF2Summ的创新点在于其多模态融合策略，特别是使用了跨模态Transformer和对齐引导的自注意力Transformer来有效处理不同模态数据间的依赖和时间对应关系。这对于捕捉视频的深层语义至关重要，是其优于传统单模态方法的关键。该研究的重要性在于推动了视频摘要领域向更全面、更智能的方向发展。"}}
{"id": "2506.10008", "title": "Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics", "authors": ["Yi-Chun Chen"], "summary": "This paper presents a hierarchical knowledge graph framework for the\nstructured understanding of visual narratives, focusing on multimodal media\nsuch as comics. The proposed method decomposes narrative content into multiple\nlevels, from macro-level story arcs to fine-grained event segments. It\nrepresents them through integrated knowledge graphs that capture semantic,\nspatial, and temporal relationships. At the panel level, we construct\nmultimodal graphs that link visual elements such as characters, objects, and\nactions with corresponding textual components, including dialogue and captions.\nThese graphs are integrated across narrative levels to support reasoning over\nstory structure, character continuity, and event progression.\n  We apply our approach to a manually annotated subset of the Manga109 dataset\nand demonstrate its ability to support symbolic reasoning across diverse\nnarrative tasks, including action retrieval, dialogue tracing, character\nappearance mapping, and panel timeline reconstruction. Evaluation results show\nhigh precision and recall across tasks, validating the coherence and\ninterpretability of the framework. This work contributes a scalable foundation\nfor narrative-based content analysis, interactive storytelling, and multimodal\nreasoning in visual media.", "comment": "This paper has been submitted to ACM Multimedia 2025 and is currently\n  under review", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10008v1", "AI": {"title_translation": "视觉叙事推理的结构化图表示：一个漫画分层框架", "tldr": "本文提出了一个分层知识图谱框架，用于理解漫画等多模态视觉叙事，并能支持多种叙事推理任务。", "motivation": "理解视觉叙事内容，特别是漫画等多模态媒体的结构化理解，并支持叙事推理。", "method": "该方法将叙事内容分解为多个层次（从宏观故事弧到细粒度事件片段），并通过集成知识图谱表示它们，捕获语义、空间和时间关系。在画格层面构建多模态图，连接视觉元素（角色、物体、动作）和文本组件（对话、标题），并跨叙事层级整合这些图以支持故事结构、角色连续性和事件进展的推理。", "result": "将该方法应用于Manga109数据集的手动标注子集，并证明其能够支持跨不同叙事任务（包括动作检索、对话追踪、角色出现映射和画格时间线重建）的符号推理。评估结果显示在各项任务中均具有高精度和高召回率。", "conclusion": "该框架具有连贯性和可解释性，为基于叙事的内容分析、交互式故事讲述和视觉媒体中的多模态推理提供了一个可扩展的基础。", "translation": "本文提出了一个用于结构化理解视觉叙事的分层知识图谱框架，重点关注漫画等多模态媒体。所提出的方法将叙事内容分解为多个层次，从宏观层面的故事弧到细粒度的事件片段。它通过集成的知识图谱来表示这些内容，捕获语义、空间和时间关系。在画格层面，我们构建多模态图，将角色、物体、动作等视觉元素与对话和标题等相应的文本组件连接起来。这些图在叙事层级之间进行整合，以支持对故事结构、角色连续性和事件进展的推理。我们将我们的方法应用于Manga109数据集的手动标注子集，并展示了其支持跨不同叙事任务（包括动作检索、对话追踪、角色出现映射和画格时间线重建）的符号推理的能力。评估结果显示在各项任务中均具有高精度和高召回率，验证了该框架的连贯性和可解释性。这项工作为基于叙事的内容分析、交互式故事讲述和视觉媒体中的多模态推理贡献了一个可扩展的基础。", "summary": "本文提出了一个针对漫画等视觉叙事的分层知识图谱框架。该框架将叙事内容分解为多层级的知识图谱，整合视觉和文本信息，以捕捉语义、空间和时间关系。实验证明，该方法能有效支持多种叙事推理任务，如动作检索和角色追踪，并为视觉媒体的内容分析和交互式叙事提供了可扩展的基础。", "keywords": "视觉叙事, 知识图谱, 漫画, 分层框架, 多模态推理", "comments": "该论文的创新点在于提出了一个分层的知识图谱框架，能够对视觉叙事进行结构化理解，并整合了多模态信息（视觉与文本）。其重要性在于为漫画等复杂视觉叙事媒体的自动化分析和推理提供了有效工具，并为未来的交互式故事讲述和多模态推理奠定了基础。框架的连贯性和可解释性是其优势。"}}
{"id": "2506.10314", "title": "Detecting Sockpuppetry on Wikipedia Using Meta-Learning", "authors": ["Luc Raszewski", "Christine De Kock"], "summary": "Malicious sockpuppet detection on Wikipedia is critical to preserving access\nto reliable information on the internet and preventing the spread of\ndisinformation. Prior machine learning approaches rely on stylistic and\nmeta-data features, but do not prioritise adaptability to author-specific\nbehaviours. As a result, they struggle to effectively model the behaviour of\nspecific sockpuppet-groups, especially when text data is limited. To address\nthis, we propose the application of meta-learning, a machine learning technique\ndesigned to improve performance in data-scarce settings by training models\nacross multiple tasks. Meta-learning optimises a model for rapid adaptation to\nthe writing style of a new sockpuppet-group. Our results show that\nmeta-learning significantly enhances the precision of predictions compared to\npre-trained models, marking an advancement in combating sockpuppetry on open\nediting platforms. We release a new dataset of sockpuppet investigations to\nfoster future research in both sockpuppetry and meta-learning fields.", "comment": "Accepted to ACL 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10314v1", "AI": {"title_translation": "使用元学习检测维基百科上的傀儡账户", "tldr": "本文提出使用元学习来更有效地检测维基百科上的傀儡账户，尤其是在数据稀缺的情况下，并展示了其优于传统方法的预测精度。", "motivation": "恶意傀儡账户检测对于维护互联网上可靠信息的访问和防止虚假信息传播至关重要。现有的机器学习方法依赖于风格和元数据特征，但在适应特定作者行为方面存在不足，导致在文本数据有限时难以有效建模特定傀儡群体的行为。", "method": "本文提出应用元学习，这是一种通过在多个任务上训练模型来提高数据稀缺环境下性能的机器学习技术。元学习优化模型以快速适应新的傀儡群体的写作风格。", "result": "结果表明，与预训练模型相比，元学习显著提高了预测的精度。", "conclusion": "元学习为打击开放编辑平台上的傀儡账户提供了一个显著的进步。此外，该研究发布了一个新的傀儡账户调查数据集，以促进未来在傀儡账户和元学习领域的研究。", "translation": "恶意傀儡账户检测对于维护互联网上可靠信息的访问和防止虚假信息传播至关重要。先前的机器学习方法依赖于风格和元数据特征，但并未优先考虑对特定作者行为的适应性。因此，它们难以有效建模特定傀儡群体的行为，尤其是在文本数据有限时。为了解决这个问题，我们提出了元学习的应用，这是一种旨在通过在多个任务上训练模型来提高数据稀缺环境下性能的机器学习技术。元学习优化模型以快速适应新的傀儡群体的写作风格。我们的结果表明，与预训练模型相比，元学习显著提高了预测的精度，标志着在打击开放编辑平台上的傀儡账户方面取得了进展。我们发布了一个新的傀儡账户调查数据集，以促进未来在傀儡账户和元学习领域的研究。", "summary": "本文提出了一种利用元学习检测维基百科上恶意傀儡账户的新方法。针对传统机器学习方法在数据稀缺和适应特定傀儡群体行为方面的不足，元学习通过在多任务上训练模型，使其能够快速适应新傀儡群体的写作风格。实验结果表明，元学习显著提高了傀儡账户检测的预测精度。此外，研究还发布了一个新的傀儡账户调查数据集，以促进相关领域的未来研究。", "keywords": "傀儡账户检测, 元学习, 维基百科, 虚假信息, 机器学习", "comments": "该论文的创新点在于首次将元学习应用于维基百科傀儡账户检测，解决了传统方法在数据稀缺和适应性方面的局限。其重要性在于提升了打击网络虚假信息的能力，并为元学习在社会安全领域的应用提供了新的视角。发布新数据集也对后续研究具有积极推动作用。"}}
{"id": "2506.10715", "title": "Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet", "authors": ["Lorenzo Augello", "John P. McCrae"], "summary": "Open English Wordnet is a key resource published in OntoLex-lemon as part of\nthe linguistic linked open data cloud. There are, however, many links missing\nin the resource, and in this paper, we look at how we can establish hypernymy\nbetween adjectives. We present a theoretical discussion of the hypernymy\nrelation and how it differs for adjectives in contrast to nouns and verbs. We\ndevelop a new resource for adjective hypernymy and fine-tune large language\nmodels to predict adjective hypernymy, showing that the methodology of\nTaxoLLaMa can be adapted to this task.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10715v1", "AI": {"title_translation": "使用语言模型推断形容词上位词以增加开放英语词网的连通性", "tldr": "本文研究如何使用语言模型推断形容词上位词，以解决开放英语词网中缺失链接的问题，并开发了新资源，证明了TaxoLLaMa方法的可行性。", "motivation": "开放英语词网（Open English Wordnet）中存在许多缺失的链接，尤其是在形容词上位词方面，这限制了其作为语言学链接开放数据云中关键资源的完整性。", "method": "本文首先对上位词关系及其在形容词与名词、动词之间的差异进行了理论探讨。然后，开发了一个新的形容词上位词资源，并微调大型语言模型来预测形容词上位词，借鉴了TaxoLLaMa的方法。", "result": "研究表明，TaxoLLaMa的方法可以成功地应用于形容词上位词推断任务。", "conclusion": "通过开发新资源并微调大型语言模型，证明了TaxoLLaMa方法可以有效应用于推断形容词上位词，从而有助于增加开放英语词网的连通性。", "translation": "开放英语词网是作为语言学链接开放数据云的一部分，以OntoLex-lemon形式发布的关键资源。然而，该资源中缺失许多链接，在本文中，我们探讨了如何建立形容词之间的上位词关系。我们对上位词关系及其与名词和动词相比在形容词中的差异进行了理论探讨。我们开发了一个新的形容词上位词资源，并微调大型语言模型来预测形容词上位词，表明TaxoLLaMa的方法可以适应这项任务。", "summary": "本研究旨在解决开放英语词网中形容词上位词链接缺失的问题。文章深入探讨了形容词上位词的理论概念，并与名词和动词的上位词进行了区分。为实现目标，研究团队开发了一个新的形容词上位词资源，并利用大型语言模型进行微调，以预测形容词的上位词关系。实验结果表明，TaxoLLaMa的现有方法能够成功地应用于此任务，有效提升了词网的连通性。", "keywords": "形容词上位词, 语言模型, 开放英语词网, TaxoLLaMa, 词汇资源", "comments": "本文的创新之处在于将大型语言模型（特别是TaxoLLaMa方法）应用于此前较少关注的形容词上位词推断任务，填补了开放英语词网中的一个重要空白。这对于提升词汇资源质量和语言理解能力具有重要意义。"}}
{"id": "2506.10452", "title": "Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts", "authors": ["Guowei Zhong", "Ruohong Huan", "Mingzhen Wu", "Ronghua Liang", "Peng Chen"], "summary": "Recent advancements in Multimodal Emotion Recognition (MER) face challenges\nin addressing both modality missing and Out-Of-Distribution (OOD) data\nsimultaneously. Existing methods often rely on specific models or introduce\nexcessive parameters, which limits their practicality. To address these issues,\nwe propose a novel robust MER framework, Causal Inference Distiller (CIDer),\nand introduce a new task, Random Modality Feature Missing (RMFM), to generalize\nthe definition of modality missing. CIDer integrates two key components: a\nModel-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal\nInference (MACI) module. MSSD enhances robustness under the RMFM task through a\nweight-sharing self-distillation approach applied across low-level features,\nattention maps, and high-level representations. Additionally, a Word-level\nSelf-aligned Attention Module (WSAM) reduces computational complexity, while a\nMultimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.\nTo tackle OOD challenges, MACI employs a tailored causal graph to mitigate\nlabel and language biases using a Multimodal Causal Module (MCM) and\nfine-grained counterfactual texts. Notably, MACI can independently enhance OOD\ngeneralization with minimal additional parameters. Furthermore, we also\nintroduce the new repartitioned MER OOD datasets. Experimental results\ndemonstrate that CIDer achieves robust performance in both RMFM and OOD\nscenarios, with fewer parameters and faster training compared to\nstate-of-the-art methods. The implementation of this work is publicly\naccessible at https://github.com/gw-zhong/CIDer.", "comment": "Submitted to TAC. The code is available at\n  https://github.com/gw-zhong/CIDer", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10452v1", "AI": {"title_translation": "面向模态缺失和分布偏移的鲁棒多模态情感识别", "tldr": "本文提出了一种名为CIDer的新型鲁棒多模态情感识别框架，通过自蒸馏和因果推理模块，有效应对模态缺失和域外数据挑战，且参数更少，训练更快。", "motivation": "多模态情感识别（MER）在同时处理模态缺失和域外（OOD）数据时面临挑战。现有方法常依赖特定模型或引入过多参数，限制了实用性。", "method": "本文提出了一种新颖的鲁棒多模态情感识别框架——因果推理蒸馏器（CIDer），并引入了随机模态特征缺失（RMFM）新任务。CIDer包含两个关键组件：模型特定自蒸馏（MSSD）模块和模型无关因果推理（MACI）模块。MSSD通过权重共享自蒸馏增强RMFM任务下的鲁棒性，并引入词级自对齐注意力模块（WSAM）和多模态复合Transformer（MCT）。MACI采用定制的因果图，利用多模态因果模块（MCM）和细粒度反事实文本来缓解标签和语言偏差，以解决OOD挑战。此外，本文还引入了新的重新划分的MER OOD数据集。", "result": "实验结果表明，与现有最先进方法相比，CIDer在RMFM和OOD场景下均实现了鲁棒性能，且参数更少，训练速度更快。", "conclusion": "CIDer框架成功地在模态缺失和分布偏移场景下实现了鲁棒的多模态情感识别，且具有更高的效率和更少的参数。", "translation": "多模态情感识别（MER）的最新进展在同时处理模态缺失和域外（OOD）数据方面面临挑战。现有方法通常依赖特定模型或引入过多参数，这限制了它们的实用性。为了解决这些问题，我们提出了一种新颖的鲁棒MER框架——因果推理蒸馏器（CIDer），并引入了一个新任务——随机模态特征缺失（RMFM），以泛化模态缺失的定义。CIDer集成了两个关键组件：模型特定自蒸馏（MSSD）模块和模型无关因果推理（MACI）模块。MSSD通过应用于低级特征、注意力图和高级表示的权重共享自蒸馏方法，增强了RMFM任务下的鲁棒性。此外，词级自对齐注意力模块（WSAM）降低了计算复杂度，而多模态复合Transformer（MCT）促进了高效的多模态融合。为了解决OOD挑战，MACI采用定制的因果图，利用多模态因果模块（MCM）和细粒度反事实文本来缓解标签和语言偏差。值得注意的是，MACI可以独立地增强OOD泛化能力，且只需极少的额外参数。此外，我们还引入了新的重新划分的MER OOD数据集。实验结果表明，与现有最先进方法相比，CIDer在RMFM和OOD场景下均实现了鲁棒性能，且参数更少，训练速度更快。本工作的实现已在https://github.com/gw-zhong/CIDer上公开访问。", "summary": "本文提出了一种名为因果推理蒸馏器（CIDer）的新型鲁棒多模态情感识别框架，旨在同时解决模态缺失和域外（OOD）数据问题。CIDer包含模型特定自蒸馏（MSSD）模块，通过自蒸馏和高效融合模块（WSAM、MCT）增强模态缺失下的鲁棒性；以及模型无关因果推理（MACI）模块，利用因果图和反事实文本缓解OOD场景中的偏差。实验证明，CIDer在处理这两种挑战方面表现出卓越的鲁棒性，同时显著减少了参数量并加快了训练速度。", "keywords": "多模态情感识别, 模态缺失, 分布偏移, 因果推理, 自蒸馏", "comments": "该论文的创新点在于提出了一个统一的框架CIDer，同时解决了多模态情感识别中长期存在的模态缺失和域外泛化两大难题。特别是引入随机模态特征缺失（RMFM）任务和结合自蒸馏与因果推理模块的设计，展示了其方法的通用性和高效性。MACI模块通过因果推断独立提升OOD泛化能力，且额外参数极少，这对于实际应用具有重要意义。此外，引入新的OOD数据集也促进了该领域的研究。"}}
{"id": "2506.10011", "title": "WDMIR: Wavelet-Driven Multimodal Intent Recognition", "authors": ["Weiyin Gong", "Kai Zhang", "Yanghai Zhang", "Qi Liu", "Xinjie Sun", "Junyu Lu", "Linbo Zhu"], "summary": "Multimodal intent recognition (MIR) seeks to accurately interpret user\nintentions by integrating verbal and non-verbal information across video, audio\nand text modalities. While existing approaches prioritize text analysis, they\noften overlook the rich semantic content embedded in non-verbal cues. This\npaper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)\nframework that enhances intent understanding through frequency-domain analysis\nof non-verbal information. To be more specific, we propose: (1) a\nwavelet-driven fusion module that performs synchronized decomposition and\nintegration of video-audio features in the frequency domain, enabling\nfine-grained analysis of temporal dynamics; (2) a cross-modal interaction\nmechanism that facilitates progressive feature enhancement from bimodal to\ntrimodal integration, effectively bridging the semantic gap between verbal and\nnon-verbal information. Extensive experiments on MIntRec demonstrate that our\napproach achieves state-of-the-art performance, surpassing previous methods by\n1.13% on accuracy. Ablation studies further verify that the wavelet-driven\nfusion module significantly improves the extraction of semantic information\nfrom non-verbal sources, with a 0.41% increase in recognition accuracy when\nanalyzing subtle emotional cues.", "comment": "Accepted at IJCAI 2025, 9pages, 6figures", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10011v1", "AI": {"title_translation": "WDMIR：小波驱动的多模态意图识别", "tldr": "WDMIR是一个新颖的多模态意图识别框架，通过对视频和音频等非语言信息进行小波驱动的频域分析，显著提升了意图理解的准确性，并在MIntRec数据集上达到了最先进的性能。", "motivation": "现有的大多数多模态意图识别方法过度侧重于文本分析，往往忽视了视频和音频等非语言信息中蕴含的丰富语义内容，导致无法充分利用这些关键线索来准确解释用户意图。", "method": "本文提出了一个新颖的小波驱动多模态意图识别（WDMIR）框架。具体包括：1) 一个小波驱动的融合模块，在频域对视频和音频特征进行同步分解和整合，实现对时间动态的细粒度分析；2) 一个跨模态交互机制，促进从双模态到三模态的渐进式特征增强，有效弥合语言和非语言信息之间的语义鸿沟。", "result": "在MIntRec数据集上的大量实验表明，WDMIR方法达到了最先进的性能，识别准确率比现有方法提高了1.13%。消融研究进一步证实，小波驱动的融合模块显著改善了非语言源的语义信息提取，在分析微妙情感线索时识别准确率提高了0.41%。", "conclusion": "WDMIR框架通过引入小波驱动的频域分析和跨模态交互机制，有效利用了非语言信息中的语义内容，显著提升了多模态意图识别的性能，并达到了最先进的水平。", "translation": "多模态意图识别（MIR）旨在通过整合视频、音频和文本模态中的语言和非语言信息来准确解释用户意图。虽然现有方法优先考虑文本分析，但它们常常忽视非语言线索中嵌入的丰富语义内容。本文提出了一个新颖的小波驱动多模态意图识别（WDMIR）框架，通过对非语言信息进行频域分析来增强意图理解。具体来说，我们提出了：(1) 一个小波驱动的融合模块，在频域执行视频-音频特征的同步分解和整合，从而实现时间动态的细粒度分析；(2) 一个跨模态交互机制，促进从双模态到三模态整合的渐进式特征增强，有效地弥合了语言和非语言信息之间的语义鸿沟。在MIntRec上的大量实验表明，我们的方法达到了最先进的性能，在准确率上超越了以往方法1.13%。消融研究进一步验证了小波驱动的融合模块显著改善了非语言源的语义信息提取，在分析微妙情感线索时识别准确率提高了0.41%。", "summary": "WDMIR是一个新颖的多模态意图识别框架，旨在通过对非语言信息（视频和音频）进行小波驱动的频域分析来克服现有方法对文本的过度依赖。该框架包含一个小波驱动的融合模块，用于频域内视频-音频特征的细粒度分析，以及一个跨模态交互机制，用于弥合语言与非语言信息间的语义鸿沟。实验结果显示，WDMIR在MIntRec数据集上取得了最先进的性能，准确率提高了1.13%，并且小波模块在提取非语言语义信息方面表现出色。", "keywords": "多模态意图识别, 小波, 频域分析, 非语言线索, 特征融合", "comments": "本文的创新点在于引入小波变换对非语言信息进行频域分析，这为多模态意图识别提供了一个新颖且有效的方法，弥补了现有方法对非语言线索利用不足的缺陷。其模块化设计也增强了方法的鲁棒性和可解释性。"}}
{"id": "2506.10315", "title": "PyLO: Towards Accessible Learned Optimizers in PyTorch", "authors": ["Paul Janson", "Benjamin Therien", "Quentin Anthony", "Xiaolong Huang", "Abhinav Moudgil", "Eugene Belilovsky"], "summary": "Learned optimizers have been an active research topic over the past decade,\nwith increasing progress toward practical, general-purpose optimizers that can\nserve as drop-in replacements for widely used methods like Adam. However,\nrecent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --\nremain largely inaccessible to the broader community, in part due to their\nreliance on JAX and the absence of user-friendly packages for applying the\noptimizers after meta-training. To address this gap, we introduce PyLO, a\nPyTorch-based library that brings learned optimizers to the broader machine\nlearning community through familiar, widely adopted workflows. Unlike prior\nwork focused on synthetic or convex tasks, our emphasis is on applying learned\noptimization to real-world large-scale pre-training tasks. Our release includes\na CUDA-accelerated version of the small_fc_lopt learned optimizer architecture\nfrom (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to\n205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO\nalso allows us to easily combine learned optimizers with existing optimization\ntools such as learning rate schedules and weight decay. When doing so, we find\nthat learned optimizers can substantially benefit. Our code is available at\nhttps://github.com/Belilovsky-Lab/pylo", "comment": "Accepted at ICML CODEML Workshop 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10315v1", "AI": {"title_translation": "PyLO：在PyTorch中实现可访问的学习型优化器", "tldr": "PyLO是一个PyTorch库，旨在使学习型优化器更易于访问和应用于大规模预训练任务，并提供了显著的速度提升。", "motivation": "现有的学习型优化器（如VeLO）因依赖JAX且缺乏用户友好的包而在更广泛的社区中难以使用；之前的研究多集中在合成或凸任务，而非实际大规模预训练任务。", "method": "引入PyLO，一个基于PyTorch的库，提供用户友好的工作流程。发布了CUDA加速版的small_fc_lopt架构，并展示了如何将学习型优化器与现有优化工具（如学习率调度和权重衰减）结合。", "result": "PyLO将ViT B/16（批大小32）的训练吞吐量从39.36提升到205.59样本/秒，实现了显著的加速。发现学习型优化器与现有优化工具结合时能获得显著益处。", "conclusion": "PyLO通过提供一个PyTorch库，使学习型优化器更易于访问和应用于实际大规模预训练任务，并展示了其潜在的速度优势和与现有工具结合的益处。", "translation": "学习型优化器在过去十年中一直是一个活跃的研究课题，在实用、通用优化器方面取得了越来越多的进展，这些优化器可以作为Adam等广泛使用方法的直接替代品。然而，最近的进展——例如VeLO，它经过4000 TPU-月的元训练——在很大程度上仍无法被更广泛的社区所访问，部分原因是它们依赖JAX，以及元训练后缺乏用于应用优化器的用户友好型软件包。为了解决这一差距，我们引入了PyLO，一个基于PyTorch的库，通过熟悉且广泛采用的工作流程，将学习型优化器带给更广泛的机器学习社区。与之前专注于合成或凸任务的工作不同，我们的重点是将学习型优化应用于实际的大规模预训练任务。我们的发布包括(Metz et al., 2022a)中small_fc_lopt学习型优化器架构的CUDA加速版本，为ViT B/16（批大小32）的训练提供了显著的速度提升——吞吐量从39.36样本/秒提高到205.59样本/秒。PyLO还允许我们轻松地将学习型优化器与现有优化工具（如学习率调度和权重衰减）结合。在这样做时，我们发现学习型优化器可以从中受益匪然。我们的代码可在https://github.com/Belilovsky-Lab/pylo获取。", "summary": "PyLO是一个新的PyTorch库，旨在解决学习型优化器（如VeLO）因依赖JAX和缺乏用户友好工具而难以广泛应用的问题。它专注于将学习型优化应用于实际大规模预训练任务，并提供了一个CUDA加速版的small_fc_lopt优化器，显著提升了训练速度（例如，ViT B/16训练吞吐量从39.36增至205.59样本/秒）。PyLO还支持学习型优化器与现有优化工具（如学习率调度和权重衰减）结合使用，并证明了这种结合的益处。", "keywords": "学习型优化器, PyTorch, 优化, 大规模预训练, PyLO", "comments": "PyLO的创新之处在于它将先进的学习型优化器引入到更主流的PyTorch生态系统中，降低了其使用门槛。通过提供CUDA加速版本并专注于实际大规模任务，它解决了现有研究的局限性，并有望加速学习型优化器在实际应用中的普及。其与现有优化工具的兼容性也增强了其实用性。"}}
{"id": "2506.10716", "title": "PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models", "authors": ["Ye Yu", "Yaoning Yu", "Haohan Wang"], "summary": "Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve\nstrong performance on mathematical benchmarks using lengthy chain-of-thought\n(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This\ninflates token usage and cost, limiting deployment in latency-sensitive or\nAPI-constrained settings. We introduce PREMISE (PRompt-based Efficient\nMathematical Inference with Strategic Evaluation), a prompt-only framework that\nreduces reasoning overhead without modifying model weights. PREMISE combines\ntrace-level diagnostics with gradient-inspired prompt optimization to minimize\nredundant computation while preserving answer accuracy. The approach jointly\noptimizes brevity and correctness through a multi-objective textual search that\nbalances token length and answer validity. Unlike prior work, PREMISE runs in a\nsingle-pass black-box interface, so it can be applied directly to commercial\nLLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy\n($96\\%\\rightarrow96\\%$ with Claude, $91\\%\\rightarrow92\\%$ with Gemini) while\nreducing reasoning tokens by up to $87.5\\%$ and cutting dollar cost by\n$69$--$82\\%$. These results show that prompt-level optimization is a practical\nand scalable path to efficient LRM inference without compromising reasoning\nquality.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10716v1", "AI": {"title_translation": "PREMISE：大型模型中高效数学推理的可扩展和战略性提示优化", "tldr": "PREMISE是一种仅基于提示的框架，通过结合轨迹级诊断和梯度启发的提示优化，显著减少大型推理模型在数学任务中的冗余计算、token使用和成本，同时保持或提高准确性，并且无需修改模型权重。", "motivation": "大型推理模型（LRMs）在数学基准测试中表现出色，但其冗长的思维链（CoT）推理导致token使用和成本过高，限制了在延迟敏感或API受限环境中的部署。", "method": "本文介绍了PREMISE（PRompt-based Efficient Mathematical Inference with Strategic Evaluation），一个仅基于提示的框架，无需修改模型权重即可减少推理开销。PREMISE结合了轨迹级诊断和梯度启发的提示优化，通过多目标文本搜索来最小化冗余计算，同时保持答案准确性，平衡token长度和答案有效性。它以单次黑盒接口运行，可直接应用于商业LLM。", "result": "在GSM8K、SVAMP和Math500数据集上，PREMISE匹配或超过了基线准确性（Claude从96%到96%，Gemini从91%到92%），同时将推理token减少了高达87.5%，并将美元成本降低了69%-82%。", "conclusion": "提示级优化是实现高效大型推理模型推理的实用且可扩展的途径，且不影响推理质量。", "translation": "大型推理模型（LRMs），如Claude 3.7 Sonnet和OpenAI o1，通过冗长的思维链（CoT）推理在数学基准测试中取得了强大的性能，但由此产生的轨迹通常不必要地冗长。这会增加token使用和成本，限制了在延迟敏感或API受限环境中的部署。我们引入了PREMISE（PRompt-based Efficient Mathematical Inference with Strategic Evaluation），一个仅基于提示的框架，无需修改模型权重即可减少推理开销。PREMISE结合了轨迹级诊断和梯度启发的提示优化，以最小化冗余计算，同时保持答案准确性。该方法通过多目标文本搜索共同优化简洁性和正确性，平衡token长度和答案有效性。与现有工作不同，PREMISE以单次黑盒接口运行，因此可以直接应用于商业LLM。在GSM8K、SVAMP和Math500数据集上，我们匹配或超过了基线准确性（Claude从96%到96%，Gemini从91%到92%），同时将推理token减少了高达87.5%，并将美元成本降低了69%-82%。这些结果表明，提示级优化是实现高效大型推理模型推理的实用且可扩展的途径，且不影响推理质量。", "summary": "PREMISE是一个创新的仅基于提示的框架，旨在解决大型推理模型在数学任务中因冗长思维链导致的token使用和成本过高问题。该框架通过结合轨迹级诊断和梯度启发的提示优化，以多目标文本搜索的方式，在不修改模型权重的前提下，显著减少冗余计算，优化提示的简洁性和正确性。实验结果表明，PREMISE在保持或提高数学基准测试准确性的同时，能够大幅减少推理token和运行成本，证明了提示级优化在提高大型推理模型效率方面的实用性和可扩展性。", "keywords": "提示优化, 数学推理, 大型语言模型, 效率, 成本降低", "comments": "PREMISE的创新之处在于其“仅基于提示”的黑盒优化方法，这使其能够直接应用于商业大型语言模型，解决了传统方法需要模型微调或架构修改的限制。通过结合轨迹级诊断和梯度启发优化，它有效地平衡了推理效率和准确性，为降低大型模型部署成本提供了实用且可扩展的解决方案。"}}
{"id": "2506.10016", "title": "Multimodal Large Language Models: A Survey", "authors": ["Longzhen Han", "Awes Mubarak", "Almas Baimagambetov", "Nikolaos Polatidis", "Thar Baker"], "summary": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text\ngeneration, now spanning diverse output modalities including images, music,\nvideo, human motion, and 3D objects, by integrating language with other sensory\nmodalities under unified architectures. This survey categorises six primary\ngenerative modalities and examines how foundational techniques, namely\nSelf-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement\nLearning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,\nenable cross-modal capabilities. We analyze key models, architectural trends,\nand emergent cross-modal synergies, while highlighting transferable techniques\nand unresolved challenges. Architectural innovations like transformers and\ndiffusion models underpin this convergence, enabling cross-modal transfer and\nmodular specialization. We highlight emerging patterns of synergy, and identify\nopen challenges in evaluation, modularity, and structured reasoning. This\nsurvey offers a unified perspective on MLLM development and identifies critical\npaths toward more general-purpose, adaptive, and interpretable multimodal\nsystems.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10016v1", "AI": {"title_translation": "多模态大型语言模型：一项综述", "tldr": "综述了多模态大型语言模型（MLLMs）的发展，涵盖其多模态输出、核心技术、架构趋势、协同效应和开放挑战。", "motivation": "多模态大型语言模型（MLLMs）已迅速发展，超越文本生成，涵盖多种输出模态。本综述旨在提供对MLLM发展的统一视角，分析其关键技术、架构趋势、协同作用以及面临的挑战，并指明未来发展方向。", "method": "本综述将六种主要的生成模态进行分类，并考察了自监督学习（SSL）、专家混合（MoE）、人类反馈强化学习（RLHF）和思维链（CoT）提示等基础技术如何实现跨模态能力。同时，分析了关键模型、架构趋势和新兴的跨模态协同作用，并强调了可迁移技术和未解决的挑战。", "result": "MLLMs已扩展到图像、音乐、视频、人体运动和3D对象等多种输出模态。Transformer和扩散模型等架构创新是其融合的基础。本综述识别了评估、模块化和结构化推理方面的开放挑战，并提出了协同作用的新兴模式。", "conclusion": "本综述为多模态大型语言模型（MLLM）的发展提供了一个统一的视角，并指明了通向更通用、适应性更强、可解释的多模态系统的关键路径。", "translation": "多模态大型语言模型（MLLM）已迅速发展，超越了文本生成，通过在统一架构下将语言与其他感官模态相结合，现已涵盖图像、音乐、视频、人体运动和3D对象等多种输出模态。本综述将六种主要的生成模态进行分类，并考察了自监督学习（SSL）、专家混合（MoE）、人类反馈强化学习（RLHF）和思维链（CoT）提示等基础技术如何实现跨模态能力。我们分析了关键模型、架构趋势和新兴的跨模态协同作用，同时强调了可迁移技术和未解决的挑战。Transformer和扩散模型等架构创新是这种融合的基础，实现了跨模态迁移和模块化专业化。我们强调了协同作用的新兴模式，并指出了评估、模块化和结构化推理方面的开放挑战。本综述为MLLM的发展提供了一个统一的视角，并指明了通向更通用、适应性更强、可解释的多模态系统的关键路径。", "summary": "这篇综述深入探讨了多模态大型语言模型（MLLMs）的快速演进，这些模型已从单一文本生成扩展到包括图像、音乐、视频等多种输出模态。文章分类了六种主要生成模态，并分析了自监督学习、专家混合等关键技术如何赋能跨模态能力。此外，综述还探讨了核心模型、架构趋势、跨模态协同效应，并指出了评估和模块化等开放挑战，旨在为构建更通用、适应性强的多模态系统提供统一视角和发展路径。", "keywords": "多模态大型语言模型, 综述, 跨模态能力, 基础技术, 开放挑战", "comments": "这篇综述的重要性在于它系统性地梳理了多模态大型语言模型（MLLMs）的最新进展和未来方向。它不仅涵盖了MLLMs如何超越传统文本生成，扩展到多种输出模态的创新点，还深入分析了支撑其发展的核心技术和架构趋势。特别地，它指出了评估、模块化和结构化推理等关键的未解决挑战，为领域内的研究者提供了明确的研究方向。其统一的视角对于理解这一快速发展领域的复杂性至关重要。"}}
{"id": "2506.10332", "title": "Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data", "authors": ["Aaryam Sharma"], "summary": "Air pollution has become a significant health risk in developing countries.\nWhile governments routinely publish air-quality index (AQI) data to track\npollution, these values fail to capture the local reality, as sensors are often\nvery sparse. In this paper, we address this gap by predicting AQI in 1 km^2\nneighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal\nGNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen\ncoordinates. New insights about AQI such as the existence of strong repetitive\nshort-term patterns and changing spatial relations are also discovered. The\ncode is available on GitHub.", "comment": "10 pages, 7 figures. Code available at\n  https://github.com/ASChampOmega/AQI_Forecasting.git", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10332v1", "AI": {"title_translation": "您社区的空气：使用移动传感器数据进行细粒度AQI预测", "tldr": "该论文使用时空GNNs预测1平方公里邻域的细粒度AQI，显著优于现有方法并发现新的AQI模式。", "motivation": "现有的政府发布的AQI数据由于传感器稀疏性，无法捕捉到当地真实的空气污染情况，这在发展中国家是一个重要的健康风险。", "method": "使用时空GNN（Spatio-temporal GNNs）模型，并利用AirDelhi数据集，预测1平方公里邻域的AQI。", "result": "模型在MSE上超越现有工作71.654，实现79%的降低，即使在未见的坐标上也能保持效果。同时发现了AQI存在强烈的重复短期模式和变化的空间关系。", "conclusion": "该研究通过使用时空GNNs，成功实现了细粒度AQI预测，显著提升了预测精度，并揭示了AQI的新特性，为解决空气污染问题提供了更精细的解决方案。", "translation": "空气污染已成为发展中国家面临的重大健康风险。尽管政府定期发布空气质量指数 (AQI) 数据来追踪污染，但由于传感器通常非常稀疏，这些数值未能捕捉到当地的真实情况。在本文中，我们以AirDelhi数据集为例，通过预测1平方公里邻域的AQI来解决这一差距。我们使用时空GNN（Spatio-temporal GNNs）模型，在MSE上超越现有工作71.654，实现了79%的降低，即使在未见的坐标上也能保持效果。此外，还发现了关于AQI的新见解，例如存在强烈的重复短期模式和变化的空间关系。代码已在GitHub上提供。", "summary": "本论文针对现有AQI数据因传感器稀疏性无法反映局部真实情况的问题，提出了一种基于时空GNNs的细粒度AQI预测方法。该方法以AirDelhi数据集为例，成功预测了1平方公里邻域的AQI，并在均方误差（MSE）上实现了79%的显著降低，甚至在未见区域也表现出色。研究还揭示了AQI的短期重复模式和动态空间关系等新特性，为更精确的空气质量监测提供了有效途径。", "keywords": "空气质量指数, AQI预测, 细粒度, 移动传感器, 时空GNN", "comments": "该论文的创新之处在于利用时空GNNs实现细粒度AQI预测，显著提升了预测精度，并在解决传感器稀疏性带来的局限性方面取得了重要进展。其发现的AQI新模式也具有重要的研究价值。代码开源也方便了后续研究和应用。"}}
{"id": "2506.10341", "title": "Provably Learning from Language Feedback", "authors": ["Wanqiao Xu", "Allen Nie", "Ruijie Zheng", "Aditya Modi", "Adith Swaminathan", "Ching-An Cheng"], "summary": "Interactively learning from observation and language feedback is an\nincreasingly studied area driven by the emergence of large language model (LLM)\nagents. While impressive empirical demonstrations have been shown, so far a\nprincipled framing of these decision problems remains lacking. In this paper,\nwe formalize the Learning from Language Feedback (LLF) problem, assert\nsufficient assumptions to enable learning despite latent rewards, and introduce\n$\\textit{transfer eluder dimension}$ as a complexity measure to characterize\nthe hardness of LLF problems. We show that transfer eluder dimension captures\nthe intuition that information in the feedback changes the learning complexity\nof the LLF problem. We demonstrate cases where learning from rich language\nfeedback can be exponentially faster than learning from reward. We develop a\nno-regret algorithm, called $\\texttt{HELiX}$, that provably solves LLF problems\nthrough sequential interactions, with performance guarantees that scale with\nthe transfer eluder dimension of the problem. Across several empirical domains,\nwe show that $\\texttt{HELiX}$ performs well even when repeatedly prompting LLMs\ndoes not work reliably. Our contributions mark a first step towards designing\nprincipled interactive learning algorithms from generic language feedback.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10341v1", "AI": {"title_translation": "可证明地从语言反馈中学习", "tldr": "本文正式化了从语言反馈中学习（LLF）问题，引入了“迁移规避维度”作为衡量复杂度的指标，并开发了一种名为HELiX的无悔算法，该算法能够可证明地解决LLF问题，且在经验领域表现良好。", "motivation": "由大型语言模型（LLM）代理的出现推动，从观察和语言反馈中进行交互式学习是一个日益受到关注的领域。尽管已经展示了令人印象深刻的经验性成果，但到目前为止，这些决策问题仍然缺乏一个原则性的框架。", "method": "本文正式化了从语言反馈中学习（LLF）问题，提出了在潜在奖励下实现学习的充分假设，并引入“迁移规避维度”作为衡量LLF问题难度的复杂度度量。研究开发了一种名为HELiX的无悔算法，通过顺序交互可证明地解决LLF问题。", "result": "研究表明，迁移规避维度捕获了反馈信息改变LLF问题学习复杂度的直觉。从丰富的语言反馈中学习可以比从奖励中学习快指数级。HELiX在多个经验领域表现良好，即使重复提示LLM不可靠时也能奏效。", "conclusion": "本文的贡献标志着设计从通用语言反馈中进行原则性交互式学习算法的第一步，通过对LLF问题进行形式化，引入新的复杂度度量，并开发出一种具有性能保证的可证明算法。", "translation": "通过观察和语言反馈进行交互式学习是一个日益受到关注的领域，这得益于大型语言模型（LLM）代理的兴起。尽管已经展示了令人印象深刻的经验性演示，但到目前为止，这些决策问题仍然缺乏一个原则性的框架。在本文中，我们正式化了从语言反馈中学习（LLF）问题，提出了在潜在奖励下实现学习的充分假设，并引入“迁移规避维度”作为衡量LLF问题难度的复杂度度量。我们表明，迁移规避维度捕捉了反馈中的信息改变LLF问题学习复杂度的直觉。我们展示了一些案例，其中从丰富的语言反馈中学习可以比从奖励中学习快指数级。我们开发了一种名为HELiX的无悔算法，通过顺序交互可证明地解决LLF问题，其性能保证与问题的迁移规避维度成比例。在多个经验领域，我们表明HELiX即使在重复提示LLM不可靠时也能表现良好。我们的贡献标志着设计从通用语言反馈中进行原则性交互式学习算法的第一步。", "summary": "本文针对从语言反馈中学习（LLF）领域缺乏原则性框架的问题，正式化了LLF问题，引入了“迁移规避维度”作为衡量学习复杂度的指标，并证明了语言反馈能够显著加速学习。研究提出了一种名为HELiX的无悔算法，该算法能够可证明地解决LLF问题，并在实际应用中展现出稳健的性能，即使在LLM重复提示不可靠的情况下也能有效工作。", "keywords": "语言反馈, 交互式学习, 迁移规避维度, LLM代理, 可证明学习", "comments": "该论文的创新之处在于其对从语言反馈中学习（LLF）这一领域进行了首次原则性形式化，引入了“迁移规避维度”这一新颖的复杂度度量，并提供了一种具有理论保证的算法（HELiX）。这为利用语言反馈实现指数级学习加速提供了坚实的基础，推动了交互式学习算法向更鲁棒和可解释的方向发展。"}}
{"id": "2506.10351", "title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation", "authors": ["Yanlong Chen", "Mattia Orlandi", "Pierangelo Maria Rapa", "Simone Benatti", "Luca Benini", "Yawei Li"], "summary": "Physiological signals are often corrupted by motion artifacts, baseline\ndrift, and other low-SNR disturbances, which pose significant challenges for\nanalysis. Additionally, these signals exhibit strong non-stationarity, with\nsharp peaks and abrupt changes that evolve continuously, making them difficult\nto represent using traditional time-domain or filtering methods. To address\nthese issues, a novel wavelet-based approach for physiological signal analysis\nis presented, aiming to capture multi-scale time-frequency features in various\nphysiological signals. Leveraging this technique, two large-scale pretrained\nmodels specific to EMG and ECG are introduced for the first time, achieving\nsuperior performance and setting new baselines in downstream tasks.\nAdditionally, a unified multi-modal framework is constructed by integrating\npretrained EEG model, where each modality is guided through its dedicated\nbranch and fused via learnable weighted fusion. This design effectively\naddresses challenges such as low signal-to-noise ratio, high inter-subject\nvariability, and device mismatch, outperforming existing methods on multi-modal\ntasks. The proposed wavelet-based architecture lays a solid foundation for\nanalysis of diverse physiological signals, while the multi-modal design points\nto next-generation physiological signal processing with potential impact on\nwearable health monitoring, clinical diagnostics, and broader biomedical\napplications.", "comment": "22 pages, 8 figures, 9 tables. Submitted to NeurIPS 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10351v1", "AI": {"title_translation": "PhysioWave：一种用于生理信号表示的多尺度小波变换器", "tldr": "PhysioWave提出了一种基于小波的Transformer模型，用于解决生理信号的表示和分析挑战，通过预训练模型和多模态融合在EMG、ECG和EEG任务中取得了优异性能。", "motivation": "生理信号常受运动伪影、基线漂移和低信噪比干扰，且具有强非平稳性、尖峰和突变，传统方法难以有效表示和分析。", "method": "提出PhysioWave，一种新颖的基于小波的方法，旨在捕获生理信号的多尺度时频特征。利用此技术，首次引入了针对EMG和ECG的两个大规模预训练模型。此外，通过整合预训练的EEG模型，构建了一个统一的多模态框架，每个模态通过专用分支引导，并通过可学习加权融合。", "result": "在下游任务中实现了卓越性能并设定了新基线。在多模态任务上优于现有方法，有效解决了低信噪比、高受试者间变异性和设备不匹配等挑战。", "conclusion": "所提出的基于小波的架构为分析多样生理信号奠定了坚实基础，而多模态设计则预示着下一代生理信号处理，对可穿戴健康监测、临床诊断和更广泛的生物医学应用具有潜在影响。", "translation": "生理信号常受到运动伪影、基线漂移和其他低信噪比干扰的影响，这给分析带来了重大挑战。此外，这些信号表现出强烈的非平稳性，具有不断演变的尖峰和突然变化，这使得使用传统的时域或滤波方法难以表示它们。为了解决这些问题，本文提出了一种新颖的基于小波的生理信号分析方法，旨在捕获各种生理信号中的多尺度时频特征。利用这项技术，首次引入了两个针对EMG和ECG的大规模预训练模型，在下游任务中取得了卓越的性能并设定了新的基线。此外，通过整合预训练的EEG模型，构建了一个统一的多模态框架，其中每个模态通过其专用分支引导，并通过可学习的加权融合进行融合。这种设计有效地解决了低信噪比、高受试者间变异性和设备不匹配等挑战，在多模态任务上优于现有方法。所提出的基于小波的架构为分析多样生理信号奠定了坚实的基础，而多模态设计则预示着下一代生理信号处理，对可穿戴健康监测、临床诊断和更广泛的生物医学应用具有潜在影响。", "summary": "PhysioWave提出了一种创新的多尺度小波Transformer模型，旨在解决生理信号因噪声、非平稳性导致的表示和分析难题。该方法通过捕获多尺度时频特征，并首次引入了针对EMG和ECG的大规模预训练模型。此外，它构建了一个统一的多模态框架，通过可学习的加权融合集成多种生理信号，有效提升了在单模态和多模态任务上的性能，为未来的可穿戴健康监测和临床诊断提供了新范式。", "keywords": "生理信号, 小波变换, Transformer, 多模态融合, 预训练模型", "comments": "该论文的创新点在于结合了小波变换和Transformer架构，有效处理了生理信号的非平稳性和噪声问题，并通过大规模预训练模型和多模态融合提升了通用性和鲁棒性。其提出的统一多模态框架在生物医学信号处理领域具有重要意义，有望推动可穿戴设备和临床诊断技术的发展。"}}
{"id": "2506.10766", "title": "One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers", "authors": ["Diana Abagyan", "Alejandro R. Salamanca", "Andres Felipe Cruz-Salinas", "Kris Cao", "Hangyu Lin", "Acyr Locatelli", "Marzieh Fadaee", "Ahmet Üstün", "Sara Hooker"], "summary": "Pretraining massively multilingual Large Language Models (LLMs) for many\nlanguages at once is challenging due to limited model capacity, scarce\nhigh-quality data, and compute constraints. Moreover, the lack of language\ncoverage of the tokenizer makes it harder to address the gap for new languages\npurely at the post-training stage. In this work, we study what relatively cheap\ninterventions early on in training improve \"language plasticity\", or adaptation\ncapabilities of the model post-training to new languages. We focus on tokenizer\ndesign and propose using a universal tokenizer that is trained for more\nlanguages than the primary pretraining languages to enable efficient adaptation\nin expanding language coverage after pretraining. Our systematic experiments\nacross diverse groups of languages and different training strategies show that\na universal tokenizer enables significantly higher language adaptation, with up\nto 20.2% increase in win rates compared to tokenizers specific to pretraining\nlanguages. Furthermore, a universal tokenizer also leads to better plasticity\ntowards languages that are completely unseen in the tokenizer and pretraining,\nby up to 5% win rate gain. We achieve this adaptation to an expanded set of\nlanguages with minimal compromise in performance on the majority of languages\nincluded in pretraining.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10766v1", "AI": {"title_translation": "一个分词器统领一切：通过多语言分词器实现涌现的语言可塑性", "tldr": "预训练大规模多语言大型语言模型（LLMs）面临语言覆盖和适应性挑战。本文提出使用一个通用分词器，该分词器训练的语言比主要预训练语言更多，以提高模型在后训练阶段对新语言的适应能力（语言可塑性）。实验表明，这种通用分词器显著提高了对新语言甚至完全未见语言的适应性，同时对预训练语言的性能影响最小。", "motivation": "大规模多语言大型语言模型（LLMs）的预训练面临挑战，包括模型容量有限、高质量数据稀缺和计算资源限制。此外，分词器缺乏语言覆盖范围，使得在后训练阶段弥补新语言的差距变得困难。本文旨在研究如何在训练早期通过相对廉价的干预措施来提高模型的“语言可塑性”，即模型在后训练阶段对新语言的适应能力。", "method": "研究分词器设计，并提出使用一个通用分词器，该分词器训练的语言比主要的预训练语言更多。通过对不同语种和不同训练策略进行系统实验来验证其有效性。", "result": "通用分词器能够显著提高语言适应性，与特定于预训练语言的分词器相比，胜率提高了高达20.2%。此外，通用分词器还对分词器和预训练中完全未见的语言表现出更好的可塑性，胜率提高了高达5%。实现这种对扩展语言集的适应，同时对预训练中包含的大多数语言的性能影响最小。", "conclusion": "在训练早期使用通用分词器是一种有效且相对廉价的干预措施，可以显著提高大型语言模型对新语言甚至未见语言的语言可塑性和适应能力，同时不影响主要预训练语言的性能。", "translation": "大规模多语言大型语言模型（LLMs）同时预训练多种语言面临挑战，原因在于模型容量有限、高质量数据稀缺以及计算资源限制。此外，分词器缺乏语言覆盖范围，使得纯粹在后训练阶段解决新语言的差距变得更加困难。在这项工作中，我们研究了在训练早期相对廉价的干预措施如何改善“语言可塑性”，即模型在后训练阶段对新语言的适应能力。我们专注于分词器设计，并提出使用一个通用分词器，该分词器训练的语言比主要预训练语言更多，以实现在预训练后扩展语言覆盖范围时的有效适应。我们对不同语种和不同训练策略进行的系统实验表明，通用分词器能够显著提高语言适应性，与特定于预训练语言的分词器相比，胜率提高了高达20.2%。此外，通用分词器还对分词器和预训练中完全未见的语言表现出更好的可塑性，胜率提高了高达5%。我们实现了对扩展语言集的适应，同时对预训练中包含的大多数语言的性能影响最小。", "summary": "本文针对大规模多语言大型语言模型（LLMs）预训练中遇到的语言覆盖和适应性挑战，提出了一种新颖的分词器设计方法。研究提出使用一种“通用分词器”，该分词器训练的语言数量多于主要预训练语言，旨在增强模型的“语言可塑性”，即模型在后训练阶段对新语言的适应能力。实验结果表明，与传统分词器相比，这种通用分词器显著提高了模型对已知和完全未见语言的适应性，胜率分别提升高达20.2%和5%，同时对主要预训练语言的性能影响极小。这表明该方法提供了一种经济有效的方式来提升多语言LLM的泛化能力。", "keywords": "多语言LLMs, 分词器设计, 语言可塑性, 语言适应性, 通用分词器", "comments": "这篇论文为多语言大型语言模型（LLM）开发中的一个主要挑战——语言覆盖和适应性——提供了一个创新且实用的解决方案。作者提出的“通用分词器”理念，即使用比主要预训练语言更广泛的语言集进行训练，是一种巧妙且看似“廉价”的干预措施，却在语言可塑性方面带来了显著改进。其对“未见”语言的适应能力提升尤其值得关注，这表明了强大的泛化能力。这项工作有望对提升LLM在全球更广泛语言范围内的可用性和性能产生重大影响。"}}
{"id": "2506.10465", "title": "MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models", "authors": ["Yu Huang", "Zelin Peng", "Yichen Zhao", "Piao Yang", "Xiaokang Yang", "Wei Shen"], "summary": "Medical image segmentation is crucial for clinical diagnosis, yet existing\nmodels are limited by their reliance on explicit human instructions and lack\nthe active reasoning capabilities to understand complex clinical questions.\nWhile recent advancements in multimodal large language models (MLLMs) have\nimproved medical question-answering (QA) tasks, most methods struggle to\ngenerate precise segmentation masks, limiting their application in automatic\nmedical diagnosis. In this paper, we introduce medical image reasoning\nsegmentation, a novel task that aims to generate segmentation masks based on\ncomplex and implicit medical instructions. To address this, we propose\nMedSeg-R, an end-to-end framework that leverages the reasoning abilities of\nMLLMs to interpret clinical questions while also capable of producing\ncorresponding precise segmentation masks for medical images. It is built on two\ncore components: 1) a global context understanding module that interprets\nimages and comprehends complex medical instructions to generate multi-modal\nintermediate tokens, and 2) a pixel-level grounding module that decodes these\ntokens to produce precise segmentation masks and textual responses.\nFurthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the\nmedical image reasoning segmentation task. It includes over 10,000 image-mask\npairs and multi-turn conversations, automatically annotated using large\nlanguage models and refined through physician reviews. Experiments show\nMedSeg-R's superior performance across several benchmarks, achieving high\nsegmentation accuracy and enabling interpretable textual analysis of medical\nimages.", "comment": "{\\dag}: Equal contribution", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10465v1", "AI": {"title_translation": "MedSeg-R：基于多模态大语言模型的医学图像推理分割", "tldr": "MedSeg-R提出了一种新的医学图像推理分割任务，并开发了一个端到端框架MedSeg-R，利用多模态大语言模型（MLLMs）的推理能力，根据复杂指令生成精确的医学图像分割掩模，并引入了大型数据集MedSeg-QA，实验证明其性能优越。", "motivation": "现有医学图像分割模型受限于对明确人工指令的依赖，缺乏理解复杂临床问题的主动推理能力，且大多数多模态大语言模型（MLLMs）在医学问答任务中难以生成精确的分割掩模，限制了其在自动化医学诊断中的应用。", "method": "本文提出了医学图像推理分割这一新任务，并引入了MedSeg-R，一个端到端框架。MedSeg-R包含两个核心组件：1) 一个全局上下文理解模块，用于解释图像和理解复杂医学指令以生成多模态中间令牌；2) 一个像素级接地模块，用于解码这些令牌以生成精确的分割掩模和文本响应。此外，本文还引入了一个大型数据集MedSeg-QA，包含超过10,000个图像-掩模对和多轮对话，通过大语言模型自动标注并经医生审核。", "result": "实验表明，MedSeg-R在多个基准测试中表现出色，实现了高分割精度，并能够对医学图像进行可解释的文本分析。", "conclusion": "MedSeg-R成功地将多模态大语言模型的推理能力与精确的医学图像分割相结合，解决了现有模型在处理复杂和隐含医学指令方面的局限性，并通过引入新任务和数据集推动了医学图像推理分割领域的发展。", "translation": "医学图像分割对临床诊断至关重要，然而现有模型受限于对明确人工指令的依赖，且缺乏理解复杂临床问题的主动推理能力。尽管多模态大语言模型（MLLMs）的最新进展改善了医学问答（QA）任务，但大多数方法难以生成精确的分割掩模，限制了它们在自动化医学诊断中的应用。在本文中，我们引入了医学图像推理分割，这是一项旨在根据复杂和隐含的医学指令生成分割掩模的新任务。为解决此问题，我们提出了MedSeg-R，一个端到端框架，它利用多模态大语言模型的推理能力来解释临床问题，同时能够为医学图像生成相应的精确分割掩模。它建立在两个核心组件之上：1）一个全局上下文理解模块，用于解释图像并理解复杂的医学指令以生成多模态中间令牌；2）一个像素级接地模块，用于解码这些令牌以生成精确的分割掩模和文本响应。此外，我们引入了MedSeg-QA，一个为医学图像推理分割任务量身定制的大型数据集。它包括超过10,000个图像-掩模对和多轮对话，通过大语言模型自动标注并经医生审核。实验表明，MedSeg-R在多个基准测试中表现出色，实现了高分割精度，并能够对医学图像进行可解释的文本分析。", "summary": "本文提出了一项名为“医学图像推理分割”的新任务，旨在根据复杂和隐含的医学指令生成精确的分割掩模。为应对此挑战，研究人员开发了MedSeg-R框架，该框架利用多模态大语言模型（MLLMs）的推理能力来解释临床问题，并通过全局上下文理解和像素级接地模块生成高精度的分割掩模和文本响应。同时，论文还构建了一个大型数据集MedSeg-QA，包含万余对图像-掩模和多轮对话。实验结果验证了MedSeg-R在多个基准上的优越性能，实现了高分割精度和可解释的文本分析。", "keywords": "医学图像分割, 多模态大语言模型, 推理分割, MedSeg-R, MedSeg-QA", "comments": "这篇论文的创新之处在于提出了“医学图像推理分割”这一新颖任务，解决了现有医学图像分割模型在处理复杂、隐含指令和缺乏主动推理能力方面的局限性。MedSeg-R框架结合了MLLMs的强大推理能力与精确像素级分割，具有重要的实践意义。此外，构建大规模、高质量的MedSeg-QA数据集对于推动该领域的研究具有里程碑意义。该研究有望显著提升自动化医学诊断的准确性和可解释性。"}}
{"id": "2506.10021", "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop", "authors": ["Jordi de la Torre"], "summary": "We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.", "comment": null, "cate": "cs.PL", "url": "http://arxiv.org/abs/2506.10021v1", "AI": {"title_translation": "从工具调用到符号思维：LLM在持久Lisp元编程循环中", "tldr": "该论文提出了一种新颖的架构，将大型语言模型（LLM）与持久的Lisp环境集成，使LLM能够通过程序化交互定义、调用和演化自己的工具，从而实现有状态的外部记忆、反射编程和动态工具创建。", "motivation": "本文旨在提出一种将大型语言模型（LLM）与持久、交互式Lisp环境集成的新架构，以使LLM能够通过程序化交互定义、调用和演化自己的工具，并为未来集成符号编程与神经语言生成的交互式AI系统提供设计框架和架构原则。", "method": "该方法提出了一种新颖的架构，通过在生成中嵌入Lisp表达式并通过中间件层拦截，使LLM能够通过与实时REPL的程序化交互来定义、调用和演化其工具。这允许实现有状态的外部记忆、反射编程和动态工具创建。", "result": "论文提出了一个设计框架和架构原则，用于指导未来集成符号编程与神经语言生成的交互式AI系统的实现。", "conclusion": "该论文提出了一种将LLM与Lisp环境深度集成的新颖架构，通过实现动态工具创建、反射编程和有状态外部记忆，为未来结合符号编程和神经语言生成的交互式AI系统提供了重要的设计框架和原则。", "translation": "我们提出了一种将大型语言模型（LLM）与持久、交互式Lisp环境集成的新颖架构。这种设置使LLM能够通过与实时REPL的程序化交互来定义、调用和演化自己的工具。通过在生成中嵌入Lisp表达式并通过中间件层拦截它们，该系统允许有状态的外部记忆、反射编程和动态工具创建。我们提出了一个设计框架和架构原则，以指导未来集成符号编程与神经语言生成的交互式AI系统的实现。", "summary": "本文提出了一种将大型语言模型（LLM）与持久、交互式Lisp环境集成的新颖架构。该架构允许LLM通过与实时REPL的程序化交互来定义、调用和演化自己的工具。通过在LLM生成中嵌入和拦截Lisp表达式，系统实现了有状态的外部记忆、反射编程和动态工具创建。论文还提出了一个设计框架和架构原则，旨在指导未来结合符号编程与神经语言生成的交互式AI系统的开发。", "keywords": "LLM, Lisp, 元编程, 符号思维, 工具创建", "comments": "这篇论文的创新之处在于提出了LLM与Lisp环境的深度集成，超越了传统的工具调用，实现了更高级的元编程能力。它在弥合神经AI和符号AI之间的鸿沟方面具有重要意义，有望带来更健壮和自适应的AI系统。"}}
{"id": "2506.10352", "title": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials", "authors": ["Binyao Guo", "Zihan Lin", "QiZhi He"], "summary": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10352v1", "AI": {"title_translation": "历史感知神经算子：路径依赖材料的鲁棒数据驱动本构建模", "tldr": "HANO是一种新型的神经网络算子，用于对路径依赖性材料进行数据驱动建模。它通过利用短期应变-应力历史来预测材料响应，克服了传统循环神经网络（RNN）模型中常见的自洽性问题和对初始隐藏状态的敏感性，并在各种复杂条件下表现出卓越的鲁棒性和准确性。", "motivation": "现有基于循环神经网络（RNN）的路径依赖材料建模方法存在自洽性问题和对初始隐藏状态的敏感性，导致模型不稳定且泛化能力差。本研究旨在开发一种更鲁棒、端到端的数据驱动框架来解决这些挑战。", "method": "本研究提出了历史感知神经算子（HANO），这是一种自回归模型，能够从短期应变-应力历史片段中预测路径依赖材料响应，而无需依赖隐藏状态变量。HANO以傅里叶基神经算子为骨干，实现了离散化不变学习。为了捕获全局加载模式和关键局部路径依赖性，HANO嵌入了分层自注意力机制以促进多尺度特征提取。它将应力-应变演化建模为连续算子，而非固定的输入-输出映射。", "result": "HANO在两个基准问题上（含硬化的弹塑性和脆性固体中的渐进各向异性损伤）进行了评估。结果表明，HANO在预测精度、泛化能力和鲁棒性方面始终优于基线模型。它在复杂条件下（包括不规则采样、多循环加载、噪声数据和预应力状态）表现出鲁棒性能。", "conclusion": "HANO为模拟非弹性材料提供了一种有效的数据驱动替代方案，并且非常适合与经典的数值求解器集成。", "translation": "本研究提出了一种用于路径依赖非弹性材料数据驱动建模的端到端学习框架，该框架使用神经算子。该框架建立在以下前提之上：材料响应的不可逆演化（由隐藏动力学控制）可以从可观测数据中推断出来。\n我们开发了历史感知神经算子（HANO），这是一种自回归模型，可以从最近应变-应力历史的短期片段中预测路径依赖材料响应，而无需依赖隐藏状态变量，从而克服了循环神经网络（RNN）模型中常见的自洽性问题。HANO以傅里叶基神经算子为骨干，实现了离散化不变学习。为了增强其捕获全局加载模式和关键局部路径依赖性的能力，我们嵌入了分层自注意力机制以促进多尺度特征提取。\n除了确保自洽性，HANO还减轻了对初始隐藏状态的敏感性，这是一个常被忽视的问题，可能导致循环模型在应用于广义加载路径时出现不稳定性。通过将应力-应变演化建模为连续算子，而非依赖固定的输入-输出映射，HANO自然地适应了不同的路径离散化，并在复杂条件下（包括不规则采样、多循环加载、噪声数据和预应力状态）表现出鲁棒性能。我们在两个基准问题上评估了HANO：含硬化的弹塑性和脆性固体中的渐进各向异性损伤。结果表明，HANO在预测精度、泛化能力和鲁棒性方面始终优于基线模型。凭借其已展示的能力，HANO为模拟非弹性材料提供了一种有效的数据驱动替代方案，并且非常适合与经典的数值求解器集成。", "summary": "本研究提出了历史感知神经算子（HANO），一个用于路径依赖非弹性材料数据驱动建模的端到端学习框架。HANO是一种自回归模型，通过从短期应变-应力历史中学习来预测材料响应，避免了传统RNN模型中常见的自洽性问题和对初始隐藏状态的敏感性。HANO基于傅里叶神经算子骨干并结合分层自注意力机制，实现了离散化不变学习和多尺度特征提取。实验结果表明，HANO在预测精度、泛化能力和鲁棒性方面均优于基线模型，并在复杂加载条件下表现出强大的性能，为非弹性材料模拟提供了一种有效且可与数值求解器集成的替代方案。", "keywords": "历史感知神经算子, 路径依赖材料, 数据驱动建模, 本构建模, 神经算子", "comments": "HANO的创新性在于其独特的自回归架构，它通过直接利用短期历史数据而非隐藏状态来预测材料响应，有效解决了传统RNN在处理路径依赖材料时面临的自洽性和初始状态敏感性问题。其基于傅里叶神经算子的离散化不变学习能力以及分层自注意力机制实现的多尺度特征提取，显著提升了模型的鲁棒性和泛化能力。这项工作为复杂材料的本构建模提供了一个强大的数据驱动工具，具有重要的工程应用潜力。"}}
{"id": "2506.10769", "title": "Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs", "authors": ["Alberto Testoni", "Iacer Calixto"], "summary": "Accurate and well-calibrated uncertainty estimates are essential for\ndeploying large language models (LLMs) in high-stakes domains such as clinical\ndecision support. We present a fine-grained evaluation of uncertainty\nestimation methods for clinical multiple-choice question answering, covering\nten open-source LLMs (general-purpose, biomedical, and reasoning models) across\ntwo datasets, eleven medical specialties, and six question types. We compare\nstandard single-generation and sampling-based methods, and present a case study\nexploring simple, single-pass estimators based on behavioral signals in\nreasoning traces. These lightweight methods approach the performance of\nSemantic Entropy while requiring only one generation. Our results reveal\nsubstantial variation across specialties and question types, underscoring the\nimportance of selecting models based on both the nature of the question and\nmodel-specific strengths.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10769v1", "AI": {"title_translation": "不同的问题，不同的模型：大型语言模型在临床问答中不确定性和校准的细粒度评估", "tldr": "本文对大型语言模型在临床问答中不确定性估计和校准方法进行了细粒度评估，发现性能因专业和问题类型而异，并提出轻量级方法。", "motivation": "在临床决策支持等高风险领域部署大型语言模型（LLMs）时，准确且校准良好的不确定性估计至关重要。", "method": "对临床多项选择问答中的不确定性估计方法进行细粒度评估。研究涵盖了十个开源LLM（通用、生物医学和推理模型），跨越两个数据集、十一个医学专业和六种问题类型。比较了标准的单次生成和基于采样的方法，并提出了一个案例研究，探讨了基于推理轨迹中行为信号的简单、单次通过估计器。", "result": "轻量级方法在仅需一次生成的情况下，性能接近语义熵。结果显示在不同专业和问题类型之间存在显著差异。", "conclusion": "选择模型时，必须根据问题的性质和模型的特定优势进行。", "translation": "准确且校准良好的不确定性估计对于在临床决策支持等高风险领域部署大型语言模型（LLMs）至关重要。我们对临床多项选择问答中的不确定性估计方法进行了细粒度评估，涵盖了十个开源LLM（通用、生物医学和推理模型），跨越两个数据集、十一个医学专业和六种问题类型。我们比较了标准的单次生成和基于采样的方法，并提出了一个案例研究，探讨了基于推理轨迹中行为信号的简单、单次通过估计器。这些轻量级方法在仅需一次生成的情况下，性能接近语义熵。我们的结果揭示了在不同专业和问题类型之间存在显著差异，强调了根据问题的性质和模型特定优势来选择模型的重要性。", "summary": "本文对大型语言模型在临床多项选择问答中的不确定性估计和校准进行了细致评估。研究涵盖了多种开源LLM、医学专业和问题类型，并比较了不同估计方法，包括提出了一种基于行为信号的轻量级单次通过估计器。研究发现，模型性能在不同专业和问题类型之间存在显著差异，强调了根据具体问题和模型特点选择LLM的重要性。", "keywords": "大型语言模型, 临床问答, 不确定性估计, 模型校准, 细粒度评估", "comments": "这项研究通过细粒度评估，揭示了LLM在临床问答中不确定性估计的复杂性，并提出了实用的轻量级方法。其创新在于强调了“不同问题，不同模型”的理念，对LLM在高风险领域（如医疗）的实际部署具有重要指导意义，提醒开发者需考虑领域和任务的细微差别。"}}
{"id": "2506.10474", "title": "LLMs Are Not Yet Ready for Deepfake Image Detection", "authors": ["Shahroz Tariq", "David Nguyen", "M. A. P. Chamikara", "Tingmin Wu", "Alsharif Abuadbba", "Kristen Moore"], "summary": "The growing sophistication of deepfakes presents substantial challenges to\nthe integrity of media and the preservation of public trust. Concurrently,\nvision-language models (VLMs), large language models enhanced with visual\nreasoning capabilities, have emerged as promising tools across various domains,\nsparking interest in their applicability to deepfake detection. This study\nconducts a structured zero-shot evaluation of four prominent VLMs: ChatGPT,\nClaude, Gemini, and Grok, focusing on three primary deepfake types: faceswap,\nreenactment, and synthetic generation. Leveraging a meticulously assembled\nbenchmark comprising authentic and manipulated images from diverse sources, we\nevaluate each model's classification accuracy and reasoning depth. Our analysis\nindicates that while VLMs can produce coherent explanations and detect\nsurface-level anomalies, they are not yet dependable as standalone detection\nsystems. We highlight critical failure modes, such as an overemphasis on\nstylistic elements and vulnerability to misleading visual patterns like vintage\naesthetics. Nevertheless, VLMs exhibit strengths in interpretability and\ncontextual analysis, suggesting their potential to augment human expertise in\nforensic workflows. These insights imply that although general-purpose models\ncurrently lack the reliability needed for autonomous deepfake detection, they\nhold promise as integral components in hybrid or human-in-the-loop detection\nframeworks.", "comment": "6 pages, 3 figures, and 2 tables. paper is under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10474v1", "AI": {"title_translation": "大型语言模型尚未准备好用于深度伪造图像检测", "tldr": "本研究评估了大型视觉语言模型在深度伪造图像检测方面的能力，发现它们虽然能提供解释并检测表面异常，但作为独立的检测系统尚不可靠，不过在混合或人机协作框架中仍有潜力。", "motivation": "深度伪造技术的日益复杂对媒体的完整性和公众信任构成了巨大挑战。同时，视觉语言模型（VLMs）作为强大的工具在各个领域崭露头角，引发了人们对其在深度伪造检测中应用潜力的兴趣。", "method": "本研究对ChatGPT、Claude、Gemini和Grok这四个主流VLM进行了结构化的零样本评估，重点关注人脸替换、重演和合成生成这三种主要的深度伪造类型。研究利用精心构建的基准数据集（包含来自不同来源的真实和篡改图像），评估了每个模型的分类准确性和推理深度。", "result": "分析表明，尽管VLM能够生成连贯的解释并检测表面异常，但它们作为独立的检测系统尚不可靠。研究发现了一些关键的失败模式，例如过度强调风格元素以及容易受到误导性视觉模式（如复古美学）的影响。然而，VLM在可解释性和上下文分析方面表现出优势。", "conclusion": "这些见解表明，尽管通用模型目前缺乏自主深度伪造检测所需的可靠性，但它们有望成为混合或人机协作检测框架中不可或缺的组成部分。", "translation": "深度伪造技术的日益复杂对媒体的完整性和公众信任构成了巨大挑战。与此同时，结合了视觉推理能力的大型语言模型（VLM）已成为各个领域中前景广阔的工具，引发了人们对其在深度伪造检测中应用潜力的兴趣。本研究对ChatGPT、Claude、Gemini和Grok这四个主流VLM进行了结构化的零样本评估，重点关注人脸替换、重演和合成生成这三种主要的深度伪造类型。我们利用精心构建的基准数据集，该数据集包含来自不同来源的真实和篡改图像，评估了每个模型的分类准确性和推理深度。我们的分析表明，尽管VLM能够生成连贯的解释并检测表面异常，但它们作为独立的检测系统尚不可靠。我们强调了关键的失败模式，例如过度强调风格元素以及容易受到误导性视觉模式（如复古美学）的影响。然而，VLM在可解释性和上下文分析方面表现出优势，这表明它们有潜力增强法医工作流程中的人类专业知识。这些见解意味着，尽管通用模型目前缺乏自主深度伪造检测所需的可靠性，但它们有望成为混合或人机协作检测框架中不可或缺的组成部分。", "summary": "本研究评估了ChatGPT、Claude、Gemini和Grok等大型视觉语言模型（VLM）在零样本深度伪造图像检测中的能力。结果显示，尽管VLM能提供解释并识别表面异常，但它们作为独立的检测系统并不可靠，存在过度关注风格等失败模式。然而，VLM在可解释性和上下文分析方面的优势表明它们未来可作为混合或人机协作检测框架中的辅助工具。", "keywords": "深度伪造检测, 视觉语言模型, 零样本评估, 大语言模型, 可解释性", "comments": "这篇论文通过对主流VLM进行严格的零样本评估，揭示了当前大型模型在深度伪造检测领域的局限性，特别是在自主检测方面的不足。其创新之处在于指出了VLM的关键失败模式，并强调了其在可解释性和辅助人类专家方面的潜力，为未来深度伪造检测系统的发展提供了宝贵的见解，即可能需要结合人类专业知识或采用混合模型。"}}
{"id": "2506.10355", "title": "TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree", "authors": ["Yu-Yang Qian", "Yuan-Ze Xu", "Zhen-Yu Zhang", "Peng Zhao", "Zhi-Hua Zhou"], "summary": "Many real-world applications collect data in a streaming environment, where\nlearning tasks are encountered sequentially. This necessitates continual\nlearning (CL) to update models online, enabling adaptation to new tasks while\npreserving past knowledge to prevent catastrophic forgetting. Nowadays, with\nthe flourish of large pre-trained models (LPMs), efficiency has become\nincreasingly critical for CL, due to their substantial computational demands\nand growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of\nLow-Rank Adapters), a novel approach that constructs layer-wise adapters by\nleveraging hierarchical gradient similarity to enable efficient CL,\nparticularly for LPMs. To reduce the computational burden of task similarity\nestimation, we employ bandit techniques to develop an algorithm based on lower\nconfidence bounds to efficiently explore the task structure. Furthermore, we\nuse sparse gradient updates to facilitate parameter optimization, making the\napproach better suited for LPMs. Theoretical analysis is provided to justify\nthe rationale behind our approach, and experiments on both vision transformers\n(ViTs) and large language models (LLMs) demonstrate the effectiveness and\nefficiency of our approach across various domains, including vision and natural\nlanguage processing tasks.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10355v1", "AI": {"title_translation": "TreeLoRA：通过分层梯度相似性树引导的层级LoRA实现高效持续学习", "tldr": "TreeLoRA提出了一种高效的持续学习方法，通过分层梯度相似性树指导的层级LoRA来适应新任务并防止灾难性遗忘，特别适用于大型预训练模型。", "motivation": "现实世界应用中数据流式传输，需要持续学习来在线更新模型以适应新任务并保留旧知识，防止灾难性遗忘。对于大型预训练模型（LPMs），由于其巨大的计算需求和参数规模，持续学习的效率变得至关重要。", "method": "论文引入了TreeLoRA（低秩适配器K-D树），通过利用分层梯度相似性构建层级适配器，实现高效持续学习，尤其针对LPMs。为减少任务相似性估计的计算负担，采用基于下限置信区间的多臂赌博机技术高效探索任务结构。此外，使用稀疏梯度更新来优化参数，使其更适合LPMs。", "result": "理论分析验证了方法的合理性，在视觉Transformer (ViTs) 和大型语言模型 (LLMs) 上的实验表明，该方法在视觉和自然语言处理任务等多个领域都具有有效性和高效性。", "conclusion": "TreeLoRA通过创新的层级LoRA和梯度相似性引导，结合高效的任务结构探索和稀疏梯度更新，为大型预训练模型提供了有效且高效的持续学习解决方案。", "translation": "许多现实世界的应用在流式环境中收集数据，学习任务是顺序遇到的。这需要持续学习（CL）在线更新模型，使其能够适应新任务，同时保留过去的知识以防止灾难性遗忘。如今，随着大型预训练模型（LPMs）的蓬勃发展，由于其巨大的计算需求和不断增长的参数规模，效率对于持续学习变得越来越关键。在本文中，我们介绍了TreeLoRA（低秩适配器K-D树），这是一种新颖的方法，通过利用分层梯度相似性构建层级适配器，以实现高效的持续学习，特别是对于LPMs。为了减少任务相似性估计的计算负担，我们采用多臂赌博机技术，开发了一种基于下限置信区间的算法，以高效探索任务结构。此外，我们使用稀疏梯度更新来促进参数优化，使该方法更适合LPMs。本文提供了理论分析来证明我们方法背后的原理，并且在视觉Transformer (ViTs) 和大型语言模型 (LLMs) 上的实验证明了我们方法在视觉和自然语言处理任务等各种领域中的有效性和高效性。", "summary": "本文提出了TreeLoRA，一种针对大型预训练模型（LPMs）的高效持续学习方法。TreeLoRA通过构建由分层梯度相似性引导的层级低秩适配器（LoRAs）来适应新任务并防止灾难性遗忘。为提高效率，该方法利用多臂赌博机技术高效探索任务结构，并采用稀疏梯度更新。理论分析和在视觉Transformer及大型语言模型上的实验证明了其在视觉和自然语言处理任务中的有效性和高效性。", "keywords": "持续学习, 大型预训练模型, 低秩适配器, 梯度相似性, 效率", "comments": "TreeLoRA的创新点在于结合了低秩适配器（LoRA）与分层梯度相似性树结构，以解决大型预训练模型在持续学习中的效率问题。通过引入多臂赌博机技术来优化任务相似性估计，并采用稀疏梯度更新，进一步提升了方法对LPMs的适用性。这为处理不断增长的大模型在线学习挑战提供了一个有前景的方向。"}}
{"id": "2506.10779", "title": "Improving Named Entity Transcription with Contextual LLM-based Revision", "authors": ["Viet Anh Trinh", "Xinlu He", "Jacob Whitehill"], "summary": "With recent advances in modeling and the increasing amount of supervised\ntraining data, automatic speech recognition (ASR) systems have achieved\nremarkable performance on general speech. However, the word error rate (WER) of\nstate-of-the-art ASR remains high for named entities. Since named entities are\noften the most critical keywords, misrecognizing them can affect all downstream\napplications, especially when the ASR system functions as the front end of a\ncomplex system. In this paper, we introduce a large language model (LLM)\nrevision mechanism to revise incorrect named entities in ASR predictions by\nleveraging the LLM's reasoning ability as well as local context (e.g., lecture\nnotes) containing a set of correct named entities. Finally, we introduce the\nNER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses\nfor development and testing. On this dataset, our proposed technique achieves\nup to 30\\% relative WER reduction for named entities.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10779v1", "AI": {"title_translation": "基于上下文LLM修订的命名实体转录改进", "tldr": "ASR在命名实体识别上表现不佳，本文提出用LLM结合上下文来修正ASR的命名实体错误，并在新数据集上实现了显著的错误率降低。", "motivation": "尽管自动语音识别（ASR）系统在通用语音上表现出色，但其在命名实体上的词错误率（WER）仍然很高。由于命名实体通常是最关键的关键词，错误识别会影响所有下游应用，尤其当ASR系统作为复杂系统的前端时。", "method": "本文引入了一种大语言模型（LLM）修订机制，通过利用LLM的推理能力以及包含一组正确命名实体的局部上下文（例如，讲义）来修正ASR预测中不正确的命名实体。此外，还引入了NER-MIT-OpenCourseWare数据集，其中包含45小时的麻省理工学院课程数据用于开发和测试。", "result": "在NER-MIT-OpenCourseWare数据集上，本文提出的技术使命名实体的相对词错误率降低了高达30%。", "conclusion": "所提出的基于LLM的修订机制结合局部上下文，能够显著提高自动语音识别系统中命名实体的转录准确性。", "translation": "随着建模的最新进展和监督训练数据量的增加，自动语音识别（ASR）系统在通用语音上取得了显著的性能。然而，最先进的ASR在命名实体上的词错误率（WER）仍然很高。由于命名实体通常是最关键的关键词，错误识别它们会影响所有下游应用，尤其当ASR系统作为复杂系统的前端时。在本文中，我们引入了一种大语言模型（LLM）修订机制，通过利用LLM的推理能力以及包含一组正确命名实体的局部上下文（例如，讲义）来修正ASR预测中不正确的命名实体。最后，我们引入了NER-MIT-OpenCourseWare数据集，其中包含45小时的麻省理工学院课程数据用于开发和测试。在该数据集上，我们提出的技术使命名实体的相对词错误率降低了高达30%。", "summary": "本文针对自动语音识别（ASR）系统在命名实体识别上的高错误率问题，提出了一种基于大语言模型（LLM）的修订机制。该机制利用LLM的推理能力和局部上下文（如讲义中包含的正确命名实体）来纠正ASR的命名实体预测错误。研究团队还构建了新的NER-MIT-OpenCourseWare数据集进行评估。实验结果表明，该方法在命名实体上的相对词错误率降低了高达30%。", "keywords": "命名实体, 自动语音识别, 大语言模型, 修订, 词错误率", "comments": "本文的创新点在于利用大语言模型结合上下文信息对ASR输出的命名实体进行后修正，这是一种有效解决ASR在特定领域命名实体识别瓶颈的方法。引入新的、专门针对命名实体识别的数据集NER-MIT-OpenCourseWare也对该领域的研究具有重要意义。该方法显著降低了命名实体的错误率，对于需要高精度命名实体识别的下游应用具有重要价值。"}}
{"id": "2506.10800", "title": "Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints", "authors": ["Wei Sun", "Tingyu Qu", "Mingxiao Li", "Jesse Davis", "Marie-Francine Moens"], "summary": "Efficiently updating multilingual knowledge in large language models (LLMs),\nwhile preserving consistent factual representations across languages, remains a\nlong-standing and unresolved challenge. While deploying separate editing\nsystems for each language might seem viable, this approach incurs substantial\ncosts due to the need to manage multiple models. A more efficient solution\ninvolves integrating knowledge updates across all languages into a unified\nmodel. However, performing sequential edits across languages often leads to\ndestructive parameter interference, significantly degrading multilingual\ngeneralization and the accuracy of injected knowledge. To address this\nchallenge, we propose LangEdit, a novel null-space constrained framework\ndesigned to precisely isolate language-specific knowledge updates. The core\ninnovation of LangEdit lies in its ability to project parameter updates for\neach language onto the orthogonal complement of previous updated subspaces.\nThis approach mathematically guarantees update independence while preserving\nmultilingual generalization capabilities. We conduct a comprehensive evaluation\nacross three model architectures, six languages, and four downstream tasks,\ndemonstrating that LangEdit effectively mitigates parameter interference and\noutperforms existing state-of-the-art editing methods. Our results highlight\nits potential for enabling efficient and accurate multilingual knowledge\nupdates in LLMs. The code is available at\nhttps://github.com/VRCMF/LangEdit.git.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10800v1", "AI": {"title_translation": "通过零空间约束缓解多语言序列知识编辑中的负面干扰", "tldr": "提出LangEdit框架，通过零空间约束隔离语言特定知识更新，有效缓解多语言LLM序列知识编辑中的负面干扰，提高更新效率和准确性。", "motivation": "在大型语言模型（LLMs）中高效更新多语言知识并保持跨语言事实表征的一致性是一个长期未解决的挑战。跨语言的序列编辑常导致破坏性参数干扰，严重降低多语言泛化能力和注入知识的准确性。", "method": "提出LangEdit框架，这是一种新颖的零空间约束框架，旨在精确隔离语言特定的知识更新。其核心创新在于将每种语言的参数更新投影到之前已更新子空间的零空间（正交补集）上，数学上保证了更新的独立性，同时保留了多语言泛化能力。", "result": "LangEdit在三种模型架构、六种语言和四项下游任务上进行了全面评估，结果表明它能有效缓解参数干扰，并优于现有的最先进编辑方法。", "conclusion": "LangEdit为LLMs中高效、准确的多语言知识更新提供了潜力，并通过零空间约束成功解决了跨语言序列知识编辑中的负面干扰问题。", "translation": "在大型语言模型（LLMs）中高效更新多语言知识，同时保持跨语言事实表征的一致性，仍然是一个长期存在且未解决的挑战。虽然为每种语言部署单独的编辑系统似乎可行，但这种方法由于需要管理多个模型而导致成本高昂。更有效的解决方案是将所有语言的知识更新整合到一个统一模型中。然而，跨语言进行序列编辑常常导致破坏性参数干扰，严重降低多语言泛化能力和注入知识的准确性。为了解决这一挑战，我们提出了LangEdit，一个新颖的零空间约束框架，旨在精确隔离语言特定的知识更新。LangEdit的核心创新在于能够将每种语言的参数更新投影到之前已更新子空间的零空间（正交补集）上。这种方法在数学上保证了更新的独立性，同时保留了多语言泛化能力。我们在三种模型架构、六种语言和四项下游任务上进行了全面评估，结果表明LangEdit能有效缓解参数干扰，并优于现有的最先进编辑方法。我们的结果凸显了其在LLMs中实现高效、准确的多语言知识更新的潜力。代码可在https://github.com/VRCMF/LangEdit.git获取。", "summary": "本文提出了LangEdit，一个用于多语言序列知识编辑的新型零空间约束框架，旨在解决大型语言模型中跨语言编辑导致的负面参数干扰问题。LangEdit通过将语言特定的参数更新投影到先前更新子空间的正交补集上，确保了更新的独立性并维护了多语言泛化能力。实验结果表明，LangEdit有效缓解了干扰，并超越了现有SOTA方法，为LLMs的高效准确多语言知识更新提供了解决方案。", "keywords": "多语言知识编辑, 零空间约束, 参数干扰, 大型语言模型, 序列编辑", "comments": "LangEdit的创新之处在于其利用零空间约束来数学上保证不同语言知识更新的独立性，这有效地解决了多语言LLM中序列编辑的负面干扰问题。这种方法不仅提高了知识更新的准确性，也优化了资源利用，避免了为每种语言维护独立模型的成本，具有重要的实际应用价值。"}}
{"id": "2506.10489", "title": "Class-Incremental Learning for Honey Botanical Origin Classification with Hyperspectral Images: A Study with Continual Backpropagation", "authors": ["Guyang Zhang", "Waleed Abdulla"], "summary": "Honey is an important commodity in the global market. Honey types of\ndifferent botanical origins provide diversified flavors and health benefits,\nthus having different market values. Developing accurate and effective\nbotanical origin-distinguishing techniques is crucial to protect consumers'\ninterests. However, it is impractical to collect all the varieties of honey\nproducts at once to train a model for botanical origin differentiation.\nTherefore, researchers developed class-incremental learning (CIL) techniques to\naddress this challenge. This study examined and compared multiple CIL\nalgorithms on a real-world honey hyperspectral imaging dataset. A novel\ntechnique is also proposed to improve the performance of class-incremental\nlearning algorithms by combining with a continual backpropagation (CB)\nalgorithm. The CB method addresses the issue of loss-of-plasticity by\nreinitializing a proportion of less-used hidden neurons to inject variability\ninto neural networks. Experiments showed that CB improved the performance of\nmost CIL methods by 1-7\\%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10489v1", "AI": {"title_translation": "基于高光谱图像的蜂蜜植物来源分类的类增量学习：一项关于持续反向传播的研究", "tldr": "本研究探讨了使用类增量学习（CIL）技术对蜂蜜植物来源进行分类，并提出了一种结合持续反向传播（CB）的新方法，通过重新初始化部分隐藏神经元来提高CIL性能，实验表明CB可将大多数CIL方法的性能提高1-7%。", "motivation": "开发准确有效的植物来源识别技术对于保护消费者利益至关重要。然而，一次性收集所有蜂蜜品种来训练模型以区分植物来源是不切实际的，因此需要类增量学习技术来解决这一挑战。", "method": "本研究在一个真实的蜂蜜高光谱图像数据集上检查并比较了多种类增量学习（CIL）算法。此外，还提出了一种通过结合持续反向传播（CB）算法来提高类增量学习算法性能的新技术。CB方法通过重新初始化一部分较少使用的隐藏神经元来解决可塑性损失问题，从而向神经网络注入变异性。", "result": "实验表明，持续反向传播（CB）将大多数类增量学习（CIL）方法的性能提高了1-7%。", "conclusion": "结合持续反向传播（CB）算法可以有效提高基于高光谱图像的蜂蜜植物来源分类中类增量学习（CIL）算法的性能。", "translation": "蜂蜜是全球市场上的重要商品。不同植物来源的蜂蜜种类提供多样化的风味和健康益处，因此具有不同的市场价值。开发准确有效的植物来源区分技术对于保护消费者利益至关重要。然而，一次性收集所有蜂蜜产品来训练模型进行植物来源区分是不切实际的。因此，研究人员开发了类增量学习（CIL）技术来解决这一挑战。本研究在一个真实的蜂蜜高光谱成像数据集上检查并比较了多种CIL算法。此外，还提出了一种通过结合持续反向传播（CB）算法来提高类增量学习算法性能的新技术。CB方法通过重新初始化一部分较少使用的隐藏神经元来解决可塑性损失问题，从而向神经网络注入变异性。实验表明，CB将大多数CIL方法的性能提高了1-7%。", "summary": "本研究探讨了利用高光谱图像对蜂蜜植物来源进行类增量分类。鉴于一次性收集所有蜂蜜品种进行模型训练的不可行性，研究人员提出了结合持续反向传播（CB）算法的新方法，以提升类增量学习（CIL）算法的性能。CB通过重新初始化部分低利用率的隐藏神经元来解决神经网络的可塑性损失问题。在真实蜂蜜高光谱数据集上的实验结果表明，CB能够将大多数CIL方法的性能提高1-7%。", "keywords": "类增量学习, 持续反向传播, 蜂蜜分类, 高光谱图像, 植物来源", "comments": "本文提出了一种新颖的持续反向传播（CB）方法，通过解决神经网络中的可塑性损失问题来提高类增量学习（CIL）的性能。这种方法通过注入变异性来优化现有CIL算法，并在实际应用中（如蜂蜜植物来源分类）显示出其有效性，具有一定的创新性和实用价值。"}}
{"id": "2506.10378", "title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning", "authors": ["Jikai Jin", "Vasilis Syrgkanis", "Sham Kakade", "Hanlin Zhang"], "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10378v1", "AI": {"title_translation": "通过因果表示学习发现语言模型的层次化潜在能力", "tldr": "论文提出一种因果表示学习框架，通过控制基础模型混淆因素，发现语言模型的层次化潜在能力，并识别出通用问题解决、指令遵循和数学推理之间的因果链。", "motivation": "语言模型能力的准确评估对于模型开发至关重要，但当前的因果评估面临复杂的混淆效应和高昂的计算成本。", "method": "提出一个因果表示学习框架，将观察到的基准性能建模为少数潜在能力因素的线性变换。通过控制基础模型作为共同混淆因素，识别这些潜在因素之间的因果关系。", "result": "在包含1500多个模型和6个基准测试的数据集上，识别出一个简洁的三节点线性因果结构，解释了观察到的性能变化。揭示了从通用问题解决能力到指令遵循能力，再到数学推理能力的明确因果方向。", "conclusion": "强调在评估中仔细控制基础模型变异的重要性，这对于准确揭示潜在模型能力之间的因果关系至关重要。", "translation": "对语言模型能力进行忠实评估对于获取可操作的洞察力至关重要，这些洞察力可以指导模型开发。然而，该领域严格的因果评估面临重大的方法论挑战，包括复杂的混淆效应和与大量再训练相关的巨大计算成本。为了解决这些挑战，我们提出了一种因果表示学习框架，其中观察到的基准性能被建模为少数潜在能力因素的线性变换。至关重要的是，在适当控制基础模型作为共同混淆因素后，这些潜在因素被识别为因果相互关联。将这种方法应用于一个包含来自Open LLM排行榜的六个基准测试中评估的1500多个模型的综合数据集，我们识别出一个简洁的三节点线性因果结构，该结构可靠地解释了观察到的性能变异。对这种因果结构的进一步解释提供了超越简单数值排名的实质性科学见解：具体来说，我们揭示了一个清晰的因果方向，从通用问题解决能力开始，通过指令遵循熟练度推进，最终达到数学推理能力。我们的结果强调了在评估过程中仔细控制基础模型变异的关键作用，这是准确揭示潜在模型能力之间因果关系的关键一步。", "summary": "本文提出一种新颖的因果表示学习框架，旨在解决语言模型能力评估中存在的混淆效应和高计算成本等挑战。该框架将观察到的基准性能建模为少数潜在能力因素的线性变换，并通过控制基础模型作为混淆因素来识别这些因素之间的因果关系。将此方法应用于包含1500多个模型的综合数据集，研究人员识别出一个简洁的三节点线性因果结构，揭示了从通用问题解决能力到指令遵循能力，再到数学推理能力的明确因果方向。研究结果强调了在评估过程中仔细控制基础模型变异对于准确揭示潜在模型能力之间因果关系的关键作用。", "keywords": "因果表示学习, 语言模型, 潜在能力, 模型评估", "comments": "这项工作的创新之处在于利用因果表示学习超越了简单的模型排名，揭示了语言模型潜在能力的深层因果结构，这对于有针对性的模型开发具有重要指导意义。强调在评估中控制基础模型变异是一个关键的方法论贡献。"}}
{"id": "2506.10822", "title": "ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization", "authors": ["Zhensheng Jin", "Xinze Li", "Yifan Ji", "Chunyi Peng", "Zhenghao Liu", "Qi Shi", "Yukun Yan", "Shuo Wang", "Furong Peng", "Ge Yu"], "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nimproved the reasoning capabilities of Large Language Models (LLMs). However,\nthese methods often suffer from overthinking, leading to unnecessarily lengthy\nor redundant reasoning traces. Existing approaches attempt to mitigate this\nissue through curating multiple reasoning chains for training LLMs, but their\neffectiveness is often constrained by the quality of the generated data and\nprone to overfitting. To address the challenge, we propose Reasoning\nCompression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing\nthe accuracy and length of reasoning trajectory. Specifically, ReCUT employs a\nstepwise exploration mechanism and a long-short switched sampling strategy,\nenabling LLMs to incrementally generate diverse reasoning paths. These paths\nare evaluated and used to construct preference pairs to train two specialized\nmodels (Gemini LLMs)-one optimized for reasoning accuracy, the other for\nshorter reasoning. A final integrated model is obtained by interpolating the\nparameters of these two models. Experimental results across multiple math\nreasoning datasets and backbone models demonstrate that ReCUT significantly\nreduces reasoning lengths by approximately 30-50%, while maintaining or\nimproving reasoning accuracy compared to various baselines. All codes and data\nwill be released via https://github.com/NEUIR/ReCUT.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10822v1", "AI": {"title_translation": "ReCUT：通过逐步试探和偏好优化平衡大型语言模型中的推理长度和准确性", "tldr": "ReCUT通过逐步探索和偏好优化，在保持或提高准确性的同时，显著缩短了LLM的推理长度。", "motivation": "现有的思维链（CoT）方法导致大型语言模型（LLM）的推理路径过长或冗余，且现有缓解方法受限于生成数据的质量并易于过拟合。", "method": "ReCUT采用逐步探索机制和长短切换采样策略，使LLM能够逐步生成多样化的推理路径。这些路径被评估并用于构建偏好对，以训练两个专门模型（一个优化推理准确性，另一个优化较短推理），最终通过这两个模型的参数插值获得一个集成模型。", "result": "在多个数学推理数据集和骨干模型上的实验结果表明，ReCUT与各种基线相比，显著减少了约30-50%的推理长度，同时保持或提高了推理准确性。", "conclusion": "ReCUT提供了一种有效的方法来平衡大型语言模型的推理长度和准确性，解决了思维链推理中过长路径的问题。", "translation": "思维链（CoT）提示的最新进展显著提高了大型语言模型（LLM）的推理能力。然而，这些方法常常存在过度思考的问题，导致推理轨迹不必要地冗长或冗余。现有方法试图通过整理多个推理链来训练LLM以缓解此问题，但其有效性常受限于生成数据的质量且易于过拟合。为了解决这一挑战，我们提出了一种新颖的方法——通过逐步试探进行推理压缩（ReCUT），旨在平衡推理轨迹的准确性和长度。具体而言，ReCUT采用逐步探索机制和长短切换采样策略，使LLM能够逐步生成多样化的推理路径。这些路径被评估并用于构建偏好对，以训练两个专门的模型（Gemini LLM）——一个优化推理准确性，另一个优化较短推理。最终通过这两个模型的参数插值获得一个集成模型。在多个数学推理数据集和骨干模型上的实验结果表明，与各种基线相比，ReCUT显著减少了约30-50%的推理长度，同时保持或提高了推理准确性。所有代码和数据将通过https://github.com/NEUIR/ReCUT发布。", "summary": "ReCUT是一种新颖的方法，旨在通过逐步探索和长短切换采样生成多样化推理路径，并利用偏好优化训练两个专门模型（注重准确性和长度），最终通过参数插值集成，以在保持或提高LLM推理准确性的同时，显著缩短推理长度。", "keywords": "推理长度, 大型语言模型, 思维链, 偏好优化, 推理压缩", "comments": "ReCUT的创新之处在于其结合了逐步探索、长短切换采样和基于偏好优化的双模型训练与参数插值，有效解决了思维链推理中冗余和过长的问题，并在平衡准确性和效率方面取得了显著进展。"}}
{"id": "2506.10503", "title": "Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation", "authors": ["Shuyang Li", "Shuang Wang", "Zhuangzhuang Sun", "Jing Xiao"], "summary": "The Reference Remote Sensing Image Segmentation (RRSIS) task generates\nsegmentation masks for specified objects in images based on textual\ndescriptions, which has attracted widespread attention and research interest.\nCurrent RRSIS methods rely on multi-modal fusion backbones and semantic\nsegmentation heads but face challenges like dense annotation requirements and\ncomplex scene interpretation. To address these issues, we propose a framework\nnamed \\textit{prompt-generated semantic localization guiding Segment Anything\nModel}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse\nlocalization and fine segmentation. In coarse localization stage, a visual\ngrounding network roughly locates the text-described object. In fine\nsegmentation stage, the coordinates from the first stage guide the Segment\nAnything Model (SAM), enhanced by a clustering-based foreground point generator\nand a mask boundary iterative optimization strategy for precise segmentation.\nNotably, the second stage can be train-free, significantly reducing the\nannotation data burden for the RRSIS task. Additionally, decomposing the RRSIS\ntask into two stages allows for focusing on specific region segmentation,\navoiding interference from complex scenes.We further contribute a high-quality,\nmulti-category manually annotated dataset. Experimental validation on two\ndatasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant\nperformance improvements and surpasses existing state-of-the-art models.Our\ncode will be made publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10503v1", "AI": {"title_translation": "语义定位引导的Segment Anything模型用于参考遥感图像分割", "tldr": "本文提出了PSLG-SAM框架，将参考遥感图像分割(RRSIS)任务分解为粗定位和精细分割两阶段，显著减少了标注负担，并在RRSIS任务上实现了最先进的性能。", "motivation": "当前参考遥感图像分割（RRSIS）方法面临密集标注要求高和复杂场景解释困难的挑战。", "method": "本文提出了PSLG-SAM（prompt-generated semantic localization guiding Segment Anything Model）框架，将RRSIS任务分解为两个阶段：粗定位和精细分割。在粗定位阶段，使用视觉定位网络粗略定位文本描述的对象。在精细分割阶段，第一阶段的坐标引导Segment Anything Model (SAM)，并通过基于聚类的前景点生成器和掩模边界迭代优化策略进行增强，以实现精确分割。值得注意的是，第二阶段可以无需训练，显著减少了标注数据负担。", "result": "PSLG-SAM在RRSIS-D和RRSIS-M两个数据集上进行了实验验证，结果表明其实现了显著的性能提升，并超越了现有最先进的模型。", "conclusion": "本文提出的PSLG-SAM框架通过两阶段方法有效解决了参考遥感图像分割的标注负担和复杂场景干扰问题，并在性能上超越了现有模型。", "translation": "参考遥感图像分割 (RRSIS) 任务根据文本描述为图像中指定对象生成分割掩模，这引起了广泛的关注和研究兴趣。当前的RRSIS方法依赖于多模态融合骨干网络和语义分割头部，但面临密集标注要求和复杂场景解释等挑战。为了解决这些问题，我们提出了一个名为prompt-generated semantic localization guiding Segment Anything Model (PSLG-SAM) 的框架，它将RRSIS任务分解为粗定位和精细分割两个阶段。在粗定位阶段，视觉定位网络粗略定位文本描述的对象。在精细分割阶段，第一阶段的坐标引导Segment Anything Model (SAM)，并通过基于聚类的M前景点生成器和掩模边界迭代优化策略进行增强，以实现精确分割。值得注意的是，第二阶段可以无需训练，显著减少了RRSIS任务的标注数据负担。此外，将RRSIS任务分解为两个阶段，可以专注于特定区域分割，避免复杂场景的干扰。我们还贡献了一个高质量、多类别的手动标注数据集。在两个数据集（RRSIS-D和RRSIS-M）上的实验验证表明，PSLG-SAM实现了显著的性能提升，并超越了现有最先进的模型。我们的代码将公开可用。", "summary": "本文针对参考遥感图像分割（RRSIS）任务中高标注需求和复杂场景干扰问题，提出了一种名为PSLG-SAM（prompt-generated semantic localization guiding Segment Anything Model）的两阶段框架。该框架首先通过视觉定位网络进行粗定位，然后利用定位结果引导Segment Anything Model (SAM) 进行精细分割，并辅以前景点生成和边界优化策略。其创新之处在于第二阶段可实现免训练，大幅减轻了数据标注负担，并能有效避免复杂场景干扰。实验结果表明，PSLG-SAM在RRSIS任务上取得了显著的性能提升，并超越了现有SOTA模型。此外，本文还贡献了一个高质量的多类别标注数据集。", "keywords": "遥感图像分割, SAM, 语义定位, 两阶段, 标注效率", "comments": "本文提出的PSLG-SAM框架具有显著的创新性，它将RRSIS任务分解为粗定位和精细分割两阶段，并巧妙地利用了SAM模型的强大能力，同时通过免训练的第二阶段设计，极大地缓解了遥感图像分割任务中长期存在的标注数据匮乏问题。这对于实际应用具有重要意义，降低了模型部署和维护的成本。此外，专注于特定区域分割也有效规避了复杂场景的干扰，提升了模型的鲁棒性。贡献高质量数据集也进一步推动了该领域的研究。"}}
{"id": "2506.10389", "title": "EQA-RM: A Generative Embodied Reward Model with Test-time Scaling", "authors": ["Yuhang Chen", "Zhen Tan", "Tianlong Chen"], "summary": "Reward Models (RMs), vital for large model alignment, are underexplored for\ncomplex embodied tasks like Embodied Question Answering (EQA) where nuanced\nevaluation of agents' spatial, temporal, and logical understanding is critical\nyet not considered by generic approaches. We introduce EQA-RM, a novel\ngenerative multimodal reward model specifically architected for EQA, trained\nvia our innovative Contrastive Group Relative Policy Optimization (C-GRPO)\nstrategy to learn fine-grained behavioral distinctions. The generative nature\nof EQA-RM provides interpretable, structured reward feedback (beyond simple\nscalars), uniquely enabling test-time scaling to dynamically adjust evaluation\ngranularity, from concise scores to detailed critiques of reasoning and\ngrounding, at inference without retraining. Concurrently, we introduce\nEQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward\nmodel assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning\nQwen2-VL-2B-Instruct) achieves 61.9\\% accuracy on EQA-RM-Bench with only 700\nsamples, outperforming strong proprietary baselines, including\nGemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art\nmodels such as RoVRM and VisualPRM. The code and dataset can be found here\nhttps://github.com/UNITES-Lab/EQA-RM.", "comment": "preprint", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10389v1", "AI": {"title_translation": "EQA-RM：一种具有测试时可伸缩性的生成式具身奖励模型", "tldr": "EQA-RM是一种新型的生成式具身奖励模型，专为具身问答（EQA）设计，通过创新的训练策略和测试时可伸缩性，在评估具身任务方面表现出色，并优于现有基线。", "motivation": "奖励模型（RMs）对于大型模型对齐至关重要，但在具身问答（EQA）等复杂具身任务中，其应用尚未得到充分探索。这些任务需要对智能体的空间、时间和逻辑理解进行细致评估，而通用方法未能考虑到这一点。", "method": "本文引入了EQA-RM，这是一种新型的生成式多模态奖励模型，专门为EQA设计，并通过创新的对比组相对策略优化（C-GRPO）策略进行训练，以学习细粒度的行为区别。EQA-RM的生成性质提供可解释的、结构化的奖励反馈，并独有地支持测试时可伸缩性，以在推理时动态调整评估粒度，无需重新训练。同时，本文还引入了EQARewardBench，一个基于OpenEQA构建的新基准，用于标准化EQA奖励模型评估。", "result": "EQA-RM（微调Qwen2-VL-2B-Instruct）在EQA-RM-Bench上仅用700个样本就达到了61.9%的准确率，展现出高样本效率，并优于强大的专有基线模型，包括Gemini-2.5-Flash、GPT-4o、Claude-3.5-Haiku，以及开源的最新模型，如RoVRM和VisualPRM。", "conclusion": "EQA-RM通过其生成性质和测试时可伸缩性，为复杂具身任务（如EQA）提供了细致、可解释的评估能力，显著优于现有模型，并为具身奖励模型的标准化评估提供了新基准。", "translation": "奖励模型（RMs）对于大型模型对齐至关重要，但在具身问答（EQA）等复杂具身任务中，其应用尚未得到充分探索。这些任务需要对智能体的空间、时间和逻辑理解进行细致评估，而通用方法未能考虑到这一点。我们引入了EQA-RM，这是一种新型的生成式多模态奖励模型，专门为EQA设计，并通过我们创新的对比组相对策略优化（C-GRPO）策略进行训练，以学习细粒度的行为区别。EQA-RM的生成性质提供可解释的、结构化的奖励反馈（超越简单的标量），独有地支持测试时可伸伸缩性，以在推理时动态调整评估粒度，从简洁的分数到对推理和基础的详细评价，而无需重新训练。同时，我们引入了EQARewardBench，一个基于OpenEQA构建的新基准，用于标准化EQA奖励模型评估。EQA-RM（微调Qwen2-VL-2B-Instruct）展示了高样本效率，在仅使用700个样本的情况下，在EQA-RM-Bench上达到了61.9%的准确率，优于强大的专有基线模型，包括Gemini-2.5-Flash、GPT-4o、Claude-3.5-Haiku，以及开源的最新模型，如RoVRM和VisualPRM。代码和数据集可在此处找到：https://github.com/UNITES-Lab/EQA-RM。", "summary": "EQA-RM是一种新型的生成式多模态奖励模型，专门为具身问答（EQA）设计，用于解决现有奖励模型在复杂具身任务评估方面的不足。它通过对比组相对策略优化（C-GRPO）进行训练，能够提供可解释的、结构化的奖励反馈，并支持测试时动态调整评估粒度。该模型在EQARewardBench上仅用少量样本就取得了61.9%的准确率，表现优于多种强大的专有和开源基线模型，证明了其在具身任务细粒度评估中的有效性和高效率。", "keywords": "奖励模型, 具身问答, 生成式模型, 测试时可伸缩性, EQARewardBench", "comments": "本文的创新点在于提出了一个专门针对具身问答的生成式奖励模型EQA-RM，其独有的生成性质和测试时可伸缩性使得评估反馈更加细致和灵活。同时，引入EQARewardBench为具身奖励模型的标准化评估提供了重要工具。其在低样本量下超越了包括Gemini-2.5-Flash和GPT-4o在内的SOTA模型，展现了强大的性能和实用价值。"}}
{"id": "2506.10505", "title": "J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft", "authors": ["Jin Huang", "Mingqiang Wei", "Zikuan Li", "Hangyu Qu", "Wei Zhao", "Xinyu Bai"], "summary": "Ensuring the safety and extended operational life of fighter aircraft\nnecessitates frequent and exhaustive inspections. While surface defect\ndetection is feasible for human inspectors, manual methods face critical\nlimitations in scalability, efficiency, and consistency due to the vast surface\narea, structural complexity, and operational demands of aircraft maintenance.\nWe propose a smart surface damage detection and localization system for fighter\naircraft, termed J-DDL. J-DDL integrates 2D images and 3D point clouds of the\nentire aircraft surface, captured using a combined system of laser scanners and\ncameras, to achieve precise damage detection and localization. Central to our\nsystem is a novel damage detection network built on the YOLO architecture,\nspecifically optimized for identifying surface defects in 2D aircraft images.\nKey innovations include lightweight Fasternet blocks for efficient feature\nextraction, an optimized neck architecture incorporating Efficient Multiscale\nAttention (EMA) modules for superior feature aggregation, and the introduction\nof a novel loss function, Inner-CIOU, to enhance detection accuracy. After\ndetecting damage in 2D images, the system maps the identified anomalies onto\ncorresponding 3D point clouds, enabling accurate 3D localization of defects\nacross the aircraft surface. Our J-DDL not only streamlines the inspection\nprocess but also ensures more comprehensive and detailed coverage of large and\ncomplex aircraft exteriors. To facilitate further advancements in this domain,\nwe have developed the first publicly available dataset specifically focused on\naircraft damage. Experimental evaluations validate the effectiveness of our\nframework, underscoring its potential to significantly advance automated\naircraft inspection technologies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10505v1", "AI": {"title_translation": "J-DDL：战斗机表面损伤检测与定位系统", "tldr": "J-DDL是一种用于战斗机表面损伤检测和定位的智能系统，它结合2D图像和3D点云，并基于优化的YOLO网络实现高效、精确的损伤识别和3D定位，旨在解决人工检测的局限性。", "motivation": "确保战斗机的安全和延长其使用寿命需要频繁且彻底的检查。然而，由于飞机表面积巨大、结构复杂和维护操作需求，人工检测方法在可扩展性、效率和一致性方面存在严重局限性。", "method": "本文提出了一个名为J-DDL的战斗机智能表面损伤检测与定位系统。J-DDL通过激光扫描仪和相机组合系统捕获整个飞机表面的2D图像和3D点云，以实现精确的损伤检测和定位。该系统的核心是一个基于YOLO架构的新型损伤检测网络，专门优化用于识别2D飞机图像中的表面缺陷。主要创新包括用于高效特征提取的轻量级Fasternet块、结合高效多尺度注意力（EMA）模块以实现卓越特征聚合的优化颈部架构，以及引入新型损失函数Inner-CIOU以提高检测精度。在2D图像中检测到损伤后，系统将识别出的异常映射到相应的3D点云上，从而实现飞机表面缺陷的精确3D定位。此外，为了促进该领域的进一步发展，研究人员开发了首个专注于飞机损伤的公开数据集。", "result": "J-DDL系统不仅简化了检测流程，而且确保了对大型复杂飞机外部更全面和详细的覆盖。实验评估验证了该框架的有效性，突显了其在显著推进自动化飞机检测技术方面的潜力。", "conclusion": "J-DDL框架通过结合2D图像和3D点云，并利用优化的深度学习网络，有效解决了当前战斗机表面人工检测的局限性，其验证的有效性预示着其在自动化飞机检测技术领域具有巨大的进步潜力。", "translation": "确保战斗机的安全和延长其使用寿命需要频繁且彻底的检查。虽然人工检查员可以进行表面缺陷检测，但由于飞机表面积巨大、结构复杂和维护操作需求，人工方法在可扩展性、效率和一致性方面面临严峻挑战。我们提出了一种名为J-DDL的战斗机智能表面损伤检测与定位系统。J-DDL整合了使用激光扫描仪和相机组合系统捕获的整个飞机表面的2D图像和3D点云，以实现精确的损伤检测和定位。我们系统的核心是一个基于YOLO架构的新型损伤检测网络，专门优化用于识别2D飞机图像中的表面缺陷。主要创新包括用于高效特征提取的轻量级Fasternet块、结合高效多尺度注意力（EMA）模块以实现卓越特征聚合的优化颈部架构，以及引入新型损失函数Inner-CIOU以提高检测精度。在2D图像中检测到损伤后，系统将识别出的异常映射到相应的3D点云上，从而实现飞机表面缺陷在整个飞机表面的精确3D定位。我们的J-DDL不仅简化了检测流程，而且确保了对大型复杂飞机外部更全面和详细的覆盖。为了促进该领域的进一步发展，我们开发了首个专注于飞机损伤的公开数据集。实验评估验证了我们框架的有效性，突显了其在显著推进自动化飞机检测技术方面的潜力。", "summary": "J-DDL是一种用于战斗机表面损伤检测和定位的智能系统，旨在克服人工检测的局限性。该系统结合激光扫描仪和相机捕获的2D图像与3D点云，利用基于YOLO架构的深度学习网络进行损伤识别，并引入Fasternet块、EMA模块和Inner-CIOU损失函数进行优化。J-DDL能将2D检测结果映射到3D点云上实现精确三维定位，从而简化并增强了飞机外部检测的全面性。研究还发布了首个飞机损伤公开数据集，实验结果验证了该框架的有效性，展示了其在自动化飞机检测技术中的巨大潜力。", "keywords": "表面损伤检测, 3D定位, 战斗机, YOLO, 点云", "comments": "该论文提出了一种创新的战斗机表面损伤检测与定位系统J-DDL，通过融合2D图像和3D点云数据，并优化YOLO架构，显著提升了检测精度和定位能力。其核心创新点在于轻量级Fasternet块、EMA模块以及Inner-CIOU损失函数的引入，这些都有效地提高了模型性能。此外，首次发布飞机损伤公开数据集对该领域未来的研究具有重要推动作用，体现了其对自动化飞机检测技术发展的重要贡献。"}}
{"id": "2506.10403", "title": "Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation", "authors": ["Tzu-Heng Huang", "Harit Vishwakarma", "Frederic Sala"], "summary": "Large language models (LLMs) are widely used to evaluate the quality of LLM\ngenerations and responses, but this leads to significant challenges: high API\ncosts, uncertain reliability, inflexible pipelines, and inherent biases. To\naddress these, we introduce PAJAMA (Program-As-a-Judge for Automated Model\nAssessment), a new alternative that uses LLMs to synthesize executable judging\nprograms instead of directly scoring responses. These synthesized programs can\nbe stored and run locally, costing orders of magnitude less while providing\ninterpretable, and auditable judging logic that can be easily adapted.\nProgram-based judges mitigate biases, improving judgment consistency by 15.83%\nand reducing biased responses by 23.7% on average compared to a\nQwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a\nmodel, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of\nRewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the\nJudgeLM dataset, all at three orders of magnitude lower cost.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10403v1", "AI": {"title_translation": "是时候弹劾LLM作为评判者了：程序是评估的未来", "tldr": "PAJAMA通过使用LLM合成可执行程序而非直接评分来评估LLM生成内容，显著降低成本、提高可靠性和可解释性，并减少偏见。", "motivation": "现有的LLM作为评判者的方法面临高API成本、不确定可靠性、不灵活的流程和固有偏见等挑战。", "method": "引入PAJAMA（Program-As-a-Judge for Automated Model Assessment），它使用LLM合成可执行的评判程序，这些程序可以本地存储和运行，提供可解释和可审计的评判逻辑。", "result": "程序评判可将判断一致性提高15.83%，并将偏见响应平均减少23.7%，与基于Qwen2.5-14B的LLM作为评判者相比。当程序判断被提炼成模型时，PAJAMA在RewardBench的CHAT-HARD子集上优于LLM作为评判者，在Prometheus上性能提高了2.19%，在JudgeLM数据集上提高了8.67%，所有这些都以低三个数量级的成本实现。", "conclusion": "PAJAMA提供了一种更经济、更可靠、更灵活且偏见更少的LLM评估替代方案，通过程序作为评判者超越了传统的LLM作为评判者的方法。", "translation": "大型语言模型（LLM）被广泛用于评估LLM生成内容和响应的质量，但这导致了重大挑战：高昂的API成本、不确定的可靠性、不灵活的流程以及固有的偏见。为了解决这些问题，我们引入了PAJAMA（Program-As-a-Judge for Automated Model Assessment），这是一种新的替代方案，它使用LLM合成可执行的评判程序，而不是直接对响应进行评分。这些合成的程序可以本地存储和运行，成本降低了几个数量级，同时提供了可解释和可审计的评判逻辑，并且易于调整。基于程序的评判者减轻了偏见，与基于Qwen2.5-14B的LLM作为评判者相比，判断一致性提高了15.83%，偏见响应平均减少了23.7%。当程序判断被提炼成模型时，PAJAMA在RewardBench具有挑战性的CHAT-HARD子集上表现优于LLM作为评判者，在Prometheus上指标提高了2.19%，在JudgeLM数据集上提高了8.67%，所有这些都以低三个数量级的成本实现。", "summary": "本文提出了一种名为PAJAMA（Program-As-a-Judge for Automated Model Assessment）的新型LLM评估方法，旨在解决传统LLM作为评判者所面临的高成本、低可靠性、不灵活性和偏见等问题。PAJAMA利用LLM生成可执行的评判程序，这些程序可在本地运行，显著降低成本，并提供可解释和可审计的判断逻辑。实验结果表明，PAJAMA在提高判断一致性、减少偏见响应方面表现出色，并在特定数据集上超越了传统LLM评判方法，同时成本大幅降低。", "keywords": "LLM评估, 程序合成, PAJAMA, 偏见缓解, 成本效益", "comments": "这篇论文提出了一种创新的LLM评估范式，从直接评分转向程序合成，有效解决了传统LLM作为评判者的高成本和黑箱问题。其核心创新在于将评估逻辑编码为可执行程序，这不仅提高了透明度和可审计性，还显著降低了运行成本。该方法通过引入程序逻辑来减轻LLM固有的偏见，为LLM评估领域带来了重要的进步。"}}
{"id": "2506.10848", "title": "Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles", "authors": ["Qingyan Wei", "Yaojie Zhang", "Zhiyuan Liu", "Dongrui Liu", "Linfeng Zhang"], "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.", "comment": "11 pages; 5 figures;", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10848v1", "AI": {"title_translation": "使用SlowFast加速扩散大语言模型：三大黄金原则", "tldr": "本文提出SlowFast动态采样策略，通过自适应阶段切换和三大原则，显著加速扩散大语言模型（dLLMs）的推理过程，实现并行生成和高吞吐量。", "motivation": "现有扩散大语言模型（dLLMs）的采样策略（如基于置信度或半自回归解码）存在静态行为，导致效率低下和灵活性受限，限制了dLLMs并行令牌生成的潜力。", "method": "提出SlowFast采样，一种新颖的动态采样策略，它自适应地在探索性解码和加速解码阶段之间交替。该方法遵循确定性原则、收敛性原则和位置原则三大黄金原则，并与dLLM-Cache集成以减少冗余计算。", "result": "SlowFast采样在LLaDA上实现了高达15.63倍的加速，且准确率下降极小；与缓存结合时，加速比可达34.22倍。在吞吐量方面，该方法优于LLaMA3 8B等强大的自回归基线。", "conclusion": "精心设计的采样策略可以充分发挥扩散大语言模型（dLLMs）在快速高质量生成方面的潜力。", "translation": "基于扩散的语言模型（dLLM）通过实现并行令牌生成和显著降低推理延迟，已成为传统自回归LLM的一种有前景的替代方案。然而，现有dLLM的采样策略，例如基于置信度或半自回归解码，通常存在静态行为，导致效率低下和灵活性受限。在本文中，我们提出了SlowFast采样，一种新颖的动态采样策略，它自适应地在探索性解码和加速解码阶段之间交替。我们的方法遵循三大黄金原则：确定性原则、收敛性原则和位置原则，这些原则决定了何时何地可以自信高效地解码令牌。我们进一步将我们的策略与dLLM-Cache集成以减少冗余计算。在基准和模型上的大量实验表明，SlowFast采样在LLaDA上实现了高达15.63倍的加速，且准确率下降极小，与缓存结合时可达34.22倍。值得注意的是，我们的方法在吞吐量方面优于LLaMA3 8B等强大的自回归基线，这表明精心设计的采样可以充分发挥dLLM在快速高质量生成方面的潜力。", "summary": "本文针对扩散大语言模型（dLLMs）现有采样策略的效率和灵活性不足问题，提出了一种名为SlowFast的动态采样策略。该策略通过自适应地在探索性解码和加速解码阶段之间切换，并遵循确定性、收敛性和位置三大原则，以优化令牌的置信和高效解码。此外，SlowFast采样与dLLM-Cache相结合以减少冗余计算。实验结果表明，该方法在LLaDA上实现了显著的推理加速，最高可达34.22倍，且在吞吐量上超越了强大的自回归基线，证实了其在实现dLLMs快速高质量生成方面的巨大潜力。", "keywords": "扩散大语言模型, 采样策略, SlowFast, 加速, 并行生成", "comments": "该研究创新性地提出了动态采样策略，解决了扩散大语言模型现有采样方法的效率和灵活性问题。通过引入三大黄金原则和与缓存机制结合，实现了显著的推理速度提升，并证明了扩散模型在特定场景下超越传统自回归模型的潜力，对未来扩散模型的发展具有重要意义。"}}
{"id": "2506.10516", "title": "CogStream: Context-guided Streaming Video Question Answering", "authors": ["Zicheng Zhao", "Kangyu Wang", "Shijie Li", "Rui Qian", "Weiyao Lin", "Huabin Liu"], "summary": "Despite advancements in Video Large Language Models (Vid-LLMs) improving\nmultimodal understanding, challenges persist in streaming video reasoning due\nto its reliance on contextual information. Existing paradigms feed all\navailable historical contextual information into Vid-LLMs, resulting in a\nsignificant computational burden for visual data processing. Furthermore, the\ninclusion of irrelevant context distracts models from key details. This paper\nintroduces a challenging task called Context-guided Streaming Video Reasoning\n(CogStream), which simulates real-world streaming video scenarios, requiring\nmodels to identify the most relevant historical contextual information to\ndeduce answers for questions about the current stream. To support CogStream, we\npresent a densely annotated dataset featuring extensive and hierarchical\nquestion-answer pairs, generated by a semi-automatic pipeline. Additionally, we\npresent CogReasoner as a baseline model. It efficiently tackles this task by\nleveraging visual stream compression and historical dialogue retrieval.\nExtensive experiments prove the effectiveness of this method. Code will be\nreleased soon.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10516v1", "AI": {"title_translation": "CogStream: 上下文引导的流媒体视频问答", "tldr": "本文提出了一个名为CogStream的挑战性任务，用于流媒体视频推理，通过半自动管道生成了一个密集标注的数据集，并提出了CogReasoner作为基线模型，有效解决了计算负担和无关上下文的问题。", "motivation": "尽管Vid-LLMs在多模态理解方面有所进步，但流媒体视频推理仍面临挑战，因为它依赖上下文信息。现有范式将所有历史上下文信息输入Vid-LLMs，导致视觉数据处理的计算负担沉重，并且包含无关上下文会分散模型对关键细节的注意力。", "method": "本文提出了一个名为Context-guided Streaming Video Reasoning (CogStream) 的挑战性任务，模拟真实世界的流媒体视频场景，要求模型识别最相关的历史上下文信息来推断当前流的问题答案。为了支持CogStream，我们提出了一个通过半自动管道生成的大量分层问答对的密集标注数据集。此外，我们提出了CogReasoner作为基线模型，它通过利用视觉流压缩和历史对话检索来高效解决此任务。", "result": "广泛的实验证明了该方法的有效性。", "conclusion": "本文成功引入了CogStream任务和支持数据集，并提出了一个有效的基线模型CogReasoner，解决了流媒体视频推理中上下文依赖和计算效率的问题。", "translation": "尽管视频大型语言模型（Vid-LLMs）在改进多模态理解方面取得了进展，但由于其对上下文信息的依赖，流媒体视频推理仍然存在挑战。现有范式将所有可用的历史上下文信息输入到Vid-LLMs中，导致视觉数据处理的计算负担显著。此外，包含不相关的上下文会分散模型对关键细节的注意力。本文引入了一项名为上下文引导流媒体视频推理（CogStream）的挑战性任务，该任务模拟了真实的流媒体视频场景，要求模型识别最相关的历史上下文信息，以推断有关当前流的问题的答案。为了支持CogStream，我们提出了一个通过半自动管道生成的、包含大量分层问答对的密集标注数据集。此外，我们提出了CogReasoner作为基线模型。它通过利用视觉流压缩和历史对话检索有效地解决了这项任务。广泛的实验证明了该方法的有效性。代码将很快发布。", "summary": "本文针对流媒体视频推理中上下文信息处理的挑战，提出了一个名为CogStream的新任务，该任务要求模型识别并利用最相关的历史上下文来回答问题。为支持此任务，研究团队构建了一个包含大量分层问答对的密集标注数据集，并提出了一个名为CogReasoner的基线模型，该模型通过视觉流压缩和历史对话检索有效处理了该任务。实验证明了所提出方法的有效性。", "keywords": "流媒体视频问答, 上下文引导, CogStream, Vid-LLMs, CogReasoner", "comments": "这项工作通过引入一个模拟真实世界场景的新任务和数据集，解决了现有Vid-LLMs在处理流媒体视频上下文信息时的计算效率和相关性问题。其创新点在于强调了“上下文引导”的重要性，并提出了一个能有效筛选相关信息的基线模型，这对于未来的流媒体视频理解和问答系统具有重要意义。"}}
{"id": "2506.10404", "title": "Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height", "authors": ["Bryan Shaddy", "Brianna Binder", "Agnimitra Dasgupta", "Haitong Qin", "James Haley", "Angel Farguell", "Kyle Hilburn", "Derek V. Mallia", "Adam Kochanski", "Jan Mandel", "Assad Oberai"], "summary": "Increasing wildfire occurrence has spurred growing interest in wildfire\nspread prediction. However, even the most complex wildfire models diverge from\nobserved progression during multi-day simulations, motivating need for data\nassimilation. A useful approach to assimilating measurement data into complex\ncoupled atmosphere-wildfire models is to estimate wildfire progression from\nmeasurements and use this progression to develop a matching atmospheric state.\nIn this study, an approach is developed for estimating fire progression from\nVIIRS active fire measurements, GOES-derived ignition times, and terrain height\ndata. A conditional Generative Adversarial Network is trained with simulations\nof historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus\nallowing incorporation of WRF-SFIRE physics into estimates. Fire progression is\nsuccinctly represented by fire arrival time, and measurements for training are\nobtained by applying an approximate observation operator to WRF-SFIRE\nsolutions, eliminating need for satellite data during training. The model is\ntrained on tuples of fire arrival times, measurements, and terrain, and once\ntrained leverages measurements of real fires and corresponding terrain data to\ngenerate samples of fire arrival times. The approach is validated on five\nPacific US wildfires, with results compared against high-resolution perimeters\nmeasured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.\nThe influence of terrain height on the arrival time inference is also evaluated\nand it is observed that terrain has minimal influence when the inference is\nconditioned on satellite measurements.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10404v1", "AI": {"title_translation": "基于多模态卫星活跃火灾测量和地形高度的野火进展重建生成算法", "tldr": "本研究开发了一种利用条件生成对抗网络（cGAN）从卫星活跃火灾测量和地形数据重建野火进展的方法，并在太平洋美国野火上进行了验证，取得了0.81的Sorensen-Dice系数。", "motivation": "即使最复杂的野火模型在多日模拟中也与观测到的进展存在偏差，这促使人们需要数据同化。将测量数据同化到复杂的大气-野火耦合模型中的一个有用方法是根据测量数据估计野火进展，并利用此进展来开发匹配的大气状态。", "method": "本研究开发了一种从VIIRS活跃火灾测量、GOES衍生的点火时间和地形高度数据估计火灾进展的方法。一个条件生成对抗网络（cGAN）使用历史野火的WRF-SFIRE模拟进行训练，从而将WRF-SFIRE物理学纳入估计中。火灾进展通过火灾到达时间来表示。训练测量数据通过对WRF-SFIRE解决方案应用近似观测算子获得。模型在火灾到达时间、测量和地形的元组上进行训练，然后利用真实火灾的测量和相应的地形数据生成火灾到达时间样本。", "result": "该方法在五个太平洋美国野火上进行了验证，结果与通过飞机测量的高分辨率边界进行了比较，发现平均Sorensen-Dice系数为0.81。同时观察到，当地形高度的推断以卫星测量为条件时，地形对其影响最小。", "conclusion": "该研究成功开发并验证了一种基于生成算法从多模态卫星数据和地形高度重建野火进展的方法，并发现地形在有卫星测量条件下对推断的影响有限。", "translation": "野火发生频率的增加激发了人们对野火蔓延预测日益增长的兴趣。然而，即使是最复杂的野火模型在多日模拟中也与观测到的进展存在偏差，这促使人们需要数据同化。将测量数据同化到复杂的大气-野火耦合模型中的一个有用方法是根据测量数据估计野火进展，并利用此进展来开发匹配的大气状态。在本研究中，开发了一种从VIIRS活跃火灾测量、GOES衍生的点火时间和地形高度数据估计火灾进展的方法。一个条件生成对抗网络（cGAN）使用历史野火的WRF-SFIRE模拟进行训练，从而将WRF-SFIRE物理学纳入估计中。火灾进展通过火灾到达时间来简洁地表示，并且通过对WRF-SFIRE解决方案应用近似观测算子来获得训练测量数据，从而消除了训练期间对卫星数据的需求。该模型在火灾到达时间、测量和地形的元组上进行训练，一旦训练完成，便利用真实火灾的测量和相应的地形数据生成火灾到达时间样本。该方法在五个太平洋美国野火上进行了验证，结果与通过飞机测量的高分辨率边界进行了比较，发现平均Sorensen-Dice系数为0.81。地形高度对到达时间推断的影响也进行了评估，并观察到当地形在以卫星测量为条件进行推断时，其影响最小。", "summary": "本研究提出一种利用条件生成对抗网络（cGAN）结合多模态卫星活跃火灾测量（VIIRS、GOES）和地形高度数据来重建野火进展的方法。该cGAN通过WRF-SFIRE模型模拟的历史野火数据进行训练，以学习野火物理特性，并以火灾到达时间表示火灾进展。模型在真实野火数据上进行了验证，对五个太平洋美国野火的重建结果与高分辨率飞机测量边界的平均Sorensen-Dice系数为0.81。研究还发现，当地形推断以卫星测量为条件时，地形高度的影响最小。", "keywords": "野火进展重建, 生成对抗网络, 卫星测量, 地形高度, 数据同化", "comments": "该论文的创新点在于将条件生成对抗网络应用于野火进展重建，并通过WRF-SFIRE模拟数据进行训练，有效融合了物理模型知识。这为数据同化提供了新的思路，有助于提高野火预测的准确性。其通过近似观测算子生成训练数据的方法，避免了对真实卫星数据的直接依赖，具有一定的实用价值。然而，该方法在真实世界复杂性下的泛化能力，以及对不同区域和火灾类型的适应性仍需进一步验证。"}}
{"id": "2506.10524", "title": "ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation", "authors": ["Teerapong Panboonyuen"], "summary": "This paper introduces ALBERT, an instance segmentation model specifically\ndesigned for comprehensive car damage and part segmentation. Leveraging the\npower of Bidirectional Encoder Representations, ALBERT incorporates advanced\nlocalization mechanisms to accurately identify and differentiate between real\nand fake damages, as well as segment individual car parts. The model is trained\non a large-scale, richly annotated automotive dataset that categorizes damage\ninto 26 types, identifies 7 fake damage variants, and segments 61 distinct car\nparts. Our approach demonstrates strong performance in both segmentation\naccuracy and damage classification, paving the way for intelligent automotive\ninspection and assessment applications.", "comment": "10 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10524v1", "AI": {"title_translation": "ALBERT：用于汽车损伤评估的先进定位和双向编码器表示转换器", "tldr": "ALBERT是一个基于Transformer的实例分割模型，用于准确识别和分割汽车损伤及零部件，能区分真假损伤，并在大规模数据集上表现出色，赋能智能汽车检测。", "motivation": "旨在解决汽车损伤和零部件的全面分割问题，以实现智能汽车检测和评估应用。", "method": "本文介绍了ALBERT，一个实例分割模型，它利用双向编码器表示和先进的定位机制，能够准确识别和区分真实与虚假损伤，并分割单个汽车零部件。该模型在一个包含26种损伤类型、7种虚假损伤变体和61个独立汽车部件的大规模、标注丰富的汽车数据集上进行训练。", "result": "该方法在分割准确性和损伤分类方面均表现出强大的性能。", "conclusion": "ALBERT为智能汽车检测和评估应用铺平了道路。", "translation": "本文介绍了ALBERT，一个专门为全面的汽车损伤和零部件分割设计的实例分割模型。ALBERT利用双向编码器表示的强大功能，结合先进的定位机制，以准确识别和区分真实损伤与虚假损伤，并分割单个汽车零部件。该模型在一个大规模、标注丰富的汽车数据集上进行训练，该数据集将损伤分为26种类型，识别7种虚假损伤变体，并分割61个不同的汽车零部件。我们的方法在分割准确性和损伤分类方面均表现出强大的性能，为智能汽车检测和评估应用铺平了道路。", "summary": "ALBERT是一种基于Transformer的实例分割模型，专为汽车损伤和零部件的全面识别与分割而设计。它利用双向编码器表示和先进的定位机制，能有效区分真实与虚假损伤，并对汽车部件进行精细分割。该模型在一个包含26种损伤、7种虚假损伤及61个部件的大规模数据集上训练，并在分割准确性和损伤分类方面展现出强大性能，有望推动智能汽车检测与评估技术的发展。", "keywords": "汽车损伤评估, 实例分割, 双向编码器表示, 智能检测, ALBERT", "comments": "ALBERT的创新之处在于其结合了Bidirectional Encoder Representations和先进的定位机制，专门针对汽车损伤和零部件进行实例分割，并能区分真实与虚假损伤，这对于自动化汽车检测具有重要意义。其在大规模数据集上的训练和所展现的强大性能，预示着该模型在实际应用中具有巨大潜力。"}}
{"id": "2506.10412", "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series", "authors": ["Ching Chang", "Jeehyun Hwang", "Yidan Shi", "Haixin Wang", "Wen-Chih Peng", "Tien-Fu Chen", "Wei Wang"], "summary": "Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the\nbenchmark library can be accessed at\nhttps://anonymous.4open.science/r/IMMTSF_NeurIPS2025.", "comment": "This paper is currently under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10412v1", "AI": {"title_translation": "Time-IMM：一个用于不规则多模态多元时间序列的数据集和基准", "tldr": "本文介绍了Time-IMM数据集和IMM-TSF基准库，用于不规则多模态时间序列，并表明显式建模多模态可显著提高预测性能。", "motivation": "现实世界的时间序列数据通常是不规则、多模态且混乱的，而现有基准通常假设数据是干净、规则采样和单模态的，这在研究和实际部署之间造成了显著差距。", "method": "引入Time-IMM数据集，旨在捕获多模态多元时间序列中由原因驱动的不规则性，包含九种不规则类型。引入IMM-TSF基准库，用于不规则多模态时间序列的预测，支持异步集成和真实评估，并包含专门的融合模块，如时间戳到文本融合和多模态融合模块，支持近期感知平均和基于注意力的集成策略。", "result": "经验结果表明，在不规则时间序列数据上显式建模多模态可以显著提高预测性能。", "conclusion": "Time-IMM和IMM-TSF为在真实世界条件下推进时间序列分析奠定了基础。", "translation": "现实世界应用中的时间序列数据，例如医疗保健、气候建模和金融领域的数据，通常是不规则、多模态且混乱的，具有不同的采样率、异步模态和普遍存在的缺失值。然而，现有基准通常假设数据是干净、规则采样和单模态的，这在研究和实际部署之间造成了显著差距。我们引入了Time-IMM，一个专门设计用于捕获多模态多元时间序列中由原因驱动的不规则性的数据集。Time-IMM代表了九种不同类型的时间序列不规则性，分为基于触发器、基于约束和基于伪影的机制。作为数据集的补充，我们引入了IMM-TSF，一个用于不规则多模态时间序列预测的基准库，它支持异步集成和真实评估。IMM-TSF包括专门的融合模块，例如时间戳到文本融合模块和多模态融合模块，它们支持近期感知平均和基于注意力的集成策略。经验结果表明，在不规则时间序列数据上显式建模多模态可以显著提高预测性能。Time-IMM和IMM-TSF为在真实世界条件下推进时间序列分析奠定了基础。该数据集已公开可用，网址为https://www.kaggle.com/datasets/blacksnail789521/time-imm/data，基准库可通过https://anonymous.4open.science/r/IMMTSF_NeurIPS2025访问。", "summary": "本文介绍了Time-IMM，一个新颖的数据集，旨在通过提供具有不同采样率、异步模态和缺失值的不规则、多模态、多元时间序列数据，来弥合研究与现实世界时间序列应用之间的差距。它将不规则性分为九种类型。作为补充，论文提出了IMM-TSF基准库，用于此类数据的预测，其特点是包含用于异步集成的专用融合模块。经验结果表明，显式建模多模态显著提高了预测性能，为真实时间序列分析奠定了基础。", "keywords": "不规则时间序列, 多模态数据, 时间序列预测, 数据集, 基准", "comments": "这篇论文通过关注现实世界中混乱、不规则和多模态的数据，解决了时间序列研究中的一个关键空白，而这往往被现有基准所忽视。Time-IMM和IMM-TSF的引入填补了这一空白，为未来的研究提供了宝贵的资源。显式建模多模态能提高性能的发现也具有重要意义。"}}
{"id": "2506.10877", "title": "Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment", "authors": ["Hongda Sun", "Jiaren Peng", "Wenzhong Yang", "Liang He", "Bo Du", "Rui Yan"], "summary": "Medical dialogue systems (MDS) have emerged as crucial online platforms for\nenabling multi-turn, context-aware conversations with patients. However,\nexisting MDS often struggle to (1) identify relevant medical knowledge and (2)\ngenerate personalized, medically accurate responses. To address these\nchallenges, we propose MedRef, a novel MDS that incorporates knowledge refining\nand dynamic prompt adjustment. First, we employ a knowledge refining mechanism\nto filter out irrelevant medical data, improving predictions of critical\nmedical entities in responses. Additionally, we design a comprehensive prompt\nstructure that incorporates historical details and evident details. To enable\nreal-time adaptability to diverse patient conditions, we implement two key\nmodules, Triplet Filter and Demo Selector, providing appropriate knowledge and\ndemonstrations equipped in the system prompt. Extensive experiments on MedDG\nand KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in\nboth generation quality and medical entity accuracy, underscoring its\neffectiveness and reliability for real-world healthcare applications.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10877v1", "AI": {"title_translation": "通过知识精炼和动态提示调整增强医疗对话生成", "tldr": "MedRef通过知识精炼和动态提示调整，显著提升了医疗对话系统的生成质量和医疗实体准确性。", "motivation": "现有的医疗对话系统（MDS）在识别相关医疗知识和生成个性化、医学准确的响应方面存在困难。", "method": "本文提出了MedRef，一种新颖的医疗对话系统，它集成了知识精炼机制以过滤不相关的医疗数据，并设计了一个包含历史和证据细节的综合提示结构。为实现对不同患者状况的实时适应性，MedRef还实现了两个关键模块：三元组过滤器（Triplet Filter）和演示选择器（Demo Selector），用于在系统提示中提供适当的知识和示例。", "result": "在MedDG和KaMed基准测试上的大量实验表明，MedRef在生成质量和医疗实体准确性方面均优于现有最先进的基线方法。", "conclusion": "MedRef通过其知识精炼和动态提示调整机制，有效提高了医疗对话系统的性能，证明了其在实际医疗应用中的有效性和可靠性。", "translation": "医疗对话系统（MDS）已成为重要的在线平台，能够与患者进行多轮、上下文感知的对话。然而，现有的MDS常常难以（1）识别相关的医学知识和（2）生成个性化、医学准确的响应。为了解决这些挑战，我们提出了MedRef，一种新颖的MDS，它结合了知识精炼和动态提示调整。首先，我们采用知识精炼机制来过滤掉不相关的医疗数据，从而提高响应中关键医疗实体的预测。此外，我们设计了一个综合的提示结构，其中包含历史细节和证据细节。为了实现对不同患者状况的实时适应性，我们实现了两个关键模块：三元组过滤器和演示选择器，它们在系统提示中提供适当的知识和示例。在MedDG和KaMed基准测试上的大量实验表明，MedRef在生成质量和医疗实体准确性方面均优于现有最先进的基线方法，突显了其在现实医疗应用中的有效性和可靠性。", "summary": "本文提出了一种名为MedRef的新型医疗对话系统，旨在解决现有系统在识别相关医疗知识和生成准确响应方面的不足。MedRef通过知识精炼机制过滤不相关数据，并采用动态提示调整，结合历史和证据细节。其核心模块三元组过滤器和演示选择器确保系统能实时适应不同患者情况。实验结果表明，MedRef在生成质量和医疗实体准确性上均超越了现有基线，验证了其在医疗应用中的潜力和可靠性。", "keywords": "医疗对话系统, 知识精炼, 动态提示调整, MedRef, 医学实体准确性", "comments": "MedRef的创新之处在于其结合了知识精炼和动态提示调整，这有助于提高医疗对话的准确性和个性化。特别是三元组过滤器和演示选择器模块，展现了系统对复杂医疗场景的实时适应性，对于提升用户体验和医疗安全性具有重要意义。"}}
{"id": "2506.10528", "title": "SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance", "authors": ["Teerapong Panboonyuen"], "summary": "We present SLICK, a novel framework for precise and robust car damage\nsegmentation that leverages structural priors and domain knowledge to tackle\nreal-world automotive inspection challenges. SLICK introduces five key\ncomponents: (1) Selective Part Segmentation using a high-resolution semantic\nbackbone guided by structural priors to achieve surgical accuracy in segmenting\nvehicle parts even under occlusion, deformation, or paint loss; (2)\nLocalization-Aware Attention blocks that dynamically focus on damaged regions,\nenhancing fine-grained damage detection in cluttered and complex street scenes;\n(3) an Instance-Sensitive Refinement head that leverages panoptic cues and\nshape priors to disentangle overlapping or adjacent parts, enabling precise\nboundary alignment; (4) Cross-Channel Calibration through multi-scale channel\nattention that amplifies subtle damage signals such as scratches and dents\nwhile suppressing noise like reflections and decals; and (5) a Knowledge Fusion\nModule that integrates synthetic crash data, part geometry, and real-world\ninsurance datasets to improve generalization and handle rare cases effectively.\nExperiments on large-scale automotive datasets demonstrate SLICK's superior\nsegmentation performance, robustness, and practical applicability for insurance\nand automotive inspection workflows.", "comment": "10 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10528v1", "AI": {"title_translation": "SLICK：用于汽车保险中知识增强型汽车损坏分割的选择性定位和实例校准", "tldr": "SLICK是一个新的框架，通过结合结构先验和领域知识，实现了精确鲁棒的汽车损坏分割，并在大规模数据集上表现出卓越性能。", "motivation": "该研究旨在解决现实世界中汽车检测的挑战，提供精确且鲁棒的汽车损坏分割方案。", "method": "SLICK框架包含五个关键组件：1) 选择性零件分割，使用高分辨率语义骨干网络和结构先验实现精确零件分割；2) 定位感知注意力模块，动态关注损坏区域以增强精细损伤检测；3) 实例敏感细化头，利用全景线索和形状先验分离重叠或相邻部件，实现精确边界对齐；4) 跨通道校准，通过多尺度通道注意力增强细微损伤信号并抑制噪声；5) 知识融合模块，整合合成碰撞数据、零件几何和真实保险数据集，以提高泛化能力并有效处理罕见情况。", "result": "在大规模汽车数据集上的实验表明，SLICK具有卓越的分割性能、鲁棒性以及在保险和汽车检测工作流程中的实用性。", "conclusion": "SLICK框架通过其多组件设计和知识融合策略，显著提升了汽车损坏分割的准确性和鲁棒性，并在实际应用中展现出强大潜力。", "translation": "我们提出了SLICK，一个用于精确和鲁棒的汽车损坏分割的新颖框架，它利用结构先验和领域知识来应对现实世界的汽车检测挑战。SLICK引入了五个关键组件：(1) 选择性零件分割，使用高分辨率语义骨干网络并由结构先验引导，即使在遮挡、变形或油漆缺失的情况下，也能实现车辆零件的手术级精确分割；(2) 定位感知注意力模块，动态聚焦于损坏区域，增强在杂乱复杂街道场景中的精细损伤检测；(3) 实例敏感细化头，利用全景线索和形状先验解开重叠或相邻零件，实现精确的边界对齐；(4) 跨通道校准，通过多尺度通道注意力放大划痕和凹痕等细微损伤信号，同时抑制反射和贴花等噪声；(5) 知识融合模块，整合合成碰撞数据、零件几何和真实世界保险数据集，以提高泛化能力并有效处理罕见情况。对大规模汽车数据集的实验表明，SLICK具有卓越的分割性能、鲁棒性以及在保险和汽车检测工作流程中的实际适用性。", "summary": "SLICK是一个新颖的汽车损坏分割框架，它结合了结构先验和领域知识，以解决汽车检测中的实际挑战。该框架包含五个核心组件：选择性零件分割、定位感知注意力、实例敏感细化、跨通道校准以及知识融合模块。这些组件协同工作，实现了对车辆零件的精确分割、细微损伤的有效检测、重叠部件的精确边界对齐以及对罕见情况的更好处理。实验证明，SLICK在大规模汽车数据集上表现出卓越的性能、鲁棒性及实用性，适用于保险和汽车检测工作流程。", "keywords": "汽车损坏分割, 知识增强, 选择性定位, 实例校准, 汽车保险", "comments": "SLICK的创新之处在于其多组件的模块化设计，特别是结合了结构先验、多种注意力机制、实例级细化以及知识融合模块，以应对汽车损坏分割的复杂挑战。它有效地利用了合成数据和领域知识来提高模型的泛化能力，这对于处理现实世界中罕见或复杂情况至关重要。该方法在汽车保险和检测领域具有重要的实际应用价值。"}}
{"id": "2506.10419", "title": "Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization", "authors": ["Weiying Zhao", "Aleksei Unagaev", "Natalia Efremova"], "summary": "Soil organic carbon (SOC) monitoring often relies on selecting representative\nfield sampling locations based on environmental covariates. We propose a novel\nhybrid methodology that integrates spectral clustering - an unsupervised\nmachine learning technique with conditioned Latin hypercube sampling (cLHS) to\nenhance the representativeness of SOC sampling. In our approach, spectral\nclustering partitions the study area into $K$ homogeneous zones using\nmultivariate covariate data, and cLHS is then applied within each zone to\nselect sampling locations that collectively capture the full diversity of\nenvironmental conditions. This hybrid spectral-cLHS method ensures that even\nminor but important environmental clusters are sampled, addressing a key\nlimitation of vanilla cLHS which can overlook such areas. We demonstrate on a\nreal SOC mapping dataset that spectral-cLHS provides more uniform coverage of\ncovariate feature space and spatial heterogeneity than standard cLHS. This\nimproved sampling design has the potential to yield more accurate SOC\npredictions by providing better-balanced training data for machine learning\nmodels.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10419v1", "AI": {"title_translation": "数据驱动的土壤有机碳采样：整合谱聚类与条件拉丁超立方优化", "tldr": "提出一种结合谱聚类和条件拉丁超立方采样的混合方法（spectral-cLHS），以提高土壤有机碳（SOC）采样的代表性，解决传统cLHS可能遗漏重要环境区域的问题，并在实际数据集中显示出更均匀的覆盖效果。", "motivation": "土壤有机碳（SOC）监测依赖于代表性的现场采样点选择，但现有方法如传统条件拉丁超立方采样（cLHS）可能忽视次要但重要的环境区域，导致采样代表性不足。", "method": "提出一种名为spectral-cLHS的混合方法。该方法首先利用谱聚类（一种无监督机器学习技术）根据多变量协变量数据将研究区域划分为K个同质区域，然后在此每个区域内应用条件拉丁超立方采样（cLHS）来选择采样点，从而确保捕获环境条件的多样性。", "result": "在真实的SOC测绘数据集上，spectral-cLHS方法比标准cLHS方法在协变量特征空间和空间异质性方面提供了更均匀的覆盖。", "conclusion": "改进的采样设计（spectral-cLHS）通过为机器学习模型提供更平衡的训练数据，有望产生更准确的土壤有机碳（SOC）预测。", "translation": "土壤有机碳（SOC）监测通常依赖于根据环境协变量选择代表性的现场采样位置。我们提出了一种新颖的混合方法，该方法将谱聚类（一种无监督机器学习技术）与条件拉丁超立方采样（cLHS）相结合，以提高SOC采样的代表性。在我们的方法中，谱聚类利用多变量协变量数据将研究区域划分为K个同质区域，然后将cLHS应用于每个区域内以选择共同捕获环境条件全部多样性的采样位置。这种混合的谱聚类-cLHS方法确保即使是次要但重要的环境簇也能被采样，解决了传统cLHS可能忽略这些区域的关键局限性。我们在一个真实的SOC测绘数据集上证明，谱聚类-cLHS比标准cLHS在协变量特征空间和空间异质性方面提供了更均匀的覆盖。这种改进的采样设计有潜力通过为机器学习模型提供更平衡的训练数据，从而产生更准确的SOC预测。", "summary": "本研究提出了一种名为spectral-cLHS的新型混合采样方法，旨在提高土壤有机碳（SOC）监测的代表性。该方法结合了谱聚类（用于划分同质区域）和条件拉丁超立方采样（cLHS，用于区域内采样）。与传统cLHS相比，spectral-cLHS能更均匀地覆盖协变量特征空间和空间异质性，有效解决了传统方法可能遗漏重要环境区域的局限性。这种改进的采样设计有望为机器学习模型提供更平衡的训练数据，从而提高SOC预测的准确性。", "keywords": "土壤有机碳, 谱聚类, 条件拉丁超立方采样, 采样设计, 机器学习", "comments": "该论文提出了一种创新的数据驱动采样方法，通过结合谱聚类和cLHS，有效地解决了传统采样方法在捕捉环境多样性方面的局限性。其创新点在于将无监督学习技术引入采样设计，确保了对次要但重要环境区域的覆盖。这项工作对于提高土壤有机碳（SOC）预测的准确性具有重要意义，尤其是在需要平衡训练数据以优化机器学习模型性能的场景中。"}}
{"id": "2506.10885", "title": "Slimming Down LLMs Without Losing Their Minds", "authors": ["Qingda", "Mai"], "summary": "This paper investigates and validates the impact of fine-tuning on large\nlanguage model performance, focusing on parameter-efficient methods (LoRA and\nQLoRA). We evaluate model capabilities across three key domains: (1)\ncommonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)\nmulti-domain knowledge (MMLU-CS).\n  Our findings demonstrate that: (1) LoRA-based methods effectively improve\ntask-specific performance while maintaining computational efficiency, and (2)\nperformance strongly depends on alignment between fine-tuning dataset and\nbenchmark tasks. The study provides both theoretical insights into\nparameter-efficient mechanisms and practical guidance for developers\nimplementing efficient LLM adaptation with limited resources.", "comment": "10 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10885v1", "AI": {"title_translation": "在不损失性能的情况下精简大型语言模型", "tldr": "本文研究LoRA和QLoRA等参数高效微调方法对大型语言模型（LLM）性能的影响，发现在HellaSwag、GSM8K和MMLU-CS基准上，LoRA类方法能有效提升特定任务性能并保持计算效率，且性能与微调数据集和基准任务的对齐度密切相关。", "motivation": "研究微调，特别是参数高效方法（LoRA和QLoRA），对大型语言模型性能的影响，旨在实现LLM的精简而不损失其能力。", "method": "通过在三个关键领域（常识推理HellaSwag、数学推理GSM8K和多领域知识MMLU-CS）评估模型能力，来验证LoRA和QLoRA等参数高效微调方法的效果。", "result": "LoRA类方法能有效提升特定任务性能并保持计算效率；性能强烈依赖于微调数据集与基准任务之间的对齐程度。", "conclusion": "本研究为参数高效机制提供了理论见解，并为资源有限的开发者实现高效LLM适应提供了实践指导。", "translation": "本文研究并验证了微调对大型语言模型性能的影响，重点关注参数高效方法（LoRA和QLoRA）。我们评估了模型在三个关键领域的能力：(1) 常识推理（HellaSwag），(2) 数学推理（GSM8K），以及 (3) 多领域知识（MMLU-CS）。\n我们的研究结果表明：(1) 基于LoRA的方法能有效提升特定任务性能，同时保持计算效率；(2) 性能强烈依赖于微调数据集与基准任务之间的对齐程度。本研究为参数高效机制提供了理论见解，并为资源有限的开发者实现高效LLM适应提供了实践指导。", "summary": "本文探讨了LoRA和QLoRA等参数高效微调方法对大型语言模型性能的影响。研究通过在常识推理、数学推理和多领域知识三个基准上评估，发现LoRA类方法能有效提升特定任务性能并保持计算效率，且性能与微调数据集和基准任务的对齐度密切相关。该研究为LLM的高效适应提供了理论洞察和实践指导。", "keywords": "大型语言模型, 微调, LoRA, QLoRA, 参数高效", "comments": "该研究的创新点在于系统地验证了LoRA/QLoRA等参数高效微调方法在不同LLM能力领域的有效性，并强调了微调数据集与基准任务对齐的重要性。这对于在资源有限环境下优化LLM性能具有重要的实践指导意义。"}}
{"id": "2506.10550", "title": "ContextRefine-CLIP for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2025", "authors": ["Jing He", "Yiqing Wang", "Lingling Li", "Kexin Zhang", "Puhua Chen"], "summary": "This report presents ContextRefine-CLIP (CR-CLIP), an efficient model for\nvisual-textual multi-instance retrieval tasks. The approach is based on the\ndual-encoder AVION, on which we introduce a cross-modal attention flow module\nto achieve bidirectional dynamic interaction and refinement between visual and\ntextual features to generate more context-aware joint representations. For\nsoft-label relevance matrices provided in tasks such as EPIC-KITCHENS-100,\nCR-CLIP can work with Symmetric Multi-Similarity Loss to achieve more accurate\nsemantic alignment and optimization using the refined features. Without using\nensemble learning, the CR-CLIP model achieves 66.78mAP and 82.08nDCG on the\nEPIC-KITCHENS-100 public leaderboard, which significantly outperforms the\nbaseline model and fully validates its effectiveness in cross-modal retrieval.\nThe code will be released open-source on\nhttps://github.com/delCayr/ContextRefine-Clip", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10550v1", "AI": {"title_translation": "ContextRefine-CLIP 用于 EPIC-KITCHENS-100 多实例检索挑战赛 2025", "tldr": "ContextRefine-CLIP (CR-CLIP) 是一种基于双编码器AVION的视觉-文本多实例检索模型，通过引入跨模态注意力流和对称多相似度损失，在EPIC-KITCHENS-100数据集上显著提高了性能。", "motivation": "本研究旨在开发一种高效的视觉-文本多实例检索模型，以提高现有方法的性能，特别是在处理EPIC-KITCHENS-100等多实例检索任务中提供的软标签相关性矩阵。", "method": "本文提出了ContextRefine-CLIP (CR-CLIP) 模型，它基于双编码器AVION。该方法引入了一个跨模态注意力流模块，以实现视觉和文本特征之间的双向动态交互和细化，从而生成更具上下文感知能力的联合表示。对于EPIC-KITCHENS-100等任务中提供的软标签相关性矩阵，CR-CLIP可以与对称多相似度损失（Symmetric Multi-Similarity Loss）协同工作，使用细化后的特征实现更准确的语义对齐和优化。", "result": "CR-CLIP模型在EPIC-KITCHENS-100公共排行榜上取得了66.78mAP和82.08nDCG的成绩，显著优于基线模型，并且无需使用集成学习。", "conclusion": "CR-CLIP模型在跨模态检索任务中表现出显著的有效性，并成功验证了其在视觉-文本多实例检索方面的能力。", "translation": "本报告介绍了ContextRefine-CLIP (CR-CLIP)，一个用于视觉-文本多实例检索任务的高效模型。该方法基于双编码器AVION，在此基础上我们引入了一个跨模态注意力流模块，以实现视觉和文本特征之间的双向动态交互和细化，从而生成更具上下文感知能力的联合表示。对于EPIC-KITCHENS-100等任务中提供的软标签相关性矩阵，CR-CLIP可以与对称多相似度损失（Symmetric Multi-Similarity Loss）协同工作，使用细化后的特征实现更准确的语义对齐和优化。在不使用集成学习的情况下，CR-CLIP模型在EPIC-KITCHENS-100公共排行榜上取得了66.78mAP和82.08nDCG的成绩，这显著优于基线模型，并充分验证了其在跨模态检索中的有效性。代码将在https://github.com/delCayr/ContextRefine-Clip 开源。", "summary": "ContextRefine-CLIP (CR-CLIP) 是一种高效的视觉-文本多实例检索模型，它在双编码器AVION的基础上，通过引入跨模态注意力流模块，实现了视觉和文本特征的双向动态交互和细化，生成了更具上下文感知能力的联合表示。结合对称多相似度损失，CR-CLIP在EPIC-KITCHENS-100数据集上取得了显著优于基线模型的性能，验证了其在跨模态检索任务中的有效性。", "keywords": "ContextRefine-CLIP, 多实例检索, 跨模态注意力, EPIC-KITCHENS-100, 视觉-文本检索", "comments": "该论文提出了一种创新的跨模态注意力流机制，用于增强视觉和文本特征的交互和细化，从而生成更具上下文感知能力的联合表示。其在EPIC-KITCHENS-100数据集上显著超越基线模型的表现，证明了其在多实例检索任务中的强大潜力。该方法的效率以及无需集成学习的特点是其重要优势，表明了其独立性能的强大。"}}
{"id": "2506.10887", "title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers", "authors": ["Yixiao Huang", "Hanlin Zhu", "Tianyu Guo", "Jiantao Jiao", "Somayeh Sojoudi", "Michael I. Jordan", "Stuart Russell", "Song Mei"], "summary": "Large language models (LLMs) can acquire new knowledge through fine-tuning,\nbut this process exhibits a puzzling duality: models can generalize remarkably\nfrom new facts, yet are also prone to hallucinating incorrect information.\nHowever, the reasons for this phenomenon remain poorly understood. In this\nwork, we argue that both behaviors stem from a single mechanism known as\nout-of-context reasoning (OCR): the ability to deduce implications by\nassociating concepts, even those without a causal link. Our experiments across\nfive prominent LLMs confirm that OCR indeed drives both generalization and\nhallucination, depending on whether the associated concepts are causally\nrelated. To build a rigorous theoretical understanding of this phenomenon, we\nthen formalize OCR as a synthetic factual recall task. We empirically show that\na one-layer single-head attention-only transformer with factorized output and\nvalue matrices can learn to solve this task, while a model with combined\nweights cannot, highlighting the crucial role of matrix factorization. Our\ntheoretical analysis shows that the OCR capability can be attributed to the\nimplicit bias of gradient descent, which favors solutions that minimize the\nnuclear norm of the combined output-value matrix. This mathematical structure\nexplains why the model learns to associate facts and implications with high\nsample efficiency, regardless of whether the correlation is causal or merely\nspurious. Ultimately, our work provides a theoretical foundation for\nunderstanding the OCR phenomenon, offering a new lens for analyzing and\nmitigating undesirable behaviors from knowledge injection.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10887v1", "AI": {"title_translation": "泛化还是幻觉？理解Transformer中的脱离语境推理", "tldr": "本文认为大型语言模型（LLM）中的泛化和幻觉行为都源于脱离语境推理（OCR），并从理论和实验两方面探究了其机制。", "motivation": "大型语言模型（LLMs）在微调后能够获取新知识，但这一过程存在泛化能力强和易产生幻觉的矛盾现象，其原因尚不清楚。本文旨在理解这种现象。", "method": "作者提出泛化和幻觉都源于脱离语境推理（OCR），即通过关联概念（即使没有因果关系）来推断含义的能力。他们通过在五种LLM上进行实验来验证OCR驱动泛化和幻觉。为了建立严格的理论理解，他们将OCR形式化为一个合成的事实回忆任务，并经验性地展示了一个具有分解输出和值矩阵的单层单头注意力Transformer可以解决此任务。理论分析表明，OCR能力归因于梯度下降的隐式偏置，该偏置有利于最小化组合输出-值矩阵核范数的解决方案。", "result": "实验证实OCR确实驱动了泛化和幻觉，这取决于关联概念是否具有因果关系。经验表明，具有分解输出和值矩阵的Transformer可以学习解决OCR任务，而具有组合权重的模型则不能，这强调了矩阵分解的关键作用。理论分析揭示，OCR能力源于梯度下降的隐式偏置，该偏置倾向于最小化组合输出-值矩阵核范数的解决方案。", "conclusion": "本文为理解脱离语境推理（OCR）现象提供了理论基础，并为分析和缓解知识注入中不良行为提供了新的视角。OCR解释了LLM为何能高效地学习关联事实和推论，无论相关性是因果的还是虚假的。", "translation": "大型语言模型（LLMs）可以通过微调获取新知识，但这一过程表现出令人费解的双重性：模型可以从新事实中出色地泛化，但也容易产生不正确信息的幻觉。然而，这种现象的原因仍然知之甚少。在这项工作中，我们认为这两种行为都源于一种称为脱离语境推理（OCR）的单一机制：即通过关联概念（即使没有因果联系）来推断含义的能力。我们在五种著名的LLM上的实验证实，OCR确实驱动了泛化和幻觉，这取决于关联概念是否具有因果关系。为了对这一现象建立严谨的理论理解，我们随后将OCR形式化为一个合成的事实回忆任务。我们通过经验证明，一个具有分解输出和值矩阵的单层单头仅注意力Transformer可以学习解决此任务，而一个具有组合权重的模型则不能，这突出了矩阵分解的关键作用。我们的理论分析表明，OCR能力可以归因于梯度下降的隐式偏置，它偏向于最小化组合输出-值矩阵核范数的解决方案。这种数学结构解释了为什么模型能够以高样本效率学习关联事实和推论，无论相关性是因果的还是仅仅是虚假的。最终，我们的工作为理解OCR现象提供了理论基础，为分析和缓解知识注入中的不良行为提供了新的视角。", "summary": "本文探讨了大型语言模型（LLMs）在微调后同时表现出的强大泛化能力和幻觉现象。作者提出，这两种看似矛盾的行为都源于一个共同机制：脱离语境推理（OCR），即模型关联概念并进行推断的能力，无论这些概念之间是否存在因果关系。通过在五种LLMs上的实验，他们证实了OCR在因果关联存在时导致泛化，在因果关联缺失时导致幻觉。为了深入理解，论文将OCR形式化为一个合成任务，并从理论和经验上证明了具有分解矩阵的Transformer架构能够学习并执行OCR，这归因于梯度下降的隐式偏置，该偏置有利于低核范数解。这项工作为理解OCR现象提供了理论基础，并为解决LLM知识注入中的问题提供了新思路。", "keywords": "大型语言模型, 脱离语境推理, 泛化, 幻觉, 矩阵分解", "comments": "这项工作创新性地将LLM的泛化和幻觉统一归结为脱离语境推理（OCR）机制，并从理论和实验两方面提供了深入的解释。特别是，它揭示了矩阵分解在模型学习OCR中的关键作用，并利用梯度下降的隐式偏置来解释其数学基础。这为理解LLM行为提供了新的视角，对于未来缓解知识注入中的不良行为具有重要指导意义。"}}
{"id": "2506.10443", "title": "MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices", "authors": ["Zhaode Wang", "Jingbang Yang", "Xinyu Qian", "Shiwen Xing", "Xiaotang Jiang", "Chengfei Lv", "Shengyu Zhang"], "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na variety of tasks. However, their substantial scale leads to significant\ncomputational resource consumption during inference, resulting in high costs.\nConsequently, edge device inference presents a promising solution. The primary\nchallenges of edge inference include memory usage and inference speed. This\npaper introduces MNN-LLM, a framework specifically designed to accelerate the\ndeployment of large language models on mobile devices. MNN-LLM addresses the\nruntime characteristics of LLMs through model quantization and DRAM-Flash\nhybrid storage, effectively reducing memory usage. It rearranges weights and\ninputs based on mobile CPU instruction sets and GPU characteristics while\nemploying strategies such as multicore load balancing, mixed-precision\nfloating-point operations, and geometric computations to enhance performance.\nNotably, MNN-LLM achieves up to a 8.6x speed increase compared to current\nmainstream LLM-specific frameworks.", "comment": "7 pages, 5 figures. Published in the Proceedings of the 6th ACM\n  International Conference on Multimedia in Asia Workshops (MMAsia '24\n  Workshops). The final authenticated version is available at\n  https://dl.acm.org/doi/10.1145/3700410.3702126", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10443v1", "AI": {"title_translation": "MNN-LLM：一种用于在移动设备上快速部署大型语言模型的通用推理引擎", "tldr": "MNN-LLM是一个针对移动设备优化的LLM推理引擎，通过内存优化和性能提升，实现了高达8.6倍的推理速度加速。", "motivation": "大型语言模型（LLMs）在推理时消耗大量计算资源导致成本高昂，边缘设备推理是一个有前景的解决方案，但面临内存使用和推理速度的挑战。", "method": "MNN-LLM通过模型量化和DRAM-Flash混合存储来减少内存使用；根据移动CPU指令集和GPU特性重新排列权重和输入；并采用多核负载均衡、混合精度浮点运算和几何计算等策略来提高性能。", "result": "相比当前主流的LLM专用框架，MNN-LLM实现了高达8.6倍的速度提升。", "conclusion": "MNN-LLM成功解决了在移动设备上部署大型语言模型所面临的内存和速度挑战，显著加速了其推理过程。", "translation": "大型语言模型（LLMs）在各种任务中都表现出卓越的性能。然而，它们庞大的规模导致推理过程中大量的计算资源消耗，从而产生高昂的成本。因此，边缘设备推理提供了一个有前景的解决方案。边缘推理的主要挑战包括内存使用和推理速度。本文介绍了MNN-LLM，一个专门为加速大型语言模型在移动设备上部署而设计的框架。MNN-LLM通过模型量化和DRAM-Flash混合存储来解决LLMs的运行时特性，有效减少内存使用。它根据移动CPU指令集和GPU特性重新排列权重和输入，同时采用多核负载均衡、混合精度浮点运算和几何计算等策略来提高性能。值得注意的是，MNN-LLM比当前主流的LLM专用框架实现了高达8.6倍的速度提升。", "summary": "MNN-LLM是一个为在移动设备上快速部署大型语言模型而设计的推理框架。它通过模型量化、DRAM-Flash混合存储来优化内存使用，并通过针对移动硬件的权重和输入排列、多核负载均衡、混合精度运算和几何计算来提升推理速度。该框架显著提高了LLM在移动设备上的推理效率，实现了高达8.6倍的速度提升。", "keywords": "大型语言模型, 移动设备, 推理引擎, MNN-LLM, 量化", "comments": "该论文提出了一种创新的移动端LLM推理引擎MNN-LLM，通过结合硬件优化（如指令集、GPU特性）和软件优化（如量化、混合存储、负载均衡），有效解决了LLM在边缘设备上面临的内存和速度挑战。其显著的性能提升（高达8.6倍）表明了其在实际应用中的巨大潜力，对于推动LLM在资源受限设备上的普及具有重要意义。"}}
{"id": "2506.10896", "title": "BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP", "authors": ["Thomas Sounack", "Joshua Davis", "Brigitte Durieux", "Antoine Chaffin", "Tom J. Pollard", "Eric Lehman", "Alistair E. W. Johnson", "Matthew McDermott", "Tristan Naumann", "Charlotta Lindvall"], "summary": "Encoder-based transformer models are central to biomedical and clinical\nNatural Language Processing (NLP), as their bidirectional self-attention makes\nthem well-suited for efficiently extracting structured information from\nunstructured text through discriminative tasks. However, encoders have seen\nslower development compared to decoder models, leading to limited domain\nadaptation in biomedical and clinical settings. We introduce BioClinical\nModernBERT, a domain-adapted encoder that builds on the recent ModernBERT\nrelease, incorporating long-context processing and substantial improvements in\nspeed and performance for biomedical and clinical NLP. BioClinical ModernBERT\nis developed through continued pretraining on the largest biomedical and\nclinical corpus to date, with over 53.5 billion tokens, and addresses a key\nlimitation of prior clinical encoders by leveraging 20 datasets from diverse\ninstitutions, domains, and geographic regions, rather than relying on data from\na single source. It outperforms existing biomedical and clinical encoders on\nfour downstream tasks spanning a broad range of use cases. We release both base\n(150M parameters) and large (396M parameters) versions of BioClinical\nModernBERT, along with training checkpoints to support further research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10896v1", "AI": {"title_translation": "BioClinical ModernBERT：一种用于生物医学和临床自然语言处理的先进长上下文编码器", "tldr": "BioClinical ModernBERT是一种新的、长上下文的、高性能编码器，专门用于生物医学和临床NLP，通过大规模预训练和多源数据集克服了现有编码器的局限性。", "motivation": "现有编码器模型在生物医学和临床自然语言处理（NLP）领域的发展速度较慢，导致其领域适应性有限，且难以有效处理长上下文信息，同时过度依赖单一数据源。", "method": "研究者开发了BioClinical ModernBERT，这是一种基于ModernBERT的领域适应性编码器，其特点是整合了长上下文处理能力。该模型通过对迄今为止最大的生物医学和临床语料库（超过535亿个token）进行持续预训练，并利用来自20个不同机构、领域和地理区域的数据集，而非单一来源数据。研究者发布了该模型的基础版（1.5亿参数）和大型版（3.96亿参数）。", "result": "BioClinical ModernBERT在生物医学和临床NLP的速度和性能方面实现了实质性提升。它在涵盖广泛用例的四个下游任务上均优于现有的生物医学和临床编码器。", "conclusion": "BioClinical ModernBERT通过大规模、多样化的预训练和长上下文处理能力，成功克服了现有生物医学和临床编码器的局限性，为该领域的判别性任务提供了先进且高性能的解决方案。", "translation": "基于编码器的Transformer模型是生物医学和临床自然语言处理（NLP）的核心，因为它们的双向自注意力使其非常适合通过判别任务从非结构化文本中有效提取结构化信息。然而，与解码器模型相比，编码器的发展速度较慢，导致在生物医学和临床环境中领域适应性有限。我们引入了BioClinical ModernBERT，这是一种领域适应性编码器，它基于最近发布的ModernBERT，整合了长上下文处理能力，并在生物医学和临床NLP的速度和性能方面进行了实质性改进。BioClinical ModernBERT通过对迄今为止最大的生物医学和临床语料库（超过535亿个token）进行持续预训练而开发，并通过利用来自不同机构、领域和地理区域的20个数据集，而不是依赖单一来源的数据，解决了先前临床编码器的一个关键限制。它在涵盖广泛用例的四个下游任务上优于现有的生物医学和临床编码器。我们发布了BioClinical ModernBERT的基础版（1.5亿参数）和大型版（3.96亿参数），以及训练检查点以支持进一步研究。", "summary": "BioClinical ModernBERT是一种专为生物医学和临床NLP设计的新型长上下文编码器。它基于ModernBERT，通过在包含535亿个token的超大规模、多样化（20个数据集）语料库上持续预训练，显著提升了处理速度和性能，并在多个下游任务上超越了现有模型，解决了传统编码器领域适应性差和上下文长度受限的问题。研究者发布了该模型的两个版本及训练检查点。", "keywords": "生物医学NLP, 临床NLP, 编码器, 长上下文, Transformer", "comments": "该论文提出了一种重要的领域特定编码器BioClinical ModernBERT，通过结合长上下文处理能力和利用迄今为止最大的、多样化的生物医学和临床语料库进行预训练，有效解决了现有编码器在特定领域适应性差和数据源单一的局限性。其性能提升和模型的公开发布对生物医学和临床NLP领域的研究和应用具有重要意义。"}}
{"id": "2506.10564", "title": "Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics", "authors": ["Imanol Solano", "Julian Fierrez", "Aythami Morales", "Alejandro Peña", "Ruben Tolosana", "Francisco Zamora-Martinez", "Javier San Agustin"], "summary": "Demographic bias in high-performance face recognition (FR) systems often\neludes detection by existing metrics, especially with respect to subtle\ndisparities in the tails of the score distribution. We introduce the\nComprehensive Equity Index (CEI), a novel metric designed to address this\nlimitation. CEI uniquely analyzes genuine and impostor score distributions\nseparately, enabling a configurable focus on tail probabilities while also\nconsidering overall distribution shapes. Our extensive experiments (evaluating\nstate-of-the-art FR systems, intentionally biased models, and diverse datasets)\nconfirm CEI's superior ability to detect nuanced biases where previous methods\nfall short. Furthermore, we present CEI^A, an automated version of the metric\nthat enhances objectivity and simplifies practical application. CEI provides a\nrobust and sensitive tool for operational FR fairness assessment. The proposed\nmethods have been developed particularly for bias evaluation in face biometrics\nbut, in general, they are applicable for comparing statistical distributions in\nany problem where one is interested in analyzing the distribution tails.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10564v1", "AI": {"title_translation": "比较分布时的尾部平衡：综合公平指数（CEI）及其在操作人脸生物识别偏差评估中的应用", "tldr": "该研究引入了综合公平指数（CEI），一种新颖的指标，旨在解决现有方法在检测高性能人脸识别（FR）系统中的细微人口统计学偏差（尤其是在分数分布尾部）方面的不足。CEI通过独立分析真实和冒充者分数分布，并可配置地关注尾部概率来提升偏差检测能力。", "motivation": "现有指标在检测高性能人脸识别（FR）系统中的人口统计学偏差方面存在不足，特别是对于分数分布尾部的细微差异，导致这些偏差难以被发现。", "method": "本文引入了综合公平指数（CEI），一种新颖的指标，它独特地独立分析真实分数和冒充者分数分布，并允许配置性地关注尾部概率，同时考虑整体分布形状。此外，还提出了CEI^A，一个自动化版本，以增强客观性并简化实际应用。", "result": "广泛的实验证实了CEI在检测现有方法无法发现的细微偏差方面的卓越能力。CEI在评估最先进的FR系统、有意偏置模型和多样化数据集时表现出色。", "conclusion": "CEI为操作人脸识别公平性评估提供了一个稳健且灵敏的工具。所提出的方法特别适用于人脸生物识别中的偏差评估，但通常也适用于任何对分析分布尾部感兴趣的问题中的统计分布比较。", "translation": "高性能人脸识别（FR）系统中的人口统计学偏差常常被现有指标所忽略，尤其是在分数分布尾部的细微差异方面。我们引入了综合公平指数（CEI），这是一种旨在解决此限制的新颖指标。CEI独特地独立分析真实分数和冒充者分数分布，从而能够可配置地关注尾部概率，同时考虑整体分布形状。我们广泛的实验（评估了最先进的FR系统、有意偏置的模型和多样化的数据集）证实了CEI在检测现有方法力所不及的细微偏差方面的卓越能力。此外，我们提出了CEI^A，该指标的自动化版本，增强了客观性并简化了实际应用。CEI为操作人脸识别公平性评估提供了一个稳健且灵敏的工具。所提出的方法是专门为面部生物识别中的偏差评估而开发的，但总的来说，它们适用于任何对分析分布尾部感兴趣的问题中的统计分布比较。", "summary": "该研究提出了一种新的指标——综合公平指数（CEI），旨在解决现有方法在检测人脸识别系统分数分布尾部细微偏差方面的不足。CEI通过独立分析真实和冒充者分数分布，并允许配置性地关注尾部概率，从而更有效地识别人口统计学偏差。实验证明CEI在检测细微偏差方面优于现有方法，并且其自动化版本CEI^A进一步提高了实用性。CEI不仅适用于人脸生物识别，也适用于其他需要分析分布尾部的统计分布比较问题。", "keywords": "人脸识别, 偏差评估, 综合公平指数, 分布尾部, 公平性", "comments": "本文的创新之处在于其提出的CEI指标能够更精细地关注分数分布的尾部差异，这是现有指标的盲点。通过独立分析真实和冒充者分布，并提供可配置的尾部关注，CEI显著提升了偏差检测的敏感度，对于促进人脸识别系统的公平性具有重要意义。其普适性也使其在其他统计分布比较领域具有潜在应用价值。"}}
{"id": "2506.10532", "title": "Equivariant Neural Diffusion for Molecule Generation", "authors": ["François Cornet", "Grigory Bartosh", "Mikkel N. Schmidt", "Christian A. Naesseth"], "summary": "We introduce Equivariant Neural Diffusion (END), a novel diffusion model for\nmolecule generation in 3D that is equivariant to Euclidean transformations.\nCompared to current state-of-the-art equivariant diffusion models, the key\ninnovation in END lies in its learnable forward process for enhanced generative\nmodelling. Rather than pre-specified, the forward process is parameterized\nthrough a time- and data-dependent transformation that is equivariant to rigid\ntransformations. Through a series of experiments on standard molecule\ngeneration benchmarks, we demonstrate the competitive performance of END\ncompared to several strong baselines for both unconditional and conditional\ngeneration.", "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10532v1", "AI": {"title_translation": "分子生成的等变神经扩散", "tldr": "提出了一种新的等变神经扩散模型（END），通过可学习的前向过程在3D分子生成上表现出竞争力。", "motivation": "旨在通过引入可学习的前向过程来增强现有等变扩散模型的生成建模能力，以改进3D分子生成。", "method": "引入了等变神经扩散（END）模型，这是一种用于3D分子生成的扩散模型，对欧几里得变换是等变的。其核心创新在于可学习的前向过程，该过程通过时间依赖和数据依赖的变换进行参数化，并且对刚体变换是等变的。", "result": "END在标准分子生成基准测试中，无论是无条件生成还是条件生成，都与几个强大的基线模型相比表现出有竞争力的性能。", "conclusion": "等变神经扩散（END）模型通过其可学习的前向过程，在3D分子生成任务上取得了成功，并达到了与现有先进模型相当的性能。", "translation": "我们引入了等变神经扩散（END），这是一种新颖的用于3D分子生成的扩散模型，它对欧几里得变换是等变的。与当前最先进的等变扩散模型相比，END的关键创新在于其可学习的前向过程，用于增强生成建模。前向过程不是预先指定的，而是通过一个与时间相关且与数据相关的变换进行参数化，该变换对刚体变换是等变的。通过在标准分子生成基准上进行一系列实验，我们证明了END在无条件和条件生成方面与几个强大的基线模型相比具有竞争力的性能。", "summary": "本文提出了一种名为等变神经扩散（END）的新型扩散模型，用于3D分子生成。END模型的核心创新在于其可学习的前向过程，该过程对欧几里得变换是等变的，并能增强生成建模能力。实验结果表明，END在标准分子生成任务中，无论是在无条件还是条件生成方面，均达到了与现有先进模型相当的性能。", "keywords": "等变神经扩散, 分子生成, 扩散模型, 可学习前向过程, 3D生成", "comments": "该论文的创新点在于引入了可学习的前向过程，这使得扩散模型在处理3D分子生成时更具灵活性和适应性，有望提升生成模型的性能。"}}
{"id": "2506.10903", "title": "Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning", "authors": ["Lan Zhang", "Marco Valentino", "Andre Freitas"], "summary": "Autoformalization plays a crucial role in formal mathematical reasoning by\nenabling the automatic translation of natural language statements into formal\nlanguages. While recent advances using large language models (LLMs) have shown\npromising results, methods for automatically evaluating autoformalization\nremain underexplored. As one moves to more complex domains (e.g., advanced\nmathematics), human evaluation requires significant time and domain expertise,\nespecially as the complexity of the underlying statements and background\nknowledge increases. LLM-as-a-judge presents a promising approach for\nautomating such evaluation. However, existing methods typically employ\ncoarse-grained and generic evaluation criteria, which limit their effectiveness\nfor advanced formal mathematical reasoning, where quality hinges on nuanced,\nmulti-granular dimensions. In this work, we take a step toward addressing this\ngap by introducing a systematic, automatic method to evaluate autoformalization\ntasks. The proposed method is based on an epistemically and formally grounded\nensemble (EFG) of LLM judges, defined on criteria encompassing logical\npreservation (LP), mathematical consistency (MC), formal validity (FV), and\nformal quality (FQ), resulting in a transparent assessment that accounts for\ndifferent contributing factors. We validate the proposed framework to serve as\na proxy for autoformalization assessment within the domain of formal\nmathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM\njudges is a suitable emerging proxy for evaluation, more strongly correlating\nwith human assessments than a coarse-grained model, especially when assessing\nformal qualities. These findings suggest that LLM-as-judges, especially when\nguided by a well-defined set of atomic properties, could offer a scalable,\ninterpretable, and reliable support for evaluating formal mathematical\nreasoning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10903v1", "AI": {"title_translation": "超越黄金标准：用于形式化数学推理的LLM评判者认知集成", "tldr": "本文提出了一种基于认知和形式化基础的LLM评判者集成（EFG）方法，用于自动评估形式化数学推理中的自动形式化任务，实验证明其评估结果与人类评估更强相关。", "motivation": "自动形式化在形式化数学推理中至关重要，但其自动评估方法仍未得到充分探索。在复杂数学领域，人工评估耗时且需要专业知识。现有LLM作为评判者的方法通常采用粗粒度且通用的评估标准，限制了其在需要细致、多粒度维度评估的高级形式化数学推理中的有效性。", "method": "提出了一种系统、自动的自动形式化任务评估方法，该方法基于认知和形式化基础的LLM评判者集成（EFG）。EFG的定义标准包括逻辑保持（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ），从而实现透明的评估，并考虑了不同的贡献因素。", "result": "实验表明，EFG LLM评判者集成作为自动形式化评估的代理是合适的，与粗粒度模型相比，它与人类评估的关联性更强，尤其是在评估形式质量时。", "conclusion": "LLM作为评判者，尤其是在一组明确定义的原子属性指导下，可以为形式化数学推理的评估提供可扩展、可解释和可靠的支持。", "translation": "自动形式化通过将自然语言语句自动翻译成形式语言，在形式化数学推理中发挥着关键作用。尽管最近使用大型语言模型（LLM）取得的进展显示出有希望的结果，但自动评估自动形式化的方法仍未得到充分探索。随着人们进入更复杂的领域（例如，高等数学），人类评估需要大量时间和领域专业知识，特别是随着底层语句和背景知识复杂性的增加。LLM作为评判者提供了一种自动化此类评估的有前景的方法。然而，现有方法通常采用粗粒度且通用的评估标准，这限制了它们在高级形式化数学推理中的有效性，因为质量取决于细致、多粒度的维度。在这项工作中，我们通过引入一种系统、自动的方法来评估自动形式化任务，从而弥补这一空白。所提出的方法基于认知和形式化基础的LLM评判者集成（EFG），其定义标准涵盖了逻辑保持（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ），从而实现透明的评估，并考虑了不同的贡献因素。我们验证了所提出的框架，以作为形式数学领域自动形式化评估的代理。总的来说，我们的实验表明，EFG LLM评判者集成是一种合适的评估代理，与粗粒度模型相比，它与人类评估的关联性更强，尤其是在评估形式质量时。这些发现表明，LLM作为评判者，特别是在一组明确定义的原子属性指导下，可以为评估形式化数学推理提供可扩展、可解释和可靠的支持。", "summary": "本文提出了一种名为认知和形式化基础集成（EFG）的新型LLM评判者方法，旨在系统地、自动地评估形式化数学推理中的自动形式化任务。针对现有评估方法粗粒度和通用性的局限性，EFG通过结合逻辑保持、数学一致性、形式有效性和形式质量等多维度标准，提供了更细致和透明的评估。实验结果表明，EFG与人类评估的相关性优于传统粗粒度模型，尤其在形式质量评估方面表现突出，证实了LLM作为评判者在结构化指导下，能够为复杂数学推理评估提供可扩展、可解释和可靠的支持。", "keywords": "LLM评判者, 自动形式化, 形式化数学推理, 评估方法, 认知集成", "comments": "这项工作在LLM作为评判者领域迈出了重要一步，特别是在需要高精度和细致评估的形式化数学推理领域。其创新点在于提出了一个基于认知和形式化基础的集成模型（EFG），并通过明确定义多维度的评估标准（LP, MC, FV, FQ），解决了现有LLM评估方法粗粒度的问题。这使得评估结果更加透明和可解释，并且实验结果也支持了其与人类评估的更高相关性，尤其是在捕捉形式质量方面。这对于自动化复杂数学证明的评估具有重要意义，有望显著降低人工评估的成本和专业门槛。"}}
{"id": "2506.10567", "title": "LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System", "authors": ["Hongbeen Park", "Minjeong Park", "Giljoo Nam", "Jinkyu Kim"], "summary": "Simultaneous Localization and Mapping (SLAM) has been crucial across various\ndomains, including autonomous driving, mobile robotics, and mixed reality.\nDense visual SLAM, leveraging RGB-D camera systems, offers advantages but faces\nchallenges in achieving real-time performance, robustness, and scalability for\nlarge-scale scenes. Recent approaches utilizing neural implicit scene\nrepresentations show promise but suffer from high computational costs and\nmemory requirements. ESLAM introduced a plane-based tensor decomposition but\nstill struggled with memory growth. Addressing these challenges, we propose a\nmore efficient visual SLAM model, called LRSLAM, utilizing low-rank tensor\ndecomposition methods. Our approach, leveraging the Six-axis and CP\ndecompositions, achieves better convergence rates, memory efficiency, and\nreconstruction/localization quality than existing state-of-the-art approaches.\nEvaluation across diverse indoor RGB-D datasets demonstrates LRSLAM's superior\nperformance in terms of parameter efficiency, processing time, and accuracy,\nretaining reconstruction and localization quality. Our code will be publicly\navailable upon publication.", "comment": "Accepted at ECCV 2024", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10567v1", "AI": {"title_translation": "LRSLAM: 稠密视觉SLAM系统中符号距离场的低秩表示", "tldr": "LRSLAM通过低秩张量分解提升了稠密视觉SLAM的效率和性能。", "motivation": "稠密视觉SLAM在实时性、鲁棒性和大规模场景的可扩展性方面面临挑战；现有基于神经隐式场景表示的方法计算成本和内存需求高，ESLAM也存在内存增长问题。", "method": "提出LRSLAM模型，利用低秩张量分解方法（特别是六轴Six-axis和CP分解）来表示符号距离场。", "result": "LRSLAM在收敛速度、内存效率、重建/定位质量、参数效率、处理时间和准确性方面均优于现有最先进方法，并在多种室内RGB-D数据集上得到了验证。", "conclusion": "LRSLAM是一种更高效的视觉SLAM模型，通过引入低秩张量分解有效解决了现有稠密视觉SLAM在计算和内存方面的瓶颈，并显著提升了性能。", "translation": "同步定位与建图（SLAM）在自动驾驶、移动机器人和混合现实等多个领域都至关重要。利用RGB-D相机系统的稠密视觉SLAM具有优势，但在实现大规模场景的实时性能、鲁棒性和可扩展性方面面临挑战。最近利用神经隐式场景表示的方法显示出前景，但存在高计算成本和内存需求的问题。ESLAM引入了基于平面的张量分解，但仍存在内存增长问题。为应对这些挑战，我们提出了一种更高效的视觉SLAM模型——LRSLAM，它利用低秩张量分解方法。我们的方法利用六轴（Six-axis）和CP分解，比现有最先进的方法实现了更好的收敛速度、内存效率和重建/定位质量。对各种室内RGB-D数据集的评估表明，LRSLAM在参数效率、处理时间和准确性方面表现卓越，同时保持了重建和定位质量。我们的代码将在发布后公开。", "summary": "针对稠密视觉SLAM在实时性、鲁棒性、可扩展性以及现有神经隐式表示计算和内存成本高的问题，本文提出了LRSLAM模型。该模型利用低秩张量分解（包括六轴和CP分解）来表示符号距离场，从而实现了更优的收敛速度、内存效率和重建/定位质量。实验证明，LRSLAM在参数效率、处理时间和准确性方面均超越了现有最先进方法。", "keywords": "稠密视觉SLAM, 低秩表示, 张量分解, 符号距离场, LRSLAM", "comments": "本文通过将低秩张量分解引入稠密视觉SLAM中的符号距离场表示，有效解决了现有方法面临的计算量大和内存消耗高的问题。这种创新性的方法显著提升了系统的效率和性能，对于实现大规模、实时、高精度的稠密视觉SLAM具有重要意义。"}}
{"id": "2506.10536", "title": "Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows", "authors": ["Vasilis Michalakopoulos", "Christoforos Menos-Aikateriniadis", "Elissaios Sarmas", "Antonis Zakynthinos", "Pavlos S. Georgilakis", "Dimitris Askounis"], "summary": "This study investigates the performance of machine learning models in\nforecasting electricity Day-Ahead Market (DAM) prices using short historical\ntraining windows, with a focus on detecting seasonal trends and price spikes.\nWe evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),\nXGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,\nBelgium, Ireland) using feature sets derived from ENTSO-E forecast data.\nTraining window lengths range from 7 to 90 days, allowing assessment of model\nadaptability under constrained data availability. Results indicate that\nLightGBM consistently achieves the highest forecasting accuracy and robustness,\nparticularly with 45 and 60 day training windows, which balance temporal\nrelevance and learning depth. Furthermore, LightGBM demonstrates superior\ndetection of seasonal effects and peak price events compared to LSTM and other\nboosting models. These findings suggest that short-window training approaches,\ncombined with boosting methods, can effectively support DAM forecasting in\nvolatile, data-scarce environments.", "comment": "13 pages, 10 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10536v1", "AI": {"title_translation": "数据驱动的日前市场价格预测：侧重于短训练集窗口", "tldr": "本研究评估了在短训练窗口下，机器学习模型预测日前电力市场（DAM）价格的性能，发现LightGBM在准确性和鲁棒性方面表现最佳，尤其适用于数据稀缺环境。", "motivation": "本研究旨在探讨在有限数据可用性（短历史训练窗口）的情况下，机器学习模型在预测日前电力市场（DAM）价格方面的表现，并侧重于检测季节性趋势和价格峰值。", "method": "研究评估了四种机器学习模型：带有前馈误差校正的LSTM（FFEC）、XGBoost、LightGBM和CatBoost。这些模型在希腊、比利时和爱尔兰三个欧洲能源市场进行了测试，使用来自ENTSO-E预测数据的特征集。训练窗口长度从7天到90天不等。", "result": "结果表明，LightGBM在预测准确性和鲁棒性方面始终表现最佳，尤其是在45天和60天的训练窗口下，这平衡了时间相关性和学习深度。此外，与LSTM和其他提升模型相比，LightGBM在检测季节性效应和峰值价格事件方面表现出卓越的性能。", "conclusion": "研究结果表明，短窗口训练方法与提升方法相结合，可以有效地支持波动性大、数据稀缺环境下的日前市场价格预测。", "translation": "本研究探讨了机器学习模型在使用短历史训练窗口预测日前市场（DAM）电价方面的表现，重点在于检测季节性趋势和价格峰值。我们评估了四种模型，即带有前馈误差校正的LSTM（FFEC）、XGBoost、LightGBM和CatBoost，在希腊、比利时、爱尔兰三个欧洲能源市场使用来自ENTSO-E预测数据的特征集。训练窗口长度从7天到90天不等，这使得可以在数据可用性受限的情况下评估模型的适应性。结果表明，LightGBM始终在预测准确性和鲁棒性方面取得最高表现，特别是在45天和60天的训练窗口下，这平衡了时间相关性和学习深度。此外，与LSTM和其他提升模型相比，LightGBM在检测季节性效应和峰值价格事件方面表现出卓越的性能。这些发现表明，短窗口训练方法与提升方法相结合，可以有效地支持波动性大、数据稀缺环境下的日前市场预测。", "summary": "本研究评估了在短历史训练窗口下，机器学习模型预测日前电力市场（DAM）价格的性能，旨在检测季节性趋势和价格峰值。研究比较了LSTM-FFEC、XGBoost、LightGBM和CatBoost四种模型在三个欧洲能源市场（希腊、比利时、爱尔兰）的表现，训练窗口从7天到90天。结果显示，LightGBM在预测准确性和鲁棒性方面表现最佳，尤其是在45天和60天的训练窗口下，并且在检测季节性效应和峰值价格事件方面优于其他模型。这表明短窗口训练结合提升方法能有效支持数据稀缺环境下的DAM预测。", "keywords": "日前市场价格预测,机器学习,短训练窗口,LightGBM,电力市场", "comments": "该论文的创新点在于专注于在数据可用性受限（短训练窗口）的条件下进行日前市场价格预测，这对于实际应用中经常面临数据稀缺或需要快速适应新市场条件的情况具有重要意义。研究明确指出了LightGBM在短训练窗口下的优越性，并强调了其在捕获季节性趋势和价格尖峰方面的能力，为能源市场的预测提供了实用的指导。"}}
{"id": "2506.10910", "title": "Magistral", "authors": ["Mistral-AI", ":", "Abhinav Rastogi", "Albert Q. Jiang", "Andy Lo", "Gabrielle Berrada", "Guillaume Lample", "Jason Rute", "Joep Barmentlo", "Karmesh Yadav", "Kartik Khandelwal", "Khyathi Raghavi Chandu", "Léonard Blier", "Lucile Saulnier", "Matthieu Dinot", "Maxime Darrin", "Neha Gupta", "Roman Soletskyi", "Sagar Vaze", "Teven Le Scao", "Yihan Wang", "Adam Yang", "Alexander H. Liu", "Alexandre Sablayrolles", "Amélie Héliou", "Amélie Martin", "Andy Ehrenberg", "Anmol Agarwal", "Antoine Roux", "Arthur Darcet", "Arthur Mensch", "Baptiste Bout", "Baptiste Rozière", "Baudouin De Monicault", "Chris Bamford", "Christian Wallenwein", "Christophe Renaudin", "Clémence Lanfranchi", "Darius Dabert", "Devon Mizelle", "Diego de las Casas", "Elliot Chane-Sane", "Emilien Fugier", "Emma Bou Hanna", "Gauthier Delerce", "Gauthier Guinet", "Georgii Novikov", "Guillaume Martin", "Himanshu Jaju", "Jan Ludziejewski", "Jean-Hadrien Chabran", "Jean-Malo Delignon", "Joachim Studnia", "Jonas Amar", "Josselin Somerville Roberts", "Julien Denize", "Karan Saxena", "Kush Jain", "Lingxiao Zhao", "Louis Martin", "Luyu Gao", "Lélio Renard Lavaud", "Marie Pellat", "Mathilde Guillaumin", "Mathis Felardos", "Maximilian Augustin", "Mickaël Seznec", "Nikhil Raghuraman", "Olivier Duchenne", "Patricia Wang", "Patrick von Platen", "Patryk Saffer", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Pavankumar Reddy Muddireddy", "Philomène Chagniot", "Pierre Stock", "Pravesh Agrawal", "Romain Sauvestre", "Rémi Delacourt", "Sanchit Gandhi", "Sandeep Subramanian", "Shashwat Dalal", "Siddharth Gandhi", "Soham Ghosh", "Srijan Mishra", "Sumukh Aithal", "Szymon Antoniak", "Thibault Schueller", "Thibaut Lavril", "Thomas Robert", "Thomas Wang", "Timothée Lacroix", "Valeriia Nemychnikova", "Victor Paltz", "Virgile Richard", "Wen-Ding Li", "William Marshall", "Xuanyu Zhang", "Yunhao Tang"], "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10910v1", "AI": {"title_translation": "Magistral 模型", "tldr": "Magistral 是 Mistral 的首个推理大模型，采用全新的、可扩展的纯强化学习（RL）流水线进行训练。研究表明，仅在文本数据上进行 RL 训练能保持并提升模型能力，并且他们开源了其中一个版本。", "motivation": "开发 Mistral 的首个推理模型和可扩展的强化学习（RL）流水线，通过采用自下而上的方法，完全依赖自己的模型和基础设施，而不是现有的实现和蒸馏的 RL 轨迹。旨在探索 LLM 纯 RL 训练的极限。", "method": "开发了一个名为 Magistral 的可扩展 RL 流水线。采用“自下而上”的方法，仅使用自己的模型和基础设施，不依赖现有的 RL 实现或轨迹。探索了 LLM 的纯 RL 训练，提出了一种简单的方法来强制模型的推理语言，并仅在文本数据上进行 RL。训练了基于 Mistral Medium 3 的 Magistral Medium，并开源了包含 Magistral Medium 冷启动数据的 Magistral Small。", "result": "展示了一个用于探索 LLM 纯 RL 训练极限的堆栈；提出了一种强制模型推理语言的简单方法；表明仅在文本数据上进行 RL 训练能保持初始检查点的大部分能力；发现文本上的 RL 保持或改进了多模态理解、指令遵循和函数调用；推出了 Magistral Medium 并开源了 Magistral Small。", "conclusion": "通过自下而上的方法和专有基础设施，纯粹在文本数据上应用的强化学习，对于训练用于推理的大型语言模型是有效的，能够保持甚至提升多模态理解、指令遵循和函数调用等核心能力。这种方法促成了 Magistral 模型的开发，包括一个开源版本。", "translation": "我们介绍了 Magistral，这是 Mistral 的第一个推理模型，也是我们自己的可扩展强化学习 (RL) 流水线。我们没有依赖现有的实现和从先前模型中提取的 RL 轨迹，而是采用了一种自下而上的方法，完全依赖我们自己的模型和基础设施。值得注意的是，我们展示了一个堆栈，使我们能够探索 LLM 纯 RL 训练的极限，提出了一种简单的方法来强制模型的推理语言，并表明仅在文本数据上进行 RL 训练可以保持初始检查点的大部分能力。我们发现文本上的 RL 保持或改进了多模态理解、指令遵循和函数调用。我们展示了 Magistral Medium，它是在 Mistral Medium 3 的基础上纯粹通过 RL 进行推理训练的，并且我们开源了 Magistral Small (Apache 2.0)，它进一步包含了来自 Magistral Medium 的冷启动数据。", "summary": "本文介绍了 Magistral，这是 Mistral 开创性的推理模型和新颖的可扩展强化学习（RL）流水线。作者摒弃传统方法，采用自下而上的策略，完全依赖其专有模型和基础设施。他们证明，仅在文本数据上进行纯 RL 训练可以有效保持甚至增强大型语言模型的初始能力，包括多模态理解、指令遵循和函数调用。这项工作还提出了一种强制推理语言的方法，并介绍了用于推理的 Magistral Medium（一个经 RL 训练的模型）以及开源的 Magistral Small。", "keywords": "强化学习, 大语言模型, 推理模型, 可扩展流水线, 文本数据", "comments": "本文具有创新性，因为它在不依赖现有 RL 轨迹或实现的情况下，探索了 LLM 纯强化学习的边界，展示了一种完全专有的自下而上方法。仅在文本上进行 RL 训练就能保留并提升多种模型能力这一发现意义重大。Magistral Small 的开源将促进 RL 驱动的 LLM 开发的进一步研究。"}}
{"id": "2506.10568", "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers", "authors": ["Lizhen Wang", "Zhurong Xia", "Tianshu Hu", "Pengrui Wang", "Pengfei Wang", "Zerong Zheng", "Ming Zhou"], "summary": "In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserves human identities and product-specific details,\nsuch as logos and textures, by injecting paired human-product reference\ninformation and utilizing an additional masked cross-attention mechanism. We\nemploy a 3D body mesh template and product bounding boxes to provide precise\nmotion guidance, enabling intuitive alignment of hand gestures with product\nplacements. Additionally, structured text encoding is used to incorporate\ncategory-level semantics, enhancing 3D consistency during small rotational\nchanges across frames. Trained on a hybrid dataset with extensive data\naugmentation strategies, our approach outperforms state-of-the-art techniques\nin maintaining the identity integrity of both humans and products and\ngenerating realistic demonstration motions. Project page:\nhttps://submit2025-dream.github.io/DreamActor-H1/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10568v1", "AI": {"title_translation": "DreamActor-H1：基于运动设计的扩散Transformer的高保真人机互动演示视频生成", "tldr": "DreamActor-H1提出了一种基于扩散Transformer的新框架，通过注入参考信息、使用3D身体网格和结构化文本编码，解决了现有方法在生成高保真人机互动演示视频时无法保留身份和缺乏空间理解的问题，并实现了优于现有技术的性能。", "motivation": "在电子商务和数字营销中，生成高保真的人机互动演示视频对于有效的产品展示至关重要。然而，大多数现有框架要么无法保留人与产品的身份，要么缺乏对人产品空间关系的理解，导致不真实的表现和不自然的互动。", "method": "我们提出了一种基于扩散Transformer (DiT) 的框架。该方法通过注入配对的人产品参考信息并利用额外的掩蔽交叉注意力机制，同时保留了人的身份和产品特定细节（如标志和纹理）。我们采用3D身体网格模板和产品边界框提供精确的运动指导，实现手势与产品放置的直观对齐。此外，使用结构化文本编码来整合类别级语义，增强帧间小旋转变化时的3D一致性。该方法在一个混合数据集上进行训练，并采用了广泛的数据增强策略。", "result": "我们的方法在保持人与产品身份完整性以及生成真实演示动作方面，优于现有最先进的技术。", "conclusion": "DreamActor-H1通过其创新的DiT框架、参考信息注入、3D运动指导和结构化文本编码，成功解决了高保真人机互动演示视频生成中的身份保留和空间关系理解问题，并显著提升了生成视频的真实感和质量。", "translation": "在电子商务和数字营销中，生成高保真的人机互动演示视频对于有效的产品展示至关重要。然而，大多数现有框架要么无法保留人与产品的身份，要么缺乏对人产品空间关系的理解，导致不真实的表现和不自然的互动。为了解决这些挑战，我们提出了一种基于扩散Transformer (DiT) 的框架。我们的方法通过注入配对的人产品参考信息并利用额外的掩蔽交叉注意力机制，同时保留了人的身份和产品特定细节，如标志和纹理。我们采用3D身体网格模板和产品边界框提供精确的运动指导，实现手势与产品放置的直观对齐。此外，使用结构化文本编码来整合类别级语义，增强帧间小旋转变化时的3D一致性。该方法在一个混合数据集上进行训练，并采用了广泛的数据增强策略，其性能在保持人与产品身份完整性以及生成真实演示动作方面，优于现有最先进的技术。项目页面：https://submit2025-dream.github.io/DreamActor-H1/。", "summary": "DreamActor-H1提出了一种基于扩散Transformer (DiT) 的新框架，旨在生成高保真的人机互动产品演示视频。该方法通过注入配对的人产品参考信息和使用掩蔽交叉注意力机制来保持人与产品的身份及细节。它利用3D身体网格模板和产品边界框提供精确的运动指导，并结合结构化文本编码来增强3D一致性。在混合数据集上训练后，DreamActor-H1在保持身份完整性和生成真实演示动作方面超越了现有技术，有效解决了现有框架在真实感和互动自然度方面的不足。", "keywords": "人机互动视频生成, 扩散Transformer, 身份保留, 3D运动指导, 产品演示", "comments": "该论文提出了一种新颖的基于扩散Transformer的方法，用于生成高保真的人机互动演示视频，其创新点在于结合了多方面的技术来解决现有痛点。通过注入参考信息、引入3D运动指导和结构化文本编码，有效提升了生成视频中人与产品身份的保持能力以及互动动作的真实感和空间一致性。这对于电子商务和数字营销领域具有重要应用价值，能够显著提升产品展示的效果。其在保持人产品身份和生成真实动作方面的SOTA表现，证明了其方法的有效性。"}}
{"id": "2506.10577", "title": "Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics", "authors": ["Pascal Plettenberg", "André Alcalde", "Bernhard Sick", "Josephine M. Thomas"], "summary": "The design and optimization of Printed Circuit Board (PCB) schematics is\ncrucial for the development of high-quality electronic devices. Thereby, an\nimportant task is to optimize drafts by adding components that improve the\nrobustness and reliability of the circuit, e.g., pull-up resistors or\ndecoupling capacitors. Since there is a shortage of skilled engineers and\nmanual optimizations are very time-consuming, these best practices are often\nneglected. However, this typically leads to higher costs for troubleshooting in\nlater development stages as well as shortened product life cycles, resulting in\nan increased amount of electronic waste that is difficult to recycle. Here, we\npresent an approach for automating the addition of new components into PCB\nschematics by representing them as bipartite graphs and utilizing a node pair\nprediction model based on Graph Neural Networks (GNNs). We apply our approach\nto three highly relevant PCB design optimization tasks and compare the\nperformance of several popular GNN architectures on real-world datasets labeled\nby human experts. We show that GNNs can solve these problems with high accuracy\nand demonstrate that our approach offers the potential to automate PCB design\noptimizations in a time- and cost-efficient manner.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10577v1", "AI": {"title_translation": "用于印刷电路板原理图自动添加优化组件的图神经网络", "tldr": "本文提出了一种基于图神经网络（GNN）的方法，通过将PCB原理图表示为二分图并利用节点对预测模型，实现自动化添加优化组件，以提高电路的鲁棒性和可靠性。", "motivation": "印刷电路板（PCB）原理图的设计和优化对高质量电子设备的开发至关重要。然而，由于熟练工程师的短缺和手动优化耗时，往往会忽略添加优化组件（如上拉电阻或去耦电容器），这导致后期开发阶段故障排除成本增加以及产品生命周期缩短，进而产生更多难以回收的电子垃圾。", "method": "通过将PCB原理图表示为二分图，并利用基于图神经网络（GNN）的节点对预测模型，自动化添加新组件。该方法应用于三个相关的PCB设计优化任务，并比较了多种流行GNN架构在人类专家标记的真实世界数据集上的性能。", "result": "图神经网络能够高精度地解决这些问题。", "conclusion": "该方法有望以省时、高效的方式实现PCB设计优化的自动化。", "translation": "印刷电路板（PCB）原理图的设计和优化对于开发高质量电子设备至关重要。其中，一项重要任务是通过添加能够提高电路鲁棒性和可靠性的组件来优化草图，例如上拉电阻或去耦电容器。由于熟练工程师的短缺以及手动优化非常耗时，这些最佳实践常常被忽视。然而，这通常会导致后期开发阶段的故障排除成本更高，以及产品生命周期缩短，从而导致难以回收的电子垃圾量增加。在此，我们提出了一种通过将PCB原理图表示为二分图并利用基于图神经网络（GNN）的节点对预测模型来自动化向PCB原理图添加新组件的方法。我们将我们的方法应用于三个高度相关的PCB设计优化任务，并比较了几种流行GNN架构在由人类专家标记的真实世界数据集上的性能。我们表明，GNN可以高精度地解决这些问题，并证明我们的方法有潜力以省时和高效的方式自动化PCB设计优化。", "summary": "本研究提出了一种利用图神经网络（GNN）自动化向印刷电路板（PCB）原理图添加优化组件的方法。该方法将PCB原理图表示为二分图，并采用基于GNN的节点对预测模型。通过在真实世界数据集上测试，结果显示GNN能够高精度地完成优化任务，有望解决工程师短缺和手动优化耗时的问题，从而提高设计效率和电路质量。", "keywords": "图神经网络, 印刷电路板, 自动化设计, 组件优化, 节点对预测", "comments": "该论文提出了一种新颖且实用的方法，将图神经网络应用于PCB设计自动化领域。其创新之处在于将PCB原理图建模为二分图，并利用GNN进行组件的自动添加。这对于解决当前电子设计领域面临的工程师短缺和效率低下问题具有重要意义。该方法有望显著缩短设计周期，降低开发成本，并提高产品质量，同时对减少电子垃圾也有潜在的积极影响。"}}
{"id": "2506.10920", "title": "Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization", "authors": ["Or Shafran", "Atticus Geiger", "Mor Geva"], "summary": "A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse autoencoders (SAEs), commonly trained over\nresidual stream activations to learn directions from scratch. However, SAEs\noften struggle in causal evaluations and lack intrinsic interpretability, as\ntheir learning is not explicitly tied to the computations of the model. Here,\nwe tackle these limitations by directly decomposing MLP activations with\nsemi-nonnegative matrix factorization (SNMF), such that the learned features\nare (a) sparse linear combinations of co-activated neurons, and (b) mapped to\ntheir activating inputs, making them directly interpretable. Experiments on\nLlama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs\nand a strong supervised baseline (difference-in-means) on causal steering,\nwhile aligning with human-interpretable concepts. Further analysis reveals that\nspecific neuron combinations are reused across semantically-related features,\nexposing a hierarchical structure in the MLP's activation space. Together,\nthese results position SNMF as a simple and effective tool for identifying\ninterpretable features and dissecting concept representations in LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10920v1", "AI": {"title_translation": "通过半非负矩阵分解将MLP激活分解为可解释特征", "tldr": "本文提出使用半非负矩阵分解（SNMF）直接分解LLM中的MLP激活，以识别可解释特征，并在因果评估和人类可解释性方面优于稀疏自编码器（SAE）。", "motivation": "现有方法，特别是基于稀疏自编码器（SAE）的字典学习，在因果评估中表现不佳且缺乏内在可解释性，因为它们的学习与模型计算没有明确关联。研究旨在克服这些局限性，找到一种无监督的方式来识别LLM中可解释的激活方向。", "method": "本文通过半非负矩阵分解（SNMF）直接分解多层感知机（MLP）的激活。学习到的特征是共激活神经元的稀疏线性组合，并映射到它们的激活输入，从而使其直接可解释。", "result": "在Llama 3.1、Gemma 2和GPT-2上的实验表明，SNMF导出的特征在因果操控方面优于SAE和一个强大的有监督基线（均值差），同时与人类可解释的概念对齐。进一步分析揭示，特定的神经元组合在语义相关的特征中被重用，暴露出MLP激活空间中的分层结构。", "conclusion": "SNMF是一种简单有效的工具，可用于识别大型语言模型中的可解释特征并剖析概念表示。", "translation": "机械可解释性的核心目标是识别大型语言模型（LLMs）中能够因果解释其输出的正确分析单元。早期工作侧重于单个神经元，但神经元通常编码多个概念的证据促使人们转向分析激活空间中的方向。一个关键问题是如何以无监督的方式找到捕捉可解释特征的方向。当前方法依赖于使用稀疏自编码器（SAE）的字典学习，通常在残差流激活上训练以从头开始学习方向。然而，SAE在因果评估中常常表现不佳，并且缺乏内在可解释性，因为它们的学习与模型的计算没有明确关联。本文通过半非负矩阵分解（SNMF）直接分解MLP激活来解决这些局限性，使得学习到的特征（a）是共激活神经元的稀疏线性组合，并且（b）映射到它们的激活输入，从而使其直接可解释。在Llama 3.1、Gemma 2和GPT-2上的实验表明，SNMF导出的特征在因果操控方面优于SAE和一个强大的有监督基线（均值差），同时与人类可解释的概念对齐。进一步分析揭示，特定的神经元组合在语义相关的特征中被重用，暴露出MLP激活空间中的分层结构。总而言之，这些结果将SNMF定位为识别可解释特征和剖析LLMs中概念表示的简单有效工具。", "summary": "该研究提出了一种名为半非负矩阵分解（SNMF）的新方法，用于直接分解大型语言模型（LLMs）中多层感知机（MLP）的激活。与现有方法（如稀疏自编码器SAE）的局限性相比，SNMF旨在学习稀疏、可解释的特征，这些特征是共激活神经元的线性组合，并可直接追溯到其输入。实验结果表明，SNMF在因果操控性能上优于SAE和监督基线，并且其发现的特征与人类可解释的概念高度一致。此外，研究还揭示了MLP激活空间中的分层结构，表明SNMF是分析LLM内部概念表示的有效工具。", "keywords": "MLP激活, 可解释性, 半非负矩阵分解, LLM, 稀疏特征", "comments": "该论文的创新点在于将半非负矩阵分解（SNMF）应用于大型语言模型（LLMs）的MLP激活分解，以获得可解释的特征。这克服了现有稀疏自编码器（SAE）在因果评估和内在可解释性方面的局限性。SNMF直接将特征与神经元组合及其激活输入关联起来，使得特征更具可解释性，并且在实验中表现出优越的因果操控能力和概念对齐。其揭示MLP激活空间中分层结构的能力也为理解LLM的内部机制提供了新的视角，为机械可解释性领域提供了一个简单而有效的工具。"}}
{"id": "2506.10573", "title": "Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration", "authors": ["Jun Wang", "Lixing Zhu", "Xiaohan Yu", "Abhir Bhalerao", "Yulan He"], "summary": "Learning medical visual representations from image-report pairs through joint\nlearning has garnered increasing research attention due to its potential to\nalleviate the data scarcity problem in the medical domain. The primary\nchallenges stem from the lengthy reports that feature complex discourse\nrelations and semantic pathologies. Previous works have predominantly focused\non instance-wise or token-wise cross-modal alignment, often neglecting the\nimportance of pathological-level consistency. This paper presents a novel\nframework PLACE that promotes the Pathological-Level Alignment and enriches the\nfine-grained details via Correlation Exploration without additional human\nannotations. Specifically, we propose a novel pathological-level cross-modal\nalignment (PCMA) approach to maximize the consistency of pathology observations\nfrom both images and reports. To facilitate this, a Visual Pathology\nObservation Extractor is introduced to extract visual pathological observation\nrepresentations from localized tokens. The PCMA module operates independently\nof any external disease annotations, enhancing the generalizability and\nrobustness of our methods. Furthermore, we design a proxy task that enforces\nthe model to identify correlations among image patches, thereby enriching the\nfine-grained details crucial for various downstream tasks. Experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance on multiple downstream tasks, including classification,\nimage-to-text retrieval, semantic segmentation, object detection and report\ngeneration.", "comment": "12 pages, 10 tables and 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10573v1", "AI": {"title_translation": "通过病理级别跨模态对齐和关联探索改进医学视觉表征学习", "tldr": "本文提出了一个新框架PLACE，通过病理级别跨模态对齐和关联探索，在无需额外人工标注的情况下，提升医学视觉表征学习，并在多项下游任务中取得了SOTA性能。", "motivation": "从图像-报告对中学习医学视觉表征有助于缓解医疗领域数据稀缺问题。主要挑战在于冗长的报告具有复杂的语篇关系和语义病理。现有工作主要侧重于实例级或令牌级跨模态对齐，而忽略了病理级别一致性的重要性。", "method": "本文提出了一个名为PLACE的新框架，该框架通过病理级别对齐和关联探索来丰富细粒度细节，且无需额外人工标注。具体来说，提出了一种新颖的病理级别跨模态对齐（PCMA）方法，以最大化图像和报告中病理观察的一致性。为此，引入了一个视觉病理观察提取器来从局部令牌中提取视觉病理观察表示。PCMA模块独立于任何外部疾病标注。此外，设计了一个代理任务，强制模型识别图像补丁之间的关联，从而丰富对各种下游任务至关重要的细粒度细节。", "result": "实验结果表明，所提出的框架在包括分类、图像到文本检索、语义分割、目标检测和报告生成在内的多项下游任务中实现了新的最先进性能。", "conclusion": "通过引入病理级别跨模态对齐和关联探索，PLACE框架显著提高了医学视觉表征学习的效果，并在多项下游任务中表现出优越性，且无需额外的人工标注，增强了方法的泛化性和鲁棒性。", "translation": "通过联合学习从图像-报告对中学习医学视觉表征因其在缓解医疗领域数据稀缺问题方面的潜力而受到越来越多的研究关注。主要挑战源于冗长的报告，其具有复杂的语篇关系和语义病理。以往的工作主要侧重于实例级或令牌级跨模态对齐，往往忽略了病理级别一致性的重要性。本文提出了一个新颖的框架PLACE，该框架通过病理级别对齐和关联探索来促进病理级别对齐并丰富细粒度细节，且无需额外的人工标注。具体来说，我们提出了一种新颖的病理级别跨模态对齐（PCMA）方法，以最大化图像和报告中病理观察的一致性。为了促进这一点，引入了一个视觉病理观察提取器，用于从局部令牌中提取视觉病理观察表示。PCMA模块独立于任何外部疾病标注，增强了我们方法的泛化性和鲁棒性。此外，我们设计了一个代理任务，强制模型识别图像补丁之间的关联，从而丰富对各种下游任务至关重要的细粒度细节。实验结果表明，我们提出的框架在包括分类、图像到文本检索、语义分割、目标检测和报告生成在内的多项下游任务中实现了新的最先进性能。", "summary": "本文提出了一种名为PLACE的新型框架，旨在通过病理级别跨模态对齐（PCMA）和关联探索来改进医学视觉表征学习。针对现有方法忽视病理级别一致性的问题，PLACE引入了无需额外标注的PCMA模块和视觉病理观察提取器，以最大化图像与报告间病理观察的一致性。同时，通过设计代理任务来强化图像补丁间的关联，丰富细粒度信息。实验证明，该框架在多项下游医学任务中取得了最先进的性能，有效缓解了医学数据稀缺问题。", "keywords": "医学视觉表征学习, 跨模态对齐, 病理级别, 关联探索, 无监督学习", "comments": "该论文的创新点在于提出了病理级别跨模态对齐的概念，并设计了无需外部标注的PCMA模块和视觉病理观察提取器，这在很大程度上解决了医学领域数据标注困难的问题，提升了模型的泛化性和实用性。通过结合关联探索代理任务，进一步丰富了细粒度特征，使其在多种下游任务中表现出色，对于医学图像分析领域具有重要意义。"}}
{"id": "2506.10934", "title": "Dynamic Epistemic Friction in Dialogue", "authors": ["Timothy Obiso", "Kenneth Lai", "Abhijnan Nath", "Nikhil Krishnaswamy", "James Pustejovsky"], "summary": "Recent developments in aligning Large Language Models (LLMs) with human\npreferences have significantly enhanced their utility in human-AI collaborative\nscenarios. However, such approaches often neglect the critical role of\n\"epistemic friction,\" or the inherent resistance encountered when updating\nbeliefs in response to new, conflicting, or ambiguous information. In this\npaper, we define dynamic epistemic friction as the resistance to epistemic\nintegration, characterized by the misalignment between an agent's current\nbelief state and new propositions supported by external evidence. We position\nthis within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,\n2011), where friction emerges as nontrivial belief-revision during the\ninteraction. We then present analyses from a situated collaborative task that\ndemonstrate how this model of epistemic friction can effectively predict belief\nupdates in dialogues, and we subsequently discuss how the model of belief\nalignment as a measure of epistemic resistance or friction can naturally be\nmade more sophisticated to accommodate the complexities of real-world dialogue\nscenarios.", "comment": "11 pages, 2 figures, 2 tables, CoNLL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10934v1", "AI": {"title_translation": "对话中的动态认知摩擦", "tldr": "本文引入并定义了“动态认知摩擦”的概念，解释了在对话中LLMs更新信念时遇到的阻力，并展示了其在预测信念更新中的有效性。", "motivation": "尽管大型语言模型（LLMs）在人机协作中表现出色，但现有方法常忽略“认知摩擦”——即在接收到新的、冲突或模糊信息时更新信念所遇到的固有阻力。本文旨在解决这一问题。", "method": "本文定义了动态认知摩擦为认知整合的阻力，其特征是代理当前信念状态与外部证据支持的新命题之间的不一致。研究将其置于动态认知逻辑框架内，其中摩擦表现为交互过程中非平凡的信念修正。通过对情境化协作任务的分析来验证了该模型。", "result": "该认知摩擦模型能有效预测对话中的信念更新。", "conclusion": "将信念对齐作为认知阻力或摩擦的衡量标准，可以自然地变得更加复杂，以适应现实世界对话场景的复杂性。", "translation": "**标题**：对话中的动态认知摩擦\n\n**摘要**：近期在使大型语言模型（LLMs）与人类偏好对齐方面的进展显著增强了它们在人机协作场景中的实用性。然而，这些方法常常忽视“认知摩擦”的关键作用，即在响应新的、冲突的或模糊的信息时更新信念所遇到的固有阻力。在本文中，我们将动态认知摩擦定义为认知整合的阻力，其特征是代理当前信念状态与外部证据支持的新命题之间的不一致。我们将此置于动态认知逻辑（Van Benthem and Pacuit, 2011）的框架内，其中摩擦表现为交互过程中非平凡的信念修正。随后，我们展示了来自情境化协作任务的分析，这些分析表明该认知摩擦模型如何能有效预测对话中的信念更新，并且我们随后讨论了如何自然地使信念对齐作为认知阻力或摩擦的衡量标准变得更加复杂，以适应现实世界对话场景的复杂性。", "summary": "本文提出了“动态认知摩擦”的概念，将其定义为代理在对话中整合新信息时遇到的信念更新阻力。该概念被置于动态认知逻辑框架内，并通过协作任务的分析验证了其在预测对话中信念更新的有效性。研究指出，将信念对齐作为认知阻力的一种衡量方式，有助于更精细地处理真实世界对话的复杂性。", "keywords": "认知摩擦, 大型语言模型, 对话, 信念更新, 动态认知逻辑", "comments": "这篇论文的创新点在于引入并形式化了“动态认知摩擦”这一关键概念，填补了LLMs在信念更新过程中考虑阻力的空白。将其置于动态认知逻辑框架下，为理解和预测对话中的信念修正提供了新的理论视角和实用模型。这对于提升LLMs在复杂、动态人机交互中的表现具有重要意义。"}}
{"id": "2506.10616", "title": "Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability", "authors": ["Yu-Jie Zhang", "Peng Zhao", "Masashi Sugiyama"], "summary": "Non-stationary online learning has drawn much attention in recent years.\nDespite considerable progress, dynamic regret minimization has primarily\nfocused on convex functions, leaving the functions with stronger curvature\n(e.g., squared or logistic loss) underexplored. In this work, we address this\ngap by showing that the regret can be substantially improved by leveraging the\nconcept of mixability, a property that generalizes exp-concavity to effectively\ncapture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path\nlength of comparators that reflects the environmental non-stationarity. We\ndemonstrate that an exponential-weight method with fixed-share updates achieves\nan $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ dynamic regret for mixable losses,\nimproving upon the best-known $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$\nresult (Baby and Wang, 2021) in $d$. More importantly, this improvement arises\nfrom a simple yet powerful analytical framework that exploits the mixability,\nwhich avoids the Karush-Kuhn-Tucker-based analysis required by existing work.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10616v1", "AI": {"title_translation": "弯曲损失的非平稳在线学习：通过可混合性改进动态遗憾", "tldr": "本文通过引入“可混合性”概念，显著改进了非平稳在线学习中针对弯曲损失的动态遗憾界限，并提供了一种更简洁的分析框架。", "motivation": "尽管动态遗憾最小化在凸函数方面取得了显著进展，但对具有更强曲率的函数（例如平方损失或逻辑损失）的研究不足。本文旨在弥补这一空白。", "method": "通过利用“可混合性”（一种推广了指数凹性以有效捕捉损失曲率的属性），采用一种带有固定份额更新的指数加权方法来改进遗憾。", "result": "对于可混合损失，该方法实现了 $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ 的动态遗憾，相较于 Baby 和 Wang (2021) 提出的 $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$ 结果，在维度 $d$ 上有所改进。更重要的是，这种改进源于一个简单而强大的分析框架。", "conclusion": "通过利用可混合性，该研究为弯曲损失的非平稳在线学习提供了一个改进的动态遗憾界限和更简洁的分析框架，避免了现有工作所需的基于 Karush-Kuhn-Tucker 的分析。", "translation": "非平稳在线学习近年来备受关注。尽管取得了相当大的进展，但动态遗憾最小化主要集中在凸函数上，对具有更强曲率的函数（例如平方损失或逻辑损失）的探索不足。在这项工作中，我们通过展示可以利用可混合性概念（一种推广了指数凹性以有效捕捉损失曲率的属性）来大幅改进遗憾，从而解决了这一空白。令 $d$ 表示维度，$P_T$ 表示反映环境非平稳性的比较器的路径长度。我们证明，一种带有固定份额更新的指数加权方法对于可混合损失实现了 $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ 的动态遗憾，改进了 Baby 和 Wang (2021) 提出的最佳已知 $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$ 结果在 $d$ 上的表现。更重要的是，这种改进源于一个简单而强大的分析框架，该框架利用了可混合性，避免了现有工作所需的基于 Karush-Kuhn-Tucker 的分析。", "summary": "本文针对非平稳在线学习中对具有强曲率的损失函数（即弯曲损失）研究不足的问题，引入了“可混合性”概念。研究表明，通过采用带有固定份额更新的指数加权方法，可以为可混合损失实现改进的动态遗憾界限 $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$，这在维度 $d$ 上优于现有最佳结果。这一突破得益于一个利用可混合性且避免复杂 KKT 分析的简洁而强大的分析框架。", "keywords": "非平稳在线学习, 动态遗憾, 可混合性, 弯曲损失, 指数加权方法", "comments": "该论文通过引入“可混合性”这一新概念，为处理非平稳在线学习中的弯曲损失提供了一种创新且有效的途径。它不仅在理论上取得了更优的动态遗憾界限，而且提供了一个更简洁的分析框架，避免了传统复杂的 KKT 分析，这对于未来该领域的研究具有重要意义和启发性。"}}
{"id": "2506.10952", "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training", "authors": ["Mozhi Zhang", "Howe Tissue", "Lu Wang", "Xipeng Qiu"], "summary": "We introduce~\\textsc{Domain2Vec}, a novel approach that decomposes any\ndataset into a linear combination of several \\emph{meta-domains}, a new concept\ndesigned to capture the key underlying features of datasets.\n\\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\emph{\\textbf{D}istribution\n\\textbf{A}lignment \\textbf{A}ssumption} (DA$^{2}$), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, \\textsc{Domain2vec} can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\n\\textsc{Domain2Vec} helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\n\\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only\n$51.5\\%$ of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, \\textsc{Domain2Vec} improves\ndownstream performance by an average of $2.83\\%$.", "comment": "Accepted to ICML2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10952v1", "AI": {"title_translation": "Domain2Vec：向量化数据集以无需训练即可找到最优数据混合", "tldr": "Domain2Vec 是一种无需训练即可向量化数据集以找到最优数据混合的新方法，能显著减少计算并提升性能。", "motivation": "寻找语言模型预训练的最优数据混合通常需要大量计算，Domain2Vec 旨在提供一种无需训练的解决方案。", "method": "Domain2Vec 将数据集分解为元域的线性组合，元域是捕获数据集关键底层特征的新概念。它维护一个元域词汇表，并使用分类器将任何给定数据集分解为对应于该词汇表分布的域向量。这些域向量在分布对齐假设（DA^2）下，能够以无需训练的方式识别语言模型预训练的最优数据混合。此外，Domain2Vec 可以与现有工作集成，以建模域向量与语言模型性能之间的关系。", "result": "Domain2Vec 能够以最小的计算开销找到增强下游任务性能的数据混合。具体而言，在 Pile-CC 上，它仅使用原始数据混合训练所需计算量的 51.5% 即可达到相同的验证损失。在同等计算预算下，Domain2Vec 将下游性能平均提高了 2.83%。", "conclusion": "Domain2Vec 是一种有效且高效的方法，用于优化语言模型预训练的数据混合，显著降低计算成本并提高性能。", "translation": "我们引入了~\textsc{Domain2Vec}，这是一种新颖的方法，它将任何数据集分解为几个元域的线性组合，元域是一个旨在捕获数据集关键底层特征的新概念。\textsc{Domain2Vec} 维护一个元域词汇表，并使用分类器将任何给定数据集分解为一个域向量，该向量对应于此词汇表上的分布。这些域向量在“分布对齐假设”（DA²）下，能够以无需训练的方式识别语言模型（LM）预训练的最优数据混合，该假设表明当训练集和验证集的数据分布更好地对齐时，可以实现更低的验证损失。此外，\textsc{Domain2vec} 可以无缝集成到以前的工作中，以建模域向量与 LM 性能之间的关系，极大地提高了以前方法的效率和可扩展性。大量的实验表明，\textsc{Domain2Vec} 有助于找到能够以最小计算开销增强下游任务性能的数据混合。具体而言，\textsc{Domain2Vec} 在 Pile-CC 上达到了相同的验证损失，而所需的计算量仅为在 The Pile 数据集原始混合上训练时所需计算量的 51.5%。在同等计算预算下，\textsc{Domain2Vec} 将下游性能平均提高了 2.83%。", "summary": "Domain2Vec 是一种新颖的无需训练的方法，它将数据集向量化为“元域”，以找到语言模型预训练的最优数据混合。它将数据集分解为表示元域分布的域向量，并利用分布对齐假设（DA^2）来实现更低的验证损失。该方法显著降低了计算开销，在实现相同验证损失的情况下，计算量减少了 51.5%，下游性能平均提高了 2.83%，同时还可以与现有方法集成以提高效率和可扩展性。", "keywords": "Domain2Vec, 数据混合, 元域, 语言模型预训练, 无训练", "comments": "Domain2Vec 的创新之处在于引入了“元域”的概念，并提出了一种无需训练即可优化数据混合的方法，这对于语言模型预训练具有重大意义。它显著降低了计算成本并提升了性能，解决了大规模模型训练中的一个关键挑战，展现出强大的实用价值和可扩展性。"}}
{"id": "2506.10575", "title": "Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning", "authors": ["Chun-Mei Feng", "Kai Yu", "Xinxing Xu", "Salman Khan", "Rick Siow Mong Goh", "Wangmeng Zuo", "Yong Liu"], "summary": "Benefited from image-text contrastive learning, pre-trained vision-language\nmodels, e.g., CLIP, allow to direct leverage texts as images (TaI) for\nparameter-efficient fine-tuning (PEFT). While CLIP is capable of making image\nfeatures to be similar to the corresponding text features, the modality gap\nremains a nontrivial issue and limits image recognition performance of TaI.\nUsing multi-label image recognition (MLR) as an example, we present a novel\nmethod, called T2I-PAL to tackle the modality gap issue when using only text\ncaptions for PEFT. The core design of T2I-PAL is to leverage pre-trained\ntext-to-image generation models to generate photo-realistic and diverse images\nfrom text captions, thereby reducing the modality gap. To further enhance MLR,\nT2I-PAL incorporates a class-wise heatmap and learnable prototypes. This\naggregates local similarities, making the representation of local visual\nfeatures more robust and informative for multi-label recognition. For better\nPEFT, we further combine both prompt tuning and adapter learning to enhance\nclassification performance. T2I-PAL offers significant advantages: it\neliminates the need for fully semantically annotated training images, thereby\nreducing the manual annotation workload, and it preserves the intrinsic mode of\nthe CLIP model, allowing for seamless integration with any existing CLIP\nframework. Extensive experiments on multiple benchmarks, including MS-COCO,\nVOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance\nby 3.47% in average above the top-ranked state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10575v1", "AI": {"title_translation": "联合提示-适配器学习的文本到图像多标签图像识别", "tldr": "针对多标签图像识别中的模态鸿沟问题，本文提出T2I-PAL方法，通过文本到图像生成来缩小模态差距，并结合提示微调和适配器学习，显著提升了性能并减少了标注需求。", "motivation": "预训练视觉-语言模型（如CLIP）在利用文本作为图像进行参数高效微调（PEFT）时存在模态鸿沟问题，限制了图像识别性能，尤其在多标签图像识别中。此外，对语义标注训练图像的需求增加了手动标注工作量。", "method": "本文提出T2I-PAL方法，其核心是利用预训练的文本到图像生成模型从文本描述中生成逼真多样的图像，以缩小模态鸿沟。为进一步增强多标签图像识别，T2I-PAL还结合了类别热图和可学习原型来聚合局部相似性。为实现更好的PEFT，该方法进一步结合了提示微调和适配器学习。", "result": "T2I-PAL消除了对完全语义标注训练图像的需求，减少了手动标注工作量，并保留了CLIP模型的内在模式，可与任何现有CLIP框架无缝集成。在MS-COCO、VOC2007和NUS-WIDE等多个基准测试中，T2I-PAL的识别性能平均比顶级现有方法提高了3.47%。", "conclusion": "T2I-PAL通过文本到图像生成有效解决了利用文本作为图像进行多标签图像识别时的模态鸿沟问题，显著提升了识别性能，并降低了数据标注成本。", "translation": "受益于图像-文本对比学习，预训练的视觉-语言模型，例如CLIP，允许直接利用文本作为图像（TaI）进行参数高效微调（PEFT）。虽然CLIP能够使图像特征与相应的文本特征相似，但模态鸿沟仍然是一个不容忽视的问题，并限制了TaI的图像识别性能。以多标签图像识别（MLR）为例，我们提出了一种名为T2I-PAL的新方法，以解决仅使用文本描述进行PEFT时的模态鸿沟问题。T2I-PAL的核心设计是利用预训练的文本到图像生成模型从文本描述中生成逼真多样的图像，从而减少模态鸿沟。为了进一步增强MLR，T2I-PAL结合了类别热图和可学习原型。这聚合了局部相似性，使局部视觉特征的表示对于多标签识别更加鲁棒和信息丰富。为了更好的PEFT，我们进一步结合了提示微调和适配器学习以提高分类性能。T2I-PAL具有显著优势：它消除了对完全语义标注训练图像的需求，从而减少了手动标注工作量，并且它保留了CLIP模型的内在模式，允许与任何现有CLIP框架无缝集成。在包括MS-COCO、VOC2007和NUS-WIDE在内的多个基准测试中进行的广泛实验表明，我们的T2I-PAL可以将识别性能平均比排名靠前的最先进方法提高3.47%。", "summary": "本文提出T2I-PAL方法，旨在解决利用预训练视觉-语言模型（如CLIP）进行多标签图像识别时存在的模态鸿沟问题。T2I-PAL通过文本到图像生成模型从文本描述中生成图像，以缩小模态差距，并通过结合类别热图、可学习原型以及提示微调和适配器学习来增强识别性能。该方法显著提升了多标签图像识别的准确性，减少了对大量标注图像的需求，并能与现有CLIP框架无缝集成。", "keywords": "文本到图像, 多标签图像识别, 提示-适配器学习, 模态鸿沟, CLIP", "comments": "该论文提出了一种新颖的方法T2I-PAL，通过引入文本到图像生成来解决视觉-语言模型在多标签图像识别中的模态鸿沟问题，这在参数高效微调背景下具有创新性。其优势在于减少了数据标注成本并保持了与现有CLIP框架的兼容性，为实际应用提供了便利。性能提升显著，表明了该方法的有效性。"}}
{"id": "2506.10617", "title": "Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code", "authors": ["Reza Karbasi", "Masoud Rahimi", "Abdol-Hossein Vahabie", "Hadi Moradi"], "summary": "This paper addresses the persistent challenge of accurately digitizing\npaper-based electrocardiogram (ECG) recordings, with a particular focus on\nrobustly handling single leads compromised by signal overlaps-a common yet\nunder-addressed issue in existing methodologies. We propose a two-stage\npipeline designed to overcome this limitation. The first stage employs a U-Net\nbased segmentation network, trained on a dataset enriched with overlapping\nsignals and fortified with custom data augmentations, to accurately isolate the\nprimary ECG trace. The subsequent stage converts this refined binary mask into\na time-series signal using established digitization techniques, enhanced by an\nadaptive grid detection module for improved versatility across different ECG\nformats and scales. Our experimental results demonstrate the efficacy of our\napproach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained\nsegmentation task. Crucially, our proposed digitization method yields superior\nperformance compared to a well-established baseline technique across both\nnon-overlapping and challenging overlapping ECG samples. For non-overlapping\nsignals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson\nCorrelation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,\nrespectively, for the baseline. On samples with signal overlap, our method\nachieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the\nbaseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to\nsignificantly enhance digitization accuracy, especially in the presence of\nsignal overlaps, thereby laying a strong foundation for the reliable conversion\nof analog ECG records into analyzable digital data for contemporary research\nand clinical applications. The implementation is publicly available at this\nGitHub repository: https://github.com/masoudrahimi39/ECG-code.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10617v1", "AI": {"title_translation": "基于深度学习的重叠心电图图像数字化及开源Python代码", "tldr": "本文提出了一种基于两阶段深度学习管道的方法，用于准确数字化纸质ECG记录，特别是处理信号重叠的单导联，并在重叠和非重叠ECG样本上均优于现有基线方法。", "motivation": "现有方法在准确数字化纸质心电图（ECG）记录时面临持续挑战，尤其是在处理信号重叠的单导联方面，这是一个常见但未被充分解决的问题。", "method": "提出一个两阶段管道：第一阶段使用U-Net分割网络，在包含重叠信号并经过自定义数据增强的数据集上训练，以精确分离主要ECG轨迹。第二阶段利用自适应网格检测模块增强的成熟数字化技术，将二值掩码转换为时间序列信号。", "result": "U-Net架构在精细分割任务中实现了0.87的IoU。与基线技术相比，所提出的数字化方法在非重叠信号上MSE为0.0010，Pearson相关系数(rho)为0.9644（基线分别为0.0015和0.9366）。在信号重叠样本上，该方法MSE为0.0029，rho为0.9641（基线分别为0.0178和0.8676），性能显著优于基线。", "conclusion": "本工作展示了一种有效策略，可显著提高数字化精度，特别是在存在信号重叠的情况下，为将模拟ECG记录可靠地转换为可分析的数字数据奠定了坚实基础，适用于当代研究和临床应用。", "translation": "本文旨在解决纸质心电图（ECG）记录准确数字化的持续挑战，特别关注如何稳健地处理受信号重叠影响的单导联——这是现有方法中常见但未被充分解决的问题。我们提出了一个旨在克服这一限制的两阶段管道。第一阶段采用基于U-Net的分割网络，该网络在一个包含重叠信号并经过自定义数据增强的数据集上进行训练，以准确分离主要的ECG轨迹。随后的第二阶段利用成熟的数字化技术，并通过自适应网格检测模块进行增强，将这种精炼的二值掩码转换为时间序列信号，从而提高了在不同ECG格式和尺度下的通用性。我们的实验结果证明了我们方法的有效性。U-Net架构在精细分割任务中实现了0.87的IoU。至关重要的是，与一个成熟的基线技术相比，我们提出的数字化方法在非重叠和具有挑战性的重叠ECG样本上均表现出卓越的性能。对于非重叠信号，我们的方法实现了0.0010的均方误差（MSE）和0.9644的皮尔逊相关系数（rho），而基线分别为0.0015和0.9366。在具有信号重叠的样本上，我们的方法实现了0.0029的MSE和0.9641的rho，显著优于基线的0.0178和0.8676。这项工作展示了一种有效策略，可显著提高数字化精度，特别是在存在信号重叠的情况下，从而为将模拟ECG记录可靠地转换为可分析的数字数据奠定了坚实基础，适用于当代研究和临床应用。该实现已在GitHub存储库公开：https://github.com/masoudrahimi39/ECG-code。", "summary": "本研究提出一种基于深度学习的两阶段管道，用于精确数字化纸质ECG图像，尤其解决了信号重叠的挑战。第一阶段利用U-Net网络分割ECG轨迹，第二阶段将分割结果转换为时间序列信号，并引入自适应网格检测。实验证明，该方法在处理重叠和非重叠ECG数据时，性能均优于现有基线方法，显著提高了数字化精度，为ECG数据数字化提供了可靠方案。", "keywords": "ECG数字化, 深度学习, U-Net, 信号重叠, 图像分割", "comments": "该论文的创新点在于提出了一个两阶段深度学习管道，特别是专注于解决ECG图像中常见的信号重叠问题，这是一个在现有方法中常被忽视的挑战。U-Net的应用结合自定义数据增强，以及自适应网格检测模块，显著提升了数字化精度和通用性。其开源实现也增加了其潜在影响力和可复现性。该工作对于将模拟ECG记录可靠地转换为数字数据具有重要意义，有助于推动临床和研究应用。"}}
{"id": "2506.10576", "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres", "authors": ["Muskan Dosi", "Chiranjeev Chiranjeev", "Kartik Thakral", "Mayank Vatsa", "Richa Singh"], "summary": "Do contemporary diffusion models preserve the class geometry of\nhyperspherical data? Standard diffusion models rely on isotropic Gaussian noise\nin the forward process, inherently favoring Euclidean spaces. However, many\nreal-world problems involve non-Euclidean distributions, such as hyperspherical\nmanifolds, where class-specific patterns are governed by angular geometry\nwithin hypercones. When modeled in Euclidean space, these angular subtleties\nare lost, leading to suboptimal generative performance. To address this\nlimitation, we introduce HyperSphereDiff to align hyperspherical structures\nwith directional noise, preserving class geometry and effectively capturing\nangular uncertainty. We demonstrate both theoretically and empirically that\nthis approach aligns the generative process with the intrinsic geometry of\nhyperspherical data, resulting in more accurate and geometry-aware generative\nmodels. We evaluate our framework on four object datasets and two face\ndatasets, showing that incorporating angular uncertainty better preserves the\nunderlying hyperspherical manifold. Resources are available at:\n{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/}", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10576v1", "AI": {"title_translation": "协调几何与不确定性：超球面上的扩散模型", "tldr": "现有扩散模型在处理超球面数据时会丢失几何信息，本文提出HyperSphereDiff，通过引入方向性噪声，更好地保留超球面数据的固有几何结构和角度不确定性，从而提升生成性能。", "motivation": "标准扩散模型依赖各向同性高斯噪声，偏向欧几里得空间，但在处理超球面流形等非欧几里得分布时，会丢失由角度几何控制的类特定模式，导致生成性能不佳。", "method": "提出HyperSphereDiff模型，通过将超球面结构与方向性噪声对齐，以保留类几何并有效捕获角度不确定性。", "result": "理论和经验上证明，该方法使生成过程与超球面数据的内在几何结构对齐，从而产生更准确且几何感知的生成模型。在四个对象数据集和两个人脸数据集上的评估显示，结合角度不确定性更好地保留了底层的超球面流形。", "conclusion": "通过在扩散模型中引入方向性噪声和处理角度不确定性，可以有效地保留超球面数据的固有几何结构，显著提升模型在非欧几里得数据上的生成性能。", "translation": "当代扩散模型是否保留了超球面数据的类别几何？标准扩散模型在前向过程中依赖各向同性高斯噪声，固有地偏向欧几里得空间。然而，许多现实世界问题涉及非欧几里得分布，例如超球面流形，其中类别特定的模式由超锥体内的角度几何控制。当在欧几里得空间中建模时，这些角度上的细微差别会丢失，导致次优的生成性能。为了解决这一限制，我们引入了HyperSphereDiff，以将超球面结构与方向性噪声对齐，从而保留类别几何并有效捕获角度不确定性。我们在理论和经验上都证明了这种方法将生成过程与超球面数据的内在几何结构对齐，从而产生了更准确和几何感知的生成模型。我们在四个对象数据集和两个人脸数据集上评估了我们的框架，结果表明，结合角度不确定性更好地保留了底层的超球面流形。资源可在以下链接获取：{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/}", "summary": "本文关注标准扩散模型在处理超球面等非欧几里得数据时，因其基于欧几里得空间的假设而导致几何信息丢失的问题。为解决此问题，研究者提出了HyperSphereDiff，该模型通过引入方向性噪声来与超球面结构对齐，从而有效保留数据的类别几何和角度不确定性。理论和实验结果表明，HyperSphereDiff能够使生成过程更好地与超球面数据的内在几何结构匹配，从而生成更准确且几何感知的模型，并在多个数据集上验证了其在保留底层超球面流形方面的优越性。", "keywords": "扩散模型, 超球面, 几何感知, 方向性噪声, 非欧几里得数据", "comments": "该论文创新性地将扩散模型与超球面几何相结合，解决了现有扩散模型在处理非欧几里得数据时的局限性。通过引入方向性噪声，模型能够更好地捕获和保留数据的内在几何结构和角度不确定性，这对于在复杂流形上进行数据生成具有重要意义。这一方法为处理非欧几里得数据（如面部、物体姿态等）提供了一个新的视角和有效的解决方案，具有广泛的应用潜力。"}}
{"id": "2506.10582", "title": "Rethinking Random Masking in Self Distillation on ViT", "authors": ["Jihyeon Seong", "Hyunkyung Han"], "summary": "Vision Transformers (ViTs) have demonstrated remarkable performance across a\nwide range of vision tasks. In particular, self-distillation frameworks such as\nDINO have contributed significantly to these advances. Within such frameworks,\nrandom masking is often utilized to improve training efficiency and introduce\nregularization. However, recent studies have raised concerns that\nindiscriminate random masking may inadvertently eliminate critical semantic\ninformation, motivating the development of more informed masking strategies. In\nthis study, we explore the role of random masking in the self-distillation\nsetting, focusing on the DINO framework. Specifically, we apply random masking\nexclusively to the student's global view, while preserving the student's local\nviews and the teacher's global view in their original, unmasked forms. This\ndesign leverages DINO's multi-view augmentation scheme to retain clean\nsupervision while inducing robustness through masked inputs. We evaluate our\napproach using DINO-Tiny on the mini-ImageNet dataset and show that random\nmasking under this asymmetric setup yields more robust and fine-grained\nattention maps, ultimately enhancing downstream performance.", "comment": "4 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10582v1", "AI": {"title_translation": "重新思考ViT自蒸馏中的随机掩码", "tldr": "本文在DINO框架下，重新思考了ViT自蒸馏中的随机掩码策略。作者提出了一种非对称掩码方法，仅对学生的全局视图应用随机掩码，同时保持学生局部视图和教师全局视图的原始形式，从而在保留干净监督的同时通过掩码输入提高鲁棒性。实验表明，该方法能产生更鲁棒、更细致的注意力图，并提升下游任务性能。", "motivation": "当前研究对无差别随机掩码可能无意中消除关键语义信息表示担忧，这促使了更具信息性的掩码策略的发展。", "method": "研究者在DINO框架下，将随机掩码仅应用于学生的全局视图，而保留学生的局部视图和教师的全局视图为原始未掩码形式。这种设计利用了DINO的多视图增强方案，以在通过掩码输入诱导鲁棒性的同时保留干净的监督。", "result": "在mini-ImageNet数据集上使用DINO-Tiny评估了该方法，结果表明在这种非对称设置下的随机掩码产生了更鲁棒、更细致的注意力图，并最终提升了下游性能。", "conclusion": "在DINO框架下，对ViT自蒸馏中的随机掩码进行非对称应用（仅对学生全局视图掩码）能够提升模型鲁棒性，产生更精细的注意力图，并提高下游任务的表现。", "translation": "Vision Transformers (ViTs) 在广泛的视觉任务中展现出卓越的性能。特别是，DINO等自蒸馏框架对这些进步做出了重大贡献。在此类框架中，随机掩码常用于提高训练效率和引入正则化。然而，最近的研究提出担忧，认为不加区分的随机掩码可能无意中消除关键语义信息，这促使了更具信息性的掩码策略的发展。在本研究中，我们探讨了在自蒸馏设置中随机掩码的作用，重点关注DINO框架。具体而言，我们仅对学生的全局视图应用随机掩码，同时保留学生的局部视图和教师的全局视图为原始、未掩码的形式。这种设计利用DINO的多视图增强方案，以在通过掩码输入诱导鲁棒性的同时保留干净的监督。我们使用DINO-Tiny在mini-ImageNet数据集上评估了我们的方法，结果表明在这种非对称设置下的随机掩码产生了更鲁棒、更细致的注意力图，并最终提升了下游性能。", "summary": "本文探讨了Vision Transformers (ViTs) 自蒸馏框架中随机掩码的作用，尤其关注DINO。针对现有随机掩码可能消除关键语义信息的问题，作者提出了一种非对称掩码策略：仅对学生的全局视图进行随机掩码，而保持学生局部视图和教师全局视图的完整性。该方法利用DINO的多视图增强机制，旨在通过掩码输入提高鲁棒性，同时保留干净的监督信号。在mini-ImageNet数据集上，使用DINO-Tiny进行的评估显示，这种非对称设置下的随机掩码能够生成更鲁棒、更精细的注意力图，并最终提升下游任务的性能。", "keywords": "ViT, 自蒸馏, 随机掩码, DINO, 注意力图", "comments": "该研究通过提出一种新颖的非对称随机掩码策略，解决了ViT自蒸馏中过度掩码可能导致语义信息丢失的问题。其创新点在于巧妙地结合了DINO的多视图特性，在引入鲁棒性的同时确保了监督信号的质量。这为自监督学习中的数据增强和正则化提供了新的视角，对于提升ViT模型在实际应用中的性能和鲁棒性具有重要意义。"}}
{"id": "2506.10979", "title": "How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?", "authors": ["Sohee Yang", "Sang-Woo Lee", "Nora Kassner", "Daniela Gottesman", "Sebastian Riedel", "Mor Geva"], "summary": "Recent reasoning models show the ability to reflect, backtrack, and\nself-validate their reasoning, which is crucial in spotting mistakes and\narriving at accurate solutions. A natural question that arises is how\neffectively models can perform such self-reevaluation. We tackle this question\nby investigating how well reasoning models identify and recover from four types\nof unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to\nthe question, thoughts misdirecting the question as a slightly different\nquestion, and thoughts that lead to incorrect answers. We show that models are\neffective at identifying most unhelpful thoughts but struggle to recover from\nthe same thoughts when these are injected into their thinking process, causing\nsignificant performance drops. Models tend to naively continue the line of\nreasoning of the injected irrelevant thoughts, which showcases that their\nself-reevaluation abilities are far from a general \"meta-cognitive\" awareness.\nMoreover, we observe non/inverse-scaling trends, where larger models struggle\nmore than smaller ones to recover from short irrelevant thoughts, even when\ninstructed to reevaluate their reasoning. We demonstrate the implications of\nthese findings with a jailbreak experiment using irrelevant thought injection,\nshowing that the smallest models are the least distracted by\nharmful-response-triggering thoughts. Overall, our findings call for\nimprovement in self-reevaluation of reasoning models to develop better\nreasoning and safer systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10979v1", "AI": {"title_translation": "推理模型识别和纠正无益思维的能力如何？", "tldr": "推理模型能够识别无益思维，但在这些思维被注入其思考过程时，它们难以从中恢复，导致性能显著下降，且大型模型在此方面表现出反向扩展趋势，揭示了其自我评估能力的局限性。", "motivation": "近期推理模型展现出反思、回溯和自我验证推理的能力，这对于发现错误和得出准确解决方案至关重要。因此，一个自然而然的问题是模型能多有效地进行这种自我重新评估。", "method": "本研究通过调查推理模型如何有效识别和纠正四种无益思维来解决这一问题：无信息量的冗长思维、与问题无关的思维、将问题误导为略有不同问题的思维，以及导致错误答案的思维。研究通过将这些思维注入模型的思考过程来评估其表现。", "result": "研究表明，模型在识别大多数无益思维方面是有效的，但当这些思维被注入其思考过程时，模型难以从中恢复，导致性能显著下降。模型倾向于天真地延续被注入的无关思维的推理路线，这表明其自我评估能力远非普遍的“元认知”意识。此外，研究观察到非/反向扩展趋势，即大型模型在纠正短无关思维方面比小型模型更困难，即使被指示重新评估其推理。研究通过使用无关思维注入的越狱实验证明了这些发现的影响，显示最小的模型最不受触发有害响应的思维干扰。", "conclusion": "总的来说，我们的发现呼吁改进推理模型的自我评估能力，以开发更好的推理和更安全的系统。", "translation": "最近的推理模型展示了反思、回溯和自我验证其推理的能力，这对于发现错误和得出准确的准确解决方案至关重要。一个自然而然的问题是模型能多有效地执行这种自我重新评估。我们通过调查推理模型如何有效地识别和纠正四种无益思维来解决这个问题：无信息量的冗长思维、与问题无关的思维、将问题误导为略有不同问题的思维，以及导致错误答案的思维。我们发现模型在识别大多数无益思维方面是有效的，但当这些思维被注入其思考过程时，它们难以从中恢复，导致性能显著下降。模型倾向于天真地延续被注入的无关思维的推理路线，这表明它们的自我重新评估能力远非普遍的“元认知”意识。此外，我们观察到非/反向扩展趋势，即大型模型在纠正短无关思维方面比小型模型更困难，即使被指示重新评估其推理。我们通过使用无关思维注入的越狱实验证明了这些发现的影响，显示最小的模型最不受触发有害响应的思维干扰。总的来说，我们的发现呼吁改进推理模型的自我重新评估能力，以开发更好的推理和更安全的系统。", "summary": "本研究探讨了推理模型识别和纠正无益思维的能力。尽管模型能有效识别大多数无益思维，但当这些思维被注入其思考过程时，它们难以恢复，导致性能显著下降。研究发现模型倾向于延续被注入的无关思维，且大型模型在恢复能力上甚至表现出反向扩展趋势。这些发现揭示了当前推理模型自我评估能力的局限性，并强调了在开发更强大和安全的人工智能系统方面改进其元认知能力的重要性。", "keywords": "推理模型, 自我评估, 无益思维, 元认知, 反向扩展", "comments": "这篇论文揭示了当前推理模型在“元认知”能力上的一个关键局限性：尽管它们可以识别无益思维，但一旦这些思维被强行植入，模型便难以有效纠正，反而会盲目沿用。其中，大型模型在恢复能力上表现出的“反向扩展”趋势尤为重要且出人意料，挑战了“越大越好”的直觉。越狱实验进一步强调了这种缺陷对系统安全的影响。这项工作对于推动AI系统在鲁棒性和安全性方面的发展具有重要意义。"}}
{"id": "2506.10594", "title": "Hierarchical Error Assessment of CAD Models for Aircraft Manufacturing-and-Measurement", "authors": ["Jin Huang", "Honghua Chen", "Mingqiang Wei"], "summary": "The most essential feature of aviation equipment is high quality, including\nhigh performance, high stability and high reliability. In this paper, we\npropose a novel hierarchical error assessment framework for aircraft CAD models\nwithin a manufacturing-and-measurement platform, termed HEA-MM. HEA-MM employs\nstructured light scanners to obtain comprehensive 3D measurements of\nmanufactured workpieces. The measured point cloud is registered with the\nreference CAD model, followed by an error analysis conducted at three\nhierarchical levels: global, part, and feature. At the global level, the error\nanalysis evaluates the overall deviation of the scanned point cloud from the\nreference CAD model. At the part level, error analysis is performed on these\npatches underlying the point clouds. We propose a novel optimization-based\nprimitive refinement method to obtain a set of meaningful patches of point\nclouds. Two basic operations, splitting and merging, are introduced to refine\nthe coarse primitives. At the feature level, error analysis is performed on\ncircular holes, which are commonly found in CAD models. To facilitate it, a\ntwo-stage algorithm is introduced for the detection of circular holes. First,\nedge points are identified using a tensor-voting algorithm. Then, multiple\ncircles are fitted through a hypothesize-and-clusterize framework, ensuring\naccurate detection and analysis of the circular features. Experimental results\non various aircraft CAD models demonstrate the effectiveness of our proposed\nmethod.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10594v1", "AI": {"title_translation": "飞机制造与测量中CAD模型的层次化误差评估", "tldr": "本文提出了一种名为HEA-MM的层次化误差评估框架，用于飞机CAD模型在制造和测量过程中的误差分析，通过结构光扫描、点云配准，并在全局、零件和特征三个层次进行误差分析。", "motivation": "航空设备对质量要求高，包括高性能、高稳定性和高可靠性。因此，需要对飞机CAD模型进行精确的误差评估以确保产品质量。", "method": "本文提出了HEA-MM层次化误差评估框架。该框架首先使用结构光扫描仪获取制造工件的全面3D点云数据，然后将测量点云与参考CAD模型进行配准。接着，在三个层次进行误差分析：全局层面评估扫描点云与CAD模型的整体偏差；零件层面通过提出一种基于优化的原始细化方法（引入分裂和合并操作）来分析有意义的点云补丁；特征层面则针对CAD模型中常见的圆形孔，引入了两阶段算法进行检测（首先使用张量投票算法识别边缘点，然后通过假设-聚类框架拟合多个圆）。", "result": "在各种飞机CAD模型上的实验结果证明了所提出方法的有效性。", "conclusion": "所提出的HEA-MM框架能够有效地评估飞机CAD模型在制造和测量过程中的误差，有助于确保航空设备的高质量。", "translation": "航空设备最重要的特点是高质量，包括高性能、高稳定性和高可靠性。本文提出了一种用于飞机CAD模型在制造和测量平台中的新型层次化误差评估框架，称为HEA-MM。HEA-MM采用结构光扫描仪获取制造工件的全面3D测量数据。测量点云与参考CAD模型配准后，在全局、零件和特征三个层次进行误差分析。在全局层面，误差分析评估扫描点云与参考CAD模型的整体偏差。在零件层面，对点云下方的这些补丁进行误差分析。我们提出了一种新颖的基于优化的原始细化方法，以获得一组有意义的点云补丁。引入了分裂和合并两种基本操作来细化粗糙的原始形状。在特征层面，对CAD模型中常见的圆形孔进行误差分析。为实现此目的，引入了两阶段算法用于圆形孔的检测。首先，使用张量投票算法识别边缘点。然后，通过假设-聚类框架拟合多个圆，确保圆形特征的准确检测和分析。在各种飞机CAD模型上的实验结果证明了我们所提出方法的有效性。", "summary": "本文提出了一种名为HEA-MM的层次化误差评估框架，旨在解决飞机制造与测量中CAD模型的质量评估问题。该框架利用结构光扫描获取工件3D点云数据，并将其与参考CAD模型配准。随后，在全局、零件和特征三个层面进行误差分析：全局层面评估整体偏差；零件层面通过优化细化方法分析点云补丁；特征层面则针对圆形孔，采用两阶段算法进行检测和分析。实验结果验证了该方法的有效性。", "keywords": "层次化误差评估, CAD模型, 飞机制造, 结构光扫描, 点云分析", "comments": "该论文提出了一种创新的、多层次的误差评估框架HEA-MM，通过结合结构光扫描、点云处理和分层分析（全局、零件、特征），为航空制造中的CAD模型质量控制提供了全面且精细的解决方案。特别是在零件和特征层面的优化和检测算法，体现了其在处理复杂几何特征方面的精细化能力。这对于确保航空设备的高质量制造具有重要意义。"}}
{"id": "2506.10630", "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs", "authors": ["Yucong Luo", "Yitong Zhou", "Mingyue Cheng", "Jiahao Wang", "Daoyu Wang", "Tingyue Pan", "Jintao Zhang"], "summary": "To advance time series forecasting (TSF), various methods have been proposed\nto improve prediction accuracy, evolving from statistical techniques to\ndata-driven deep learning architectures. Despite their effectiveness, most\nexisting methods still adhere to a fast thinking paradigm-relying on extracting\nhistorical patterns and mapping them to future values as their core modeling\nphilosophy, lacking an explicit thinking process that incorporates intermediate\ntime series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)\nhave shown remarkable multi-step reasoning capabilities, offering an\nalternative way to overcome these issues. However, prompt engineering alone\npresents several limitations - including high computational cost, privacy\nrisks, and limited capacity for in-depth domain-specific time series reasoning.\nTo address these limitations, a more promising approach is to train LLMs to\ndevelop slow thinking capabilities and acquire strong time series reasoning\nskills. For this purpose, we propose Time-R1, a two-stage reinforcement\nfine-tuning framework designed to enhance multi-step reasoning ability of LLMs\nfor time series forecasting. Specifically, the first stage conducts supervised\nfine-tuning for warmup adaptation, while the second stage employs reinforcement\nlearning to improve the model's generalization ability. Particularly, we design\na fine-grained multi-objective reward specifically for time series forecasting,\nand then introduce GRIP (group-based relative importance for policy\noptimization), which leverages non-uniform sampling to further encourage and\noptimize the model's exploration of effective reasoning paths. Experiments\ndemonstrate that Time-R1 significantly improves forecast performance across\ndiverse datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10630v1", "AI": {"title_translation": "时间序列预测即推理：一种基于强化LLM的慢思考方法", "tldr": "Time-R1提出了一种两阶段强化微调框架，使LLM具备慢思考和多步推理能力，显著提升了时间序列预测性能。", "motivation": "现有时间序列预测（TSF）方法多采用“快思考”范式，缺乏显式推理过程。新兴的慢思考大型语言模型（LLMs）虽展现出多步推理能力，但单独的提示工程存在计算成本高、隐私风险和领域特定推理能力有限等局限性。为克服这些问题，研究旨在训练LLMs发展慢思考能力并获得强大的时间序列推理技能。", "method": "本文提出了Time-R1，一个两阶段强化微调框架，旨在增强LLM用于时间序列预测的多步推理能力。第一阶段进行有监督微调以进行预热适应；第二阶段采用强化学习来提高模型的泛化能力。特别地，设计了一种针对时间序列预测的细粒度多目标奖励，并引入了GRIP（group-based relative importance for policy optimization），利用非均匀采样进一步鼓励和优化模型对有效推理路径的探索。", "result": "实验证明，Time-R1显著提高了跨多样数据集的预测性能。", "conclusion": "Time-R1通过两阶段强化微调，成功使LLM具备了慢思考和增强的多步时间序列推理能力，从而显著提升了预测性能。", "translation": "为了推进时间序列预测（TSF），已经提出了各种方法来提高预测精度，从统计技术发展到数据驱动的深度学习架构。尽管它们有效，但大多数现有方法仍然遵循“快思考”范式——依赖于提取历史模式并将其映射到未来值作为其核心建模理念，缺乏结合中间时间序列推理的显式思考过程。与此同时，新兴的慢思考LLMs（例如OpenAI-o1）已经展示出卓越的多步推理能力，为克服这些问题提供了另一种途径。然而，单独的提示工程存在一些局限性——包括高计算成本、隐私风险以及领域特定时间序列推理能力有限。为了解决这些局限性，一种更有前景的方法是训练LLMs发展慢思考能力并获得强大的时间序列推理技能。为此，我们提出了Time-R1，一个两阶段强化微调框架，旨在增强LLM用于时间序列预测的多步推理能力。具体而言，第一阶段进行有监督微调以进行预热适应，而第二阶段采用强化学习来提高模型的泛化能力。特别地，我们设计了一种针对时间序列预测的细粒度多目标奖励，然后引入了GRIP（group-based relative importance for policy optimization），它利用非均匀采样进一步鼓励和优化模型对有效推理路径的探索。实验表明，Time-R1显著提高了跨多样数据集的预测性能。", "summary": "该论文针对现有时间序列预测（TSF）方法缺乏显式推理过程以及LLM提示工程的局限性，提出了一种名为Time-R1的两阶段强化微调框架。Time-R1旨在训练LLM发展“慢思考”能力，增强其多步时间序列推理技能。框架包括有监督预热微调和强化学习泛化阶段，并引入了细粒度多目标奖励和GRIP机制以优化推理路径探索。实验结果表明，Time-R1显著提升了多样数据集上的预测性能。", "keywords": "时间序列预测, LLMs, 强化学习, 慢思考, 多步推理", "comments": "该论文通过将LLMs的“慢思考”能力与强化学习相结合，为时间序列预测引入了一种新颖的推理范式，有效解决了传统方法的局限性。其两阶段微调框架以及为时间序列预测量身定制的多目标奖励和GRIP优化机制是重要的创新点。"}}
{"id": "2506.10601", "title": "Semantic-decoupled Spatial Partition Guided Point-supervised Oriented Object Detection", "authors": ["Xinyuan Liu", "Hang Xu", "Yike Ma", "Yucheng Zhang", "Feng Dai"], "summary": "Recent remote sensing tech advancements drive imagery growth, making oriented\nobject detection rapid development, yet hindered by labor-intensive annotation\nfor high-density scenes. Oriented object detection with point supervision\noffers a cost-effective solution for densely packed scenes in remote sensing,\nyet existing methods suffer from inadequate sample assignment and instance\nconfusion due to rigid rule-based designs. To address this, we propose SSP\n(Semantic-decoupled Spatial Partition), a unified framework that synergizes\nrule-driven prior injection and data-driven label purification. Specifically,\nSSP introduces two core innovations: 1) Pixel-level Spatial Partition-based\nSample Assignment, which compactly estimates the upper and lower bounds of\nobject scales and mines high-quality positive samples and hard negative samples\nthrough spatial partitioning of pixel maps. 2) Semantic Spatial Partition-based\nBox Extraction, which derives instances from spatial partitions modulated by\nsemantic maps and reliably converts them into bounding boxes to form\npseudo-labels for supervising the learning of downstream detectors. Experiments\non DOTA-v1.0 and others demonstrate SSP\\' s superiority: it achieves 45.78% mAP\nunder point supervision, outperforming SOTA method PointOBB-v2 by 4.10%.\nFurthermore, when integrated with ORCNN and ReDet architectures, the SSP\nframework achieves mAP values of 47.86% and 48.50%, respectively. The code is\navailable at https://github.com/antxinyuan/ssp.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10601v1", "AI": {"title_translation": "语义解耦空间划分引导的点监督定向目标检测", "tldr": "提出SSP框架，通过像素级和语义级空间划分解决点监督定向目标检测中样本分配和实例混淆问题，显著提升性能。", "motivation": "现有遥感图像中高密度场景的定向目标检测面临劳动密集型标注的挑战。点监督虽然成本效益高，但现有方法因僵化的基于规则的设计而存在样本分配不足和实例混淆问题。", "method": "提出SSP（语义解耦空间划分）统一框架，结合规则驱动的先验注入和数据驱动的标签净化。核心创新包括：1) 基于像素级空间划分的样本分配，用于估计目标尺度并挖掘高质量正负样本。2) 基于语义空间划分的框提取，用于从空间划分中导出实例并转换为伪标签以监督下游检测器。", "result": "在DOTA-v1.0数据集上，SSP在点监督下实现了45.78%的mAP，比SOTA方法PointOBB-v2高出4.10%。与ORCNN和ReDet架构集成时，SSP框架分别达到47.86%和48.50%的mAP。", "conclusion": "SSP框架有效解决了点监督定向目标检测中的挑战，通过其创新的空间划分方法显著提升了检测性能。", "translation": "近期遥感技术进步推动了图像增长，使定向目标检测快速发展，但高密度场景劳动密集型标注阻碍了其发展。点监督的定向目标检测为遥感中密集场景提供了成本效益高的解决方案，但现有方法由于僵化的基于规则的设计，存在样本分配不足和实例混淆的问题。为解决此问题，我们提出了SSP（语义解耦空间划分），一个统一的框架，协同规则驱动的先验注入和数据驱动的标签净化。具体而言，SSP引入了两项核心创新：1）基于像素级空间划分的样本分配，它紧凑地估计目标尺度的上下限，并通过像素图的空间划分挖掘高质量的正样本和难负样本。2）基于语义空间划分的框提取，它从语义图调制的空间划分中导出实例，并可靠地将其转换为边界框以形成伪标签，用于监督下游检测器的学习。在DOTA-v1.0及其他数据集上的实验表明SSP的优越性：在点监督下实现了45.78%的mAP，比SOTA方法PointOBB-v2高出4.10%。此外，当与ORCNN和ReDet架构集成时，SSP框架分别实现了47.86%和48.50%的mAP值。代码可在https://github.com/antxinyuan/ssp获取。", "summary": "本文针对遥感图像中高密度场景下点监督定向目标检测存在的样本分配不足和实例混淆问题，提出了SSP（语义解耦空间划分）框架。SSP通过像素级空间划分进行样本分配和语义空间划分进行框提取两项核心创新，有效地结合了规则驱动的先验知识和数据驱动的标签净化。实验证明，SSP在DOTA-v1.0数据集上显著优于现有SOTA方法，并在与主流检测器集成时表现出色，为点监督定向目标检测提供了一种高效且准确的解决方案。", "keywords": "点监督, 定向目标检测, 空间划分, 样本分配, 伪标签", "comments": "SSP框架的创新之处在于其将规则驱动的先验知识与数据驱动的标签净化相结合，通过精巧的空间划分机制解决了点监督下样本分配和实例混淆的难题。这种方法在降低标注成本的同时，显著提升了高密度遥感场景中定向目标检测的性能，具有重要的实际应用价值。"}}
{"id": "2506.10632", "title": "Hessian Geometry of Latent Space in Generative Models", "authors": ["Alexander Lobashev", "Dmitry Guskov", "Maria Larchenko", "Mikhail Tamm"], "summary": "This paper presents a novel method for analyzing the latent space geometry of\ngenerative models, including statistical physics models and diffusion models,\nby reconstructing the Fisher information metric. The method approximates the\nposterior distribution of latent variables given generated samples and uses\nthis to learn the log-partition function, which defines the Fisher metric for\nexponential families. Theoretical convergence guarantees are provided, and the\nmethod is validated on the Ising and TASEP models, outperforming existing\nbaselines in reconstructing thermodynamic quantities. Applied to diffusion\nmodels, the method reveals a fractal structure of phase transitions in the\nlatent space, characterized by abrupt changes in the Fisher metric. We\ndemonstrate that while geodesic interpolations are approximately linear within\nindividual phases, this linearity breaks down at phase boundaries, where the\ndiffusion model exhibits a divergent Lipschitz constant with respect to the\nlatent space. These findings provide new insights into the complex structure of\ndiffusion model latent spaces and their connection to phenomena like phase\ntransitions. Our source code is available at\nhttps://github.com/alobashev/hessian-geometry-of-diffusion-models.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10632v1", "AI": {"title_translation": "生成模型潜在空间的Hessian几何", "tldr": "本文提出了一种新颖的方法，通过重建Fisher信息度量来分析生成模型（包括统计物理模型和扩散模型）的潜在空间几何，揭示了扩散模型潜在空间中相变的碎形结构。", "motivation": "该研究旨在提出一种新方法来分析生成模型（包括统计物理模型和扩散模型）的潜在空间几何，并深入理解扩散模型潜在空间的复杂结构及其与相变等现象的联系。", "method": "该方法通过重建Fisher信息度量来分析潜在空间几何。它近似给定生成样本的潜在变量的后验分布，并利用此来学习定义指数族Fisher度量的对数配分函数。该方法提供了理论收敛保证，并在Ising和TASEP模型上进行了验证，随后应用于扩散模型。", "result": "该方法在重建热力学量方面优于现有基线（在Ising和TASEP模型上）。应用于扩散模型时，它揭示了潜在空间中相变的碎形结构，其特征是Fisher度量的突然变化。研究表明，测地线插值在单个相内近似线性，但在相边界处这种线性失效，此时扩散模型表现出相对于潜在空间的发散Lipschitz常数。", "conclusion": "这些发现为扩散模型潜在空间的复杂结构及其与相变等现象的联系提供了新的见解。", "translation": "本文提出了一种分析生成模型（包括统计物理模型和扩散模型）潜在空间几何的新方法，通过重建Fisher信息度量。该方法近似给定生成样本的潜在变量的后验分布，并利用此来学习对数配分函数，该函数定义了指数族的Fisher度量。文中提供了理论收敛保证，并在Ising和TASEP模型上验证了该方法，其在重建热力学量方面优于现有基线。应用于扩散模型时，该方法揭示了潜在空间中相变的碎形结构，其特征是Fisher度量的突然变化。我们证明了测地线插值在单个相内近似线性，但在相边界处这种线性失效，此时扩散模型相对于潜在空间表现出散度的Lipschitz常数。这些发现为扩散模型潜在空间的复杂结构及其与相变等现象的联系提供了新的见解。我们的源代码可在https://github.com/alobashev/hessian-geometry-of-diffusion-models获取。", "summary": "本文提出了一种通过重建Fisher信息度量来分析生成模型（包括统计物理模型和扩散模型）潜在空间几何的新方法。该方法通过近似潜在变量的后验分布来学习对数配分函数。研究在统计物理模型上表现出优越性，并在扩散模型中揭示了潜在空间中相变的碎形结构，其中测地线插值在相边界处失去线性，并且Lipschitz常数发散。这些发现为扩散模型潜在空间的复杂结构及其与相变现象的联系提供了新见解。", "keywords": "潜在空间几何, Fisher信息度量, 生成模型, 扩散模型, 相变", "comments": "该论文通过利用Fisher信息度量，为理解生成模型（特别是扩散模型）的复杂潜在空间提供了一种创新方法。对碎形相变和边界处线性失效的发现是重要成果，它将统计物理学的概念与深度生成模型联系起来。这有助于更好地理解和控制生成过程。"}}
{"id": "2506.10605", "title": "High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model", "authors": ["Eshan Ramesh", "Nishio Takayuki"], "summary": "We present LatentCSI, a novel method for generating images of the physical\nenvironment from WiFi CSI measurements that leverages a pretrained latent\ndiffusion model (LDM). Unlike prior approaches that rely on complex and\ncomputationally intensive techniques such as GANs, our method employs a\nlightweight neural network to map CSI amplitudes directly into the latent space\nof an LDM. We then apply the LDM's denoising diffusion model to the latent\nrepresentation with text-based guidance before decoding using the LDM's\npretrained decoder to obtain a high-resolution image. This design bypasses the\nchallenges of pixel-space image generation and avoids the explicit image\nencoding stage typically required in conventional image-to-image pipelines,\nenabling efficient and high-quality image synthesis. We validate our approach\non two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi\ndevices and cameras; and a subset of the publicly available MM-Fi dataset. The\nresults demonstrate that LatentCSI outperforms baselines of comparable\ncomplexity trained directly on ground-truth images in both computational\nefficiency and perceptual quality, while additionally providing practical\nadvantages through its unique capacity for text-guided controllability.", "comment": "6 pages, 4 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10605v1", "AI": {"title_translation": "使用预训练潜在扩散模型从WiFi CSI生成高分辨率高效图像", "tldr": "LatentCSI是一种新方法，利用预训练的潜在扩散模型，将WiFi CSI测量数据高效地生成高分辨率图像，并通过文本指导实现可控性，且性能优于现有基线。", "motivation": "现有从WiFi CSI生成图像的方法通常依赖于复杂且计算密集的技术（如GANs），且面临像素空间图像生成和图像编码阶段的挑战。本文旨在提出一种更高效、高质量且可控的方法来解决这些问题。", "method": "提出LatentCSI，一种利用预训练潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像的方法。该方法使用一个轻量级神经网络将CSI幅度直接映射到LDM的潜在空间，然后应用LDM的去噪扩散模型对潜在表示进行文本指导，最后通过LDM的预训练解码器生成高分辨率图像。该方法避免了像素空间图像生成和显式图像编码阶段。", "result": "LatentCSI在计算效率和感知质量方面均优于直接在真实图像上训练的、复杂度相当的基线方法。此外，它还通过其独特的文本引导可控性提供了实际优势。该方法在自行收集的宽带CSI数据集和MM-Fi数据集子集上进行了验证。", "conclusion": "LatentCSI是一种有效且高效的从WiFi CSI生成高分辨率图像的方法，通过利用预训练的潜在扩散模型和文本指导，在性能和实用性上均超越了现有基线。", "translation": "我们提出了LatentCSI，一种利用预训练潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像的新方法。与以往依赖复杂且计算密集型技术（如GAN）的方法不同，我们的方法采用轻量级神经网络将CSI幅度直接映射到LDM的潜在空间。然后，我们将LDM的去噪扩散模型应用于潜在表示，并进行基于文本的指导，最后使用LDM的预训练解码器进行解码以获得高分辨率图像。这种设计绕过了像素空间图像生成的挑战，并避免了传统图像到图像管道中通常所需的显式图像编码阶段，从而实现了高效、高质量的图像合成。我们在两个数据集上验证了我们的方法：一个是我们使用现成WiFi设备和相机收集的宽带CSI数据集；以及公开可用的MM-Fi数据集的一个子集。结果表明，LatentCSI在计算效率和感知质量方面均优于直接在真实图像上训练的、复杂度相当的基线方法，同时通过其独特的文本引导可控性提供了实际优势。", "summary": "LatentCSI是一种新颖的方法，它利用预训练的潜在扩散模型，将WiFi CSI测量数据高效地转换为高分辨率物理环境图像。该方法通过轻量级神经网络将CSI直接映射到LDM的潜在空间，并结合文本指导进行图像生成，避免了传统方法的复杂性和计算密集性。实验证明，LatentCSI在计算效率和图像质量上均优于现有基线，并具备独特的文本引导可控性。", "keywords": "WiFi CSI, 图像生成, 潜在扩散模型, 高分辨率, 文本指导", "comments": "这篇论文提出了一种创新的方法，将WiFi CSI数据与潜在扩散模型结合，实现了高效且高质量的图像生成。其创新点在于避免了传统的像素空间生成和显式编码阶段，并通过文本指导增加了实用性。该方法在利用非传统传感器数据进行环境感知方面具有重要意义，可能为未来低成本、非接触式感知技术开辟新途径。"}}
{"id": "2506.10647", "title": "Data Shifts Hurt CoT: A Theoretical Study", "authors": ["Lang Yin", "Debangshu Banerjee", "Gagandeep Singh"], "summary": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10647v1", "AI": {"title_translation": "数据漂移损害CoT：一项理论研究", "tldr": "本文首次从理论上严谨研究了数据漂移（包括分布漂移和数据投毒）对思维链（CoT）在解决k-parity问题时性能的负面影响，发现CoT在这种情况下甚至比直接预测表现更差。", "motivation": "尽管思维链（CoT）已被证明能有效提升大型语言模型（LLMs）解决复杂问题的能力（如k-parity问题），但其成功依赖于两个关键假设：训练和测试数据分布一致，以及训练数据无污染且推理步骤正确。然而，在现实世界中这些假设往往不成立。虽然数据漂移的风险已引起关注，但其对CoT的具体损害尚未得到严谨的理论研究。", "method": "本文通过聚焦k-parity问题，利用一种成熟的CoT分解方法，深入研究了两种类型的数据漂移（分布漂移和数据投毒）对经CoT训练的模型质量的联合影响。", "result": "研究揭示了一个令人惊讶的现象：在数据漂移条件下，思维链（CoT）在学习奇偶校验问题上的表现甚至比直接生成预测更差。技术结果还对这种影响的内在机制原因提供了严谨而全面的解释。", "conclusion": "结论是数据漂移（包括分布漂移和数据投毒）对思维链（CoT）的性能有显著的负面影响，尤其是在k-parity问题上，甚至可能导致CoT表现劣于直接预测。本研究为理解CoT在非理想数据条件下的脆弱性提供了理论基础。", "translation": "思维链（CoT）已被应用于各种大型语言模型（LLMs），并被证明能有效提高输出质量。在最近的研究中，Transformer被证明在表达能力方面具有绝对上限，因此它们无法解决许多计算上困难的问题。然而，在CoT的赋能下，Transformer被证明能够有效解决一些困难问题，例如k-parity问题。然而，这些工作依赖于两个必要的假设：(1) 相同的训练和测试分布，以及 (2) 训练数据无污染且推理步骤正确。然而，在现实世界中，这些假设并非总是成立。尽管数据漂移的风险已引起关注，但据我们所知，我们的工作是第一个严谨研究此类漂移所造成的确切损害。本文聚焦于k-parity问题，研究了两种类型的数据漂移：分布漂移和数据投毒，对通过成熟的CoT分解方法获得的训练模型质量的联合影响。除了揭示CoT在学习奇偶校验方面比直接生成预测导致更差性能的惊人现象外，我们的技术结果还对这种影响的内在机制原因提供了严谨而全面的解释。", "summary": "本文首次从理论上严谨探讨了数据漂移（包括分布漂移和数据投毒）对思维链（CoT）在大型语言模型中性能的影响。研究发现，在非理想数据条件下，CoT在解决k-parity问题时表现出人意料地差，甚至不如直接预测。文章深入分析了造成这种现象的机制原因，强调了CoT在现实世界应用中面临的脆弱性。", "keywords": "数据漂移, 思维链 (CoT), k-parity问题, 分布漂移, 数据投毒", "comments": "这项工作具有重要的创新性，因为它首次对数据漂移对思维链（CoT）性能的损害进行了严谨的理论研究。其发现CoT在数据漂移下表现甚至不如直接预测，这一现象令人惊讶，并对CoT在现实世界非理想条件下的鲁棒性提出了质疑。该研究为理解CoT的局限性提供了新的视角，并可能启发未来在鲁棒CoT方面的研究。"}}
{"id": "2506.10609", "title": "MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling", "authors": ["Liang Yin", "Xudong Xie", "Zhang Li", "Xiang Bai", "Yuliang Liu"], "summary": "Scene text retrieval has made significant progress with the assistance of\naccurate text localization. However, existing approaches typically require\ncostly bounding box annotations for training. Besides, they mostly adopt a\ncustomized retrieval strategy but struggle to unify various types of queries to\nmeet diverse retrieval needs. To address these issues, we introduce Muti-query\nScene Text retrieval with Attention Recycling (MSTAR), a box-free approach for\nscene text retrieval. It incorporates progressive vision embedding to\ndynamically capture the multi-grained representation of texts and harmonizes\nfree-style text queries with style-aware instructions. Additionally, a\nmulti-instance matching module is integrated to enhance vision-language\nalignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset,\nthe first benchmark designed to evaluate the multi-query scene text retrieval\ncapability of models, comprising four query types and 16k images. Extensive\nexperiments demonstrate the superiority of our method across seven public\ndatasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous\nstate-of-the-art model by 6.4% in MAP on Total-Text while eliminating box\nannotation costs. Moreover, on the MQTR benchmark, MSTAR significantly\noutperforms the previous models by an average of 8.5%. The code and datasets\nare available at https://github.com/yingift/MSTAR.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10609v1", "AI": {"title_translation": "MSTAR：基于注意力循环的无框多查询场景文本检索", "tldr": "MSTAR是一种无需边界框、支持多查询的场景文本检索方法，性能超越现有SOTA模型，并引入了新的多查询数据集MQTR。", "motivation": "现有场景文本检索方法通常需要昂贵的边界框标注进行训练，并且大多采用定制的检索策略，难以统一各种查询类型以满足多样化的检索需求。", "method": "本文引入了MSTAR（多查询场景文本检索与注意力循环），一种无框的场景文本检索方法。它整合了渐进式视觉嵌入以动态捕获文本的多粒度表示，并协调自由风格文本查询与风格感知指令。此外，还集成了一个多实例匹配模块来增强视觉-语言对齐。研究团队还构建了Multi-Query Text Retrieval (MQTR) 数据集，这是第一个旨在评估模型多查询场景文本检索能力的基准，包含四种查询类型和16k图像。", "result": "广泛的实验表明MSTAR在七个公共数据集和MQTR数据集上均表现出优越性。值得注意的是，MSTAR在Total-Text数据集上的平均精度（MAP）比之前的最先进模型高出6.4%，同时消除了边界框标注成本。此外，在MQTR基准测试中，MSTAR比之前的模型平均高出8.5%。", "conclusion": "MSTAR成功地提出了一种无需边界框的场景文本检索方法，有效解决了传统方法对昂贵标注的依赖和多样化查询的统一问题，并在多个基准测试中取得了显著的性能提升。", "translation": "场景文本检索在精确文本定位的辅助下取得了显著进展。然而，现有方法通常需要昂贵的边界框标注进行训练。此外，它们大多采用定制的检索策略，但难以统一各种类型的查询以满足多样化的检索需求。为了解决这些问题，我们引入了MSTAR（基于注意力循环的多查询场景文本检索），一种无需边界框的场景文本检索方法。它结合了渐进式视觉嵌入以动态捕获文本的多粒度表示，并协调自由风格文本查询与风格感知指令。此外，还集成了一个多实例匹配模块来增强视觉-语言对齐。更进一步，我们构建了Multi-Query Text Retrieval (MQTR) 数据集，这是第一个旨在评估模型多查询场景文本检索能力的基准，包含四种查询类型和16k图像。广泛的实验表明我们的方法在七个公共数据集和MQTR数据集上均表现出优越性。值得注意的是，MSTAR在Total-Text数据集上的MAP指标上比之前的最先进模型高出6.4%，同时消除了边界框标注成本。此外，在MQTR基准测试中，MSTAR比之前的模型平均高出8.5%。代码和数据集可在https://github.com/yingift/MSTAR获取。", "summary": "MSTAR是一种创新的无框多查询场景文本检索方法，旨在解决现有方法对边界框标注的依赖和难以统一多种查询类型的问题。该方法通过渐进式视觉嵌入、自由风格查询与风格感知指令的协调以及多实例匹配模块来提升文本表示和视觉-语言对齐。此外，本文还引入了首个多查询场景文本检索基准MQTR数据集。实验证明，MSTAR在多个公共数据集和MQTR数据集上均显著超越了现有最先进模型，尤其是在消除标注成本的同时取得了性能提升。", "keywords": "场景文本检索, 无框, 多查询, 注意力循环, MQTR数据集", "comments": "MSTAR的创新性在于其“无框”设计，这极大地降低了场景文本检索的标注成本，具有重要的实践意义。同时，它统一了多种查询类型，提升了方法的普适性。引入MQTR数据集填补了多查询场景文本检索基准的空白，对该领域的研究发展具有推动作用。"}}
{"id": "2506.10680", "title": "Saturation Self-Organizing Map", "authors": ["Igor Urbanik", "Paweł Gajewski"], "summary": "Continual learning poses a fundamental challenge for neural systems, which\noften suffer from catastrophic forgetting when exposed to sequential tasks.\nSelf-Organizing Maps (SOMs), despite their interpretability and efficiency, are\nnot immune to this issue. In this paper, we introduce Saturation\nSelf-Organizing Maps (SatSOM)-an extension of SOMs designed to improve\nknowledge retention in continual learning scenarios. SatSOM incorporates a\nnovel saturation mechanism that gradually reduces the learning rate and\nneighborhood radius of neurons as they accumulate information. This effectively\nfreezes well-trained neurons and redirects learning to underutilized areas of\nthe map.", "comment": "github repository: https://github.com/Radinyn/satsom", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10680v1", "AI": {"title_translation": "饱和自组织映射", "tldr": "提出饱和自组织映射（SatSOM），通过新的饱和机制逐步冻结已训练神经元并重定向学习，以改善自组织映射（SOM）在持续学习中的知识保留问题。", "motivation": "持续学习中神经网络系统面临灾难性遗忘的挑战，即使自组织映射（SOMs）也无法避免。", "method": "引入饱和自组织映射（SatSOM），这是SOM的扩展。SatSOM包含一个新的饱和机制，该机制随着神经元积累信息，逐渐降低其学习率和邻域半径。这有效地冻结了训练良好的神经元，并将学习重定向到地图中未充分利用的区域。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "持续学习对神经网络系统提出了根本性挑战，这些系统在接触到顺序任务时经常遭受灾难性遗忘。自组织映射（SOMs）尽管具有可解释性和效率，也未能幸免于此问题。在本文中，我们介绍了饱和自组织映射（SatSOM）——SOMs的扩展，旨在改善持续学习场景中的知识保留。SatSOM引入了一种新颖的饱和机制，该机制随着神经元积累信息，逐渐降低其学习率和邻域半径。这有效地冻结了训练良好的神经元，并将学习重定向到地图中未充分利用的区域。", "summary": "本文提出了饱和自组织映射（SatSOM），作为自组织映射（SOM）在持续学习场景下的改进。SatSOM通过引入独特的饱和机制，能够根据神经元积累的信息量逐步降低其学习率和邻域半径。这种机制旨在“冻结”已充分训练的神经元，并将学习过程引导至地图中尚未充分利用的区域，从而有效缓解持续学习中常见的灾难性遗忘问题，增强知识保留能力。", "keywords": "持续学习, 自组织映射, 灾难性遗忘, 饱和机制, 知识保留", "comments": "该论文提出了一种新颖的机制，通过动态调整神经元的学习行为来解决持续学习中的灾难性遗忘问题。其创新点在于引入“饱和”概念，使模型能够自适应地平衡新旧知识的学习，避免过度训练和遗忘。这种方法提高了SOM在动态环境中的适应性和鲁棒性。"}}
{"id": "2506.10612", "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling", "authors": ["Suin Lee", "Dae-Shik Kim"], "summary": "We present TexTailor, a novel method for generating consistent object\ntextures from textual descriptions. Existing text-to-texture synthesis\napproaches utilize depth-aware diffusion models to progressively generate\nimages and synthesize textures across predefined multiple viewpoints. However,\nthese approaches lead to a gradual shift in texture properties across\nviewpoints due to (1) insufficient integration of previously synthesized\ntextures at each viewpoint during the diffusion process and (2) the\nautoregressive nature of the texture synthesis process. Moreover, the\npredefined selection of camera positions, which does not account for the\nobject's geometry, limits the effective use of texture information synthesized\nfrom different viewpoints, ultimately degrading overall texture consistency. In\nTexTailor, we address these issues by (1) applying a resampling scheme that\nrepeatedly integrates information from previously synthesized textures within\nthe diffusion process, and (2) fine-tuning a depth-aware diffusion model on\nthese resampled textures. During this process, we observed that using only a\nfew training images restricts the model's original ability to generate\nhigh-fidelity images aligned with the conditioning, and therefore propose an\nperformance preservation loss to mitigate this issue. Additionally, we improve\nthe synthesis of view-consistent textures by adaptively adjusting camera\npositions based on the object's geometry. Experiments on a subset of the\nObjaverse dataset and the ShapeNet car dataset demonstrate that TexTailor\noutperforms state-of-the-art methods in synthesizing view-consistent textures.\nThe source code for TexTailor is available at\nhttps://github.com/Adios42/Textailor", "comment": "Submitted to ICLR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10612v1", "AI": {"title_translation": "TexTailor：通过有效重采样实现定制化文本对齐纹理生成", "tldr": "TexTailor是一种新方法，通过有效重采样和自适应相机调整，解决现有文本到纹理合成中纹理一致性差的问题，生成高质量的视图一致性纹理。", "motivation": "现有的文本到纹理合成方法（基于深度感知扩散模型）在生成纹理时，由于在扩散过程中未能充分整合先前合成的纹理信息以及纹理合成过程的自回归特性，导致纹理属性在不同视角间逐渐漂移。此外，预定义的相机位置未考虑物体几何形状，限制了不同视角纹理信息的有效利用，从而降低了整体纹理一致性。", "method": "TexTailor通过以下方法解决问题：1) 应用重采样方案，在扩散过程中重复整合先前合成的纹理信息；2) 在这些重采样纹理上微调深度感知扩散模型；3) 提出性能保持损失以缓解仅使用少量训练图像限制模型生成高保真图像能力的问题；4) 根据物体几何形状自适应调整相机位置，改进视图一致性纹理的合成。", "result": "在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视图一致性纹理方面优于现有最先进的方法。", "conclusion": "TexTailor能够有效解决文本到纹理合成中纹理一致性问题，生成高质量的视图一致性纹理，并超越了现有技术。", "translation": "我们提出了TexTailor，一种从文本描述生成一致对象纹理的新方法。现有的文本到纹理合成方法利用深度感知扩散模型逐步生成图像并在预定义的多视角下合成纹理。然而，这些方法导致纹理属性在不同视角间逐渐漂移，原因在于(1)在扩散过程中未能充分整合每个视角下先前合成的纹理，以及(2)纹理合成过程的自回归特性。此外，未考虑物体几何形状的预定义相机位置限制了从不同视角合成的纹理信息的有效利用，最终降低了整体纹理一致性。在TexTailor中，我们通过(1)应用重采样方案，在扩散过程中重复整合先前合成的纹理信息，以及(2)在这些重采样纹理上微调深度感知扩散模型来解决这些问题。在此过程中，我们观察到仅使用少量训练图像会限制模型生成与条件对齐的高保真图像的原始能力，因此我们提出了一个性能保持损失来缓解这个问题。此外，我们通过根据物体几何形状自适应调整相机位置来改进视图一致性纹理的合成。在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视图一致性纹理方面优于现有最先进的方法。TexTailor的源代码可在https://github.com/Adios42/Textailor 获取。", "summary": "TexTailor提出了一种新颖的文本到纹理生成方法，旨在解决现有深度感知扩散模型在多视角纹理合成中存在的纹理一致性差的问题。通过引入有效的重采样方案整合先前纹理信息、微调扩散模型并提出性能保持损失，以及根据物体几何自适应调整相机位置，TexTailor显著提升了视图一致性纹理的合成质量，并在多个数据集上超越了现有SOTA方法。", "keywords": "文本到纹理, 纹理合成, 扩散模型, 视图一致性, 重采样", "comments": "这篇论文的创新点在于提出了一个全面的解决方案来解决文本到纹理合成中长期存在的纹理一致性问题。通过结合重采样、模型微调、性能保持损失以及几何感知相机调整，TexTailor有效地克服了现有方法在跨视角纹理漂移和信息利用效率上的局限性，对3D内容生成领域具有重要意义。"}}
{"id": "2506.10703", "title": "Preserving Task-Relevant Information Under Linear Concept Removal", "authors": ["Floris Holstege", "Shauli Ravfogel", "Bram Wouters"], "summary": "Modern neural networks often encode unwanted concepts alongside task-relevant\ninformation, leading to fairness and interpretability concerns. Existing\npost-hoc approaches can remove undesired concepts but often degrade useful\nsignals. We introduce SPLICE-Simultaneous Projection for LInear concept removal\nand Covariance prEservation-which eliminates sensitive concepts from\nrepresentations while exactly preserving their covariance with a target label.\nSPLICE achieves this via an oblique projection that \"splices out\" the unwanted\ndirection yet protects important label correlations. Theoretically, it is the\nunique solution that removes linear concept predictability and maintains target\ncovariance with minimal embedding distortion. Empirically, SPLICE outperforms\nbaselines on benchmarks such as Bias in Bios and Winobias, removing protected\nattributes while minimally damaging main-task information.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10703v1", "AI": {"title_translation": "在线性概念去除下保留任务相关信息", "tldr": "SPLICE是一种新的方法，可以在从表示中去除敏感概念的同时，精确保留其与目标标签的协方差，从而在不损害主要任务信息的情况下提高公平性和可解释性。", "motivation": "现代神经网络在编码任务相关信息的同时常常编码不必要的概念，这导致了公平性和可解释性问题。现有的事后方法可以去除不期望的概念，但通常会降低有用信号。", "method": "本文引入了SPLICE（Simultaneous Projection for LInear concept removal and Covariance prEservation），通过一种斜投影来消除表示中的敏感概念，同时精确保留它们与目标标签的协方差。SPLICE通过“剪除”不需要的方向来保护重要的标签相关性。理论上，它是去除线性概念可预测性并以最小嵌入失真保持目标协方度的唯一解决方案。", "result": "在Bias in Bios和Winobias等基准测试中，SPLICE的表现优于基线，它能够去除受保护的属性，同时最大限度地减少对主要任务信息的损害。", "conclusion": "SPLICE提供了一种独特且有效的方法，可以在去除表示中敏感概念的同时，精确保留任务相关信息，从而在公平性和性能之间取得平衡。", "translation": "现代神经网络在编码任务相关信息的同时常常编码不必要的概念，这导致了公平性和可解释性问题。现有的事后方法可以去除不期望的概念，但通常会降低有用信号。我们引入了SPLICE——用于线性概念去除和协方差保留的同步投影——它在从表示中消除敏感概念的同时，精确保留其与目标标签的协方差。SPLICE通过一种斜投影来实现这一点，该投影“剪除”了不希望的方向，但保护了重要的标签相关性。从理论上讲，它是去除线性概念可预测性并以最小嵌入失真保持目标协方差的唯一解决方案。从经验上讲，SPLICE在Bias in Bios和Winobias等基准测试中表现优于基线，它能够去除受保护的属性，同时最大限度地减少对主要任务信息的损害。", "summary": "本文提出了一种名为SPLICE的新方法，用于在神经网络表示中去除不必要的线性概念，同时保留任务相关信息。针对现有方法在去除敏感概念时会损害有用信号的问题，SPLICE通过一种独特的斜投影技术，精确地保持表示与目标标签的协方差，从而在去除偏见的同时，最大程度地减少对主要任务性能的损害。实验结果表明，SPLICE在多个基准测试中优于现有方法。", "keywords": "概念去除, 公平性, 神经网络, 斜投影, 协方差保留", "comments": "SPLICE的创新之处在于其理论保证，即它是去除线性概念可预测性并以最小嵌入失真保持目标协方差的唯一解决方案。这提供了一个强大的理论基础，解释了其在实践中优于基线的原因。该方法在解决公平性和可解释性问题方面具有重要意义，因为它能够在不显著损害模型性能的情况下消除有害偏见。"}}
{"id": "2506.10633", "title": "Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models", "authors": ["Konstantinos Vilouras", "Ilias Stogiannidis", "Junyu Yan", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "summary": "Latent Diffusion Models have shown remarkable results in text-guided image\nsynthesis in recent years. In the domain of natural (RGB) images, recent works\nhave shown that such models can be adapted to various vision-language\ndownstream tasks with little to no supervision involved. On the contrary,\ntext-to-image Latent Diffusion Models remain relatively underexplored in the\nfield of medical imaging, primarily due to limited data availability (e.g., due\nto privacy concerns). In this work, focusing on the chest X-ray modality, we\nfirst demonstrate that a standard text-conditioned Latent Diffusion Model has\nnot learned to align clinically relevant information in free-text radiology\nreports with the corresponding areas of the given scan. Then, to alleviate this\nissue, we propose a fine-tuning framework to improve multi-modal alignment in a\npre-trained model such that it can be efficiently repurposed for downstream\ntasks such as phrase grounding. Our method sets a new state-of-the-art on a\nstandard benchmark dataset (MS-CXR), while also exhibiting robust performance\non out-of-distribution data (VinDr-CXR). Our code will be made publicly\navailable.", "comment": "14 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10633v1", "AI": {"title_translation": "胸部X光潜扩散模型的解剖学弱监督提示调优", "tldr": "本文提出了一种解剖学弱监督提示调优框架，以改善胸部X光潜扩散模型中的多模态对齐，并在基准数据集上实现了最先进的性能。", "motivation": "文本到图像的潜扩散模型在医学影像领域（特别是胸部X光）中探索不足，主要原因是数据可用性有限。标准模型未能将放射报告中的临床相关信息与扫描的相应区域对齐。", "method": "提出了一种微调框架，用于改善预训练模型中的多模态对齐，使其能够有效地重新用于下游任务，如短语定位。该方法是“解剖学弱监督提示调优”。", "result": "在标准基准数据集（MS-CXR）上达到了新的最先进水平，并且在分布外数据（VinDr-CXR）上表现出稳健的性能。", "conclusion": "通过提出的微调框架，可以有效改善胸部X光潜扩散模型的多模态对齐，从而在医学影像下游任务中取得优异表现。", "translation": "潜在扩散模型近年来在文本引导图像合成方面取得了显著成果。在自然（RGB）图像领域，最近的工作表明，此类模型可以适应各种视觉-语言下游任务，仅需很少或无需监督。相反，文本到图像的潜在扩散模型在医学影像领域仍相对未被充分探索，这主要是由于数据可用性有限（例如，由于隐私问题）。在这项工作中，我们专注于胸部X光模态，首先证明了标准的文本条件潜在扩散模型尚未学会将自由文本放射报告中的临床相关信息与给定扫描的相应区域对齐。然后，为了缓解这个问题，我们提出了一种微调框架，以改善预训练模型中的多模态对齐，使其能够有效地重新用于下游任务，例如短语定位。我们的方法在标准基准数据集（MS-CXR）上设定了新的最先进水平，同时在分布外数据（VinDr-CXR）上也表现出稳健的性能。我们的代码将公开可用。", "summary": "本文针对医学影像领域（特别是胸部X光）中文本到图像潜扩散模型的多模态对齐问题，提出了一种解剖学弱监督提示调优的微调框架。研究发现标准模型无法有效关联文本报告与图像区域。通过该框架，显著提升了模型在临床信息对齐方面的能力，并在MS-CXR和VinDr-CXR数据集上取得了最先进的性能和稳健性，为医疗影像下游任务提供了有效方案。", "keywords": "潜扩散模型, 胸部X光, 弱监督, 提示调优, 多模态对齐", "comments": "这项工作创新性地将弱监督提示调优应用于医学影像领域的潜扩散模型，解决了数据稀缺和多模态对齐的挑战。其在胸部X光领域的SOTA表现突显了其重要性，为医学图像分析和生成提供了新的思路。"}}
{"id": "2506.10707", "title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner", "authors": ["Marco Spinaci", "Marek Polewczyk", "Maximilian Schambach", "Sam Thelin"], "summary": "Tabular in-context learning (ICL) has recently achieved state-of-the-art\n(SOTA) performance on several tabular prediction tasks. Previously restricted\nto classification problems on small tables, recent advances such as TabPFN and\nTabICL have extended its use to larger datasets. While being architecturally\nefficient and well-adapted to tabular data structures, current table-native ICL\narchitectures, being trained exclusively on synthetic data, do not fully\nleverage the rich semantics and world knowledge contained in real-world tabular\ndata. On another end of this spectrum, tabular ICL models based on pretrained\nlarge language models such as TabuLa-8B integrate deep semantic understanding\nand world knowledge but are only able to make use of a small amount of context\ndue to inherent architectural limitations. With the aim to combine the best of\nboth these worlds, we introduce ConTextTab, integrating semantic understanding\nand alignment into a table-native ICL framework. By employing specialized\nembeddings for different data modalities and by training on large-scale\nreal-world tabular data, our model is competitive with SOTA across a broad set\nof benchmarks while setting a new standard on the semantically rich CARTE\nbenchmark.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10707v1", "AI": {"title_translation": "ConTextTab：一种语义感知的表格上下文学习器", "tldr": "ConTextTab 结合了表格原生上下文学习器的高效性和预训练大语言模型的语义理解能力，通过使用专用嵌入并在大规模真实世界数据上训练，实现了与SOTA相当的性能，并在语义丰富的基准上表现出色。", "motivation": "现有的表格原生上下文学习器（ICL）虽然高效且适应表格结构，但因仅在合成数据上训练，未能充分利用真实世界表格数据中的丰富语义和世界知识。而基于预训练大语言模型的表格ICL模型虽然具有深度语义理解能力，但受限于架构，只能利用少量上下文。", "method": "论文引入了 ConTextTab，旨在结合表格原生ICL的优点和基于LLM的语义理解能力。它通过将语义理解和对齐集成到表格原生ICL框架中，并采用针对不同数据模态的专用嵌入，同时在大规模真实世界表格数据上进行训练。", "result": "ConTextTab 在一系列广泛的基准测试中与最先进（SOTA）模型具有竞争力，并在语义丰富的 CARTE 基准测试上设立了新标准。", "conclusion": "ConTextTab 成功地将语义理解融入到表格原生上下文学习框架中，有效结合了现有方法的优点，并在多个基准测试中取得了优异表现，尤其在语义丰富的任务上树立了新标准，证明了其在处理真实世界表格数据方面的强大能力。", "translation": "表格上下文学习（ICL）最近在多项表格预测任务中取得了最先进（SOTA）的性能。此前，ICL仅限于小型表格的分类问题，但TabPFN和TabICL等最新进展已将其应用扩展到更大的数据集。尽管当前的表格原生ICL架构在结构上高效且非常适合表格数据结构，但由于它们完全在合成数据上训练，未能充分利用真实世界表格数据中包含的丰富语义和世界知识。另一方面，基于预训练大型语言模型（如TabuLa-8B）的表格ICL模型集成了深度语义理解和世界知识，但由于固有的架构限制，只能利用少量上下文。为了结合这两种方法的优点，我们引入了ConTextTab，它将语义理解和对齐集成到表格原生ICL框架中。通过采用针对不同数据模态的专用嵌入，并在大规模真实世界表格数据上进行训练，我们的模型在广泛的基准测试中与SOTA模型具有竞争力，同时在语义丰富的CARTE基准测试上设立了新标准。", "summary": "ConTextTab 是一种新型的表格上下文学习器，旨在解决现有表格原生ICL缺乏语义理解和基于LLM的ICL上下文受限的问题。它将语义理解集成到表格原生框架中，利用专用嵌入并在大规模真实世界数据上训练。实验证明，ConTextTab 在多个基准测试中与SOTA模型表现相当，并在语义丰富的CARTE基准上树立了新标准，展示了其在处理真实世界表格数据方面的强大潜力。", "keywords": "表格上下文学习, 语义理解, 真实世界数据, ConTextTab, 表格预测", "comments": "ConTextTab 的创新之处在于它成功地弥合了表格原生ICL在语义理解方面的不足和基于LLM的ICL在上下文限制方面的缺陷。通过引入专用嵌入和利用大规模真实世界数据训练，该模型有效地提升了表格ICL处理复杂语义信息的能力，为未来表格数据分析提供了新的方向。其在CARTE基准上的优异表现尤其突出，证明了语义感知的重要性。"}}
{"id": "2506.10634", "title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models", "authors": ["Francisco Caetano", "Christiaan Viviers", "Peter H. N. De With", "Fons van der Sommen"], "summary": "Flow Matching has emerged as a powerful framework for learning continuous\ntransformations between distributions, enabling high-fidelity generative\nmodeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new\nformulation that unifies semantic segmentation, classification, and image\ngeneration within a single model. Using a symmetric learning objective,\nSymmFlow models forward and reverse transformations jointly, ensuring\nbi-directional consistency, while preserving sufficient entropy for generative\ndiversity. A new training objective is introduced to explicitly retain semantic\ninformation across flows, featuring efficient sampling while preserving\nsemantic structure, allowing for one-step segmentation and classification\nwithout iterative refinement. Unlike previous approaches that impose strict\none-to-one mapping between masks and images, SymmFlow generalizes to flexible\nconditioning, supporting both pixel-level and image-level class labels.\nExperimental results on various benchmarks demonstrate that SymmFlow achieves\nstate-of-the-art performance on semantic image synthesis, obtaining FID scores\nof 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.\nAdditionally, it delivers competitive results on semantic segmentation and\nshows promising capabilities in classification tasks. The code will be publicly\navailable.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10634v1", "AI": {"title_translation": "对称流匹配：基于分数的生成模型统一图像生成、分割和分类", "tldr": "SymmFlow 是一种新的流匹配框架，通过对称学习目标统一了图像生成、语义分割和分类，实现了高效采样和最先进的生成性能。", "motivation": "现有的流匹配框架虽然在生成建模方面表现强大，但缺乏一个能统一图像生成、语义分割和分类的单一模型，并且在掩码和图像之间存在严格的一对一映射限制，限制了其灵活性。", "method": "本文提出了对称流匹配 (SymmFlow)，通过对称学习目标联合建模前向和反向变换，确保双向一致性并保留生成多样性。引入了一个新的训练目标以显式保留流中的语义信息，实现高效采样，并在不迭代细化的情况下进行一步分割和分类。它支持像素级和图像级类标签的灵活条件设置。", "result": "SymmFlow 在语义图像合成方面取得了最先进的性能，在 CelebAMask-HQ 上 FID 分数为 11.9，在 COCO-Stuff 上为 7.0（仅需 25 个推理步骤）。此外，它在语义分割方面表现出有竞争力的结果，并在分类任务中显示出有前景的能力。", "conclusion": "SymmFlow 成功地将图像生成、语义分割和分类统一在一个单一模型中，通过其对称学习和新的训练目标，实现了高效且高性能的多任务处理，并在多个基准测试中取得了显著成果。", "translation": "流匹配已成为一种强大的框架，用于学习分布之间的连续变换，从而实现高保真生成建模。这项工作引入了对称流匹配 (SymmFlow)，这是一种新的公式，将语义分割、分类和图像生成统一到一个单一模型中。通过使用对称学习目标，SymmFlow 联合建模前向和反向变换，确保双向一致性，同时保留足够的熵以实现生成多样性。引入了一个新的训练目标，以显式保留流中的语义信息，其特点是高效采样，同时保留语义结构，允许一步分割和分类而无需迭代细化。与以前在掩码和图像之间施加严格一对一映射的方法不同，SymmFlow 概括为灵活的条件设置，支持像素级和图像级类标签。在各种基准测试上的实验结果表明，SymmFlow 在语义图像合成方面取得了最先进的性能，在 CelebAMask-HQ 上 FID 分数为 11.9，在 COCO-Stuff 上为 7.0（仅需 25 个推理步骤）。此外，它在语义分割方面取得了有竞争力的结果，并在分类任务中显示出有前景的能力。代码将公开发布。", "summary": "本文提出了对称流匹配 (SymmFlow)，一种统一图像生成、语义分割和分类的单一模型。SymmFlow 采用对称学习目标，联合建模前向和反向变换，确保双向一致性和生成多样性。它引入了新的训练目标以高效保留语义信息，实现一步式分割和分类。实验证明，SymmFlow 在语义图像合成上达到最先进性能，并在分割和分类任务中表现出色，支持灵活的条件设置。", "keywords": "流匹配, 图像生成, 语义分割, 图像分类, 对称学习", "comments": "SymmFlow 的创新之处在于将图像生成、语义分割和分类统一到一个单一的流匹配框架中，并通过对称学习目标实现了双向一致性与生成多样性。其新的训练目标允许高效的一步式语义任务处理，避免了迭代细化，并且能够处理更灵活的条件设置，这显著提升了多任务处理的效率和泛化能力。"}}
{"id": "2506.10751", "title": "Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering", "authors": ["Sai Prasanna Teja Reddy Bogireddy", "Abrar Majeedi", "Viswanatha Reddy Gajjala", "Zhuoyan Xu", "Siddhant Rai", "Vaishnav Potlapalli"], "summary": "Automated question answering (QA) over electronic health records (EHRs) can\nbridge critical information gaps for clinicians and patients, yet it demands\nboth precise evidence retrieval and faithful answer generation under limited\nsupervision. In this work, we present Neural, the runner-up in the BioNLP 2025\nArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method\ndecouples the task into (1) sentence-level evidence identification and (2)\nanswer synthesis with explicit citations. For each stage, we automatically\nexplore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning\ninstructions and few-shot demonstrations on the development set. A\nself-consistency voting scheme further improves evidence recall without\nsacrificing precision. On the hidden test set, our method attains an overall\nscore of 51.5, placing second stage while outperforming standard zero-shot and\nfew-shot prompting by over 20 and 10 points, respectively. These results\nindicate that data-driven prompt optimization is a cost-effective alternative\nto model fine-tuning for high-stakes clinical QA, advancing the reliability of\nAI assistants in healthcare.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10751v1", "AI": {"title_translation": "神经元在ArchEHR-QA 2025：基于证据的临床问答的代理式提示优化", "tldr": "本文介绍了Neural系统，它通过解耦任务和代理式提示优化，在基于证据的临床问答任务中取得了第二名，证明了提示优化在临床QA中的有效性。", "motivation": "自动化电子健康记录（EHR）问答可以弥补临床医生和患者的关键信息鸿沟，但它需要在有限监督下进行精确的证据检索和忠实的答案生成。", "method": "该方法将任务解耦为句子级证据识别和带明确引用的答案合成。在每个阶段，使用DSPy的MIPROv2优化器自动探索提示空间，共同调整指令和少量示例。此外，采用自洽性投票方案以提高证据召回率而不牺牲精确度。", "result": "在隐藏测试集上，该方法获得了51.5的总分，位列第二，并分别比标准的零样本和少量样本提示方法高出20和10分。", "conclusion": "数据驱动的提示优化是针对高风险临床问答任务中模型微调的一种经济高效的替代方案，提升了医疗领域AI助手的可靠性。", "translation": "针对电子健康记录（EHR）的自动化问答（QA）可以弥合临床医生和患者之间的关键信息鸿沟，但它需要在有限监督下进行精确的证据检索和忠实的答案生成。在这项工作中，我们介绍了Neural，它在BioNLP 2025 ArchEHR-QA关于基于证据的临床问答的共享任务中获得亚军。我们提出的方法将任务解耦为（1）句子级证据识别和（2）带有明确引用的答案合成。在每个阶段，我们利用DSPy的MIPROv2优化器自动探索提示空间，在开发集上联合调整指令和少量示例。自洽性投票方案进一步提高了证据召回率，而不会牺牲精确度。在隐藏测试集上，我们的方法获得了51.5的总分，位居第二，并且分别比标准的零样本和少量样本提示方法高出20和10分。这些结果表明，数据驱动的提示优化是高风险临床问答中模型微调的一种经济高效的替代方案，提升了医疗领域AI助手的可靠性。", "summary": "本文介绍了在BioNLP 2025 ArchEHR-QA共享任务中获得亚军的Neural系统，该系统专注于基于证据的临床问答。其方法将任务分解为证据识别和带引用的答案合成，并利用DSPy的MIPROv2优化器进行代理式提示优化，同时结合自洽性投票机制。实验结果表明，该方法在性能上显著优于零样本和少量样本提示，证明了数据驱动提示优化在临床QA领域作为模型微调的有效替代方案，提高了AI助手的可靠性。", "keywords": "临床问答, 电子健康记录, 提示优化, 证据识别, 自洽性", "comments": "本文的创新之处在于提出了一种代理式提示优化方法，作为模型微调的经济高效替代方案，特别适用于高风险的临床问答场景。其将任务解耦并结合自洽性投票的策略，有效地提高了证据召回率和整体性能，对于提升医疗AI助手的可靠性具有重要意义。"}}
{"id": "2506.10639", "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning", "authors": ["Xiaoyi Bao", "Jindi Lv", "Xiaofeng Wang", "Zheng Zhu", "Xinze Chen", "YuKun Zhou", "Jiancheng Lv", "Xingang Wang", "Guan Huang"], "summary": "Recent progress in diffusion models has greatly enhanced video generation\nquality, yet these models still require fine-tuning to improve specific\ndimensions like instance preservation, motion rationality, composition, and\nphysical plausibility. Existing fine-tuning approaches often rely on human\nannotations and large-scale computational resources, limiting their\npracticality. In this work, we propose GigaVideo-1, an efficient fine-tuning\nframework that advances video generation without additional human supervision.\nRather than injecting large volumes of high-quality data from external sources,\nGigaVideo-1 unlocks the latent potential of pre-trained video diffusion models\nthrough automatic feedback. Specifically, we focus on two key aspects of the\nfine-tuning process: data and optimization. To improve fine-tuning data, we\ndesign a prompt-driven data engine that constructs diverse, weakness-oriented\ntraining samples. On the optimization side, we introduce a reward-guided\ntraining strategy, which adaptively weights samples using feedback from\npre-trained vision-language models with a realism constraint. We evaluate\nGigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17\nevaluation dimensions. Experiments show that GigaVideo-1 consistently improves\nperformance on almost all the dimensions with an average gain of about 4% using\nonly 4 GPU-hours. Requiring no manual annotations and minimal real data,\nGigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and\ndata will be publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10639v1", "AI": {"title_translation": "GigaVideo-1：通过自动反馈和4 GPU小时微调推进视频生成", "tldr": "GigaVideo-1提出了一种高效的视频生成微调框架，利用自动反馈在仅4小时GPU时间内显著提升性能，无需人工标注。", "motivation": "现有视频生成模型的微调方法依赖于人工标注和大量计算资源，限制了其实用性，且在实例保留、运动合理性、构图和物理真实性等方面仍需改进。", "method": "本文提出了GigaVideo-1，一个无需人工监督的视频生成高效微调框架。它通过自动反馈释放预训练视频扩散模型的潜力，专注于数据和优化两个方面。在数据方面，设计了一个提示驱动的数据引擎来构建多样化、面向弱点的训练样本；在优化方面，引入了一种奖励引导的训练策略，利用预训练的视觉-语言模型反馈和真实性约束来自适应地加权样本。", "result": "GigaVideo-1在VBench-2.0基准上，以Wan2.1为基线，在17个评估维度上进行了评估。实验表明，GigaVideo-1在几乎所有维度上都持续改进了性能，平均增益约为4%，且仅使用了4 GPU小时。", "conclusion": "GigaVideo-1无需人工标注和最少的真实数据，展示了其在视频生成微调方面的有效性和效率。", "translation": "扩散模型在视频生成质量方面取得了巨大进展，但这些模型仍需要微调以改进特定维度，如实例保留、运动合理性、构图和物理真实性。现有微调方法通常依赖于人工标注和大规模计算资源，限制了其实用性。在这项工作中，我们提出了GigaVideo-1，一个无需额外人工监督即可推进视频生成的高效微调框架。GigaVideo-1不是从外部来源注入大量高质量数据，而是通过自动反馈解锁预训练视频扩散模型的潜在能力。具体来说，我们关注微调过程的两个关键方面：数据和优化。为了改进微调数据，我们设计了一个提示驱动的数据引擎，用于构建多样化、面向弱点的训练样本。在优化方面，我们引入了一种奖励引导的训练策略，该策略利用预训练视觉-语言模型的反馈并结合真实性约束来自适应地加权样本。我们使用Wan2.1作为基线，在VBench-2.0基准上对GigaVideo-1进行了评估，涵盖17个评估维度。实验表明，GigaVideo-1在几乎所有维度上都持续改进了性能，平均增益约为4%，仅使用了4 GPU小时。GigaVideo-1无需人工标注和最少的真实数据，展示了其有效性和效率。代码、模型和数据将公开可用。", "summary": "GigaVideo-1提出了一种无需人工监督且高效的视频生成微调框架。该框架通过自动反馈机制，利用提示驱动的数据引擎生成弱点导向的训练样本，并结合奖励引导的优化策略，显著提升了预训练视频扩散模型的性能。在VBench-2.0基准测试中，GigaVideo-1在仅4 GPU小时的微调下，平均性能提升了约4%，展现了其在效率和效果上的优势。", "keywords": "视频生成, 微调, 自动反馈, 扩散模型, GigaVideo-1", "comments": "GigaVideo-1的创新之处在于其无需人工标注的自动反馈微调机制，这大大降低了视频生成模型微调的成本和复杂性。它通过弱点导向的数据生成和奖励引导的优化策略，有效地提升了模型在多个维度的表现，且仅需极低的计算资源（4 GPU小时），对于推动视频生成技术的实际应用具有重要意义。"}}
{"id": "2506.10772", "title": "Skillful joint probabilistic weather forecasting from marginals", "authors": ["Ferran Alet", "Ilan Price", "Andrew El-Kadi", "Dominic Masters", "Stratis Markou", "Tom R. Andersson", "Jacklynn Stott", "Remi Lam", "Matthew Willson", "Alvaro Sanchez-Gonzalez", "Peter Battaglia"], "summary": "Machine learning (ML)-based weather models have rapidly risen to prominence\ndue to their greater accuracy and speed than traditional forecasts based on\nnumerical weather prediction (NWP), recently outperforming traditional\nensembles in global probabilistic weather forecasting. This paper presents FGN,\na simple, scalable and flexible modeling approach which significantly\noutperforms the current state-of-the-art models. FGN generates ensembles via\nlearned model-perturbations with an ensemble of appropriately constrained\nmodels. It is trained directly to minimize the continuous rank probability\nscore (CRPS) of per-location forecasts. It produces state-of-the-art ensemble\nforecasts as measured by a range of deterministic and probabilistic metrics,\nmakes skillful ensemble tropical cyclone track predictions, and captures joint\nspatial structure despite being trained only on marginals.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10772v1", "AI": {"title_translation": "基于边缘分布的熟练联合概率天气预报", "tldr": "FGN是一种新的机器学习模型，显著优于现有最先进的概率天气预报模型，它通过学习模型扰动生成集合预测，即使仅在边缘分布上训练也能捕获联合空间结构。", "motivation": "机器学习（ML）天气模型因其比传统数值天气预报（NWP）更准确、更快而迅速崛起，并在全球概率天气预报中超越了传统集合预报。本研究旨在提出一种新的方法，以进一步提升当前最先进模型的性能。", "method": "本文提出了FGN，一种简单、可扩展且灵活的建模方法。FGN通过学习模型扰动和适当约束的模型集合来生成集合预报。它直接训练以最小化每个位置预报的连续秩概率分数（CRPS）。", "result": "FGN显著优于当前最先进的模型。它在各种确定性和概率性指标上生成了最先进的集合预报，能够熟练地进行集合热带气旋路径预测，并且尽管仅在边缘分布上进行训练，也能捕获联合空间结构。", "conclusion": "FGN模型在概率天气预报方面表现出色，显著超越了现有最先进的模型，并且能够有效地捕获联合空间结构，即使仅在边缘分布上进行训练也能实现。", "translation": "机器学习（ML）天气模型因其比传统数值天气预报（NWP）更准确、更快而迅速崛起，最近在全球概率天气预报中超越了传统集合预报。本文提出了FGN，一种简单、可扩展且灵活的建模方法，它显著优于当前最先进的模型。FGN通过学习模型扰动和适当约束的模型集合来生成集合预报。它直接训练以最小化每个位置预报的连续秩概率分数（CRPS）。它在各种确定性和概率性指标上生成了最先进的集合预报，能够熟练地进行集合热带气旋路径预测，并且尽管仅在边缘分布上进行训练，也能捕获联合空间结构。", "summary": "本文介绍了FGN，一种创新的机器学习模型，专门用于概率天气预报。该模型通过学习模型扰动生成集合预报，并直接优化连续秩概率分数。FGN在多项指标上显著超越了现有最先进的模型，尤其是在捕获联合空间结构方面表现出色，尽管其训练仅基于边缘分布。此外，它在热带气旋路径预测方面也展现了高超的技能。", "keywords": "概率天气预报, 机器学习, 集合预报, FGN, 联合空间结构", "comments": "FGN的创新之处在于其通过学习模型扰动生成集合预报的方法，以及在仅基于边缘分布训练的情况下仍能有效捕获联合空间结构的能力。这对于提高概率天气预报的准确性和实用性具有重要意义，尤其是在处理复杂天气系统时。"}}
{"id": "2506.10669", "title": "PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis", "authors": ["Marzieh Oghbaie", "Teresa Araújoa", "Hrvoje Bogunović"], "summary": "Background and Objective: Prototype-based methods improve interpretability by\nlearning fine-grained part-prototypes; however, their visualization in the\ninput pixel space is not always consistent with human-understandable\nbiomarkers. In addition, well-known prototype-based approaches typically learn\nextremely granular prototypes that are less interpretable in medical imaging,\nwhere both the presence and extent of biomarkers and lesions are critical.\n  Methods: To address these challenges, we propose PiPViT (Patch-based Visual\nInterpretable Prototypes), an inherently interpretable prototypical model for\nimage recognition. Leveraging a vision transformer (ViT), PiPViT captures\nlong-range dependencies among patches to learn robust, human-interpretable\nprototypes that approximate lesion extent only using image-level labels.\nAdditionally, PiPViT benefits from contrastive learning and multi-resolution\ninput processing, which enables effective localization of biomarkers across\nscales.\n  Results: We evaluated PiPViT on retinal OCT image classification across four\ndatasets, where it achieved competitive quantitative performance compared to\nstate-of-the-art methods while delivering more meaningful explanations.\nMoreover, quantitative evaluation on a hold-out test set confirms that the\nlearned prototypes are semantically and clinically relevant. We believe PiPViT\ncan transparently explain its decisions and assist clinicians in understanding\ndiagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10669v1", "AI": {"title_translation": "PiPViT：基于图像块的视网膜图像分析视觉可解释原型", "tldr": "PiPViT是一种基于ViT的可解释原型模型，用于视网膜图像分析，它学习人类可理解的原型来近似病变范围，并在四个OCT数据集上取得了竞争性性能和更有意义的解释。", "motivation": "现有的基于原型的方法在像素空间中的可视化与人类可理解的生物标志物不一致，并且学习到的原型过于细粒度，在医学影像中解释性较差，无法有效表示生物标志物和病变的出现及程度。", "method": "提出PiPViT（Patch-based Visual Interpretable Prototypes），一个固有的可解释的图像识别原型模型。它利用Vision Transformer (ViT) 捕获图像块之间的长程依赖，学习鲁棒、人类可解释的原型，仅使用图像级标签就能近似病变范围。此外，PiPViT还受益于对比学习和多分辨率输入处理，从而实现跨尺度的生物标志物有效定位。", "result": "PiPViT在四个视网膜OCT图像分类数据集上进行了评估，与最先进的方法相比，它取得了具有竞争力的定量性能，同时提供了更有意义的解释。此外，在独立的测试集上的定量评估证实，所学习的原型在语义和临床上都具有相关性。", "conclusion": "PiPViT能够透明地解释其决策，并协助临床医生理解诊断结果。", "translation": "背景与目标：基于原型的方法通过学习细粒度的部分原型来提高可解释性；然而，它们在输入像素空间中的可视化并不总是与人类可理解的生物标志物一致。此外，众所周知的基于原型的方法通常学习极其细粒度的原型，这在医学影像中解释性较差，而医学影像中生物标志物和病变的存在及程度都至关重要。\n方法：为了解决这些挑战，我们提出了PiPViT（基于图像块的视觉可解释原型），这是一种固有的可解释的图像识别原型模型。PiPViT利用视觉Transformer (ViT) 捕获图像块之间的长程依赖，以学习鲁棒的、人类可解释的原型，仅使用图像级标签就能近似病变范围。此外，PiPViT受益于对比学习和多分辨率输入处理，这使得能够有效定位跨尺度的生物标志物。\n结果：我们在四个视网膜OCT图像分类数据集上评估了PiPViT，与最先进的方法相比，它取得了具有竞争力的定量性能，同时提供了更有意义的解释。此外，在独立的测试集上的定量评估证实，所学习的原型在语义和临床上都具有相关性。我们相信PiPViT可以透明地解释其决策，并协助临床医生理解诊断结果。Github页面：https://github.com/marziehoghbaie/PiPViT", "summary": "PiPViT是一种针对视网膜图像分析提出的可解释原型模型，旨在解决现有原型方法在医学影像中解释性不足的问题。该模型结合了Vision Transformer捕获长程依赖的能力、对比学习和多分辨率处理，使其能够学习到人类可理解且与病变范围相关的原型，仅需图像级标签。在视网膜OCT图像分类任务上的评估表明，PiPViT在保持竞争性性能的同时，提供了更具临床意义的解释，有助于医生理解诊断。", "keywords": "视网膜图像分析, 可解释AI, 原型学习, Vision Transformer, OCT图像", "comments": "这篇论文的创新点在于将Vision Transformer引入到原型学习中，以解决传统原型方法在医学影像中解释性差和原型粒度过细的问题。通过学习基于图像块的原型并结合对比学习和多分辨率处理，PiPViT不仅提高了模型的可解释性，还能在仅使用图像级标签的情况下近似病变范围，这对于临床诊断具有重要意义。其在保持高性能的同时提供有意义的解释，是医学AI领域的一大进步。"}}
{"id": "2506.10775", "title": "Monotone Classification with Relative Approximations", "authors": ["Yufei Tao"], "summary": "In monotone classification, the input is a multi-set $P$ of points in\n$\\mathbb{R}^d$, each associated with a hidden label from $\\{-1, 1\\}$. The goal\nis to identify a monotone function $h$, which acts as a classifier, mapping\nfrom $\\mathbb{R}^d$ to $\\{-1, 1\\}$ with a small {\\em error}, measured as the\nnumber of points $p \\in P$ whose labels differ from the function values $h(p)$.\nThe cost of an algorithm is defined as the number of points having their labels\nrevealed. This article presents the first study on the lowest cost required to\nfind a monotone classifier whose error is at most $(1 + \\epsilon) \\cdot k^*$\nwhere $\\epsilon \\ge 0$ and $k^*$ is the minimum error achieved by an optimal\nmonotone classifier -- in other words, the error is allowed to exceed the\noptimal by at most a relative factor. Nearly matching upper and lower bounds\nare presented for the full range of $\\epsilon$. All previous work on the\nproblem can only achieve an error higher than the optimal by an absolute\nfactor.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10775v1", "AI": {"title_translation": "具有相对近似的单调分类", "tldr": "本文首次研究了在单调分类中，以相对近似误差找到分类器的最低成本，并给出了近似匹配的上下界。", "motivation": "在单调分类中，以往的工作只能找到一个误差比最优解高出绝对因子的分类器。本文的动机是研究找到一个误差最多比最优解高出相对因子 (1 + ε) 倍的单调分类器所需的最低成本。", "method": "本文通过理论分析，提出了用于在单调分类中实现相对近似误差的分类器的近似匹配的上下界。", "result": "本文为在单调分类中实现相对近似误差的分类器，提供了在 ε 的整个范围内近似匹配的成本上下界。", "conclusion": "本文成功地首次研究了在单调分类中以相对近似误差找到分类器的最低成本问题，通过提供紧密的上下界，解决了先前工作只能实现绝对误差近似的局限性。", "translation": "在单调分类中，输入是 $\\mathbb{R}^d$ 中的一个多点集 $P$，每个点都关联着一个来自 $\\{-1, 1\\}$ 的隐藏标签。目标是识别一个单调函数 $h$，它作为分类器，将 $\\mathbb{R}^d$ 映射到 $\\{-1, 1\\}$，且具有较小的“误差”，误差的衡量标准是点集 $P$ 中标签与函数值 $h(p)$ 不同的点的数量。算法的成本定义为揭示标签的点的数量。本文首次研究了找到一个单调分类器所需的最低成本，该分类器的误差最多为 $(1 + \\epsilon) \\cdot k^*$，其中 $\\epsilon \\ge 0$，而 $k^*$ 是最优单调分类器所能达到的最小误差——换句话说，允许误差最多超过最优误差一个相对因子。本文为 $\\epsilon$ 的整个范围提供了近似匹配的上下界。之前关于该问题的所有工作都只能实现比最优误差高出绝对因子的误差。", "summary": "本文首次探讨了在单调分类中寻找具有相对近似误差的分类器的最低成本问题。单调分类旨在从给定点集中识别一个误差较小的单调函数。与以往工作仅能实现绝对误差近似不同，本文旨在找到一个误差最多比最优误差高出相对因子 (1 + ε) 的分类器。研究为此问题在 ε 的整个范围内提供了近似匹配的成本上下界，填补了该领域的一个空白。", "keywords": "单调分类, 相对近似, 成本, 上下界, 分类器", "comments": "本文的创新之处在于首次将相对近似的概念引入到单调分类问题中，并提供了理论上的成本上下界。这解决了先前工作只能实现绝对误差近似的局限性，为单调分类的理论研究和算法设计开辟了新的方向，具有重要的理论意义。"}}
{"id": "2506.10683", "title": "Enhancing Deepfake Detection using SE Block Attention with CNN", "authors": ["Subhram Dasgupta", "Janelle Mason", "Xiaohong Yuan", "Olusola Odeyomi", "Kaushik Roy"], "summary": "In the digital age, Deepfake present a formidable challenge by using advanced\nartificial intelligence to create highly convincing manipulated content,\nundermining information authenticity and security. These sophisticated\nfabrications surpass traditional detection methods in complexity and realism.\nTo address this issue, we aim to harness cutting-edge deep learning\nmethodologies to engineer an innovative deepfake detection model. However, most\nof the models designed for deepfake detection are large, causing heavy storage\nand memory consumption. In this research, we propose a lightweight convolution\nneural network (CNN) with squeeze and excitation block attention (SE) for\nDeepfake detection. The SE block module is designed to perform dynamic\nchannel-wise feature recalibration. The SE block allows the network to\nemphasize informative features and suppress less useful ones, which leads to a\nmore efficient and effective learning module. This module is integrated with a\nsimple sequential model to perform Deepfake detection. The model is smaller in\nsize and it achieves competing accuracy with the existing models for deepfake\ndetection tasks. The model achieved an overall classification accuracy of\n94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse\nFake Face Dataset. Our proposed approach presents a promising avenue for\ncombating the Deepfake challenge with minimal computational resources,\ndeveloping efficient and scalable solutions for digital content verification.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10683v1", "AI": {"title_translation": "使用SE块注意力与CNN增强Deepfake检测", "tldr": "本文提出了一种轻量级CNN模型，结合SE块注意力机制，用于Deepfake检测，该模型在保持高准确率的同时，显著降低了计算资源消耗。", "motivation": "Deepfake技术利用先进AI制造高度逼真的篡改内容，严重威胁信息真实性和安全性，而现有Deepfake检测模型通常庞大且消耗大量存储和内存。", "method": "提出了一种结合Squeeze-and-Excitation (SE) 块注意力的轻量级卷积神经网络 (CNN) 模型。SE块旨在执行动态通道维度特征重新校准，以强调有用特征并抑制不重要特征。该模块与一个简单的顺序模型集成以进行Deepfake检测。", "result": "该模型在Deepfake检测任务上实现了与现有模型相当的准确性，且模型尺寸更小。在Style GAN数据集上，分类准确率达到94.14%，AUC-ROC得分为0.985。", "conclusion": "所提出的轻量级CNN模型结合SE块注意力，为对抗Deepfake挑战提供了一种有前景的途径，它以最少的计算资源实现了高效且可扩展的数字内容验证解决方案。", "translation": "在数字时代，Deepfake利用先进的人工智能创造出高度逼真的篡改内容，对信息真实性和安全性构成了严峻挑战。这些复杂的伪造品在复杂性和真实性上超越了传统的检测方法。为了解决这个问题，我们旨在利用尖端深度学习方法来设计一个创新的Deepfake检测模型。然而，大多数用于Deepfake检测的模型都很大，导致存储和内存消耗巨大。在这项研究中，我们提出了一种结合Squeeze-and-Excitation (SE) 块注意力的轻量级卷积神经网络 (CNN) 用于Deepfake检测。SE块模块旨在执行动态通道维度特征重新校准。SE块允许网络强调信息丰富的特征并抑制不太有用的特征，从而形成一个更高效、更有效的学习模块。该模块与一个简单的顺序模型集成以执行Deepfake检测。该模型尺寸更小，并且在Deepfake检测任务上达到了与现有模型相当的准确性。该模型在来自Diverse Fake Face Dataset的Style GAN数据集上实现了94.14%的总体分类准确率和0.985的AUC-ROC得分。我们提出的方法为以最少的计算资源对抗Deepfake挑战提供了一条有前景的途径，开发了用于数字内容验证的高效和可扩展的解决方案。", "summary": "本文提出了一种结合Squeeze-and-Excitation (SE) 块注意力的轻量级卷积神经网络 (CNN) 模型，旨在解决Deepfake检测中模型庞大、资源消耗高的问题。该SE块通过动态通道特征重校准，使网络能更有效地学习。实验结果表明，该模型在保持较小尺寸的同时，在Style GAN数据集上取得了94.14%的分类准确率和0.985的AUC-ROC得分，表现出与现有模型相当的性能，为Deepfake检测提供了高效且资源友好的解决方案。", "keywords": "Deepfake检测, CNN, SE块注意力, 轻量级模型, 数字内容验证", "comments": "该研究的创新点在于将SE块注意力机制引入到轻量级CNN中进行Deepfake检测，有效解决了现有模型资源消耗大的问题。其重要性在于为数字内容验证提供了一种高效且实用的方法，尤其适用于资源受限的环境。"}}
{"id": "2506.10801", "title": "Dense Associative Memory with Epanechnikov Energy", "authors": ["Benjamin Hoover", "Zhaoyang Shi", "Krishnakumar Balasubramanian", "Dmitry Krotov", "Parikshit Ram"], "summary": "We propose a novel energy function for Dense Associative Memory (DenseAM)\nnetworks, the log-sum-ReLU (LSR), inspired by optimal kernel density\nestimation. Unlike the common log-sum-exponential (LSE) function, LSR is based\non the Epanechnikov kernel and enables exact memory retrieval with exponential\ncapacity without requiring exponential separation functions. Moreover, it\nintroduces abundant additional \\emph{emergent} local minima while preserving\nperfect pattern recovery -- a characteristic previously unseen in DenseAM\nliterature. Empirical results show that LSR energy has significantly more local\nminima (memories) that have comparable log-likelihood to LSE-based models.\nAnalysis of LSR's emergent memories on image datasets reveals a degree of\ncreativity and novelty, hinting at this method's potential for both large-scale\nmemory storage and generative tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10801v1", "AI": {"title_translation": "具有Epanechnikov能量的稠密联想记忆", "tldr": "本文提出了一种基于Epanechnikov核的新型稠密联想记忆能量函数LSR，它能实现指数容量的精确记忆检索，并产生大量新兴局部极小值，具有生成任务潜力。", "motivation": "现有稠密联想记忆网络中常用能量函数（如LSE）可能存在局限性，作者旨在提出一种新的能量函数来改进记忆检索能力、容量和探索新的特性。", "method": "提出了一种名为log-sum-ReLU (LSR) 的新型能量函数，该函数受最优核密度估计启发，并基于Epanechnikov核。", "result": "LSR能量函数实现了指数容量的精确记忆检索，无需指数分离函数。它引入了大量额外的“新兴”局部极小值，同时保持了完美的模式恢复。经验结果表明，LSR能量具有显著更多的局部极小值（记忆），且其对数似然与基于LSE的模型相当。对LSR新兴记忆在图像数据集上的分析显示出一定程度的创造性和新颖性。", "conclusion": "LSR能量函数在稠密联想记忆网络中表现出卓越的性能，不仅提高了记忆容量和检索精度，还展现了生成任务的潜力，这在DenseAM领域是前所未有的。", "translation": "我们为稠密联想记忆 (DenseAM) 网络提出了一种新型能量函数，即对数-和-ReLU (LSR)，其灵感来源于最优核密度估计。与常见的对数-和-指数 (LSE) 函数不同，LSR基于Epanechnikov核，无需指数分离函数即可实现指数容量的精确记忆检索。此外，它引入了大量额外的“新兴”局部极小值，同时保持了完美的模式恢复——这是DenseAM文献中前所未有的特性。经验结果表明，LSR能量具有显著更多的局部极小值（记忆），其对数似然与基于LSE的模型相当。对LSR新兴记忆在图像数据集上的分析揭示了一定程度的创造性和新颖性，暗示了该方法在大规模记忆存储和生成任务方面的潜力。", "summary": "本文提出了一种用于稠密联想记忆（DenseAM）网络的新型能量函数——log-sum-ReLU（LSR），其灵感来源于最优核密度估计，并基于Epanechnikov核。LSR能量函数能够实现指数容量的精确记忆检索，且无需复杂的指数分离函数。与现有方法不同，LSR引入了大量“新兴”局部极小值，同时保持了完美的模式恢复能力。实验结果表明，LSR具有更多可与LSE模型媲美的局部极小值（记忆），并且在图像数据集上分析其新兴记忆时展现出创造性和新颖性，预示其在大规模记忆存储和生成任务方面的应用潜力。", "keywords": "稠密联想记忆, Epanechnikov核, 能量函数, 记忆检索, 局部极小值", "comments": "这篇论文通过提出一种基于Epanechnikov核的新型能量函数LSR，为稠密联想记忆网络带来了显著的创新。其核心贡献在于实现了指数容量的精确记忆检索，并引入了大量独特的“新兴”局部极小值，这不仅扩展了网络的记忆能力，还展现了其在生成任务上的潜力。这种“新兴”记忆的特性是该领域前所未见的，为联想记忆模型开辟了新的研究方向。"}}
{"id": "2506.10805", "title": "Detecting High-Stakes Interactions with Activation Probes", "authors": ["Alex McKenzie", "Urja Pawar", "Phil Blandfort", "William Bankes", "David Krueger", "Ekdeep Singh Lubana", "Dmitrii Krasheninnikov"], "summary": "Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study.", "comment": "33 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10805v1", "AI": {"title_translation": "使用激活探针检测高风险交互", "tldr": "本研究探讨了使用激活探针检测LLM中的高风险交互，发现其性能与大型LLM监控器相当，但计算成本显著降低，并可作为分层监控系统的有效初始过滤器。", "motivation": "安全部署大型语言模型（LLM）的一个重要方面是监控，而检测可能导致重大危害的“高风险”交互是一个关键但未被充分探索的监控目标。", "method": "研究评估了几种在合成数据上训练的探针架构，并测试了它们对多样化、分布外、真实世界数据的泛化能力。实验还探讨了构建资源感知分层监控系统的潜力。", "result": "激活探针在检测高风险交互方面表现出对多样化真实世界数据的鲁棒泛化能力。其性能与提示或微调的中型LLM监控器相当，但计算成本降低了六个数量级。探针可以作为高效的初始过滤器，用于更昂贵的下游分析。", "conclusion": "激活探针是检测LLM中高风险交互的一种高效且计算成本低的替代方案，适用于构建分层监控系统。研究发布了新的合成数据集和代码库以促进进一步研究。", "translation": "监控是安全部署大型语言模型（LLM）的一个重要方面。本文研究了激活探针在检测“高风险”交互（即文本表明交互可能导致重大危害）方面的应用，认为这是一个关键但尚未充分探索的监控目标。我们评估了几种在合成数据上训练的探针架构，发现它们对多样化、分布外、真实世界数据表现出鲁棒的泛化能力。探针的性能与提示或微调的中型LLM监控器相当，同时计算成本降低了六个数量级。我们的实验还强调了构建资源感知分层监控系统的潜力，其中探针可作为高效的初始过滤器，并标记出需要更昂贵下游分析的案例。我们发布了我们新颖的合成数据集和代码库，以鼓励进一步研究。", "summary": "本论文探讨了利用激活探针来检测大型语言模型（LLM）中的“高风险”交互，这是一种对安全部署LLM至关重要的监控目标。研究评估了在合成数据上训练的多种探针架构，并发现它们对真实世界数据具有强大的泛化能力。这些探针在性能上与中型LLM监控器相当，但计算效率提高了六个数量级。此外，研究提出将探针作为分层监控系统中的高效初始过滤器，以识别需要更深入分析的案例。作者还发布了相关数据集和代码库。", "keywords": "激活探针, 高风险交互, LLM监控, 计算效率, 分层监控系统", "comments": "本文的创新点在于提出了使用激活探针作为一种计算效率极高的方式来监控LLM中的高风险交互，解决了现有监控方法计算成本高昂的问题。其重要性在于为LLM的安全部署提供了一种可行的、资源友好的解决方案，尤其是在构建分层监控系统方面具有潜力。发布数据集和代码库也有助于推动该领域的未来研究。"}}
{"id": "2506.10689", "title": "Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery", "authors": ["Christopher Gaul", "Eduardo Fidalgo", "Enrique Alegre", "Rocío Alaiz Rodríguez", "Eri Pérez Corral"], "summary": "Accurate automatic screening of minors in unconstrained images demands models\nthat are robust to distribution shift and resilient to the children\nunder-representation in publicly available data. To overcome these issues, we\npropose a multi-task architecture with dedicated under/over-age discrimination\ntasks based on a frozen FaRL vision-language backbone joined with a compact\ntwo-layer MLP that shares features across one age-regression head and four\nbinary under-age heads for age thresholds of 12, 15, 18, and 21 years, focusing\non the legally critical age range. To address the severe class imbalance, we\nintroduce an $\\alpha$-reweighted focal-style loss and age-balanced mini-batch\nsampling, which equalizes twelve age bins during stochastic optimization.\nFurther improvement is achieved with an age gap that removes edge cases from\nthe loss.\n  Moreover, we set a rigorous evaluation by proposing the Overall Under-Age\nBenchmark, with 303k cleaned training images and 110k test images, defining\nboth the \"ASORES-39k\" restricted overall test, which removes the noisiest\ndomains, and the age estimation wild shifts test \"ASWIFT-20k\" of 20k-images,\nstressing extreme pose ($>$45{\\deg}), expression, and low image quality to\nemulate real-world shifts.\n  Trained on the cleaned overall set with resampling and age gap, our multiage\nmodel \"F\" lowers the root-mean-square-error on the ASORES-39k restricted test\nfrom 5.733 (age-only baseline) to 5.656 years and lifts under-18 detection from\nF2 score of 0.801 to 0.857 at 1% false-adult rate. Under the domain shift to\nthe wild data of ASWIFT-20k, the same configuration nearly sustains 0.99 recall\nwhile boosting F2 from 0.742 to 0.833 with respect to the age-only baseline,\ndemonstrating strong generalization under distribution shift. For the under-12\nand under-15 tasks, the respective boosts in F2 are from 0.666 to 0.955 and\nfrom 0.689 to 0.916, respectively.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10689v1", "AI": {"title_translation": "非受限图像中基于多任务和多年龄方法的未成年人检测", "tldr": "本文提出了一种多任务、多年龄模型，用于在非受限图像中准确筛选未成年人，通过改进架构、损失函数和数据采样，显著提高了在挑战性数据集上的未成年人检测性能和泛化能力。", "motivation": "在非受限图像中准确自动筛选未成年人面临模型对分布偏移的鲁棒性不足以及公开数据中儿童代表性不足的问题。", "method": "本文提出了一种多任务架构，使用冻结的FaRL视觉-语言骨干网络与紧凑的两层MLP结合，该MLP共享一个年龄回归头和四个二元未成年人头（针对12、15、18和21岁阈值）。为解决严重的类别不平衡问题，引入了α-重加权焦点式损失和年龄平衡的小批量采样。此外，通过年龄间隙移除损失中的边缘情况。提出了“Overall Under-Age Benchmark”数据集，包含303k训练图像和110k测试图像，以及“ASORES-39k”和“ASWIFT-20k”测试集进行严格评估。", "result": "在ASORES-39k受限测试集上，模型“F”将均方根误差从5.733（仅年龄基线）降低到5.656年，在1%假成人率下，18岁以下检测的F2分数从0.801提升到0.857。在ASWIFT-20k的领域偏移下，召回率接近0.99，F2分数从0.742提升到0.833。对于12岁以下和15岁以下任务，F2分数分别从0.666提升到0.955和从0.689提升到0.916。", "conclusion": "本文提出的多任务、多年龄模型在非受限图像中的未成年人检测方面表现出强大的泛化能力和显著的性能提升，尤其是在处理分布偏移和数据不平衡方面。", "translation": "在非受限图像中准确自动筛选未成年人，要求模型对分布偏移具有鲁棒性，并能应对公开数据中儿童代表性不足的问题。为了克服这些问题，我们提出了一种多任务架构，该架构具有专门的未成年/超龄判别任务，基于冻结的FaRL视觉-语言骨干网络，并结合一个紧凑的两层MLP，该MLP在年龄回归头和四个二元未成年人头（针对12、15、18和21岁年龄阈值）之间共享特征，重点关注法律上关键的年龄范围。为了解决严重的类别不平衡问题，我们引入了一种α-重加权焦点式损失和年龄平衡的小批量采样，在随机优化过程中平衡了十二个年龄段。通过年龄间隙进一步改进，该间隙从损失中移除了边缘情况。此外，我们通过提出“Overall Under-Age Benchmark”进行了严格的评估，该基准包含303k清理后的训练图像和110k测试图像，定义了移除最嘈杂域的“ASORES-39k”受限整体测试，以及强调极端姿势（>45度）、表情和低图像质量以模拟真实世界偏移的20k图像的年龄估计野外偏移测试“ASWIFT-20k”。在经过重采样和年龄间隙处理的清理后的整体数据集上训练后，我们的多年龄模型“F”在ASORES-39k受限测试集上的均方根误差从5.733（仅年龄基线）降低到5.656年，并在1%假成人率下将18岁以下检测的F2分数从0.801提升到0.857。在ASWIFT-20k野外数据的领域偏移下，相同的配置几乎保持了0.99的召回率，同时相对于仅年龄基线将F2分数从0.742提升到0.833，证明了在分布偏移下的强大泛化能力。对于12岁以下和15岁以下的任务，F2分数分别从0.666提升到0.955和从0.689提升到0.916。", "summary": "本文提出了一种用于非受限图像中未成年人检测的多任务、多年龄方法。该方法采用基于FaRL骨干网络的多任务架构，结合年龄回归和多个二元未成年人分类头。为解决数据不平衡问题，引入了改进的损失函数和年龄平衡采样策略。研究还构建了新的评估基准，并展示了所提模型在未成年人检测精度和泛化能力上的显著提升，尤其是在处理野外数据和分布偏移方面的优越性。", "keywords": "未成年人检测, 多任务学习, 年龄估计, 分布偏移, 深度学习", "comments": "本文的创新点在于提出了一个结合多任务学习和多年龄分类的架构，并针对未成年人数据稀缺和类别不平衡问题，引入了α-重加权焦点式损失和年龄平衡采样。此外，构建了严格的评估基准，特别是模拟真实世界偏移的ASWIFT-20k数据集，这对于推动该领域的研究具有重要意义。该方法在实际应用中，对于在线内容审核和未成年人保护具有重要价值。"}}
{"id": "2506.10831", "title": "Efficiency Robustness of Dynamic Deep Learning Systems", "authors": ["Ravishka Rathnasuriya", "Tingxi Li", "Zexin Xu", "Zihe Song", "Mirazul Haque", "Simin Chen", "Wei Yang"], "summary": "Deep Learning Systems (DLSs) are increasingly deployed in real-time\napplications, including those in resourceconstrained environments such as\nmobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning\nSystems (DDLSs) adapt inference computation based on input complexity, reducing\noverhead. While this dynamic behavior improves efficiency, such behavior\nintroduces new attack surfaces. In particular, efficiency adversarial attacks\nexploit these dynamic mechanisms to degrade system performance. This paper\nsystematically explores efficiency robustness of DDLSs, presenting the first\ncomprehensive taxonomy of efficiency attacks. We categorize these attacks based\non three dynamic behaviors: (i) attacks on dynamic computations per inference,\n(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic\noutput production for downstream tasks. Through an in-depth evaluation, we\nanalyze adversarial strategies that target DDLSs efficiency and identify key\nchallenges in securing these systems. In addition, we investigate existing\ndefense mechanisms, demonstrating their limitations against increasingly\npopular efficiency attacks and the necessity for novel mitigation strategies to\nsecure future adaptive DDLSs.", "comment": "Accepted to USENIX Security '25", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10831v1", "AI": {"title_translation": "动态深度学习系统的效率鲁棒性", "tldr": "动态深度学习系统（DDLS）提高了效率，但也引入了新的效率对抗攻击面。本文系统探索了DDLS的效率鲁棒性，提出了首个效率攻击分类法，并分析了现有防御机制的局限性，强调了新缓解策略的必要性。", "motivation": "深度学习系统（DLSs）在实时和资源受限环境中部署时面临效率挑战。动态深度学习系统（DDLSs）通过自适应推理计算来提高效率，但这种动态行为引入了新的攻击面，即效率对抗攻击，这些攻击利用动态机制来降低系统性能。因此，需要系统地探索DDLS的效率鲁棒性。", "method": "本文系统地探索了DDLS的效率鲁棒性，提出了首个效率攻击的综合分类法，并根据三种动态行为（每次推理的动态计算、动态推理迭代、下游任务的动态输出生产）进行分类。通过深入评估，分析了针对DDLS效率的对抗策略，识别了保护这些系统的关键挑战，并调查了现有防御机制及其局限性。", "result": "提出了首个全面的效率攻击分类法。分析了针对DDLS效率的对抗策略，并识别了保护这些系统的关键挑战。揭示了现有防御机制在对抗效率攻击方面的局限性。", "conclusion": "动态深度学习系统虽然提高了效率，但其动态性引入了效率对抗攻击的新漏洞。现有防御机制不足以应对这些攻击，未来需要新的缓解策略来保护自适应DDLS。", "translation": "深度学习系统（DLSs）越来越多地部署在实时应用中，包括移动和物联网设备等资源受限环境。为了解决效率挑战，动态深度学习系统（DDLSs）根据输入复杂性调整推理计算，从而降低开销。虽然这种动态行为提高了效率，但它也引入了新的攻击面。特别是，效率对抗攻击利用这些动态机制来降低系统性能。本文系统地探索了DDLS的效率鲁棒性，提出了首个全面的效率攻击分类法。我们根据三种动态行为对这些攻击进行分类：(i) 针对每次推理的动态计算的攻击，(ii) 针对动态推理迭代的攻击，以及 (iii) 针对下游任务的动态输出生产的攻击。通过深入评估，我们分析了针对DDLS效率的对抗策略，并识别了保护这些系统的关键挑战。此外，我们还调查了现有防御机制，证明了它们在对抗日益流行的效率攻击方面的局限性，以及为保护未来自适应DDLS而采取新缓解策略的必要性。", "summary": "本文探讨了动态深度学习系统（DDLSs）的效率鲁棒性，因为其自适应性在提高效率的同时也引入了效率对抗攻击。作者提出了首个全面的效率攻击分类法，并基于三种动态行为（计算、迭代、输出）对攻击进行分类。研究通过深入评估分析了攻击策略，并指出保护DDLSs面临的关键挑战。此外，研究还揭示了现有防御机制的不足，强调了开发新缓解策略以保护未来自适应DDLSs的重要性。", "keywords": "动态深度学习系统, 效率鲁棒性, 对抗攻击, 攻击分类, 防御机制", "comments": "这篇论文创新性地关注了动态深度学习系统在效率方面的安全问题，填补了现有研究对效率对抗攻击关注不足的空白。其提出的效率攻击分类法具有重要的理论和实践意义，为后续研究提供了框架。论文也强调了现有防御机制的局限性，指明了未来研究的方向，即开发针对此类新型攻击的有效缓解策略。"}}
{"id": "2506.10710", "title": "Continual Hyperbolic Learning of Instances and Classes", "authors": ["Melika Ayoughi", "Mina Ghadimi Atigh", "Mohammad Mahdi Derakhshani", "Cees G. M. Snoek", "Pascal Mettes", "Paul Groth"], "summary": "Continual learning has traditionally focused on classifying either instances\nor classes, but real-world applications, such as robotics and self-driving\ncars, require models to handle both simultaneously. To mirror real-life\nscenarios, we introduce the task of continual learning of instances and\nclasses, at the same time. This task challenges models to adapt to multiple\nlevels of granularity over time, which requires balancing fine-grained instance\nrecognition with coarse-grained class generalization. In this paper, we\nidentify that classes and instances naturally form a hierarchical structure. To\nmodel these hierarchical relationships, we propose HyperCLIC, a continual\nlearning algorithm that leverages hyperbolic space, which is uniquely suited\nfor hierarchical data due to its ability to represent tree-like structures with\nlow distortion and compact embeddings. Our framework incorporates hyperbolic\nclassification and distillation objectives, enabling the continual embedding of\nhierarchical relations. To evaluate performance across multiple granularities,\nwe introduce continual hierarchical metrics. We validate our approach on\nEgoObjects, the only dataset that captures the complexity of hierarchical\nobject recognition in dynamic real-world environments. Empirical results show\nthat HyperCLIC operates effectively at multiple granularities with improved\nhierarchical generalization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10710v1", "AI": {"title_translation": "实例和类的持续双曲学习", "tldr": "引入了同时进行实例和类持续学习的新任务，并提出了HyperCLIC，一种利用双曲空间处理层次化数据的持续学习算法，实现了多粒度下的有效操作和改进的层次化泛化。", "motivation": "传统的持续学习侧重于实例或类的单一分类，但机器人和自动驾驶等现实应用需要模型同时处理实例和类。为了反映真实场景，本文引入了同时进行实例和类持续学习的任务，挑战模型适应随时间变化的多粒度数据。", "method": "本文提出了HyperCLIC，一种持续学习算法，利用双曲空间来建模类和实例之间自然形成的层次结构。该框架结合了双曲分类和蒸馏目标，以实现层次关系的持续嵌入。", "result": "经验结果表明，HyperCLIC能在多个粒度下有效运行，并改进了层次化泛化能力。", "conclusion": "HyperCLIC成功地解决了同时进行实例和类持续学习的新任务，通过利用双曲空间有效处理了层次化数据，并在多粒度下表现出优异的性能和泛化能力。", "translation": "持续学习传统上专注于对实例或类进行分类，但机器人和自动驾驶汽车等现实世界应用要求模型同时处理两者。为了反映现实生活场景，我们引入了同时进行实例和类持续学习的任务。这项任务挑战模型随着时间推移适应多个粒度级别，这需要平衡细粒度的实例识别与粗粒度的类泛化。在本文中，我们发现类和实例自然形成一个层次结构。为了建模这些层次关系，我们提出了HyperCLIC，一种利用双曲空间的持续学习算法，双曲空间因其能够以低失真和紧凑嵌入来表示树状结构而特别适合层次数据。我们的框架结合了双曲分类和蒸馏目标，从而能够持续嵌入层次关系。为了评估跨多个粒度的性能，我们引入了持续层次度量。我们在EgoObjects上验证了我们的方法，这是唯一捕获动态现实世界环境中层次对象识别复杂性的数据集。经验结果表明，HyperCLIC在多个粒度下有效运行，并改进了层次化泛化能力。", "summary": "本研究提出了一项新的持续学习任务：同时学习实例和类，以满足机器人和自动驾驶等现实世界应用的需求。针对类和实例的层次结构特性，论文提出了HyperCLIC算法，该算法利用双曲空间对层次数据进行建模，并结合双曲分类和蒸馏目标。通过在EgoObjects数据集上的验证，HyperCLIC在多粒度下表现出有效的操作和改进的层次化泛化能力。", "keywords": "持续学习, 双曲学习, 层次化数据, 实例分类, 类泛化", "comments": "本文的创新点在于引入了一个全新的持续学习任务，即同时处理实例和类的持续学习，这更贴近现实世界的复杂场景。此外，其将双曲几何引入持续学习领域，利用双曲空间对层次化数据进行建模，有效解决了多粒度下平衡细粒度识别和粗粒度泛化的挑战，为持续学习开辟了新的研究方向。"}}
{"id": "2506.10842", "title": "Advanced fraud detection using machine learning models: enhancing financial transaction security", "authors": ["Nudrat Fariha", "Md Nazmuddin Moin Khan", "Md Iqbal Hossain", "Syed Ali Reza", "Joy Chakra Bortty", "Kazi Sharmin Sultana", "Md Shadidur Islam Jawad", "Saniah Safat", "Md Abdul Ahad", "Maksuda Begum"], "summary": "The rise of digital payments has accelerated the need for intelligent and\nscalable systems to detect fraud. This research presents an end-to-end,\nfeature-rich machine learning framework for detecting credit card transaction\nanomalies and fraud using real-world data. The study begins by merging\ntransactional, cardholder, merchant, and merchant category datasets from a\nrelational database to create a unified analytical view. Through the feature\nengineering process, we extract behavioural signals such as average spending,\ndeviation from historical patterns, transaction timing irregularities, and\ncategory frequency metrics. These features are enriched with temporal markers\nsuch as hour, day of week, and weekend indicators to expose all latent patterns\nthat indicate fraudulent behaviours. Exploratory data analysis reveals\ncontextual transaction trends across all the dataset features. Using the\ntransactional data, we train and evaluate a range of unsupervised models:\nIsolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct\nnormal behavior. These models flag the top 1% of reconstruction errors as\noutliers. PCA visualizations illustrate each models ability to separate\nanomalies into a two-dimensional latent space. We further segment the\ntransaction landscape using K-Means clustering and DBSCAN to identify dense\nclusters of normal activity and isolate sparse, suspicious regions.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10842v1", "AI": {"title_translation": "使用机器学习模型进行高级欺诈检测：增强金融交易安全性", "tldr": "本研究开发了一个端到端的机器学习框架，用于使用真实世界数据检测信用卡交易异常和欺诈。", "motivation": "数字支付的兴起加速了对智能和可扩展欺诈检测系统的需求。", "method": "研究通过合并交易、持卡人、商户和商户类别数据集来创建统一的分析视图。通过特征工程，提取了平均消费、偏离历史模式、交易时间不规律和类别频率等行为信号，并用时间标记（如小时、星期几、周末指示器）进行丰富。使用交易数据训练和评估了一系列无监督模型：Isolation Forest、One Class SVM 和深度自编码器，这些模型将前1%的重建错误标记为异常值。此外，还使用K-Means聚类和DBSCAN来识别正常活动的密集簇和稀疏的可疑区域。", "result": "探索性数据分析揭示了所有数据集特征中的上下文交易趋势。所使用的模型能够将异常值分离到二维潜在空间中，并且聚类技术能够识别正常活动的密集簇和稀疏的可疑区域。", "conclusion": "Not mentioned in abstract", "translation": "数字支付的兴起加速了对智能和可扩展欺诈检测系统的需求。本研究提出了一个端到端的、特征丰富的机器学习框架，用于使用真实世界数据检测信用卡交易异常和欺诈。该研究首先通过合并关系数据库中的交易、持卡人、商户和商户类别数据集，创建一个统一的分析视图。通过特征工程过程，我们提取了行为信号，如平均消费、偏离历史模式、交易时间不规律和类别频率指标。这些特征通过时间标记（如小时、星期几和周末指示器）进行丰富，以揭示所有指示欺诈行为的潜在模式。探索性数据分析揭示了所有数据集特征中的上下文交易趋势。使用交易数据，我们训练并评估了一系列无监督模型：Isolation Forest、One Class SVM 和一个训练用于重建正常行为的深度自编码器。这些模型将前1%的重建错误标记为异常值。PCA可视化展示了每个模型将异常值分离到二维潜在空间的能力。我们通过K-Means聚类和DBSCAN进一步分割交易格局，以识别正常活动的密集簇并隔离稀疏的可疑区域。", "summary": "本研究提出了一个端到端的机器学习框架，旨在通过利用真实世界的信用卡交易数据来增强金融交易的安全性。该框架通过整合多个数据集、进行丰富的特征工程（包括行为信号和时间标记）来识别潜在的欺诈模式。研究训练并评估了多种无监督模型，包括Isolation Forest、One Class SVM和深度自编码器，以检测异常交易。此外，还利用PCA可视化和K-Means、DBSCAN等聚类技术来区分正常活动区域和可疑区域，从而有效地识别和隔离欺诈行为。", "keywords": "欺诈检测, 机器学习, 无监督学习, 异常检测, 金融安全", "comments": "该论文的创新之处在于其构建了一个全面的端到端机器学习框架，整合了多源数据、细致的特征工程以及多种无监督学习模型和聚类技术，以应对复杂的金融欺诈检测问题。其重要性在于为信用卡交易安全提供了一个智能且可扩展的解决方案，能够从真实世界数据中有效识别异常行为。"}}
{"id": "2506.10712", "title": "Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement", "authors": ["Yuqi Shen", "Fengyang Xiao", "Sujie Hu", "Youwei Pang", "Yifan Pu", "Chengyu Fang", "Xiu Li", "Chunming He"], "summary": "Camouflaged Object Detection (COD) presents inherent challenges due to the\nsubtle visual differences between targets and their backgrounds. While existing\nmethods have made notable progress, there remains significant potential for\npost-processing refinement that has yet to be fully explored. To address this\nlimitation, we propose the Uncertainty-Masked Bernoulli Diffusion (UMBD) model,\nthe first generative refinement framework specifically designed for COD. UMBD\nintroduces an uncertainty-guided masking mechanism that selectively applies\nBernoulli diffusion to residual regions with poor segmentation quality,\nenabling targeted refinement while preserving correctly segmented areas. To\nsupport this process, we design the Hybrid Uncertainty Quantification Network\n(HUQNet), which employs a multi-branch architecture and fuses uncertainty from\nmultiple sources to improve estimation accuracy. This enables adaptive guidance\nduring the generative sampling process. The proposed UMBD framework can be\nseamlessly integrated with a wide range of existing Encoder-Decoder-based COD\nmodels, combining their discriminative capabilities with the generative\nadvantages of diffusion-based refinement. Extensive experiments across multiple\nCOD benchmarks demonstrate consistent performance improvements, achieving\naverage gains of 5.5% in MAE and 3.2% in weighted F-measure with only modest\ncomputational overhead. Code will be released.", "comment": "16 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10712v1", "AI": {"title_translation": "针对伪装目标检测精修的不确定性掩膜伯努利扩散", "tldr": "提出UMBD模型，首个用于伪装目标检测(COD)的生成式精修框架，通过不确定性引导的伯努利扩散提升COD性能。", "motivation": "现有伪装目标检测方法在后处理精修方面仍有未充分探索的潜力，目标与背景的细微视觉差异导致固有的挑战。", "method": "提出不确定性掩膜伯努利扩散(UMBD)模型，首个专为COD设计的生成式精修框架。它引入不确定性引导的掩膜机制，选择性地将伯努利扩散应用于分割质量差的残差区域。为支持此过程，设计了混合不确定性量化网络(HUQNet)，采用多分支架构并融合多源不确定性以提高估计精度，从而在生成采样过程中提供自适应指导。UMBD可与现有编码器-解码器COD模型无缝集成。", "result": "在多个COD基准测试中表现出持续的性能改进，平均MAE提升5.5%，加权F-measure提升3.2%，计算开销适中。", "conclusion": "提出的UMBD框架通过结合判别式能力和扩散生成优势，显著提升了伪装目标检测的精修效果，且计算开销适中。", "translation": "伪装目标检测（COD）由于目标与背景之间细微的视觉差异而带来固有的挑战。尽管现有方法取得了显著进展，但在后处理精修方面仍存在尚未充分探索的巨大潜力。为了解决这一局限性，我们提出了不确定性掩膜伯努利扩散（UMBD）模型，这是首个专门为COD设计的生成式精修框架。UMBD引入了一种不确定性引导的掩膜机制，选择性地将伯努利扩散应用于分割质量差的残差区域，从而实现有针对性的精修，同时保留正确分割的区域。为了支持这一过程，我们设计了混合不确定性量化网络（HUQNet），它采用多分支架构并融合来自多个源的不确定性以提高估计精度。这使得在生成采样过程中能够进行自适应指导。所提出的UMBD框架可以与各种现有的基于编码器-解码器的COD模型无缝集成，将它们的判别能力与基于扩散的精修的生成优势相结合。在多个COD基准测试中进行的广泛实验表明，性能持续提升，平均MAE提升5.5%，加权F-measure提升3.2%，而计算开销仅适中。代码将发布。", "summary": "本文提出不确定性掩膜伯努利扩散(UMBD)模型，这是首个用于伪装目标检测(COD)的生成式精修框架。UMBD通过不确定性引导的掩膜机制，将伯努利扩散选择性地应用于分割质量差的区域，并设计了混合不确定性量化网络(HUQNet)来提高不确定性估计精度。该框架可与现有COD模型无缝集成，并在多个基准测试中实现了显著的性能提升，同时保持适度的计算开销。", "keywords": "伪装目标检测, 伯努利扩散, 不确定性量化, 生成模型, 后处理精修", "comments": "这篇论文的创新点在于首次将生成式扩散模型应用于伪装目标检测的后处理精修，并引入了不确定性引导的掩膜机制，实现了有针对性的精修。其提出的HUQNet也有效地提升了不确定性估计的准确性，使得模型能够更智能地进行精修。该方法能够与现有COD模型无缝集成，具有良好的通用性和实用价值，为COD领域提供了一个新的精修范式。"}}
{"id": "2506.10871", "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization", "authors": ["Pierre-François Massiani", "Alexander von Rohr", "Lukas Haverbeck", "Sebastian Trimpe"], "summary": "Despite the many recent advances in reinforcement learning (RL), the question\nof learning policies that robustly satisfy state constraints under unknown\ndisturbances remains open. In this paper, we offer a new perspective on\nachieving robust safety by analyzing the interplay between two well-established\ntechniques in model-free RL: entropy regularization, and constraints\npenalization. We reveal empirically that entropy regularization in constrained\nRL inherently biases learning toward maximizing the number of future viable\nactions, thereby promoting constraints satisfaction robust to action noise.\nFurthermore, we show that by relaxing strict safety constraints through\npenalties, the constrained RL problem can be approximated arbitrarily closely\nby an unconstrained one and thus solved using standard model-free RL. This\nreformulation preserves both safety and optimality while empirically improving\nresilience to disturbances. Our results indicate that the connection between\nentropy regularization and robustness is a promising avenue for further\nempirical and theoretical investigation, as it enables robust safety in RL\nthrough simple reward shaping.", "comment": "24 pages, 11 figures, 2 tables. Accepted for publication at ECML-PKDD\n  2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10871v1", "AI": {"title_translation": "未来行动的生存能力：通过熵正则化实现强化学习中的鲁棒安全性", "tldr": "本文通过分析熵正则化和约束惩罚的相互作用，提出了一种在强化学习中实现鲁棒安全性的新方法，并发现熵正则化能促进未来可行行动的数量，从而提高对动作噪声的鲁棒性。", "motivation": "尽管强化学习（RL）取得了许多进展，但在未知扰动下学习能够稳健满足状态约束的策略仍然是一个悬而未决的问题。", "method": "作者分析了无模型强化学习中熵正则化和约束惩罚这两种技术之间的相互作用。他们通过将严格安全约束通过惩罚松弛，将有约束的RL问题近似为一个无约束问题，并使用标准的无模型RL方法解决。", "result": "经验表明，在约束RL中，熵正则化固有地使学习偏向于最大化未来可行行动的数量，从而促进对动作噪声鲁棒的约束满足。这种重新表述在经验上提高了对扰动的弹性，同时保留了安全性和最优性。", "conclusion": "熵正则化与鲁棒性之间的联系是一个有前景的研究方向，因为它通过简单的奖励塑形实现了RL中的鲁棒安全性。", "translation": "尽管强化学习（RL）最近取得了许多进展，但在未知扰动下学习能够稳健满足状态约束的策略仍然是一个悬而未决的问题。在本文中，我们通过分析无模型RL中两种成熟技术——熵正则化和约束惩罚——之间的相互作用，提供了一种实现鲁棒安全性的新视角。我们通过经验揭示，约束RL中的熵正则化本质上偏向于最大化未来可行行动的数量，从而促进对动作噪声具有鲁棒性的约束满足。此外，我们表明，通过惩罚放松严格的安全约束，有约束的RL问题可以被任意接近地近似为一个无约束问题，从而可以使用标准的无模型RL解决。这种重新表述在经验上提高了对扰动的弹性，同时保留了安全性和最优性。我们的结果表明，熵正则化与鲁棒性之间的联系是一个有前景的进一步经验和理论研究方向，因为它通过简单的奖励塑形实现了RL中的鲁棒安全性。", "summary": "本文探讨了在强化学习中实现鲁棒安全性的问题，特别是在未知扰动下。作者提出了一种新的方法，通过分析熵正则化和约束惩罚在无模型RL中的相互作用。研究发现，熵正则化有助于最大化未来可行行动的数量，从而提高对动作噪声的鲁棒性。此外，通过将严格安全约束转化为惩罚项，可以将有约束的RL问题近似为无约束问题，并用标准方法求解，同时保持安全性和最优性并提高对扰动的弹性。这表明熵正则化与鲁棒性之间的联系是实现RL中鲁棒安全性的有效途径。", "keywords": "强化学习, 鲁棒安全性, 熵正则化, 约束惩罚, 奖励塑形", "comments": "这篇论文的创新点在于揭示了熵正则化在约束强化学习中对促进鲁棒安全性的内在作用，并提出了通过奖励塑形将有约束问题转化为无约束问题的实用方法。其重要性在于为解决强化学习中的安全问题提供了一个新的视角和实用的解决方案，特别是在存在未知扰动的情况下。"}}
{"id": "2506.10888", "title": "Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers", "authors": ["Lucas Gnecco-Heredia", "Benjamin Negrevergne", "Yann Chevaleyre"], "summary": "Finite mixtures of classifiers (a.k.a. randomized ensembles) have been\nproposed as a way to improve robustness against adversarial attacks. However,\nexisting attacks have been shown to not suit this kind of classifier. In this\npaper, we discuss the problem of attacking a mixture in a principled way and\nintroduce two desirable properties of attacks based on a geometrical analysis\nof the problem (effectiveness and maximality). We then show that existing\nattacks do not meet both of these properties. Finally, we introduce a new\nattack called {\\em lattice climber attack} with theoretical guarantees in the\nbinary linear setting, and demonstrate its performance by conducting\nexperiments on synthetic and real datasets.", "comment": "17 pages including bibliography + 13 pages of supplementary material.\n  Extended version of the article accepted at ECML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10888v1", "AI": {"title_translation": "格子攀爬攻击：针对分类器随机混合的对抗性攻击", "tldr": "现有对抗性攻击不适用于分类器随机混合。本文提出了一种新的“格子攀爬攻击”，具有理论保证，并在实验中表现良好。", "motivation": "分类器随机混合被提出以提高对抗性攻击的鲁棒性，但现有攻击对这类分类器不适用。", "method": "本文首先从几何分析的角度讨论了攻击分类器混合的问题，并引入了攻击的两个理想特性（有效性和最大性）。随后，提出了一种名为“格子攀爬攻击”的新型攻击，并在二元线性设置下提供了理论保证。", "result": "现有攻击无法同时满足有效性和最大性这两个特性。新提出的“格子攀爬攻击”在合成数据集和真实数据集上都展示了其性能。", "conclusion": "论文成功地提出了一个针对分类器随机混合的有效对抗性攻击方法——格子攀爬攻击，解决了现有攻击的局限性，并提供了理论和实验支持。", "translation": "有限分类器混合（又称随机集成）已被提出作为提高对抗性攻击鲁棒性的一种方式。然而，现有攻击已被证明不适用于这类分类器。在本文中，我们以一种原则性的方式讨论了攻击混合模型的问题，并基于问题的几何分析引入了攻击的两个理想特性（有效性和最大性）。然后我们表明现有攻击不能同时满足这两个特性。最后，我们引入了一种新的攻击，称为“格子攀爬攻击”，它在二元线性设置中具有理论保证，并通过在合成数据集和真实数据集上进行实验证明了其性能。", "summary": "本文针对分类器随机混合的对抗性攻击问题，指出现有攻击的不足。通过几何分析，提出了攻击的有效性和最大性两个新特性，并证明现有攻击未能满足。为此，论文引入了“格子攀爬攻击”，该攻击在二元线性设置下具有理论保证，并通过在合成和真实数据集上的实验验证了其有效性。", "keywords": "对抗性攻击, 分类器混合, 随机集成, 格子攀爬攻击, 鲁棒性", "comments": "这篇论文的创新点在于从几何分析的角度提出了评估对抗性攻击有效性的新特性，并针对性地设计了“格子攀爬攻击”来克服现有方法在随机混合分类器上的局限性。其理论保证和实验验证增加了研究的严谨性。"}}
{"id": "2506.10730", "title": "IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain", "authors": ["Hong Huang", "Weixiang Sun", "Zhijian Wu", "Jingwen Niu", "Donghuan Lu", "Xian Wu", "Yefeng Zheng"], "summary": "Recent advances in vision-language models, such as CLIP, have significantly\nimproved performance in zero- and few-shot anomaly detection (ZFSAD) tasks.\nHowever, most existing CLIP-based methods assume prior knowledge of categories\nand rely on carefully designed prompts tailored to specific scenarios. While\nthese text prompts capture semantic information in the textual space, they\noften fail to distinguish normal and anomalous instances in the joint embedding\nspace. Moreover, most ZFSAD approaches focus on industrial domains, with\nlimited exploration in medical tasks. To address these limitations, we propose\nIQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query\nembeddings integrating both textual and instance-aware visual information serve\nas more effective indicators of anomalies. Specifically, we introduce\nclass-based and learnable prompting tokens to better adapt CLIP to the medical\nsetting. Furthermore, we design an instance-aware query module that extracts\nregion-level contextual information from both modalities, enabling the\ngeneration of anomaly-sensitive embeddings. Extensive experiments on six\nmedical datasets demonstrate that IQE-CLIP achieves state-of-the-art\nperformance in both zero-shot and few-shot settings. Code and data are\navailable at \\href{https://github.com/hongh0/IQE-CLIP/}{this https URL}.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10730v1", "AI": {"title_translation": "IQE-CLIP：用于医学领域零/少样本异常检测的实例感知查询嵌入", "tldr": "IQE-CLIP提出了一种新的框架，通过整合文本和实例感知视觉信息，在医学领域的零/少样本异常检测任务中实现了最先进的性能。", "motivation": "现有的基于CLIP的方法在零/少样本异常检测（ZFSAD）中表现良好，但通常需要先验类别知识和精心设计的提示，并且难以区分正常和异常实例。此外，大多数ZFSAD方法集中在工业领域，在医学任务中的探索有限。", "method": "本文提出了IQE-CLIP框架，通过整合文本和实例感知视觉信息来生成更有效的异常指示器。具体来说，引入了基于类别和可学习的提示标记以更好地适应医学设置，并设计了一个实例感知查询模块，从两种模态中提取区域级上下文信息，生成对异常敏感的嵌入。", "result": "在六个医学数据集上进行了广泛实验，证明IQE-CLIP在零样本和少样本设置中均实现了最先进的性能。", "conclusion": "IQE-CLIP通过结合文本和实例感知的视觉信息，显著提升了医学领域零/少样本异常检测的性能，证明了其作为有效异常指示器的潜力。", "translation": "视觉-语言模型（如CLIP）的最新进展显著提高了零样本和少样本异常检测（ZFSAD）任务的性能。然而，大多数现有的基于CLIP的方法假设类别先验知识，并依赖于针对特定场景精心设计的提示。虽然这些文本提示在文本空间中捕获语义信息，但它们通常无法在联合嵌入空间中区分正常和异常实例。此外，大多数ZFSAD方法侧重于工业领域，在医学任务中的探索有限。为了解决这些限制，我们提出了IQE-CLIP，一个用于医学领域ZFSAD的新颖框架。我们表明，整合了文本和实例感知视觉信息的查询嵌入可以作为更有效的异常指示器。具体来说，我们引入了基于类别和可学习的提示标记，以更好地使CLIP适应医学环境。此外，我们设计了一个实例感知查询模块，从两种模态中提取区域级上下文信息，从而生成对异常敏感的嵌入。在六个医学数据集上进行的广泛实验表明，IQE-CLIP在零样本和少样本设置中均实现了最先进的性能。代码和数据可在https://github.com/hongh0/IQE-CLIP/获取。", "summary": "本文提出了IQE-CLIP，一个用于医学领域零/少样本异常检测的新框架。它解决了现有CLIP方法在区分正常/异常实例以及在医学领域应用受限的问题。IQE-CLIP通过结合文本和实例感知视觉信息生成异常敏感的查询嵌入，并引入了类基础和可学习的提示标记以及实例感知查询模块。实验证明其在六个医学数据集上达到了最先进的性能。", "keywords": "零样本异常检测, 少样本异常检测, 医疗图像, CLIP, 实例感知查询嵌入", "comments": "IQE-CLIP的创新之处在于其将实例感知视觉信息融入查询嵌入，以解决传统CLIP方法在医学领域异常检测中区分正常与异常实例的局限性。其引入的类基础和可学习提示标记以及实例感知查询模块，增强了模型对医学图像特性的适应性。该工作对于推动视觉-语言模型在医疗AI领域的应用具有重要意义。"}}
{"id": "2506.10892", "title": "The Diffusion Duality", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo", "comment": "ICML 2025. We provide the code at: https://github.com/s-sahoo/duo", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10892v1", "AI": {"title_translation": "扩散对偶性", "tldr": "通过利用高斯扩散与统一状态离散扩散之间的对偶性，Duo方法显著提升了离散扩散模型的训练速度和采样效率，使其在文本生成任务上超越了自回归模型。", "motivation": "统一状态离散扩散模型在文本生成方面具有自我纠正的潜力，但其性能通常不如自回归模型和掩码扩散模型。本文旨在缩小这一性能差距。", "method": "本文提出了Duo方法，利用统一状态扩散过程源于高斯扩散的关键见解，将高斯扩散的强大技术应用于改进训练和采样。具体包括：1) 引入由高斯过程引导的课程学习策略，通过减少方差使训练速度加倍。2) 提出离散一致性蒸馏，将连续设置下的一致性蒸馏适应到离散设置，以实现扩散语言模型的少步生成。", "result": "1) 课程学习策略将训练速度提高了一倍。2) 经课程学习训练的模型在7个基准测试中的3个上，零样本困惑度超越了自回归模型。3) 离散一致性蒸馏算法将采样速度加快了两个数量级，实现了扩散语言模型的少步生成。", "conclusion": "本文通过引入Duo方法，成功地缩小了统一状态离散扩散模型与自回归及掩码扩散模型之间的性能差距，显著提升了训练和采样效率，并使其在文本生成任务中表现出色。", "translation": "统一状态离散扩散模型因其固有的自我纠正能力，有望实现快速文本生成。然而，它们的性能通常不如自回归模型和掩码扩散模型。在这项工作中，我们通过利用一个关键见解来缩小这一性能差距：统一状态扩散过程自然地从底层高斯扩散中涌现。我们的方法Duo，将高斯扩散的强大技术应用于改进训练和采样。首先，我们引入了一种由高斯过程引导的课程学习策略，通过减少方差使训练速度加倍。使用课程学习训练的模型在7个基准测试中的3个上，零样本困惑度超过了自回归模型。其次，我们提出了离散一致性蒸馏，它将一致性蒸馏从连续设置适应到离散设置。该算法通过将采样速度加快两个数量级，解锁了扩散语言模型中的少步生成。我们提供了项目页面上的代码和模型检查点：http://s-sahoo.github.io/duo", "summary": "本文提出了一种名为Duo的新方法，旨在提升统一状态离散扩散模型在文本生成任务上的性能。通过利用统一状态扩散与高斯扩散之间的对偶性，Duo引入了两种关键技术：一是基于高斯过程的课程学习策略，将训练速度提升一倍并在部分基准测试上超越自回归模型；二是离散一致性蒸馏，显著加速采样并实现少步生成。这些创新显著缩小了离散扩散模型与其他先进模型之间的性能差距。", "keywords": "离散扩散模型, 高斯扩散, 文本生成, 课程学习, 一致性蒸馏", "comments": "这篇论文通过揭示统一状态离散扩散过程与高斯扩散之间的对偶性，为离散扩散模型的性能提升提供了新的视角和方法。其创新点在于将连续扩散领域的先进技术（如一致性蒸馏和课程学习）巧妙地迁移到离散设置，有效解决了离散扩散模型训练慢、采样效率低的问题。特别是采样速度提升两个数量级和在部分基准测试上超越自回归模型的结果，显示了其在快速高质量文本生成方面的巨大潜力。"}}
{"id": "2506.10741", "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework", "authors": ["SiXiang Chen", "Jianyu Lai", "Jialin Gao", "Tian Ye", "Haoyu Chen", "Hengyu Shi", "Shitong Shao", "Yunlong Lin", "Song Fei", "Zhaohu Xing", "Yeying Jin", "Junfeng Luo", "Xiaoming Wei", "Lei Zhu"], "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10741v1", "AI": {"title_translation": "PosterCraft：统一框架下高质量美学海报生成的再思考", "tldr": "PosterCraft是一个统一的框架，用于生成高质量的美学海报。它通过一个级联工作流优化了文本渲染、布局和整体美学，显著优于现有基线，并接近SOTA商业系统的质量。", "motivation": "生成美学海报比简单的设计图像更具挑战性，因为它不仅需要精确的文本渲染，还需要抽象艺术内容、引人注目的布局和整体风格和谐的无缝整合。", "method": "本文提出了PosterCraft，一个统一的框架，放弃了传统的模块化流水线和僵化的预定义布局。它采用精心设计的级联工作流来优化高美学海报的生成，包括：(i) 在新引入的Text-Render-2M数据集上进行大规模文本渲染优化；(ii) 在HQ-Poster100K上进行区域感知监督微调；(iii) 通过n选1偏好优化进行美学文本强化学习；(iv) 联合视觉-语言反馈细化。每个阶段都由专门定制的自动化数据构建管道支持。", "result": "在多项实验中，PosterCraft在渲染准确性、布局连贯性和整体视觉吸引力方面显著优于开源基线，其质量接近最先进的商业系统。", "conclusion": "PosterCraft通过其统一的框架和级联工作流，有效解决了美学海报生成中的复杂挑战，实现了高质量的输出，并提供了接近商业系统水平的性能。", "translation": "生成美学海报比简单的设计图像更具挑战性：它不仅需要精确的文本渲染，还需要抽象艺术内容、引人注目的布局和整体风格和谐的无缝整合。为了解决这个问题，我们提出了PosterCraft，一个统一的框架，它放弃了先前的模块化流水线和僵化的预定义布局，允许模型自由探索连贯、视觉上引人注目的构图。PosterCraft采用精心设计的级联工作流来优化高美学海报的生成：(i) 在我们新引入的Text-Render-2M数据集上进行大规模文本渲染优化；(ii) 在HQ-Poster100K上进行区域感知监督微调；(iii) 通过n选1偏好优化进行美学文本强化学习；(iv) 联合视觉-语言反馈细化。每个阶段都由专门定制的完全自动化数据构建管道支持，无需复杂的架构修改即可实现稳健训练。在多项实验中，PosterCraft在渲染准确性、布局连贯性和整体视觉吸引力方面显著优于开源基线，其质量接近最先进的商业系统。我们的代码、模型和数据集可在项目页面找到：https://ephemeral182.github.io/PosterCraft", "summary": "PosterCraft是一个创新的统一框架，旨在解决高质量美学海报生成中的复杂挑战。它摒弃了传统模块化方法和固定布局，通过一个包含文本渲染优化、区域感知微调、美学文本强化学习和视觉-语言反馈细化的级联工作流，实现自由且视觉吸引人的构图。该框架在自动化数据构建管道的支持下，显著提升了渲染准确性、布局连贯性和整体视觉效果，性能超越现有开源基线，并接近顶尖商业系统。", "keywords": "美学海报生成, 统一框架, 文本渲染, 强化学习, 计算机视觉", "comments": "PosterCraft的创新之处在于其统一的框架设计，摒弃了传统的模块化和预设布局，允许模型自由探索构图。其级联工作流和自动化数据构建管道是关键，特别是在大规模文本渲染优化和美学文本强化学习方面。该研究的重要性在于其在美学海报生成领域取得了显著进展，提升了生成质量，并为未来的研究提供了新的数据集和方法。"}}
{"id": "2506.10911", "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models", "authors": ["Jari Kolehmainen", "Nikolay Blagoev", "John Donaghy", "Oğuzhan Ersoy", "Christopher Nies"], "summary": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of\nmodel sizes and accelerator counts.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10911v1", "AI": {"title_translation": "NoLoCo: 大型模型无all-reduce低通信训练方法", "tldr": "NoLoCo是一种新型优化方法，用于训练大型模型，它通过隐式同步而非显式集体通信来显著减少通信开销，并提高了收敛速度。", "motivation": "训练大型语言模型通常需要大量加速器的集群，并通过高带宽互连进行通信。扩展这些集群成本高昂且不切实际，限制了可训练模型的规模。现有的低通信训练方法仍需要模型参数的同步步骤，这在低带宽网络上成本很高。", "method": "本文提出了一种名为NoLoCo的新型优化方法，它在训练期间不显式同步所有模型参数，因此不需要任何集体通信。NoLoCo通过Nesterov动量优化器的一种新颖变体，通过将模型权重与随机选择的另一个模型权重进行部分平均来隐式同步模型权重。论文提供了理论收敛性分析和语言模型训练的实证结果。", "result": "NoLoCo在1.25亿到68亿参数的模型尺寸和广泛的加速器数量上进行了基准测试。与完全分片数据并行训练或广泛使用的低通信训练方法DiLoCo相比，NoLoCo所需的通信开销显著减少。对于数百个加速器通过互联网训练，其同步步骤比DiLoCo中使用的all-reduce快一个数量级。NoLoCo没有全局阻塞通信，从而减少了加速器空闲时间。与DiLoCo相比，NoLoCo在各种模型尺寸和加速器数量下，收敛速度最高可提高4%。", "conclusion": "NoLoCo是一种高效的低通信训练方法，通过避免显式集体通信和采用创新的隐式同步机制，显著降低了大型模型训练的通信开销，并提高了训练效率和收敛速度。", "translation": "训练大型语言模型通常通过在包含数万个加速器的集群上进行优化方法来完成，这些集群通过高带宽互连进行通信。扩展这些集群成本高昂且可能不切实际，从而限制了可训练模型的规模。最近的几项研究提出了通信密集度较低的训练方法，避免了对高度连接的计算集群的需求。这些最先进的低通信训练方法仍然采用模型参数的同步步骤，当在所有模型副本上执行时，这在低带宽网络上可能会变得非常昂贵。\n在这项工作中，我们提出了一种新颖的优化方法NoLoCo，它在训练期间不显式同步所有模型参数，因此不需要任何集体通信。NoLoCo通过Nesterov动量优化器的一种新颖变体，通过将模型权重与随机选择的另一个模型权重进行部分平均来隐式同步模型权重。我们为我们提出的优化器提供了理论收敛性分析以及语言模型训练的实证结果。\n我们在1.25亿到68亿参数的广泛加速器数量和模型尺寸上对NoLoCo进行了基准测试。我们的方法比完全分片数据并行训练甚至广泛使用的低通信训练方法DiLoCo所需的通信开销显著减少。对于数百个加速器通过互联网训练，同步步骤本身估计比DiLoCo中使用的all-reduce快一个数量级。我们也没有任何全局阻塞通信，从而减少了加速器空闲时间。与DiLoCo相比，我们还在各种模型尺寸和加速器数量下观察到高达4%的收敛速度。", "summary": "NoLoCo是一种创新的大型模型训练优化方法，旨在解决现有分布式训练中高昂的通信成本问题。它通过避免显式集体通信（如all-reduce）和采用基于Nesterov动量优化器变体的隐式权重同步机制来实现低通信。实验证明，NoLoCo显著减少了通信开销，同步速度比现有方法快一个数量级，消除了全局阻塞通信，并能将收敛速度提高高达4%。这使得NoLoCo成为在低带宽网络或大规模集群上训练大型模型的更高效选择。", "keywords": "NoLoCo, 低通信, 大型模型, 分布式训练, 隐式同步", "comments": "NoLoCo的主要创新在于其独特的隐式权重同步机制，通过部分平均随机选择的副本权重，彻底避免了代价高昂的all-reduce操作。这对于在低带宽网络或极其大规模的分布式环境中训练大型模型具有重要意义，因为它直接解决了通信瓶颈，提高了训练效率和可扩展性。"}}
{"id": "2506.10774", "title": "Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales", "authors": ["Wenhao Guo", "Peng Lu", "Xujun Peng", "Zhaoran Zhao", "Sheng Li"], "summary": "Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience\na significant performance decline when the upsampling factor exceeds the range\ncovered by the training data, introducing substantial blurring. To address this\nissue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for\nultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier,\nwhich decomposes the image into a series of strokes represented as vector\ngraphics for magnification. Then, the detail completion module also restores\nmissing details, ensuring high-fidelity image reconstruction. Our cyclic\nstrategy achieves ultra-large upsampling by iteratively refining details with\nthis unified SbCA model, trained only once for all, while keeping sub-scales\nwithin the training range. Our approach effectively addresses the distribution\ndrift issue and eliminates artifacts, noise and blurring, producing\nhigh-quality, high-resolution super-resolved images. Experimental validations\non both synthetic and real-world datasets demonstrate that our approach\nsignificantly outperforms existing methods in ultra-large upsampling tasks\n(e.g. $\\times100$), delivering visual quality far superior to state-of-the-art\ntechniques.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10774v1", "AI": {"title_translation": "基于笔画的循环放大器：任意超大尺度图像超分辨率", "tldr": "提出了一种名为Stroke-based Cyclic Amplifier (SbCA)的统一模型，用于解决现有任意尺度图像超分辨率方法在超大尺度放大时性能显著下降的问题。SbCA通过将图像分解为笔画并迭代细化细节，实现了高质量的超大尺度图像超分辨率，并在实验中显著优于现有方法。", "motivation": "现有的任意尺度图像超分辨率（ASISR）方法在放大因子超出训练数据范围时，性能会显著下降，并引入大量模糊。", "method": "本文提出了一种统一模型Stroke-based Cyclic Amplifier (SbCA)，其核心是笔画向量放大器，将图像分解为一系列表示为矢量图形的笔画进行放大。随后，细节补全模块恢复缺失细节。通过循环策略，SbCA模型迭代细化细节以实现超大尺度上采样，且只需训练一次即可适用于所有子尺度，同时保持子尺度在训练范围内。", "result": "该方法有效解决了分布漂移问题，消除了伪影、噪声和模糊，生成了高质量、高分辨率的超分辨率图像。在合成和真实世界数据集上的实验验证表明，该方法在超大尺度上采样任务（例如×100）中显著优于现有方法，提供了远超现有技术的视觉质量。", "conclusion": "SbCA模型通过其独特的笔画分解和循环细化策略，成功解决了任意超大尺度图像超分辨率的挑战，实现了卓越的图像质量，超越了现有技术水平。", "translation": "先前的任意尺度图像超分辨率（ASISR）方法在放大因子超出训练数据范围时，性能常会显著下降，引入大量模糊。为解决此问题，我们提出了一种统一模型——基于笔画的循环放大器（SbCA），用于超大尺度上采样任务。SbCA的关键在于笔画向量放大器，它将图像分解为一系列表示为矢量图形的笔画进行放大。然后，细节补全模块也恢复缺失的细节，确保高保真图像重建。我们的循环策略通过使用这个统一的SbCA模型迭代细化细节来实现超大尺度上采样，该模型只需训练一次即可用于所有子尺度，同时将子尺度保持在训练范围内。我们的方法有效解决了分布漂移问题，消除了伪影、噪声和模糊，生成高质量、高分辨率的超分辨率图像。在合成和真实世界数据集上的实验验证表明，我们的方法在超大尺度上采样任务（例如×100）中显著优于现有方法，提供的视觉质量远超现有技术。", "summary": "本文提出了一种名为Stroke-based Cyclic Amplifier (SbCA) 的统一模型，旨在解决现有任意尺度图像超分辨率方法在超大尺度放大时性能显著下降的问题。SbCA通过将图像分解为笔画向量进行放大，并结合细节补全模块，确保高保真重建。其独特的循环策略允许模型在一次训练后，通过迭代细化实现任意超大尺度的上采样。实验证明，SbCA在超大尺度任务（如×100）中显著优于现有技术，生成高质量、无伪影的超分辨率图像。", "keywords": "图像超分辨率, 任意尺度, 超大尺度, 笔画分解, 循环放大", "comments": "该论文提出了一种新颖的基于笔画分解的图像超分辨率方法，创新性地将图像视为可放大的矢量图形，并通过循环策略实现了任意超大尺度的上采样，有效解决了现有方法在大放大倍数下的性能瓶颈，具有重要的实际应用价值。"}}
{"id": "2506.10914", "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10914v1", "AI": {"title_translation": "通过先验数据拟合网络实现因果推断的基础模型", "tldr": "本文介绍了CausalFM，一个基于先验数据拟合网络（PFNs）的综合框架，用于在各种因果推断设置中训练基础模型，并展示了其在条件平均治疗效果（CATE）估计上的竞争力。", "motivation": "先验数据拟合网络（PFNs）已被提出作为训练表格基础模型的一种有前景的方法，但目前缺乏将PFNs应用于因果推断的综合框架。本文旨在填补这一空白，提供一个通用范式来改变因果推断的实践方式。", "method": "本文提出了CausalFM框架，用于训练基于PFN的因果推断基础模型。首先，它基于结构因果模型（SCMs）原则性地形式化了因果推断的贝叶斯先验构建，并推导了先验有效性的必要标准。其次，它提出了一种利用因果启发式贝叶斯神经网络的新型先验分布族，使CausalFM能够在包括后门、前门和工具变量调整在内的各种设置中进行贝叶斯因果推断。最后，通过实例化CausalFM并训练一个用于估计条件平均治疗效果（CATEs）的基础模型，使用后门调整方法进行验证。", "result": "CausalFM在各种合成和半合成基准测试中，在CATE估计方面表现出竞争力。", "conclusion": "CausalFM提供了一个新的范式，可以作为训练各种因果推断设置中基础模型的通用方法，并有潜力从根本上改变医学、经济学和其他学科中因果推断的实践方式。", "translation": "先验数据拟合网络（PFNs）最近被提出作为训练表格基础模型的一种有前景的方法。PFNs是基于预先指定先验分布生成的合成数据进行预训练的Transformer，并通过上下文学习实现贝叶斯推断。在本文中，我们引入了CausalFM，一个用于在各种因果推断设置中训练基于PFN的基础模型的综合框架。首先，我们以原则性的方式，基于结构因果模型（SCMs）形式化了因果推断的贝叶斯先验构建，并推导了此类先验有效性的必要标准。在此基础上，我们提出了一种利用因果启发式贝叶斯神经网络的新型先验分布族，使CausalFM能够在包括后门、前门和工具变量调整在内的各种设置中执行贝叶斯因果推断。最后，我们实例化了CausalFM，并明确训练了一个用于使用后门调整估计条件平均治疗效果（CATEs）的基础模型。我们表明，CausalFM在使用各种合成和半合成基准测试进行CATE估计时表现出竞争力。总而言之，我们的框架可以作为训练各种因果推断设置中基础模型的通用方法。与当前因果推断的最新技术相比，CausalFM提供了一种新的范式，有可能从根本上改变从业者在医学、经济学和其他学科中执行因果推断的方式。", "summary": "本文介绍了CausalFM，一个基于先验数据拟合网络（PFNs）的综合框架，用于训练各种因果推断设置中的基础模型。CausalFM通过原则性地构建基于结构因果模型（SCMs）的贝叶斯先验，并利用因果启发式贝叶斯神经网络，实现了在后门、前门和工具变量调整等多种设置下的贝叶斯因果推断。实验结果表明，CausalFM在条件平均治疗效果（CATE）估计方面表现出竞争力，有望成为因果推断领域的一种通用且具有变革性的新范式。", "keywords": "基础模型, 因果推断, 先验数据拟合网络, 贝叶斯推断, 结构因果模型", "comments": "CausalFM的创新之处在于将PFNs这种新兴的表格基础模型技术与因果推断相结合，提出了一种统一的框架。通过形式化贝叶斯先验和引入因果启发式贝叶斯神经网络，它为构建可泛化到不同因果设置的基础模型提供了新的思路。其潜力在于能够简化和标准化因果推断的流程，尤其是在数据驱动的决策领域，具有重要的实践意义。"}}
{"id": "2506.10778", "title": "SlotPi: Physics-informed Object-centric Reasoning Models", "authors": ["Jian Li", "Wan Han", "Ning Lin", "Yu-Liang Zhan", "Ruizhi Chengze", "Haining Wang", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Fan Yu", "Hao Sun"], "summary": "Understanding and reasoning about dynamics governed by physical laws through\nvisual observation, akin to human capabilities in the real world, poses\nsignificant challenges. Currently, object-centric dynamic simulation methods,\nwhich emulate human behavior, have achieved notable progress but overlook two\ncritical aspects: 1) the integration of physical knowledge into models. Humans\ngain physical insights by observing the world and apply this knowledge to\naccurately reason about various dynamic scenarios; 2) the validation of model\nadaptability across diverse scenarios. Real-world dynamics, especially those\ninvolving fluids and objects, demand models that not only capture object\ninteractions but also simulate fluid flow characteristics. To address these\ngaps, we introduce SlotPi, a slot-based physics-informed object-centric\nreasoning model. SlotPi integrates a physical module based on Hamiltonian\nprinciples with a spatio-temporal prediction module for dynamic forecasting.\nOur experiments highlight the model's strengths in tasks such as prediction and\nVisual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,\nwe have created a real-world dataset encompassing object interactions, fluid\ndynamics, and fluid-object interactions, on which we validated our model's\ncapabilities. The model's robust performance across all datasets underscores\nits strong adaptability, laying a foundation for developing more advanced world\nmodels.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10778v1", "AI": {"title_translation": "SlotPi: 物理信息对象中心推理模型", "tldr": "SlotPi是一个基于槽位的物理信息对象中心推理模型，它结合了哈密顿原理和时空预测模块，用于理解和预测复杂的物理动态，并在各种数据集上表现出强大的适应性。", "motivation": "现有对象中心动态模拟方法忽视了物理知识的整合以及模型在多样化场景中的适应性验证，尤其是在涉及流体和物体交互的真实世界动态中。", "method": "引入SlotPi，一个基于槽位的物理信息对象中心推理模型。它将基于哈密顿原理的物理模块与用于动态预测的时空预测模块相结合。", "result": "模型在基准和流体数据集的预测和视觉问答(VQA)任务中表现出色。此外，还在包含物体交互、流体动力学和流体-物体交互的真实世界数据集上验证了模型的性能。", "conclusion": "SlotPi在所有数据集上的强大性能强调了其强大的适应性，为开发更先进的世界模型奠定了基础。", "translation": "理解和推理受物理定律支配的动态，通过视觉观察，类似于人类在现实世界中的能力，带来了巨大的挑战。目前，模仿人类行为的以对象为中心的动态模拟方法取得了显著进展，但忽略了两个关键方面：1）将物理知识整合到模型中。人类通过观察世界获得物理洞察力，并将这些知识应用于准确推理各种动态场景；2）验证模型在不同场景中的适应性。现实世界的动态，特别是涉及流体和物体的动态，要求模型不仅能捕捉物体交互，还能模拟流体流动特性。为了解决这些空白，我们引入了SlotPi，一个基于槽位的物理信息对象中心推理模型。SlotPi将基于哈密顿原理的物理模块与用于动态预测的时空预测模块相结合。我们的实验强调了模型在基准和流体数据集上的预测和视觉问答（VQA）等任务中的优势。此外，我们创建了一个包含物体交互、流体动力学和流体-物体交互的真实世界数据集，并在该数据集上验证了我们模型的能力。模型在所有数据集上的强大性能突显了其强大的适应性，为开发更先进的世界模型奠定了基础。", "summary": "SlotPi是一种新颖的基于槽位的物理信息对象中心推理模型，旨在解决现有动态模拟方法中物理知识整合不足和跨场景适应性差的问题。它通过结合基于哈密顿原理的物理模块和时空预测模块，能够理解和预测复杂的物理动态，包括流体与物体的交互。实验证明，SlotPi在预测和VQA任务上，以及在自建的真实世界数据集上均表现出强大的性能和适应性，为未来世界模型的发展奠定了基础。", "keywords": "物理信息, 对象中心推理, 动态预测, 哈密顿原理, 流体动力学", "comments": "这篇论文的创新点在于将物理知识（特别是哈密顿原理）整合到对象中心推理模型中，这有助于模型更好地理解和预测复杂的物理动态，尤其是流体和物体交互。同时，通过创建新的真实世界数据集并在此上验证模型，强调了其在多样化场景下的鲁棒性和适应性，为构建更接近人类认知的世界模型提供了有益探索。"}}
{"id": "2506.10918", "title": "Sequential-Parallel Duality in Prefix Scannable Models", "authors": ["Morris Yau", "Sharut Gupta", "Valerie Engelmayer", "Kazuki Irie", "Stefanie Jegelka", "Jacob Andreas"], "summary": "Modern neural sequence models are designed to meet the dual mandate of\nparallelizable training and fast sequential inference. Recent developments have\ngiven rise to various models, such as Gated Linear Attention (GLA) and Mamba,\nthat achieve such ``sequential-parallel duality.'' This raises a natural\nquestion: can we characterize the full class of neural sequence models that\nsupport near-constant-time parallel evaluation and linear-time, constant-space\nsequential inference? We begin by describing a broad class of such models --\nstate space models -- as those whose state updates can be computed using the\nclassic parallel prefix scan algorithm with a custom associative aggregation\noperator. We then define a more general class, Prefix-Scannable Models (PSMs),\nby relaxing the state aggregation operator to allow arbitrary (potentially\nnon-associative) functions such as softmax attention. This generalization\nunifies many existing architectures, including element-wise RNNs (e.g., Mamba)\nand linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new\nmodels with softmax-like operators that achieve O(1) amortized compute per\ntoken and log(N) memory for sequence length N. We empirically evaluate such\nmodels on illustrative small-scale language modeling and canonical synthetic\ntasks, including state tracking and associative recall. Empirically, we find\nthat PSMs retain the expressivity of transformer-based architectures while\nmatching the inference efficiency of state space models -- in some cases\nexhibiting better length generalization than either.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10918v1", "AI": {"title_translation": "前缀可扫描模型中的序列-并行对偶性", "tldr": "本文引入了前缀可扫描模型 (PSM) 的概念，该模型统一了许多现有序列架构，并实现了并行训练和高效序列推理的“序列-并行对偶性”。PSM 在保持表达能力的同时，实现了与状态空间模型相当的推理效率，甚至在某些情况下表现出更好的长度泛化能力。", "motivation": "现代神经网络序列模型旨在实现可并行训练和快速序列推理的双重目标。最近的发展涌现出一些实现了这种“序列-并行对偶性”的模型，如门控线性注意力 (GLA) 和 Mamba。这引出了一个自然的问题：我们能否表征支持近乎常数时间并行评估和线性时间、常数空间序列推理的神经网络序列模型的完整类别？", "method": "本文首先将一类广泛的模型——状态空间模型——描述为那些其状态更新可以使用经典并行前缀扫描算法和自定义结合聚合运算符计算的模型。然后，通过放宽状态聚合运算符以允许任意（可能非结合的）函数（例如 softmax 注意力），定义了一个更通用的类别——前缀可扫描模型（PSMs）。这种泛化统一了许多现有架构，包括逐元素RNN（例如 Mamba）和线性Transformer（例如 GLA, Mamba2, mLSTM），同时也引入了具有类 softmax 运算符的新模型，这些模型实现了每个 token O(1) 的摊销计算和序列长度 N 的 log(N) 内存。", "result": "经验评估表明，PSMs 在说明性的小规模语言建模和规范的合成任务（包括状态跟踪和关联回忆）上表现良好。经验上，我们发现 PSMs 保留了基于 Transformer 架构的表达能力，同时匹配了状态空间模型的推理效率——在某些情况下甚至比两者都表现出更好的长度泛化能力。", "conclusion": "前缀可扫描模型 (PSM) 统一了多种现有序列模型，并提供了一种表征具有序列-并行对偶性的神经网络序列模型的方法。它们在保持表达能力的同时，实现了高效的并行训练和序列推理，并在某些情况下表现出优于现有模型的长度泛化能力。", "translation": "现代神经网络序列模型旨在满足可并行训练和快速序列推理的双重需求。最近的发展催生了各种模型，如门控线性注意力 (GLA) 和 Mamba，它们实现了这种“序列-并行对偶性”。这提出了一个自然的问题：我们能否表征支持近乎常数时间并行评估和线性时间、常数空间序列推理的神经网络序列模型的完整类别？我们首先将一类广泛的模型——状态空间模型——描述为那些其状态更新可以使用经典并行前缀扫描算法和自定义结合聚合运算符计算的模型。然后，通过放宽状态聚合运算符以允许任意（可能非结合的）函数（例如 softmax 注意力），定义了一个更通用的类别——前缀可扫描模型 (PSMs)。这种泛化统一了许多现有架构，包括逐元素RNN（例如 Mamba）和线性Transformer（例如 GLA, Mamba2, mLSTM），同时也引入了具有类 softmax 运算符的新模型，这些模型实现了每个 token O(1) 的摊销计算和序列长度 N 的 log(N) 内存。我们对这些模型在说明性的小规模语言建模和规范的合成任务（包括状态跟踪和关联回忆）上进行了经验评估。经验上，我们发现 PSMs 保留了基于 Transformer 架构的表达能力，同时匹配了状态空间模型的推理效率——在某些情况下甚至比两者都表现出更好的长度泛化能力。", "summary": "本文介绍了前缀可扫描模型 (PSM)，这是一个通用类别的神经网络序列模型，旨在实现并行训练和高效序列推理的“序列-并行对偶性”。PSM 通过放宽状态空间模型中的结合聚合运算符，允许任意函数（如 softmax 注意力），从而统一了多种现有架构，包括 Mamba 和线性 Transformer。实验结果表明，PSM 在保持 Transformer 架构表达能力的同时，实现了与状态空间模型相当的推理效率，并且在某些任务上表现出更好的长度泛化能力。", "keywords": "前缀可扫描模型, 序列-并行对偶性, 神经网络序列模型, 状态空间模型, 线性Transformer", "comments": "本文提出了前缀可扫描模型（PSM）的通用框架，这是一个重要的创新，因为它提供了一种统一现有高效序列模型（如Mamba和GLA）的理论基础。通过将这些模型置于一个共同的“前缀可扫描”框架下，作者不仅澄清了它们共享的计算特性，还为设计新的高效架构提供了指导。PSM能够保持Transformer的表达能力同时实现状态空间模型的推理效率，这对于未来大规模序列模型的开发具有重要意义。特别是其在长度泛化方面的改进潜力，解决了现有模型的一个关键限制。"}}
{"id": "2506.10790", "title": "Human-Robot Navigation using Event-based Cameras and Reinforcement Learning", "authors": ["Ignacio Bugueno-Cordova", "Javier Ruiz-del-Solar", "Rodrigo Verschae"], "summary": "This work introduces a robot navigation controller that combines event\ncameras and other sensors with reinforcement learning to enable real-time\nhuman-centered navigation and obstacle avoidance. Unlike conventional\nimage-based controllers, which operate at fixed rates and suffer from motion\nblur and latency, this approach leverages the asynchronous nature of event\ncameras to process visual information over flexible time intervals, enabling\nadaptive inference and control. The framework integrates event-based\nperception, additional range sensing, and policy optimization via Deep\nDeterministic Policy Gradient, with an initial imitation learning phase to\nimprove sample efficiency. Promising results are achieved in simulated\nenvironments, demonstrating robust navigation, pedestrian following, and\nobstacle avoidance. A demo video is available at the project website.", "comment": "https://ibugueno.github.io/hr-navigation-using-event-cameras-and-rl/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10790v1", "AI": {"title_translation": "基于事件相机的强化学习人机导航", "tldr": "本文提出了一种结合事件相机和强化学习的机器人导航控制器，实现了实时以人为中心的导航和避障。", "motivation": "传统的基于图像的控制器存在固定帧率、运动模糊和延迟等问题。本文旨在利用事件相机的异步特性来处理视觉信息，实现自适应推理和控制，从而克服这些限制，实现以人为中心的实时导航和避障。", "method": "该方法结合了事件相机和其他传感器与强化学习，形成一个机器人导航控制器。具体而言，它整合了基于事件的感知、额外的测距传感，并通过深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）进行策略优化，同时引入了初始的模仿学习阶段以提高样本效率。", "result": "在模拟环境中取得了可喜的成果，展示了鲁棒的导航、行人跟随和避障能力。", "conclusion": "该研究成功地将事件相机与强化学习相结合，开发出一种能够进行实时、自适应以人为中心导航和避障的机器人控制器，并在模拟环境中验证了其有效性。", "translation": "这项工作引入了一种机器人导航控制器，它结合了事件相机和其他传感器与强化学习，以实现实时以人为中心的导航和避障。与传统的基于图像的控制器不同，后者以固定速率运行并受运动模糊和延迟的影响，这种方法利用事件相机的异步特性，在灵活的时间间隔内处理视觉信息，从而实现自适应推理和控制。该框架整合了基于事件的感知、额外的测距传感，并通过深度确定性策略梯度进行策略优化，并辅以初始的模仿学习阶段以提高样本效率。在模拟环境中取得了可喜的成果，展示了鲁棒的导航、行人跟随和避障能力。项目网站上提供了演示视频。", "summary": "本文提出了一种新颖的机器人导航控制器，该控制器将事件相机和其他传感器与强化学习相结合，以实现实时的以人为中心的导航和避障。与传统的基于图像的系统相比，该方法利用事件相机的异步特性，克服了运动模糊和延迟的限制，实现了自适应的视觉信息处理和控制。该框架集成了基于事件的感知、测距传感和通过DDPG进行的策略优化，并通过模仿学习提高了样本效率。在模拟环境中，该控制器展示了强大的导航、行人跟随和避障性能。", "keywords": "事件相机, 强化学习, 机器人导航, 避障, 人机交互", "comments": "该论文的创新点在于将事件相机（其异步特性有助于克服传统相机的运动模糊和延迟问题）与强化学习相结合，应用于机器人导航。通过结合模仿学习，提高了样本效率，这是强化学习应用中的一个重要考量。然而，目前的结果仅在模拟环境中验证，未来需要进行真实世界的实验来进一步验证其有效性和鲁棒性。"}}
{"id": "2506.10922", "title": "Robustly Improving LLM Fairness in Realistic Settings via Interpretability", "authors": ["Adam Karvonen", "Samuel Marks"], "summary": "Large language models (LLMs) are increasingly deployed in high-stakes hiring\napplications, making decisions that directly impact people's careers and\nlivelihoods. While prior studies suggest simple anti-bias prompts can eliminate\ndemographic biases in controlled evaluations, we find these mitigations fail\nwhen realistic contextual details are introduced. We address these failures\nthrough internal bias mitigation: by identifying and neutralizing sensitive\nattribute directions within model activations, we achieve robust bias reduction\nacross all tested scenarios. Across leading commercial (GPT-4o, Claude 4\nSonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,\nMistral-24B), we find that adding realistic context such as company names,\nculture descriptions from public careers pages, and selective hiring\nconstraints (e.g.,``only accept candidates in the top 10\\%\") induces\nsignificant racial and gender biases (up to 12\\% differences in interview\nrates). When these biases emerge, they consistently favor Black over White\ncandidates and female over male candidates across all tested models and\nscenarios. Moreover, models can infer demographics and become biased from\nsubtle cues like college affiliations, with these biases remaining invisible\neven when inspecting the model's chain-of-thought reasoning. To address these\nlimitations, our internal bias mitigation identifies race and gender-correlated\ndirections and applies affine concept editing at inference time. Despite using\ndirections from a simple synthetic dataset, the intervention generalizes\nrobustly, consistently reducing bias to very low levels (typically under 1\\%,\nalways below 2.5\\%) while largely maintaining model performance. Our findings\nsuggest that practitioners deploying LLMs for hiring should adopt more\nrealistic evaluation methodologies and consider internal mitigation strategies\nfor equitable outcomes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10922v1", "AI": {"title_translation": "通过可解释性在现实场景中稳健地提升大型语言模型（LLM）的公平性", "tldr": "在现实招聘场景中，简单的LLM偏见缓解方法失效，但通过可解释性实现的内部偏见缓解策略能够稳健地减少偏见并保持模型性能。", "motivation": "大型语言模型（LLM）在高风险招聘应用中的部署日益增多，其决策直接影响个人职业和生计。尽管先前的研究表明简单的反偏见提示在受控评估中能消除人口统计学偏见，但本研究发现当引入现实上下文细节时，这些缓解措施会失效。", "method": "本研究通过内部偏见缓解来解决问题。具体方法是：识别并中和模型激活中的敏感属性方向，并在推理时应用仿射概念编辑。尽管这些方向来自一个简单的合成数据集，但该干预措施表现出强大的泛化能力。", "result": "在引入公司名称、文化描述和招聘限制等现实上下文后，先前的偏见缓解措施失效，并会引入显著的种族和性别偏见（面试率差异高达12%）。这些偏见在所有测试模型和场景中一致偏向黑人而非白人候选人，以及女性而非男性候选人。模型甚至可以从大学隶属关系等细微线索中推断出人口统计信息并产生偏见，且这些偏见在思维链推理中也无法察觉。然而，本研究提出的内部偏见缓解方法能够将偏见稳健地降低到非常低的水平（通常低于1%，始终低于2.5%），同时基本保持了模型性能。", "conclusion": "部署LLM进行招聘的从业者应采用更现实的评估方法，并考虑内部缓解策略以实现公平结果。", "translation": "大型语言模型（LLM）越来越多地部署在高风险的招聘应用中，其决策直接影响人们的职业生涯和生计。虽然先前的研究表明简单的反偏见提示可以在受控评估中消除人口统计学偏见，但我们发现当引入现实的上下文细节时，这些缓解措施会失效。我们通过内部偏见缓解来解决这些失败：通过识别和中和模型激活中的敏感属性方向，我们在所有测试场景中实现了稳健的偏见减少。在领先的商业模型（GPT-4o、Claude 4 Sonnet、Gemini 2.5 Flash）和开源模型（Gemma-2 27B、Gemma-3、Mistral-24B）中，我们发现添加公司名称、来自公开招聘页面的文化描述以及选择性招聘限制（例如“只接受前10%的候选人”）等现实上下文会引入显著的种族和性别偏见（面试率差异高达12%）。当这些偏见出现时，在所有测试模型和场景中，它们始终偏向黑人而非白人候选人，以及女性而非男性候选人。此外，模型可以从大学隶属关系等细微线索中推断出人口统计信息并变得有偏见，即使检查模型的思维链推理，这些偏见仍然是不可见的。为了解决这些限制，我们的内部偏见缓解方法识别出与种族和性别相关的方向，并在推理时应用仿射概念编辑。尽管使用了来自简单合成数据集的方向，但这种干预措施具有强大的泛化能力，始终将偏见降低到非常低的水平（通常低于1%，始终低于2.5%），同时基本保持了模型性能。我们的研究结果表明，部署LLM进行招聘的从业者应采用更现实的评估方法，并考虑内部缓解策略以实现公平结果。", "summary": "本文研究了大型语言模型（LLM）在现实招聘场景中的公平性问题。研究发现，尽管简单的反偏见提示在受控环境下有效，但在引入公司名称、文化描述和招聘限制等现实背景时，这些缓解措施会失效，导致显著的种族和性别偏见。为解决此问题，作者提出了一种内部偏见缓解方法，通过识别并中和模型激活中的敏感属性方向（即使是从合成数据集中提取的方向），在所有测试场景中实现了稳健的偏见减少（通常降至1%以下）。该方法在保持模型性能的同时，有效解决了LLM在现实应用中可能因细微线索推断人口统计信息而产生的隐形偏见。研究强调了在LLM招聘应用中采用更现实的评估方法和内部缓解策略的重要性。", "keywords": "LLM公平性, 偏见缓解, 内部干预, 可解释性, 招聘应用", "comments": "本文的创新点在于提出了内部偏见缓解策略，通过直接干预模型激活层面的敏感属性方向，而非仅仅依赖外部提示，从而在更复杂的现实场景中实现了鲁棒的偏见消除。这对于LLM在如招聘等高风险应用中的公平性至关重要，揭示了传统反偏见方法的局限性并提供了一种更深层次的解决方案。研究还强调了在现实场景中评估LLM公平性的重要性。"}}
{"id": "2506.10807", "title": "Prompts to Summaries: Zero-Shot Language-Guided Video Summarization", "authors": ["Mario Barbara", "Alaa Maalouf"], "summary": "The explosive growth of video data intensified the need for flexible\nuser-controllable summarization tools that can operate without domain-specific\ntraining data. Existing methods either rely on datasets, limiting\ngeneralization, or cannot incorporate user intent expressed in natural\nlanguage. We introduce Prompts-to-Summaries: the first zero-shot,\ntext-queryable video summarizer that converts off-the-shelf video-language\nmodels (VidLMs) captions into user-guided skims via large language models\n(LLMs) judging, without the use of training data at all, beating all\nunsupervised and matching supervised methods. Our pipeline (i) segments raw\nvideo footage into coherent scenes, (ii) generates rich scene-level\ndescriptions through a memory-efficient, batch-style VidLM prompting scheme\nthat scales to hours-long videos on a single GPU, (iii) leverages an LLM as a\njudge to assign scene-level importance scores under a carefully crafted prompt,\nand finally, (iv) propagates those scores to short segments level via two new\nmetrics: consistency (temporal coherency) and uniqueness (novelty), yielding\nfine-grained frame importance. On SumMe and TVSum, our data-free approach\nsurpasses all prior data-hungry unsupervised methods. It also performs\ncompetitively on the Query-Focused Video Summarization (QFVS) benchmark,\ndespite using no training data and the competing methods requiring supervised\nframe-level importance. To spur further research, we release VidSum-Reason, a\nnew query-driven dataset featuring long-tailed concepts and multi-step\nreasoning; our framework attains robust F1 scores and serves as the first\nchallenging baseline. Overall, our results demonstrate that pretrained\nmultimodal models, when orchestrated with principled prompting and score\npropagation, already provide a powerful foundation for universal,\ntext-queryable video summarization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10807v1", "AI": {"title_translation": "提示到摘要：零样本语言引导的视频摘要", "tldr": "本文提出了一种名为 Prompts-to-Summaries 的零样本、文本可查询视频摘要器，它利用现成的视频语言模型和大型语言模型，无需训练数据即可生成用户引导的视频摘要，性能超越所有无监督方法并与有监督方法相媲美。", "motivation": "视频数据的爆炸式增长使得对灵活、用户可控且无需领域特定训练数据的摘要工具的需求日益增加。现有方法要么依赖数据集，限制了泛化能力，要么无法整合自然语言表达的用户意图。", "method": "本文提出 Prompts-to-Summaries，一个零样本、文本可查询的视频摘要器。其管道包括：(i) 将原始视频素材分割成连贯的场景；(ii) 通过内存高效的批处理式 VidLM 提示方案生成丰富的场景级描述；(iii) 利用大型语言模型（LLM）作为判断器，在精心设计的提示下分配场景级重要性分数；(iv) 通过一致性（时间连贯性）和独特性（新颖性）两个新指标将分数传播到短片段级别，从而获得细粒度的帧重要性。", "result": "在 SumMe 和 TVSum 数据集上，该无数据方法超越了所有先前依赖数据的无监督方法。在 Query-Focused Video Summarization (QFVS) 基准测试中，尽管未使用训练数据且竞争方法需要有监督的帧级重要性，但其表现仍具竞争力。此外，作者发布了一个新的查询驱动数据集 VidSum-Reason，并在此数据集上获得了稳健的 F1 分数。", "conclusion": "研究结果表明，预训练的多模态模型，当与原则性提示和分数传播相结合时，已经为通用、文本可查询的视频摘要提供了强大的基础。", "translation": "视频数据的爆炸式增长加剧了对灵活、用户可控且无需领域特定训练数据的摘要工具的需求。现有方法要么依赖数据集，限制了泛化能力，要么无法整合自然语言表达的用户意图。我们引入了 Prompts-to-Summaries：第一个零样本、文本可查询的视频摘要器，它通过大型语言模型（LLM）的判断，将现成的视频语言模型（VidLM）的字幕转换为用户引导的摘要，完全无需使用训练数据，其性能超越所有无监督方法并与有监督方法相媲美。我们的管道包括：(i) 将原始视频素材分割成连贯的场景；(ii) 通过内存高效的批处理式 VidLM 提示方案生成丰富的场景级描述，该方案可在单个 GPU 上扩展到数小时长的视频；(iii) 利用 LLM 作为判断器，在精心设计的提示下分配场景级重要性分数；最后，(iv) 通过两个新指标：一致性（时间连贯性）和独特性（新颖性），将这些分数传播到短片段级别，从而获得细粒度的帧重要性。在 SumMe 和 TVSum 数据集上，我们的无数据方法超越了所有先前依赖数据的无监督方法。尽管未使用训练数据且竞争方法需要有监督的帧级重要性，但它在查询聚焦视频摘要（QFVS）基准测试中也表现出色。为了促进进一步研究，我们发布了 VidSum-Reason，一个包含长尾概念和多步推理的新型查询驱动数据集；我们的框架获得了稳健的 F1 分数，并作为第一个具有挑战性的基线。总的来说，我们的结果表明，预训练的多模态模型，当与原则性提示和分数传播相结合时，已经为通用、文本可查询的视频摘要提供了强大的基础。", "summary": "本文提出了一种名为 Prompts-to-Summaries 的创新方法，旨在实现零样本、语言引导的视频摘要。该方法利用现成的视频语言模型（VidLMs）生成场景描述，并结合大型语言模型（LLMs）作为判断器来评估场景重要性，最终通过新颖的分数传播机制生成细粒度的帧重要性。该框架无需任何训练数据，在多个标准视频摘要数据集上，其性能超越了所有无监督方法，并与有监督方法表现相当。此外，论文还发布了一个新的查询驱动数据集 VidSum-Reason，以推动该领域的进一步研究。", "keywords": "零样本, 视频摘要, 语言模型, 多模态, 提示工程", "comments": "该论文的创新点在于提出了首个零样本、文本可查询的视频摘要器，彻底摆脱了对训练数据的依赖，解决了现有方法泛化性差和无法整合用户意图的痛点。其核心思想是巧妙地结合了预训练的视频语言模型和大型语言模型的能力，通过精心设计的提示和分数传播机制，实现了高效且高质量的视频摘要。这一方法为视频摘要领域提供了一个全新的范式，证明了预训练多模态模型在无需微调的情况下处理复杂任务的巨大潜力，为未来的研究开辟了新方向。"}}
{"id": "2506.10930", "title": "Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction", "authors": ["Thanathai Lertpetchpun", "Tiantian Feng", "Dani Byrd", "Shrikanth Narayanan"], "summary": "Speech emotion recognition (SER) in naturalistic conditions presents a\nsignificant challenge for the speech processing community. Challenges include\ndisagreement in labeling among annotators and imbalanced data distributions.\nThis paper presents a reproducible framework that achieves superior (top 1)\nperformance in the Emotion Recognition in Naturalistic Conditions Challenge\n(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system\nis designed to tackle the aforementioned challenges through multimodal\nlearning, multi-task learning, and imbalanced data handling. Specifically, our\nbest system is trained by adding text embeddings, predicting gender, and\nincluding ``Other'' (O) and ``No Agreement'' (X) samples in the training set.\nOur system's results secured both first and second places in the IS25-SER\nChallenge, and the top performance was achieved by a simple two-system\nensemble.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10930v1", "AI": {"title_translation": "开发一个用于自然条件下语音情感识别的高性能框架——情感属性预测的挑战", "tldr": "本文提出一个高性能框架，通过多模态、多任务学习和不平衡数据处理，在自然条件下的语音情感识别挑战赛（IS25-SER）中获得第一名。", "motivation": "在自然条件下进行语音情感识别（SER）对语音处理领域提出了重大挑战，主要问题包括标注者之间标签不一致和数据分布不平衡。", "method": "本文通过多模态学习、多任务学习和不平衡数据处理来解决挑战。具体方法包括添加文本嵌入、预测性别以及在训练集中包含“Other”（O）和“No Agreement”（X）样本。最佳性能通过一个简单的两系统集成实现。", "result": "该系统在IS25-SER挑战赛中获得了第一名和第二名，其中最佳性能通过一个简单的两系统集成实现，并在MSP-Podcast数据集上进行了评估。", "conclusion": "所提出的框架在自然条件下的语音情感识别方面表现出色，有效解决了标注不一致和数据不平衡等挑战。", "translation": "自然条件下的语音情感识别（SER）对语音处理领域提出了重大挑战。挑战包括标注者之间的标签分歧和不平衡的数据分布。本文提出了一个可复现的框架，在自然条件下情感识别挑战赛（IS25-SER Challenge）——任务2中取得了卓越（前1名）的性能，并在MSP-Podcast数据集上进行了评估。我们的系统旨在通过多模态学习、多任务学习和不平衡数据处理来应对上述挑战。具体而言，我们最好的系统通过添加文本嵌入、预测性别以及在训练集中包含“Other”（O）和“No Agreement”（X）样本进行训练。我们的系统结果在IS25-SER挑战赛中获得了第一名和第二名，最佳性能通过一个简单的两系统集成实现。", "summary": "本文针对自然条件下语音情感识别（SER）面临的标签不一致和数据不平衡等挑战，提出了一种高性能、可复现的SER框架。该框架结合了多模态学习、多任务学习和不平衡数据处理技术，并通过添加文本嵌入、预测性别以及利用“Other”和“No Agreement”样本进行训练优化。实验结果表明，该系统在IS25-SER挑战赛中表现卓越，获得了第一名和第二名，最佳性能通过简单的两系统集成实现。", "keywords": "语音情感识别, 多模态学习, 多任务学习, 不平衡数据, IS25-SER", "comments": "该论文提出了一种有效的语音情感识别框架，创新性地结合了多模态和多任务学习，并解决了自然条件下SER常见的标注不一致和数据不平衡问题。其在IS25-SER挑战赛中取得的优异成绩证明了方法的有效性，尤其是通过集成学习获得最佳性能，显示了其在实际应用中的潜力。"}}
{"id": "2506.10943", "title": "Self-Adapting Language Models", "authors": ["Adam Zweiger", "Jyothish Pari", "Han Guo", "Ekin Akyürek", "Yoon Kim", "Pulkit Agrawal"], "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10943v1", "AI": {"title_translation": "自适应语言模型", "tldr": "SEAL是一个使大型语言模型能够通过生成微调数据和更新指令来自适应的框架，通过监督微调实现持久更新，并通过强化学习进行训练。", "motivation": "大型语言模型（LLMs）功能强大但静态，缺乏根据新任务、知识或示例调整其权重的机制。", "method": "SEAL框架通过让LLMs生成自己的微调数据和更新指令来实现自适应。模型对新输入生成“自编辑”，这可能包括信息重组、指定优化超参数或调用工具进行数据增强和基于梯度的更新。通过监督微调（SFT），这些自编辑导致持久的权重更新。为了训练模型生成有效的自编辑，使用强化学习循环，以下游模型的性能作为奖励信号。与以往方法不同，SEAL直接使用模型自身的生成来控制其适应过程。", "result": "在知识整合和少样本泛化方面的实验表明，SEAL是迈向能够自我导向适应的语言模型的一个有前景的步骤。", "conclusion": "SEAL框架通过使LLMs能够生成和利用自编辑来实现自适应，并在知识整合和少样本泛化方面显示出潜力，是实现自我导向适应语言模型的重要一步。", "translation": "大型语言模型（LLMs）功能强大但静态；它们缺乏根据新任务、知识或示例调整其权重的机制。我们引入了自适应LLMs（SEAL），这是一个使LLMs能够通过生成自己的微调数据和更新指令来自适应的框架。给定一个新的输入，模型会产生一个自编辑——一个可能以不同方式重组信息、指定优化超参数或调用工具进行数据增强和基于梯度的更新的生成。通过监督微调（SFT），这些自编辑导致持久的权重更新，从而实现持久适应。为了训练模型产生有效的自编辑，我们使用强化学习循环，以下游更新模型的性能作为奖励信号。与以往依赖单独适应模块或辅助网络的方法不同，SEAL直接使用模型自身的生成来控制其适应过程。在知识整合和少样本泛化方面的实验表明，SEAL是迈向能够自我导向适应的语言模型的一个有前景的步骤。我们的网站和代码可在https://jyopari.github.io/posts/seal获取。", "summary": "本文提出了自适应语言模型（SEAL）框架，旨在解决大型语言模型（LLMs）静态、无法适应新信息的问题。SEAL允许LLMs通过生成“自编辑”（包括微调数据和更新指令）来动态调整自身权重。这些自编辑通过监督微调（SFT）实现持久更新，并通过强化学习（RL）进行训练，以优化下游任务性能。与现有方法不同，SEAL直接利用模型自身生成来控制适应过程。实验证明，SEAL在知识整合和少样本泛化方面表现出良好前景，是实现自我导向适应LLMs的重要进展。", "keywords": "自适应语言模型, LLM, 微调, 强化学习, 自编辑", "comments": "SEAL的创新之处在于其“自编辑”机制，允许LLMs直接生成并利用微调数据和更新指令，实现自我适应，而无需外部模块。这代表了LLMs发展的一个重要方向，即从静态模型转向更具动态性和自主学习能力的模型。其结合SFT和RL的训练范式也值得关注。"}}
{"id": "2506.10584", "title": "Encoding call-by-push-value in the pi-calculus", "authors": ["Benjamin Bennetzen", "Nikolaj Rossander Kristensen", "Peter Buus Steffensen"], "summary": "In this report we define an encoding of Levys call-by-push-value\nlambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both\nsound and complete. We present informal (by-hand) proofs of soundness,\ncompleteness, and all required lemmas. The encoding is specialized to the\ninternal pi-calculus (pi-i-calculus) to circumvent certain challenges\nassociated with using de Bruijn index in a formalization, and it also helps\nwith bisimulation as early-, late- and open-bisimulation coincide in this\nsetting, furthermore bisimulation is a congruence. Additionally, we argue that\nour encoding also satisfies the five criteria for good encodings proposed by\nGorla, as well as show similarities between Milners and our encoding. This\npaper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic\npi-calculus and the local pi-calculus. We begin a formalization of the proof in\nCoq for the soundness and completeness of the encoding in the pi-i-calculus.\nNot all lemmas used in the formalization are themselves formally proven.\nHowever, we argue that the non-proven lemmas are reasonable, as they are proven\nby hand, or amount to Coq formalities that are straightforward given informal\narguments.", "comment": "56 pages", "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.10584v1", "AI": {"title_translation": "在 Pi 演算中编码按值推送调用", "tldr": "本文定义了 Levy 的按值推送 λ 演算 (CBPV) 在 Pi 演算中的编码，并证明了其完备性和可靠性，同时讨论了与 Gorla 准则的符合性以及与 Milner 编码的相似性。", "motivation": "定义 Levy 的按值推送 λ 演算 (CBPV) 在 Pi 演算中的编码，并证明其可靠性和完备性。", "method": "在 Pi 演算中定义了 CBPV 的编码，并提供了可靠性、完备性及所有必需引理的非正式（手动）证明。编码专门针对内部 Pi 演算（pi-i-calculus）以规避 De Bruijn 索引的挑战，并有助于双模拟。此外，还包括异步多重 Pi 演算和局部 Pi 演算中的编码。部分证明在 Coq 中进行了形式化。", "result": "编码被证明是可靠且完备的。编码满足 Gorla 提出的良好编码的五项标准，并显示出与 Milner 编码的相似性。", "conclusion": "本文成功地在 Pi 演算中编码了 CBPV，并验证了其关键属性。尽管部分引理未完全形式化证明，但其合理性得到了论证，且正在进行 Coq 中的形式化工作。", "translation": "本报告定义了 Levy 的按值推送 λ 演算 (CBPV) 在 Pi 演算中的编码，并证明我们的编码既可靠又完备。我们提供了可靠性、完备性以及所有必需引理的非正式（手动）证明。该编码专门针对内部 Pi 演算（pi-i-calculus），以规避在使用 De Bruijn 索引进行形式化时遇到的某些挑战，并且它还有助于双模拟，因为在这种设置下，早期、晚期和开放双模拟是重合的，此外双模拟是一种同余。此外，我们认为我们的编码也满足 Gorla 提出的良好编码的五项标准，并展示了 Milner 编码与我们编码之间的相似性。本文包括从 CBPV 到 pi-i-calculus、异步多重 Pi 演算和局部 Pi 演算的编码。我们开始在 Coq 中对 pi-i-calculus 中编码的可靠性和完备性证明进行形式化。并非所有在形式化中使用的引理本身都经过了形式化证明。然而，我们认为未证明的引理是合理的，因为它们是手动证明的，或者鉴于非正式论证，它们仅仅是 Coq 的形式化细节。", "summary": "本文定义了 Levy 的按值推送 λ 演算 (CBPV) 在 Pi 演算中的编码，并证明了其可靠性和完备性。研究专注于内部 Pi 演算以简化形式化和双模拟。作者提供了非正式证明，并论证了编码符合 Gorla 的良好编码标准，同时指出与 Milner 编码的相似之处。部分证明正在 Coq 中进行形式化。", "keywords": "按值推送调用, Pi 演算, 编码, 可靠性, 完备性", "comments": "该论文在理论计算机科学领域，尤其是在进程演算和 λ 演算的交叉点上，具有重要意义。通过在 Pi 演算中编码 CBPV 并严格证明其可靠性和完备性，为理解不同计算模型之间的关系提供了深刻见解。值得注意的是，虽然部分证明是手动的或非形式化的，但作者明确指出了这一点，并正在进行 Coq 中的形式化工作，这表明了对严谨性的追求。满足 Gorla 的良好编码标准进一步增强了这项工作的价值。"}}
{"id": "2506.10816", "title": "Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders", "authors": ["Hui Yang", "Wei Sun", "Jian Liu", "Jin Zheng", "Jian Xiao", "Ajmal Mian"], "summary": "Hand-object pose estimation from monocular RGB images remains a significant\nchallenge mainly due to the severe occlusions inherent in hand-object\ninteractions. Existing methods do not sufficiently explore global structural\nperception and reasoning, which limits their effectiveness in handling occluded\nhand-object interactions. To address this challenge, we propose an\nocclusion-aware hand-object pose estimation method based on masked\nautoencoders, termed as HOMAE. Specifically, we propose a target-focused\nmasking strategy that imposes structured occlusion on regions of hand-object\ninteraction, encouraging the model to learn context-aware features and reason\nabout the occluded structures. We further integrate multi-scale features\nextracted from the decoder to predict a signed distance field (SDF), capturing\nboth global context and fine-grained geometry. To enhance geometric perception,\nwe combine the implicit SDF with an explicit point cloud derived from the SDF,\nleveraging the complementary strengths of both representations. This fusion\nenables more robust handling of occluded regions by combining the global\ncontext from the SDF with the precise local geometry provided by the point\ncloud. Extensive experiments on challenging DexYCB and HO3Dv2 benchmarks\ndemonstrate that HOMAE achieves state-of-the-art performance in hand-object\npose estimation. We will release our code and model.", "comment": "10 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10816v1", "AI": {"title_translation": "基于掩码自编码器的遮挡感知三维手物姿态估计", "tldr": "本文提出HOMAE，一种基于掩码自编码器的遮挡感知方法，用于解决手物交互中严重遮挡导致的三维手物姿态估计难题，通过目标聚焦掩码策略、SDF与点云融合，实现了最先进的性能。", "motivation": "单目RGB图像中的手物姿态估计因手物交互中严重的遮挡而面临巨大挑战。现有方法未能充分探索全局结构感知和推理，限制了其在处理遮挡手物交互时的有效性。", "method": "本文提出HOMAE，一种基于掩码自编码器的遮挡感知手物姿态估计方法。具体包括：1) 目标聚焦掩码策略，对交互区域施加结构化遮挡，促使模型学习上下文感知特征并推断遮挡结构。2) 整合解码器提取的多尺度特征以预测有符号距离场（SDF），捕获全局上下文和细粒度几何。3) 将隐式SDF与显式点云结合，利用两者互补优势，增强对遮挡区域的处理能力。", "result": "在DexYCB和HO3Dv2等挑战性基准测试中，HOMAE实现了手物姿态估计的最先进性能。", "conclusion": "HOMAE通过其创新的遮挡感知机制和多模态几何表示融合，有效解决了手物交互中的严重遮挡问题，并在三维手物姿态估计中取得了领先成果。", "translation": "从单目RGB图像中估计手物姿态仍然是一个重大挑战，这主要是由于手物交互中固有的严重遮挡。现有方法未能充分探索全局结构感知和推理，这限制了它们在处理遮挡手物交互时的有效性。为了解决这一挑战，我们提出了一种基于掩码自编码器的遮挡感知手物姿态估计方法，命名为HOMAE。具体来说，我们提出了一种目标聚焦掩码策略，该策略对手物交互区域施加结构化遮挡，鼓励模型学习上下文感知特征并推断遮挡结构。我们进一步整合从解码器中提取的多尺度特征来预测有符号距离场（SDF），捕获全局上下文和细粒度几何。为了增强几何感知，我们将隐式SDF与从SDF导出的显式点云相结合，利用两种表示的互补优势。这种融合通过将SDF的全局上下文与点云提供的精确局部几何相结合，从而实现对遮挡区域更鲁棒的处理。在具有挑战性的DexYCB和HO3Dv2基准测试中进行的广泛实验表明，HOMAE在手物姿态估计方面取得了最先进的性能。我们将发布我们的代码和模型。", "summary": "本文提出HOMAE，一种基于掩码自编码器的遮挡感知方法，用于解决单目RGB图像中手物交互的严重遮挡问题。HOMAE引入目标聚焦掩码策略，鼓励模型学习上下文感知特征和推断遮挡结构。它还通过整合多尺度特征预测有符号距离场（SDF），并将其与显式点云融合，以捕捉全局上下文和精确局部几何。在DexYCB和HO3Dv2数据集上的广泛实验表明，HOMAE在手物姿态估计方面达到了最先进的性能。", "keywords": "手物姿态估计, 遮挡感知, 掩码自编码器, 有符号距离场, 点云", "comments": "该论文的关键创新在于引入了目标聚焦掩码策略，这是一种新颖的自监督学习机制，专门用于处理手物交互中的复杂遮挡。通过结合隐式SDF和显式点云，该方法有效地融合了全局上下文和局部几何细节，显著提升了在严重遮挡情况下的鲁棒性。其在SOTA性能上的表现证明了该方法的有效性和重要性。"}}
{"id": "2506.10946", "title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models", "authors": ["Evelyn Ma", "Duo Zhou", "Peizhi Niu", "Huiting Zhou", "Huan Zhang", "Olgica Milenkovic", "S. Rasoul Etesami"], "summary": "Unlearning in large language models (LLMs) is becoming increasingly important\ndue to regulatory compliance, copyright protection, and privacy concerns.\nHowever, a key challenge in LLM unlearning is unintended forgetting, where the\nremoval of specific data inadvertently impairs the utility of the model and its\nretention of valuable, desired information. While prior work has primarily\nfocused on architectural innovations, the influence of data-level factors on\nunlearning performance remains underexplored. As a result, existing methods\noften suffer from degraded retention when forgetting high-impact data. To\naddress this, we propose GUARD-a novel framework for Guided Unlearning And\nRetention via Data attribution. At its core, GUARD introduces a lightweight\nproxy data attribution metric tailored for LLM unlearning, which quantifies the\n\"alignment\" between the forget and retain sets while remaining computationally\nefficient. Building on this, we design a novel unlearning objective that\nassigns adaptive, nonuniform unlearning weights to samples, inversely\nproportional to their proxy attribution scores. Through such a reallocation of\nunlearning power, GUARD mitigates unintended losses in retention. We provide\nrigorous theoretical guarantees that GUARD significantly enhances retention\nwhile maintaining forgetting metrics comparable to prior methods. Extensive\nexperiments on the TOFU benchmark across multiple LLM architectures demonstrate\nthat GUARD substantially improves utility preservation while ensuring effective\nunlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to\n194.92% in terms of Truth Ratio when forgetting 10% of the training data.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10946v1", "AI": {"title_translation": "GUARD：基于数据归因的大语言模型引导式遗忘与保留", "tldr": "GUARD是一个新颖的框架，通过数据归因解决大语言模型中遗忘特定数据时意外损害模型效用和有用信息保留的问题，显著提高了模型效用保留。", "motivation": "由于法规遵从、版权保护和隐私问题，大语言模型（LLMs）中的遗忘变得日益重要。然而，LLM遗忘的一个关键挑战是意外遗忘，即特定数据的移除无意中损害了模型的效用及其对有价值信息的保留。现有方法主要关注架构创新，但对数据层面因素的探索不足，导致在遗忘高影响数据时经常出现保留性能下降的问题。", "method": "我们提出了GUARD——一个通过数据归因实现引导式遗忘和保留的新颖框架。GUARD的核心是引入了一个轻量级的代理数据归因度量，该度量专门为LLM遗忘定制，用于量化遗忘集和保留集之间的“对齐”，同时保持计算效率。在此基础上，我们设计了一个新的遗忘目标，该目标根据样本的代理归因分数，以反比关系为其分配自适应的非均匀遗忘权重。通过这种遗忘能力的重新分配，GUARD减轻了保留中的意外损失。", "result": "我们提供了严格的理论保证，证明GUARD显著增强了保留，同时保持了与现有方法相当的遗忘指标。在TOFU基准测试和多种LLM架构上的广泛实验表明，GUARD在确保有效遗忘的同时，大幅提高了效用保留。值得注意的是，当遗忘10%的训练数据时，GUARD在保留集上的效用牺牲（以真实性比率衡量）减少了高达194.92%。", "conclusion": "GUARD通过引入轻量级代理数据归因度量和自适应的非均匀遗忘权重，有效解决了大语言模型遗忘中意外遗忘的问题，显著提高了模型在遗忘特定数据时的效用保留，同时保持了有效的遗忘性能。", "translation": "大语言模型（LLMs）中的遗忘由于法规遵从、版权保护和隐私问题而变得日益重要。然而，LLM遗忘的一个关键挑战是意外遗忘，即特定数据的移除无意中损害了模型的效用及其对有价值、所需信息的保留。虽然现有工作主要关注架构创新，但数据层面因素对遗忘性能的影响仍未得到充分探索。因此，现有方法在遗忘高影响数据时经常导致保留性能下降。为了解决这个问题，我们提出了GUARD——一个通过数据归因实现引导式遗忘和保留的新颖框架。GUARD的核心是引入了一个轻量级的代理数据归因度量，该度量专门为LLM遗忘定制，用于量化遗忘集和保留集之间的“对齐”，同时保持计算效率。在此基础上，我们设计了一个新的遗忘目标，该目标为样本分配自适应的非均匀遗忘权重，这些权重与它们的代理归因分数成反比。通过这种遗忘能力的重新分配，GUARD减轻了保留中的意外损失。我们提供了严格的理论保证，证明GUARD显著增强了保留，同时保持了与现有方法相当的遗忘指标。在TOFU基准测试和多种LLM架构上的广泛实验表明，GUARD在确保有效遗忘的同时，大幅提高了效用保留。值得注意的是，当遗忘10%的训练数据时，GUARD在保留集上的效用牺牲（以真实性比率衡量）减少了高达194.92%。", "summary": "GUARD是一个新颖的框架，旨在解决大语言模型（LLMs）中遗忘特定数据时意外损害模型效用和有用信息保留的问题。它通过引入一个轻量级的数据归因度量来量化遗忘集和保留集之间的“对齐”，并设计了一个新的遗忘目标，根据此归因分数自适应地分配非均匀遗忘权重。这种方法有效减轻了意外遗忘，显著提高了LLMs的效用保留，同时保持了有效的遗忘性能，尤其在减少保留集上的效用牺牲方面表现突出。", "keywords": "大语言模型遗忘, 数据归因, 效用保留, 意外遗忘, GUARD", "comments": "GUARD的创新点在于其引入了数据归因的概念来指导大语言模型的遗忘过程，并设计了自适应的非均匀遗忘权重，这与以往主要关注架构创新的方法不同。这种数据层面的优化有效解决了意外遗忘问题，显著提升了模型在遗忘后对有用信息的保留能力，对于LLM在法规遵从和隐私保护方面的应用具有重要意义。其高达194.92%的效用牺牲减少是一个非常显著的成果。"}}
{"id": "2506.10821", "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using", "authors": ["Huaying Yuan", "Zheng Liu", "Junjie Zhou", "Ji-Rong Wen", "Zhicheng Dou"], "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10821v1", "AI": {"title_translation": "VideoDeepResearch：使用代理工具进行长视频理解", "tldr": "VideoDeepResearch是一个新颖的代理框架，它结合了文本推理模型和多模态工具包，以解决长视频理解（LVU）的挑战，并在多个基准测试中超越了现有MLLM基线。", "motivation": "当前多模态大型语言模型（MLLM）在处理长视频理解（LVU）任务时面临显著挑战，这主要由于任务的固有复杂性和上下文窗口限制。普遍认为解决LVU任务需要具有扩展上下文窗口、强大视觉感知能力和熟练领域专业知识的基础MLLM，本研究旨在挑战这一普遍观点。", "method": "本研究提出了VideoDeepResearch，一个新颖的代理框架，用于长视频理解。该方法仅依赖于一个文本推理模型（LRM）与一个模块化的多模态工具包（包括多模态检索器和视觉感知器）相结合，所有这些工具在实践中都易于获得。对于每个LVU任务，系统通过推理制定问题解决策略，同时通过工具使用选择性地访问和利用必要的视频内容。", "result": "VideoDeepResearch在流行LVU基准测试（包括MLVU、Video-MME和LVBench）中进行了广泛实验。结果表明，它在MLVU（测试集）、LVBench和LongVideoBench上分别比现有MLLM基线实现了9.6%、6.6%和3.9%的显著改进，超越了之前的最新技术水平。", "conclusion": "这些发现突显了代理系统在克服长视频理解（LVU）问题中关键挑战方面的潜力。", "translation": "长视频理解（LVU）对当前多模态大型语言模型（MLLM）构成了重大挑战，原因在于任务固有的复杂性和上下文窗口限制。人们普遍认为，解决LVU任务需要具备扩展上下文窗口、强大视觉感知能力和熟练领域专业知识的基础MLLM。在这项工作中，我们通过引入VideoDeepResearch，一个用于长视频理解的新颖代理框架，挑战了这一普遍观念。我们的方法仅依赖于一个纯文本的大型推理模型（LRM），结合一个模块化的多模态工具包，包括多模态检索器和视觉感知器，所有这些在实践中都易于获得。对于每个LVU任务，系统通过推理制定问题解决策略，同时通过工具使用选择性地访问和利用必要的视频内容。我们在流行的LVU基准测试（包括MLVU、Video-MME和LVBench）上进行了广泛实验。我们的结果表明，VideoDeepResearch在现有MLLM基线上取得了显著改进，在MLVU（测试集）、LVBench和LongVideoBench上分别超越了之前的最新技术水平9.6%、6.6%和3.9%。这些发现突显了代理系统在克服LVU问题中关键挑战方面的潜力。", "summary": "本研究提出了VideoDeepResearch，一个新颖的代理框架，旨在解决长视频理解（LVU）中多模态大型语言模型（MLLM）面临的挑战。与依赖大型MLLM的传统观点不同，VideoDeepResearch结合了一个文本推理模型和可用的多模态工具包，通过推理制定策略并选择性地利用视频内容。实验结果表明，该框架在多个LVU基准测试中显著优于现有MLLM基线，证明了代理系统在LVU问题中的有效性。", "keywords": "长视频理解, 代理系统, 多模态工具, 大型推理模型, VideoDeepResearch", "comments": "VideoDeepResearch的创新之处在于它挑战了现有范式，即认为解决长视频理解问题必须依赖于具有巨大上下文窗口的“大而全”MLLM。相反，它提出了一种更模块化、更具代理性的方法，通过将文本推理模型与可插拔的多模态工具相结合，实现了高效且高性能的长视频理解。这种“小模型+工具”的范式可能为未来多模态AI系统的设计提供新的思路，尤其是在资源受限或需要更高灵活性和可解释性的场景中。"}}
{"id": "2506.10948", "title": "Execution Guided Line-by-Line Code Generation", "authors": ["Boaz Lavon", "Shahar Katz", "Lior Wolf"], "summary": "We present a novel approach to neural code generation that incorporates\nreal-time execution signals into the language model generation process. While\nlarge language models (LLMs) have demonstrated impressive code generation\ncapabilities, they typically do not utilize execution feedback during\ninference, a critical signal that human programmers regularly leverage. Our\nmethod, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically\nincorporates execution signals as the model generates code, providing\nline-by-line feedback that guides the generation process toward executable\nsolutions. EG-CFG employs a multi-stage process: first, we conduct beam search\nto sample candidate program completions for each line; second, we extract\nexecution signals by executing these candidates against test cases; and\nfinally, we incorporate these signals into the prompt during generation. By\nmaintaining consistent signals across tokens within the same line and\nrefreshing signals at line boundaries, our approach provides coherent guidance\nwhile preserving syntactic structure. Moreover, the method naturally supports\nnative parallelism at the task level in which multiple agents operate in\nparallel, exploring diverse reasoning paths and collectively generating a broad\nset of candidate solutions. Our experiments across diverse coding tasks\ndemonstrate that EG-CFG significantly improves code generation performance\ncompared to standard approaches, achieving state-of-the-art results across\nvarious levels of complexity, from foundational problems to challenging\ncompetitive programming tasks. Our code is available at:\nhttps://github.com/boazlavon/eg_cfg", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10948v1", "AI": {"title_translation": "执行引导的逐行代码生成", "tldr": "本文提出了一种名为EG-CFG的新型神经代码生成方法，它在语言模型生成过程中实时融入执行信号，逐行指导代码生成，显著提升了性能，并在各种编码任务中取得了最先进的结果。", "motivation": "虽然大型语言模型（LLMs）在代码生成方面表现出色，但它们在推理过程中通常不利用执行反馈，而这正是人类程序员经常利用的关键信号，限制了其生成可执行代码的能力。", "method": "本文提出的Execution-Guided Classifier-Free Guidance (EG-CFG) 方法动态地将执行信号整合到模型生成代码的过程中，提供逐行反馈。该方法包括多阶段过程：首先，进行束搜索以采样每行的候选程序补全；其次，通过针对测试用例执行这些候选来提取执行信号；最后，在生成过程中将这些信号整合到提示中。通过在同一行内的不同token之间保持一致的信号并在行边界刷新信号，该方法提供了连贯的指导，同时保留了句法结构。此外，它自然支持任务级别的原生并行性。", "result": "实验表明，EG-CFG 方法与标准方法相比，显著提高了代码生成性能，并在从基础问题到具有挑战性的竞争性编程任务的各种复杂程度中取得了最先进的结果。", "conclusion": "通过在语言模型生成过程中逐行整合实时执行信号，可以显著增强代码生成性能，使其更接近人类程序员的实践。", "translation": "我们提出了一种新颖的神经代码生成方法，该方法将实时执行信号整合到语言模型生成过程中。虽然大型语言模型（LLM）已展示出令人印象深刻的代码生成能力，但它们在推理过程中通常不利用执行反馈，而这正是人类程序员经常利用的关键信号。我们的方法，执行引导的无分类器指导（EG-CFG），在模型生成代码时动态地整合执行信号，提供逐行反馈，引导生成过程趋向于可执行的解决方案。EG-CFG 采用多阶段过程：首先，我们进行束搜索以采样每行的候选程序补全；其次，我们通过针对测试用例执行这些候选来提取执行信号；最后，我们将这些信号整合到生成过程中的提示中。通过在同一行内的不同token之间保持一致的信号并在行边界刷新信号，我们的方法提供了连贯的指导，同时保留了句法结构。此外，该方法自然支持任务级别的原生并行性，其中多个代理并行操作，探索不同的推理路径并共同生成广泛的候选解决方案。我们在各种编码任务上的实验表明，与标准方法相比，EG-CFG 显著提高了代码生成性能，在从基础问题到具有挑战性的竞争性编程任务的各种复杂程度上都取得了最先进的结果。我们的代码可在以下网址获取：https://github.com/boazlavon/eg_cfg", "summary": "本文提出了一种名为Execution-Guided Classifier-Free Guidance (EG-CFG) 的新型神经代码生成方法，旨在解决大型语言模型在代码生成时缺乏实时执行反馈的问题。EG-CFG通过一个多阶段过程，包括束搜索生成候选、执行候选以提取信号，并将这些信号动态整合到生成提示中，从而实现逐行指导。该方法在保持语法结构的同时提供连贯的指导，并支持并行探索。实验证明，EG-CFG在多种编码任务中显著优于传统方法，并在不同复杂程度的任务中达到了最先进的性能。", "keywords": "神经代码生成, 执行反馈, 大型语言模型, 逐行生成, 无分类器指导", "comments": "该论文的创新之处在于将实时执行反馈直接整合到大型语言模型的代码生成循环中，这模仿了人类程序员的调试和迭代过程。逐行反馈机制和对并行性的支持是其显著的优势，有望大幅提升生成代码的实用性和正确性。这代表了代码生成领域的一个重要进步，使其生成的解决方案更可靠。"}}
{"id": "2506.10840", "title": "Post-Training Quantization for Video Matting", "authors": ["Tianrui Zhu", "Houyuan Chen", "Ruihao Gong", "Michele Magno", "Haotong Qin", "Kai Zhang"], "summary": "Video matting is crucial for applications such as film production and virtual\nreality, yet deploying its computationally intensive models on\nresource-constrained devices presents challenges. Quantization is a key\ntechnique for model compression and acceleration. As an efficient approach,\nPost-Training Quantization (PTQ) is still in its nascent stages for video\nmatting, facing significant hurdles in maintaining accuracy and temporal\ncoherence. To address these challenges, this paper proposes a novel and general\nPTQ framework specifically designed for video matting models, marking, to the\nbest of our knowledge, the first systematic attempt in this domain. Our\ncontributions include: (1) A two-stage PTQ strategy that combines\nblock-reconstruction-based optimization for fast, stable initial quantization\nand local dependency capture, followed by a global calibration of quantization\nparameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine\nCalibration (GAC) method that enables the network to compensate for cumulative\nstatistical distortions arising from factors such as neglected BN layer\neffects, even reducing the error of existing PTQ methods on video matting tasks\nup to 20%. (3) An Optical Flow Assistance (OFA) component that leverages\ntemporal and semantic priors from frames to guide the PTQ process, enhancing\nthe model's ability to distinguish moving foregrounds in complex scenes and\nultimately achieving near full-precision performance even under ultra-low-bit\nquantization. Comprehensive quantitative and visual results show that our\nPTQ4VM achieves the state-of-the-art accuracy performance across different\nbit-widths compared to the existing quantization methods. We highlight that the\n4-bit PTQ4VM even achieves performance close to the full-precision counterpart\nwhile enjoying 8x FLOP savings.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10840v1", "AI": {"title_translation": "视频抠图的训练后量化", "tldr": "提出了一种新颖的训练后量化（PTQ）框架（PTQ4VM），用于视频抠图模型，通过两阶段策略、全局仿射校准和光流辅助，实现了在资源受限设备上部署视频抠图模型的近全精度性能和显著计算节省。", "motivation": "视频抠图模型计算量大，难以部署在资源受限设备上。训练后量化（PTQ）作为一种模型压缩和加速技术，在视频抠图领域仍处于初期阶段，面临保持精度和时间一致性的挑战。", "method": "本文提出了一个新颖且通用的视频抠图模型训练后量化（PTQ）框架。其贡献包括：1. 两阶段PTQ策略，结合基于块重建的优化（用于快速、稳定的初始量化和局部依赖捕获），然后对量化参数进行全局校准以最小化精度损失。2. 统计驱动的全局仿射校准（GAC）方法，使网络能够补偿由忽略BN层效应等因素引起的累积统计失真，甚至将现有PTQ方法在视频抠图任务上的误差减少高达20%。3. 光流辅助（OFA）组件，利用帧的时间和语义先验来指导PTQ过程，增强模型在复杂场景中区分移动前景的能力，即使在超低比特量化下也能实现接近全精度的性能。", "result": "本文提出的PTQ4VM在不同比特宽度下，相比现有量化方法，实现了最先进的精度性能。特别是，4比特的PTQ4VM性能接近全精度模型，同时实现了8倍的FLOPs节省。", "conclusion": "本文提出的PTQ4VM框架有效解决了视频抠图模型在资源受限设备上部署的挑战，通过创新的量化策略和组件，实现了高精度和显著的计算效率提升。", "translation": "视频抠图对于电影制作和虚拟现实等应用至关重要，但将计算密集型模型部署到资源受限设备上带来了挑战。量化是模型压缩和加速的关键技术。作为一种高效的方法，训练后量化（PTQ）在视频抠图领域仍处于起步阶段，在保持精度和时间一致性方面面临重大障碍。为了解决这些挑战，本文提出了一种新颖且通用的专门为视频抠图模型设计的PTQ框架，据我们所知，这是该领域首次系统的尝试。我们的贡献包括：(1) 两阶段PTQ策略，结合基于块重建的优化以实现快速、稳定的初始量化和局部依赖捕获，随后进行量化参数的全局校准以最小化精度损失。(2) 一种统计驱动的全局仿射校准（GAC）方法，使网络能够补偿由忽略BN层效应等因素引起的累积统计失真，甚至将现有PTQ方法在视频抠图任务上的误差降低高达20%。(3) 光流辅助（OFA）组件，利用帧的时间和语义先验来指导PTQ过程，增强模型在复杂场景中区分移动前景的能力，最终即使在超低比特量化下也能实现接近全精度的性能。全面的定量和视觉结果表明，与现有量化方法相比，我们的PTQ4VM在不同比特宽度下均实现了最先进的精度性能。我们强调，4比特的PTQ4VM甚至实现了接近全精度对应模型的性能，同时享受8倍的FLOPs节省。", "summary": "本文针对视频抠图模型在资源受限设备上部署的挑战，提出了首个系统性的训练后量化（PTQ）框架PTQ4VM。该框架包含两阶段PTQ策略、统计驱动的全局仿射校准（GAC）和光流辅助（OFA）组件。实验结果表明，PTQ4VM在保持高精度的同时显著降低了计算成本，尤其在4比特量化下实现了接近全精度的性能并节省了8倍的FLOPs。", "keywords": "视频抠图, 训练后量化, 模型压缩, 量化参数校准, 光流辅助", "comments": "这篇论文通过引入一个专门为视频抠图设计的系统性训练后量化（PTQ）框架，展现了显著的创新性。其亮点在于结合了多方面的优化策略：两阶段量化保证了稳定性和精度，GAC有效补偿了量化带来的统计失真，而OFA则巧妙地利用了视频的时间信息来提升量化性能。这些方法的结合使得PTQ在视频抠图这一复杂任务上实现了突破，尤其是在超低比特量化下仍能保持高精度，这对于实际部署在边缘设备上具有重要意义。该工作填补了视频抠图领域PTQ的空白，并为未来的模型压缩研究提供了新的思路。"}}
{"id": "2506.10953", "title": "Build the web for agents, not agents for the web", "authors": ["Xing Han Lù", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10953v1", "AI": {"title_translation": "为智能体构建网络，而非为网络构建智能体", "tldr": "本文提出了一种范式转变：与其让网络智能体适应为人类设计的界面，不如开发专门为智能体优化的交互范式，并引入了智能体网络接口（AWI）的概念。", "motivation": "当前的网络智能体研究面临巨大挑战，因为为人类设计的网页界面与大型语言模型（LLM）的能力之间存在根本性不匹配，导致现有方法在处理复杂的网页输入时举步维艰。", "method": "本文提出了一种网络智能体研究的范式转变：不再强制网络智能体适应为人类设计的界面，而是开发一种专门为智能体能力优化的新型交互范式。为此，论文引入了智能体网络接口（AWI）的概念，这是一种专门为智能体导航网站而设计的界面，并为AWI设计建立了六项指导原则，强调安全性、效率和标准化。", "result": "作为一篇立场论文，其结果是提出了一个新概念和设计原则，旨在克服现有界面的基本局限性，为更高效、可靠和透明的网络智能体设计铺平道路。", "conclusion": "结论是，通过重新构建网络智能体的设计理念，即开发专门为智能体设计的界面（AWI），可以克服现有界面的局限性，从而实现更高效、可靠和透明的网络智能体设计，这将需要整个机器学习社区的协作努力。", "translation": "大型语言模型（LLM）和多模态对应物最近的进展激发了开发网络智能体——能够在网络环境中自主导航和完成任务的AI系统——的巨大兴趣。尽管在自动化复杂的网络交互方面前景广阔，但由于人类设计的界面与LLM能力之间存在根本性不匹配，当前的方法面临巨大挑战。现有方法在处理复杂的网络输入时举步维艰，无论是处理庞大的DOM树、依赖于增强额外信息的截图，还是通过API交互完全绕过用户界面。这篇立场论文倡导网络智能体研究的范式转变：与其强迫网络智能体适应为人类设计的界面，我们应该开发一种专门为智能体能力优化的新型交互范式。为此，我们引入了智能体网络接口（AWI）的概念，这是一种专门为智能体导航网站而设计的界面。我们为AWI设计建立了六项指导原则，强调安全性、效率和标准化，以兼顾所有主要利益相关者的利益。这种重新构建旨在克服现有界面的基本局限性，为更高效、可靠和透明的网络智能体设计铺平道路，这将是一项涉及更广泛机器学习社区的协作努力。", "summary": "本文针对当前网络智能体在人类设计界面中面临的挑战，提出了一种范式转变。作者认为，与其让智能体适应现有网页，不如构建专门为智能体优化的“智能体网络接口”（AWI）。论文提出了AWI的六项设计原则，旨在提高智能体与网络的交互效率、可靠性和透明度，并强调这是需要机器学习社区协作的未来方向。", "keywords": "网络智能体, 大型语言模型, 智能体网络接口, 范式转变, 界面设计", "comments": "这篇立场论文提出了一个非常新颖且重要的观点，即从“智能体适应网络”转变为“网络适应智能体”。AWI的概念及其六项设计原则为未来的网络智能体设计提供了清晰的方向，有望从根本上解决现有方法在复杂网页交互中的效率和可靠性问题。其强调的安全性、效率和标准化原则也考虑到了实际应用中的关键因素。"}}
{"id": "2506.10857", "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos", "authors": ["Jiashuo Yu", "Yue Wu", "Meng Chu", "Zhifei Ren", "Zizheng Huang", "Pei Chu", "Ruijie Zhang", "Yinan He", "Qirui Li", "Songze Li", "Zhenxiang Li", "Zhongying Tu", "Conghui He", "Yu Qiao", "Yali Wang", "Yi Wang", "Limin Wang"], "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.", "comment": "Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10857v1", "AI": {"title_translation": "VRBench：长叙事视频中的多步推理基准", "tldr": "VRBench是一个新的长叙事视频基准，用于评估大型模型的多步推理能力，解决了现有评估中忽视时间推理和程序有效性的问题。它包含1010个长视频、9468个人工标注的多步问答对和30292个带有时间戳的推理步骤。", "motivation": "现有的大型模型评估在多步推理方面存在局限性，特别是忽视了时间推理和过程有效性。为了解决这些问题，本研究提出了VRBench。", "method": "VRBench是一个包含1010个平均时长1.6小时的长视频的基准，配有9468个多步问答对和30292个带时间戳的推理步骤。视频通过多阶段过滤（包括专家互评）以确保情节连贯性。研究开发了一个人机协作框架来生成连贯的推理链，这些推理链需要多个时间定位的步骤，涵盖七种类型。VRBench设计了一个多阶段评估流程，从结果和过程层面评估模型。除了最终结果的MCQ，还提出了一个由LLM引导的进度级评分指标，以多维度全面评估推理链的质量。", "result": "通过对VRBench上的12个LLM和16个VLM进行广泛评估，本研究进行了深入分析，并提供了宝贵的见解。", "conclusion": "VRBench的提出及其评估结果，旨在推动多步推理领域的发展。", "translation": "我们提出了VRBench，这是第一个为评估大型模型多步推理能力而精心制作的长叙事视频基准，解决了现有评估中忽视时间推理和程序有效性的局限性。它包括1010个长视频（平均时长1.6小时），以及9468个人工标注的多步问答对和30292个带有时间戳的推理步骤。这些视频通过多阶段过滤过程进行整理，包括专家互评以优先考虑情节连贯性。我们开发了一个人机协作框架，用于生成连贯的推理链，每个推理链都需要多个时间定位的步骤，涵盖七种类型（例如，事件归因、隐式推理）。VRBench设计了一个多阶段评估流程，从结果和过程层面评估模型。除了最终结果的MCQ，我们还提出了一种进度级LLM引导的评分指标，以多维度全面评估推理链的质量。通过对VRBench上的12个LLM和16个VLM进行广泛评估，我们进行了彻底的分析并提供了宝贵的见解，从而推动了多步推理领域的发展。", "summary": "VRBench是首个专为评估大型模型在长叙事视频中多步推理能力而设计的基准。它旨在解决现有评估中对时间推理和过程有效性的忽视。VRBench包含1010个长视频，配有9468个多步问答对和30292个带时间戳的推理步骤，这些数据经过精心策划以确保情节连贯性。研究开发了一个人机协作框架来生成多步推理链，并设计了一个多阶段评估流程，结合MCQ和LLM引导的评分指标来全面评估模型。通过对12个LLM和16个VLM的广泛评估，VRBench提供了有价值的见解，以推动多步推理领域的发展。", "keywords": "VRBench, 多步推理, 长叙事视频, 基准, 大模型评估", "comments": "VRBench的创新之处在于其首次为长叙事视频中的多步推理提供了大规模基准，填补了现有评估的空白。它通过引入时间推理和过程有效性评估，提高了对大型模型理解复杂叙事能力的衡量标准。人机协作框架和多阶段评估管道的设计也十分巧妙，特别是LLM引导的进度级评分指标，为评估推理过程提供了更细致的方法。这项工作对于推动视频理解和多模态推理领域具有重要意义。"}}
{"id": "2506.10955", "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "comment": "38 pages, 14 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10955v1", "AI": {"title_translation": "ReGuidance：一个用于提升困难逆问题样本质量的简单扩散包装器", "tldr": "ReGuidance是一个简单的扩散模型包装器，通过逆转候选解并将其作为DPS的初始化，显著提升了在低信噪比困难逆问题中样本的真实性和奖励，并提供了理论保证。", "motivation": "现有基于扩散模型的训练无关方法（如DPS）在面对低信噪比的困难逆问题时，由于奖励信息不足，容易偏离数据流形，导致无法生成真实的输出。", "method": "提出ReGuidance，一个简单的包装器。给定用户选择算法生成的候选解$\\hat{x}$，通过从$\\hat{x}$开始反向运行无条件概率流ODE来反演该解，然后将得到的潜在变量作为DPS的初始化。", "result": "在大型方框图像修复和高放大倍数超分辨率等困难逆问题上，当现有最先进的基线方法明显失败时，ReGuidance显著提升了样本质量和测量一致性。理论上证明，在某些多模态数据分布上，ReGuidance同时提升了奖励并将候选解拉近数据流形。", "conclusion": "ReGuidance通过其独特的反演和初始化策略，有效解决了扩散模型在困难逆问题中生成不真实输出的问题，并提供了首个关于DPS的严格算法保证，显著提升了样本质量和测量一致性。", "translation": "关于使用预训练扩散模型作为解决逆问题的先验数据，以及更广泛地使用奖励模型引导这些模型的研究活动非常活跃。像扩散后验采样（DPS）及其许多变体等无需训练的方法为这些任务提供了灵活的启发式算法，但当奖励信息不足时，例如在低信噪比的困难逆问题中，这些技术会偏离数据流形，无法产生真实的输出。在这项工作中，我们设计了一个简单的包装器ReGuidance，用于提升这些方法的样本真实性和所获得的奖励。给定用户选择的算法生成的候选解$\\hat{x}$，我们建议通过从$\\hat{x}$开始反向运行无条件概率流ODE来反演该解，然后将得到的潜在变量作为DPS的初始化。我们在大型方框图像修复和高放大倍数超分辨率等困难逆问题上评估了我们的包装器。当最先进的基线方法明显失败时，我们发现将我们的包装器应用于这些基线之上显著提升了样本质量和测量一致性。我们通过理论补充了这些发现，证明在某些多模态数据分布上，ReGuidance同时提升了奖励并将候选解拉近数据流形。据我们所知，这构成了DPS的第一个严格算法保证。", "summary": "ReGuidance是一个针对扩散模型的简单包装器，旨在解决在低信噪比困难逆问题中，现有训练无关方法（如DPS）生成不真实输出的问题。它通过反演候选解并将其潜在变量作为DPS的初始化来提升样本真实性和奖励。实验证明，ReGuidance在图像修复和超分辨率等任务上显著优于现有基线，并提供了理论保证，是DPS的首个严格算法保证。", "keywords": "扩散模型, 逆问题, ReGuidance, 样本质量, DPS", "comments": "这篇论文通过引入一个新颖的“反演-初始化”策略，有效解决了扩散模型在处理困难逆问题时可能出现的样本真实性下降问题。其创新点在于将无条件概率流ODE的反向运行与DPS相结合，为现有方法提供了有效的改进方案。此外，提供了理论证明，增强了其方法的说服力，填补了DPS在理论保证方面的空白。"}}
{"id": "2506.10890", "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation", "authors": ["Zhao Zhang", "Yutao Cheng", "Dexiang Hong", "Maoke Yang", "Gonglei Shi", "Lei Ma", "Hui Zhang", "Jie Shao", "Xinglong Wu"], "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10890v1", "AI": {"title_translation": "CreatiPoster：迈向可编辑和可控的多层图形设计生成", "tldr": "CreatiPoster是一个新的AI框架，可以根据文本指令或资产生成可编辑的多层图形设计，解决了现有AI工具在用户资产整合、可编辑性和专业视觉效果方面的不足，并优于现有方法。", "motivation": "高质量、可编辑且美观的图形设计耗时且需要技巧，尤其是对初学者。现有AI工具难以准确整合用户资产、保持可编辑性并达到专业视觉效果，而商业系统依赖难以复制的模板库。", "method": "我们引入了CreatiPoster框架，它通过一个协议模型（RGBA大型多模态模型）首先生成一个详细说明每个层（文本或资产）的JSON规范，包括布局、层次、内容、风格和背景提示。然后，一个条件背景模型根据渲染的前景层合成一个连贯的背景。", "result": "我们构建了一个图形设计生成的基准测试，并使用自动化指标，结果表明CreatiPoster超越了领先的开源方法和专有商业系统。我们还发布了一个包含10万个多层设计的无版权语料库。", "conclusion": "CreatiPoster通过生成可编辑的多层设计，支持多种应用，如画布编辑、文本叠加、响应式调整大小、多语言适应和动画海报，从而推动了AI辅助图形设计的普及。", "translation": "图形设计在商业和个人环境中都扮演着至关重要的角色，然而，创建高质量、可编辑且美观的图形作品仍然是一项耗时且需要技巧的任务，特别是对于初学者而言。当前的AI工具自动化了部分工作流程，但在准确整合用户提供的资产、保持可编辑性以及实现专业视觉吸引力方面仍面临挑战。像Canva Magic Design这样的商业系统依赖于庞大的模板库，这对于复制来说是不切实际的。在本文中，我们介绍了CreatiPoster，一个可以根据可选的自然语言指令或资产生成可编辑、多层作品的框架。一个协议模型，即一个RGBA大型多模态模型，首先生成一个JSON规范，详细说明每个层（文本或资产）的精确布局、层次、内容和风格，以及一个简洁的背景提示。然后，一个条件背景模型根据渲染的前景层合成一个连贯的背景。我们构建了一个带有自动化指标的图形设计生成基准，并表明CreatiPoster超越了领先的开源方法和专有商业系统。为了促进进一步的研究，我们发布了一个包含10万个多层设计的无版权语料库。CreatiPoster支持多种应用，例如画布编辑、文本叠加、响应式调整大小、多语言适应和动画海报，从而推动了AI辅助图形设计的普及。项目主页：https://github.com/graphic-design-ai/creatiposter", "summary": "CreatiPoster是一个新颖的AI框架，旨在解决图形设计中创建高质量、可编辑多层作品的挑战。它通过一个协议模型生成详细的JSON规范，再由背景模型合成背景，从而实现了从自然语言指令或资产生成可编辑设计。该框架在性能上超越了现有开源和商业系统，并通过发布大规模数据集促进了研究。CreatiPoster支持多种应用，有效推动了AI辅助图形设计的民主化。", "keywords": "图形设计, 多层生成, 可编辑性, AI辅助设计, CreatiPoster", "comments": "CreatiPoster的创新之处在于其独特的分层生成方法，特别是协议模型生成JSON规范的机制，这使得输出具有高度的可编辑性和可控性，解决了现有AI工具的痛点。其发布的大规模无版权数据集对社区具有重要贡献，有望加速该领域的研究。该系统在商业和个人应用中具有巨大的潜力，能够显著降低图形设计的门槛。"}}
{"id": "2506.10959", "title": "Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods", "authors": ["Zhaiming Shen", "Alexander Hsu", "Rongjie Lai", "Wenjing Liao"], "summary": "While in-context learning (ICL) has achieved remarkable success in natural\nlanguage and vision domains, its theoretical understanding--particularly in the\ncontext of structured geometric data--remains unexplored. In this work, we\ninitiate a theoretical study of ICL for regression of H\\\"older functions on\nmanifolds. By establishing a novel connection between the attention mechanism\nand classical kernel methods, we derive generalization error bounds in terms of\nthe prompt length and the number of training tasks. When a sufficient number of\ntraining tasks are observed, transformers give rise to the minimax regression\nrate of H\\\"older functions on manifolds, which scales exponentially with the\nintrinsic dimension of the manifold, rather than the ambient space dimension.\nOur result also characterizes how the generalization error scales with the\nnumber of training tasks, shedding light on the complexity of transformers as\nin-context algorithm learners. Our findings provide foundational insights into\nthe role of geometry in ICL and novels tools to study ICL of nonlinear models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10959v1", "AI": {"title_translation": "理解结构化流形上的上下文学习：连接注意力与核方法", "tldr": "本文首次对结构化几何数据上的上下文学习（ICL）进行了理论研究，通过建立注意力机制与经典核方法的新连接，推导了泛化误差界限。研究发现，当训练任务充足时，Transformer在流形上对H\"older函数回归可达到最优速率，其误差与流形内在维度而非环境空间维度呈指数关系，为理解ICL中的几何作用和Transformer的复杂性提供了见解。", "motivation": "尽管上下文学习（ICL）在自然语言和视觉领域取得了显著成功，但其理论理解，特别是在结构化几何数据背景下，仍未被探索。", "method": "本文通过建立注意力机制与经典核方法之间的新颖联系，对流形上H\"older函数的回归上下文学习（ICL）进行了理论研究，并推导了基于提示长度和训练任务数量的泛化误差界限。", "result": "1. 当观察到足够多的训练任务时，Transformer在流形上对H\"older函数回归可达到极小极大回归率。2. 该回归率与流形的内在维度呈指数关系，而非环境空间维度。3. 研究结果还描述了泛化误差如何随训练任务数量的变化而变化，从而揭示了Transformer作为上下文算法学习器的复杂性。", "conclusion": "本研究的发现为几何在上下文学习（ICL）中的作用提供了基础性见解，并为研究非线性模型的ICL提供了新工具。", "translation": "尽管上下文学习（ICL）在自然语言和视觉领域取得了显著成功，但其理论理解——特别是在结构化几何数据背景下——仍未被探索。在这项工作中，我们对流形上H\"older函数的回归ICL进行了理论研究。通过建立注意力机制与经典核方法之间的新颖联系，我们推导了关于提示长度和训练任务数量的泛化误差界限。当观察到足够多的训练任务时，Transformer能够达到流形上H\"older函数的极小极大回归率，该速率与流形的内在维度呈指数关系，而非环境空间维度。我们的结果还描述了泛化误差如何随训练任务数量的变化而变化，从而揭示了Transformer作为上下文算法学习器的复杂性。我们的发现为几何在ICL中的作用提供了基础性见解，并为研究非线性模型的ICL提供了新工具。", "summary": "本文首次对结构化几何数据上的上下文学习（ICL）进行了理论探索，特别是针对流形上H\"older函数的回归问题。研究通过将注意力机制与经典核方法相结合，推导了ICL的泛化误差界限。结果表明，当有足够训练任务时，Transformer能够达到流形上H\"older函数的极小极大回归率，且该速率的缩放与流形的内在维度而非环境空间维度相关。这些发现不仅揭示了ICL中几何的作用，也为理解Transformer作为上下文算法学习器的复杂性提供了深入见解。", "keywords": "上下文学习, 结构化流形, 注意力机制, 核方法, 泛化误差", "comments": "这项工作具有重要的理论意义，它首次将ICL的理论研究扩展到结构化几何数据，并通过连接注意力机制和核方法提供了新的分析工具。其关于泛化误差与内在维度相关的发现尤其创新，为理解Transformer在复杂数据上的学习能力提供了基础。"}}
{"id": "2506.10895", "title": "AIR: Zero-shot Generative Model Adaptation with Iterative Refinement", "authors": ["Guimeng Liu", "Milad Abdollahzadeh", "Ngai-Man Cheung"], "summary": "Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained\ngenerator to a target domain using only text guidance and without any samples\nfrom the target domain. Central to recent ZSGM approaches are directional loss\nwhich use the text guidance in the form of aligning the image offset with text\noffset in the embedding space of a vision-language model like CLIP. This is\nsimilar to the analogical reasoning in NLP where the offset between one pair of\nwords is used to identify a missing element in another pair by aligning the\noffset between these two pairs. However, a major limitation of existing ZSGM\nmethods is that the learning objective assumes the complete alignment between\nimage offset and text offset in the CLIP embedding space, resulting in quality\ndegrade in generated images. Our work makes two main contributions. Inspired by\nthe offset misalignment studies in NLP, as our first contribution, we perform\nan empirical study to analyze the misalignment between text offset and image\noffset in CLIP embedding space for various large publicly available datasets.\nOur important finding is that offset misalignment in CLIP embedding space is\ncorrelated with concept distance, i.e., close concepts have a less offset\nmisalignment. To address the limitations of the current approaches, as our\nsecond contribution, we propose Adaptation with Iterative Refinement (AIR)\nwhich is the first ZSGM approach to focus on improving target domain image\nquality based on our new insight on offset misalignment.Qualitative,\nquantitative, and user study in 26 experiment setups consistently demonstrate\nthe proposed AIR approach achieves SOTA performance. Additional experiments are\nin Supp.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10895v1", "AI": {"title_translation": "AIR：基于迭代细化的零样本生成模型适应", "tldr": "本文提出了一种名为AIR的零样本生成模型适应（ZSGM）方法，通过解决CLIP嵌入空间中的偏移未对齐问题，显著提高了生成图像的质量，并达到了最先进的性能。", "motivation": "现有零样本生成模型适应（ZSGM）方法的一个主要局限性是，其学习目标假设CLIP嵌入空间中图像偏移与文本偏移之间存在完全对齐，导致生成图像的质量下降。", "method": "本文首先对CLIP嵌入空间中文本偏移和图像偏移之间的未对齐进行了实证研究，发现偏移未对齐与概念距离相关，即相近概念的偏移未对齐程度较低。在此基础上，提出了名为“基于迭代细化的适应”（AIR）的方法，这是第一个专注于基于偏移未对齐新见解来提高目标域图像质量的ZSGM方法。", "result": "研究发现CLIP嵌入空间中的偏移未对齐与概念距离相关，即相近概念的偏移未对齐程度较低。所提出的AIR方法在26种实验设置中，通过定性、定量和用户研究一致证明实现了最先进（SOTA）的性能。", "conclusion": "AIR是第一个专注于基于偏移未对齐新见解来提高目标域图像质量的零样本生成模型适应（ZSGM）方法，并在多项实验中实现了最先进的性能。", "translation": "零样本生成模型适应（ZSGM）旨在仅使用文本指导且不使用目标域的任何样本来适应预训练的生成器。近期ZSGM方法的核心是方向性损失，它以在视觉-语言模型（如CLIP）的嵌入空间中对齐图像偏移与文本偏移的形式使用文本指导。这类似于自然语言处理（NLP）中的类比推理，其中一对词之间的偏移用于通过对齐这两对之间的偏移来识别另一对中缺失的元素。然而，现有ZSGM方法的一个主要局限性是，其学习目标假设CLIP嵌入空间中图像偏移与文本偏移之间存在完全对齐，导致生成图像的质量下降。我们的工作做出了两项主要贡献。受NLP中偏移未对齐研究的启发，作为我们的第一项贡献，我们进行了一项实证研究，分析了各种大型公开数据集中CLIP嵌入空间中文本偏移和图像偏移之间的未对齐。我们的重要发现是，CLIP嵌入空间中的偏移未对齐与概念距离相关，即相近概念的偏移未对齐程度较低。为了解决当前方法的局限性，作为我们的第二项贡献，我们提出了基于迭代细化的适应（AIR），这是第一个专注于基于我们对偏移未对齐的新见解来提高目标域图像质量的ZSGM方法。在26种实验设置中，定性、定量和用户研究一致表明所提出的AIR方法实现了最先进的性能。补充实验在附录中。", "summary": "本文针对零样本生成模型适应（ZSGM）中CLIP嵌入空间内图像偏移与文本偏移未对齐导致生成图像质量下降的问题，进行了深入研究。研究发现偏移未对齐与概念距离相关。在此基础上，提出了一种名为“基于迭代细化的适应”（AIR）的新方法，该方法通过迭代细化过程解决了偏移未对齐问题，显著提高了目标域图像的生成质量，并在多项实验中取得了最先进的性能。", "keywords": "零样本生成模型适应, CLIP, 偏移未对齐, 迭代细化, 图像生成", "comments": "本文的创新点在于首次深入研究了零样本生成模型适应（ZSGM）中CLIP嵌入空间内的偏移未对齐问题，并提出了与概念距离相关的发现。在此基础上，提出的AIR方法通过迭代细化有效解决了这一限制，显著提升了生成图像的质量，并达到了SOTA性能，对零样本生成领域具有重要意义。"}}
{"id": "2506.10972", "title": "Farseer: A Refined Scaling Law in Large Language Models", "authors": ["Houyi Li", "Wenzhen Zheng", "Qiufeng Wang", "Zhenyu Ding", "Haoying Wang", "Zili Wang", "Shijie Xuyang", "Ning Ding", "Shuigeng Zhou", "Xiangyu Zhang", "Daxin Jiang"], "summary": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research.", "comment": "34", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10972v1", "AI": {"title_translation": "Farseer: 大型语言模型中的一种改进的缩放定律", "tldr": "本文提出了Farseer，一种改进的LLM缩放定律，显著提高了预测准确性，减少了外推误差，有助于高效创新和计算优化。", "motivation": "训练大型语言模型（LLMs）成本极高，导致小规模实验的见解难以推广到资源密集型生产系统，从而阻碍了高效创新。", "method": "本文引入了Farseer，一种新的改进缩放定律，通过系统地构建模型损失表面L(N,D)来提高预测准确性。为验证方法，研究人员训练了大约1000个不同规模和配置的LLM，并开源了所有模型、数据、结果和日志。", "result": "Farseer比现有定律（如Chinchilla定律）与经验数据拟合得显著更好，将外推误差减少了433%。它能够可靠地评估各种训练策略，将小规模研究结论推广到大规模性能预测，并为最佳计算分配提供新见解。", "conclusion": "Farseer提供了一种更准确、鲁棒和可泛化的缩放定律，能够将小规模研究结论自信地推广到大规模性能预测，从而促进LLM训练的创新和优化。", "translation": "训练大型语言模型（LLMs）成本极高，造成了一个关键的缩放差距，即小规模实验的见解往往无法转移到资源密集型生产系统，从而阻碍了高效创新。为了弥补这一差距，我们引入了Farseer，一种新颖且经过改进的缩放定律，能够在不同规模上提供更高的预测准确性。通过系统地构建模型损失表面L(N,D)，Farseer比先前的定律（例如Chinchilla定律）与经验数据拟合得显著更好。我们的方法产生了准确、鲁棒且高度可泛化的预测，展示了出色的外推能力，通过将外推误差减少433%来改进Chinchilla定律。这使得能够可靠地评估所有(N,D)设置下的竞争性训练策略，从而使小规模消融研究的结论能够自信地外推以预测大规模性能。此外，Farseer为最佳计算分配提供了新的见解，更好地反映了现代LLM训练的细微需求。为了验证我们的方法，我们训练了大约1000个不同规模和配置的LLM，消耗了大约300万NVIDIA H100 GPU小时。我们正在全面开源所有模型、数据、结果和日志，网址为https://github.com/Farseer-Scaling-Law/Farseer，以促进进一步的研究。", "summary": "本文引入了Farseer，一种用于大型语言模型（LLM）的新型改进缩放定律，旨在弥补小规模实验与大规模生产系统之间的缩放差距。Farseer通过构建模型损失表面L(N,D)显著提高了预测准确性，比现有定律（如Chinchilla定律）更符合经验数据，并将外推误差减少了433%。该方法能够可靠地评估各种训练策略，并将小规模研究结果自信地推广到大规模性能预测，同时为优化计算分配提供了新见解。为验证Farseer，研究人员训练了约1000个LLM，并全面开源了所有相关数据。", "keywords": "大型语言模型, 缩放定律, Farseer, 预测准确性, 计算优化", "comments": "Farseer的创新在于其通过构建损失表面L(N,D)来提供更精确的LLM缩放定律，显著优于现有方法。其重要性在于能够降低LLM训练的试错成本，加速创新，并优化资源分配。通过开源所有数据，该研究也极大地促进了社区的进一步探索。"}}
{"id": "2506.10915", "title": "M4V: Multi-Modal Mamba for Text-to-Video Generation", "authors": ["Jiancheng Huang", "Gengwei Zhang", "Zequn Jie", "Siyu Jiao", "Yinlong Qian", "Ling Chen", "Yunchao Wei", "Lin Ma"], "summary": "Text-to-video generation has significantly enriched content creation and\nholds the potential to evolve into powerful world simulators. However, modeling\nthe vast spatiotemporal space remains computationally demanding, particularly\nwhen employing Transformers, which incur quadratic complexity in sequence\nprocessing and thus limit practical applications. Recent advancements in\nlinear-time sequence modeling, particularly the Mamba architecture, offer a\nmore efficient alternative. Nevertheless, its plain design limits its direct\napplicability to multi-modal and spatiotemporal video generation tasks. To\naddress these challenges, we introduce M4V, a Multi-Modal Mamba framework for\ntext-to-video generation. Specifically, we propose a multi-modal diffusion\nMamba (MM-DiM) block that enables seamless integration of multi-modal\ninformation and spatiotemporal modeling through a multi-modal token\nre-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45%\ncompared to the attention-based alternative when generating videos at\n768$\\times$1280 resolution. Additionally, to mitigate the visual quality\ndegradation in long-context autoregressive generation processes, we introduce a\nreward learning strategy that further enhances per-frame visual realism.\nExtensive experiments on text-to-video benchmarks demonstrate M4V's ability to\nproduce high-quality videos while significantly lowering computational costs.\nCode and models will be publicly available at\nhttps://huangjch526.github.io/M4V_project.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10915v1", "AI": {"title_translation": "M4V：多模态Mamba用于文本到视频生成", "tldr": "M4V引入多模态Mamba架构，显著降低文本到视频生成的计算成本，同时保持高质量，解决了Transformer的二次复杂度问题。", "motivation": "文本到视频生成在计算上要求很高，特别是Transformer模型存在序列处理的二次复杂度，限制了实际应用。虽然Mamba架构更高效，但其简单的设计限制了其直接应用于多模态和时空视频生成任务。", "method": "提出M4V框架，核心是多模态扩散Mamba (MM-DiM) 块，通过多模态token重组设计实现多模态信息和时空建模的无缝集成。此外，引入奖励学习策略以缓解长上下文自回归生成过程中的视觉质量下降问题。", "result": "与基于注意力的替代方案相比，M4V中的Mamba块在生成768x1280分辨率视频时，FLOPs减少了45%。在文本到视频基准测试上，M4V能够生成高质量视频并显著降低计算成本。", "conclusion": "M4V通过引入多模态Mamba架构，成功解决了文本到视频生成中的计算效率和质量问题，实现了高性能和低成本的视频生成。", "translation": "文本到视频生成极大地丰富了内容创作，并有望发展成为强大的世界模拟器。然而，建模广阔的时空空间仍然计算量巨大，特别是在使用Transformer时，其在序列处理中产生二次复杂度，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。尽管如此，其简单的设计限制了其直接应用于多模态和时空视频生成任务。为了解决这些挑战，我们引入了M4V，一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态token重组设计实现多模态信息和时空建模的无缝集成。因此，在生成768×1280分辨率的视频时，M4V中的Mamba块与基于注意力的替代方案相比，FLOPs减少了45%。此外，为了减轻长上下文自回归生成过程中视觉质量的下降，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。在文本到视频基准上的广泛实验表明，M4V能够生成高质量视频，同时显著降低计算成本。代码和模型将在https://huangjch526.github.io/M4V_project公开。", "summary": "本文提出了M4V，一个基于多模态Mamba架构的文本到视频生成框架，旨在解决传统Transformer在处理大时空数据时计算成本高的问题。M4V引入了多模态扩散Mamba (MM-DiM) 块，有效整合多模态信息和时空建模，并通过奖励学习策略提升视觉质量。实验证明，M4V显著降低了计算量（FLOPs减少45%），同时生成了高质量视频。", "keywords": "文本到视频生成, 多模态Mamba, 计算效率, 时空建模, 扩散模型", "comments": "M4V的创新点在于将Mamba架构引入多模态文本到视频生成领域，并通过MM-DiM块和奖励学习策略解决了Mamba在多模态和长上下文生成中的局限性。其重要性在于显著提高了文本到视频生成的计算效率，为高质量内容创作和未来世界模拟器发展提供了更实用的基础。"}}
{"id": "2506.10941", "title": "VINCIE: Unlocking In-context Image Editing from Video", "authors": ["Leigang Qu", "Feng Cheng", "Ziyan Yang", "Qi Zhao", "Shanchuan Lin", "Yichun Shi", "Yicong Li", "Wenjie Wang", "Tat-Seng Chua", "Lu Jiang"], "summary": "In-context image editing aims to modify images based on a contextual sequence\ncomprising text and previously generated images. Existing methods typically\ndepend on task-specific pipelines and expert models (e.g., segmentation and\ninpainting) to curate training data. In this work, we explore whether an\nin-context image editing model can be learned directly from videos. We\nintroduce a scalable approach to annotate videos as interleaved multimodal\nsequences. To effectively learn from this data, we design a block-causal\ndiffusion transformer trained on three proxy tasks: next-image prediction,\ncurrent segmentation prediction, and next-segmentation prediction.\nAdditionally, we propose a novel multi-turn image editing benchmark to advance\nresearch in this area. Extensive experiments demonstrate that our model\nexhibits strong in-context image editing capabilities and achieves\nstate-of-the-art results on two multi-turn image editing benchmarks. Despite\nbeing trained exclusively on videos, our model also shows promising abilities\nin multi-concept composition, story generation, and chain-of-editing\napplications.", "comment": "Project page: https://vincie2025.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10941v1", "AI": {"title_translation": "VINCIE：从视频中解锁上下文图像编辑", "tldr": "VINCIE提出了一种直接从视频中学习上下文图像编辑模型的方法，通过新的数据标注和块因果扩散Transformer，实现了最先进的性能，并展现了多概念组合等潜力。", "motivation": "现有的上下文图像编辑方法通常依赖于特定任务的管道和专家模型（如分割和修复）来整理训练数据，这限制了其泛化能力。本文旨在探索是否可以直接从视频中学习上下文图像编辑模型。", "method": "本文引入了一种可扩展的方法来将视频标注为交错的多模态序列。为了有效学习，设计了一个块因果扩散Transformer，并在三个代理任务上进行训练：下一图像预测、当前分割预测和下一分割预测。此外，还提出了一个新的多轮图像编辑基准。", "result": "模型展现出强大的上下文图像编辑能力，并在两个多轮图像编辑基准上取得了最先进的结果。尽管仅使用视频进行训练，模型在多概念组合、故事生成和编辑链应用中也显示出有前景的能力。", "conclusion": "VINCIE成功证明了可以直接从视频数据中学习上下文图像编辑模型，并在多个任务上取得了显著成果，为该领域的研究开辟了新的方向。", "translation": "上下文图像编辑旨在根据包含文本和先前生成图像的上下文序列来修改图像。现有方法通常依赖于特定任务的管道和专家模型（例如，分割和修复）来整理训练数据。在这项工作中，我们探索是否可以直接从视频中学习一个上下文图像编辑模型。我们引入了一种可扩展的方法来将视频标注为交错的多模态序列。为了有效地从这些数据中学习，我们设计了一个块因果扩散Transformer，并在三个代理任务上进行训练：下一图像预测、当前分割预测和下一分割预测。此外，我们提出了一个新的多轮图像编辑基准，以推动该领域的研究。大量的实验表明，我们的模型展现出强大的上下文图像编辑能力，并在两个多轮图像编辑基准上取得了最先进的结果。尽管仅使用视频进行训练，我们的模型在多概念组合、故事生成和编辑链应用中也显示出有前景的能力。", "summary": "VINCIE提出了一种新颖的上下文图像编辑方法，通过直接从视频中学习来克服现有方法对特定任务流水线和专家模型的依赖。该方法包括将视频标注为交错多模态序列的可扩展技术，并设计了一个块因果扩散Transformer，在图像和分割预测代理任务上进行训练。实验证明，VINCIE在多轮图像编辑基准上实现了最先进的性能，并展示了在多概念组合、故事生成和编辑链应用中的潜力，突显了视频作为训练数据源的有效性。", "keywords": "上下文图像编辑, 视频学习, 扩散模型, VINCIE, 多模态序列", "comments": "这项研究的创新之处在于提出了直接从视频中学习上下文图像编辑模型的方法，避免了对昂贵且费时的专家标注数据的依赖。通过将视频转化为多模态序列和设计专门的块因果扩散Transformer，该模型在无需传统分割/修复模型的情况下实现了SOTA性能，并在多概念组合等新兴应用中展现了潜力，为图像编辑领域提供了新思路。"}}
{"id": "2506.10982", "title": "Rethinking Losses for Diffusion Bridge Samplers", "authors": ["Sebastian Sanokowski", "Lukas Gruber", "Christoph Bartmann", "Sepp Hochreiter", "Sebastian Lehner"], "summary": "Diffusion bridges are a promising class of deep-learning methods for sampling\nfrom unnormalized distributions. Recent works show that the Log Variance (LV)\nloss consistently outperforms the reverse Kullback-Leibler (rKL) loss when\nusing the reparametrization trick to compute rKL-gradients. While the on-policy\nLV loss yields identical gradients to the rKL loss when combined with the\nlog-derivative trick for diffusion samplers with non-learnable forward\nprocesses, this equivalence does not hold for diffusion bridges or when\ndiffusion coefficients are learned. Based on this insight we argue that for\ndiffusion bridges the LV loss does not represent an optimization objective that\ncan be motivated like the rKL loss via the data processing inequality. Our\nanalysis shows that employing the rKL loss with the log-derivative trick\n(rKL-LD) does not only avoid these conceptual problems but also consistently\noutperforms the LV loss. Experimental results with different types of diffusion\nbridges on challenging benchmarks show that samplers trained with the rKL-LD\nloss achieve better performance. From a practical perspective we find that\nrKL-LD requires significantly less hyperparameter optimization and yields more\nstable training behavior.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10982v1", "AI": {"title_translation": "重新思考扩散桥采样器的损失函数", "tldr": "对于扩散桥采样器，带有对数导数技巧的rKL损失（rKL-LD）在概念上更合理，并且在实验中始终优于对数方差（LV）损失，从而实现更好、更稳定的训练。", "motivation": "最近的研究表明，在扩散采样器中，对数方差（LV）损失的表现优于反向Kullback-Leibler（rKL）损失。然而，本文指出，LV和rKL之间的等价性（在使用对数导数技巧时）不适用于扩散桥或当扩散系数可学习时。作者认为，对于扩散桥，LV损失缺乏像rKL损失那样可以通过数据处理不等式进行激励的优化目标。", "method": "作者分析了扩散桥中LV损失的概念性问题，并提出采用带有对数导数技巧的rKL损失（rKL-LD）。他们通过在具有挑战性的基准上对不同类型的扩散桥进行实验来验证其方法的有效性。", "result": "实验结果表明，rKL-LD损失始终优于LV损失。使用rKL-LD损失训练的采样器实现了更好的性能，需要显著更少的超参数优化，并产生了更稳定的训练行为。", "conclusion": "对于扩散桥采样器，rKL-LD损失在概念上更为合理，并且在实践中优于LV损失，从而提高了性能和稳定性。", "translation": "扩散桥是一类很有前景的深度学习方法，用于从非归一化分布中采样。最近的研究表明，当使用重参数化技巧计算rKL梯度时，对数方差（LV）损失始终优于反向Kullback-Leibler（rKL）损失。虽然当结合对数导数技巧用于具有不可学习前向过程的扩散采样器时，on-policy的LV损失会产生与rKL损失相同的梯度，但这种等价性不适用于扩散桥或当扩散系数可学习时。基于这一见解，我们认为对于扩散桥，LV损失不能像rKL损失那样通过数据处理不等式来激励。我们的分析表明，采用对数导数技巧的rKL损失（rKL-LD）不仅避免了这些概念性问题，而且始终优于LV损失。在具有挑战性的基准上，对不同类型扩散桥的实验结果表明，使用rKL-LD损失训练的采样器取得了更好的性能。从实践角度来看，我们发现rKL-LD需要显著更少的超参数优化，并产生更稳定的训练行为。", "summary": "本文重新评估了扩散桥采样器的损失函数，挑战了对数方差（LV）损失优于反向Kullback-Leibler（rKL）损失的普遍看法。作者认为，LV和rKL之间的理论等价性不适用于扩散桥，并且LV在此背景下缺乏适当的概念性动机。他们的分析表明，使用带有对数导数技巧的rKL损失（rKL-LD）不仅解决了这些概念性问题，而且在各种基准测试中，始终能为扩散桥采样器带来更优的性能、更少的超参数调优和更稳定的训练。", "keywords": "扩散桥, 损失函数, rKL损失, 对数方差损失, 采样器", "comments": "这篇论文通过重新评估扩散桥采样器中损失函数的理论基础和实际性能，做出了重要贡献。它挑战了关于LV损失优越性的普遍假设，并提供了一种理论上更健全、经验上更有效的替代方案（rKL-LD）。关于提高稳定性和减少超参数优化的发现对于实际应用尤其有价值。"}}
{"id": "2506.10962", "title": "SpectralAR: Spectral Autoregressive Visual Generation", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Yueqi Duan", "Jie Zhou", "Jiwen Lu"], "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.", "comment": "Project Page: https://huang-yh.github.io/spectralar/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10962v1", "AI": {"title_translation": "SpectralAR：频谱自回归视觉生成", "tldr": "SpectralAR通过将图像转换为有序频谱token，实现了因果性视觉序列生成，解决了现有自回归模型中空间补丁缺乏因果性的问题，并在ImageNet-1K上取得了良好性能。", "motivation": "现有大多数自回归视觉生成方法将图像构建为空间补丁序列，但图像补丁本质上是并行的，这与自回归建模的因果性质相矛盾。", "method": "本文提出了一种名为Spectral AutoRegressive (SpectralAR) 的视觉生成框架，该框架从频谱角度实现视觉序列的因果性。具体而言，首先通过嵌套频谱分词（Nested Spectral Tokenization）将图像转换为有序的频谱token，这些token代表从低频到高频的分量。然后，利用这些频谱token序列以从粗到细的方式进行自回归生成。", "result": "在ImageNet-1K上进行了图像重建和自回归生成实验，SpectralAR仅用64个token和3.1亿参数就达到了3.02 gFID的性能。", "conclusion": "SpectralAR通过考虑图像不同细节层次，在不增加复杂性的情况下，实现了序列因果性和token效率。", "translation": "自回归视觉生成因其与扩散模型相比具有可扩展性和与其他模态的兼容性而受到越来越多的关注。大多数现有方法将视觉序列构建为空间补丁用于自回归生成。然而，图像补丁本质上是并行的，这与自回归建模的因果性质相矛盾。为了解决这个问题，我们提出了一种频谱自回归（Spectral AutoRegressive, SpectralAR）视觉生成框架，该框架从频谱角度实现了视觉序列的因果性。具体而言，我们首先通过嵌套频谱分词将图像转换为有序的频谱token，这些token代表从低频到高频的分量。然后，我们利用频谱token序列以从粗到细的方式进行自回归生成。通过考虑图像中不同级别的细节，我们的SpectralAR在不增加额外复杂性的情况下实现了序列因果性和token效率。我们在ImageNet-1K上进行了广泛的图像重建和自回归生成实验，SpectralAR仅用64个token和3.1亿参数就达到了3.02 gFID的性能。项目页面：https://huang-yh.github.io/spectralar/。", "summary": "本文提出了SpectralAR，一个新颖的频谱自回归视觉生成框架，旨在解决现有方法中空间图像补丁缺乏因果性的问题。通过将图像转换为有序的频谱token并进行从粗到细的自回归生成，SpectralAR有效地实现了序列因果性和token效率。在ImageNet-1K上的实验表明，该模型在图像生成方面表现出色，展现了其在视觉生成领域的潜力。", "keywords": "频谱自回归, 视觉生成, 因果性, 频谱分词, ImageNet-1K", "comments": "这篇论文的创新点在于从频谱角度构建视觉序列，解决了传统基于空间补丁的自回归模型中因果性不足的问题。通过引入有序的频谱token和从粗到细的生成策略，SpectralAR在保持模型效率的同时，提高了生成质量，对于自回归视觉生成领域是一个重要的进展。"}}
{"id": "2406.15669", "title": "CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes", "authors": ["Jason Yang", "Ariane Mora", "Shengchao Liu", "Bruce J. Wittmann", "Anima Anandkumar", "Frances H. Arnold", "Yisong Yue"], "summary": "Enzymes are important proteins that catalyze chemical reactions. In recent\nyears, machine learning methods have emerged to predict enzyme function from\nsequence; however, there are no standardized benchmarks to evaluate these\nmethods. We introduce CARE, a benchmark and dataset suite for the\nClassification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1)\nclassification of a protein sequence by its enzyme commission (EC) number and\n(2) retrieval of an EC number given a chemical reaction. For each task, we\ndesign train-test splits to evaluate different kinds of out-of-distribution\ngeneralization that are relevant to real use cases. For the classification\ntask, we provide baselines for state-of-the-art methods. Because the retrieval\ntask has not been previously formalized, we propose a method called Contrastive\nReaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task\nand compare it to the recent method, CLIPZyme. CARE is available at\nhttps://github.com/jsunn-y/CARE/.", "comment": null, "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2406.15669v3", "AI": {"title_translation": "CARE：一个酶分类与检索的基准测试套件", "tldr": "CARE是一个新的基准测试套件和数据集，用于评估酶的分类和检索方法，旨在解决现有方法缺乏标准化基准的问题。", "motivation": "当前预测酶功能的机器学习方法缺乏标准化的评估基准。", "method": "引入了CARE，一个包含分类和检索两种任务的基准测试套件和数据集。分类任务是根据EC号对蛋白质序列进行分类；检索任务是给定化学反应检索EC号。设计了训练-测试划分以评估不同类型的分布外泛化能力。为分类任务提供了最先进方法的基线，并针对首次形式化的检索任务提出了Contrastive Reaction-EnzymE Pretraining (CREEP) 方法作为基线，并与CLIPZyme进行了比较。", "result": "为分类任务提供了最先进方法的基线。针对检索任务，提出了CREEP作为首批基线之一，并将其与CLIPZyme进行了比较。", "conclusion": "CARE为酶的分类和检索提供了标准化的基准和数据集，有助于评估和推进相关机器学习方法的发展。", "translation": "酶是催化化学反应的重要蛋白质。近年来，机器学习方法已兴起，用于从序列预测酶功能；然而，目前还没有标准化基准来评估这些方法。我们引入了CARE，一个用于酶分类和检索（Classification And Retrieval of Enzymes, CARE）的基准测试套件和数据集。CARE围绕两项任务展开：（1）通过酶学委员会（EC）编号对蛋白质序列进行分类，以及（2）给定化学反应检索EC编号。对于每项任务，我们设计了训练-测试划分，以评估与实际用例相关的不同类型的分布外泛化能力。对于分类任务，我们提供了最先进方法的基线。由于检索任务以前没有被形式化，我们提出了一种名为对比反应-酶预训练（Contrastive Reaction-EnzymE Pretraining, CREEP）的方法，作为该任务的首批基线之一，并将其与最近的方法CLIPZyme进行了比较。CARE可在https://github.com/jsunn-y/CARE/获取。", "summary": "本论文介绍了CARE，一个用于酶分类和检索的标准化基准测试套件和数据集。它包含两个核心任务：基于EC号的蛋白质序列分类和给定化学反应的EC号检索。CARE设计了特殊的训练-测试划分以评估模型的分布外泛化能力。论文为分类任务提供了现有方法的基线，并针对首次形式化的检索任务提出了新的基线方法CREEP，并与CLIPZyme进行了比较，旨在弥补当前机器学习方法缺乏统一评估标准的不足。", "keywords": "酶, 分类, 检索, 基准测试, 机器学习", "comments": "该论文的创新之处在于首次提出了一个标准化基准CARE，解决了酶功能预测领域缺乏统一评估标准的问题。它不仅涵盖了传统的酶分类任务，还首次形式化并提出了酶检索任务，并提供了相应的基线方法，对推动该领域的机器学习研究具有重要意义。"}}
{"id": "2506.10963", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10963v1", "AI": {"title_translation": "MMMG：一个用于文本到图像推理的大规模、多学科、多层次生成基准", "tldr": "本文引入了知识图像生成任务和MMMG基准，以评估图像生成模型的推理能力，并揭示了现有模型的推理缺陷。", "motivation": "现有图像生成模型在推理能力方面存在不足，特别是在生成需要融合世界知识和像素级细节的“知识图像”时面临挑战。为了全面评估和推动这一领域的发展，需要一个新的任务和基准。", "method": "1. 引入了“知识图像生成”新任务。2. 构建了MMMG基准，包含4,456个专家验证的图像-提示对，涵盖10个学科、6个教育水平和多种知识格式（图表、图示、思维导图）。3. 采用统一的知识图谱（KG）表示来描述图像的核心实体及其依赖关系。4. 提出了MMMG-Score评估指标，结合了事实准确性（通过KG之间的图编辑距离衡量）和视觉清晰度。5. 提供了FLUX-Reason作为开放基线，它结合了推理LLM和扩散模型，并在16,000个知识图像-提示对上训练。", "result": "对16个最先进的文本到图像生成模型进行全面评估后发现，它们在推理方面存在严重缺陷，表现为实体保真度低、关系薄弱和图像混乱。例如，GPT-4o的MMMG-Score仅为50.20，这凸显了该基准的难度。", "conclusion": "本文引入的知识图像生成任务和MMMG基准揭示了当前文本到图像生成模型在多模态推理能力上的显著不足。通过发布FLUX-Reason基线，旨在促进该领域未来的研究进展。", "translation": "在本文中，我们引入了知识图像生成作为一项新任务，并同时推出了大规模、多学科、多层次知识图像生成基准（MMMG），以探究图像生成模型的推理能力。知识图像在人类文明和人类学习机制中一直占据核心地位——双重编码理论和图像优势效应都强调了这一点。生成此类图像具有挑战性，需要多模态推理，将世界知识与像素级基础融合为清晰的解释性视觉效果。为了实现全面评估，MMMG提供了4,456个经过专家验证的（知识）图像-提示对，涵盖10个学科、6个教育水平以及图表、图示和思维导图等多种知识格式。为了消除评估过程中的混淆复杂性，我们采用了统一的知识图谱（KG）表示。每个KG都明确描绘了目标图像的核心实体及其依赖关系。我们进一步引入了MMMG-Score来评估生成的知识图像。该指标结合了事实保真度（通过KG之间的图编辑距离衡量）和视觉清晰度评估。对16个最先进的文本到图像生成模型的全面评估揭示了严重的推理缺陷——实体保真度低、关系薄弱和混乱——其中GPT-4o的MMMG-Score仅为50.20，这凸显了该基准的难度。为了促进进一步的进展，我们发布了FLUX-Reason（MMMG-Score为34.45），这是一个有效且开放的基线，它结合了推理LLM和扩散模型，并在16,000个精选知识图像-提示对上进行了训练。", "summary": "本文提出“知识图像生成”这一新任务，并发布了大规模、多学科、多层次知识图像生成基准（MMMG），旨在评估文本到图像生成模型的推理能力。MMMG包含4,456个专家验证的图像-提示对，覆盖多学科和教育层次，并采用统一的知识图谱表示和MMMG-Score进行评估。对现有SOTA模型的测试表明，它们在生成知识图像时存在严重的推理缺陷。为推动研究，作者还开源了FLUX-Reason基线模型。", "keywords": "知识图像生成, 文本到图像推理, 基准测试, 知识图谱, 多模态推理", "comments": "这篇论文通过引入“知识图像生成”这一新颖且重要的任务，并构建了一个大规模、多学科、多层次的MMMG基准，填补了现有文本到图像生成模型在评估复杂推理能力方面的空白。其创新之处在于定义了需要融合世界知识和视觉细节的知识图像，并设计了基于知识图谱的事实保真度评估方法MMMG-Score。该研究揭示了当前SOTA模型在推理方面的显著不足，对未来文本到图像生成技术的发展方向具有重要指导意义。发布FLUX-Reason基线模型也体现了其推动社区进步的积极作用。"}}
{"id": "2506.10967", "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs", "authors": ["Qizhe Zhang", "Mengzhen Liu", "Lichen Li", "Ming Lu", "Yuan Zhang", "Junwen Pan", "Qi She", "Shanghang Zhang"], "summary": "In multimodal large language models (MLLMs), the length of input visual\ntokens is often significantly greater than that of their textual counterparts,\nleading to a high inference cost. Many works aim to address this issue by\nremoving redundant visual tokens. However, current approaches either rely on\nattention-based pruning, which retains numerous duplicate tokens, or use\nsimilarity-based pruning, overlooking the instruction relevance, consequently\ncausing suboptimal performance. In this paper, we go beyond attention or\nsimilarity by proposing a novel visual token pruning method named CDPruner,\nwhich maximizes the conditional diversity of retained tokens. We first define\nthe conditional similarity between visual tokens conditioned on the\ninstruction, and then reformulate the token pruning problem with determinantal\npoint process (DPP) to maximize the conditional diversity of the selected\nsubset. The proposed CDPruner is training-free and model-agnostic, allowing\neasy application to various MLLMs. Extensive experiments across diverse MLLMs\nshow that CDPruner establishes new state-of-the-art on various vision-language\nbenchmarks. By maximizing conditional diversity through DPP, the selected\nsubset better represents the input images while closely adhering to user\ninstructions, thereby preserving strong performance even with high reduction\nratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency\nby 78\\%, while maintaining 94\\% of the original accuracy. Our code is available\nat https://github.com/Theia-4869/CDPruner.", "comment": "22 pages, 5 figures, code: https://github.com/Theia-4869/CDPruner,\n  project page: https://theia-4869.github.io/CDPruner", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10967v1", "AI": {"title_translation": "超越注意力或相似性：在多模态大语言模型中最大化条件多样性以进行Token剪枝", "tldr": "提出CDPruner，一种新颖的视觉token剪枝方法，通过最大化条件多样性来降低MLLM的推理成本，实现了SOTA性能。", "motivation": "多模态大语言模型（MLLMs）中视觉token输入长度远大于文本，导致高昂的推理成本。现有剪枝方法（基于注意力或相似性）存在冗余或忽略指令相关性，导致性能次优。", "method": "提出CDPruner方法，通过定义基于指令的视觉token条件相似性，并利用行列式点过程（DPP）重新 формуulate token剪枝问题，以最大化所选子集的条件多样性。该方法是免训练且模型无关的。", "result": "CDPruner在各种MLLMs上的广泛实验表明，它在各种视觉-语言基准测试中达到了新的最先进水平。通过DPP最大化条件多样性，所选子集能更好地表示输入图像并紧密遵循用户指令，即使在高剪枝率下也能保持强大性能。应用于LLaVA时，FLOPs减少95%，CUDA延迟减少78%，同时保持94%的原始精度。", "conclusion": "CDPruner通过最大化条件多样性，有效解决了MLLMs中视觉token冗余问题，显著降低了推理成本，同时保持了高精度，并在多个基准测试中表现出SOTA性能。", "translation": "在多模态大型语言模型（MLLMs）中，输入视觉token的长度通常远大于其对应的文本token，导致高昂的推理成本。许多工作旨在通过移除冗余视觉token来解决这个问题。然而，当前的方法要么依赖于基于注意力的剪枝，保留了大量的重复token，要么使用基于相似性的剪枝，忽略了指令相关性，从而导致次优性能。在本文中，我们超越了注意力或相似性，提出了一种新颖的视觉token剪枝方法，名为CDPruner，它最大化了保留token的条件多样性。我们首先定义了基于指令的视觉token之间的条件相似性，然后利用行列式点过程（DPP）重新 формуulate token剪枝问题，以最大化所选子集的条件多样性。所提出的CDPruner是免训练且模型无关的，可以轻松应用于各种MLLMs。在各种MLLMs上进行的广泛实验表明，CDPruner在各种视觉-语言基准测试中建立了新的最先进水平。通过DPP最大化条件多样性，所选子集能够更好地表示输入图像，同时紧密遵循用户指令，从而即使在高剪枝率下也能保持强大的性能。当应用于LLaVA时，CDPruner将FLOPs减少了95%，CUDA延迟减少了78%，同时保持了94%的原始精度。我们的代码可在https://github.com/Theia-4869/CDPruner获取。", "summary": "本文提出了一种名为CDPruner的新型视觉token剪枝方法，用于解决多模态大语言模型（MLLMs）中视觉token冗余导致的高推理成本问题。与传统基于注意力或相似性的方法不同，CDPruner通过定义基于指令的条件相似性，并利用行列式点过程（DPP）最大化保留token的条件多样性。该方法是免训练且模型无关的，在多个MLLMs和视觉-语言基准测试中实现了最先进的性能，显著降低了计算成本（如LLaVA上FLOPs减少95%，CUDA延迟减少78%），同时保持了高精度。", "keywords": "多模态大语言模型, Token剪枝, 条件多样性, 行列式点过程, 推理成本优化", "comments": "这篇论文的创新点在于提出了“条件多样性”的概念，并创造性地将其与行列式点过程（DPP）结合起来解决多模态大语言模型中的视觉token剪枝问题。相较于传统的注意力或相似性剪枝，它考虑了指令相关性，并通过最大化多样性来确保保留的token更具代表性。其免训练和模型无关的特性使其具有很高的实用性和普适性。在降低计算成本的同时保持甚至提升性能，对MLLMs的实际部署具有重要意义。"}}
{"id": "2506.10975", "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "summary": "The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10975v1", "AI": {"title_translation": "GenWorld：迈向检测AI生成真实世界模拟视频", "tldr": "本文提出了GenWorld，一个用于AI生成视频检测的大规模高质量真实世界模拟数据集，并引入了SpannDetector，一个利用多视角一致性进行检测的模型，实验证明其效果优于现有方法。", "motivation": "视频生成技术的蓬勃发展威胁了真实世界信息的可信度，并增加了对AI生成视频检测器的需求。尽管有所进展，但缺乏高质量的真实世界数据集阻碍了可信检测器的开发。", "method": "本文提出了GenWorld，一个大规模、高质量、真实世界模拟的AI生成视频检测数据集。GenWorld具有以下特点：真实世界模拟、高质量和跨提示多样性。此外，本文还提出了SpannDetector，一个简单但有效的模型，利用多视角一致性作为检测真实世界AI生成视频的强有力标准。", "result": "分析发现现有方法无法检测世界模型（如Cosmos）生成的高质量视频，这揭示了忽视真实世界线索的潜在缺点。实验表明，所提出的方法取得了优异的结果，突出了基于物理合理性的可解释AI生成视频检测的有前景方向。", "conclusion": "GenWorld有望推动AI生成视频检测领域的发展。", "translation": "视频生成技术的蓬勃发展危及了真实世界信息的可信度，并加剧了对AI生成视频检测器的需求。尽管取得了一些进展，但高质量真实世界数据集的缺乏阻碍了可信检测器的发展。在本文中，我们提出了GenWorld，一个用于AI生成视频检测的大规模、高质量、真实世界模拟数据集。GenWorld具有以下特点：(1) 真实世界模拟：GenWorld专注于复制真实世界场景的视频，这些视频因其真实性和潜在影响力而具有显著影响；(2) 高质量：GenWorld采用多种最先进的视频生成模型来提供真实且高质量的伪造视频；(3) 跨提示多样性：GenWorld包含由不同生成器和各种提示模态（例如文本、图像、视频）生成的视频，提供了学习更通用取证特征的潜力。我们分析了现有方法，发现它们无法检测世界模型（即Cosmos）生成的高质量视频，揭示了忽视真实世界线索的潜在缺点。为了解决这个问题，我们提出了一个简单但有效的模型SpannDetector，以利用多视角一致性作为检测真实世界AI生成视频的有力标准。实验表明，我们的方法取得了优异的结果，突出了基于物理合理性的可解释AI生成视频检测的有前景方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld", "summary": "本文针对AI生成视频检测中高质量真实世界数据集的缺失问题，提出了GenWorld数据集。GenWorld是一个大规模、高质量、包含真实世界模拟和跨提示多样性的数据集。研究发现现有方法在检测世界模型生成的高质量视频时存在不足。为此，本文提出了一种名为SpannDetector的新模型，该模型利用多视角一致性来提高检测性能。实验结果表明，SpannDetector取得了优异的检测效果，为基于物理合理性的可解释AI生成视频检测提供了新方向。", "keywords": "AI生成视频检测, 真实世界模拟, 数据集, 多视角一致性, 可解释性AI", "comments": "本文的创新点在于构建了一个大规模、高质量的真实世界模拟数据集GenWorld，这对于推动AI生成视频检测领域的发展至关重要。同时，提出的SpannDetector模型利用多视角一致性，并结合物理合理性进行可解释检测，解决了现有方法在检测世界模型生成视频时的局限性，具有重要的实践意义和研究价值。"}}
{"id": "2506.10977", "title": "QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy Prediction", "authors": ["Sicheng Zuo", "Wenzhao Zheng", "Xiaoyong Han", "Longchao Yang", "Yong Pan", "Jiwen Lu"], "summary": "3D occupancy prediction is crucial for robust autonomous driving systems as\nit enables comprehensive perception of environmental structures and semantics.\nMost existing methods employ dense voxel-based scene representations, ignoring\nthe sparsity of driving scenes and resulting in inefficiency. Recent works\nexplore object-centric representations based on sparse Gaussians, but their\nellipsoidal shape prior limits the modeling of diverse structures. In\nreal-world driving scenes, objects exhibit rich geometries (e.g., cuboids,\ncylinders, and irregular shapes), necessitating excessive ellipsoidal Gaussians\ndensely packed for accurate modeling, which leads to inefficient\nrepresentations. To address this, we propose to use geometrically expressive\nsuperquadrics as scene primitives, enabling efficient representation of complex\nstructures with fewer primitives through their inherent shape diversity. We\ndevelop a probabilistic superquadric mixture model, which interprets each\nsuperquadric as an occupancy probability distribution with a corresponding\ngeometry prior, and calculates semantics through probabilistic mixture.\nBuilding on this, we present QuadricFormer, a superquadric-based model for\nefficient 3D occupancy prediction, and introduce a pruning-and-splitting module\nto further enhance modeling efficiency by concentrating superquadrics in\noccupied regions. Extensive experiments on the nuScenes dataset demonstrate\nthat QuadricFormer achieves state-of-the-art performance while maintaining\nsuperior efficiency.", "comment": "Project page: https://zuosc19.github.io/QuadricFormer/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10977v1", "AI": {"title_translation": "QuadricFormer：基于超二次曲面的3D语义占用预测场景", "tldr": "QuadricFormer利用超二次曲面作为场景基元，解决了传统3D占用预测方法效率低下和表达能力有限的问题，实现了高效且最先进的3D语义占用预测。", "motivation": "现有的3D占用预测方法存在效率低下（基于密集体素）和表达能力不足（基于稀疏高斯）的问题。密集体素表示忽略了驾驶场景的稀疏性，而稀疏高斯方法由于其椭球形状先验限制了对多样化结构的建模，导致需要大量高斯来准确建模，从而效率低下。", "method": "本文提出使用几何表达能力强的超二次曲面作为场景基元，以更少的基元高效表示复杂结构。开发了一个概率超二次曲面混合模型，将每个超二次曲面解释为具有相应几何先验的占用概率分布，并通过概率混合计算语义。在此基础上，提出了QuadricFormer模型，并引入了剪枝和分裂模块，通过将超二次曲面集中在占用区域来进一步提高建模效率。", "result": "在nuScenes数据集上进行的广泛实验表明，QuadricFormer在保持卓越效率的同时，实现了最先进的性能。", "conclusion": "QuadricFormer通过引入几何表达能力强的超二次曲面作为场景基元，有效解决了现有3D语义占用预测方法在效率和表达能力上的局限性，实现了高效且高性能的3D语义占用预测。", "translation": "3D占用预测对于鲁棒的自动驾驶系统至关重要，因为它能够全面感知环境结构和语义。大多数现有方法采用基于密集体素的场景表示，忽略了驾驶场景的稀疏性，导致效率低下。最近的工作探索了基于稀疏高斯的对象中心表示，但其椭球形状先验限制了对多样化结构的建模。在现实世界的驾驶场景中，物体呈现出丰富的几何形状（例如，长方体、圆柱体和不规则形状），需要过度密集堆叠的椭球高斯才能准确建模，这导致了低效的表示。为了解决这个问题，我们提出使用几何表达能力强的超二次曲面作为场景基元，通过其固有的形状多样性，用更少的基元高效表示复杂结构。我们开发了一个概率超二次曲面混合模型，该模型将每个超二次曲面解释为具有相应几何先验的占用概率分布，并通过概率混合计算语义。在此基础上，我们提出了QuadricFormer，一个基于超二次曲面的高效3D占用预测模型，并引入了一个剪枝和分裂模块，通过将超二次曲面集中在占用区域来进一步提高建模效率。在nuScenes数据集上进行的广泛实验表明，QuadricFormer在保持卓越效率的同时，实现了最先进的性能。", "summary": "本文针对3D语义占用预测中现有方法存在的效率低下（密集体素）和表达能力有限（稀疏高斯）问题，提出了一种名为QuadricFormer的新模型。该模型核心在于利用几何表达能力强的超二次曲面作为场景基元，能够以更少的基元高效表示复杂结构。通过开发概率超二次曲面混合模型，并引入剪枝和分裂模块，QuadricFormer显著提升了建模效率。实验证明，QuadricFormer在nuScenes数据集上实现了最先进的性能，同时保持了卓越的效率。", "keywords": "3D占用预测, 超二次曲面, 场景表示, 自动驾驶, QuadricFormer", "comments": "QuadricFormer的创新点在于引入超二次曲面作为3D场景表示的基元，这显著提升了对复杂几何形状的建模能力，同时解决了传统体素和高斯方法在效率和表达能力上的局限性。其概率混合模型和剪枝-分裂模块进一步优化了效率。该方法在自动驾驶等需要精确环境感知的领域具有重要意义。"}}
{"id": "2506.10015", "title": "Identifying critical residues of a protein using meaningfully-thresholded Random Geometric Graphs", "authors": ["Chuqiao Zhang", "Sarath Chandra Dantu", "Debarghya Mitra", "Dalia Chakrabarty"], "summary": "Identification of critical residues of a protein is actively pursued, since\nsuch residues are essential for protein function. We present three ways of\nrecognising critical residues of an example protein, the evolution of which is\ntracked via molecular dynamical simulations. Our methods are based on learning\na Random Geometric Graph (RGG) variable, where the state variable of each of\n156 residues, is attached to a node of this graph, with the RGG learnt using\nthe matrix of correlations between state variables of each residue-pair. Given\nthe categorical nature of the state variable, correlation between a residue\npair is computed using Cramer's V. We advance an organic thresholding to learn\nan RGG, and compare results against extant thresholding techniques, when\nparametrising criticality as the nodal degree in the learnt RGG. Secondly, we\ndevelop a criticality measure by ranking the computed differences between the\nposterior probability of the full graph variable defined on all 156 residues,\nand that of the graph with all but one residue omitted. A third parametrisation\nof criticality informs on the dynamical variation of nodal degrees as the\nprotein evolves during the simulation. Finally, we compare results obtained\nwith the three distinct criticality parameters, against\nexperimentally-ascertained critical residues.", "comment": "submitted to Journal of Computational and Graphical Statistics", "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.10015v1", "AI": {"title_translation": "使用有意义阈值随机几何图识别蛋白质关键残基", "tldr": "本文提出三种基于随机几何图的方法来识别蛋白质的关键残基，并通过与实验数据对比验证了其有效性。", "motivation": "蛋白质关键残基的识别是当前研究热点，因为这些残基对蛋白质功能至关重要。", "method": "本文提出三种识别蛋白质关键残基的方法。首先，通过学习随机几何图（RGG）来识别，其中每个残基的状态变量作为图的节点，RGG通过残基对状态变量的相关矩阵（使用Cramer's V计算）学习得到。引入了一种“有机阈值”方法来学习RGG，并将关键性参数化为节点度，与其他现有阈值技术进行比较。其次，通过计算包含所有残基的完整图的后验概率与移除一个残基后的图的后验概率之间的差异来衡量关键性。第三种方法通过模拟过程中蛋白质演化时节点度的动态变化来表征关键性。", "result": "本文将通过上述三种不同关键性参数获得的结果与实验确定的关键残基进行了比较。", "conclusion": "本文提出了多种基于随机几何图的方法来识别蛋白质关键残基，并通过与实验数据的比较验证了这些方法的有效性。", "translation": "蛋白质关键残基的识别是当前积极追求的目标，因为这些残基对蛋白质功能至关重要。我们提出了三种识别示例蛋白质关键残基的方法，该蛋白质的演化通过分子动力学模拟进行追踪。我们的方法基于学习一个随机几何图（RGG）变量，其中156个残基中每个残基的状态变量都附着到该图的一个节点上，RGG是利用每个残基对状态变量之间的相关矩阵学习得到的。鉴于状态变量的分类性质，残基对之间的相关性使用Cramer's V计算。我们提出了一种有机阈值方法来学习RGG，并将其结果与现有阈值技术进行比较，其中关键性被参数化为学习到的RGG中的节点度。其次，我们通过计算定义在所有156个残基上的完整图变量的后验概率与除一个残基外所有残基都省略的图的后验概率之间的计算差异进行排序，从而开发了一种关键性度量。第三种关键性参数化方法说明了蛋白质在模拟过程中演化时节点度的动态变化。最后，我们将通过三种不同关键性参数获得的结果与实验确定的关键残基进行了比较。", "summary": "本文旨在识别蛋白质的关键残基，这些残基对蛋白质功能至关重要。研究人员提出并比较了三种基于随机几何图（RGG）的方法。这些方法利用残基状态变量之间的相关性（通过Cramer's V计算）来构建RGG。第一种方法通过“有机阈值”学习RGG，并将节点度作为关键性参数。第二种方法通过比较完整图与移除单一残基图的后验概率差异来评估关键性。第三种方法关注蛋白质演化过程中节点度的动态变化。所有方法的结果都与实验确定的关键残基进行了比较。", "keywords": "蛋白质关键残基, 随机几何图, Cramer's V, 有机阈值, 分子动力学模拟", "comments": "本文创新性地将随机几何图应用于蛋白质关键残基的识别，并提出了多种新颖的关键性参数化方法，包括“有机阈值”和基于后验概率差异的度量。通过与实验数据进行对比验证，增加了研究的可靠性。该方法为蛋白质功能研究提供了新的计算工具。"}}
{"id": "2506.10978", "title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Minjae Kim", "Jaewon Min", "Wooseok Jang", "Saungwu Lee", "Sayak Paul", "Susung Hong", "Seungryong Kim"], "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.", "comment": "Project page: https://cvlab-kaist.github.io/HeadHunter/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10978v1", "AI": {"title_translation": "通过注意力头选择实现细粒度扰动引导", "tldr": "论文提出了HeadHunter框架和SoftPAG方法，通过选择性扰动扩散模型中的注意力头，实现对图像生成质量和视觉属性的细粒度控制，解决了现有方法缺乏精确扰动位置的问题。", "motivation": "现有的扩散模型引导方法，特别是注意力扰动，在无分类器指导不适用的场景表现良好，但缺乏确定扰动精确位置的原则性方法，尤其是在DiT架构中，导致质量相关计算分布在各层，无法实现细粒度控制。", "method": "本文调查了注意力扰动的粒度，从层级到单个注意力头，并发现特定的注意力头控制着不同的视觉概念（如结构、风格和纹理质量）。基于此，提出了“HeadHunter”框架，用于迭代选择与用户中心目标对齐的注意力头，实现对生成质量和视觉属性的细粒度控制。此外，引入了SoftPAG，通过将每个选定注意头部的注意力图线性插值到单位矩阵，提供了一个连续的旋钮来调整扰动强度并抑制伪影。", "result": "该方法缓解了现有层级扰动导致的过度平滑问题，并通过组合式头部选择实现了对特定视觉风格的定向操纵。在现代大型基于DiT的文本到图像模型（包括Stable Diffusion 3和FLUX.1）上验证了方法的有效性，在通用质量增强和风格特异性引导方面均表现出卓越性能。本工作首次提供了扩散模型中注意力扰动的头部级分析，揭示了注意力层内的可解释专业化。", "conclusion": "本工作首次对扩散模型中的注意力扰动进行了头部级分析，揭示了注意力层内可解释的专业化，并为有效扰动策略的实际设计提供了可能。", "translation": "扩散模型中最近的引导方法通过扰动模型来构建一个隐式的弱模型，并引导生成远离它，从而引导反向采样。在这些方法中，注意力扰动在无分类器引导不适用的无条件场景中表现出强大的经验性能。然而，现有的注意力扰动方法缺乏确定扰动应施加位置的原则性方法，特别是在质量相关计算分布在各层的扩散Transformer (DiT) 架构中。在本文中，我们研究了注意力扰动的粒度，从层级到单个注意力头，并发现特定的注意力头控制着不同的视觉概念，如结构、风格和纹理质量。基于这一见解，我们提出了“HeadHunter”，一个系统框架，用于迭代选择与用户中心目标对齐的注意力头，从而实现对生成质量和视觉属性的细粒度控制。此外，我们引入了SoftPAG，它将每个选定注意头部的注意力图线性插值到单位矩阵，提供了一个连续的旋钮来调整扰动强度并抑制伪影。我们的方法不仅缓解了现有层级扰动的过度平滑问题，而且通过组合式头部选择实现了对特定视觉风格的定向操纵。我们在现代大型基于DiT的文本到图像模型（包括Stable Diffusion 3和FLUX.1）上验证了我们的方法，在通用质量增强和风格特异性引导方面均表现出卓越性能。我们的工作首次提供了扩散模型中注意力扰动的头部级分析，揭示了注意力层内的可解释专业化，并为有效扰动策略的实际设计提供了可能。", "summary": "本文针对扩散模型中注意力扰动缺乏细粒度控制的问题，特别是DiT架构中扰动位置不明确的挑战，首次提出了头部级注意力扰动分析。研究发现特定注意力头控制不同的视觉概念。基于此，提出了“HeadHunter”框架，通过迭代选择注意力头实现对生成质量和视觉属性的细粒度控制。同时引入SoftPAG以连续调节扰动强度和抑制伪影。实验证明，该方法在提高图像生成质量和实现特定风格引导方面优于现有层级扰动方法，并在Stable Diffusion 3和FLUX.1等模型上表现出色。", "keywords": "注意力扰动, 扩散模型, 细粒度控制, DiT, HeadHunter", "comments": "这篇论文的创新点在于首次将注意力扰动的粒度细化到“注意力头”级别，而不是传统的“层级”。通过发现不同注意力头控制不同视觉概念，并提出“HeadHunter”框架和SoftPAG，实现了对扩散模型生成过程前所未有的细粒度控制。这对于理解扩散模型内部机制、提升生成质量以及实现用户定制化的视觉风格具有重要意义，为未来的可控生成研究开辟了新方向。"}}
{"id": "2506.10980", "title": "InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model", "authors": ["Junqi You", "Chieh Hubert Lin", "Weijie Lyu", "Zhengbo Zhang", "Ming-Hsuan Yang"], "summary": "Recent advances in 3D scene reconstruction enable real-time viewing in\nvirtual and augmented reality. To support interactive operations for better\nimmersiveness, such as moving or editing objects, 3D scene inpainting methods\nare proposed to repair or complete the altered geometry. However, current\napproaches rely on lengthy and computationally intensive optimization, making\nthem impractical for real-time or online applications. We propose InstaInpaint,\na reference-based feed-forward framework that produces 3D-scene inpainting from\na 2D inpainting proposal within 0.4 seconds. We develop a self-supervised\nmasked-finetuning strategy to enable training of our custom large\nreconstruction model (LRM) on the large-scale dataset. Through extensive\nexperiments, we analyze and identify several key designs that improve\ngeneralization, textural consistency, and geometric correctness. InstaInpaint\nachieves a 1000x speed-up from prior methods while maintaining a\nstate-of-the-art performance across two standard benchmarks. Moreover, we show\nthat InstaInpaint generalizes well to flexible downstream applications such as\nobject insertion and multi-region inpainting. More video results are available\nat our project page: https://dhmbb2.github.io/InstaInpaint_page/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10980v1", "AI": {"title_translation": "InstaInpaint：基于掩模大重建模型的即时三维场景修复", "tldr": "InstaInpaint是一个快速的3D场景修复框架，它通过参考式前馈方法和自监督掩模微调策略，实现了比现有方法快1000倍的速度，同时保持了最先进的性能。", "motivation": "当前3D场景修复方法依赖于耗时且计算密集型的优化，使其不适用于实时或在线应用。", "method": "提出InstaInpaint，一个基于参考的前馈框架，能够在0.4秒内从2D修复建议生成3D场景修复。开发了一种自监督掩模微调策略，用于在大规模数据集上训练自定义的大型重建模型（LRM）。", "result": "InstaInpaint比现有方法实现了1000倍的速度提升，同时在两个标准基准上保持了最先进的性能。它还能很好地推广到下游应用，如对象插入和多区域修复。", "conclusion": "InstaInpaint提供了一种高效且高性能的3D场景修复解决方案，克服了现有方法的实时性限制，并展现了良好的泛化能力。", "translation": "近期3D场景重建的进展使得在虚拟和增强现实中实现实时观看成为可能。为了支持更好的沉浸式交互操作，例如移动或编辑物体，提出了3D场景修复方法来修复或完成被改变的几何结构。然而，目前的方法依赖于漫长且计算密集的优化，使其不适用于实时或在线应用。我们提出了InstaInpaint，一个基于参考的前馈框架，能够在0.4秒内从2D修复建议生成3D场景修复。我们开发了一种自监督掩模微调策略，以实现在大规模数据集上训练我们定制的大型重建模型（LRM）。通过广泛的实验，我们分析并确定了几个关键设计，这些设计改善了泛化能力、纹理一致性和几何正确性。InstaInpaint比现有方法实现了1000倍的速度提升，同时在两个标准基准上保持了最先进的性能。此外，我们展示了InstaInpaint能够很好地推广到灵活的下游应用，例如物体插入和多区域修复。更多视频结果可在我们的项目页面获取：https://dhmbb2.github.io/InstaInpaint_page/。", "summary": "本文提出了InstaInpaint，一个用于即时3D场景修复的参考式前馈框架。针对现有方法计算量大、无法实时应用的痛点，InstaInpaint通过自监督掩模微调策略训练大型重建模型，实现了在0.4秒内完成3D场景修复，速度比现有方法快1000倍，并保持了最先进的性能。该方法在泛化能力、纹理一致性和几何正确性方面表现出色，并能应用于物体插入和多区域修复等下游任务。", "keywords": "3D场景修复, 实时, 大型重建模型, 自监督学习, 计算机图形学", "comments": "InstaInpaint的创新之处在于其结合了参考式前馈框架和自监督掩模微调策略，显著提升了3D场景修复的速度，使其首次达到实时应用的要求。其在保持SOTA性能的同时实现1000倍加速，是该领域的重要突破，为VR/AR等交互式应用提供了关键技术支持。"}}
{"id": "2506.10981", "title": "SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis", "authors": ["Weiliang Chen", "Jiayi Bi", "Yuanhui Huang", "Wenzhao Zheng", "Yueqi Duan"], "summary": "Generative models have gained significant attention in novel view synthesis\n(NVS) by alleviating the reliance on dense multi-view captures. However,\nexisting methods typically fall into a conventional paradigm, where generative\nmodels first complete missing areas in 2D, followed by 3D recovery techniques\nto reconstruct the scene, which often results in overly smooth surfaces and\ndistorted geometry, as generative models struggle to infer 3D structure solely\nfrom RGB data. In this paper, we propose SceneCompleter, a novel framework that\nachieves 3D-consistent generative novel view synthesis through dense 3D scene\ncompletion. SceneCompleter achieves both visual coherence and 3D-consistent\ngenerative scene completion through two key components: (1) a\ngeometry-appearance dual-stream diffusion model that jointly synthesizes novel\nviews in RGBD space; (2) a scene embedder that encodes a more holistic scene\nunderstanding from the reference image. By effectively fusing structural and\ntextural information, our method demonstrates superior coherence and\nplausibility in generative novel view synthesis across diverse datasets.\nProject Page: https://chen-wl20.github.io/SceneCompleter", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10981v1", "AI": {"title_translation": "SceneCompleter: 用于生成式新颖视图合成的密集三维场景补全", "tldr": "SceneCompleter提出了一种新的框架，通过密集三维场景补全实现三维一致的生成式新颖视图合成，解决了现有方法在二维补全后三维恢复导致的表面平滑和几何扭曲问题。", "motivation": "现有生成模型在2D补全后进行3D恢复，常导致过于平滑的表面和扭曲的几何形状，因为它们难以仅从RGB数据推断3D结构。", "method": "本文提出了SceneCompleter框架，通过密集三维场景补全实现三维一致的生成式新颖视图合成。它包含两个关键组件：1) 一个几何-外观双流扩散模型，用于在RGBD空间中联合合成新颖视图；2) 一个场景嵌入器，用于从参考图像编码更全面的场景理解。", "result": "SceneCompleter在生成式新颖视图合成中展示了卓越的连贯性和合理性。", "conclusion": "通过有效融合结构和纹理信息，SceneCompleter能够实现视觉连贯且三维一致的生成式场景补全，并在多样化数据集上表现出优越的性能。", "translation": "生成模型通过减轻对密集多视图捕获的依赖，在新颖视图合成（NVS）中获得了显著关注。然而，现有方法通常遵循传统范式，即生成模型首先在2D中补全缺失区域，然后通过3D恢复技术重建场景，这常常导致过于平滑的表面和扭曲的几何形状，因为生成模型难以仅从RGB数据推断3D结构。在本文中，我们提出了SceneCompleter，一个新颖的框架，通过密集三维场景补全实现三维一致的生成式新颖视图合成。SceneCompleter通过两个关键组件实现视觉连贯和三维一致的生成式场景补全：(1) 一个几何-外观双流扩散模型，用于在RGBD空间中联合合成新颖视图；(2) 一个场景嵌入器，用于从参考图像编码更全面的场景理解。通过有效融合结构和纹理信息，我们的方法在多样化数据集上展示了生成式新颖视图合成的卓越连贯性和合理性。项目页面：https://chen-wl20.github.io/SceneCompleter", "summary": "SceneCompleter是一个新颖的框架，旨在通过密集的三维场景补全实现三维一致的生成式新颖视图合成。针对现有方法在2D补全后进行3D恢复导致几何失真和表面平滑的问题，SceneCompleter引入了几何-外观双流扩散模型在RGBD空间联合合成视图，并利用场景嵌入器从参考图像获取全面的场景理解。该方法有效融合结构和纹理信息，在新颖视图合成中展现出卓越的连贯性和合理性。", "keywords": "三维场景补全, 新颖视图合成, 生成模型, 扩散模型, RGBD", "comments": "该论文通过提出直接在3D空间进行场景补全的新范式，解决了传统2D补全再3D恢复方法导致的几何失真问题，具有重要的创新性。其双流扩散模型和场景嵌入器的结合，有效提升了生成视图的3D一致性和视觉质量。"}}
{"id": "2506.10031", "title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data", "authors": ["Olga Ovcharenko", "Florian Barkmann", "Philip Toma", "Imant Daunhawer", "Julia Vogt", "Sebastian Schelter", "Valentina Boeva"], "summary": "Self-supervised learning (SSL) has proven to be a powerful approach for\nextracting biologically meaningful representations from single-cell data. To\nadvance our understanding of SSL methods applied to single-cell data, we\npresent scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL\nmethods. Our evaluation spans nine datasets and focuses on three common\ndownstream tasks: batch correction, cell type annotation, and missing modality\nprediction. Furthermore, we systematically assess various data augmentation\nstrategies. Our analysis reveals task-specific trade-offs: the specialized\nsingle-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at\nuni-modal batch correction, while generic SSL methods, such as VICReg and\nSimCLR, demonstrate superior performance in cell typing and multi-modal data\nintegration. Random masking emerges as the most effective augmentation\ntechnique across all tasks, surpassing domain-specific augmentations. Notably,\nour results indicate the need for a specialized single-cell multi-modal data\nintegration framework. scSSL-Bench provides a standardized evaluation platform\nand concrete recommendations for applying SSL to single-cell analysis,\nadvancing the convergence of deep learning and single-cell genomics.", "comment": "Accepted at ICML 2025 (Spotlight)", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.10031v1", "AI": {"title_translation": "scSSL-Bench：单细胞数据自监督学习基准测试", "tldr": "scSSL-Bench是一个综合性的基准测试平台，评估了19种自监督学习（SSL）方法在9个单细胞数据集上的表现，涵盖了批次校正、细胞类型注释和缺失模态预测等任务，并提供了具体的推荐。", "motivation": "为了加深对应用于单细胞数据的自监督学习（SSL）方法的理解，本文提出了scSSL-Bench。", "method": "scSSL-Bench是一个综合性的基准测试平台，评估了19种自监督学习方法。评估范围涵盖了9个数据集，并专注于三个常见的下游任务：批次校正、细胞类型注释和缺失模态预测。此外，还系统地评估了各种数据增强策略。", "result": "分析揭示了任务特定的权衡：专门的单细胞框架（scVI、CLAIRE和微调后的scGPT）在单模态批次校正方面表现出色，而通用SSL方法（如VICReg和SimCLR）在细胞分型和多模态数据整合方面表现更优。随机掩蔽是所有任务中最有效的数据增强技术，超越了领域特定的增强方法。值得注意的是，结果表明需要一个专门的单细胞多模态数据整合框架。", "conclusion": "scSSL-Bench提供了一个标准化的评估平台和将SSL应用于单细胞分析的具体建议，推动了深度学习和单细胞基因组学的融合。", "translation": "自监督学习（SSL）已被证明是一种从单细胞数据中提取具有生物学意义的表示的强大方法。为了增进我们对应用于单细胞数据的SSL方法的理解，我们提出了scSSL-Bench，这是一个评估19种SSL方法的综合基准测试平台。我们的评估涵盖了9个数据集，并专注于三个常见的下游任务：批次校正、细胞类型注释和缺失模态预测。此外，我们系统地评估了各种数据增强策略。我们的分析揭示了任务特定的权衡：专门的单细胞框架，如scVI、CLAIRE和经过微调的scGPT，在单模态批次校正方面表现出色，而通用SSL方法，如VICReg和SimCLR，在细胞分型和多模态数据整合方面表现更优。随机掩蔽成为所有任务中最有效的数据增强技术，超越了领域特定的增强方法。值得注意的是，我们的结果表明需要一个专门的单细胞多模态数据整合框架。scSSL-Bench提供了一个标准化的评估平台和将SSL应用于单细胞分析的具体建议，推动了深度学习和单细胞基因组学的融合。", "summary": "scSSL-Bench是一个综合性基准测试平台，旨在评估自监督学习（SSL）方法在单细胞数据上的性能。它分析了19种SSL方法在9个数据集上的表现，并聚焦于批次校正、细胞类型注释和缺失模态预测等下游任务。研究发现，专业单细胞框架在单模态批次校正上表现优异，而通用SSL方法在细胞分型和多模态整合上更出色。随机掩蔽被认为是最佳的数据增强技术。该研究强调了对专门单细胞多模态数据整合框架的需求，并为SSL在单细胞分析中的应用提供了标准化评估和具体建议。", "keywords": "自监督学习, 单细胞数据, 基准测试, 数据增强, 批次校正", "comments": "该论文通过提供一个全面的基准测试平台scSSL-Bench，对单细胞数据领域的自监督学习方法进行了系统性评估，具有重要的创新性。它不仅评估了多种现有方法，还深入分析了数据增强策略的效果，并揭示了不同方法在特定任务上的优势和劣势。其提供的具体建议和指出的未来研究方向（如专门的多模态整合框架）对于推动深度学习与单细胞基因组学的融合具有重要意义。"}}
{"id": "2506.10073", "title": "Patient-Specific Deep Reinforcement Learning for Automatic Replanning in Head-and-Neck Cancer Proton Therapy", "authors": ["Malvern Madondo", "Yuan Shao", "Yingzi Liu", "Jun Zhou", "Xiaofeng Yang", "Zhen Tian"], "summary": "Anatomical changes during intensity-modulated proton therapy (IMPT) for\nhead-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and\norgan-at-risk overdosing. As a result, treatment replanning is often required\nto maintain clinically acceptable treatment quality. However, current manual\nreplanning processes are resource-intensive and time-consuming. We propose a\npatient-specific deep reinforcement learning (DRL) framework for automated IMPT\nreplanning, with a reward-shaping mechanism based on a $150$-point plan quality\nscore addressing competing clinical objectives. We formulate the planning\nprocess as an RL problem where agents learn control policies to adjust\noptimization priorities, maximizing plan quality. Unlike population-based\napproaches, our framework trains personalized agents for each patient using\ntheir planning CT (Computed Tomography) and augmented anatomies simulating\nanatomical changes (tumor progression and regression). This patient-specific\napproach leverages anatomical similarities throughout treatment, enabling\neffective plan adaptation. We implemented two DRL algorithms, Deep Q-Network\nand Proximal Policy Optimization, using dose-volume histograms (DVHs) as state\nrepresentations and a $22$-dimensional action space of priority adjustments.\nEvaluation on five HNC patients using actual replanning CT data showed both DRL\nagents improved initial plan scores from $120.63 \\pm 21.40$ to $139.78 \\pm\n6.84$ (DQN) and $142.74 \\pm 5.16$ (PPO), surpassing manual replans generated by\na human planner ($137.20 \\pm 5.58$). Clinical validation confirms that\nimprovements translate to better tumor coverage and OAR sparing across diverse\nanatomical changes. This work demonstrates DRL's potential in addressing\ngeometric and dosimetric complexities of adaptive proton therapy, offering\nefficient offline adaptation solutions and advancing online adaptive proton\ntherapy.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.10073v1", "AI": {"title_translation": "头颈部癌症质子治疗中患者特异性深度强化学习用于自动再计划", "tldr": "该研究提出了一种患者特异性的深度强化学习框架，用于头颈部癌症质子治疗中的自动再计划，通过学习调整优化优先级来提高计划质量，并显示出优于手动再计划的性能。", "motivation": "头颈部癌症的调强质子治疗过程中，解剖结构变化可能导致布拉格峰位移，增加肿瘤欠剂量和危及器官过剂量的风险，因此需要再计划。然而，目前的手动再计划过程资源密集且耗时。", "method": "本研究提出了一个患者特异性的深度强化学习(DRL)框架，用于自动化调强质子治疗再计划。该框架采用基于150点计划质量评分的奖励塑形机制，将计划过程表述为RL问题，智能体学习调整优化优先级。与基于人群的方法不同，该框架为每位患者训练个性化智能体，利用其计划CT和模拟解剖变化的增强解剖结构。研究实现了两种DRL算法：深度Q网络(DQN)和近端策略优化(PPO)，使用剂量体积直方图(DVH)作为状态表示，并采用22维的优先级调整动作空间。", "result": "在五名头颈部癌症患者的实际再计划CT数据上进行评估，结果显示两种DRL智能体均将初始计划分数从120.63 ± 21.40提高到139.78 ± 6.84 (DQN) 和 142.74 ± 5.16 (PPO)，超过了人类规划师生成的手动再计划分数 (137.20 ± 5.58)。临床验证证实，这些改进转化为更好的肿瘤覆盖和危及器官保护，适用于各种解剖结构变化。", "conclusion": "这项工作展示了深度强化学习在解决自适应质子治疗的几何和剂量学复杂性方面的潜力，提供了高效的离线适应解决方案，并推动了在线自适应质子治疗的发展。", "translation": "在头颈部癌症(HNC)的调强质子治疗(IMPT)期间，解剖结构变化可能导致布拉格峰位移，从而有肿瘤欠剂量和危及器官过剂量的风险。因此，通常需要治疗再计划以保持临床可接受的治疗质量。然而，目前的手动再计划过程资源密集且耗时。我们提出了一种患者特异性的深度强化学习(DRL)框架，用于自动化IMPT再计划，该框架具有基于150点计划质量评分的奖励塑形机制，以解决相互竞争的临床目标。我们将计划过程表述为一个RL问题，其中智能体学习控制策略以调整优化优先级，从而最大化计划质量。与基于人群的方法不同，我们的框架使用每位患者的计划CT(计算机断层扫描)和模拟解剖变化的增强解剖结构(肿瘤进展和退化)来训练个性化智能体。这种患者特异性的方法利用了整个治疗过程中的解剖相似性，从而实现了有效的计划适应。我们使用剂量体积直方图(DVHs)作为状态表示和22维的优先级调整动作空间，实现了两种DRL算法：深度Q网络和近端策略优化。对五名HNC患者使用实际再计划CT数据进行的评估显示，两种DRL智能体都将初始计划分数从120.63 ± 21.40提高到139.78 ± 6.84 (DQN) 和 142.74 ± 5.16 (PPO)，超过了人类规划师生成的手动再计划分数(137.20 ± 5.58)。临床验证证实，这些改进转化为更好的肿瘤覆盖和危及器官保护，适用于各种解剖结构变化。这项工作展示了DRL在解决自适应质子治疗的几何和剂量学复杂性方面的潜力，提供了高效的离线适应解决方案，并推动了在线自适应质子治疗的发展。", "summary": "本研究提出了一种创新的患者特异性深度强化学习(DRL)框架，旨在自动化头颈部癌症质子治疗中的再计划过程。鉴于解剖结构变化导致的手动再计划耗时且资源密集，该框架将治疗计划制定为强化学习问题，智能体学习调整优化优先级以最大化基于150点评分的计划质量。与传统方法不同，该模型为每位患者训练个性化智能体，利用其CT影像和模拟解剖变化的增强数据。实验结果表明，该DRL方法（DQN和PPO）在五名患者中显著提高了计划质量分数，且优于人类规划师的手动再计划。临床验证进一步证实了其在肿瘤覆盖和危及器官保护方面的有效性，展示了DRL在自适应质子治疗中的巨大潜力。", "keywords": "深度强化学习, 质子治疗, 自动再计划, 头颈部癌症, 患者特异性", "comments": "这项研究的创新之处在于提出了一个患者特异性的深度强化学习框架来自动化质子治疗的再计划过程，这在很大程度上解决了当前手动再计划耗时且效率低下的问题。其独特之处在于为每位患者训练个性化智能体，利用患者自身的解剖数据进行学习，这比传统的基于人群的方法更具适应性。研究结果表明，该方法不仅提高了计划质量，甚至超越了人类规划师的水平，这对于临床实践具有重要意义，预示着未来自适应质子治疗的效率和精准度将得到显著提升。同时，它也为在线自适应质子治疗的发展奠定了基础。"}}
{"id": "2506.10460", "title": "Equitable Mechanism Design for Facility Location", "authors": ["Toby Walsh"], "summary": "We consider strategy proof mechanisms for facility location which maximize\nequitability between agents. As is common in the literature, we measure\nequitability with the Gini index. We first prove a simple but fundamental\nimpossibility result that no strategy proof mechanism can bound the\napproximation ratio of the optimal Gini index of utilities for one or more\nfacilities. We propose instead computing approximation ratios of the\ncomplemented Gini index of utilities, and consider how well both deterministic\nand randomized mechanisms approximate this. In addition, as Nash welfare is\noften put forwards as an equitable compromise between egalitarian and\nutilitarian outcomes, we consider how well mechanisms approximate the Nash\nwelfare.", "comment": "To appear in Proceedings of IJCAI 2025", "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.10460v1", "AI": {"title_translation": "设施选址的公平机制设计", "tldr": "本文研究了设施选址中最大化代理人之间公平性的策略证明机制，证明了在传统基尼指数下无法界定最优近似比的不可能性结果，并提出使用补足基尼指数和纳什福利作为新的衡量标准。", "motivation": "在设施选址问题中，研究者旨在设计策略证明机制，以最大化代理人之间的公平性。传统上以基尼指数衡量公平性，但存在无法界定近似比的问题，因此需要探索新的衡量方法和机制设计。", "method": "本文首先证明了一个不可能性结果，即没有策略证明机制可以界定一个或多个设施效用最优基尼指数的近似比。作为替代，提出计算效用补足基尼指数的近似比，并考虑确定性和随机机制如何近似它。此外，还考虑了机制如何近似纳什福利。", "result": "证明了一个简单但根本性的不可能性结果：对于一个或多个设施的效用最优基尼指数，没有策略证明机制能够界定其近似比。提出了使用效用补足基尼指数的近似比进行计算，并探讨了确定性和随机机制对其的近似效果。同时，也考虑了机制对纳什福利的近似效果。", "conclusion": "在设施选址的策略证明机制中，直接优化基尼指数存在根本性的近似比界定不可能性。因此，提出并探索了补足基尼指数和纳什福利作为衡量公平性的替代方法，为未来公平机制设计提供了新的方向。", "translation": "我们考虑了设施选址的策略证明机制，旨在最大化代理人之间的公平性。正如文献中常见的，我们使用基尼指数来衡量公平性。我们首先证明了一个简单但根本性的不可能性结果，即没有策略证明机制能够界定一个或多个设施效用最优基尼指数的近似比。我们转而建议计算效用补足基尼指数的近似比，并考虑确定性和随机机制如何很好地近似它。此外，由于纳什福利常被认为是平均主义和功利主义结果之间的一种公平折衷，我们还考虑了机制如何很好地近似纳什福利。", "summary": "本研究探讨了设施选址中的策略证明机制，旨在最大化代理人间的公平性。论文指出，使用传统基尼指数衡量公平性时存在一个根本性的不可能性结果，即无法界定最优近似比。为克服此限制，作者提出转而计算效用补足基尼指数的近似比，并分析了确定性和随机机制在此方面的表现。此外，研究还评估了机制对纳什福利的近似效果，将其作为一种公平折衷的衡量标准。", "keywords": "策略证明机制, 设施选址, 基尼指数, 公平性, 纳什福利", "comments": "本文的创新之处在于揭示了在设施选址的策略证明机制中，以基尼指数衡量公平性时存在一个重要的不可能性结果，这挑战了现有的一些假设。同时，提出使用补足基尼指数和纳什福利作为替代衡量标准，为解决公平性问题提供了新的视角和研究方向，具有重要的理论意义。"}}
{"id": "2506.10101", "title": "Fundamental Limits of Learning High-dimensional Simplices in Noisy Regimes", "authors": ["Seyed Amir Hossein Saberi", "Amir Najafi", "Abolfazl Motahari", "Babak H. khalaj"], "summary": "In this paper, we establish sample complexity bounds for learning\nhigh-dimensional simplices in $\\mathbb{R}^K$ from noisy data. Specifically, we\nconsider $n$ i.i.d. samples uniformly drawn from an unknown simplex in\n$\\mathbb{R}^K$, each corrupted by additive Gaussian noise of unknown variance.\nWe prove an algorithm exists that, with high probability, outputs a simplex\nwithin $\\ell_2$ or total variation (TV) distance at most $\\varepsilon$ from the\ntrue simplex, provided $n \\ge (K^2/\\varepsilon^2)\ne^{\\mathcal{O}(K/\\mathrm{SNR}^2)}$, where $\\mathrm{SNR}$ is the signal-to-noise\nratio. Extending our prior work~\\citep{saberi2023sample}, we derive new\ninformation-theoretic lower bounds, showing that simplex estimation within TV\ndistance $\\varepsilon$ requires at least $n \\ge \\Omega(K^3\n\\sigma^2/\\varepsilon^2 + K/\\varepsilon)$ samples, where $\\sigma^2$ denotes the\nnoise variance. In the noiseless scenario, our lower bound $n \\ge\n\\Omega(K/\\varepsilon)$ matches known upper bounds up to constant factors. We\nresolve an open question by demonstrating that when $\\mathrm{SNR} \\ge\n\\Omega(K^{1/2})$, noisy-case complexity aligns with the noiseless case. Our\nanalysis leverages sample compression techniques (Ashtiani et al., 2018) and\nintroduces a novel Fourier-based method for recovering distributions from noisy\nobservations, potentially applicable beyond simplex learning.", "comment": "Extension of our ICML 2023 paper, 44 pages", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10101v1", "AI": {"title_translation": "噪声环境下高维单纯形学习的根本限制", "tldr": "本文研究了在噪声数据下学习高维单纯形的样本复杂度，推导了上下界，并证明在特定信噪比下，噪声情况下的复杂度与无噪声情况一致，同时引入了一种新的傅里叶方法。", "motivation": "本文旨在建立从噪声数据中学习高维单纯形的样本复杂度界限，以理解在高维和噪声环境下学习的根本限制。", "method": "研究人员证明了一个算法的存在性，该算法能够以高概率从噪声数据中恢复出与真实单纯形距离在$\\ell_2$或全变差（TV）距离内误差不超过$\\varepsilon$的单纯形。他们还推导了新的信息论下界，并利用样本压缩技术和一种新颖的基于傅里叶的方法来从噪声观测中恢复分布。", "result": "结果表明，存在一个算法，在样本量$n \\ge (K^2/\\varepsilon^2) e^{\\mathcal{O}(K/\\mathrm{SNR}^2)}$时，能以高概率输出一个与真实单纯形距离在$\\varepsilon$内的单纯形。信息论下界显示，在TV距离$\\varepsilon$内估计单纯形至少需要$n \\ge \\Omega(K^3 \\sigma^2/\\varepsilon^2 + K/\\varepsilon)$个样本。在无噪声情况下，下界$n \\ge \\Omega(K/\\varepsilon)$与已知上界匹配。研究还发现，当$\\mathrm{SNR} \\ge \\Omega(K^{1/2})$时，噪声情况下的复杂度与无噪声情况一致。", "conclusion": "本文解决了高维单纯形学习中的一个开放问题，证明了在足够高的信噪比下，噪声情况下的学习复杂度与无噪声情况相同。此外，引入的傅里叶方法具有超越单纯形学习的潜在应用。", "translation": "在本文中，我们建立了从噪声数据中学习$\\mathbb{R}^K$中高维单纯形的样本复杂度界限。具体来说，我们考虑了$n$个独立同分布的样本，这些样本均匀地从$\\mathbb{R}^K$中一个未知单纯形中抽取，每个样本都受到未知方差的加性高斯噪声污染。我们证明存在一个算法，它能以高概率输出一个与真实单纯形在$\\ell_2$或全变差（TV）距离上至多$\\varepsilon$的单纯形，前提是$n \\ge (K^2/\\varepsilon^2) e^{\\mathcal{O}(K/\\mathrm{SNR}^2)}$，其中$\\mathrm{SNR}$是信噪比。扩展我们之前的工作\\citep{saberi2023sample}，我们推导了新的信息论下界，表明在TV距离$\\varepsilon$内估计单纯形至少需要$n \\ge \\Omega(K^3 \\sigma^2/\\varepsilon^2 + K/\\varepsilon)$个样本，其中$\\sigma^2$表示噪声方差。在无噪声情况下，我们的下界$n \\ge \\Omega(K/\\varepsilon)$与已知的上界在常数因子内匹配。我们通过证明当$\\mathrm{SNR} \\ge \\Omega(K^{1/2})$时，噪声情况下的复杂度与无噪声情况一致，从而解决了一个开放问题。我们的分析利用了样本压缩技术（Ashtiani et al., 2018），并引入了一种新颖的基于傅里叶的方法，用于从噪声观测中恢复分布，这可能适用于单纯形学习之外的领域。", "summary": "本文研究了在加性高斯噪声下从高维单纯形中均匀抽取的独立同分布样本的学习问题。研究人员建立了学习高维单纯形的样本复杂度上界和信息论下界，并证明了在特定信噪比条件下，噪声情况下的复杂度与无噪声情况一致，从而解决了一个开放问题。此外，论文还引入了一种新颖的基于傅里叶的方法，用于从噪声观测中恢复分布，该方法具有广泛的应用潜力。", "keywords": "高维单纯形, 样本复杂度, 噪声学习, 信息论下界, 傅里叶方法", "comments": "这篇论文通过推导严格的样本复杂度界限，对噪声环境下高维单纯形学习的理论基础做出了重要贡献。特别是，它解决了噪声与无噪声复杂度对齐的开放问题，并引入了一种新颖的傅里叶方法，该方法可能在其他数据恢复任务中找到应用，显示出其创新性和重要性。"}}
{"id": "2506.10141", "title": "Diffusion prior as a direct regularization term for FWI", "authors": ["Yuke Xie", "Hervé Chauris", "Nicolas Desassis"], "summary": "Diffusion models have recently shown promise as powerful generative priors\nfor inverse problems. However, conventional applications require solving the\nfull reverse diffusion process and operating on noisy intermediate states,\nwhich poses challenges for physics-constrained computational seismic imaging.\nIn particular, such instability is pronounced in non-linear solvers like those\nused in Full Waveform Inversion (FWI), where wave propagation through noisy\nvelocity fields can lead to numerical artifacts and poor inversion quality. In\nthis work, we propose a simple yet effective framework that directly integrates\na pretrained Denoising Diffusion Probabilistic Model (DDPM) as a score-based\ngenerative diffusion prior into FWI through a score rematching strategy. Unlike\ntraditional diffusion approaches, our method avoids the reverse diffusion\nsampling and needs fewer iterations. We operate the image inversion entirely in\nthe clean image space, eliminating the need to operate through noisy velocity\nmodels. The generative diffusion prior can be introduced as a simple\nregularization term in the standard FWI update rule, requiring minimal\nmodification to existing FWI pipelines. This promotes stable wave propagation\nand can improve convergence behavior and inversion quality. Numerical\nexperiments suggest that the proposed method offers enhanced fidelity and\nrobustness compared to conventional and GAN-based FWI approaches, while\nremaining practical and computationally efficient for seismic imaging and other\ninverse problem tasks.", "comment": null, "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.10141v1", "AI": {"title_translation": "扩散先验作为全波形反演的直接正则化项", "tldr": "本文提出将扩散模型作为全波形反演（FWI）的直接正则化项，避免了传统扩散方法中的噪声中间状态和反向扩散过程，从而提高了FWI的稳定性和效率。", "motivation": "传统的扩散模型在逆问题中应用时，需要解决完整的反向扩散过程并在有噪声的中间状态下操作，这在全波形反演（FWI）等非线性求解器中会导致不稳定性、数值伪影和较差的反演质量。", "method": "提出通过分数匹配策略，将预训练的去噪扩散概率模型（DDPM）作为基于分数的生成扩散先验直接集成到FWI中。该方法避免了反向扩散采样，减少了迭代次数，并在干净图像空间中进行操作，将生成扩散先验作为标准FWI更新规则中的简单正则化项。", "result": "数值实验表明，与传统和基于GAN的FWI方法相比，所提出的方法提供了增强的保真度和鲁棒性，同时在地震成像和其他逆问题任务中保持实用和计算效率。", "conclusion": "本文提出的方法将扩散先验作为直接正则化项有效地集成到FWI中，显著改善了波传播的稳定性、收敛行为和反演质量，为地震成像及其他逆问题提供了更优的解决方案。", "translation": "扩散模型最近在逆问题中显示出作为强大生成先验的潜力。然而，传统应用需要解决完整的反向扩散过程并在有噪声的中间状态下操作，这给受物理约束的计算地震成像带来了挑战。特别是，这种不稳定性在全波形反演（FWI）等非线性求解器中尤为明显，其中通过噪声速度场的波传播可能导致数值伪影和较差的反演质量。在这项工作中，我们提出了一种简单而有效的框架，通过分数匹配策略将预训练的去噪扩散概率模型（DDPM）作为基于分数的生成扩散先验直接集成到FWI中。与传统扩散方法不同，我们的方法避免了反向扩散采样，并且需要的迭代次数更少。我们在干净图像空间中完全进行图像反演，无需通过噪声速度模型操作。生成扩散先验可以作为标准FWI更新规则中的一个简单正则化项引入，对现有FWI管道的修改最小。这促进了稳定的波传播，并可以改善收敛行为和反演质量。数值实验表明，与传统和基于GAN的FWI方法相比，所提出的方法提供了增强的保真度和鲁棒性，同时在地震成像和其他逆问题任务中保持实用和计算效率。", "summary": "本文提出一种新颖的框架，将预训练的去噪扩散概率模型（DDPM）作为直接正则化项集成到全波形反演（FWI）中。与传统扩散方法不同，该方法避免了复杂的反向扩散过程和在噪声中间空间中的操作，而是在干净图像空间中进行。通过将扩散先验作为FWI更新规则中的简单正则化项，该方法增强了波传播的稳定性，改善了收敛性，并产生了更优的反演质量。数值实验表明，与现有FWI技术相比，其在保真度、鲁棒性、实用性和计算效率方面均有所提升。", "keywords": "扩散模型, 全波形反演, 正则化, 生成先验, 地震成像", "comments": "本文的创新之处在于将扩散先验直接用作干净图像空间中的正则化项，从而绕过了计算密集且不稳定的反向扩散过程和噪声中间状态。这简化了与现有FWI管道的集成，同时提高了稳定性和性能，是生成模型应用于物理约束逆问题的重要一步。"}}
{"id": "2506.10153", "title": "Attention on flow control: transformer-based reinforcement learning for lift regulation in highly disturbed flows", "authors": ["Zhecheng Liu", "Jeff D. Eldredge"], "summary": "A linear flow control strategy designed for weak disturbances may not remain\neffective in sequences of strong disturbances due to nonlinear interactions,\nbut it is sensible to leverage it for developing a better strategy. In the\npresent study, we propose a transformer-based reinforcement learning (RL)\nframework to learn an effective control strategy for regulating aerodynamic\nlift in gust sequences via pitch control. The transformer addresses the\nchallenge of partial observability from limited surface pressure sensors. We\ndemonstrate that the training can be accelerated with two techniques --\npretraining with an expert policy (here, linear control) and task-level\ntransfer learning (here, extending a policy trained on isolated gusts to\nmultiple gusts). We show that the learned strategy outperforms the best\nproportional control, with the performance gap widening as the number of gusts\nincreases. The control strategy learned in an environment with a small number\nof successive gusts is shown to effectively generalize to an environment with\nan arbitrarily long sequence of gusts. We investigate the pivot configuration\nand show that quarter-chord pitching control can achieve superior lift\nregulation with substantially less control effort compared to mid-chord\npitching control. Through a decomposition of the lift, we attribute this\nadvantage to the dominant added-mass contribution accessible via quarter-chord\npitching. The success on multiple configurations shows the generalizability of\nthe proposed transformer-based RL framework, which offers a promising approach\nto solve more computationally demanding flow control problems when combined\nwith the proposed acceleration techniques.", "comment": null, "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.10153v1", "AI": {"title_translation": "流体控制中的注意力机制：基于Transformer的强化学习用于强扰流中的升力调节", "tldr": "本文提出了一种基于Transformer的强化学习框架，通过俯仰控制在强扰流中调节气动升力。该方法优于传统控制，并具有良好的泛化能力，同时通过预训练和迁移学习加速训练。", "motivation": "针对弱扰动设计的线性流体控制策略在强扰动序列中可能失效，因为存在非线性相互作用。因此，需要开发一种更有效的策略。", "method": "本文提出了一种基于Transformer的强化学习（RL）框架，通过俯仰控制来学习调节阵风序列中的气动升力。Transformer用于处理有限表面压力传感器导致的局部可观察性挑战。训练通过专家策略（线性控制）预训练和任务级迁移学习（将针对单一阵风训练的策略扩展到多个阵风）两种技术加速。研究了枢轴配置，并分解了升力以分析优势。", "result": "学习到的策略优于最佳比例控制，且随着阵风数量增加，性能差距扩大。在少量连续阵风环境中学习到的控制策略能有效泛化到任意长序列的阵风环境。四分之一弦长俯仰控制比中弦长俯仰控制能以更少的控制力实现更优的升力调节，这归因于通过四分之一弦长俯仰可获得的主导附加质量贡献。", "conclusion": "所提出的基于Transformer的强化学习框架及其加速技术，在多种配置下表现出良好的泛化性，为解决计算要求更高的流体控制问题提供了一种有前景的方法。", "translation": "为弱扰动设计的线性流体控制策略在强扰动序列中可能因非线性相互作用而失效，但利用其来开发更好的策略是明智的。在本研究中，我们提出了一种基于Transformer的强化学习（RL）框架，以通过俯仰控制学习在阵风序列中调节气动升力的有效控制策略。Transformer解决了有限表面压力传感器导致的局部可观察性挑战。我们证明了通过两种技术可以加速训练——使用专家策略（此处为线性控制）进行预训练和任务级迁移学习（此处为将针对孤立阵风训练的策略扩展到多个阵风）。我们表明，学习到的策略优于最佳比例控制，并且随着阵风数量的增加，性能差距扩大。在少量连续阵风环境中学习到的控制策略被证明可以有效泛化到具有任意长序列阵风的环境中。我们研究了枢轴配置，并表明与中弦长俯仰控制相比，四分之一弦长俯仰控制可以以显著更少的控制力实现更优的升力调节。通过升力分解，我们将这一优势归因于通过四分之一弦长俯仰可获得的主导附加质量贡献。在多种配置上的成功表明了所提出的基于Transformer的RL框架的通用性，当与所提出的加速技术结合时，它为解决计算要求更高的流体控制问题提供了一种有前景的方法。", "summary": "本研究提出了一种基于Transformer的强化学习（RL）框架，用于通过俯仰控制在强扰流（阵风序列）中有效地调节气动升力。该框架通过Transformer处理有限传感器输入带来的局部可观察性问题。为加速训练，研究采用了专家策略预训练和任务级迁移学习。实验结果表明，该学习策略优于传统比例控制，且在阵风数量增加时性能优势更显著，并能有效泛化到长序列阵风环境。此外，研究发现四分之一弦长俯仰控制比中弦长俯仰控制能以更小的控制代价实现更优的升力调节。该方法具有良好的通用性，有望解决更复杂的流体控制问题。", "keywords": "强化学习, Transformer, 流体控制, 升力调节, 俯仰控制", "comments": "本文创新性地将Transformer模型引入强化学习框架，以解决流体控制中局部可观察性问题，并结合预训练和迁移学习显著加速了训练过程。其在强扰动流体控制领域的应用展示了强化学习的巨大潜力，尤其是在泛化能力和控制效率方面的提升。对不同枢轴配置的深入分析也为实际应用提供了有价值的指导。该研究为未来解决更复杂、计算量更大的流体控制问题奠定了基础。"}}
{"id": "2506.10558", "title": "StepProof: Step-by-step verification of natural language mathematical proofs", "authors": ["Xiaolin Hu", "Qinghua Zhou", "Bogdan Grechuk", "Ivan Y. Tyukin"], "summary": "Interactive theorem provers (ITPs) are powerful tools for the formal\nverification of mathematical proofs down to the axiom level. However, their\nlack of a natural language interface remains a significant limitation. Recent\nadvancements in large language models (LLMs) have enhanced the understanding of\nnatural language inputs, paving the way for autoformalization - the process of\ntranslating natural language proofs into formal proofs that can be verified.\nDespite these advancements, existing autoformalization approaches are limited\nto verifying complete proofs and lack the capability for finer, sentence-level\nverification. To address this gap, we propose StepProof, a novel\nautoformalization method designed for granular, step-by-step verification.\nStepProof breaks down complete proofs into multiple verifiable subproofs,\nenabling sentence-level verification. Experimental results demonstrate that\nStepProof significantly improves proof success rates and efficiency compared to\ntraditional methods. Additionally, we found that minor manual adjustments to\nthe natural language proofs, tailoring them for step-level verification,\nfurther enhanced StepProof's performance in autoformalization.", "comment": null, "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.10558v1", "AI": {"title_translation": "StepProof：自然语言数学证明的逐步验证", "tldr": "StepProof是一种新的自动形式化方法，通过将自然语言数学证明分解为可验证的子证明，实现了细粒度、逐句的验证，显著提高了证明成功率和效率。", "motivation": "交互式定理证明器（ITPs）缺乏自然语言接口是一个重要限制。尽管大型语言模型（LLMs）增强了对自然语言输入的理解，但现有的自动形式化方法仅限于验证完整的证明，无法进行更细致的句子级别验证。", "method": "本文提出了StepProof，一种新颖的自动形式化方法，专为细粒度、逐步验证而设计。StepProof将完整的证明分解为多个可验证的子证明，从而实现了句子级别的验证。", "result": "实验结果表明，与传统方法相比，StepProof显著提高了证明成功率和效率。此外，对自然语言证明进行少量手动调整以适应步骤级验证，可以进一步提升StepProof在自动形式化方面的性能。", "conclusion": "StepProof通过实现对自然语言数学证明的细粒度、逐步验证，有效解决了现有自动形式化方法在验证粒度上的局限性，并显著提升了验证性能。", "translation": "交互式定理证明器（ITPs）是强大的工具，可以将数学证明形式化验证到公理层面。然而，它们缺乏自然语言接口仍然是一个显著的限制。大型语言模型（LLMs）的最新进展增强了对自然语言输入的理解，为自动形式化铺平了道路——即将自然语言证明翻译成可以验证的形式化证明。尽管有这些进展，现有的自动形式化方法仅限于验证完整的证明，并且缺乏更细致的、句子级别的验证能力。为了解决这个空白，我们提出了StepProof，一种新颖的自动形式化方法，专为细粒度、逐步验证而设计。StepProof将完整的证明分解为多个可验证的子证明，从而实现了句子级别的验证。实验结果表明，与传统方法相比，StepProof显著提高了证明成功率和效率。此外，我们发现对自然语言证明进行少量手动调整，使其适应步骤级验证，进一步提升了StepProof在自动形式化方面的性能。", "summary": "本文提出了一种名为StepProof的新型自动形式化方法，旨在解决现有方法在自然语言数学证明验证中缺乏细粒度、句子级别验证的问题。StepProof通过将完整证明分解为多个可验证的子证明来实现逐句验证。实验结果表明，StepProof显著提高了证明的成功率和效率，并且通过少量手动调整可进一步提升性能。", "keywords": "StepProof, 自动形式化, 自然语言证明, 逐步验证, 交互式定理证明器", "comments": "StepProof的创新之处在于其实现了自然语言数学证明的细粒度、逐句验证，这弥补了现有自动形式化方法只能验证完整证明的不足。通过将复杂证明分解为更小的可管理单元，它提高了验证的效率和成功率，为将自然语言与形式化证明系统结合迈出了重要一步。"}}
{"id": "2506.10168", "title": "Momentum Multi-Marginal Schrödinger Bridge Matching", "authors": ["Panagiotis Theodoropoulos", "Augustinos D. Saravanos", "Evangelos A. Theodorou", "Guan-Horng Liu"], "summary": "Understanding complex systems by inferring trajectories from sparse sample\nsnapshots is a fundamental challenge in a wide range of domains, e.g.,\nsingle-cell biology, meteorology, and economics. Despite advancements in Bridge\nand Flow matching frameworks, current methodologies rely on pairwise\ninterpolation between adjacent snapshots. This hinders their ability to capture\nlong-range temporal dependencies and potentially affects the coherence of the\ninferred trajectories. To address these issues, we introduce \\textbf{Momentum\nMulti-Marginal Schr\\\"odinger Bridge Matching (3MSBM)}, a novel matching\nframework that learns smooth measure-valued splines for stochastic systems that\nsatisfy multiple positional constraints. This is achieved by lifting the\ndynamics to phase space and generalizing stochastic bridges to be conditioned\non several points, forming a multi-marginal conditional stochastic optimal\ncontrol problem. The underlying dynamics are then learned by minimizing a\nvariational objective, having fixed the path induced by the multi-marginal\nconditional bridge. As a matching approach, 3MSBM learns transport maps that\npreserve intermediate marginals throughout training, significantly improving\nconvergence and scalability. Extensive experimentation in a series of\nreal-world applications validates the superior performance of 3MSBM compared to\nexisting methods in capturing complex dynamics with temporal dependencies,\nopening new avenues for training matching frameworks in multi-marginal\nsettings.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10168v1", "AI": {"title_translation": "动量多边际薛定谔桥匹配", "tldr": "本文提出了一种名为3MSBM的新型匹配框架，通过将动力学提升到相空间并推广随机桥以适应多个条件点，解决了从稀疏快照推断轨迹时现有方法无法捕获长程时间依赖性的问题，并在实际应用中展示了卓越性能。", "motivation": "从稀疏样本快照中推断复杂系统的轨迹是一个基本挑战。现有的桥和流匹配框架依赖于相邻快照之间的成对插值，这阻碍了它们捕获长程时间依赖性的能力，并可能影响推断轨迹的连贯性。", "method": "本文引入了动量多边际薛定谔桥匹配（3MSBM）框架。该方法通过将动力学提升到相空间，并将随机桥推广为以多个点为条件，从而形成一个多边际条件随机最优控制问题。通过最小化一个变分目标来学习底层动力学，同时固定由多边际条件桥引起的路径。作为一种匹配方法，3MSBM在训练过程中保留了中间边际，显著提高了收敛性和可扩展性。", "result": "在多项实际应用中进行的广泛实验验证了3MSBM在捕获具有时间依赖性的复杂动力学方面优于现有方法，表现出卓越的性能。", "conclusion": "3MSBM框架的提出为多边际设置中的匹配框架训练开辟了新途径，显著提高了从稀疏数据推断复杂系统轨迹的能力。", "translation": "通过从稀疏样本快照推断轨迹来理解复杂系统是广泛领域（例如单细胞生物学、气象学和经济学）中的一个基本挑战。尽管桥和流匹配框架取得了进展，但当前方法依赖于相邻快照之间的成对插值。这阻碍了它们捕获长程时间依赖性的能力，并可能影响推断轨迹的连贯性。为了解决这些问题，我们引入了动量多边际薛定谔桥匹配（3MSBM），这是一种新颖的匹配框架，用于学习满足多个位置约束的随机系统的平滑测度值样条。这是通过将动力学提升到相空间并将随机桥推广为以多个点为条件来实现的，从而形成一个多边际条件随机最优控制问题。然后通过最小化一个变分目标来学习底层动力学，同时固定由多边际条件桥引起的路径。作为一种匹配方法，3MSBM在整个训练过程中保留了中间边际，显著提高了收敛性和可扩展性。在系列实际应用中的广泛实验验证了3MSBM在捕获具有时间依赖性的复杂动力学方面优于现有方法，为多边际设置中的匹配框架训练开辟了新途径。", "summary": "本文针对从稀疏数据推断复杂系统轨迹时现有方法无法有效捕获长程时间依赖性的问题，提出了一种名为“动量多边际薛定谔桥匹配（3MSBM）”的新型匹配框架。3MSBM通过将系统动力学提升到相空间，并推广随机桥以适应多个条件点，将其建模为一个多边际条件随机最优控制问题。该方法通过最小化变分目标学习动力学，并在训练中保持中间边际，从而显著提升了收敛性和可扩展性。实验证明，3MSBM在处理复杂时间依赖性动力学方面表现优异，为多边际匹配框架的研究开辟了新方向。", "keywords": "薛定谔桥, 多边际, 轨迹推断, 动量匹配, 随机系统", "comments": "3MSBM的创新点在于将动力学提升到相空间并引入多边际条件，这使得模型能够捕获长程时间依赖性，解决了现有方法仅限于成对插值的问题。其作为匹配方法，在训练中保持中间边际的特性，对提高收敛性和可扩展性至关重要。这项工作对于单细胞生物学、气象学和经济学等需要从稀疏快照推断复杂轨迹的领域具有重要意义。"}}
{"id": "2506.10195", "title": "Exploring Topological and Localization Phenomena in SSH Chains under Generalized AAH Modulation: A Computational Approach", "authors": ["Souvik Ghosh", "Sayak Roy"], "summary": "The Su-Schrieffer-Heeger (SSH) model serves as a canonical example of a\none-dimensional topological insulator, yet its behavior under more complex,\nrealistic conditions remains a fertile ground for research. This paper presents\na comprehensive computational investigation into generalized SSH models,\nexploring the interplay between topology, quasi-periodic disorder,\nnon-Hermiticity, and time-dependent driving. Using exact diagonalization and\nspecialized numerical solvers, we map the system's phase space through its\nspectral properties and localization characteristics, quantified by the Inverse\nParticipation Ratio (IPR). We demonstrate that while the standard SSH model\nexhibits topologically protected edge states, these are destroyed by a\nlocalization transition induced by strong Aubry-Andr\\'e-Harper (AAH)\nmodulation. Further, we employ unsupervised machine learning (PCA) to\nautonomously classify the system's phases, revealing that strong localization\ncan obscure underlying topological signatures. Extending the model beyond\nHermiticity, we uncover the non-Hermitian skin effect, a dramatic localization\nof all bulk states at a boundary. Finally, we apply a periodic Floquet drive to\na topologically trivial chain, successfully engineering a Floquet topological\ninsulator characterized by the emergence of anomalous edge states at the\nboundaries of the quasi-energy zone. These findings collectively provide a\nmulti-faceted view of the rich phenomena hosted in generalized 1D topological\nsystems.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.10195v1", "AI": {"title_translation": "探索广义AAH调制下SSH链中的拓扑和局域化现象：一种计算方法", "tldr": "本文计算研究了广义SSH模型中拓扑、准周期无序、非厄米性及时间驱动的相互作用，发现AAH调制可破坏拓扑态，非厄米性导致非厄米趋肤效应，并成功工程化了Floquet拓扑绝缘体。", "motivation": "探索Su-Schrieffer-Heeger (SSH) 模型在更复杂、更真实的条件下的行为，因为这是研究的肥沃领域。特别关注拓扑、准周期无序、非厄米性以及时间相关驱动之间的相互作用。", "method": "使用精确对角化和专门的数值求解器，通过光谱特性和局域化特征（由逆参与比 IPR 量化）绘制系统相空间。采用无监督机器学习（PCA）自动分类系统相位。将模型扩展到非厄米性。对拓扑平凡链施加周期性Floquet驱动。", "result": "强Aubry-Andr\\'e-Harper (AAH) 调制引起的局域化跃迁会破坏标准SSH模型的拓扑保护边缘态。强局域化会掩盖潜在的拓扑特征。在非厄米模型中发现了非厄米趋肤效应，即所有体态在边界的显著局域化。成功地将拓扑平凡链工程化为Floquet拓扑绝缘体，其特征是在准能量区边界出现反常边缘态。", "conclusion": "这些发现共同提供了对广义一维拓扑系统中丰富现象的多方面视角。", "translation": "Su-Schrieffer-Heeger (SSH) 模型是一维拓扑绝缘体的典型例子，但其在更复杂、更真实条件下的行为仍然是研究的热点。本文对广义SSH模型进行了全面的计算研究，探索了拓扑、准周期无序、非厄米性和时间依赖驱动之间的相互作用。我们使用精确对角化和专门的数值求解器，通过系统的光谱特性和局域化特征（由逆参与比 (IPR) 量化）绘制了系统的相空间。我们证明，虽然标准SSH模型表现出拓扑保护的边缘态，但这些态会被强Aubry-Andr\\'e-Harper (AAH) 调制引起的局域化跃迁破坏。此外，我们采用无监督机器学习（主成分分析PCA）自主分类了系统的相位，揭示了强局域化可以掩盖潜在的拓扑特征。将模型扩展到非厄米性，我们发现了非厄米趋肤效应，即所有体态在边界处的显著局域化。最后，我们将周期性Floquet驱动应用于拓扑平凡链，成功地工程化了一个Floquet拓扑绝缘体，其特征是在准能量区边界出现反常边缘态。这些发现共同提供了对广义一维拓扑系统中丰富现象的多方面视角。", "summary": "本文通过计算方法深入研究了广义SSH模型，探讨了拓扑、准周期无序、非厄米性及时间驱动的复杂耦合。研究发现，强AAH调制能破坏SSH模型的拓扑边缘态，并揭示了强局域化对拓扑特征的掩盖作用。进一步，论文揭示了非厄米趋肤效应，并通过Floquet驱动成功设计出具有反常边缘态的拓扑绝缘体，为理解广义一维拓扑系统提供了多维度视角。", "keywords": "SSH模型, 拓扑绝缘体, 局域化, 非厄米性, Floquet驱动", "comments": "这篇论文通过结合精确对角化、数值求解器以及无监督机器学习（PCA）等多种计算方法，对广义SSH模型进行了深入且全面的研究，其创新性在于系统地探索了多种复杂因素（如准周期无序、非厄米性和时间依赖驱动）对拓扑现象的影响。特别是非厄米趋肤效应的发现以及Floquet拓扑绝缘体的工程化，为理解和设计新型拓扑材料提供了重要见解。论文也指出了强局域化可能掩盖拓扑特征的局限性，这对于未来实验和理论研究具有指导意义。"}}
{"id": "2506.10271", "title": "Predicting function of evolutionarily implausible DNA sequences", "authors": ["Shiyu Jiang", "Xuyin Liu", "Zitong Jerry Wang"], "summary": "Genomic language models (gLMs) show potential for generating novel,\nfunctional DNA sequences for synthetic biology, but doing so requires them to\nlearn not just evolutionary plausibility, but also sequence-to-function\nrelationships. We introduce a set of prediction tasks called Nullsettes, which\nassesses a model's ability to predict loss-of-function mutations created by\ntranslocating key control elements in synthetic expression cassettes. Across 12\nstate-of-the-art models, we find that mutation effect prediction performance\nstrongly correlates with the predicted likelihood of the nonmutant.\nFurthermore, the range of likelihood values predictive of strong model\nperformance is highly dependent on sequence length. Our work highlights the\nimportance of considering both sequence likelihood and sequence length when\nusing gLMs for mutation effect prediction.", "comment": "13 pages, 6 figures, accepted to ICML 2025 Generative AI and Biology\n  Workshop", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.10271v1", "AI": {"title_translation": "预测进化上不合理的DNA序列的功能", "tldr": "本文引入了Nullsettes任务来评估基因组语言模型(gLMs)预测DNA序列功能丧失突变的能力，发现突变效应预测性能与非突变序列的预测似然度强相关，且受序列长度影响，强调了在使用gLMs进行突变效应预测时考虑序列似然度和序列长度的重要性。", "motivation": "基因组语言模型(gLMs)在合成生物学中生成新颖、功能性DNA序列方面显示出潜力，但要实现这一目标，它们不仅需要学习进化上的合理性，还需要学习序列到功能的关系。", "method": "研究引入了一系列名为Nullsettes的预测任务，通过评估模型预测由合成表达盒中关键控制元件易位引起的失功能突变的能力来进行评估。研究在12个最先进的模型上进行了测试。", "result": "研究发现，突变效应预测性能与非突变序列的预测似然度强烈相关。此外，预测模型良好性能的似然度值范围高度依赖于序列长度。", "conclusion": "研究强调了在使用基因组语言模型(gLMs)进行突变效应预测时，考虑序列似然度和序列长度的重要性。", "translation": "基因组语言模型（gLMs）在为合成生物学生成新颖、功能性DNA序列方面显示出潜力，但要实现这一目标，它们不仅需要学习进化上的合理性，还需要学习序列到功能的关系。我们引入了一组名为Nullsettes的预测任务，用于评估模型预测通过易位合成表达盒中关键控制元件产生的失功能突变的能力。在12个最先进的模型中，我们发现突变效应预测性能与非突变体的预测似然度密切相关。此外，预测模型强大性能的似然度值范围高度依赖于序列长度。我们的工作强调了在使用gLMs进行突变效应预测时，考虑序列似然度和序列长度的重要性。", "summary": "本文引入了Nullsettes预测任务来评估基因组语言模型(gLMs)预测DNA序列失功能突变的能力。研究发现，在12个主流gLMs中，突变效应预测性能与非突变序列的预测似然度呈强相关性，并且这种相关性受序列长度显著影响。这项工作强调了在使用gLMs进行突变效应预测时，必须同时考虑序列似然度和序列长度。", "keywords": "基因组语言模型, DNA序列, 功能预测, 突变效应, 序列似然度, 序列长度", "comments": "这项研究通过引入Nullsettes任务，为评估基因组语言模型在预测非进化合理序列功能方面的能力提供了一个新颖且重要的基准。它揭示了gLMs在处理合成生物学中关键挑战时的局限性和关键影响因素，特别是强调了序列似然度和长度对模型性能的影响，这对于未来gLMs的设计和应用具有指导意义。"}}
{"id": "2506.10275", "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning", "authors": ["Jun Qi", "Chao-Han Yang", "Pin-Yu Chen", "Min-Hsiu Hsieh"], "summary": "Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine\nlearning, yet their practical application is hindered by inherent limitations\nsuch as constrained linear expressivity, optimization challenges, and acute\nsensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a\nscalable and robust hybrid quantum-classical architecture designed to overcome\nthese obstacles. By innovatively employing quantum circuits to dynamically\ngenerate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude\nencoding and parameterized quantum operations, VQC-MLPNet substantially expands\nrepresentation capabilities and augments training stability. We provide\nrigorous theoretical guarantees via statistical learning techniques and Neural\nTangent Kernel analysis, explicitly deriving upper bounds on approximation,\nuniform deviation, and optimization errors. These theoretical insights\ndemonstrate exponential improvements in representation capacity relative to\nquantum circuit depth and the number of qubits, providing clear computational\nadvantages over standalone quantum circuits and existing hybrid quantum\narchitectures. Our theoretical claims are empirically corroborated through\nextensive experiments, including classifying semiconductor quantum-dot charge\nstates and predicting genomic transcription factor binding sites, demonstrating\nresilient performance even under realistic IBM quantum noise simulations. This\nresearch establishes a theoretically sound and practically robust framework,\nadvancing the frontiers of quantum-enhanced learning for unconventional\ncomputing paradigms in the Noisy Intermediate-Scale Quantum era and beyond.", "comment": "31 pages, 11 figures, under review", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10275v1", "AI": {"title_translation": "VQC-MLPNet：一种用于可扩展和鲁棒量子机器学习的非常规混合量子-经典架构", "tldr": "VQC-MLPNet提出了一种混合量子-经典架构，通过量子电路动态生成经典MLP参数，以克服变分量子电路的局限性，实现可扩展和鲁棒的量子机器学习。", "motivation": "变分量子电路（VQCs）在量子机器学习中面临线性表达能力受限、优化困难和对量子硬件噪声敏感等固有限制。", "method": "VQC-MLPNet通过创新性地利用量子电路（通过幅度编码和参数化量子操作）动态生成经典多层感知器（MLPs）的参数。该方法通过统计学习技术和神经正切核分析提供了严格的理论保证，明确推导了逼近、均匀偏差和优化误差的上限。", "result": "理论上，VQC-MLPNet在表示能力上相对于量子电路深度和量子比特数量实现了指数级提升，相较于独立量子电路和现有混合量子架构具有计算优势。经验上，通过分类半导体量子点电荷态和预测基因组转录因子结合位点等实验证实了其理论主张，即使在真实的IBM量子噪声模拟下也能展现出弹性性能。", "conclusion": "这项研究建立了一个理论上健全且实践中鲁棒的框架，推动了在噪声中等规模量子（NISQ）时代及未来非常规计算范式下量子增强学习的前沿发展。", "translation": "变分量子电路（VQCs）为量子机器学习提供了一条新途径，但其实际应用受到固有局限性的阻碍，例如受限的线性表达能力、优化挑战以及对量子硬件噪声的极度敏感。本工作介绍了VQC-MLPNet，一种可扩展且鲁棒的混合量子-经典架构，旨在克服这些障碍。通过创新性地利用量子电路，通过幅度编码和参数化量子操作为经典多层感知器（MLPs）动态生成参数，VQC-MLPNet大大扩展了表示能力并增强了训练稳定性。我们通过统计学习技术和神经正切核分析提供了严格的理论保证，明确推导了逼近、均匀偏差和优化误差的上限。这些理论见解表明，相对于量子电路深度和量子比特数量，表示能力有指数级提升，与独立量子电路和现有混合量子架构相比具有明显的计算优势。我们的理论主张通过广泛的实验得到了经验证实，包括分类半导体量子点电荷态和预测基因组转录因子结合位点，即使在真实的IBM量子噪声模拟下也能展现出弹性性能。这项研究建立了一个理论上健全且实践中鲁棒的框架，推动了在噪声中等规模量子时代及未来非常规计算范式下量子增强学习的前沿发展。", "summary": "VQC-MLPNet是一种新型混合量子-经典架构，旨在解决变分量子电路（VQCs）在量子机器学习中面临的表达能力、优化和噪声敏感性问题。该架构通过量子电路动态生成经典多层感知器（MLPs）的参数，显著提升了表示能力和训练稳定性。论文提供了严格的理论保证，并通过半导体量子点分类和基因组预测等实验验证了其在噪声环境下的鲁棒性能，为量子增强学习提供了一个可扩展且稳健的框架。", "keywords": "量子机器学习, 混合量子-经典架构, 变分量子电路, 神经网络, 噪声鲁棒性", "comments": "VQC-MLPNet的创新之处在于其独特的混合架构，通过量子电路动态生成经典MLP参数，有效弥补了VQC的局限性。该研究不仅提供了扎实的理论基础（统计学习和神经正切核分析），还通过经验实验验证了其在噪声环境下的鲁棒性，这对于当前噪声中等规模量子（NISQ）时代的应用至关重要。其对表示能力指数级提升的理论证明，显示出该架构在可扩展性上的巨大潜力。"}}
{"id": "2506.10293", "title": "Distributionally-Constrained Adversaries in Online Learning", "authors": ["Moïse Blanchard", "Samory Kpotufe"], "summary": "There has been much recent interest in understanding the continuum from\nadversarial to stochastic settings in online learning, with various frameworks\nincluding smoothed settings proposed to bridge this gap. We consider the more\ngeneral and flexible framework of distributionally constrained adversaries in\nwhich instances are drawn from distributions chosen by an adversary within some\nconstrained distribution class [RST11]. Compared to smoothed analysis, we\nconsider general distributional classes which allows for a fine-grained\nunderstanding of learning settings between fully stochastic and fully\nadversarial for which a learner can achieve non-trivial regret. We give a\ncharacterization for which distribution classes are learnable in this context\nagainst both oblivious and adaptive adversaries, providing insights into the\ntypes of interplay between the function class and distributional constraints on\nadversaries that enable learnability. In particular, our results recover and\ngeneralize learnability for known smoothed settings. Further, we show that for\nseveral natural function classes including linear classifiers, learning can be\nachieved without any prior knowledge of the distribution class -- in other\nwords, a learner can simultaneously compete against any constrained adversary\nwithin learnable distribution classes.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10293v1", "AI": {"title_translation": "在线学习中受分布约束的对抗者", "tldr": "本文研究了在线学习中受分布约束的对抗者，刻画了可学习的分布类别，并表明对于自然函数类别，无需先验分布知识即可实现学习。", "motivation": "理解在线学习中从对抗性到随机性设置的连续统一体，并弥合这一差距；现有的框架（如平滑设置）存在局限性。", "method": "提出并考虑了受分布约束的对抗者的更通用和灵活的框架，其中实例由对抗者在受约束的分布类别中选择的分布中抽取。他们刻画了针对盲目和自适应对抗者的可学习分布类别。", "result": "他们刻画了哪些分布类别在这种情况下是可学习的。他们的结果恢复并推广了已知平滑设置的可学习性。此外，他们表明对于包括线性分类器在内的几种自然函数类别，无需任何先验分布类别知识即可实现学习。", "conclusion": "本文深入探讨了函数类别与对抗者分布约束之间相互作用的类型，这些相互作用使得可学习性成为可能，表明即使对于特定函数类型，在没有先验分布类别知识的情况下也可以进行学习。", "translation": "最近人们对理解在线学习中从对抗性到随机性设置的连续统一体产生了浓厚兴趣，各种框架（包括平滑设置）被提出以弥合这一差距。我们考虑了受分布约束的对抗者的更通用和灵活的框架，其中实例由对抗者在某些受约束的分布类别 [RST11] 中选择的分布中抽取。与平滑分析相比，我们考虑了通用分布类别，这使得能够对完全随机和完全对抗性之间的学习设置进行细粒度理解，学习者可以在其中实现非平凡的遗憾。我们刻画了在这种情况下针对盲目和自适应对抗者的哪些分布类别是可学习的，从而深入了解了函数类别与对抗者分布约束之间相互作用的类型，这些相互作用使得可学习性成为可能。特别是，我们的结果恢复并推广了已知平滑设置的可学习性。此外，我们表明对于包括线性分类器在内的几种自然函数类别，无需任何先验分布类别知识即可实现学习——换句话说，学习者可以同时与可学习分布类别中的任何受约束对抗者竞争。", "summary": "本文研究了在线学习中受分布约束的对抗者框架，旨在弥合完全随机和完全对抗性设置之间的差距。它刻画了针对盲目和自适应对抗者的可学习分布类别，从而深入了解了实现可学习性的条件。研究结果推广了现有的平滑设置，并表明对于某些函数类别，学习可以在不具备分布类别先验知识的情况下实现，从而允许学习者与任何受约束的对抗者竞争。", "keywords": "在线学习, 受分布约束的对抗者, 可学习性, 平滑分析, 遗憾", "comments": "这篇论文引入了一个更通用的在线学习框架，统一了随机和对抗性设置，从而更深入地理解了在不同对抗性控制水平下的可学习性。在没有先验分布类别知识的情况下进行学习的能力是一个重要的实际优势。"}}
{"id": "2506.10305", "title": "Self-learning signal classifier for decameter coherent scatter radars", "authors": ["Oleg Berngardt", "Ivan Lavygin"], "summary": "The paper presents a method for automatic constructing a classifier for\nprocessed data obtained by decameter coherent scatter radars. Method is based\nonly on the radar data obtained, the results of automatic modeling of radio\nwave propagation in the ionosphere, and mathematical criteria for estimating\nthe quality of the models. The final classifier is the model trained at data\nobtained by 12 radars of the SuperDARN and SECIRA networks over two years for\neach radar. The number of the model coefficients is 2669. For the\nclassification, the model uses both the calculated parameters of radio wave\npropagation in the model ionosphere and the parameters directly measured by the\nradar. Calibration of radiowave elevation measurements at each radar was made\nusing meteor trail scattered signals. The analysis showed that the optimal\nnumber of classes in the data is 37, of which 25 are frequently observed. The\nanalysis made it possible to choose 14 classes from them, which are confidently\nseparated in other variants of model training. A preliminary interpretation of\n10 of them was carried out. The dynamics of observation of various classes and\ntheir dependence on the geographical latitude of radars at different levels of\nsolar and geomagnetic activity were presented, it was shown that it does not\ncontradict with known physical mechanisms. The analysis showed that the most\nimportant parameters to identify the classes are the shape of the signal\nray-tracing trajectory in its second half, the ray-traced scattering height and\nthe Doppler velocity measured by the radar.", "comment": "30 pages, 10 figures, 4 tables. To be submitted to Advances in Space\n  Research", "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.10305v1", "AI": {"title_translation": "分米波相干散射雷达自学习信号分类器", "tldr": "本文提出并构建了一种基于雷达数据、电离层无线电波传播模型和数学准则的自学习分类器，用于自动分类分米波相干散射雷达数据，并识别出关键分类参数和信号类别。", "motivation": "旨在为分米波相干散射雷达的已处理数据自动构建一个分类器，以实现数据的自动化分析和模式识别。", "method": "该方法基于雷达数据、电离层无线电波传播的自动建模结果以及评估模型质量的数学标准。最终分类器在来自SuperDARN和SECIRA网络12个雷达两年（每个雷达）的数据上进行训练，模型系数为2669。分类时同时使用模型电离层中计算的无线电波传播参数和雷达直接测量的参数。通过流星尾迹散射信号对每个雷达的无线电波仰角测量进行了校准。", "result": "分析表明数据中最佳类别数为37个（其中25个频繁观测）。从中选出了14个在其他模型训练变体中能可靠分离的类别，并对其中10个进行了初步解释。展示了不同类别的观测动态及其与雷达地理纬度、太阳和地磁活动水平的依赖关系，结果与已知物理机制不矛盾。分析显示，识别类别的最重要参数是信号射线追踪轨迹后半部分的形状、射线追踪散射高度和雷达测量的多普勒速度。", "conclusion": "该研究成功构建了一个自学习信号分类器，能够有效分类分米波相干散射雷达数据，并识别出关键的分类参数和信号类别，其结果与已知物理机制一致。", "translation": "本文提出了一种自动构建用于分米波相干散射雷达处理数据的分类器的方法。该方法仅基于获得的雷达数据、电离层中无线电波传播的自动建模结果以及评估模型质量的数学标准。最终的分类器是在SuperDARN和SECIRA网络中12个雷达两年（每个雷达）的数据上训练的模型。模型系数为2669。为了进行分类，该模型同时使用了模型电离层中计算的无线电波传播参数和雷达直接测量的参数。每个雷达的无线电波仰角测量都通过流星尾迹散射信号进行了校准。分析表明，数据中最佳类别数为37个，其中25个是频繁观测的。分析使得从中选择了14个类别，这些类别在其他模型训练变体中能够可靠地分离。对其中10个进行了初步解释。展示了不同类别的观测动态及其与雷达地理纬度在不同太阳和地磁活动水平下的依赖关系，结果表明这与已知的物理机制不矛盾。分析显示，识别类别的最重要参数是信号射线追踪轨迹后半部分的形状、射线追踪散射高度和雷达测量的多普勒速度。", "summary": "本文提出了一种用于分米波相干散射雷达数据的自学习信号分类器。该分类器整合了实际雷达数据、电离层无线电波传播模型及质量评估标准进行训练。通过在SuperDARN和SECIRA网络12个雷达两年数据上的训练，模型能有效识别并区分出37个潜在类别中的14个关键类别，并初步解释了其中10个。研究还揭示了这些类别的观测动态与地理纬度、太阳及地磁活动的关系，并指出信号射线追踪轨迹形状、散射高度和多普勒速度是识别类别的最重要参数，其结果与现有物理机制相符。", "keywords": "自学习分类器, 分米波相干散射雷达, 电离层, 无线电波传播, 数据分类", "comments": "这项研究的创新之处在于提出了一种完全基于数据和物理模型相结合的自学习方法来自动构建雷达信号分类器，大大减少了对人工干预的需求。其重要性体现在能够系统化地对复杂的雷达散射数据进行分类和解释，为理解电离层物理现象提供了新的工具。通过识别关键分类参数和分析类别动态，为进一步的科学研究奠定了基础。"}}
{"id": "2506.10797", "title": "Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac Substructure Segmentation", "authors": ["Nicholas Summerfield", "Qisheng He", "Alex Kuo", "Ahmed I. Ghanem", "Simeng Zhu", "Chase Ruff", "Joshua Pan", "Anudeep Kumar", "Prashant Nagpal", "Jiwei Zhao", "Ming Dong", "Carri K. Glide-Hurst"], "summary": "Cardiac substructures are essential in thoracic radiation therapy planning to\nminimize risk of radiation-induced heart disease. Deep learning (DL) offers\nefficient methods to reduce contouring burden but lacks generalizability across\ndifferent modalities and overlapping structures. This work introduces and\nvalidates a Modality-AGnostic Image Cascade (MAGIC) for comprehensive and\nmulti-modal cardiac substructure segmentation. MAGIC is implemented through\nreplicated encoding and decoding branches of an nnU-Net-based, U-shaped\nbackbone conserving the function of a single model. Twenty cardiac\nsubstructures (heart, chambers, great vessels (GVs), valves, coronary arteries\n(CAs), and conduction nodes) from simulation CT (Sim-CT), low-field MR-Linac,\nand cardiac CT angiography (CCTA) modalities were manually delineated and used\nto train (n=76), validate (n=15), and test (n=30) MAGIC. Twelve comparison\nmodels (four segmentation subgroups across three modalities) were equivalently\ntrained. All methods were compared for training efficiency and against\nreference contours using the Dice Similarity Coefficient (DSC) and two-tailed\nWilcoxon Signed-Rank test (threshold, p<0.05). Average DSC scores were\n0.75(0.16) for Sim-CT, 0.68(0.21) for MR-Linac, and 0.80(0.16) for CCTA. MAGIC\noutperforms the comparison in 57% of cases, with limited statistical\ndifferences. MAGIC offers an effective and accurate segmentation solution that\nis lightweight and capable of segmenting multiple modalities and overlapping\nstructures in a single model. MAGIC further enables clinical implementation by\nsimplifying the computational requirements and offering unparalleled\nflexibility for clinical settings.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.10797v1", "AI": {"title_translation": "模态无关图像级联（MAGIC）用于多模态心脏亚结构分割", "tldr": "本文提出并验证了模态无关图像级联（MAGIC），一个轻量级、单模型的深度学习解决方案，用于准确的多模态心脏亚结构分割，其性能优于现有方法，并简化了临床实施。", "motivation": "心脏亚结构在胸部放射治疗计划中至关重要，但现有深度学习方法在不同模态和重叠结构上的泛化能力不足，增加了勾画负担。", "method": "本文引入了模态无关图像级联（MAGIC），该方法通过nnU-Net基础的U形骨干网的复制编码和解码分支实现。研究使用来自模拟CT（Sim-CT）、低场MR-Linac和心脏CT血管造影（CCTA）三种模态的二十个心脏亚结构进行训练（n=76）、验证（n=15）和测试（n=30）。通过Dice相似系数（DSC）和Wilcoxon符号秩检验，将MAGIC与十二个比较模型在训练效率和分割精度上进行比较。", "result": "MAGIC在Sim-CT上的平均DSC得分为0.75(0.16)，MR-Linac为0.68(0.21)，CCTA为0.80(0.16)。MAGIC在57%的情况下优于比较模型，统计差异有限。", "conclusion": "MAGIC提供了一种有效、准确、轻量化且灵活的分割解决方案，能够在单个模型中处理多种模态和重叠的心脏结构。它通过简化计算要求和提供灵活性，进一步推动了临床应用。", "translation": "心脏亚结构在胸部放射治疗计划中至关重要，旨在最大限度地降低放射性心脏病风险。深度学习（DL）提供了有效的减少勾画负担的方法，但其在不同模态和重叠结构上的泛化能力不足。本工作介绍并验证了一种模态无关图像级联（MAGIC），用于全面、多模态的心脏亚结构分割。MAGIC通过nnU-Net基础的U形骨干网的复制编码和解码分支实现，保持了单个模型的功能。来自模拟CT（Sim-CT）、低场MR-Linac和心脏CT血管造影（CCTA）模态的二十个心脏亚结构（心脏、心腔、大血管（GVs）、瓣膜、冠状动脉（CAs）和传导结）被手动勾画，并用于训练（n=76）、验证（n=15）和测试（n=30）MAGIC。十二个比较模型（三个模态下的四个分割子组）进行了等效训练。所有方法都通过Dice相似系数（DSC）和双尾Wilcoxon符号秩检验（阈值，p<0.05）进行训练效率和与参考轮廓的比较。Sim-CT的平均DSC评分为0.75(0.16)，MR-Linac为0.68(0.21)，CCTA为0.80(0.16)。MAGIC在57%的情况下优于比较模型，统计差异有限。MAGIC提供了一种有效且准确的分割解决方案，该方案轻量化，并能够在单个模型中分割多种模态和重叠结构。MAGIC通过简化计算要求并为临床环境提供无与伦比的灵活性，进一步实现了临床实施。", "summary": "本文提出了一种名为模态无关图像级联（MAGIC）的新型深度学习框架，旨在解决现有深度学习模型在多模态心脏亚结构分割中泛化能力不足的问题。MAGIC基于nnU-Net架构，通过复制编码和解码分支实现，使其能够在一个单一模型中处理来自Sim-CT、MR-Linac和CCTA等多种模态的图像。研究在20个心脏亚结构上对MAGIC进行了训练、验证和测试，并与12个比较模型进行了评估。结果显示，MAGIC在不同模态上均取得了良好的分割精度（Sim-CT平均DSC为0.75，MR-Linac为0.68，CCTA为0.80），并在57%的情况下优于比较模型。该研究认为MAGIC提供了一个有效、准确、轻量且灵活的单模型解决方案，有望通过简化计算需求和增加临床适用性来促进其在临床中的应用。", "keywords": "心脏亚结构分割, 多模态, 深度学习, MAGIC, nnU-Net, 胸部放射治疗", "comments": "本文引入的“模态无关图像级联（MAGIC）”架构是一项创新，它通过在单一nnU-Net骨干网中集成复制的编码/解码分支，有效解决了当前深度学习分割模型在不同成像模态之间泛化能力不足的关键限制。这种方法避免了为每种模态训练单独模型的需求，显著提升了临床工作流程的效率和便利性。该模型“轻量化”和“简化计算要求”的特性对于实际的临床部署至关重要。尽管MAGIC在超过一半的案例中表现优于比较模型，但“有限的统计差异”表明在某些特定情况下仍有进一步改进或更深入分析的空间。总体而言，该研究为开发更通用、更适合临床应用的心脏分割工具迈出了有希望的一步。"}}
{"id": "2506.10433", "title": "Measuring Semantic Information Production in Generative Diffusion Models", "authors": ["Florian Handke", "Félix Koulischer", "Gabriel Raya", "Luca Ambrogioni"], "summary": "It is well known that semantic and structural features of the generated\nimages emerge at different times during the reverse dynamics of diffusion, a\nphenomenon that has been connected to physical phase transitions in magnets and\nother materials. In this paper, we introduce a general information-theoretic\napproach to measure when these class-semantic \"decisions\" are made during the\ngenerative process. By using an online formula for the optimal Bayesian\nclassifier, we estimate the conditional entropy of the class label given the\nnoisy state. We then determine the time intervals corresponding to the highest\ninformation transfer between noisy states and class labels using the time\nderivative of the conditional entropy. We demonstrate our method on\none-dimensional Gaussian mixture models and on DDPM models trained on the\nCIFAR10 dataset. As expected, we find that the semantic information transfer is\nhighest in the intermediate stages of diffusion while vanishing during the\nfinal stages. However, we found sizable differences between the entropy rate\nprofiles of different classes, suggesting that different \"semantic decisions\"\nare located at different intermediate times.", "comment": "4 pages, 3 figures, an appendix with derivations and implementation\n  details, accepted at ICLR DeLTa 2025", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10433v1", "AI": {"title_translation": "测量生成扩散模型中的语义信息生成", "tldr": "本研究提出了一种通用的信息论方法来量化生成扩散模型中语义信息“决策”的时间点，并发现语义信息传递在扩散的中间阶段最高，且不同类别之间存在显著差异。", "motivation": "生成扩散模型中图像的语义和结构特征在逆向扩散过程中不同时间出现，这种现象与磁体和其他材料中的物理相变有关。本研究旨在引入一种通用的信息论方法来测量这些类别语义“决策”在生成过程中何时做出。", "method": "本研究采用一种在线最优贝叶斯分类器公式来估计给定噪声状态下类别标签的条件熵。然后，利用条件熵的时间导数来确定噪声状态与类别标签之间信息传递最高的时段。该方法在一维高斯混合模型和在CIFAR10数据集上训练的DDPM模型上进行了验证。", "result": "研究发现，语义信息传递在扩散的中间阶段最高，而在最后阶段消失，这与预期相符。然而，不同类别的熵率分布存在显著差异，表明不同的“语义决策”发生在不同的中间时间。", "conclusion": "本研究的结论是，在生成扩散模型中，语义信息传递主要发生在扩散的中间阶段，并且不同类别的语义信息生成时间点存在差异。", "translation": "众所周知，生成图像的语义和结构特征在扩散的逆向动态过程中在不同时间出现，这种现象已与磁体和其他材料中的物理相变联系起来。在本文中，我们引入了一种通用的信息论方法来测量这些类别语义“决策”在生成过程中何时做出。通过使用最优贝叶斯分类器的在线公式，我们估计了给定噪声状态下类别标签的条件熵。然后，我们利用条件熵的时间导数来确定噪声状态与类别标签之间信息传递最高的时段。我们在​​一维高斯混合模型和在CIFAR10数据集上训练的DDPM模型上验证了我们的方法。正如预期的那样，我们发现语义信息传递在扩散的中间阶段最高，而在最后阶段消失。然而，我们发现不同类别的熵率分布存在显著差异，这表明不同的“语义决策”位于不同的中间时间。", "summary": "本论文提出了一种新的信息论方法，用于量化生成扩散模型中类别语义信息形成的精确时间点。通过计算条件熵及其时间导数，该方法能够识别噪声状态与类别标签之间信息传递最活跃的阶段。研究结果表明，语义信息在扩散过程的中间阶段达到峰值，并在最终阶段消失。此外，不同类别的语义信息生成时间存在显著差异，揭示了扩散过程中类别特有的信息编码模式。", "keywords": "语义信息, 扩散模型, 信息论, 条件熵, CIFAR10", "comments": "这项研究的创新之处在于提出了一个通用的信息论框架来量化扩散模型中的语义信息生成过程，这有助于更深入地理解这些模型的工作机制。通过将信息论与扩散模型相结合，为分析生成过程中的“决策”点提供了新的视角。研究结果对理解扩散模型的内部动态和潜在优化方向具有重要意义。"}}
{"id": "2506.10475", "title": "Prediction of steady states in a marine ecosystem model by a machine learning technique", "authors": ["Sarker Miraz Mahfuz", "Thomas Slawig"], "summary": "We used precomputed steady states obtained by a spin-up for a global marine\necosystem model as training data to build a mapping from the small number of\nbiogeochemical model parameters onto the three-dimensional converged steady\nannual cycle. The mapping was performed by a conditional variational\nautoencoder (CVAE) with mass correction. Applied for test data, we show that\nthe prediction obtained by the CVAE already gives a reasonable good\napproximation of the steady states obtained by a regular spin-up. However, the\npredictions do not reach the same level of annual periodicity as those obtained\nin the original spin-up data. Thus, we took the predictions as initial values\nfor a spin-up. We could show that the number of necessary iterations,\ncorresponding to model years, to reach a prescribed stopping criterion in the\nspin-up could be significantly reduced compared to the use of the originally\nuniform, constant initial value. The amount of reduction depends on the applied\nstopping criterion, measuring the periodicity of the solution. The savings in\nneeded iterations and, thus, computing time for the spin-up ranges from 50 to\n95\\%, depending on the stopping criterion for the spin-up. We compared these\nresults with the use of the mean of the training data as an initial value. We\nfound that this also accelerates the spin-up, but only by a much lower factor.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.10475v1", "AI": {"title_translation": "利用机器学习技术预测海洋生态系统模型的稳态", "tldr": "本文利用条件变分自编码器（CVAE）预测海洋生态系统模型的稳态，并将其预测结果作为初始值，显著加速了模型自旋启动过程，节省了50%到95%的计算时间。", "motivation": "海洋生态系统模型的自旋启动（spin-up）过程需要大量的计算时间来达到稳态，这限制了模型的使用和参数探索。因此，需要一种更高效的方法来加速这一过程。", "method": "研究人员使用预先计算好的全球海洋生态系统模型的稳态数据作为训练数据，通过带有质量校正的条件变分自编码器（CVAE）将少量生物地球化学模型参数映射到三维收敛的稳态年循环。然后，将CVAE的预测结果作为自旋启动的初始值，以减少达到预设停止准则所需的迭代次数。", "result": "CVAE的预测结果可以合理地近似通过常规自旋启动获得的稳态，但年度周期性不如原始数据。然而，将CVAE预测作为自旋启动的初始值，可以显著减少达到稳态所需的迭代次数（模型年数），与使用均匀常数初始值相比，计算时间节省了50%到95%，具体取决于停止准则。与使用训练数据平均值作为初始值相比，CVAE的加速效果更显著。", "conclusion": "机器学习技术，特别是CVAE，可以有效地预测海洋生态系统模型的稳态，并作为自旋启动的优秀初始值，从而大幅缩短模型达到稳态所需的计算时间，显著提高了模型运行效率。", "translation": "我们使用通过自旋启动获得的全球海洋生态系统模型的预计算稳态作为训练数据，以构建从少量生物地球化学模型参数到三维收敛稳态年循环的映射。该映射通过带有质量校正的条件变分自编码器（CVAE）进行。应用于测试数据时，我们发现CVAE获得的预测已经能够很好地近似通过常规自旋启动获得的稳态。然而，预测的年度周期性未能达到原始自旋启动数据所达到的水平。因此，我们将预测结果作为自旋启动的初始值。结果表明，与使用原始均匀常数初始值相比，达到自旋启动中预设停止准则所需的迭代次数（对应于模型年数）可以显著减少。减少量取决于所应用的停止准则，即衡量解的周期性。自旋启动所需迭代次数和计算时间的节省范围为50%到95%，具体取决于自旋启动的停止准则。我们将这些结果与使用训练数据平均值作为初始值的情况进行了比较。我们发现这也加速了自旋启动，但加速因子要低得多。", "summary": "本研究利用条件变分自编码器（CVAE）预测全球海洋生态系统模型的稳态。通过将预计算的稳态数据作为训练集，CVAE能够从模型参数映射到三维稳态年循环。尽管CVAE的直接预测在周期性上略逊于传统自旋启动结果，但当其预测值作为自旋启动的初始条件时，能显著减少达到稳态所需的迭代次数，从而节省50%至95%的计算时间。这表明机器学习方法能有效加速大型海洋生态系统模型的模拟过程。", "keywords": "海洋生态系统模型, 稳态预测, 机器学习, 条件变分自编码器, 自旋启动", "comments": "这项研究的创新之处在于将机器学习，特别是CVAE，应用于加速计算密集型的海洋生态系统模型自旋启动过程。通过利用ML模型预测的近似稳态作为初始值，极大地提高了模拟效率。这对于需要大量模拟实验（如参数敏感性分析、不确定性量化）的领域具有重要意义。其局限性可能在于CVAE预测的周期性仍需后续自旋启动修正，以及该方法对模型复杂度和数据量的依赖性。"}}
{"id": "2506.10552", "title": "On the role of non-linear latent features in bipartite generative neural networks", "authors": ["Tony Bonnaire", "Giovanni Catania", "Aurélien Decelle", "Beatriz Seoane"], "summary": "We investigate the phase diagram and memory retrieval capabilities of\nbipartite energy-based neural networks, namely Restricted Boltzmann Machines\n(RBMs), as a function of the prior distribution imposed on their hidden units -\nincluding binary, multi-state, and ReLU-like activations. Drawing connections\nto the Hopfield model and employing analytical tools from statistical physics\nof disordered systems, we explore how the architectural choices and activation\nfunctions shape the thermodynamic properties of these models. Our analysis\nreveals that standard RBMs with binary hidden nodes and extensive connectivity\nsuffer from reduced critical capacity, limiting their effectiveness as\nassociative memories. To address this, we examine several modifications, such\nas introducing local biases and adopting richer hidden unit priors. These\nadjustments restore ordered retrieval phases and markedly improve recall\nperformance, even at finite temperatures. Our theoretical findings, supported\nby finite-size Monte Carlo simulations, highlight the importance of hidden unit\ndesign in enhancing the expressive power of RBMs.", "comment": "23 pages, 5 figures", "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.10552v1", "AI": {"title_translation": "双向生成神经网络中非线性潜在特征的作用", "tldr": "研究了RBM中隐藏单元先验分布对其相图和记忆检索能力的影响，发现标准RBM容量受限，通过引入局部偏差和更丰富的隐藏单元先验可显著改善性能。", "motivation": "探讨限制玻尔兹曼机（RBM）作为联想记忆的有效性问题，特别是标准RBM在广泛连接下临界容量降低的问题。", "method": "运用无序系统统计物理学分析工具，并与Hopfield模型建立联系，探索架构选择和激活函数如何影响RBM的热力学性质。通过引入局部偏差和采用更丰富的隐藏单元先验分布（包括二元、多态和ReLU类激活）来修改RBM。通过有限尺寸蒙特卡洛模拟支持理论发现。", "result": "标准RBM在二元隐藏节点和广泛连接下，临界容量降低，限制了其作为联想记忆的有效性。引入局部偏差和采用更丰富的隐藏单元先验可恢复有序检索阶段，并显著改善召回性能，即使在有限温度下也是如此。", "conclusion": "隐藏单元设计对于增强RBM的表达能力至关重要。", "translation": "我们研究了双向基于能量的神经网络，即受限玻尔兹曼机（RBMs）的相图和记忆检索能力，作为对其隐藏单元施加的先验分布（包括二元、多态和类ReLU激活）的函数。通过与Hopfield模型的联系，并运用无序系统统计物理学的分析工具，我们探索了架构选择和激活函数如何塑造这些模型的热力学性质。我们的分析揭示，具有二元隐藏节点和广泛连接的标准RBMs的临界容量降低，限制了它们作为联想记忆的有效性。为了解决这个问题，我们研究了几种修改，例如引入局部偏差和采用更丰富的隐藏单元先验。这些调整恢复了有序检索阶段，并显著提高了召回性能，即使在有限温度下也是如此。我们的理论发现得到了有限尺寸蒙特卡洛模拟的支持，突出了隐藏单元设计在增强RBMs表达能力方面的重要性。", "summary": "这项研究调查了受限玻尔兹曼机（RBMs）的相图和记忆检索能力，重点关注隐藏单元的先验分布（如二元、多态和ReLU类激活）。研究发现，标准RBMs由于临界容量受限而作为联想记忆效率不高。通过引入局部偏差和更丰富的隐藏单元先验，可以显著提高RBM的召回性能并恢复有序检索，强调了隐藏单元设计对RBM表达能力的关键作用。", "keywords": "受限玻尔兹曼机, 联想记忆, 隐藏单元, 统计物理学, 相图", "comments": "这篇论文通过统计物理学的方法深入分析了受限玻尔兹曼机（RBMs）的内在机制和局限性。其创新点在于揭示了标准RBM在联想记忆方面的容量瓶颈，并提出了通过优化隐藏单元设计（如引入局部偏差和丰富先验分布）来显著提升其性能的有效策略。这对于理解和改进基于能量的生成模型具有重要意义。"}}
{"id": "2506.10572", "title": "Box-Constrained Softmax Function and Its Application for Post-Hoc Calibration", "authors": ["Kyohei Atarashi", "Satoshi Oyama", "Hiromi Arai", "Hisashi Kashima"], "summary": "Controlling the output probabilities of softmax-based models is a common\nproblem in modern machine learning. Although the $\\mathrm{Softmax}$ function\nprovides soft control via its temperature parameter, it lacks the ability to\nenforce hard constraints, such as box constraints, on output probabilities,\nwhich can be critical in certain applications requiring reliable and\ntrustworthy models. In this work, we propose the box-constrained softmax\n($\\mathrm{BCSoftmax}$) function, a novel generalization of the\n$\\mathrm{Softmax}$ function that explicitly enforces lower and upper bounds on\noutput probabilities. While $\\mathrm{BCSoftmax}$ is formulated as the solution\nto a box-constrained optimization problem, we develop an exact and efficient\ncomputation algorithm for $\\mathrm{BCSoftmax}$. As a key application, we\nintroduce two post-hoc calibration methods based on $\\mathrm{BCSoftmax}$. The\nproposed methods mitigate underconfidence and overconfidence in predictive\nmodels by learning the lower and upper bounds of the output probabilities or\nlogits after model training, thereby enhancing reliability in downstream\ndecision-making tasks. We demonstrate the effectiveness of our methods\nexperimentally using the TinyImageNet, CIFAR-100, and 20NewsGroups datasets,\nachieving improvements in calibration metrics.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10572v1", "AI": {"title_translation": "盒约束Softmax函数及其在事后校准中的应用", "tldr": "本文提出了一种新的盒约束Softmax函数（BCSoftmax），它能对输出概率施加硬性上下限约束。我们还开发了一种高效的计算算法，并将其应用于事后校准，通过学习输出概率的边界来缓解模型的欠置信和过置信问题，从而提高了可靠性。", "motivation": "现代机器学习中，控制基于Softmax模型的输出概率是一个常见问题。尽管Softmax函数可以通过温度参数提供软控制，但它缺乏对输出概率施加硬性约束（如盒约束）的能力，这对于需要可靠和可信模型的特定应用至关重要。", "method": "本文提出了盒约束Softmax（BCSoftmax）函数，它是Softmax函数的一种新颖泛化，明确地对输出概率施加了下限和上限。BCSoftmax被表述为盒约束优化问题的解，并开发了一种精确高效的计算算法。作为关键应用，引入了两种基于BCSoftmax的事后校准方法，通过在模型训练后学习输出概率或logits的下限和上限来缓解预测模型中的欠置信和过置信。", "result": "在TinyImageNet、CIFAR-100和20NewsGroups数据集上进行了实验，证明了所提方法的有效性，并在校准指标方面取得了改进。", "conclusion": "BCSoftmax函数能够对输出概率施加硬性约束，并通过事后校准有效缓解了模型的欠置信和过置信问题，提高了预测模型的可靠性。", "translation": "控制基于Softmax模型的输出概率是现代机器学习中的一个常见问题。尽管Softmax函数通过其温度参数提供软控制，但它缺乏对输出概率施加硬性约束（如盒约束）的能力，这在需要可靠和可信模型的某些应用中可能至关重要。在这项工作中，我们提出了盒约束Softmax（BCSoftmax）函数，它是Softmax函数的一种新颖泛化，明确地对输出概率施加了下限和上限。虽然BCSoftmax被表述为盒约束优化问题的解，但我们开发了一种精确高效的BCSoftmax计算算法。作为一个关键应用，我们引入了两种基于BCSoftmax的事后校准方法。所提出的方法通过在模型训练后学习输出概率或logits的下限和上限来缓解预测模型中的欠置信和过置信，从而增强下游决策任务的可靠性。我们使用TinyImageNet、CIFAR-100和20NewsGroups数据集通过实验证明了我们方法的有效性，并在校准指标方面取得了改进。", "summary": "本文提出了一种名为盒约束Softmax（BCSoftmax）的新型函数，它扩展了传统Softmax，允许对输出概率施加明确的上下限硬约束。针对BCSoftmax的计算，论文开发了一种精确高效的算法。作者将BCSoftmax应用于事后校准任务，设计了两种方法，通过学习输出概率的边界来纠正模型的欠置信和过置信问题，从而提升了模型在下游任务中的可靠性。实验结果表明，该方法在多个数据集上有效改善了校准性能。", "keywords": "盒约束Softmax, 事后校准, 概率校准, 模型可靠性, 硬约束", "comments": "该论文的创新点在于提出了BCSoftmax，解决了传统Softmax无法施加硬性输出概率约束的问题，这对于需要高可靠性和可信度的应用场景具有重要意义。所提出的高效计算算法和在事后校准中的应用，为提升模型置信度校准提供了一种新颖且实用的方法。这对于模型部署和风险敏感型应用具有潜在的积极影响。"}}
{"id": "2506.10660", "title": "Pushing the Limits of Extreme Weather: Constructing Extreme Heatwave Storylines with Differentiable Climate Models", "authors": ["Tim Whittaker", "Alejandro Di Luca"], "summary": "Understanding the plausible upper bounds of extreme weather events is\nessential for risk assessment in a warming climate. Existing methods, based on\nlarge ensembles of physics-based models, are often computationally expensive or\nlack the fidelity needed to simulate rare, high-impact extremes. Here, we\npresent a novel framework that leverages a differentiable hybrid climate model,\nNeuralGCM, to optimize initial conditions and generate physically consistent\nworst-case heatwave trajectories. Applied to the 2021 Pacific Northwest\nheatwave, our method produces temperature anomalies up to 3.7 $^\\circ$C above\nthe most extreme member of a 75-member ensemble. These trajectories feature\nintensified atmospheric blocking and amplified Rossby wave patterns--hallmarks\nof severe heat events. Our results demonstrate that differentiable climate\nmodels can efficiently explore the upper tails of event likelihoods, providing\na powerful new approach for constructing targeted storylines of extreme weather\nunder climate change.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.10660v1", "AI": {"title_translation": "突破极端天气极限：利用可微分气候模型构建极端热浪情景", "tldr": "本文提出一种利用可微分混合气候模型（NeuralGCM）生成极端热浪最坏情景的新框架，能高效探索极端事件的上限，为气候变化下的风险评估提供新方法。", "motivation": "在气候变暖背景下，了解极端天气事件的合理上限对于风险评估至关重要。现有基于物理模型的集合方法计算成本高昂或缺乏模拟罕见高影响极端事件所需的保真度。", "method": "本文提出一种新颖的框架，利用可微分混合气候模型NeuralGCM优化初始条件，生成符合物理规律的最坏情况热浪轨迹。该方法应用于2021年太平洋西北部热浪。", "result": "该方法产生的温度异常比75个成员集合中最极端成员高出3.7°C。这些轨迹表现出大气阻塞加剧和罗斯贝波模式放大。", "conclusion": "研究结果表明，可微分气候模型可以有效地探索事件发生可能性的上限，为构建气候变化下极端天气的特定情景提供了一种强大的新方法。", "translation": "理解极端天气事件的合理上限对于变暖气候下的风险评估至关重要。现有基于大量物理模型集合的方法通常计算成本高昂，或者缺乏模拟罕见、高影响极端事件所需的保真度。在此，我们提出一个新颖的框架，该框架利用可微分混合气候模型NeuralGCM，优化初始条件并生成符合物理规律的最坏情况热浪轨迹。将该方法应用于2021年太平洋西北部热浪，我们的方法产生的温度异常比75个成员集合中最极端成员高出3.7°C。这些轨迹的特点是大气阻塞加剧和罗斯贝波模式放大——这是严重热事件的标志。我们的结果表明，可微分气候模型可以有效地探索事件发生可能性的上限，为构建气候变化下极端天气的特定情景提供了一种强大的新方法。", "summary": "本研究提出了一种利用可微分混合气候模型NeuralGCM来构建极端热浪情景的新框架。该方法通过优化初始条件，能够生成符合物理规律的最坏情况热浪轨迹。应用于2021年太平洋西北部热浪，该方法产生了比现有集合模型更极端的温度异常，并揭示了加剧的大气阻塞和罗斯贝波模式。这表明可微分气候模型能高效探索极端事件的上限，为气候变化下的风险评估提供了一种创新方法。", "keywords": "极端天气, 热浪, 可微分气候模型, NeuralGCM, 风险评估", "comments": "本文提出了一种新颖且具有潜力的极端天气事件模拟方法，通过引入可微分气候模型（如NeuralGCM），克服了传统物理模型集合在计算成本和对罕见极端事件模拟保真度方面的局限性。其创新点在于将优化技术与气候模型相结合，能够高效地探索极端事件的“上尾”可能性。这对于提升气候风险评估的准确性和针对性具有重要意义，尤其是在应对日益频繁和强烈的极端热浪方面。该方法为未来极端天气事件的预测和情景构建开辟了新途径。"}}
{"id": "2506.10664", "title": "Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning", "authors": ["Maxime Haddouche", "Otmane Sakhi"], "summary": "Off-policy learning serves as the primary framework for learning optimal\npolicies from logged interactions collected under a static behavior policy. In\nthis work, we investigate the more practical and flexible setting of adaptive\noff-policy learning, where policies are iteratively refined and re-deployed to\ncollect higher-quality data. Building on the success of PAC-Bayesian learning\nwith Logarithmic Smoothing (LS) in static settings, we extend this framework to\nthe adaptive scenario using tools from online PAC-Bayesian theory. Furthermore,\nwe demonstrate that a principled adjustment to the LS estimator naturally\naccommodates multiple rounds of deployment and yields faster convergence rates\nunder mild conditions. Our method matches the performance of leading offline\napproaches in static settings, and significantly outperforms them when\nintermediate policy deployments are allowed. Empirical evaluations across\ndiverse scenarios highlight both the advantages of adaptive data collection and\nthe strength of the PAC-Bayesian formulation.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10664v1", "AI": {"title_translation": "自适应PAC-贝叶斯离策略学习中的对数平滑", "tldr": "本文将对数平滑（LS）的PAC-贝叶斯学习框架扩展到自适应离策略学习场景，通过调整LS估计器以适应多轮部署，实现了更快的收敛速度，并在允许中间策略部署时显著优于现有方法。", "motivation": "现有的离策略学习主要关注静态行为策略下的数据收集，而本文旨在研究更实用和灵活的自适应离策略学习设置，其中策略被迭代优化和重新部署以收集更高质量的数据。", "method": "本文将静态设置中成功的对数平滑（LS）PAC-贝叶斯学习框架，利用在线PAC-贝叶斯理论的工具，扩展到自适应场景。此外，对LS估计器进行了一项原则性调整，使其自然地适应多轮部署。", "result": "该方法在温和条件下能产生更快的收敛速度。在静态设置中，其性能与领先的离线方法相匹配；在允许中间策略部署时，其性能显著优于这些方法。", "conclusion": "经验评估突出了自适应数据收集的优势以及PAC-贝叶斯公式的强大之处。", "translation": "离策略学习是根据静态行为策略下收集的记录交互来学习最优策略的主要框架。在这项工作中，我们研究了更实用和灵活的自适应离策略学习设置，其中策略被迭代优化并重新部署以收集更高质量的数据。基于对数平滑（LS）PAC-贝叶斯学习在静态设置中取得的成功，我们利用在线PAC-贝叶斯理论的工具将该框架扩展到自适应场景。此外，我们证明了对LS估计器进行一项原则性调整可以自然地适应多轮部署，并在温和条件下产生更快的收敛速度。我们的方法在静态设置中与领先的离线方法表现相当，并且在允许中间策略部署时显著优于它们。对各种场景的经验评估突出了自适应数据收集的优势和PAC-贝叶斯公式的强大之处。", "summary": "本文将对数平滑（LS）的PAC-贝叶斯学习框架从静态设置扩展到自适应离策略学习场景。通过对LS估计器进行调整，该方法能够适应多轮策略部署，并在温和条件下实现更快的收敛速度。实验结果表明，该方法在静态设置中性能与现有领先方法相当，而在允许中间策略部署时则表现出显著优势，证明了自适应数据收集和PAC-贝叶斯公式的有效性。", "keywords": "离策略学习, PAC-贝叶斯, 对数平滑, 自适应学习, 收敛速度", "comments": "本文的创新点在于将对数平滑（LS）PAC-贝叶斯学习框架扩展到更具挑战性和实际意义的自适应离策略学习场景。通过对LS估计器进行巧妙调整，该方法不仅在理论上保证了更快的收敛速度，而且在实践中也展现出优于传统离线方法的性能，尤其是在允许策略迭代部署的情况下。这对于需要持续学习和改进的实际应用具有重要意义。"}}
{"id": "2506.10868", "title": "A multi-scale loss formulation for learning a probabilistic model with proper score optimisation", "authors": ["Simon Lang", "Martin Leutbecher", "Pedro Maciel"], "summary": "We assess the impact of a multi-scale loss formulation for training\nprobabilistic machine-learned weather forecasting models. The multi-scale loss\nis tested in AIFS-CRPS, a machine-learned weather forecasting model developed\nat the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS-CRPS is\ntrained by directly optimising the almost fair continuous ranked probability\nscore (afCRPS). The multi-scale loss better constrains small scale variability\nwithout negatively impacting forecast skill. This opens up promising directions\nfor future work in scale-aware model training.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.10868v1", "AI": {"title_translation": "一种用于学习具有适当分数优化的概率模型的多尺度损失公式", "tldr": "评估多尺度损失在概率天气预报模型中对小尺度变异性的约束作用。", "motivation": "为了改进概率机器学习天气预报模型的训练，特别是更好地约束小尺度变异性，同时不负面影响预测技能。", "method": "提出并评估了一种多尺度损失公式，将其应用于欧洲中期天气预报中心（ECMWF）开发的AIFS-CRPS模型进行训练，该模型通过直接优化几乎公平的连续分级概率得分（afCRPS）。", "result": "多尺度损失能够更好地约束小尺度变异性，同时不负面影响预测技能。", "conclusion": "多尺度损失公式为未来尺度感知模型训练开辟了有前景的方向。", "translation": "我们评估了一种多尺度损失公式对训练概率机器学习天气预报模型的影响。这种多尺度损失在AIFS-CRPS中进行了测试，AIFS-CRPS是欧洲中期天气预报中心（ECMWF）开发的一种机器学习天气预报模型。AIFS-CRPS通过直接优化几乎公平的连续分级概率得分（afCRPS）进行训练。多尺度损失能更好地约束小尺度变异性，而不会对预测技能产生负面影响。这为未来尺度感知模型训练开辟了有前景的方向。", "summary": "本文评估了一种多尺度损失公式在训练概率机器学习天气预报模型中的效果。该损失在ECMWF开发的AIFS-CRPS模型中进行测试，该模型通过优化afCRPS进行训练。结果显示，多尺度损失能有效约束小尺度变异性，同时保持或不损害预测性能，为未来的尺度感知模型训练提供了新方向。", "keywords": "多尺度损失, 概率模型, 天气预报, afCRPS, 变异性约束", "comments": "这项工作通过引入多尺度损失，解决了天气预报模型中对小尺度变异性约束不足的问题，其创新性在于在不牺牲整体预测技能的前提下提升了模型对细节的捕捉能力。这对于提高天气预报的精度和实用性具有重要意义。"}}
{"id": "2506.10677", "title": "Practical Improvements of A/B Testing with Off-Policy Estimation", "authors": ["Sakhi Otmane", "Gilotte Alexandre", "Rohde David"], "summary": "We address the problem of A/B testing, a widely used protocol for evaluating\nthe potential improvement achieved by a new decision system compared to a\nbaseline. This protocol segments the population into two subgroups, each\nexposed to a version of the system and estimates the improvement as the\ndifference between the measured effects. In this work, we demonstrate that the\ncommonly used difference-in-means estimator, while unbiased, can be improved.\nWe introduce a family of unbiased off-policy estimators that achieves lower\nvariance than the standard approach. Among this family, we identify the\nestimator with the lowest variance. The resulting estimator is simple, and\noffers substantial variance reduction when the two tested systems exhibit\nsimilarities. Our theoretical analysis and experimental results validate the\neffectiveness and practicality of the proposed method.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10677v1", "AI": {"title_translation": "A/B测试中离线策略评估的实用改进", "tldr": "本文提出了一种新的无偏离线策略估计器家族，用于A/B测试，能够比传统方法显著降低方差，尤其当测试系统相似时效果更佳。", "motivation": "A/B测试中常用的均值差估计器虽然无偏，但仍有改进空间，旨在提高其效率和准确性。", "method": "引入了一个无偏离线策略估计器家族，通过理论分析和实验验证，从中确定了方差最低的估计器。", "result": "所提出的估计器简单易用，并且在两个测试系统具有相似性时，能够实现显著的方差降低。理论分析和实验结果验证了该方法的有效性和实用性。", "conclusion": "通过引入新的无偏离线策略估计器，可以显著提高A/B测试的效率和准确性，尤其在测试系统相似时效果显著，具有很强的实用性。", "translation": "我们解决了A/B测试的问题，这是一种广泛用于评估新决策系统相对于基线所能实现潜在改进的协议。该协议将人群分为两个子组，每个子组接触一个版本的系统，并通过测量效果之间的差异来估计改进。在这项工作中，我们证明了常用的均值差估计器虽然无偏，但可以得到改进。我们引入了一个无偏离线策略估计器家族，该家族比标准方法实现了更低的方差。在这个家族中，我们确定了方差最低的估计器。由此产生的估计器简单，并且当两个测试系统表现出相似性时，提供了显著的方差降低。我们的理论分析和实验结果验证了所提出方法的有效性和实用性。", "summary": "本文针对A/B测试中均值差估计器存在的方差问题，提出了一种新的无偏离线策略估计器家族。研究者通过理论分析和实验，从该家族中识别出方差最低的估计器。结果表明，这种新的估计器简单且能显著降低方差，尤其在测试系统相似时效果更佳，从而提高了A/B测试的效率和准确性。", "keywords": "A/B测试, 离线策略估计, 方差降低, 估计器", "comments": "本文提出了一种针对A/B测试中方差问题的实用改进方法。通过引入离线策略估计器，解决了传统均值差估计器效率不高的问题，特别是在系统相似性场景下实现了显著的方差降低，具有重要的实际应用价值。创新点在于提供了一个更优的估计器，提高了A/B测试的统计效率。"}}
{"id": "2506.10862", "title": "OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics", "authors": ["Rui Zhang", "Qi Meng", "Han Wan", "Yang Liu", "Zhi-Ming Ma", "Hao Sun"], "summary": "High-fidelity and efficient simulation of fluid dynamics drive progress in\nvarious scientific and engineering applications. Traditional computational\nfluid dynamics methods offer strong interpretability and guaranteed\nconvergence, but rely on fine spatial and temporal meshes, incurring\nprohibitive computational costs. Physics-informed neural networks (PINNs) and\nneural operators aim to accelerate PDE solvers using deep learning techniques.\nHowever, PINNs require extensive retraining and careful tuning, and purely\ndata-driven operators demand large labeled datasets. Hybrid physics-aware\nmethods embed numerical discretizations into network architectures or loss\nfunctions, but achieve marginal speed gains and become unstable when balancing\ncoarse priors against high-fidelity measurements. To this end, we introduce\nOmniFluids, a unified physics pre-trained operator learning framework that\nintegrates physics-only pre-training, coarse-grid operator distillation, and\nfew-shot fine-tuning, which enables fast inference and accurate prediction\nunder limited or zero data supervision. For architectural design, the key\ncomponents of OmniFluids include a mixture of operators, a multi-frame decoder,\nand factorized Fourier layers, which enable efficient and scalable modeling of\ndiverse physical tasks while maintaining seamless integration with\nphysics-based supervision. Across a broad range of two- and three-dimensional\nbenchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven\nmethods in flow field reconstruction and turbulence statistics accuracy,\ndelivering 10-100x speedups compared to classical solvers, and accurately\nrecovers unknown physical parameters from sparse, noisy data. This work\nestablishes a new paradigm for efficient and generalizable surrogate modeling\nin complex fluid systems under limited data availability.", "comment": null, "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.10862v1", "AI": {"title_translation": "OmniFluids：流体动力学统一物理预训练建模", "tldr": "OmniFluids是一个统一的物理预训练算子学习框架，通过整合物理预训练、粗网格蒸馏和少样本微调，实现了在有限或零数据监督下流体动力学的快速准确预测，相比传统求解器提速10-100倍。", "motivation": "高精度和高效的流体动力学模拟对于科学和工程应用至关重要。传统的计算流体力学方法计算成本高昂。物理信息神经网络（PINNs）和神经算子需要大量再训练和调整，纯数据驱动算子需要大量标记数据集。混合物理感知方法速度提升有限，且在平衡粗粒度先验与高精度测量时不稳定。", "method": "本文引入了OmniFluids，一个统一的物理预训练算子学习框架。该框架整合了纯物理预训练、粗网格算子蒸馏和少样本微调，以实现有限或零数据监督下的快速推理和准确预测。OmniFluids的关键架构组件包括算子混合器、多帧解码器和分解傅里叶层，这些组件能够高效、可扩展地建模各种物理任务，同时保持与基于物理的监督无缝集成。", "result": "在广泛的二维和三维基准测试中，OmniFluids在流场重建和湍流统计精度方面显著优于最先进的AI驱动方法，与经典求解器相比实现了10-100倍的速度提升，并能从稀疏、嘈杂的数据中准确恢复未知物理参数。", "conclusion": "这项工作为在数据有限的复杂流体系统中实现高效、可泛化的替代模型建立了一个新范式。", "translation": "高精度和高效的流体动力学模拟推动了各种科学和工程应用的进步。传统的计算流体力学方法具有很强的可解释性和收敛性保证，但依赖于精细的空间和时间网格，导致计算成本过高。物理信息神经网络（PINNs）和神经算子旨在利用深度学习技术加速偏微分方程求解器。然而，PINNs需要大量的再训练和仔细调整，而纯数据驱动的算子则需要大量的标记数据集。混合物理感知方法将数值离散化嵌入到网络架构或损失函数中，但速度提升微乎其微，并且在平衡粗粒度先验与高精度测量时变得不稳定。为此，我们引入了OmniFluids，一个统一的物理预训练算子学习框架，它整合了纯物理预训练、粗网格算子蒸馏和少样本微调，从而在有限或零数据监督下实现快速推理和准确预测。在架构设计方面，OmniFluids的关键组件包括算子混合器、多帧解码器和分解傅里叶层，这些组件能够高效、可扩展地建模各种物理任务，同时保持与基于物理的监督无缝集成。在广泛的二维和三维基准测试中，OmniFluids在流场重建和湍流统计精度方面显著优于最先进的AI驱动方法，与经典求解器相比实现了10-100倍的速度提升，并能从稀疏、嘈杂的数据中准确恢复未知物理参数。这项工作为在数据有限的复杂流体系统中实现高效、可泛化的替代模型建立了一个新范式。", "summary": "本文提出了OmniFluids，一个统一的物理预训练算子学习框架，旨在克服传统计算流体力学方法计算成本高昂、现有深度学习方法（如PINNs和纯数据驱动算子）需要大量数据或难以调整的局限性。OmniFluids通过整合纯物理预训练、粗网格算子蒸馏和少样本微调，实现了在有限或零数据监督下对流体动力学的快速准确预测。其核心架构包括算子混合器、多帧解码器和分解傅里叶层。实验结果表明，OmniFluids在二维和三维流体模拟中显著优于现有AI方法，实现了10-100倍的速度提升，并能准确恢复未知物理参数，为复杂流体系统在数据有限条件下的高效通用代理建模提供了新范式。", "keywords": "流体动力学, 算子学习, 物理预训练, 深度学习, 计算流体力学", "comments": "OmniFluids的创新之处在于其统一的框架，有效地结合了物理知识和深度学习的优势，通过预训练、蒸馏和微调的策略，解决了现有方法在效率、数据依赖和稳定性方面的痛点。其在速度和精度上的显著提升，尤其是在数据稀缺条件下的表现，预示着其在实际工程和科学应用中巨大的潜力。"}}
{"id": "2506.10872", "title": "The Gittins Index: A Design Principle for Decision-Making Under Uncertainty", "authors": ["Ziv Scully", "Alexander Terenin"], "summary": "The Gittins index is a tool that optimally solves a variety of\ndecision-making problems involving uncertainty, including multi-armed bandit\nproblems, minimizing mean latency in queues, and search problems like the\nPandora's box model. However, despite the above examples and later extensions\nthereof, the space of problems that the Gittins index can solve perfectly\noptimally is limited, and its definition is rather subtle compared to those of\nother multi-armed bandit algorithms. As a result, the Gittins index is often\nregarded as being primarily a concept of theoretical importance, rather than a\npractical tool for solving decision-making problems.\n  The aim of this tutorial is to demonstrate that the Gittins index can be\nfruitfully applied to practical problems. We start by giving an example-driven\nintroduction to the Gittins index, then walk through several examples of\nproblems it solves - some optimally, some suboptimally but still with excellent\nperformance. Two practical highlights in the latter category are applying the\nGittins index to Bayesian optimization, and applying the Gittins index to\nminimizing tail latency in queues.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10872v1", "AI": {"title_translation": "Gittins 指数：不确定性下决策的设计原则", "tldr": "Gittins 指数是一种解决不确定性下决策问题的工具。尽管其应用范围有限且定义微妙，本教程旨在通过示例展示其在贝叶斯优化和队列尾部延迟最小化等实际问题中的有效应用。", "motivation": "Gittins 指数常被视为纯粹的理论概念而非实用工具，且其完美最优解的问题空间有限、定义微妙。本文旨在纠正这种看法，证明 Gittins 指数可以有效地应用于实际问题。", "method": "本教程通过示例驱动的方式介绍 Gittins 指数，并逐步展示其解决多种问题的能力，包括一些最优解和一些次优但性能优异的例子，特别强调了其在贝叶斯优化和队列尾部延迟最小化中的应用。", "result": "论文展示了 Gittins 指数可以有效地应用于实际问题，包括在贝叶斯优化和队列尾部延迟最小化等场景中实现优秀性能，即使是在提供次优解的情况下。", "conclusion": "Gittins 指数不仅仅是一个理论概念，它也是一个可以有效应用于实际决策问题的工具，即便在某些情况下只能提供次优解，其性能依然出色。", "translation": "Gittins 指数是一种能够最优地解决各种涉及不确定性的决策问题的工具，包括多臂老虎机问题、最小化队列平均延迟以及潘多拉魔盒模型等搜索问题。然而，尽管有上述例子及其后续扩展，Gittins 指数能够完美最优解决的问题空间是有限的，而且与其他的多臂老虎机算法相比，它的定义相当微妙。因此，Gittins 指数通常被认为主要是一个具有理论重要性的概念，而非解决决策问题的实用工具。\n本教程的目的是证明 Gittins 指数可以有效地应用于实际问题。我们首先通过示例驱动的方式介绍 Gittins 指数，然后逐步讲解它解决的几个问题示例——有些是最佳的，有些是次优但仍表现出色的。后一类中的两个实际亮点是将 Gittins 指数应用于贝叶斯优化，以及将其应用于最小化队列中的尾部延迟。", "summary": "本教程旨在纠正 Gittins 指数被视为纯理论工具的普遍看法，通过提供示例驱动的介绍，并展示其在解决多臂老虎机、队列优化和搜索问题中的应用，证明其在实际问题中的有效性。论文强调了 Gittins 指数在贝叶斯优化和队列尾部延迟最小化等实际场景中，即使是次优解也能表现出色。", "keywords": "Gittins 指数, 不确定性决策, 多臂老虎机, 贝叶斯优化, 队列优化", "comments": "本文创新性地将 Gittins 指数从一个理论概念提升为实用的决策工具，通过具体示例展示了其在复杂不确定性问题中的应用潜力。其重要性在于拓宽了 Gittins 指数的应用边界，特别是指出其在次优解情况下仍能提供优秀性能，这对于实际工程应用具有重要指导意义。"}}
{"id": "2506.10879", "title": "A Goemans-Williamson type algorithm for identifying subcohorts in clinical trials", "authors": ["Pratik Worah"], "summary": "We design an efficient algorithm that outputs a linear classifier for\nidentifying homogeneous subsets (equivalently subcohorts) from large\ninhomogeneous datasets. Our theoretical contribution is a rounding technique,\nsimilar to that of Goemans and Williamson (1994), that approximates the optimal\nsolution of the underlying optimization problem within a factor of $0.82$. As\nan application, we use our algorithm to design a simple test that can identify\nhomogeneous subcohorts of patients, that are mainly comprised of metastatic\ncases, from the RNA microarray dataset for breast cancer by Curtis et al.\n(2012). Furthermore, we also use the test output by the algorithm to\nsystematically identify subcohorts of patients in which statistically\nsignificant changes in methylation levels of tumor suppressor genes co-occur\nwith statistically significant changes in nuclear receptor expression.\nIdentifying such homogeneous subcohorts of patients can be useful for the\ndiscovery of disease pathways and therapeutics, specific to the subcohort.", "comment": null, "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.10879v1", "AI": {"title_translation": "临床试验中识别亚群的Goemans-Williamson型算法", "tldr": "开发了一种基于Goemans-Williamson型算法的线性分类器，用于从异质数据集中识别同质亚群，并应用于乳腺癌数据以发现疾病相关亚群。", "motivation": "从大型异质数据集中识别同质子群（亚群），这对于发现疾病通路和特定治疗方法非常有用。", "method": "设计了一种高效的算法，该算法输出一个线性分类器。其理论贡献是一种类似于Goemans和Williamson (1994) 的舍入技术，能将底层优化问题的最优解近似到0.82的因子。", "result": "该算法成功应用于乳腺癌RNA微阵列数据集，识别出主要由转移性病例组成的同质患者亚群。此外，还系统地识别出肿瘤抑制基因甲基化水平和核受体表达同时发生显著变化的患者亚群。", "conclusion": "识别同质患者亚群对于发现疾病通路和特定于亚群的治疗方法具有潜在的帮助。", "translation": "我们设计了一种高效的算法，该算法输出一个线性分类器，用于从大型异质数据集中识别同质子集（等同于亚群）。我们的理论贡献是一种类似于Goemans和Williamson (1994) 的舍入技术，它能将底层优化问题的最优解近似到0.82的因子。作为一项应用，我们使用我们的算法设计了一个简单的测试，可以从Curtis等人(2012)的乳腺癌RNA微阵列数据集中识别出主要由转移性病例组成的同质患者亚群。此外，我们还使用该算法输出的测试系统地识别出肿瘤抑制基因甲基化水平发生统计学显著变化与核受体表达发生统计学显著变化同时出现的患者亚群。识别此类同质患者亚群对于发现疾病通路和特定于该亚群的治疗方法可能有用。", "summary": "本文提出了一种基于Goemans-Williamson型舍入技术的高效线性分类算法，用于从大型异质数据集中识别同质亚群。该算法能将优化问题近似到0.82的因子。作为应用，该算法被用于乳腺癌RNA微阵列数据集，成功识别出转移性病例亚群以及肿瘤抑制基因甲基化和核受体表达同时变化的患者亚群，这对于疾病通路和特定治疗的发现具有重要意义。", "keywords": "亚群识别, 线性分类器, Goemans-Williamson, 临床试验, 乳腺癌", "comments": "该论文的创新点在于引入了Goemans-Williamson类型的舍入技术来解决亚群识别问题，并提供了理论近似保证。其重要性在于提供了一种有效工具来处理临床试验中的异质性数据，有助于发现疾病的生物标志物和潜在治疗靶点。"}}
{"id": "2506.10899", "title": "Demystifying Spectral Feature Learning for Instrumental Variable Regression", "authors": ["Dimitri Meunier", "Antoine Moulin", "Jakub Wornbard", "Vladimir R. Kostic", "Arthur Gretton"], "summary": "We address the problem of causal effect estimation in the presence of hidden\nconfounders, using nonparametric instrumental variable (IV) regression. A\nleading strategy employs spectral features - that is, learned features spanning\nthe top eigensubspaces of the operator linking treatments to instruments. We\nderive a generalization error bound for a two-stage least squares estimator\nbased on spectral features, and gain insights into the method's performance and\nfailure modes. We show that performance depends on two key factors, leading to\na clear taxonomy of outcomes. In a good scenario, the approach is optimal. This\noccurs with strong spectral alignment, meaning the structural function is\nwell-represented by the top eigenfunctions of the conditional operator, coupled\nwith this operator's slow eigenvalue decay, indicating a strong instrument.\nPerformance degrades in a bad scenario: spectral alignment remains strong, but\nrapid eigenvalue decay (indicating a weaker instrument) demands significantly\nmore samples for effective feature learning. Finally, in the ugly scenario,\nweak spectral alignment causes the method to fail, regardless of the\neigenvalues' characteristics. Our synthetic experiments empirically validate\nthis taxonomy.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10899v1", "AI": {"title_translation": "揭秘工具变量回归中的谱特征学习", "tldr": "本文分析了非参数工具变量（IV）回归中谱特征的性能，并根据谱对齐和特征值衰减识别出三种情景（好、坏、差），同时推导了泛化误差界。", "motivation": "解决在存在隐藏混杂因素的情况下，使用非参数工具变量（IV）回归进行因果效应估计的问题，并深入理解谱特征学习方法的性能。", "method": "本文推导了基于谱特征的两阶段最小二乘估计器的泛化误差界。通过分析谱对齐和特征值衰减这两个关键因素，提出了该方法性能的三种分类（好、坏、差）。通过合成实验验证了这种分类。", "result": "方法性能取决于两个关键因素：谱对齐（结构函数被条件算子的顶部特征函数表示的程度）和特征值衰减（工具变量的强度）。这导致了三种结果：1. 好情景：强谱对齐和慢特征值衰减，性能最佳。2. 坏情景：强谱对齐但快特征值衰减，需要更多样本。3. 差情景：弱谱对齐，方法失败。合成实验验证了此分类。", "conclusion": "谱特征学习在工具变量回归中的性能关键取决于谱对齐和工具变量的强度（特征值衰减），这导致了从最优到完全失败的可预测结果。", "translation": "我们解决了在存在隐藏混杂因素的情况下，使用非参数工具变量（IV）回归进行因果效应估计的问题。一种主要策略是采用谱特征——即，学习到的特征跨越连接处理与工具的操作符的顶部特征子空间。我们推导了基于谱特征的两阶段最小二乘估计器的泛化误差界，并深入了解了该方法的性能和失败模式。我们表明，性能取决于两个关键因素，从而形成清晰的结果分类。在好的情况下，该方法是最佳的。这发生在强谱对齐时，意味着结构函数通过条件算子的顶部特征函数得到了很好的表示，并且该算子的特征值衰减缓慢，表明工具变量强。在坏的情况下，性能会下降：谱对齐仍然很强，但特征值衰减迅速（表明工具变量弱）需要显著更多的样本才能有效学习特征。最后，在糟糕的情况下，弱谱对齐会导致该方法失败，无论特征值的特性如何。我们的合成实验经验性地验证了这种分类。", "summary": "本论文深入探讨了在存在隐藏混杂因素的情况下，非参数工具变量（IV）回归中谱特征学习的有效性。作者为基于谱特征的两阶段最小二乘估计器推导了泛化误差界，并揭示了该方法性能的两个核心决定因素：谱对齐度（结构函数与条件算子顶部特征函数的匹配程度）和特征值衰减速度（工具变量的强度）。基于这些因素，论文提出了一个清晰的性能分类体系，包括“好”、“坏”和“差”三种情景，详述了每种情景下的表现。合成实验经验性地验证了这一分类，为理解谱特征学习何时表现最佳、何时性能下降以及何时完全失效提供了宝贵见解。", "keywords": "谱特征, 工具变量回归, 因果推断, 泛化误差界, 隐藏混杂因素", "comments": "该论文对工具变量回归中的谱特征学习提供了有价值的理论分析，通过引入谱对齐和特征值衰减的概念，清晰地解释了其性能表现，并提出了一个可预测的分类体系。这有助于“揭秘”该方法的内部机制，使研究人员和实践者能更好地理解其适用范围和局限性。特别是识别出导致方法失效的“差”情景，具有重要的实践指导意义。"}}
{"id": "2506.10908", "title": "Probably Approximately Correct Labels", "authors": ["Emmanuel J. Candès", "Andrew Ilyas", "Tijana Zrnic"], "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10908v1", "AI": {"title_translation": "可能近似正确的标签", "tldr": "该论文提出了一种通过结合专家标签和AI预测来经济高效地构建高质量标注数据集的方法，确保了高概率下的低总体标注错误，并在文本、图像和蛋白质折叠分析中得到验证。", "motivation": "获取高质量的标注数据集通常成本高昂，需要大量人工标注或昂贵的实验。", "method": "提出一种方法，用预训练模型的人工智能预测来补充“专家”标签，以更具成本效益的方式构建标注数据集。该方法旨在实现“可能近似正确的标签”，即总体标注错误以高概率保持较小。", "result": "该方法能够实现严谨而高效的数据集整理。通过使用大型语言模型进行文本标注、使用预训练视觉模型进行图像标注以及使用AlphaFold进行蛋白质折叠分析，展示了该方法的好处。", "conclusion": "通过利用AI预测补充专家标签，该方法能够以成本效益高的方式创建高质量数据集，产生“可能近似正确的标签”，且总体错误率较低。", "translation": "获取高质量的标注数据集通常成本高昂，需要大量人工标注或昂贵的实验。我们提出一种方法，用预训练模型的人工智能预测来补充此类“专家”标签，以更具成本效益的方式构建标注数据集。我们的方法产生了可能近似正确的标签：以高概率，总体标注错误很小。该解决方案能够使用现代人工智能模型进行严谨而高效的数据集整理。我们通过使用大型语言模型进行文本标注、使用预训练视觉模型进行图像标注以及使用AlphaFold进行蛋白质折叠分析，展示了该方法的好处。", "summary": "本文提出了一种经济高效的方法来构建高质量的标注数据集，通过将昂贵的“专家”标签与预训练模型的AI预测相结合。该方法确保了“可能近似正确的标签”，即总体标注错误以高概率保持较小，从而实现高效且严谨的数据集整理。其有效性在文本标注、图像标注和蛋白质折叠分析等多个领域得到了验证。", "keywords": "标注数据集, AI预测, 成本效益, 数据整理, 可能近似正确", "comments": "该论文的创新之处在于利用AI预测来降低获取高质量标注数据的成本和工作量，解决了AI发展中的一个重要瓶颈。‘可能近似正确的标签’的概念为生成数据集的质量提供了理论保证，这对于实际应用至关重要。其在自然语言处理、计算机视觉和生物信息学等不同领域的适用性突出了其广泛的实用性。"}}
{"id": "2506.10929", "title": "On feature selection in double-imbalanced data settings: a Random Forest approach", "authors": ["Fabio Demaria"], "summary": "Feature selection is a critical step in high-dimensional classification\ntasks, particularly under challenging conditions of double imbalance, namely\nsettings characterized by both class imbalance in the response variable and\ndimensional asymmetry in the data $(n \\gg p)$. In such scenarios, traditional\nfeature selection methods applied to Random Forests (RF) often yield unstable\nor misleading importance rankings. This paper proposes a novel thresholding\nscheme for feature selection based on minimal depth, which exploits the tree\ntopology to assess variable relevance. Extensive experiments on simulated and\nreal-world datasets demonstrate that the proposed approach produces more\nparsimonious and accurate subsets of variables compared to conventional minimal\ndepth-based selection. The method provides a practical and interpretable\nsolution for variable selection in RF under double imbalance conditions.", "comment": "Working paper", "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.10929v1", "AI": {"title_translation": "双重不平衡数据设置下的特征选择：一种随机森林方法", "tldr": "本文提出了一种基于最小深度的随机森林新颖阈值方案，用于在响应变量存在类别不平衡和数据维度不对称（$n \text{»} p$）的双重不平衡数据设置下进行特征选择，实验证明该方法比传统方法更精简和准确。", "motivation": "在响应变量存在类别不平衡和数据维度不对称（$n \text{»} p$）的双重不平衡数据设置下，高维分类任务中的特征选择是一个关键步骤，但传统应用于随机森林的特征选择方法往往产生不稳定或误导性的重要性排名。", "method": "本文提出了一种基于最小深度的特征选择新颖阈值方案，该方案利用树拓扑结构来评估变量相关性。", "result": "在模拟和真实世界数据集上的大量实验表明，所提出的方法与传统的基于最小深度的选择方法相比，产生了更精简和准确的变量子集。", "conclusion": "该方法为双重不平衡条件下的随机森林变量选择提供了一种实用且可解释的解决方案。", "translation": "特征选择是高维分类任务中的关键一步，特别是在双重不平衡的挑战性条件下，即响应变量存在类别不平衡和数据维度不对称（$n \\gg p$）的设置。在这种情况下，应用于随机森林（RF）的传统特征选择方法通常会产生不稳定或误导性的重要性排名。本文提出了一种基于最小深度的特征选择新颖阈值方案，该方案利用树拓扑结构来评估变量相关性。在模拟和真实世界数据集上的大量实验表明，所提出的方法与传统的基于最小深度的选择方法相比，产生了更精简和准确的变量子集。该方法为双重不平衡条件下的随机森林变量选择提供了一种实用且可解释的解决方案。", "summary": "本文针对响应变量类别不平衡和数据维度不对称的双重不平衡数据设置，提出了一种基于最小深度的新颖阈值方案，用于随机森林的特征选择。该方法利用树拓扑结构评估变量相关性，并通过实验证明其在生成更精简和准确的变量子集方面优于传统方法，为双重不平衡条件下的变量选择提供了一个实用且可解释的解决方案。", "keywords": "特征选择, 双重不平衡数据, 随机森林, 最小深度, 变量选择", "comments": "本文创新性地解决了随机森林在双重不平衡数据下特征选择不稳定的问题，提出了一种基于最小深度的新颖阈值方案。其重要性在于提供了一种实用且可解释的方法，显著提高了特征选择的准确性和精简性。"}}
{"id": "2506.10944", "title": "Coupled reaction and diffusion governing interface evolution in solid-state batteries", "authors": ["Jingxuan Ding", "Laura Zichi", "Matteo Carli", "Menghang Wang", "Albert Musaelian", "Yu Xie", "Boris Kozinsky"], "summary": "Understanding and controlling the atomistic-level reactions governing the\nformation of the solid-electrolyte interphase (SEI) is crucial for the\nviability of next-generation solid state batteries. However, challenges persist\ndue to difficulties in experimentally characterizing buried interfaces and\nlimits in simulation speed and accuracy. We conduct large-scale explicit\nreactive simulations with quantum accuracy for a symmetric battery cell,\n{\\symcell}, enabled by active learning and deep equivariant neural network\ninteratomic potentials. To automatically characterize the coupled reactions and\ninterdiffusion at the interface, we formulate and use unsupervised\nclassification techniques based on clustering in the space of local atomic\nenvironments. Our analysis reveals the formation of a previously unreported\ncrystalline disordered phase, Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$, in the\nSEI, that evaded previous predictions based purely on thermodynamics,\nunderscoring the importance of explicit modeling of full reaction and transport\nkinetics. Our simulations agree with and explain experimental observations of\nthe SEI formations and elucidate the Li creep mechanisms, critical to dendrite\ninitiation, characterized by significant Li motion along the interface. Our\napproach is to crease a digital twin from first principles, without adjustable\nparameters fitted to experiment. As such, it offers capabilities to gain\ninsights into atomistic dynamics governing complex heterogeneous processes in\nsolid-state synthesis and electrochemistry.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.10944v1", "AI": {"title_translation": "耦合反应和扩散控制固态电池中的界面演化", "tldr": "本文利用主动学习和深度等变神经网络势能，进行了大规模量子精度反应模拟，揭示了固态电池SEI中一个未曾报道的晶体无序相的形成，并解释了实验观察到的SEI形成和锂蠕变机制。", "motivation": "理解和控制固态电解质界面（SEI）的原子级反应对于下一代固态电池的实际应用至关重要。然而，由于实验表征埋藏界面的困难以及模拟速度和精度的限制，挑战依然存在。", "method": "研究人员利用主动学习和深度等变神经网络原子间势能，对对称电池单元进行了大规模、显式的量子精度反应模拟。为了自动表征界面处的耦合反应和相互扩散，他们制定并使用了基于局部原子环境空间聚类的无监督分类技术。", "result": "分析揭示了SEI中形成了先前未报道的晶体无序相Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$，该相规避了之前纯粹基于热力学的预测，强调了明确建模完整反应和传输动力学的重要性。模拟结果与SEI形成的实验观察结果一致并对其进行了解释，阐明了锂蠕变机制（对枝晶形成至关重要，其特征是锂沿界面发生显著运动）。", "conclusion": "该方法从第一性原理创建了一个数字孪生，没有经过实验拟合的可调参数。因此，它提供了深入了解固态合成和电化学中复杂异质过程的原子动力学的能力。", "translation": "理解和控制固态电解质界面（SEI）形成的原子级反应对于下一代固态电池的实际应用至关重要。然而，由于实验表征埋藏界面的困难以及模拟速度和精度的限制，挑战依然存在。我们利用主动学习和深度等变神经网络原子间势能，对对称电池单元进行了大规模、显式的量子精度反应模拟。为了自动表征界面处的耦合反应和相互扩散，我们制定并使用了基于局部原子环境空间聚类的无监督分类技术。我们的分析揭示了SEI中形成了先前未报道的晶体无序相Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$，该相规避了之前纯粹基于热力学的预测，强调了明确建模完整反应和传输动力学的重要性。我们的模拟结果与SEI形成的实验观察结果一致并对其进行了解释，阐明了锂蠕变机制（对枝晶形成至关重要，其特征是锂沿界面发生显著运动）。我们的方法是从第一性原理创建了一个数字孪生，没有经过实验拟合的可调参数。因此，它提供了深入了解固态合成和电化学中复杂异质过程的原子动力学的能力。", "summary": "本研究通过结合主动学习和深度等变神经网络原子间势能，对固态电池中的界面演化进行了大规模、量子精度的反应模拟。利用无监督分类技术分析了界面处的耦合反应和相互扩散，首次发现了SEI中一个未曾报道的晶体无序相Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$。该工作强调了显式建模反应和传输动力学的重要性，并成功解释了SEI形成和锂蠕变的实验观察结果，为固态电池材料设计提供了从第一性原理出发的见解。", "keywords": "固态电池, SEI, 反应扩散, 原子模拟, 活性学习", "comments": "本文的创新之处在于结合了主动学习、深度等变神经网络势能和大规模量子精度模拟，以克服传统模拟和实验在表征埋藏界面时的局限性。通过发现新的SEI相并解释锂蠕变机制，为固态电池关键界面的理解提供了重要见解，尤其强调了动力学在SEI形成中的作用，而非仅仅是热力学。其“数字孪生”方法无需实验拟合参数，具有普适性和前瞻性。"}}
{"id": "2506.10971", "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10971v1", "AI": {"title_translation": "引导在掩码离散扩散模型中究竟做了什么", "tldr": "本文研究了掩码离散扩散模型中的无分类器引导（CFG），推导了引导反向动力学的显式解，揭示了引导如何影响采样行为，并观察到引导在放大特定类别区域、抑制共享区域以及影响采样轨迹动力学方面的作用。", "motivation": "研究无分类器引导（CFG）在掩码离散扩散模型中的作用，以精确刻画引导如何影响采样行为。", "method": "在没有分数误差和离散化误差的假设下，推导了引导反向动力学的显式解，并通过理论分析和实验支持了研究发现。", "result": "1. 引导能够放大类别特定区域并抑制与其他类别共享的区域。2. 引导效果取决于引导强度w，并导致采样分布中独特的协方差结构。3. 在1D和2D中观察到定量上不同的行为。4. 对于大的w，反向动力学沿线的总变异（TV）衰减率在1D和2D中都是w的双指数级。", "conclusion": "引导不仅塑造输出分布，还控制采样轨迹的动力学。", "translation": "我们研究了带有无分类器引导（CFG）的掩码离散扩散模型。假设没有分数误差和离散化误差，我们推导出了引导反向动力学的显式解，从而可以精确地描述引导如何影响采样行为。当完整数据分布是类别混合体且目标是从特定类别中采样时，引导会放大类别特定区域，同时抑制与其他类别共享的区域。这种效应取决于引导强度w，并在采样分布中诱导独特的协方差结构。值得注意的是，我们观察到1D和2D中定量上不同的行为。我们还表明，对于大的w，反向动力学沿线的总变异（TV）衰减率在1D和2D中都是w的双指数级。这些发现强调了引导的作用，它不仅塑造了输出分布，而且控制着采样轨迹的动力学。我们的理论分析得到了实验的支持，这些实验说明了引导的几何效应及其对收敛的影响。", "summary": "本文深入研究了无分类器引导（CFG）在掩码离散扩散模型中的作用。通过推导引导反向动力学的显式解，作者精确地刻画了引导如何影响采样过程。研究发现，引导能够放大类别特有的区域并抑制共享区域，其效果与引导强度w相关，并导致独特的协方差结构。此外，文章还指出总变异的衰减率对于大w是双指数级的。这些发现揭示了引导不仅影响最终输出分布，还控制采样轨迹的动态。", "keywords": "掩码离散扩散模型, 无分类器引导, 采样行为, 总变异, 动力学", "comments": "这篇论文通过严格的理论推导，为理解无分类器引导在离散扩散模型中的作用提供了深刻见解。其创新之处在于提供了引导反向动力学的显式解，这对于精确分析引导机制至关重要。研究结果揭示了引导对类别区域的放大作用以及对采样轨迹动力学的控制，这对于改进扩散模型的生成质量和效率具有重要意义。"}}
