{"id": "2506.21688", "title": "CyGym: A Simulation-Based Game-Theoretic Analysis Framework for Cybersecurity", "authors": ["Michael Lanier", "Yevgeniy Vorobeychik"], "summary": "We introduce a novel cybersecurity encounter simulator between a network\ndefender and an attacker designed to facilitate game-theoretic modeling and\nanalysis while maintaining many significant features of real cyber defense. Our\nsimulator, built within the OpenAI Gym framework, incorporates realistic\nnetwork topologies, vulnerabilities, exploits (including-zero-days), and\ndefensive mechanisms. Additionally, we provide a formal simulation-based\ngame-theoretic model of cyberdefense using this simulator, which features a\nnovel approach to modeling zero-days exploits, and a PSRO-style approach for\napproximately computing equilibria in this game. We use our simulator and\nassociated game-theoretic framework to analyze the Volt Typhoon advanced\npersistent threat (APT). Volt Typhoon represents a sophisticated cyber attack\nstrategy employed by state-sponsored actors, characterized by stealthy,\nprolonged infiltration and exploitation of network vulnerabilities. Our\nexperimental results demonstrate the efficacy of game-theoretic strategies in\nunderstanding network resilience against APTs and zero-days, such as Volt\nTyphoon, providing valuable insight into optimal defensive posture and\nproactive threat mitigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21688v1", "categories": ["cs.CR", "cs.GT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21688v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21874", "title": "On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling", "authors": ["Stanley Wu", "Ronik Bhaskar", "Anna Yoo Jeong Ha", "Shawn Shan", "Haitao Zheng", "Ben Y. Zhao"], "summary": "Today's text-to-image generative models are trained on millions of images\nsourced from the Internet, each paired with a detailed caption produced by\nVision-Language Models (VLMs). This part of the training pipeline is critical\nfor supplying the models with large volumes of high-quality image-caption pairs\nduring training. However, recent work suggests that VLMs are vulnerable to\nstealthy adversarial attacks, where adversarial perturbations are added to\nimages to mislead the VLMs into producing incorrect captions.\n  In this paper, we explore the feasibility of adversarial mislabeling attacks\non VLMs as a mechanism to poisoning training pipelines for text-to-image\nmodels. Our experiments demonstrate that VLMs are highly vulnerable to\nadversarial perturbations, allowing attackers to produce benign-looking images\nthat are consistently miscaptioned by the VLM models. This has the effect of\ninjecting strong \"dirty-label\" poison samples into the training pipeline for\ntext-to-image models, successfully altering their behavior with a small number\nof poisoned samples. We find that while potential defenses can be effective,\nthey can be targeted and circumvented by adaptive attackers. This suggests a\ncat-and-mouse game that is likely to reduce the quality of training data and\nincrease the cost of text-to-image model development. Finally, we demonstrate\nthe real-world effectiveness of these attacks, achieving high attack success\n(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex\nAI and Microsoft Azure).", "comment": "ACM Conference on Computer and Communications Security 2025", "pdf_url": "http://arxiv.org/pdf/2506.21874v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21874v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21897", "title": "One Video to Steal Them All: 3D-Printing IP Theft through Optical Side-Channels", "authors": ["Twisha Chattopadhyay", "Fabricio Ceschin", "Marco E. Garza", "Dymytriy Zyunkin", "Animesh Chhotaray", "Aaron P. Stebner", "Saman Zonouz", "Raheem Beyah"], "summary": "The 3D printing industry is rapidly growing and increasingly adopted across\nvarious sectors including manufacturing, healthcare, and defense. However, the\noperational setup often involves hazardous environments, necessitating remote\nmonitoring through cameras and other sensors, which opens the door to\ncyber-based attacks. In this paper, we show that an adversary with access to\nvideo recordings of the 3D printing process can reverse engineer the underlying\n3D print instructions. Our model tracks the printer nozzle movements during the\nprinting process and maps the corresponding trajectory into G-code\ninstructions. Further, it identifies the correct parameters such as feed rate\nand extrusion rate, enabling successful intellectual property theft. To\nvalidate this, we design an equivalence checker that quantitatively compares\ntwo sets of 3D print instructions, evaluating their similarity in producing\nobjects alike in shape, external appearance, and internal structure. Unlike\nsimple distance-based metrics such as normalized mean square error, our\nequivalence checker is both rotationally and translationally invariant,\naccounting for shifts in the base position of the reverse engineered\ninstructions caused by different camera positions. Our model achieves an\naverage accuracy of 90.87 percent and generates 30.20 percent fewer\ninstructions compared to existing methods, which often produce faulty or\ninaccurate prints. Finally, we demonstrate a fully functional counterfeit\nobject generated by reverse engineering 3D print instructions from video.", "comment": "17 pages [Extended Version]", "pdf_url": "http://arxiv.org/pdf/2506.21897v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21897v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21914", "title": "Consumer Beware! Exploring Data Brokers' CCPA Compliance", "authors": ["Elina van Kempen", "Isita Bagayatkar", "Pavel Frolikov", "Chloe Georgiou", "Gene Tsudik"], "summary": "Data brokers collect and sell the personal information of millions of\nindividuals, often without their knowledge or consent. The California Consumer\nPrivacy Act (CCPA) grants consumers the legal right to request access to, or\ndeletion of, their data. To facilitate these requests, California maintains an\nofficial registry of data brokers. However, the extent to which these entities\ncomply with the law is unclear.\n  This paper presents the first large-scale, systematic study of CCPA\ncompliance of all 543 officially registered data brokers. Data access requests\nwere manually submitted to each broker, followed by in-depth analyses of their\nresponses (or lack thereof). Above 40% failed to respond at all, in an apparent\nviolation of the CCPA. Data brokers that responded requested personal\ninformation as part of their identity verification process, including details\nthey had not previously collected. Paradoxically, this means that exercising\none's privacy rights under CCPA introduces new privacy risks.\n  Our findings reveal rampant non-compliance and lack of standardization of the\ndata access request process. These issues highlight an urgent need for stronger\nenforcement, clearer guidelines, and standardized, periodic compliance checks\nto enhance consumers' privacy protections and improve data broker\naccountability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21914v1", "categories": ["cs.CR", "cs.CY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21914v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21931", "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21931v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21931v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21627", "title": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models", "authors": ["Shiyi Wang", "Wenbo Li", "Yiteng Chen", "Qingyao Wu", "Huiping Zhuang"], "summary": "Developing a general robot manipulation system capable of performing a wide\nrange of tasks in complex, dynamic, and unstructured real-world environments\nhas long been a challenging task. It is widely recognized that achieving\nhuman-like efficiency and robustness manipulation requires the robotic brain to\nintegrate a comprehensive set of functions, such as task planning, policy\ngeneration, anomaly monitoring and handling, and long-term memory, achieving\nhigh-efficiency operation across all functions. Vision-Language Models (VLMs),\npretrained on massive multimodal data, have acquired rich world knowledge,\nexhibiting exceptional scene understanding and multimodal reasoning\ncapabilities. However, existing methods typically focus on realizing only a\nsingle function or a subset of functions within the robotic brain, without\nintegrating them into a unified cognitive architecture. Inspired by a\ndivide-and-conquer strategy and the architecture of the human brain, we propose\nFrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that\nachieves both comprehensive functionality and high operational efficiency. Our\nframework includes a suite of components, decoupling a part of key functions\nfrom frequent VLM calls, striking an optimal balance between functional\ncompleteness and system efficiency. Specifically, we map task planning, policy\ngeneration, memory management, and low-level interfacing to the cortex,\ncerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and\ndesign efficient coordination mechanisms for the modules. We conducted\ncomprehensive experiments in both simulation and real-world robotic\nenvironments, demonstrating that our method offers significant advantages in\nanomaly detection and handling, long-term memory, operational efficiency, and\nstability -- all without requiring any fine-tuning or retraining.", "comment": "15 pages, 4 figures, under review of NeurIPS", "pdf_url": "http://arxiv.org/pdf/2506.21627v1", "categories": ["cs.RO", "cs.AI", "F.4.3; I.2.9"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21627v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21634", "title": "How (Not) To Write a Software Engineering Abstract", "authors": ["Lutz Prechelt", "Lloyd Montgomery", "Julian Frattini", "Franz Zieris"], "summary": "Background: Abstracts are a particularly valuable element in a software\nengineering research article. However, not all abstracts are as informative as\nthey could be. Objective: Characterize the structure of abstracts in\nhigh-quality software engineering venues. Observe and quantify deficiencies.\nSuggest guidelines for writing informative abstracts. Methods: Use qualitative\nopen coding to derive concepts that explain relevant properties of abstracts.\nIdentify the archetypical structure of abstracts. Use quantitative content\nanalysis to objectively characterize abstract structure of a sample of 362\nabstracts from five presumably high-quality venues. Use exploratory data\nanalysis to find recurring issues in abstracts. Compare the archetypical\nstructure to actual structures. Infer guidelines for producing informative\nabstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,\nprovide background, objective, method, result, and conclusion information. For\nstructured abstracts, the ratio is twice as big. Only 4% of the abstracts are\nproper, i.e., they also have good readability (Flesch-Kincaid score) and have\nno informativeness gaps, understandability gaps, nor highly ambiguous\nsentences. Conclusions: (1) Even in top venues, a large majority of abstracts\nare far from ideal. (2) Structured abstracts tend to be better than\nunstructured ones. (3) Artifact-centric works need a different structured\nformat. (4) The community should start requiring conclusions that generalize,\nwhich currently are often missing in abstracts.", "comment": "16 pages, 11 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.21634v1", "categories": ["cs.SE", "D.2.0; A.m; K.m"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21634v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.21762", "title": "ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues", "authors": ["Oliver Huang", "Carolina Nobre"], "summary": "Data visualization tasks often require multi-step reasoning, and the\ninterpretive strategies experts use, such as decomposing complex goals into\nsmaller subtasks and selectively attending to key chart regions are rarely made\nexplicit. ViStruct is an automated pipeline that simulates these expert\nbehaviours by breaking high-level visual questions into structured analytic\nsteps and highlighting semantically relevant chart areas. Leveraging large\nlanguage and vision-language models, ViStruct identifies chart components, maps\nsubtasks to spatial regions, and presents visual attention cues to externalize\nexpert-like reasoning flows. While not designed for direct novice instruction,\nViStruct provides a replicable model of expert interpretation that can inform\nthe development of future visual literacy tools. We evaluate the system on 45\ntasks across 12 chart types and validate its outputs with trained visualization\nusers, confirming its ability to produce interpretable and expert-aligned\nreasoning sequences.", "comment": "VIS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21762v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21762v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22103", "title": "Quantifying Institutional Gender Inequality in Contemporary Visual Art", "authors": ["Xindi Wang", "Alexander J. Gates", "Magnus Resch", "Albert-Laszlo Barabasi"], "summary": "From disparities in the number of exhibiting artists to auction\nopportunities, there is evidence of women's under-representation in visual art.\nHere we explore the exhibition history and auction sales of 65,768 contemporary\nartists in 20,389 institutions, revealing gender differences in the artist\npopulation, exhibitions and auctions. We distinguish between two criteria for\ngender equity: gender-neutrality, when artists have gender-independent access\nto exhibition opportunities, and gender-balanced, that strives for gender\nparity in representation, finding that 58\\% of institutions are gender-neutral\nbut only 24\\% are gender-balanced, and that the fraction of man-overrepresented\ninstitutions increases with institutional prestige. We define artist's\nco-exhibition gender to capture the gender inequality of the institutions that\nan artist exhibits. Finally, we use logistic regression to predict an artist's\naccess to the auction market, finding that co-exhibition gender has a stronger\ncorrelation with success than the artist's gender. These results help unveil\nand quantify the institutional forces that relate to the persistent gender\nimbalance in the art world.", "comment": "35 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22103v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22103v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21933", "title": "Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion", "authors": ["Yifan Xue", "Ruihuai Liang", "Bo Yang", "Xuelin Cao", "Zhiwen Yu", "Mérouane Debbah", "Chau Yuen"], "summary": "With the rapid development of the low-altitude economy, air-ground integrated\nmulti-access edge computing (MEC) systems are facing increasing demands for\nreal-time and intelligent task scheduling. In such systems, task offloading and\nresource allocation encounter multiple challenges, including node\nheterogeneity, unstable communication links, and dynamic task variations. To\naddress these issues, this paper constructs a three-layer heterogeneous MEC\nsystem architecture for low-altitude economic networks, encompassing aerial and\nground users as well as edge servers. The system is systematically modeled from\nthe perspectives of communication channels, computational costs, and constraint\nconditions, and the joint optimization problem of offloading decisions and\nresource allocation is uniformly abstracted into a graph-structured modeling\ntask. On this basis, we propose a graph attention diffusion-based solution\ngenerator (GADSG). This method integrates the contextual awareness of graph\nattention networks with the solution distribution learning capability of\ndiffusion models, enabling joint modeling and optimization of discrete\noffloading variables and continuous resource allocation variables within a\nhigh-dimensional latent space. We construct multiple simulation datasets with\nvarying scales and topologies. Extensive experiments demonstrate that the\nproposed GADSG model significantly outperforms existing baseline methods in\nterms of optimization performance, robustness, and generalization across task\nstructures, showing strong potential for efficient task scheduling in dynamic\nand complex low-altitude economic network environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21933v1", "categories": ["cs.NI", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.21933v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21968", "title": "Multi-IRS Aided ISAC System: Multi-Path Exploitation Versus Reduction", "authors": ["Guangji Chen", "Qingqing Wu", "Shihang Lu", "Meng Hua", "Wen Chen"], "summary": "This paper investigates a multi-intelligent reflecting surface (IRS) aided\nintegrated sensing and communication (ISAC) system, where multiple IRSs are\nstrategically deployed not only to assist the communication from a\nmulti-antenna base station (BS) to a multi-antenna communication user (CU), but\nalso enable the sensing service for a point target in the non-line-of-sight\n(NLoS) region of the BS. First, we propose a hybrid multi-IRS architecture,\nwhich consists of several passive IRSs and one semi-passive IRS equipped with\nboth active sensors and reflecting elements. To be specific, the active sensors\nare exploited to receive the echo signals for estimating the target's angle\ninformation, and the multiple reflecting paths provided by multi-IRS are\nemployed to improve the degree of freedoms (DoFs) of communication. Under the\ngiven budget on the number of total IRSs elements, we theoretically show that\nincreasing the number of deployed IRSs is beneficial for improving DoFs of\nspatial multiplexing for communication while increasing the Cramer-Rao bound\n(CRB) of target estimation, which unveils a fundamental tradeoff between the\nsensing and communication performance. To characterize the rate-CRB tradeoff,\nwe study a rate maximization problem, by optimizing the BS transmit covariance\nmatrix, IRSs phase-shifts, and the number of deployed IRSs, subject to a\nmaximum CRB constraint. Analytical results reveal that the\ncommunication-oriented design becomes optimal when the total number of IRSs\nelements exceeds a certain threshold, wherein the relationships of the rate and\nCRB with the number of IRS elements/sensors, transmit power, and the number of\ndeployed IRSs are theoretically derived and demystified. Simulation results\nvalidate our theoretical findings and also demonstrate the superiority of our\nproposed designs over the benchmark schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21968v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21968v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21656", "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.", "comment": "29 pages", "pdf_url": "http://arxiv.org/pdf/2506.21656v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21656v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22107", "title": "Power- and Area-Efficient Unary Sorting Architecture Using FSM-Based Unary Number Generator", "authors": ["Amir Hossein Jalilvand", "M. Hassan Najafi"], "summary": "Sorting is a fundamental operation in computer systems and is widely used in\napplications such as databases, data analytics, and hardware accelerators.\nUnary computing has recently emerged as a low-cost and power-efficient paradigm\nfor implementing hardware sorters by eliminating the need for complex\narithmetic operations. However, existing comparison-free unary computing-based\ndesigns suffer from significant area and power overhead due to costly unary\nnumber generators.\n  In this paper, we present a novel ascending-order unary sorting module\nfeaturing a finite-state-machine-based unary number generator that\nsignificantly reduces implementation costs. By generating right-aligned unary\nstreams using a two-state finite-state machine, our architecture iteratively\nidentifies the minimum input value in each cycle without conventional\ncomparators. Synthesis results in a 45nm technology node demonstrate up to 82%\nreduction in area and 70% reduction in power consumption compared to\nstate-of-the-art unary designs. The proposed sorter offers a promising solution\nfor energy-constrained and resource-limited hardware systems.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2506.22107v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.22107v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22033", "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference", "authors": ["Yongchao He", "Bohan Zhao", "Zheng Cao"], "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22033v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22033v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21743", "title": "Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting", "authors": ["Jinpai Zhao", "Albert Cerrone", "Eirik Valseth", "Leendert Westerink", "Clint Dawson"], "summary": "Storm surge forecasting plays a crucial role in coastal disaster\npreparedness, yet existing machine learning approaches often suffer from\nlimited spatial resolution, reliance on coastal station data, and poor\ngeneralization. Moreover, many prior models operate directly on unstructured\nspatial data, making them incompatible with modern deep learning architectures.\nIn this work, we introduce a novel approach that projects unstructured water\nelevation fields onto structured Red Green Blue (RGB)-encoded image\nrepresentations, enabling the application of Convolutional Long Short Term\nMemory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our\nmodel further integrates ground-truth wind fields as dynamic conditioning\nsignals and topo-bathymetry as a static input, capturing physically meaningful\ndrivers of surge evolution. Evaluated on a large-scale dataset of synthetic\nstorms in the Gulf of Mexico, our method demonstrates robust 48-hour\nforecasting performance across multiple regions along the Texas coast and\nexhibits strong spatial extensibility to other coastal areas. By combining\nstructured representation, physically grounded forcings, and scalable deep\nlearning, this study advances the frontier of storm surge forecasting in\nusability, adaptability, and interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21743v1", "categories": ["cs.CE", "cs.LG"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21743v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21669", "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "authors": ["Wanxin Tian", "Shijie Zhang", "Kevin Zhang", "Xiaowei Chi", "Yulin Luo", "Junyu Lu", "Chunkai Fan", "Qiang Zhou", "Yiming Zhao", "Ning Liu Siyu Lin", "Zhiyuan Qin", "Xiaozhu Ju", "Shanghang Zhang", "Jian Tang"], "summary": "Self-evolution, the ability of agents to autonomously improve their reasoning\nand behavior, is essential for the embodied domain with long-horizon,\nreal-world tasks. Despite current advancements in reinforcement fine-tuning\n(RFT) showing strong performance in enhancing reasoning in LLMs, its potential\nto enable self-evolving embodied intelligence with multi-modal interactions\nremains largely unexplored. Specifically, reinforcement fine-tuning faces two\nfundamental obstacles in embodied settings: (i) the lack of accessible\nintermediate rewards in multi-step reasoning tasks limits effective learning\nsignals, and (ii) reliance on hand-crafted reward functions restricts\ngeneralization to novel tasks and environments. To address these challenges, we\npresent Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework\ndesigned for enabling the self-evolving capabilities of embodied agents.\nSpecifically, to convert sparse delayed rewards into denser intermediate\nsignals that improve multi-step reasoning, we propose Tree-based group relative\npolicy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into\nGRPO. To generalize reward estimation across tasks and scenes, supporting\nautonomous adaptation and reward-driven self-evolution, we further introduce\nMulti-modal Generative Reward Model (MGRM). To holistically evaluate the\neffectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing\nstate-of-the-art methods with scores of 85.07% (textual) and 36.19%\n(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also\nachieves scores of 80.3% without environmental reward, surpassing all\nopen-source baselines and highlighting its scalability as a self-evolving\nembodied agent. Additional experiments and qualitative analysis further support\nthe potential of SEEA-R1 for future research in scalable embodied intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21669v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21669v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21697", "title": "Stochastic Neural Control Barrier Functions", "authors": ["Hongchao Zhang", "Manan Tayal", "Jackson Cox", "Pushpak Jagtap", "Shishir Kolathaya", "Andrew Clark"], "summary": "Control Barrier Functions (CBFs) are utilized to ensure the safety of control\nsystems. CBFs act as safety filters in order to provide safety guarantees\nwithout compromising system performance. These safety guarantees rely on the\nconstruction of valid CBFs. Due to their complexity, CBFs can be represented by\nneural networks, known as neural CBFs (NCBFs). Existing works on the\nverification of the NCBF focus on the synthesis and verification of NCBFs in\ndeterministic settings, leaving the stochastic NCBFs (SNCBFs) less studied. In\nthis work, we propose a verifiably safe synthesis for SNCBFs. We consider the\ncases of smooth SNCBFs with twice-differentiable activation functions and\nSNCBFs that utilize the Rectified Linear Unit or ReLU activation function. We\npropose a verification-free synthesis framework for smooth SNCBFs and a\nverification-in-the-loop synthesis framework for both smooth and ReLU SNCBFs.\nand we validate our frameworks in three cases, namely, the inverted pendulum,\nDarboux, and the unicycle model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21697v1", "categories": ["eess.SY", "cs.RO", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21697v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21664", "title": "When Every Symbol Counts: Resilient Wireless Systems Under Finite Blocklength Constraints", "authors": ["Kevin Weinberger", "Aydin Sezgin"], "summary": "As 6G evolves, wireless networks become essential for critical operations and\nenable innovative applications that demand seamless adaptation to dynamic\nenvironments and disruptions. Because these vital services require\nuninterrupted operation, their resilience to unforeseen disruptions is\nessential. However, implementing resilience necessitates rapid recovery\nprocedures, which operate in the finite blocklength (FBL) regime, where short\npackets and added error-correction overhead can severely degrade communication\nefficiency. Due to this performance loss, always attempting recovery can\nbackfire and result in worse outcomes than simply enduring the disruption under\nlonger blocklengths. In this work, we study these effects of FBL constraints\nwithin a resilience framework, incorporating reconfigurable intelligent\nsurfaces (RIS) to enhance adaptation capabilities. By actively shaping the\nwireless environment, RIS help counteract some of the performance losses caused\nby FBL, enabling more effective recovery from disruptions. Numerical results\nreveal two critical blocklength thresholds: the first enables full recovery\nfrom the FBL penalty, while the second, at a higher blocklength, allows the\nsystem to recover from both the FBL penalty and the initial disruption,\nyielding a significant improvement in resilience performance. Additionally, we\nshow that the number of RIS elements shifts these thresholds, enabling faster\nreconfiguration with shorter blocklengths and providing insights to the\ntrade-offs between rate, blocklength, and reconfiguration effort under FBL\nconditions.", "comment": "6 pages, 3 figures, submitted to European Wireless 2025. arXiv admin\n  note: text overlap with arXiv:2504.11589", "pdf_url": "http://arxiv.org/pdf/2506.21664v1", "categories": ["eess.SP", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21664v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21680", "title": "PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors", "authors": ["Sai Sri Teja", "Sreevidya Chintalapati", "Vinayak Gupta", "Mukund Varma T", "Haejoon Lee", "Aswin Sankaranarayanan", "Kaushik Mitra"], "summary": "Advances in 3D reconstruction using neural rendering have enabled\nhigh-quality 3D capture. However, they often fail when the input imagery is\ncorrupted by motion blur, due to fast motion of the camera or the objects in\nthe scene. This work advances neural rendering techniques in such scenarios by\nusing single-photon avalanche diode (SPAD) arrays, an emerging sensing\ntechnology capable of sensing images at extremely high speeds. However, the use\nof SPADs presents its own set of unique challenges in the form of binary\nimages, that are driven by stochastic photon arrivals. To address this, we\nintroduce PhotonSplat, a framework designed to reconstruct 3D scenes directly\nfrom SPAD binary images, effectively navigating the noise vs. blur trade-off.\nOur approach incorporates a novel 3D spatial filtering technique to reduce\nnoise in the renderings. The framework also supports both no-reference using\ngenerative priors and reference-based colorization from a single blurry image,\nenabling downstream applications such as segmentation, object detection and\nappearance editing tasks. Additionally, we extend our method to incorporate\ndynamic scene representations, making it suitable for scenes with moving\nobjects. We further contribute PhotonScenes, a real-world multi-view dataset\ncaptured with the SPAD sensors.", "comment": "Accepted at the International Conference on Computational\n  Photography(ICCP) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21680v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21680v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21998", "title": "INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User Privacy at the Edge", "authors": ["Rémy Raes", "Olivier Ruas", "Adrien Luxey-Bitri", "Romain Rouvoy"], "summary": "Data streams produced by mobile devices, such as smartphones, offer highly\nvaluable sources of information to build ubiquitous services. Such data streams\nare generally uploaded and centralized to be processed by third parties,\npotentially exposing sensitive personal information. In this context, existing\nprotection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),\nhave been investigated. Alas, none of them have actually been implemented, nor\ndeployed in real-life, in mobile devices to enforce user privacy at the edge.\nMoreover, the diversity of embedded sensors and the resulting data deluge makes\nit impractical to provision such services directly on mobiles, due to their\nconstrained storage capacity, communication bandwidth and processing power.\nThis article reports on the FLI technique, which leverages a piece-wise linear\napproximation technique to capture compact representations of data streams in\nmobile devices. Beyond the FLI storage layer, we introduce Divide \\& Stay, a\nnew privacy preservation technique to execute Points of Interest (POIs)\ninference. Finally, we deploy both of them on Android and iOS as the INTACT\nframework, making a concrete step towards enforcing privacy and trust in\nubiquitous computing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21998v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21998v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21629", "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian\nSplatting (3DGS) have made significant progress in scene reconstruction and\nnovel view synthesis. However, they heavily rely on preprocessed camera poses\nand 3D structural priors from structure-from-motion (SfM), which are\nchallenging to obtain in outdoor scenarios. To address this challenge, we\npropose to incorporate Iterative Closest Point (ICP) with optimization-based\nrefinement to achieve accurate camera pose estimation under large camera\nmovements. Additionally, we introduce a voxel-based scene densification\napproach to guide the reconstruction in large-scale scenes. Experiments\ndemonstrate that our approach ICP-3DGS outperforms existing methods in both\ncamera pose estimation and novel view synthesis across indoor and outdoor\nscenes of various scales. Source code is available at\nhttps://github.com/Chenhao-Z/ICP-3DGS.", "comment": "6 pages, Source code is available at\n  https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "pdf_url": "http://arxiv.org/pdf/2506.21629v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21629v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21951", "title": "HighRateMOS: Sampling-Rate Aware Modeling for Speech Quality Assessment", "authors": ["Wenze Ren", "Yi-Cheng Lin", "Wen-Chin Huang", "Ryandhimas E. Zezario", "Szu-Wei Fu", "Sung-Feng Huang", "Erica Cooper", "Haibin Wu", "Hung-Yu Wei", "Hsin-Min Wang", "Hung-yi Lee", "Yu Tsao"], "summary": "Modern speech quality prediction models are trained on audio data resampled\nto a specific sampling rate. When faced with higher-rate audio at test time,\nthese models can produce biased scores. We introduce HighRateMOS, the first\nnon-intrusive mean opinion score (MOS) model that explicitly considers sampling\nrate. HighRateMOS ensembles three model variants that exploit the following\ninformation: (i) a learnable embedding of speech sampling rate, (ii) Wav2vec\n2.0 self-supervised embeddings, (iii) multi-scale CNN spectral features, and\n(iv) MFCC features. In AudioMOS 2025 Track3, HighRateMOS ranked first in five\nout of eight metrics. Our experiments confirm that modeling the sampling rate\ndirectly leads to more robust and sampling-rate-agnostic speech quality\npredictions.", "comment": "Under Review, 3 pages + 1 References", "pdf_url": "http://arxiv.org/pdf/2506.21951v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21951v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21555", "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "comment": "Accepted in Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21555v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21555v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21579", "title": "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation", "authors": ["Yingzhi He", "Xiaohao Liu", "An Zhang", "Yunshan Ma", "Tat-Seng Chua"], "summary": "Sequential recommendation aims to predict users' future interactions by\nmodeling collaborative filtering (CF) signals from historical behaviors of\nsimilar users or items. Traditional sequential recommenders predominantly rely\non ID-based embeddings, which capture CF signals through high-order\nco-occurrence patterns. However, these embeddings depend solely on past\ninteractions, lacking transferable knowledge to generalize to unseen domains.\nRecent advances in large language models (LLMs) have motivated text-based\nrecommendation approaches that derive item representations from textual\ndescriptions. While these methods enhance generalization, they fail to encode\nCF signals-i.e., latent item correlations and preference patterns-crucial for\neffective recommendation. We argue that an ideal embedding model should\nseamlessly integrate CF signals with rich semantic representations to improve\nboth in-domain and out-of-domain recommendation performance.\n  To this end, we propose LLM2Rec, a novel embedding model tailored for\nsequential recommendation, integrating the rich semantic understanding of LLMs\nwith CF awareness. Our approach follows a two-stage training framework: (1)\nCollaborative Supervised Fine-tuning, which adapts LLMs to infer item\nrelationships based on historical interactions, and (2) Item-level Embedding\nModeling, which refines these specialized LLMs into structured item embedding\nmodels that encode both semantic and collaborative information. Extensive\nexperiments on real-world datasets demonstrate that LLM2Rec effectively\nimproves recommendation quality across both in-domain and out-of-domain\nsettings. Our findings highlight the potential of leveraging LLMs to build more\nrobust, generalizable embedding models for sequential recommendation. Our codes\nare available at https://github.com/HappyPointer/LLM2Rec.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.21579v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21579v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21699", "title": "Inverse scattering without phase: Carleman convexification and phase retrieval via the Wentzel--Kramers--Brillouin approximation", "authors": ["Thuy T. Le", "Phuong M. Nguyen", "Loc H. Nguyen"], "summary": "This paper addresses the challenging and interesting inverse problem of\nreconstructing the spatially varying dielectric constant of a medium from\nphaseless backscattering measurements generated by single-point illumination.\nThe underlying mathematical model is governed by the three-dimensional\nHelmholtz equation, and the available data consist solely of the magnitude of\nthe scattered wave field. To address the nonlinearity and severe ill-posedness\nof this phaseless inverse scattering problem, we introduce a robust, globally\nconvergent numerical framework combining several key regularization strategies.\nOur method first employs a phase retrieval step based on the\nWentzel--Kramers--Brillouin (WKB) ansatz, where the lost phase information is\nreconstructed by solving a nonlinear optimization problem. Subsequently, we\nimplement a Fourier-based dimension reduction technique, transforming the\noriginal problem into a more stable system of elliptic equations with Cauchy\nboundary conditions. To solve this resulting system reliably, we apply the\nCarleman convexification approach, constructing a strictly convex weighted cost\nfunctional whose global minimizer provides an accurate approximation of the\ntrue solution. Numerical simulations using synthetic data with high noise\nlevels demonstrate the effectiveness and robustness of the proposed method,\nconfirming its capability to accurately recover both the geometric location and\ncontrast of hidden scatterers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21699v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21699v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22113", "title": "Exploring Commutative Matrix Multiplication Schemes via Flip Graphs", "authors": ["Isaac Wood"], "summary": "We explore new approaches for finding matrix multiplication algorithms in the\ncommutative setting by adapting the flip graph technique: a method previously\nshown to be effective for discovering fast algorithms in the non-commutative\ncase. While an earlier attempt to apply flip graphs to commutative algorithms\nsaw limited success, we overcome both theoretical and practical obstacles using\ntwo strategies: one inspired by Marakov's algorithm to multiply 3x3 matrices,\nin which we construct a commutative tensor and approximate its rank using the\nstandard flip graph; and a second that introduces a fully commutative variant\nof the flip graph defined via a quotient tensor space. We also present a hybrid\nmethod that combines the strengths of both. Across all matrix sizes up to 5x5,\nthese methods recover the best known bounds on the number of multiplications\nand allow for a comparison of their efficiency and efficacy. Although no new\nimprovements are found, our results demonstrate strong potential for these\ntechniques at larger scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22113v1", "categories": ["cs.SC"], "cate": "cs.SC", "url": "http://arxiv.org/abs/2506.22113v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21915", "title": "An Effective Two-Phase Genetic Algorithm for Solving the Resource Constrained Project Scheduling Problem (RCPSP)", "authors": ["D. Sun", "S. Zhou"], "summary": "This note presents a simple and effective variation of genetic algorithm (GA)\nfor solving RCPSP, denoted as 2-Phase Genetic Algorithm (2PGA). The 2PGA\nimplements GA parent selection in two phases: Phase-1 includes the best current\nsolutions in the parent pool, and Phase-2 excludes the best current solutions\nfrom the parent pool. The 2PGA carries out the GA evolution by alternating the\ntwo phases iteratively. In exploring a solution space, the Phase-1 emphasizes\nintensification in current neighborhood, while the Phase-2 emphasizes\ndiversification to escape local traps. The 2PGA was tested on the standard\nbenchmark problems in PSPLIB, the results have shown that the algorithm is\neffective and has improved some of the best heuristic solutions.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.21915v1", "categories": ["cs.NE", "math.OC", "90-08"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21915v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22052", "title": "Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles", "authors": ["Nico Ostendorf", "Keno Garlichs", "Lars Wolf"], "summary": "V2X communication has become crucial for enhancing road safety, especially\nfor Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the\nincreasing number of devices communicating on the same channels will lead to\nsignificant channel load. To address this issue this study evaluates the\neffectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),\nfocusing specifically on cyclists. The objective of RM is to minimize the\ntransmission of redundant information. We conducted a simulation study using a\nurban scenario with a high bicycle density based on traffic data from Hannover,\nGermany. This study assessed the impact of RM on channel load, measured by\nChannel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in\nsimulation. To evaluate the accuracy and reliability of the RM mechanisms, we\nanalyzed the actual differences in position, speed, and heading between the ego\nVRU and the VRU, which was assumed to be redundant. Our findings indicate that\nwhile RM can reduce channel congestion, it also leads to a decrease in VPR. The\nanalysis of actual differences revealed that the RM mechanism standardized by\nETSI often uses outdated information, leading to significant discrepancies in\nposition, speed, and heading, which could result in dangerous situations. To\naddress these limitations, we propose an adapted RM mechanism that improves the\nbalance between reducing channel load and maintaining VRU awareness. The\nadapted approach shows a significant reduction in maximum CBR and a less\nsignificant decrease in VPR compared to the standardized RM. Moreover, it\ndemonstrates better performance in the actual differences in position, speed,\nand heading, thereby enhancing overall safety. Our results highlight the need\nfor further research to optimize RM techniques and ensure they effectively\nenhance V2X communication without compromising the safety of VRUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22052v1", "categories": ["cs.ET", "cs.NI", "eess.SP"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.22052v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21655", "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21655v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21655v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21794", "title": "Shifting Narratives: A Longitudinal Analysis of Media Trends and Public Attitudes on Homelessness", "authors": ["Akshay Irudayaraj", "Nathan Ye", "Yash Chainani"], "summary": "Within the field of media framing, homelessness has been a historically\nunder-researched topic. Framing theory states that the media's method of\npresenting information plays a pivotal role in controlling public sentiment\ntoward a topic. The sentiment held towards homeless individuals influences\ntheir ability to access jobs, housing, and resources as a result of\ndiscrimination. This study analyzes the topic and sentiment trends in related\nmedia articles to validate framing theory within the scope of homelessness. It\ncorrelates these shifts in media reporting with public sentiment. We examine\nstate-level trends in California, Florida, Washington, Oregon, and New York\nfrom 2015 to 2023. We utilize the GDELT 2.0 Global Knowledge Graph (GKG)\ndatabase to gather article data and use X to measure public sentiment towards\nhomeless individuals. Additionally, to identify if there is a correlation\nbetween media reporting and public policy, we examine the media's impact on\nstate-level legislation. Our research uses Granger-causality tests and vector\nautoregressive (VAR) models to establish a correlation between media framing\nand public sentiment. We also use latent Dirichlet allocation (LDA) and GPT-3.5\n(LLM-as-annotator paradigm) for topic modeling and sentiment analysis. Our\nfindings demonstrate a statistically significant correlation between media\nframing and public sentiment, especially in states with high homelessness\nrates. We found no significant correlation between media framing and\nlegislation, suggesting a possible disconnect between public opinion and\npolicy-making. These findings reveal the broader impact of the media's framing\ndecisions and delineate its ability to affect society.", "comment": "21 pages, 7 figures, 12 tables", "pdf_url": "http://arxiv.org/pdf/2506.21794v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21794v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21728", "title": "A Finite-State Symbolic Automaton Model for the Collatz Map and Its Convergence Properties", "authors": ["Leonard Ben Aurel Brauer"], "summary": "We present a finite-state, deterministic automaton that emulates the Collatz\nfunction by operating on base-10 digit sequences. Each digit is represented as\na symbolic triplet capturing its value, the parity of the next digit, and a\nlocal carry value, resulting in a state space of exactly 60 configurations. The\ntransition rules are local, total, and parity-dependent, yet collectively\nreproduce the global behavior of the Collatz map through digitwise operations.\nAll symbolic trajectories reduce to the unique terminal cycle (4, 0, 0) -> (2,\n0, 0) -> (1, 0, 0), offering a constructive, automaton-theoretic encoding of\nthe Collatz dynamics. A primitive recursive ranking function ensures symbolic\ntermination within the proposed model and supports a convergence argument that\nis fully formalizable in Peano Arithmetic. This approach introduces a novel\nframework for analyzing arithmetic dynamics via symbolic computation and\nautomata theory.", "comment": "Version 1. A related preprint is available on Zenodo:\n  https://doi.org/10.5281/zenodo.15742096. Formalization in Lean is ongoing.\n  Comments appreciated", "pdf_url": "http://arxiv.org/pdf/2506.21728v1", "categories": ["cs.FL"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.21728v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22180", "title": "Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study", "authors": ["Önder Gürcan"], "summary": "The industrial market continuously needs reliable solutions to secure\nautonomous systems. Especially as these systems become more complex and\ninterconnected, reliable security solutions are becoming increasingly\nimportant. One promising solution to tackle this challenge is using smart\ncontracts designed to meet contractual conditions, avoid malicious errors,\nsecure exchanges, and minimize the need for reliable intermediaries. However,\nsmart contracts are immutable. Moreover, there are different smart contract\nexecution architectures (namely Order-Execute and Execute-Order-Validate) that\nhave different throughputs. In this study, we developed an evaluation model for\nassessing the security of reliable smart contract execution. We then developed\na realistic smart contract enabled IoT energy case study. Finally, we simulate\nthe developed case study to evaluate several smart contract security\nvulnerabilities reported in the literature. Our results show that the\nExecute-Order-Validate architecture is more promising regarding reliability and\nsecurity.", "comment": "23 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.22180v1", "categories": ["cs.CR", "cs.DC"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22180v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21976", "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21976v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21976v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21628", "title": "Ark: An Open-source Python-based Framework for Robot Learning", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21628v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21628v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21654", "title": "Experience converting a large mathematical software package written in C++ to C++20 modules", "authors": ["Wolfgang Bangerth"], "summary": "Mathematical software has traditionally been built in the form of \"packages\"\nthat build on each other. A substantial fraction of these packages is written\nin C++ and, as a consequence, the interface of a package is described in the\nform of header files that downstream packages and applications can then\n#include. C++ has inherited this approach towards exporting interfaces from C,\nbut the approach is clunky, unreliable, and slow. As a consequence, C++20 has\nintroduced a \"module\" system in which packages explicitly export declarations\nand code that compilers then store in machine-readable form and that downstream\nusers can \"import\" -- a system in line with what many other programming\nlanguages have used for decades.\n  Herein, I explore how one can convert large mathematical software packages\nwritten in C++ to this system, using the deal.II finite element library with\nits around 800,000 lines of code as an example. I describe an approach that\nallows providing both header-based and module-based interfaces from the same\ncode base, discuss the challenges one encounters, and how modules actually work\nin practice in a variety of technical and human metrics. The results show that\nwith a non-trivial, but also not prohibitive effort, the conversion to modules\nis possible, resulting in a reduction in compile time for the converted library\nitself; on the other hand, for downstream projects, compile times show no clear\ntrend. I end with thoughts about long-term strategies for converting the entire\necosystem of mathematical software over the coming years or decades.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21654v1", "categories": ["cs.SE", "cs.MS"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21654v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21780", "title": "Avatars and Environments for Meetings in Social VR: What Styles and Choices Matter to People in Group Creativity Tasks?", "authors": ["Anya Osborne", "Sabrina Fielder", "Lee Taber", "Tara Lamb", "Joshua McVeigh-Schultz", "Katherine Isbister"], "summary": "Due to the COVID-19 pandemic, many professional entities shifted toward\nremote collaboration and video conferencing (VC) tools. Social virtual reality\n(VR) platforms present an alternative to VC for meetings and collaborative\nactivities. Well-crafted social VR environments could enhance feelings of\nco-presence and togetherness at meetings, helping reduce the need for\ncarbon-intensive travel to face-to-face meetings. This research contributes to\ncreating meeting tools in VR by exploring the effects of avatar styles and\nvirtual environments on groups creative performance using the Mozilla Hubs\nplatform. We present the results of two sequential studies. Study One surveys\navatar and environment preferences in various VR meeting contexts (N=87). Study\nTwo applies these findings to the design of a between-subjects and\nwithin-subjects research where participants (N=40) perform creativity tasks in\npairs as embodied avatars in different virtual settings using VR headsets. We\ndiscuss the design implications of avatar appearances and meeting settings on\nteamwork.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21780v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21780v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22165", "title": "The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment", "authors": ["Lorenz Wendlinger", "Simon Alexander Nonn", "Abdullah Al Zubaer", "Michael Granitzer"], "summary": "Legal systems heavily rely on cross-citations of legal norms as well as\nprevious court decisions. Practitioners, novices and legal AI systems need\naccess to these relevant data to inform appraisals and judgments. We propose a\nGraph-Neural-Network (GNN) link prediction model that can identify Case-Law and\nCase-Case citations with high proficiency through fusion of semantic and\ntopological information. We introduce adapted relational graph convolutions\noperating on an extended and enriched version of the original citation graph\nthat allow the topological integration of semantic meta-information. This\nfurther improves prediction by 3.1 points of average precision and by 8.5\npoints in data sparsity as well as showing robust performance over time and in\nchallenging fully inductive prediction. Jointly learning and predicting case\nand norm citations achieves a large synergistic effect that improves case\ncitation prediction by up to 4.7 points, at almost doubled efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22165v1", "categories": ["cs.SI", "cs.IR"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22165v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22148", "title": "Resilient Communication For Avalanche Response in Infrastructure-Limited Environments", "authors": ["Joshua Goulton", "Milena Radenkovic"], "summary": "Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining\ncommunication in infrastructure limited environments, such as those encountered\nduring natural disasters. This paper investigates the viability of leveraging\nan existing national transport system - the Swiss rail network - as a data mule\nbackbone for disseminating critical avalanche alerts. Using The Opportunistic\nNetwork Environment (ONE) simulator, we model the entire Swiss rail network and\nconduct a rigorous comparative analysis of two seminal DTN routing protocols:\nEpidemic and PROPHET. Experiments are performed in two distinct scenarios:\nalerts originating from dense urban centres and from sparse, remote mountainous\nregions. Our results demonstrate that the rail network provides robust\nconnectivity for opportunistic communication in both environments thus\nvalidating the integration of DTN principles in remote scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22148v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22148v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22000", "title": "Heterogeneous Massive MIMO: A Cost-Efficient Technique for Uniform Service in Cellular Networks", "authors": ["Wei Jiang", "Hans D. Schotten"], "summary": "Massive multi-input multi-output (MIMO) has evolved along two tracks:\ncellular and cell-free, each with unique advantages and limitations. The\ncellular approach suffers from worse user spectral efficiency at cell edges,\nwhereas the cell-free approach incurs high implementation costs due to a\nlarge-scale distributed infrastructure. This paper introduces a novel\nnetworking paradigm, termed heterogeneous massive MIMO (HmMIMO), which\nseamlessly integrates co-located and distributed antennas. Differing from two\nconventional paradigms, HmMIMO remains a base station with a large antenna\narray at the center of each cell, aided by distributed antennas deployed at\ncell edges. Our findings demonstrate that this paradigm achieves a favorable\ntrade-off between performance and implementation complexity.", "comment": "IEEE ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2506.22000v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22000v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21681", "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation", "authors": ["Hakan Çapuk", "Andrew Bond", "Muhammed Burak Kızıl", "Emir Göçen", "Erkut Erdem", "Aykut Erdem"], "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21681v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21681v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22156", "title": "Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction", "authors": ["Mattia Ricchi", "Fabrizio Alfonsi", "Camilla Marella", "Marco Barbieri", "Alessandra Retico", "Leonardo Brizi", "Alessandro Gabrielli", "Claudia Testa"], "summary": "Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging\ntechnique that provides multi-parametric maps with a single acquisition. Neural\nNetworks (NNs) accelerate reconstruction but require significant resources for\ntraining. We propose an FPGA-based NN for real-time brain parameter\nreconstruction from MRF data. Training the NN takes an estimated 200 seconds,\nsignificantly faster than standard CPU-based training, which can be up to 250\ntimes slower. This method could enable real-time brain analysis on mobile\ndevices, revolutionizing clinical decision-making and telemedicine.", "comment": "8 pages, 2 figures, to be published in conference proceedings of SDPS\n  2024: 2024 International Conference of the Society for Design and Process\n  Science on Advances and Challenges of Applying AI/GenAI in Design and Process\n  Science", "pdf_url": "http://arxiv.org/pdf/2506.22156v1", "categories": ["cs.AR", "cs.CV", "physics.ins-det"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.22156v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22035", "title": "SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap", "authors": ["Qiqi GU", "Chenpeng Wu", "Heng Shi", "Jianguo Yao"], "summary": "Stencil computation, a pivotal numerical method in science and engineering,\niteratively updates grid points using weighted neighbor contributions and\nexhibits strong parallelism for multi-core processors. Current optimization\ntechniques targeting conducting stencil computation on tensor core accelerators\nincur substantial overheads due to redundant zero-padding during the\ntransformation to matrix multiplication. To address this, we introduce a sparse\ncomputation paradigm that eliminates inefficiencies by exploiting specialized\nhardware units.\n  This paper exploits the sparsity in these matrices as a feature and presents\nSPTCStencil, a high-performance stencil computation system accelerated by\nSparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for\nacceleration beyond deep learning domains. First, Our approach generalizes an\nefficient transformation of stencil computation into matrix multiplications and\nspecializes this conversion for SpTC compatibility through a novel\nsparsification strategy. Furthermore, SPTCStencil incorporates a\nhigh-performance GPU kernel with systematic optimizations designed to maximize\nefficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil\n5.46$\\times$ and Tensor Core-based approaches by 2.00$\\times$ on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22035v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22035v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21815", "title": "Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning", "authors": ["Augustine Twumasi", "Prokash Chandra Roy", "Zixun Li", "Soumya Shouvik Bhattacharjee", "Zhengtao Gan"], "summary": "Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing\ntechnology for producing intricate metal components with exceptional accuracy.\nA key challenge in L-PBF is the formation of complex microstructures affecting\nproduct quality. We propose a physics-guided, machine-learning approach to\noptimize scan paths for desired microstructure outcomes, such as equiaxed\ngrains. We utilized a phase-field method (PFM) to model crystalline grain\nstructure evolution. To reduce computational costs, we trained a surrogate\nmachine learning model, a 3D U-Net convolutional neural network, using\nsingle-track phase-field simulations with various laser powers to predict\ncrystalline grain orientations based on initial microstructure and thermal\nhistory. We investigated three scanning strategies across various hatch\nspacings within a square domain, achieving a two-orders-of-magnitude speedup\nusing the surrogate model. To reduce trial and error in designing laser scan\ntoolpaths, we used deep reinforcement learning (DRL) to generate optimized scan\npaths for target microstructure. Results from three cases demonstrate the DRL\napproach's effectiveness. We integrated the surrogate 3D U-Net model into our\nDRL environment to accelerate the reinforcement learning training process. The\nreward function minimizes both aspect ratio and grain volume of the predicted\nmicrostructure from the agent's scan path. The reinforcement learning algorithm\nwas benchmarked against conventional zigzag approach for smaller and larger\ndomains, showing machine learning methods' potential to enhance microstructure\ncontrol and computational efficiency in L-PBF optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21815v1", "categories": ["cs.CE", "cs.LG", "math.OC"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21815v1", "date": "2025-04-12", "updated": "2025-04-12"}
{"id": "2506.21734", "title": "Hierarchical Reasoning Model", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21734v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21734v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21754", "title": "Online design of experiments by active learning for nonlinear system identification", "authors": ["Kui Xie", "Alberto Bemporad"], "summary": "We investigate the use of active-learning (AL) strategies to generate the\ninput excitation signal at runtime for system identification of linear and\nnonlinear autoregressive and state-space models. We adapt various existing AL\napproaches for static model regression to the dynamic context, coupling them\nwith a Kalman filter to update the model parameters recursively, and also cope\nwith the presence of input and output constraints. We show the increased sample\nefficiency of the proposed approaches with respect to random excitation on\ndifferent nonlinear system identification benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21754v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21754v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21690", "title": "Joint RIS-UE Association and Beamforming Design in RIS-Assisted Cell-Free MIMO Network", "authors": ["Hongqin Ke", "Jindan Xu", "Wei Xu", "Chau Yuen", "Zhaohua Lu"], "summary": "Reconfigurable intelligent surface (RIS)-assisted cell-free (CF)\nmultiple-input multiple-output (MIMO) networks can significantly enhance system\nperformance. However, the extensive deployment of RIS elements imposes\nconsiderable channel acquisition overhead, with the high density of nodes and\nantennas in RIS-assisted CF networks amplifying this challenge. To tackle this\nissue, in this paper, we explore integrating RIS-user equipment (UE)\nassociation into downlink RIS-assisted CF transmitter design, which greatly\nreduces the channel acquisition costs. The key point is that once UEs are\nassociated with specific RISs, there is no need to frequently acquire channels\nfrom non-associated RISs. Then, we formulate the problem of joint RIS-UE\nassociation and beamforming at APs and RISs to maximize the weighted sum rate\n(WSR). In particular, we propose a two-stage framework to solve it. In the\nfirst stage, we apply a many-to-many matching algorithm to establish the RIS-UE\nassociation. In the second stage, we introduce a sequential optimization-based\nmethod that decomposes the joint optimization of RIS phase shifts and AP\nbeamforming into two distinct subproblems. To optimize the RIS phase shifts, we\nemploy the majorization-minimization (MM) algorithm to obtain a\nsemi-closed-form solution. For AP beamforming, we develop a joint block\ndiagonalization algorithm, which yields a closed-form solution. Simulation\nresults demonstrate the effectiveness of the proposed algorithm and show that,\nwhile RIS-UE association significantly reduces overhead, it incurs a minor\nperformance loss that remains within an acceptable range. Additionally, we\ninvestigate the impact of RIS deployment and conclude that RISs exhibit\nenhanced performance when positioned between APs and UEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21690v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21690v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21765", "title": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker", "authors": ["Qi Li", "Shaheer U. Saeed", "Yuliang Huang", "Mingyuan Luo", "Zhongnuo Yan", "Jiongquan Chen", "Xin Yang", "Dong Ni", "Nektarios Winter", "Phuc Nguyen", "Lucas Steinberger", "Caelan Haney", "Yuan Zhao", "Mingjie Jiang", "Bowen Ren", "SiYeoul Lee", "Seonho Kim", "MinKyung Seo", "MinWoo Kim", "Yimeng Dou", "Zhiwei Zhang", "Yin Li", "Tomy Varghese", "Dean C. Barratt", "Matthew J. Clarkson", "Tom Vercauteren", "Yipeng Hu"], "summary": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes\nfrom sequences of 2D ultrasound images without relying on external tracking\nsystems, offering a low-cost, portable, and widely deployable alternative for\nvolumetric imaging. However, it presents significant challenges, including\naccurate inter-frame motion estimation, minimisation of drift accumulation over\nlong sequences, and generalisability across scanning protocols. The TUS-REC2024\nChallenge was established to benchmark and accelerate progress in trackerless\n3D ultrasound reconstruction by providing a publicly available dataset for the\nfirst time, along with a baseline model and evaluation framework. The Challenge\nattracted over 43 registered teams, of which 6 teams submitted 21 valid\ndockerized solutions. Submitted methods spanned a wide range of algorithmic\napproaches, including recurrent models, registration-driven volume refinement,\nattention, and physics-informed models. This paper presents an overview of the\nChallenge design, summarises the key characteristics of the dataset, provides a\nconcise literature review, introduces the technical details of the underlying\nmethodology working with tracked freehand ultrasound data, and offers a\ncomparative analysis of submitted methods across multiple evaluation metrics.\nThe results highlight both the progress and current limitations of\nstate-of-the-art approaches in this domain, and inform directions for future\nresearch. The data, evaluation code, and baseline are publicly available to\nfacilitate ongoing development and reproducibility. As a live and evolving\nbenchmark, this Challenge is designed to be continuously developed and\nimproved. The Challenge was held at MICCAI 2024 and will be organised again at\nMICCAI 2025, reflecting its growing impact and the sustained commitment to\nadvancing this field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21765v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21765v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22010", "title": "Fault-Tolerant Matroid Bases", "authors": ["Matthias Bentert", "Fedor V. Fomin", "Petr A. Golovach", "Laure Morelle"], "summary": "We investigate the problem of constructing fault-tolerant bases in matroids.\nGiven a matroid M and a redundancy parameter k, a k-fault-tolerant basis is a\nminimum-size set of elements such that, even after the removal of any k\nelements, the remaining subset still spans the entire ground set. Since\nmatroids generalize linear independence across structures such as vector\nspaces, graphs, and set systems, this problem unifies and extends several\nfault-tolerant concepts appearing in prior research.\n  Our main contribution is a fixed-parameter tractable (FPT) algorithm for the\nk-fault-tolerant basis problem, parameterized by both k and the rank r of the\nmatroid. This two-variable parameterization by k + r is shown to be tight in\nthe following sense. On the one hand, the problem is already NP-hard for k=1.\nOn the other hand, it is Para-NP-hard for r \\geq 3 and polynomial-time solvable\nfor r \\leq 2.", "comment": "An extended abstract of this paper appears in the proceedings of ESA\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22010v1", "categories": ["cs.DS", "cs.DM"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22010v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21632", "title": "SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model", "authors": ["Da Li", "Donggang Jia", "Markus Hadwiger", "Ivan Viola"], "summary": "Reconstructing an interactive human avatar and the background from a\nmonocular video of a dynamic human scene is highly challenging. In this work we\nadopt a strategy of point cloud decoupling and joint optimization to achieve\nthe decoupled reconstruction of backgrounds and human bodies while preserving\nthe interactivity of human motion. We introduce a position texture to subdivide\nthe Skinned Multi-Person Linear (SMPL) body model's surface and grow the human\npoint cloud. To capture fine details of human dynamics and deformations, we\nincorporate a convolutional neural network structure to predict human body\npoint cloud features based on texture. This strategy makes our approach free of\nhyperparameter tuning for densification and efficiently represents human points\nwith half the point cloud of HUGS. This approach ensures high-quality human\nreconstruction and reduces GPU resource consumption during training. As a\nresult, our method surpasses the previous state-of-the-art HUGS in\nreconstruction metrics while maintaining the ability to generalize to novel\nposes and views. Furthermore, our technique achieves real-time rendering at\nover 100 FPS, $\\sim$6$\\times$ the HUGS speed using only Linear Blend Skinning\n(LBS) weights for human transformation. Additionally, this work demonstrates\nthat this framework can be extended to animal scene reconstruction when an\naccurately-posed model of an animal is available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21632v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21632v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22001", "title": "WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation", "authors": ["Lu Han", "Junqi Zhao", "Renhua Peng"], "summary": "Current multi-channel speech enhancement systems mainly adopt single-output\narchitecture, which face significant challenges in preserving spatio-temporal\nsignal integrity during multiple-input multiple-output (MIMO) processing. To\naddress this limitation, we propose a novel neural network, termed WTFormer,\nfor MIMO speech enhancement that leverages the multi-resolution characteristics\nof wavelet transform and multi-dimensional collaborative attention to\neffectively capture globally distributed spatial features, while using\nConformer for time-frequency modeling. A multi task loss strategy accompanying\nMUSIC algorithm is further proposed for optimization training to protect\nspatial information to the greatest extent. Experimental results on the\nLibriSpeech dataset show that WTFormer can achieve comparable denoising\nperformance to advanced systems while preserving more spatial information with\nonly 0.98M parameters.", "comment": "Accepted by Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2506.22001v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22001v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21556", "title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "authors": ["Hyeongcheol Park", "MinHyuk Jang", "Ha Dam Baek", "Gyusam Chang", "Jiyoung Seo", "Jiwan Park", "Hogun Park", "Sangpil Kim"], "summary": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge\nacross multiple modalities, play a pivotal role by complementing the implicit\nknowledge of Multimodal Large Language Models (MLLMs) and enabling more\ngrounded reasoning via Retrieval Augmented Generation (RAG). However, existing\nMMKGs are generally limited in scope: they are often constructed by augmenting\npre-existing knowledge graphs, which restricts their knowledge, resulting in\noutdated or incomplete knowledge coverage, and they often support only a narrow\nrange of modalities, such as text and visual information. These limitations\nreduce their extensibility and applicability to a broad range of multimodal\ntasks, particularly as the field shifts toward richer modalities such as video\nand audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text\nKnowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive\nmultimodal knowledge graph that covers visual, audio, and text information,\nwhere each triplet is linked to multimodal data and enriched with detailed\ndescriptions of concepts. Specifically, our construction pipeline ensures\ncross-modal knowledge alignment between multimodal data and fine-grained\nsemantics through a series of stringent filtering and alignment steps, enabling\nthe automatic generation of MMKGs from any multimodal dataset. We further\nintroduce a novel multimodal RAG framework that retrieves detailed\nconcept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate\nthe effectiveness of VAT-KG in supporting MLLMs, highlighting its practical\nvalue in unifying and leveraging multimodal knowledge.", "comment": "Project Page: https://vatkg.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.21556v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21556v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21581", "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21581v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21581v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21700", "title": "Genuinely multi-dimensional stationarity preserving global flux Finite Volume formulation for nonlinear hyperbolic PDEs", "authors": ["Wasilij Barsukow", "Mirco Ciallella", "Mario Ricchiuto", "Davide Torlo"], "summary": "Classical Finite Volume methods for multi-dimensional problems include\nstabilization (e.g. via a Riemann solver), that is derived by considering\nseveral one-dimensional problems in different directions. Such methods\ntherefore ignore a possibly existing balance of contributions coming from\ndifferent directions, such as the one characterizing multi-dimensional\nstationary states. Instead being preserved, they are usually diffused away by\nsuch methods. Stationarity preserving methods use a better suited stabilization\nterm that vanishes at the stationary state, allowing the method to preserve it.\nThis work presents a general approach to stationarity preserving Finite Volume\nmethods for nonlinear conservation/balance laws. It is based on a\nmulti-dimensional extension of the global flux approach. The new methods are\nshown to significantly outperform existing ones even if the latter are of\nhigher order of accuracy and even on non-stationary solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21700v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21700v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22122", "title": "In situ fine-tuning of in silico trained Optical Neural Networks", "authors": ["Gianluca Kosmella", "Ripalta Stabile", "Jaron Sanders"], "summary": "Optical Neural Networks (ONNs) promise significant advantages over\ntraditional electronic neural networks, including ultrafast computation, high\nbandwidth, and low energy consumption, by leveraging the intrinsic capabilities\nof photonics. However, training ONNs poses unique challenges, notably the\nreliance on simplified in silico models whose trained parameters must\nsubsequently be mapped to physical hardware. This process often introduces\ninaccuracies due to discrepancies between the idealized digital model and the\nphysical ONN implementation, particularly stemming from noise and fabrication\nimperfections.\n  In this paper, we analyze how noise misspecification during in silico\ntraining impacts ONN performance and we introduce Gradient-Informed Fine-Tuning\n(GIFT), a lightweight algorithm designed to mitigate this performance\ndegradation. GIFT uses gradient information derived from the noise structure of\nthe ONN to adapt pretrained parameters directly in situ, without requiring\nexpensive retraining or complex experimental setups. GIFT comes with formal\nconditions under which it improves ONN performance.\n  We also demonstrate the effectiveness of GIFT via simulation on a five-layer\nfeed forward ONN trained on the MNIST digit classification task. GIFT achieves\nup to $28\\%$ relative accuracy improvement compared to the baseline performance\nunder noise misspecification, without resorting to costly retraining. Overall,\nGIFT provides a practical solution for bridging the gap between simplified\ndigital models and real-world ONN implementations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22122v1", "categories": ["cs.NE", "cs.ET", "eess.SP"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.22122v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22227", "title": "Unified Memcapacitor-Memristor Memory for Synaptic Weights and Neuron Temporal Dynamics", "authors": ["Simone D'Agostino", "Marco Massarotto", "Tristan Torchet", "Filippo Moro", "Niccolò Castellani", "Laurent Grenouillet", "Yann Beilliard", "David Esseni", "Melika Payvand", "Elisa Vianello"], "summary": "We present a fabricated and experimentally characterized memory stack that\nunifies memristive and memcapacitive behavior. Exploiting this dual\nfunctionality, we design a circuit enabling simultaneous control of spatial and\ntemporal dynamics in recurrent spiking neural networks (RSNNs). Hardware-aware\nsimulations highlight its promise for efficient neuromorphic processing.", "comment": "2 pages, accepted and discussed at Silicon Nanoelectronics Workshop\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22227v1", "categories": ["cs.ET", "cs.NE", "eess.SP"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.22227v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21683", "title": "Risk-Averse Total-Reward Reinforcement Learning", "authors": ["Xihong Su", "Jia Lin Hau", "Gersi Doko", "Kishan Panaganti", "Marek Petrik"], "summary": "Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising\nframework for modeling and solving undiscounted infinite-horizon objectives.\nExisting model-based algorithms for risk measures like the entropic risk\nmeasure (ERM) and entropic value-at-risk (EVaR) are effective in small\nproblems, but require full access to transition probabilities. We propose a\nQ-learning algorithm to compute the optimal stationary policy for total-reward\nERM and EVaR objectives with strong convergence and performance guarantees. The\nalgorithm and its optimality are made possible by ERM's dynamic consistency and\nelicitability. Our numerical results on tabular domains demonstrate quick and\nreliable convergence of the proposed Q-learning algorithm to the optimal\nrisk-averse value function.", "comment": "The paper is under review now", "pdf_url": "http://arxiv.org/pdf/2506.21683v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21683v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21816", "title": "The First Compute Arms Race: the Early History of Numerical Weather Prediction", "authors": ["Charles Yang"], "summary": "This paper traces the global race to apply early electronic computers to\nnumerical weather prediction in the decades following World War Two. A brief\noverview of the early history of numerical weather prediction in the United\nStates, United Kingdom, Sweden, Canada, and Japan is provided. Three critical\nfactors that shaped the development of a national numerical weather prediction\nare identified: compute capabilities, institution building and state capacity,\nand talent. Several generalizable lessons are identified with a lens towards\nmodern-day development of national strategies to leverage AI to accelerate\nscientific competitiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21816v1", "categories": ["cs.CY", "physics.ao-ph"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21816v1", "date": "2025-04-13", "updated": "2025-04-13"}
{"id": "2506.22047", "title": "Shape Preserving Tree Transducers", "authors": ["Paul Gallot", "Sebastian Maneth"], "summary": "It is shown that shape preservation is decidable for top-down tree\ntransducers, bottom-up tree transducers, and for compositions of total\ndeterministic macro tree transducers. Moreover, if a transducer is shape\npreserving, then it can be brought into a particular normal form, where every\ninput node creates exactly one output node.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22047v1", "categories": ["cs.FL", "68U99"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.22047v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22323", "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America", "authors": ["Alessio Di Santo"], "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22323v1", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.OS", "cs.PL"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22323v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22189", "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sjölund"], "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22189v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22189v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21630", "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "comment": "8 pages, 9 figures, 2025 IJCNN", "pdf_url": "http://arxiv.org/pdf/2506.21630v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21630v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21693", "title": "The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Fredrik Törner", "Christian Berger"], "summary": "Developing autonomous driving (AD) systems is challenging due to the\ncomplexity of the systems and the need to assure their safe and reliable\noperation. The widely adopted approach of DevOps seems promising to support the\ncontinuous technological progress in AI and the demand for fast reaction to\nincidents, which necessitate continuous development, deployment, and\nmonitoring. We present a systematic literature review meant to identify,\nanalyse, and synthesise a broad range of existing literature related to usage\nof DevOps in autonomous driving development. Our results provide a structured\noverview of challenges and solutions, arising from applying DevOps to\nsafety-related AI-enabled functions. Our results indicate that there are still\nseveral open topics to be addressed to enable safe DevOps for the development\nof safe AD.", "comment": "Accepted for publication in the Journal of Systems and Software (JSS)", "pdf_url": "http://arxiv.org/pdf/2506.21693v1", "categories": ["cs.SE", "cs.RO"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21693v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21814", "title": "Validation of the MySurgeryRisk Algorithm for Predicting Complications and Death after Major Surgery: A Retrospective Multicenter Study Using OneFlorida Data Trust", "authors": ["Yuanfang Ren", "Esra Adiyeke", "Ziyuan Guan", "Zhenhong Hu", "Mackenzie J Meni", "Benjamin Shickel", "Parisa Rashidi", "Tezcan Ozrazgat-Baslanti", "Azra Bihorac"], "summary": "Despite advances in surgical techniques and care, postoperative complications\nare prevalent and effects up to 15% of the patients who underwent a major\nsurgery. The objective of this study is to develop and validate models for\npredicting postoperative complications and death after major surgery on a large\nand multicenter dataset, following the previously validated MySurgeryRisk\nalgorithm. This retrospective, longitudinal and multicenter cohort analysis\nincluded 508,097 encounters from 366,875 adult inpatients who underwent major\nsurgeries and were admitted to healthcare institutions within the OneFlorida+\nnetwork between 01/01/2012 and 04/29/2023. We applied the validated feature\nselection and transformation approach in MySurgeryRisk models and redeveloped\neXtreme Gradient Boosting (XGBoost) models for predicting risk of postoperative\nacute kidney injury (AKI), need for intensive care unit (ICU) admission, need\nfor mechanical ventilation (MV) therapy and in-hospital mortality on a\ndevelopment set and evaluated the model performance on a validation set. Area\nunder the receiver operating characteristics curve values were obtained for\nneed for ICU admission, 0.93 (95% Confidence Interval [CI], 0.93-0.93); need\nfor MV, 0.94 (95% CI, 0.94-0.94); AKI, 0.92 (95% CI, 0.92-0.92); and\nin-hospital mortality, 0.95 (95% CI, 0.94-0.95). Area under the\nprecision-recall curve values were computed for need for ICU admission, 0.62\n(95% CI, 0.62-0.63); need for MV, 0.51 (95% CI, 0.49-0.52); AKI, 0.53 (95% CI,\n0.53-0.54); and in-hospital mortality, 0.26 (95% CI, 0.24-0.29). The\nperformance of these models is comparable to that of the previously validated\nMySurgeryRisk models, suggesting the enhanced generalizability of the models.\nPrimary procedure code and provider specialty consistently appeared as the top\ninfluential variables, providing valuable insights into the factors influencing\nsurgical outcomes.", "comment": "28 pages, 4 figures, 6 tables, 1 supplemental table", "pdf_url": "http://arxiv.org/pdf/2506.21814v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21814v1", "date": "2025-03-31", "updated": "2025-03-31"}
{"id": "2506.22224", "title": "A Decade of News Forum Interactions: Threaded Conversations, Signed Votes, and Topical Tags", "authors": ["Emma Fraxanet", "Vicenç Gómez", "Andreas Kaltenbrunner", "Max Pellert"], "summary": "We present a large-scale, longitudinal dataset capturing user activity on the\nonline platform of DerStandard, a major Austrian newspaper. The dataset spans\nten years (2013-2022) and includes over 75 million user comments, more than 400\nmillion votes, and detailed metadata on articles and user interactions. It\nprovides structured conversation threads, explicit up- and downvotes of user\ncomments and editorial topic labels, enabling rich analyses of online discourse\nwhile preserving user privacy. To ensure this privacy, all persistent\nidentifiers are anonymized using salted hash functions, and the raw comment\ntexts are not publicly shared. Instead, we release pre-computed vector\nrepresentations derived from a state-of-the-art embedding model. The dataset\nsupports research on discussion dynamics, network structures, and semantic\nanalyses in the mid-resourced language German, offering a reusable resource\nacross computational social science and related fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22224v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22224v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22223", "title": "V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles", "authors": ["Felipe Valle Quiroz", "Johan Elfing", "Joel Pålsson", "Elena Haller", "Oscar Amador Molina"], "summary": "This paper introduces a novel intention-sharing mechanism for Electrically\nPower-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing\nthe ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete\npredicted trajectory points with a compact elliptical geographical area\nrepresentation derived via quadratic polynomial fitting and Least Squares\nMethod (LSM). This approach encodes trajectory predictions with fixed-size data\npayloads, independent of the number of forecasted points, enabling\nhigher-frequency transmissions and improved network reliability. Simulation\nresults demonstrate superior inter-packet gap (IPG) performance compared to\nstandard ETSI VAMs, particularly under constrained communication conditions. A\nphysical experiment validates the feasibility of real-time deployment on\nembedded systems. The method supports scalable, low-latency intention sharing,\ncontributing to cooperative perception and enhanced safety for vulnerable road\nusers in connected and automated mobility ecosystems. Finally, we discuss the\nviability of LSM and open the door to other methods for prediction.", "comment": "Accepted into FAST-zero'25: 8th International Symposium on Future\n  Active Safety Technology toward zero traffic accidents", "pdf_url": "http://arxiv.org/pdf/2506.22223v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22223v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22053", "title": "The Condition Number in Phase Retrieval from Intensity Measurements", "authors": ["Haiyang Peng", "Deren Han", "Meng Huang"], "summary": "This paper investigates the stability of phase retrieval by analyzing the\ncondition number of the nonlinear map $\\Psi_{\\boldsymbol{A}}(\\boldsymbol{x}) =\n\\bigl(\\lvert \\langle {\\boldsymbol{a}}_j, \\boldsymbol{x} \\rangle \\rvert^2\n\\bigr)_{1 \\le j \\le m}$, where $\\boldsymbol{a}_j \\in \\mathbb{H}^n$ are known\nsensing vectors with $\\mathbb{H} \\in \\{\\mathbb{R}, \\mathbb{C}\\}$. For each $p\n\\ge 1$, we define the condition number $\\beta_{\\Psi_{\\boldsymbol{A}}}^{\\ell_p}$\nas the ratio of optimal upper and lower Lipschitz constants of\n$\\Psi_{\\boldsymbol{A}}$ measured in the $\\ell_p$ norm, with respect to the\nmetric $\\mathrm {dist}_\\mathbb{H}\\left(\\boldsymbol{x}, \\boldsymbol{y}\\right) =\n\\|\\boldsymbol{x} \\boldsymbol{x}^\\ast - \\boldsymbol{y} \\boldsymbol{y}^\\ast\\|_*$.\nWe establish universal lower bounds on $\\beta_{\\Psi_{\\boldsymbol{A}}}^{\\ell_p}$\nfor any sensing matrix $\\boldsymbol{A} \\in \\mathbb{H}^{m \\times d}$, proving\nthat $\\beta_{\\Psi_{\\boldsymbol{A}}}^{\\ell_1} \\ge \\pi/2$ and\n$\\beta_{\\Psi_{\\boldsymbol{A}}}^{\\ell_2} \\ge \\sqrt{3}$ in the real case\n$(\\mathbb{H} = \\mathbb{R})$, and $\\beta_{\\Psi_{\\boldsymbol{A}}}^{\\ell_p} \\ge 2$\nfor $p=1,2$ in the complex case $(\\mathbb{H} = \\mathbb{C})$. These bounds are\nshown to be asymptotically tight: both a deterministic harmonic frame\n$\\boldsymbol{E}_m \\in \\mathbb{R}^{m \\times 2}$ and Gaussian random matrices\n$\\boldsymbol{A} \\in \\mathbb{H}^{m \\times d}$ asymptotically attain them.\nNotably, the harmonic frame $\\boldsymbol{E}_m \\in \\mathbb{R}^{m \\times 2}$\nachieves the optimal lower bound $\\sqrt{3}$ for all $m \\ge 3$ when $p=2$, thus\nserving as an optimal sensing matrix within $\\boldsymbol{A} \\in \\mathbb{R}^{m\n\\times 2}$. Our results provide the first explicit uniform lower bounds on\n$\\beta_{\\Psi_{\\boldsymbol{A}}}^{\\ell_p}$ and offer insights into the\nfundamental stability limits of phase retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22053v1", "categories": ["cs.IT", "math.FA", "math.IT", "94A12, 65H10, 65F35"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22053v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21710", "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering", "authors": ["Liangyu Zhong", "Fabio Rosenthal", "Joachim Sicking", "Fabian Hüger", "Thorsten Bagdonat", "Hanno Gottschalk", "Leo Schwinn"], "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2506.21710v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21710v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21562", "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21562v1", "categories": ["cs.CL", "cs.AI", "cs.AR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21562v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22169", "title": "MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators", "authors": ["Zheng Zhang", "Donglin Yang", "Xiaobo Zhou", "Dazhao Cheng"], "summary": "Operator fusion, a key technique to improve data locality and alleviate GPU\nmemory bandwidth pressure, often fails to extend to the fusion of multiple\ncompute-intensive operators due to saturated computation throughput. However,\nthe dynamicity of tensor dimension sizes could potentially lead to these\noperators becoming memory-bound, necessitating the generation of fused kernels,\na task hindered by limited search spaces for fusion strategies, redundant\nmemory access, and prolonged tuning time, leading to sub-optimal performance\nand inefficient deployment.\n  We introduce MCFuser, a pioneering framework designed to overcome these\nobstacles by generating high-performance fused kernels for what we define as\nmemory-bound compute-intensive (MBCI) operator chains. Leveraging high-level\ntiling expressions to delineate a comprehensive search space, coupled with\nDirected Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,\nMCFuser streamlines kernel optimization. By implementing guidelines to prune\nthe search space and incorporating an analytical performance model with a\nheuristic search, MCFuser not only significantly accelerates the tuning process\nbut also demonstrates superior performance. Benchmarked against leading\ncompilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a\n5.9x speedup in kernel performance and outpaces other baselines while reducing\ntuning time by over 70-fold, showcasing its agility.", "comment": "12 pages, accepted at SC 2024", "pdf_url": "http://arxiv.org/pdf/2506.22169v1", "categories": ["cs.DC", "cs.PL"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22169v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21918", "title": "Model-free Forecasting of Rogue Waves using Reservoir Computing", "authors": ["Abrari Noor Hasmi", "Hadi Susanto"], "summary": "Recent research has demonstrated Reservoir Computing's capability to model\nvarious chaotic dynamical systems, yet its application to Hamiltonian systems\nremains relatively unexplored. This paper investigates the effectiveness of\nReservoir Computing in capturing rogue wave dynamics from the nonlinear\nSchr\\\"{o}dinger equation, a challenging Hamiltonian system with modulation\ninstability. The model-free approach learns from breather simulations with five\nunstable modes. A properly tuned parallel Echo State Network can predict\ndynamics from two distinct testing datasets. The first set is a continuation of\nthe training data, whereas the second set involves a higher-order breather. An\ninvestigation of the one-step prediction capability shows remarkable agreement\nbetween the testing data and the models. Furthermore, we show that the trained\nreservoir can predict the propagation of rogue waves over a relatively long\nprediction horizon, despite facing unseen dynamics. Finally, we introduce a\nmethod to significantly improve the Reservoir Computing prediction in\nautonomous mode, enhancing its long-term forecasting ability. These results\nadvance the application of Reservoir Computing to spatio-temporal Hamiltonian\nsystems and highlight the critical importance of phase space coverage in the\ndesign of training data.", "comment": "26 pages 14 figures. To appear Communications in Nonlinear Science\n  and Numerical Simulation (CNSNS), 2025 , 109087", "pdf_url": "http://arxiv.org/pdf/2506.21918v1", "categories": ["cs.CE", "nlin.PS"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21918v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21763", "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?", "authors": ["Xin Wang", "Jiyao Liu", "Yulong Xiao", "Junzhi Ning", "Lihao Liu", "Junjun He", "Botian Shi", "Kaicheng Yu"], "summary": "Large Language Models (LLMs) are accelerating scientific idea generation, but\nrigorously evaluating these numerous, often superficial, AI-generated\npropositions for novelty and factual accuracy is a critical bottleneck; manual\nverification is too slow.Existing validation methods are inadequate: LLMs as\nstandalone verifiers may hallucinate and lack domain knowledge (our findings\nshow ~60\\% unawareness of relevant papers in specific domains), while\ntraditional citation networks lack explicit causality and narrative surveys are\nunstructured.This underscores a core challenge: the absence of structured,\nverifiable, and causally-linked historical data of scientific evolution.To\naddress this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology\n\\textbf{H}istory \\textbf{E}volution Tree), a computational framework that\nconstructs such domain-specific evolution trees from scientific\nliterature.THE-Tree employs a search algorithm to explore evolutionary paths.\nDuring its node expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\"\nprocess: an LLM proposes potential advancements and cites supporting\nliterature. Critically, each proposed evolutionary link is then validated for\nlogical coherence and evidential support by a recovered natural language\ninference mechanism that interrogates the cited literature, ensuring that each\nstep is grounded.We construct and validate 88 THE-Trees across diverse domains\nand release a benchmark dataset including up to 71k fact verifications covering\n27k papers to foster further research.Experiments demonstrate that i) in graph\ncompletion, our THE-Tree improves hit@1 by 8\\% to 14\\% across multiple models\ncompared to traditional citation networks; ii) for predicting future scientific\ndevelopments, it improves hit@1 metric by nearly 10\\%; and iii) when combined\nwith other methods, it boosts the performance of evaluating important\nscientific papers by almost 100\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21763v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21763v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22054", "title": "Complex Phase Analysis of Power Grid Dynamics", "authors": ["Jakob Niehues", "Anna Büttner", "Anne Riegler", "Frank Hellmann"], "summary": "With an increasing share of renewable energy sources, accurate and efficient\nmodeling of grid-forming inverters is becoming crucial for system stability.\nLinear methods are a powerful tool for understanding dynamics close to an\noperating point, but usually depend on the reference trajectory. Thus, small\ndeviations can render linear models invalid over time, posing a significant\nchallenge in practice, and complicating theoretical analysis. As a solution, we\nshow that the complex phase offers a robust formulation independent of\nreference phases and frequencies, thus preserving invariance properties under\nlinearization. This enables robust system identification during realistic\nconditions and opens the road to powerful stability analysis of inverter-based\ngrids.", "comment": "IEEE PowerTech 2025", "pdf_url": "http://arxiv.org/pdf/2506.22054v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22054v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21772", "title": "Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search", "authors": ["Noé Lallouet", "Tristan Cazenave", "Cyrille Enderli", "Stéphanie Gourdin"], "summary": "Recent research works establish deep neural networks as high performing tools\nfor radar target detection, especially on challenging environments (presence of\nclutter or interferences, multi-target scenarii...). However, the usually large\ncomputational complexity of these networks is one of the factors preventing\nthem from being widely implemented in embedded radar systems. We propose to\ninvestigate novel neural architecture search (NAS) methods, based on\nMonte-Carlo Tree Search (MCTS), for finding neural networks achieving the\nrequired detection performance and striving towards a lower computational\ncomplexity. We evaluate the searched architectures on endoclutter radar\nsignals, in order to compare their respective performance metrics and\ngeneralization properties. A novel network satisfying the required detection\nprobability while being significantly lighter than the expert-designed baseline\nis proposed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21772v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21772v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21880", "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer", "authors": ["Yuansheng Li", "Yunhao Zou", "Linwei Chen", "Ying Fu"], "summary": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for\nlarge-scale remote sensing tasks due to its advantages in flux and spectral\nresolution. However, IHI is susceptible to complex errors arising from imaging\nsteps, and its quality is limited by existing signal processing-based\nreconstruction algorithms. Two key challenges hinder performance enhancement:\n1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific\ndegradation components through learning-based methods. To address these\nchallenges, we propose a novel IHI reconstruction pipeline. First, based on\nimaging physics and radiometric calibration data, we establish a simplified yet\naccurate IHI degradation model and a parameter estimation method. This model\nenables the synthesis of realistic IHI training datasets from hyperspectral\nimages (HSIs), bridging the gap between IHI reconstruction and deep learning.\nSecond, we design the Interferometric Hyperspectral Reconstruction Unfolding\nTransformer (IHRUT), which achieves effective spectral correction and detail\nrestoration through a stripe-pattern enhancement mechanism and a\nspatial-spectral transformer architecture. Experimental results demonstrate the\nsuperior performance and generalization capability of our method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21880v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21880v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22127", "title": "Parameterized Complexity of Directed Traveling Salesman Problem", "authors": ["Václav Blažej", "Andreas Emil Feldmann", "Foivos Fioravantes", "Paweł Rzążewski", "Ondřej Suchý"], "summary": "The Directed Traveling Salesman Problem (DTSP) is a variant of the classical\nTraveling Salesman Problem in which the edges in the graph are directed and a\nvertex and edge can be visited multiple times. The goal is to find a directed\nclosed walk of minimum length (or total weight) that visits every vertex of the\ngiven graph at least once. In a yet more general version, Directed Waypoint\nRouting Problem (DWRP), some vertices are marked as terminals and we are only\nrequired to visit all terminals. Furthermore, each edge has its capacity\nbounding the number of times this edge can be used by a solution.\n  While both problems (and many other variants of TSP) were extensively\ninvestigated, mostly from the approximation point of view, there are\nsurprisingly few results concerning the parameterized complexity. Our starting\npoint is the result of Marx et al. [APPROX/RANDOM 2016] who proved that DTSP is\nW[1]-hard parameterized by distance to pathwidth 3. In this paper we aim to\ninitiate the systematic complexity study of variants of DTSP with respect to\nvarious, mostly structural, parameters.\n  We show that DWRP is FPT parameterized by the solution size, the feedback\nedge number, and the vertex integrity of the underlying undirected graph.\nFurthermore, the problem is XP parameterized by treewidth. On the complexity\nside, we show that the problem is W[1]-hard parameterized by the distance to\nconstant treedepth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22127v1", "categories": ["cs.DS", "68Q27"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22127v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21633", "title": "SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction", "authors": ["Aobo Li", "Zhengxin Lei", "Jiangtao Wei", "Feng Xu"], "summary": "Three-dimensional target reconstruction from synthetic aperture radar (SAR)\nimagery is crucial for interpreting complex scattering information in SAR data.\nHowever, the intricate electromagnetic scattering mechanisms inherent to SAR\nimaging pose significant reconstruction challenges. Inspired by the remarkable\nsuccess of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this\npaper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR)\nspecifically designed for SAR target reconstruction. Our approach combines\nGaussian splatting with the Mapping and Projection Algorithm to compute\nscattering intensities of Gaussian primitives and generate simulated SAR images\nthrough SDGR. Subsequently, the loss function between the rendered image and\nthe ground truth image is computed to optimize the Gaussian primitive\nparameters representing the scene, while a custom CUDA gradient flow is\nemployed to replace automatic differentiation for accelerated gradient\ncomputation. Through experiments involving the rendering of simplified\narchitectural targets and SAR images of multiple vehicle targets, we validate\nthe imaging rationality of SDGR on simulated SAR imagery. Furthermore, the\neffectiveness of our method for target reconstruction is demonstrated on both\nsimulated and real-world datasets containing multiple vehicle targets, with\nquantitative evaluations conducted to assess its reconstruction performance.\nExperimental results indicate that our approach can effectively reconstruct the\ngeometric structures and scattering properties of targets, thereby providing a\nnovel solution for 3D reconstruction in the field of SAR imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21633v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21633v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22194", "title": "Cross-lingual Data Selection Using Clip-level Acoustic Similarity for Enhancing Low-resource Automatic Speech Recognition", "authors": ["Shunsuke Mitsumori", "Sara Kashiwagi", "Keitaro Tanaka", "Shigeo Morishima"], "summary": "This paper presents a novel donor data selection method to enhance\nlow-resource automatic speech recognition (ASR). While ASR performs well in\nhigh-resource languages, its accuracy declines in low-resource settings due to\nlimited training data. A common solution is to leverage multilingual\nself-supervised learning (SSL) models with donor languages. However, existing\nmethods rely on language-level similarity, overlooking clip-level variations.\nTo address this limitation, we propose clip-wise acoustic token distribution\nsimilarity (CATDS), a fine-grained selection method that identifies\nacoustically relevant donor clips for better alignment with the target\nlanguage. Unlike existing clip-level selection methods, our method aligns with\nthe representation of SSL models and offers more challenging yet valuable\nsamples. Experimental results show that CATDS outperforms traditional selection\nmethods and can even utilize donor languages previously considered detrimental.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2506.22194v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22194v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21557", "title": "Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning", "authors": ["Kaiying Yan", "Moyang Liu", "Yukun Liu", "Ruibo Fu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu"], "summary": "The rapid spread of fake news across multimedia platforms presents serious\nchallenges to information credibility. In this paper, we propose a\nDebunk-and-Infer framework for Fake News Detection(DIFND) that leverages\ndebunking knowledge to enhance both the performance and interpretability of\nfake news detection. DIFND integrates the generative strength of conditional\ndiffusion models with the collaborative reasoning capabilities of multimodal\nlarge language models (MLLMs). Specifically, debunk diffusion is employed to\ngenerate refuting or authenticating evidence based on the multimodal content of\nnews videos, enriching the evaluation process with diverse yet semantically\naligned synthetic samples. To improve inference, we propose a chain-of-debunk\nstrategy where a multi-agent MLLM system produces logic-grounded,\nmultimodal-aware reasoning content and final veracity judgment. By jointly\nmodeling multimodal features, generative debunking cues, and reasoning-rich\nverification within a unified architecture, DIFND achieves notable improvements\nin detection accuracy. Extensive experiments on the FakeSV and FVC datasets\nshow that DIFND not only outperforms existing approaches but also delivers\ntrustworthy decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21557v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21557v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21593", "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications", "authors": ["Abu Hanif Muhammad Syarubany", "Chang Dong Yoo"], "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.", "comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers", "pdf_url": "http://arxiv.org/pdf/2506.21593v1", "categories": ["cs.IR", "cs.DB"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21593v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.21830", "title": "Optimizing Mixed Quantum Channels via Projected Gradient Dynamics", "authors": ["Matthew M. Lin", "Bing-Ze Lu"], "summary": "Designing a mixed quantum channel is challenging due to the complexity of the\ntransformations and the probabilistic mixtures of more straightforward channels\ninvolved. Fully characterizing a quantum channel generally requires preparing a\ncomplete set of input states, such as a basis for the state space, and\nmeasuring the corresponding output states. In this work, we begin by\ninvestigating a single input-output pair using projected gradient dynamics.\nThis approach applies optimization flows constrained to the Stiefel manifold\nand the probabilistic simplex to identify the original quantum channel. The\nconvergence of the flow is guaranteed by its relationship to the Zariski\ntopology. We present numerical investigations of models adapted to various\nscenarios, including those with multiple input-output pairs, highlighting the\nflexibility and efficiency of our proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21830v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21830v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22122", "title": "In situ fine-tuning of in silico trained Optical Neural Networks", "authors": ["Gianluca Kosmella", "Ripalta Stabile", "Jaron Sanders"], "summary": "Optical Neural Networks (ONNs) promise significant advantages over\ntraditional electronic neural networks, including ultrafast computation, high\nbandwidth, and low energy consumption, by leveraging the intrinsic capabilities\nof photonics. However, training ONNs poses unique challenges, notably the\nreliance on simplified in silico models whose trained parameters must\nsubsequently be mapped to physical hardware. This process often introduces\ninaccuracies due to discrepancies between the idealized digital model and the\nphysical ONN implementation, particularly stemming from noise and fabrication\nimperfections.\n  In this paper, we analyze how noise misspecification during in silico\ntraining impacts ONN performance and we introduce Gradient-Informed Fine-Tuning\n(GIFT), a lightweight algorithm designed to mitigate this performance\ndegradation. GIFT uses gradient information derived from the noise structure of\nthe ONN to adapt pretrained parameters directly in situ, without requiring\nexpensive retraining or complex experimental setups. GIFT comes with formal\nconditions under which it improves ONN performance.\n  We also demonstrate the effectiveness of GIFT via simulation on a five-layer\nfeed forward ONN trained on the MNIST digit classification task. GIFT achieves\nup to $28\\%$ relative accuracy improvement compared to the baseline performance\nunder noise misspecification, without resorting to costly retraining. Overall,\nGIFT provides a practical solution for bridging the gap between simplified\ndigital models and real-world ONN implementations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22122v1", "categories": ["cs.NE", "cs.ET", "eess.SP"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.22122v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21695", "title": "Unimodal Strategies in Density-Based Clustering", "authors": ["Oron Nir", "Jay Tenenbaum", "Ariel Shamir"], "summary": "Density-based clustering methods often surpass centroid-based counterparts,\nwhen addressing data with noise or arbitrary data distributions common in\nreal-world problems. In this study, we reveal a key property intrinsic to\ndensity-based clustering methods regarding the relation between the number of\nclusters and the neighborhood radius of core points - we empirically show that\nit is nearly unimodal, and support this claim theoretically in a specific\nsetting. We leverage this property to devise new strategies for finding\nappropriate values for the radius more efficiently based on the Ternary Search\nalgorithm. This is especially important for large scale data that is\nhigh-dimensional, where parameter tuning is computationally intensive. We\nvalidate our methodology through extensive applications across a range of\nhigh-dimensional, large-scale NLP, Audio, and Computer Vision tasks,\ndemonstrating its practical effectiveness and robustness. This work not only\noffers a significant advancement in parameter control for density-based\nclustering but also broadens the understanding regarding the relations between\ntheir guiding parameters. Our code is available at\nhttps://github.com/oronnir/UnimodalStrategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21695v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21695v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21818", "title": "A systematic review of research on large language models for computer programming education", "authors": ["Meina Zhu", "Lanyu Xu", "Barbara Ericson"], "summary": "Given the increasing demands in computer programming education and the rapid\nadvancement of large language models (LLMs), LLMs play a critical role in\nprogramming education. This study provides a systematic review of selected\nempirical studies on LLMs in computer programming education, published from\n2023 to March 2024. The data for this review were collected from Web of Science\n(SCI/SSCI), SCOPUS, and EBSCOhost databases, as well as three conference\nproceedings specialized in computer programming education. In total, 42 studies\nmet the selection criteria and were reviewed using methods, including\nbibliometric analysis, thematic analysis, and structural topic modeling. This\nstudy offers an overview of the current state of LLMs in computer programming\neducation research. It outlines LLMs' applications, benefits, limitations,\nconcerns, and implications for future research and practices, establishing\nconnections between LLMs and their practical use in computer programming\neducation. This review also provides examples and valuable insights for\ninstructional designers, instructors, and learners. Additionally, a conceptual\nframework is proposed to guide education practitioners in integrating LLMs into\ncomputer programming education. This study suggests future research directions\nfrom various perspectives, emphasizing the need to expand research methods and\ntopics in computer programming education as LLMs evolve. Additionally, future\nresearch in the field should incorporate collaborative, interdisciplinary, and\ntransdisciplinary efforts on a large scale, focusing on longitudinal research\nand development initiatives.", "comment": "41 pages except references, 7 figures, 3 tables, a systematic review\n  paper", "pdf_url": "http://arxiv.org/pdf/2506.21818v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21818v1", "date": "2025-04-13", "updated": "2025-04-13"}
{"id": "2506.22172", "title": "Bridging CGR and $k$-mer Frequencies of DNA", "authors": ["Haoze He", "Lila Kari", "Pablo Millan Arias"], "summary": "This paper establishes formal mathematical foundations linking Chaos Game\nRepresentations (CGR) of DNA sequences to their underlying $k$-mer frequencies.\nWe prove that the Frequency CGR (FCGR) of order $k$ is mathematically\nequivalent to a discretization of CGR at resolution $2^k \\times 2^k$, and its\nvectorization corresponds to the $k$-mer frequencies of the sequence.\nAdditionally, we characterize how symmetry transformations of CGR images\ncorrespond to specific nucleotide permutations in the originating sequences.\nLeveraging these insights, we introduce an algorithm that generates synthetic\nDNA sequences from prescribed $k$-mer distributions by constructing Eulerian\npaths on De Bruijn multigraphs. This enables reconstruction of sequences\nmatching target $k$-mer profiles with arbitrarily high precision, facilitating\nthe creation of synthetic CGR images for applications such as data augmentation\nfor machine learning-based taxonomic classification of DNA sequences. Numerical\nexperiments validate the effectiveness of our method across both real genomic\ndata and artificially sampled distributions. To our knowledge, this is the\nfirst comprehensive framework that unifies CGR geometry, $k$-mer statistics,\nand sequence reconstruction, offering new tools for genomic analysis and\nvisualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22172v1", "categories": ["cs.FL", "D.3.1"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.22172v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22023", "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.", "comment": "17 pages, 8 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.22023v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22023v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21571", "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21571v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21571v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.21631", "title": "Real-Time 3D Guidewire Reconstruction from Intraoperative DSA Images for Robot-Assisted Endovascular Interventions", "authors": ["Tianliang Yao", "Bingrui Li", "Bo Lu", "Zhiqiang Pei", "Yixuan Yuan", "Peng Qi"], "summary": "Accurate three-dimensional (3D) reconstruction of guidewire shapes is crucial\nfor precise navigation in robot-assisted endovascular interventions.\nConventional 2D Digital Subtraction Angiography (DSA) is limited by the absence\nof depth information, leading to spatial ambiguities that hinder reliable\nguidewire shape sensing. This paper introduces a novel multimodal framework for\nreal-time 3D guidewire reconstruction, combining preoperative 3D Computed\nTomography Angiography (CTA) with intraoperative 2D DSA images. The method\nutilizes robust feature extraction to address noise and distortion in 2D DSA\ndata, followed by deformable image registration to align the 2D projections\nwith the 3D CTA model. Subsequently, the inverse projection algorithm\nreconstructs the 3D guidewire shape, providing real-time, accurate spatial\ninformation. This framework significantly enhances spatial awareness for\nrobotic-assisted endovascular procedures, effectively bridging the gap between\npreoperative planning and intraoperative execution. The system demonstrates\nnotable improvements in real-time processing speed, reconstruction accuracy,\nand computational efficiency. The proposed method achieves a projection error\nof 1.76$\\pm$0.08 pixels and a length deviation of 2.93$\\pm$0.15\\%, with a frame\nrate of 39.3$\\pm$1.5 frames per second (FPS). These advancements have the\npotential to optimize robotic performance and increase the precision of complex\nendovascular interventions, ultimately contributing to better clinical\noutcomes.", "comment": "This paper has been accepted by IEEE/RSJ IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21631v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21631v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.21703", "title": "Using Generative AI in Software Design Education: An Experience Report", "authors": ["Victoria Jackson", "Susannah Liu", "Andre van der Hoek"], "summary": "With the rapid adoption of Generative AI (GenAI) tools, software engineering\neducators have grappled with how best to incorporate them into the classroom.\nWhile some research discusses the use of GenAI in the context of learning to\ncode, there is little research that explores the use of GenAI in the classroom\nfor other areas of software development. This paper provides an experience\nreport on introducing GenAI into an undergraduate software design class.\nStudents were required to use GenAI (in the form of ChatGPT) to help complete a\nteam-based assignment. The data collected consisted of the ChatGPT conversation\nlogs and students' reflections on using ChatGPT for the assignment.\nSubsequently, qualitative analysis was undertaken on the data. Students\nidentified numerous ways ChatGPT helped them in their design process while\nrecognizing the need to critique the response before incorporating it into\ntheir design. At the same time, we identified several key lessons for educators\nin how to deploy GenAI in a software design class effectively. Based on our\nexperience, we believe students can benefit from using GenAI in software design\neducation as it helps them design and learn about the strengths and weaknesses\nof GenAI.", "comment": "12 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.21703v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21703v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21845", "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "authors": ["Zhuodi Cai"], "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "pdf_url": "http://arxiv.org/pdf/2506.21845v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21845v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22293", "title": "The Effect of Network Topology on the Equilibria of Influence-Opinion Games", "authors": ["Yigit Ege Bayiz", "Arash Amini", "Radu Marculescu", "Ufuk Topcu"], "summary": "Online social networks exert a powerful influence on public opinion.\nAdversaries weaponize these networks to manipulate discourse, underscoring the\nneed for more resilient social networks. To this end, we investigate the impact\nof network connectivity on Stackelberg equilibria in a two-player game to shape\npublic opinion. We model opinion evolution as a repeated competitive\ninfluence-propagation process. Players iteratively inject \\textit{messages}\nthat diffuse until reaching a steady state, modeling the dispersion of two\ncompeting messages. Opinions then update according to the discounted sum of\nexposure to the messages. This bi-level model captures viral-media correlation\neffects omitted by standard opinion-dynamics models. To solve the resulting\nhigh-dimensional game, we propose a scalable, iterative algorithm based on\nlinear-quadratic regulators that approximates local feedback Stackelberg\nstrategies for players with limited cognition. We analyze how the network\ntopology shapes equilibrium outcomes through experiments on synthetic networks\nand real Facebook data. Our results identify structural characteristics that\nimprove a network's resilience to adversarial influence, guiding the design of\nmore resilient social networks.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.22293v1", "categories": ["cs.SI", "cs.SY", "eess.SY", "91D30, 91D10"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22293v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22260", "title": "Design and Evaluation of IEEE 802.11ax Uplink Orthogonal Frequency Division Multiple Random Access in ns-3", "authors": ["Douglas Dziedzorm Agbeve", "Andrey Belogaev", "Jeroen Famaey"], "summary": "Wi-Fi networks have long relied on the Enhanced Distributed Channel Access\n(EDCA) mechanism, allowing stations to compete for transmission opportunities.\nHowever, as networks become denser and emerging applications demand lower\nlatency and higher reliability, the limitations of EDCA such as overhead due to\ncontention and collisions have become more pronounced. To address these\nchallenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been\nintroduced in Wi-Fi, enabling more efficient channel utilization through\nscheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal\nFrequency Division Multiple Random Access (UORA), a hybrid mechanism that\ncombines both scheduled and random access, balancing efficiency and\nresponsiveness in resource allocation. Despite significant research on UORA,\nmost studies rely on custom simulators that are not publicly available,\nlimiting reproducibility and preventing validation of the presented results.\nThe only known open-source UORA implementation in the ns-3 simulator exhibits\nkey limitations, such as usage of the same trigger frame (TF) to schedule\nresources for buffer status reports and data transmissions, and lack of\nsignaling for UORA configuration. In this paper, we present a fully\nstandard-compliant and open source UORA implementation that is compatible with\nns-3 version 3.38, addressing these limitations to improve resource allocation\nefficiency and adaptability. This implementation enables more accurate and\nflexible evaluation of UORA, fostering future research on Wi-Fi resource\nallocation strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22260v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22260v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22094", "title": "Nonlinear Power Amplifier-Resilient Cell-Free Massive MIMO: A Joint Optimization Approach", "authors": ["Wei Jiang", "Hans D. Schotten"], "summary": "This letter analyzes the effects of power amplifiers (PAs) on the downlink of\ncell-free massive MIMO systems. We model signal transmission incorporating\nnonlinear PA distortion and derive a unified spectral efficiency (SE)\nexpression applicable to arbitrary precoding schemes. To combat PA-induced\nperformance degradation, a joint optimization approach for user association and\nmax-min power control is proposed. Furthermore, a low-complexity alternative is\ndeveloped to approximate the joint optimization with reduced computational\noverhead. Simulations validate the analysis and demonstrate significant\nperformance gains of the proposed approaches over conventional techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22094v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22094v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21711", "title": "CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection", "authors": ["Aryan Thakre", "Omkar Nagwekar", "Vedang Talekar", "Aparna Santra Biswas"], "summary": "Deepfakes have emerged as a significant threat to digital media authenticity,\nincreasing the need for advanced detection techniques that can identify subtle\nand time-dependent manipulations. CNNs are effective at capturing spatial\nartifacts, and Transformers excel at modeling temporal inconsistencies.\nHowever, many existing CNN-Transformer models process spatial and temporal\nfeatures independently. In particular, attention-based methods often use\nseparate attention mechanisms for spatial and temporal features and combine\nthem using naive approaches like averaging, addition, or concatenation, which\nlimits the depth of spatio-temporal interaction. To address this challenge, we\npropose a unified CAST model that leverages cross-attention to effectively fuse\nspatial and temporal features in a more integrated manner. Our approach allows\ntemporal features to dynamically attend to relevant spatial regions, enhancing\nthe model's ability to detect fine-grained, time-evolving artifacts such as\nflickering eyes or warped lips. This design enables more precise localization\nand deeper contextual understanding, leading to improved performance across\ndiverse and challenging scenarios. We evaluate the performance of our model\nusing the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both\nintra- and cross-dataset settings to affirm the superiority of our approach.\nOur model achieves strong performance with an AUC of 99.49 percent and an\naccuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset\ntesting, it demonstrates impressive generalization by achieving a 93.31 percent\nAUC on the unseen DeepfakeDetection dataset. These results highlight the\neffectiveness of cross-attention-based feature fusion in enhancing the\nrobustness of deepfake video detection.", "comment": "50 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21711v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21711v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22171", "title": "Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "summary": "Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure\nthe ledger yet cannot measure validator trustworthiness, allowing subtle\nmisconduct that is especially damaging in decentralized-finance (DeFi)\nsettings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)\ngives each action a layered utility score -- covering motivation and outcome,\n(ii) adapts validator weights using recent scores, and (iii) applies\ndecentralized verification with proportional slashing. The reward design is\nincentive-compatible, yielding a Nash equilibrium in which honest behavior\nmaximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,\nreputation-weighted validation) show that PoB cuts fraud acceptance by more\nthan 90%, demotes malicious validators within two rounds, and improves proposer\nfairness versus standard PoS, all with no more than a 5% throughput overhead.\nBy linking consensus influence to verifiably trustworthy conduct, PoB offers a\nscalable, regulation-friendly foundation for secure and fair blockchain\ngovernance in financial applications.", "comment": "8 pages, submitted to WI IAT 2025", "pdf_url": "http://arxiv.org/pdf/2506.22171v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22171v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21927", "title": "A Deep Learning Algorithm Based on CNN-LSTM Framework for Predicting Cancer Drug Sales Volume", "authors": ["Yinghan Li", "Yilin Yao", "Junghua Lin", "Nanxi Wang"], "summary": "This study explores the application potential of a deep learning model based\non the CNN-LSTM framework in forecasting the sales volume of cancer drugs, with\na focus on modeling complex time series data. As advancements in medical\ntechnology and cancer treatment continue, the demand for oncology medications\nis steadily increasing. Accurate forecasting of cancer drug sales plays a\ncritical role in optimizing production planning, supply chain management, and\nhealthcare policy formulation. The dataset used in this research comprises\nquarterly sales records of a specific cancer drug in Egypt from 2015 to 2024,\nincluding multidimensional information such as date, drug type, pharmaceutical\ncompany, price, sales volume, effectiveness, and drug classification. To\nimprove prediction accuracy, a hybrid deep learning model combining\nConvolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks\nis employed. The CNN component is responsible for extracting local temporal\nfeatures from the sales data, while the LSTM component captures long-term\ndependencies and trends. Model performance is evaluated using two widely\nadopted metrics: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\nThe results demonstrate that the CNN-LSTM model performs well on the test set,\nachieving an MSE of 1.150 and an RMSE of 1.072, indicating its effectiveness in\nhandling nonlinear and volatile sales data. This research provides theoretical\nand technical support for data-driven decision-making in pharmaceutical\nmarketing and healthcare resource planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21927v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21927v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21784", "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models", "authors": ["Yifan Liu", "Xishun Liao", "Haoxuan Ma", "Jonathan Liu", "Rohan Jadhav", "Jiaqi Ma"], "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21784v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21784v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22073", "title": "Linear-Quadratic Discrete-Time Dynamic Games with Unknown Dynamics", "authors": ["Shengyuan Huang", "Xiaoguang Yang", "Zhigang Cao", "Wenjun Mei"], "summary": "Considering linear-quadratic discrete-time games with unknown\ninput/output/state (i/o/s) dynamics and state, we provide necessary and\nsufficient conditions for the existence and uniqueness of feedback Nash\nequilibria (FNE) in the finite-horizon game, based entirely on offline\ninput/output data. We prove that the finite-horizon unknown-dynamics game and\nits corresponding known-dynamics game have the same FNEs, and provide detailed\nrelationships between their respective FNE matrices. To simplify the\ncomputation of FNEs, we provide an invertibility condition and a corresponding\nalgorithm that computes one FNE by solving a finite number of linear equation\nsystems using offline data. For the infinite-horizon unknown-dynamics game,\nlimited offline data restricts players to computing optimal strategies only\nover a finite horizon. We prove that the finite-horizon strategy ``watching $T$\nsteps into the future and moving one step now,'' which is commonly used in\nclassical optimal control, exhibits convergence in both the FNE matrices and\nthe total costs in the infinite-horizon unknown-dynamics game, and further\nprovide an analysis of the convergence rate of the total cost. The\ncorresponding algorithm for the infinite-horizon game is proposed and its\nefficacy is demonstrated through a non-scalar numerical example.", "comment": "25 pages, 2 figures, 2 algorithms", "pdf_url": "http://arxiv.org/pdf/2506.22073v1", "categories": ["eess.SY", "cs.SY", "math.OC", "91A50, 90C39"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22073v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21796", "title": "Demonstrating Interoperable Channel State Feedback Compression with Machine Learning", "authors": ["Dani Korpi", "Rachel Wang", "Jerry Wang", "Abdelrahman Ibrahim", "Carl Nuzman", "Runxin Wang", "Kursat Rasim Mestav", "Dustin Zhang", "Iraj Saniee", "Shawn Winston", "Gordana Pavlovic", "Wei Ding", "William J. Hillery", "Chenxi Hao", "Ram Thirunagari", "Jung Chang", "Jeehyun Kim", "Bartek Kozicki", "Dragan Samardzija", "Taesang Yoo", "Andreas Maeder", "Tingfang Ji", "Harish Viswanathan"], "summary": "Neural network-based compression and decompression of channel state feedback\nhas been one of the most widely studied applications of machine learning (ML)\nin wireless networks. Various simulation-based studies have shown that ML-based\nfeedback compression can result in reduced overhead and more accurate channel\ninformation. However, to the best of our knowledge, there are no real-life\nproofs of concepts demonstrating the benefits of ML-based channel feedback\ncompression in a practical setting, where the user equipment (UE) and base\nstation have no access to each others' ML models. In this paper, we present a\nnovel approach for training interoperable compression and decompression ML\nmodels in a confidential manner, and demonstrate the accuracy of the ensuing\nmodels using prototype UEs and base stations. The performance of the ML-based\nchannel feedback is measured both in terms of the accuracy of the reconstructed\nchannel information and achieved downlink throughput gains when using the\nchannel information for beamforming. The reported measurement results\ndemonstrate that it is possible to develop an accurate ML-based channel\nfeedback link without having to share ML models between device and network\nvendors. These results pave the way for a practical implementation of ML-based\nchannel feedback in commercial 6G networks.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.21796v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21796v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21884", "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "comment": "Paper accepted at ICCV 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2506.21884v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21884v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22261", "title": "Shortest Paths in Multimode Graphs", "authors": ["Yael Kirkpatrick", "Virginia Vassilevska Williams"], "summary": "In this work we study shortest path problems in multimode graphs, a\ngeneralization of the min-distance measure introduced by Abboud, Vassilevska W.\nand Wang in [SODA'16]. A multimode shortest path is the shortest path using one\nof multiple `modes' of transportation that cannot be combined. This represents\nreal-world scenarios where different modes are not combinable, such as flights\noperated by different airlines. More precisely, a $k$-multimode graph is a\ncollection of $k$ graphs on the same vertex set and the $k$-mode distance\nbetween two vertices is defined as the minimum among the distances computed in\neach individual graph.\n  We focus on approximating fundamental graph parameters on these graphs,\nspecifically diameter and radius. In undirected multimode graphs we first show\nan elegant linear time 3-approximation algorithm for 2-mode diameter. We then\nextend this idea into a general subroutine that can be used as a part of any\n$\\alpha$-approximation, and use it to construct a 2 and 2.5 approximation\nalgorithm for 2-mode diameter. For undirected radius, we introduce a general\nscheme that can compute a 3-approximation of the $k$-mode radius for any $k$.\nIn the directed case we develop novel techniques to construct a linear time\nalgorithm to determine whether the diameter is finite.\n  We also develop many conditional fine-grained lower bounds for various\nmultimode diameter and radius approximation problems. We are able to show that\nmany of our algorithms are tight under popular fine-grained complexity\nhypotheses, including our linear time 3-approximation for $3$-mode undirected\ndiameter and radius. As part of this effort we propose the first extension to\nthe Hitting Set Hypothesis [SODA'16], which we call the $\\ell$-Hitting Set\nHypothesis. We use this hypothesis to prove the first parameterized lower bound\ntradeoff for radius approximation algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22261v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22261v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22250", "title": "A Design Space for Visualization Transitions of 3D Spatial Data in Hybrid AR-Desktop Environments", "authors": ["Yucheng Lu", "Tobias Rau", "Benjamin Lee", "Andreas Köhn", "Michael Sedlmair", "Christian Sandor", "Tobias Isenberg"], "summary": "We present a design space for animated transitions of the appearance of 3D\nspatial datasets in a hybrid Augmented Reality (AR)-desktop context. Such\nhybrid interfaces combine both traditional and immersive displays to facilitate\nthe exploration of 2D and 3D data representations in the environment in which\nthey are best displayed. One key aspect is to introduce transitional animations\nthat change between the different dimensionalities to illustrate the connection\nbetween the different representations and to reduce the potential cognitive\nload on the user. The specific transitions to be used depend on the type of\ndata, the needs of the application domain, and other factors. We summarize\nthese as a transition design space to simplify the decision-making process and\nprovide inspiration for future designs. First, we discuss 3D visualizations\nfrom a spatial perspective: a spatial encoding pipeline, where 3D data sampled\nfrom the physical world goes through various transformations, being mapped to\nvisual representations, and then being integrated into a hybrid AR-desktop\nenvironment. The transition design then focuses on interpolating between two\nspatial encoding pipelines to provide a smooth experience. To illustrate the\nuse of our design space, we apply it to three case studies that focus on\napplications in astronomy, radiology, and chemistry; we then discuss lessons\nlearned from these applications.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22250v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22250v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22362", "title": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding", "authors": ["Yang Yang", "Yunpeng Li", "George Sung", "Shao-Fu Shih", "Craig Dooley", "Alessio Centazzo", "Ramanan Rajeswaran"], "summary": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22362v1", "categories": ["eess.AS", "cs.LG"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22362v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21558", "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter Mühlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21558v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21558v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21598", "title": "SERP Interference Network and Its Applications in Search Advertising", "authors": ["Purak Jain", "Sandeep Appala"], "summary": "Search Engine marketing teams in the e-commerce industry manage global search\nengine traffic to their websites with the aim to optimize long-term\nprofitability by delivering the best possible customer experience on Search\nEngine Results Pages (SERPs). In order to do so, they need to run continuous\nand rapid Search Marketing A/B tests to continuously evolve and improve their\nproducts. However, unlike typical e-commerce A/B tests that can randomize based\non customer identification, their tests face the challenge of anonymized users\non search engines. On the other hand, simply randomizing on products violates\nStable Unit Treatment Value Assumption for most treatments of interest. In this\nwork, we propose leveraging censored observational data to construct bipartite\n(Search Query to Product Ad or Text Ad) SERP interference networks. Using a\nnovel weighting function, we create weighted projections to form unipartite\ngraphs which can then be use to create clusters to randomized on. We\ndemonstrate this experimental design's application in evaluating a new bidding\nalgorithm for Paid Search. Additionally, we provide a blueprint of a novel\nsystem architecture utilizing SageMaker which enables polyglot programming to\nimplement each component of the experimental framework.", "comment": "This is an extended version of our paper published at the AdKDD 2024\n  workshop, co-located with ACM KDD. CEUR-WS proceedings:\n  https://ceur-ws.org/Vol-3837/paper_12_ceur_paper.pdf", "pdf_url": "http://arxiv.org/pdf/2506.21598v1", "categories": ["cs.IR", "stat.ME", "H.3.3; I.2.6"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21598v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21888", "title": "Semi Analytical Solution of a Nonlinear Oblique Boundary Value Problem", "authors": ["Mriganka Shekhar Chaki", "Maria C. Jorge"], "summary": "A new numerical method is developed to approximate the solution of Laplace's\nequation in the exterior of the sphere with a strongly nonlinear boundary value\nof oblique type. A functional analysis attempt to solve this type of boundary\ncondition is not straight forward since results about existence and uniqueness\nof solution are still limited. Hence, a semi analytical method is described\nhere to approach a solution. A perturbation solution around the monopole\nconverts the nonlinear oblique problem into a series of known Neumann problems\nin the exterior of the sphere. The corresponding Green's function\nrepresentation for the exterior Neumann problem gives an exact analytic\nsolution for each perturbation step as an integral on the surface of the\nsphere. Nevertheless, the boundary conditions become very complicated and\nrequire to be approximated numerically. The perturbation solutions given by\nintegrals of the Green's function on the sphere are computed at each\nperturbation step using different subdivisions of the surface integrals with\nthe help of adaptive quadrature method. We call icosahedron method to the\nintegration on the sphere with an icosahedron mesh using Gauss 5-point or\nadaptive quadrature, according to the integration parameter. This method was\nvery effective to deal with the singularity of the Green's function\nsuccessfully avoiding inaccuracies on the numerical approximation and is an\nimportant contribution of this work. The numerical perturbation scheme is\nperformed for two given exact solutions. The icosahedron method is found to be\nvery precise. The approximations show the desired properties: they get closer\nto the exact solutions as the perturbation parameter gets smaller, show rapid\nconvergence in the exterior of the unit sphere and converge to zero as the\nradius grows.", "comment": "23 pages, 21 figures", "pdf_url": "http://arxiv.org/pdf/2506.21888v1", "categories": ["math.NA", "cs.NA", "math-ph", "math.MP", "34B15, 74G10, 34B27"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21888v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22137", "title": "On Drug Delivery System Parameter Optimisation via Semantic Information Theory", "authors": ["Milica Lekić", "Mohammad Zoofaghari", "Ilangko Balasingham", "Mladen Veletić"], "summary": "We investigate the application of semantic information theory to drug\ndelivery systems (DDS) within the molecular communication (MC) framework. To\noperationalise this, we observe a DDS as a molecular concentration-based\nchannel. Semantic information is defined as the amount of information required\nfor a DDS to achieve its therapeutic goal in a dynamic environment. We derive\nit by introducing interventions, defined as modifications to DDS parameters, a\nviability function, and system-environment correlations quantified via the\nchannel capacity. Here, the viability function represents DDS performance based\non a drug dose-response relationship. Our model considers a system capable of\ninducing functional changes in a receiver cancer cell, where exceeding critical\nDDS parameter values can significantly reduce performance or\ncost-effectiveness. By analysing the MC-based DDS model through a semantic\ninformation perspective, we examine how correlations between the internalised\nparticle concentration $(Y)$ and the particle concentration in the\nextracellular environment $(X)$ evolve under interventions. The final catalogue\nof results provides a quantitative basis for DDS design and optimisation,\noffering a method to determine optimal DDS parameter values under constraints\nsuch as chemical budget, desired effect and accuracy. Thus, the proposed\nframework can serve as a novel tool for guiding DDS design and optimisation.", "comment": "This work has been submitted for possible publication in the IEEE\n  TRANSACTIONS ON MOLECULAR, BIOLOGICAL, AND MULTI-SCALE COMMUNICATIONS journal", "pdf_url": "http://arxiv.org/pdf/2506.22137v1", "categories": ["cs.IT", "cs.ET", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22137v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21714", "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have\nbeen studied using the unified theoretical framework. Although such models can\ngenerate high-quality data points from a noise distribution, the sampling\ndemands multiple iterations to solve an ordinary differential equation (ODE)\nwith high computational complexity. Most existing methods focus on reducing the\nnumber of time steps during the sampling process to improve efficiency. In this\nwork, we explore a complementary direction in which the quality-complexity\ntradeoff can be dynamically controlled in terms of time steps and in the length\nof the neural network. We achieve this by rewiring the blocks in the\ntransformer-based architecture to solve an inner discretized ODE w.r.t. its\nlength. Then, we employ time- and length-wise consistency terms during flow\nmatching training, and as a result, the sampling can be performed with an\narbitrary number of time steps and transformer blocks. Unlike others, our\n$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in\ntime dimension and decreases both latency and memory usage. Compared to the\nprevious state of the art, image generation experiments on CelebA-HQ and\nImageNet show a latency reduction of up to $3\\times$ in the most efficient\nsampling mode, and a FID score improvement of up to $3.5$ points for\nhigh-quality sampling. We release our code and model weights with fully\nreproducible experiments.", "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "pdf_url": "http://arxiv.org/pdf/2506.21714v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21714v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21825", "title": "Exploring the change in scientific readability following the release of ChatGPT", "authors": ["Abdulkareem Alsudais"], "summary": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21825v1", "categories": ["cs.CY", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21825v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22061", "title": "Negated String Containment is Decidable (Technical Report)", "authors": ["Vojtěch Havlena", "Michal Hečko", "Lukáš Holík", "Ondřej Lengál"], "summary": "We provide a positive answer to a long-standing open question of the\ndecidability of the not-contains string predicate. Not-contains is practically\nrelevant, for instance in symbolic execution of string manipulating programs.\nParticularly, we show that the predicate notContains(x1 ... xn, y1 ... ym),\nwhere x1 ... xn and y1 ... ym are sequences of string variables constrained by\nregular languages, is decidable. Decidability of a not-contains predicate\ncombined with chain-free word equations and regular membership constraints\nfollows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22061v1", "categories": ["cs.LO", "cs.FL"], "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.22061v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22237", "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.", "comment": "9 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.22237v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22237v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21609", "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models", "authors": ["Junhao Liu", "Zhenhao Xu", "Yuxin Fang", "Yichuan Chen", "Zuobin Ying", "Wenhan Chang"], "summary": "Recently, there have been notable advancements in large language models\n(LLMs), demonstrating their growing abilities in complex reasoning. However,\nexisting research largely overlooks a thorough and systematic comparison of\nthese models' reasoning processes and outputs, particularly regarding their\nself-reflection pattern (also termed \"Aha moment\") and the interconnections\nacross diverse domains. This paper proposes a novel framework for analyzing the\nreasoning characteristics of four cutting-edge large reasoning models (GPT-o1,\nDeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge\nparadigm. Our approach connects their internal thinking processes with their\nfinal outputs. A diverse dataset consists of real-world scenario-based\nquestions covering logical deduction, causal inference, and multi-step\nproblem-solving. Additionally, a set of metrics is put forward to assess both\nthe coherence of reasoning and the accuracy of the outputs. The research\nresults uncover various patterns of how these models balance exploration and\nexploitation, deal with problems, and reach conclusions during the reasoning\nprocess. Through quantitative and qualitative comparisons, disparities among\nthese models are identified in aspects such as the depth of reasoning, the\nreliance on intermediate steps, and the degree of similarity between their\nthinking processes and output patterns and those of GPT-o1. This work offers\nvaluable insights into the trade-off between computational efficiency and\nreasoning robustness and provides practical recommendations for enhancing model\ndesign and evaluation in practical applications. We publicly release our\nproject at: https://github.com/ChangWenhan/FromThinking2Output", "comment": "18 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21609v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21609v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21635", "title": "AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing", "authors": ["Haiping Yang", "Huaxing Liu", "Wei Wu", "Zuohui Chen", "Ning Wu"], "summary": "Unmanned aerial vehicles (UAVs) are increasingly employed in diverse\napplications such as land surveying, material transport, and environmental\nmonitoring. Following missions like data collection or inspection, UAVs must\nland safely at docking stations for storage or recharging, which is an\nessential requirement for ensuring operational continuity. However, accurate\nlanding remains challenging due to factors like GPS signal interference. To\naddress this issue, we propose a deviation warning system for UAV landings,\npowered by a novel vision-based model called AeroLite-MDNet. This model\nintegrates a multiscale fusion module for robust cross-scale object detection\nand incorporates a segmentation branch for efficient orientation estimation. We\nintroduce a new evaluation metric, Average Warning Delay (AWD), to quantify the\nsystem's sensitivity to landing deviations. Furthermore, we contribute a new\ndataset, UAVLandData, which captures real-world landing deviation scenarios to\nsupport training and evaluation. Experimental results show that our system\nachieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\\%,\ndemonstrating its effectiveness in enhancing UAV landing reliability. Code will\nbe available at https://github.com/ITTTTTI/Maskyolo.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21635v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21635v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22037", "title": "KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering", "authors": ["Jiawei Li", "Zan Liang", "Guoxin Wang", "Jinzhi Lu", "Yan Yan", "Shouxuan Wu", "Hao Wang"], "summary": "Model reconstruction is a method used to drive the development of complex\nsystem development processes in model-based systems engineering. Currently,\nduring the iterative design process of a system, there is a lack of an\neffective method to manage changes in development requirements, such as\ndevelopment cycle requirements and cost requirements, and to realize the\nreconstruction of the system development process model. To address these\nissues, this paper proposes a model reconstruction method to support the\ndevelopment process model. Firstly, the KARMA language, based on the GOPPRR-E\nmetamodeling method, is utilized to uniformly formalize the process models\nconstructed based on different modeling languages. Secondly, a model\nreconstruction framework is introduced. This framework takes a structured\ndevelopment requirements based natural language as input, employs natural\nlanguage processing techniques to analyze the development requirements text,\nand extracts structural and optimization constraint information. Then, after\nstructural reorganization and algorithm optimization, a development process\nmodel that meets the development requirements is obtained. Finally, as a case\nstudy, the development process of the aircraft onboard maintenance system is\nreconstructed. The results demonstrate that this method can significantly\nenhance the design efficiency of the development process.", "comment": "12 pages, 9 figures, submitted to the 15th international Complex\n  Systems Design & Management (CSD&M) conference", "pdf_url": "http://arxiv.org/pdf/2506.22037v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22037v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21896", "title": "Focus on the Experts: Co-designing an Augmented Reality Eye-Gaze Tracking System with Surgical Trainees to Improve Endoscopic Instruction", "authors": ["Jumanh Atoum", "Jinkyung Park", "Mamtaj Akter", "Nicholas Kavoussi", "Pamela Wisniewski", "Jie Ying Wu"], "summary": "The current apprenticeship model for surgical training requires a high level\nof supervision, which does not scale well to meet the growing need for more\nsurgeons. Many endoscopic procedures are directly taught in the operating room\n(OR) while the attending surgeon and trainee operate on patients. The need to\nprioritize patient care limits the trainees' opportunities to experiment and\nreceive feedback on their performance. Augmented reality (AR) has the potential\nto increase efficiency in endoscopic surgical training, but additional research\nis critical to understanding the needs of surgical trainees to inform the\ndesign of AR training systems. Therefore, we worked with 18 surgical trainees\nto understand the strengths, limitations, and unmet needs of their current\ntraining environment and to co-design an AR eye-gaze tracking system based on\ntheir preferences. Trainees emphasized the need to practice the 2D to 3D\nmapping needed to properly familiarize oneself with the anatomy of patients to\nprepare for real surgery. The trainees felt that an AR-based eye gaze tracking\nsystem would be a useful supplemental training method that would improve their\nlearning in OR cases without detracting from patient care. To tailor the AR\nsystem to their needs, they co-designed features to improve their ability to\ntrack the attending surgeon's eye gaze and to provide a real-time, interactive\nsystem. Our results are valuable in shaping the endoscopic training modules by\ngenerating user-informed guidelines to design future collaborative AR-based\neye-gaze tracking systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21896v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21896v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21620", "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit", "authors": ["Daniele Cirulli", "Giulio Cimini", "Giovanni Palermo"], "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for\nnatural language generation, with applications spanning from content creation\nto social simulations. Their ability to mimic human interactions raises both\nopportunities and concerns, particularly in the context of politically relevant\nonline discussions. In this study, we evaluate the performance of LLMs in\nreplicating user-generated content within a real-world, divisive scenario:\nReddit conversations during the 2016 US Presidential election. In particular,\nwe conduct three different experiments, asking GPT-4 to generate comments by\nimpersonating either real or artificial partisan users. We analyze the\ngenerated comments in terms of political alignment, sentiment, and linguistic\nfeatures, comparing them against real user contributions and benchmarking\nagainst a null model. We find that GPT-4 is able to produce realistic comments,\nboth in favor of or against the candidate supported by the community, yet\ntending to create consensus more easily than dissent. In addition we show that\nreal and artificial comments are well separated in a semantically embedded\nspace, although they are indistinguishable by manual inspection. Our findings\nprovide insights on the potential use of LLMs to sneak into online discussions,\ninfluence political debate and shape political narratives, bearing broader\nimplications of AI-driven discourse manipulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21620v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21620v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22359", "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models", "authors": ["Viswanath Kumarskandpriya", "Abdulhalim Dandoush", "Abbas Bradai", "Ali Belgacem"], "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22359v1", "categories": ["cs.NI", "cs.AI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22359v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22137", "title": "On Drug Delivery System Parameter Optimisation via Semantic Information Theory", "authors": ["Milica Lekić", "Mohammad Zoofaghari", "Ilangko Balasingham", "Mladen Veletić"], "summary": "We investigate the application of semantic information theory to drug\ndelivery systems (DDS) within the molecular communication (MC) framework. To\noperationalise this, we observe a DDS as a molecular concentration-based\nchannel. Semantic information is defined as the amount of information required\nfor a DDS to achieve its therapeutic goal in a dynamic environment. We derive\nit by introducing interventions, defined as modifications to DDS parameters, a\nviability function, and system-environment correlations quantified via the\nchannel capacity. Here, the viability function represents DDS performance based\non a drug dose-response relationship. Our model considers a system capable of\ninducing functional changes in a receiver cancer cell, where exceeding critical\nDDS parameter values can significantly reduce performance or\ncost-effectiveness. By analysing the MC-based DDS model through a semantic\ninformation perspective, we examine how correlations between the internalised\nparticle concentration $(Y)$ and the particle concentration in the\nextracellular environment $(X)$ evolve under interventions. The final catalogue\nof results provides a quantitative basis for DDS design and optimisation,\noffering a method to determine optimal DDS parameter values under constraints\nsuch as chemical budget, desired effect and accuracy. Thus, the proposed\nframework can serve as a novel tool for guiding DDS design and optimisation.", "comment": "This work has been submitted for possible publication in the IEEE\n  TRANSACTIONS ON MOLECULAR, BIOLOGICAL, AND MULTI-SCALE COMMUNICATIONS journal", "pdf_url": "http://arxiv.org/pdf/2506.22137v1", "categories": ["cs.IT", "cs.ET", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22137v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21722", "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration", "authors": ["Xin Lu", "Xueyang Fu", "Jie Xiao", "Zihao Fan", "Yurui Zhu", "Zheng-Jun Zha"], "summary": "While diffusion models demonstrate strong generative capabilities in image\nrestoration (IR) tasks, their complex architectures and iterative processes\nlimit their practical application compared to mainstream reconstruction-based\ngeneral ordinary IR networks. Existing approaches primarily focus on optimizing\nnetwork architecture and diffusion paths but overlook the integration of the\ndiffusion training paradigm within general ordinary IR frameworks. To address\nthese challenges, this paper elucidates key principles for adapting the\ndiffusion training paradigm to general IR training through systematic analysis\nof time-step dependencies, network hierarchies, noise-level relationships, and\nmulti-restoration task correlations, proposing a new IR framework supported by\ndiffusion-based training. To enable IR networks to simultaneously restore\nimages and model generative representations, we introduce a series of\nregularization strategies that align diffusion objectives with IR tasks,\nimproving generalization in single-task scenarios. Furthermore, recognizing\nthat diffusion-based generation exerts varying influences across different IR\ntasks, we develop an incremental training paradigm and task-specific adaptors,\nfurther enhancing performance in multi-task unified IR. Experiments demonstrate\nthat our method significantly improves the generalization of IR networks in\nsingle-task IR and achieves superior performance in multi-task unified IR.\nNotably, the proposed framework can be seamlessly integrated into existing\ngeneral IR architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21722v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21722v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22175", "title": "MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism", "authors": ["Zheng Zhang", "Donglin Yang", "Yaqi Xia", "Liang Ding", "Dacheng Tao", "Xiaobo Zhou", "Dazhao Cheng"], "summary": "Recently, Mixture-of-Experts (MoE) has become one of the most popular\ntechniques to scale pre-trained models to extraordinarily large sizes. Dynamic\nactivation of experts allows for conditional computation, increasing the number\nof parameters of neural networks, which is critical for absorbing the vast\namounts of knowledge available in many deep learning areas. However, despite\nthe existing system and algorithm optimizations, there are significant\nchallenges to be tackled when it comes to the inefficiencies of communication\nand memory consumption.\n  In this paper, we present the design and implementation of MPipeMoE, a\nhigh-performance library that accelerates MoE training with adaptive and\nmemory-efficient pipeline parallelism. Inspired by that the MoE training\nprocedure can be divided into multiple independent sub-stages, we design\nadaptive pipeline parallelism with an online algorithm to configure the\ngranularity of the pipelining. Further, we analyze the memory footprint\nbreakdown of MoE training and identify that activations and temporary buffers\nare the primary contributors to the overall memory footprint. Toward memory\nefficiency, we propose memory reusing strategies to reduce memory requirements\nby eliminating memory redundancies, and develop an adaptive selection component\nto determine the optimal strategy that considers both hardware capacities and\nmodel characteristics at runtime. We implement MPipeMoE upon PyTorch and\nevaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA\nDGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up\nto 2.8x speedup and reduces memory footprint by up to 47% in training large\nmodels.", "comment": "11 pages, accepted at IPDPS 2023", "pdf_url": "http://arxiv.org/pdf/2506.22175v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22175v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2501.06184", "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06184v1", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2501.06184v1", "date": "2025-01-10", "updated": "2025-01-10"}
{"id": "2506.21805", "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21805v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21805v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22117", "title": "Learning Distributed Safe Multi-Agent Navigation via Infinite-Horizon Optimal Graph Control", "authors": ["Fenglan Wang", "Xinguo Shu", "Lei He", "Lin Zhao"], "summary": "Distributed multi-agent navigation faces inherent challenges due to the\ncompeting requirements of maintaining safety and achieving goal-directed\nbehavior, particularly for agents with limited sensing range operating in\nunknown environments with dense obstacles. Existing approaches typically\nproject predefined goal-reaching controllers onto control barrier function\n(CBF) constraints, often resulting in conservative and suboptimal trade-offs\nbetween safety and goal-reaching performance. We propose an infinite-horizon\nCBF-constrained optimal graph control formulation for distributed safe\nmulti-agent navigation. By deriving the analytical solution structure, we\ndevelop a novel Hamilton-Jacobi-Bellman (HJB)-based learning framework to\napproximate the solution. In particular, our algorithm jointly learns a CBF and\na distributed control policy, both parameterized by graph neural networks\n(GNNs), along with a value function that robustly guides agents toward their\ngoals. Moreover, we introduce a state-dependent parameterization of Lagrange\nmultipliers, enabling dynamic trade-offs between safety and performance. Unlike\ntraditional short-horizon, quadratic programming-based CBF methods, our\napproach leverages long-horizon optimization to proactively avoid deadlocks and\nnavigate complex environments more effectively. Extensive simulation results\ndemonstrate substantial improvements in safety and task success rates across\nvarious agent dynamics, with strong scalability and generalization to\nlarge-scale teams in previously unseen environments. Real-world experiments\nusing Crazyflie drone swarms on challenging antipodal position-swapping tasks\nfurther validate the practicality, generalizability, and robustness of the\nproposed HJB-GNN learning framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22117v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22117v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21798", "title": "Adaptive Multipath-Based SLAM for Distributed MIMO Systems", "authors": ["Xuhong Li", "Benjamin J. B. Deutschmann", "Erik Leitinger", "Florian Meyer"], "summary": "Localizing users and mapping the environment using radio signals is a key\ntask in emerging applications such as reliable communications, location-aware\nsecurity, and safety critical navigation. Recently introduced multipath-based\nsimultaneous localization and mapping (MP-SLAM) can jointly localize a mobile\nagent and the reflective surfaces in radio frequency (RF) environments. Most\nexisting MP-SLAM methods assume that map features and their corresponding RF\npropagation paths are statistically independent, which neglects inherent\ndependencies arising when a single reflective surface contributes to different\npropagation paths or when an agent communicates with more than one base\nstation. Previous approaches that aim to fuse information across propagation\npaths are limited by their inability to perform ray tracing in environments\nwith nonconvex geometries. In this paper, we propose a Bayesian MP-SLAM method\nfor distributed MIMO systems that addresses this limitation. In particular, we\nuse amplitude statistics to establish adaptive time-varying detection\nprobabilities. Based on the resulting \"soft\" ray-tracing strategy, our method\ncan fuse information across propagation paths in RF environments with nonconvex\ngeometries. A Bayesian estimation method for the joint estimation of map\nfeatures and agent position is established by applying the message passing\nrules of the sum-product algorithm (SPA) to the factor graph that represents\nthe proposed statistical model. We also introduce an improved proposal PDF for\nparticle-based computation of SPA messages. This proposal PDF enables the early\ndetection of new surfaces that are solely supported by double-bounce paths. Our\nmethod is validated using synthetic RF measurements in a challenging scenario\nwith nonconvex geometries. The results demonstrate that it can provide accurate\nlocalization and mapping estimates as well as attain the posterior CRLB.", "comment": "30 pages. Submitted to IEEE Transactions on Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2506.21798v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21798v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21977", "title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression", "authors": ["Tianyu Zhang", "Xin Luo", "Li Li", "Dong Liu"], "summary": "Diffusion-based image compression has shown remarkable potential for\nachieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high\nrealism, by leveraging the generative priors of large pre-trained text-to-image\ndiffusion models. However, current approaches require a large number of\ndenoising steps at the decoder to generate realistic results under extreme\nbitrate constraints, limiting their application in real-time compression\nscenarios. Additionally, these methods often sacrifice reconstruction fidelity,\nas diffusion models typically fail to guarantee pixel-level consistency. To\naddress these challenges, we introduce StableCodec, which enables one-step\ndiffusion for high-fidelity and high-realism extreme image compression with\nimproved coding efficiency. To achieve ultra-low bitrates, we first develop an\nefficient Deep Compression Latent Codec to transmit a noisy latent\nrepresentation for a single-step denoising process. We then propose a\nDual-Branch Coding Structure, consisting of a pair of auxiliary encoder and\ndecoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end\noptimization with joint bitrate and pixel-level constraints. Extensive\nexperiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that\nStableCodec outperforms existing methods in terms of FID, KID and DISTS by a\nsignificant margin, even at bitrates as low as 0.005 bits per pixel, while\nmaintaining strong fidelity. Additionally, StableCodec achieves inference\nspeeds comparable to mainstream transform coding schemes. All source code are\navailable at https://github.com/LuizScarlet/StableCodec.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21977v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21977v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22281", "title": "Faster exponential algorithms for cut problems via geometric data structures", "authors": ["László Kozma", "Junqi Tan"], "summary": "For many hard computational problems, simple algorithms that run in time $2^n\n\\cdot n^{O(1)}$ arise, say, from enumerating all subsets of a size-$n$ set.\nFinding (exponentially) faster algorithms is a natural goal that has driven\nmuch of the field of exact exponential algorithms (e.g., see Fomin and Kratsch,\n2010). In this paper we obtain algorithms with running time $O(1.9999977^n)$ on\ninput graphs with $n$ vertices, for the following well-studied problems:\n  - $d$-Cut: find a proper cut in which no vertex has more than $d$ neighbors\non the other side of the cut;\n  - Internal Partition: find a proper cut in which every vertex has at least as\nmany neighbors on its side of the cut as on the other side; and\n  - ($\\alpha,\\beta$)-Domination: given intervals $\\alpha,\\beta \\subseteq\n[0,n]$, find a subset $S$ of the vertices, so that for every vertex $v \\in S$\nthe number of neighbors of $v$ in $S$ is from $\\alpha$ and for every vertex $v\n\\notin S$, the number of neighbors of $v$ in $S$ is from $\\beta$.\n  Our algorithms are exceedingly simple, combining the split and list technique\n(Horowitz and Sahni, 1974; Williams, 2005) with a tool from computational\ngeometry: orthogonal range searching in the moderate dimensional regime (Chan,\n2017). Our technique is applicable to the decision, optimization and counting\nversions of these problems and easily extends to various generalizations with\nmore fine-grained, vertex-specific constraints, as well as to directed,\nbalanced, and other variants. Algorithms with running times of the form $c^n$,\nfor $c<2$, were known for the first problem only for constant $d$, and for the\nthird problem for certain special cases of $\\alpha$ and $\\beta$; for the second\nproblem we are not aware of such results.", "comment": "10 pages; to be presented at ESA 2025", "pdf_url": "http://arxiv.org/pdf/2506.22281v1", "categories": ["cs.DS", "cs.CG"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22281v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2109.05721", "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "pdf_url": "http://arxiv.org/pdf/2109.05721v2", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2109.05721v2", "date": "2021-09-13", "updated": "2022-12-19"}
{"id": "2506.21555", "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "comment": "Accepted in Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21555v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21555v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21559", "title": "GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations", "authors": ["Junze Chen", "Cheng Yang", "Shujie Li", "Zhiqiang Zhang", "Yawen Li", "Junping Du", "Chuan Shi"], "summary": "Large language models (LLMs) have demonstrated their strong capabilities in\nvarious domains, and have been recently integrated for graph analysis as graph\nlanguage models (GLMs). With LLMs as the predictor, some GLMs can interpret\nunseen tasks described by natural language, and learn from a few examples in\nthe prompts without parameter tuning, known as in-context learning (ICL).\nAnother subset of GLMs utilizes abundant training labels to enhance model\nperformance, known as instruction tuning. However, we argue that ICL on graphs\nhas effectiveness issues due to fixed parameters and efficiency issues due to\nlong context. Meanwhile, the large amount of labeled data required for\ninstruction tuning can be difficult to obtain in real-world scenarios. To this\nend, we aim to introduce an extra parameter adaptation stage that can\nefficiently tailor GLMs to an unseen graph and task with only a few labeled\nexamples, in exchange for better prediction accuracy and faster inference\nspeed. For implementation, in this paper we propose GraphLAMA method, with its\nmodel backbone and learning schemes specialized for efficient tuning and\ninference. Specifically, for model backbone, we use a graph neural network\n(GNN) with several well-designed components to transform nodes into the\nrepresentation space of LLM tokens. Task instructions can then be represented\nas a mixture of node and language tokens. In the pre-training stage, model\nparameters except the LLM will be trained with different tasks to capture\ngeneral knowledge. In the adaptation stage, only a few pre-trained parameters\nwill be updated based on few-shot examples. Extensive experiments on\nfew/zero-shot node classification and summary generation show that our proposed\nGraphLAMA achieves state-of-the-art performance with 4.91% absolution\nimprovement in accuracy. Compared with ICL, our inference speed can be 10 times\nfaster under 5-shot setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21559v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21559v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21599", "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barthélemy", "Tomasz Bednarz", "Flora D. Salim"], "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21599v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21599v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21932", "title": "StructMG: A Fast and Scalable Structured Algebraic Multigrid", "authors": ["Yi Zong", "Peinan Yu", "Haopeng Huang", "Zhengding Hu", "Xinliang Wang", "Qin Wang", "Chensong Zhang", "Xiaowen Xu", "Jian Sun", "Yongxiao Zhou", "Wei Xue"], "summary": "Parallel multigrid is widely used as preconditioners in solving large-scale\nsparse linear systems. However, the current multigrid library still needs more\nsatisfactory performance for structured grid problems regarding speed and\nscalability. Based on the classical 'multigrid seesaw', we derive three\nnecessary principles for an efficient structured multigrid, which instructs our\ndesign and implementation of StructMG, a fast and scalable algebraic multigrid\nthat constructs hierarchical grids automatically. As a preconditioner, StructMG\ncan achieve both low cost per iteration and good convergence when solving\nlarge-scale linear systems with iterative methods in parallel. A stencil-based\ntriple-matrix product via symbolic derivation and code generation is proposed\nfor multi-dimensional Galerkin coarsening to reduce grid complexity, operator\ncomplexity, and implementation effort. A unified parallel framework of sparse\ntriangular solver is presented to achieve fast convergence and high parallel\nefficiency for smoothers, including dependence-preserving Gauss-Seidel and\nincomplete LU methods. Idealized and real-world problems from radiation\nhydrodynamics, petroleum reservoir simulation, numerical weather prediction,\nand solid mechanics, are evaluated on ARM and X86 platforms to show StructMG's\neffectiveness. In comparison to \\textit{hypre}'s structured and general\nmultigrid preconditioners, StructMG achieves the fastest time-to-solutions in\nall cases with average speedups of 15.5x, 5.5x, 6.7x, 7.3x over SMG, PFMG,\nSysPFMG, and BoomerAMG, respectively. StructMG also significantly improves\nstrong and weak scaling efficiencies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21932v1", "categories": ["math.NA", "cs.CE", "cs.NA", "cs.PF"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21932v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21718", "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "pdf_url": "http://arxiv.org/pdf/2506.21718v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21718v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21946", "title": "Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling", "authors": ["Till Wenke"], "summary": "Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded\nsystematic study due to its informal nature. This paper presents and analyzes\nthe largest known structured dataset of hitchhiking rides, comprising over\n63,000 entries collected over nearly two decades through platforms associated\nwith hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced\ncontributions, the dataset captures key spatiotemporal and strategic aspects of\nhitchhiking. This work documents the dataset's origins, evolution, and\ncommunity-driven maintenance, highlighting its Europe-centric distribution,\nseasonal patterns, and reliance on a small number of highly active\ncontributors. Through exploratory analyses, I examine waiting times, user\nbehavior, and comment metadata, shedding light on the lived realities of\nhitchhikers. While the dataset has inherent biases and limitations - such as\ndemographic skew and unverifiable entries it offers a rare and valuable window\ninto an alternative form of mobility. I conclude by outlining future directions\nfor enriching the dataset and advancing research on hitchhiking as both a\ntransportation practice and cultural phenomenon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21946v1", "categories": ["cs.CY", "cs.LG"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21946v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22344", "title": "Nets-within-Nets through the Lens of Data Nets", "authors": ["Francesco Di Cosmo", "Soumodev Mal", "Tephilla Prince"], "summary": "Elementary Object Systems (EOSs) are a model in the nets-within-nets (NWNs)\nparadigm, where tokens in turn can host standard Petri nets. We study the\ncomplexity of the reachability problem of EOSs when subjected to\nnon-deterministic token losses. It is known that this problem is equivalent to\nthe coverability problem with no lossiness of conservative EOSs (cEOSs). We\nprecisely characterize cEOS coverability into the framework of data nets, whose\ntokens carry data from an infinite domain. Specifically, we show that cEOS\ncoverability is equivalent to the coverability of an interesting fragment of\ndata nets that extends beyond $\\nu$PNs (featuring globally fresh name\ncreation), yet remains less expressive than Unordered Data Nets (featuring\nlossy name creation as well as powerful forms of whole-place operations and\nbroadcasts). This insight bridges two apparently orthogonal approaches to PN\nextensions, namely data nets and NWNs. At the same time, it enables us to\nanalyze cEOS coverability taking advantage of known results on data nets. As a\nbyproduct, we immediately get that the complexity of cEOS coverability lies\nbetween $\\mathbf{F}_{\\omega 2}$ and $\\mathbf{F}_{\\omega^\\omega}$, two classes\nbeyond Primitive Recursive.", "comment": "34 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.22344v1", "categories": ["cs.CC", "cs.FL", "cs.LO"], "cate": "cs.CC", "url": "http://arxiv.org/abs/2506.22344v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22311", "title": "Reconstructing Intelligible Speech from the Pressure Sensor Data in HVACs", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Anomadarshi Barua"], "summary": "Pressure sensors are an integrated component of modern Heating, Ventilation,\nand Air Conditioning (HVAC) systems. As these pressure sensors operate within\nthe 0-10 Pa range, support high sampling frequencies of 0.5-2 kHz, and are\noften placed close to human proximity, they can be used to eavesdrop on\nconfidential conversation, since human speech has a similar audible range of\n0-10 Pa and a bandwidth of 4 kHz for intelligible quality. This paper presents\nWaLi, which reconstructs intelligible speech from the low-resolution and noisy\npressure sensor data by providing the following technical contributions: (i)\nWaLi reconstructs intelligible speech from a minimum of 0.5 kHz sampling\nfrequency of pressure sensors, whereas previous work can only detect hot\nwords/phrases. WaLi uses complex-valued conformer and Complex Global Attention\nBlock (CGAB) to capture inter-phoneme and intra-phoneme dependencies that exist\nin the low-resolution pressure sensor data. (ii) WaLi handles the transient\nnoise injected from HVAC fans and duct vibrations, by reconstructing both the\nclean magnitude and phase of the missing frequencies of the low-frequency\naliased components. Extensive measurement studies on real-world pressure\nsensors show an LSD of 1.24 and NISQA-MOS of 1.78 for 0.5 kHz to 8 kHz\nupsampling. We believe that such levels of accuracy pose a significant threat\nwhen viewed from a privacy perspective that has not been addressed before for\npressure sensors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22311v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22311v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21842", "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses", "authors": ["Archisman Ghosh", "Satwik Kundu", "Swaroop Ghosh"], "summary": "Quantum Machine Learning (QML) integrates quantum computing with classical\nmachine learning, primarily to solve classification, regression and generative\ntasks. However, its rapid development raises critical security challenges in\nthe Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines\nadversarial threats unique to QML systems, focusing on vulnerabilities in\ncloud-based deployments, hybrid architectures, and quantum generative models.\nKey attack vectors include model stealing via transpilation or output\nextraction, data poisoning through quantum-specific perturbations, reverse\nengineering of proprietary variational quantum circuits, and backdoor attacks.\nAdversaries exploit noise-prone quantum hardware and insufficiently secured\nQML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,\nand functionality. Defense mechanisms leverage quantum properties to counter\nthese threats. Noise signatures from training hardware act as non-invasive\nwatermarks, while hardware-aware obfuscation techniques and ensemble strategies\ndisrupt cloning attempts. Emerging solutions also adapt classical adversarial\ntraining and differential privacy to quantum settings, addressing\nvulnerabilities in quantum neural networks and generative architectures.\nHowever, securing QML requires addressing open challenges such as balancing\nnoise levels for reliability and security, mitigating cross-platform attacks,\nand developing quantum-classical trust frameworks. This chapter summarizes\nrecent advances in attacks and defenses, offering a roadmap for researchers and\npractitioners to build robust, trustworthy QML systems resilient to evolving\nadversarial landscapes.", "comment": "23 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21842v1", "categories": ["quant-ph", "cs.CR", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21842v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21689", "title": "Optimal Motion Scaling for Delayed Telesurgery", "authors": ["Jason Lim", "Florian Richter", "Zih-Yun Chiu", "Jaeyon Lee", "Ethan Quist", "Nathan Fisher", "Jonathan Chambers", "Steven Hong", "Michael C. Yip"], "summary": "Robotic teleoperation over long communication distances poses challenges due\nto delays in commands and feedback from network latency. One simple yet\neffective strategy to reduce errors and increase performance under delay is to\ndownscale the relative motion between the operating surgeon and the robot. The\nquestion remains as to what is the optimal scaling factor, and how this value\nchanges depending on the level of latency as well as operator tendencies. We\npresent user studies investigating the relationship between latency, scaling\nfactor, and performance. The results of our studies demonstrate a statistically\nsignificant difference in performance between users and across scaling factors\nfor certain levels of delay. These findings indicate that the optimal scaling\nfactor for a given level of delay is specific to each user, motivating the need\nfor personalized models for optimal performance. We present techniques to model\nthe user-specific mapping of latency level to scaling factor for optimal\nperformance, leading to an efficient and effective solution to optimizing\nperformance of robotic teleoperation and specifically telesurgery under large\ncommunication delay.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21689v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21689v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22185", "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22185v1", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22185v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21898", "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models", "authors": ["Aimen Gaba", "Emily Wall", "Tejas Ramkumar Babu", "Yuriy Brun", "Kyle Hall", "Cindy Xiong Bearfield"], "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21898v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21898v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22108", "title": "The relationship between episcopal genealogy and ideology in the Roman Catholic Church", "authors": ["Marta Baratto", "Ivan Casanovas", "Ivan Decostanzi", "Henrique M. Borges", "Samuel Martínez Alcalá", "Ilaria Stanzani", "Alberto Antonioni", "Iacopo Iacopini", "Michele Re Fiorentin", "Eugenio Valdano"], "summary": "In this study we investigate how hierarchical structures within the Roman\nCatholic Church shape the ideological orientation of its leadership. The full\nepiscopal genealogy dataset comprises over 35,000 bishops, each typically\nconsecrated by one principal consecrator and two co-consecrators, forming a\ndense and historically continuous directed network of episcopal lineage. Within\nthis broader structure, we focus on a dataset of 245 living cardinals to\nexamine whether genealogical proximity correlates with doctrinal alignment on a\nbroad set of theological and sociopolitical issues. We identify motifs that\ncapture recurring patterns of lineage, such as shared consecrators or\nco-consecrators. In parallel, we apply natural language processing techniques\nto extract each cardinal's publicly stated positions on ten salient topics,\nincluding LGBTQIA+ rights, women's roles in the Church, liturgy, bioethics,\npriestly celibacy, and migration. Our results show that cardinals linked by\nspecific genealogical motifs, particularly those who share the same principal\nconsecrator, are significantly more likely to exhibit ideological similarity.\nWe find that the influence of pope John Paul II persists through the bishops he\nconsecrated, who demonstrate systematically more conservative views than their\npeers. These findings underscore the role of hierarchical mentorship in shaping\nideological coherence within large-scale religious institutions. Our\ncontribution offers quantitative evidence that institutional lineages, beyond\nindividual background factors, may have an impact on the transmission and\nconsolidation of doctrinal positions over time.", "comment": "This work is the output of the Complexity72h workshop, held at the\n  Universidad Carlos III de Madrid in Legan\\'es, Spain, 23-27 June 2025,\n  https://www.complexity72h.com", "pdf_url": "http://arxiv.org/pdf/2506.22108v1", "categories": ["physics.soc-ph", "cs.SI"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.22108v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22052", "title": "Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles", "authors": ["Nico Ostendorf", "Keno Garlichs", "Lars Wolf"], "summary": "V2X communication has become crucial for enhancing road safety, especially\nfor Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the\nincreasing number of devices communicating on the same channels will lead to\nsignificant channel load. To address this issue this study evaluates the\neffectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),\nfocusing specifically on cyclists. The objective of RM is to minimize the\ntransmission of redundant information. We conducted a simulation study using a\nurban scenario with a high bicycle density based on traffic data from Hannover,\nGermany. This study assessed the impact of RM on channel load, measured by\nChannel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in\nsimulation. To evaluate the accuracy and reliability of the RM mechanisms, we\nanalyzed the actual differences in position, speed, and heading between the ego\nVRU and the VRU, which was assumed to be redundant. Our findings indicate that\nwhile RM can reduce channel congestion, it also leads to a decrease in VPR. The\nanalysis of actual differences revealed that the RM mechanism standardized by\nETSI often uses outdated information, leading to significant discrepancies in\nposition, speed, and heading, which could result in dangerous situations. To\naddress these limitations, we propose an adapted RM mechanism that improves the\nbalance between reducing channel load and maintaining VRU awareness. The\nadapted approach shows a significant reduction in maximum CBR and a less\nsignificant decrease in VPR compared to the standardized RM. Moreover, it\ndemonstrates better performance in the actual differences in position, speed,\nand heading, thereby enhancing overall safety. Our results highlight the need\nfor further research to optimize RM techniques and ensure they effectively\nenhance V2X communication without compromising the safety of VRUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22052v1", "categories": ["cs.ET", "cs.NI", "eess.SP"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.22052v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21664", "title": "When Every Symbol Counts: Resilient Wireless Systems Under Finite Blocklength Constraints", "authors": ["Kevin Weinberger", "Aydin Sezgin"], "summary": "As 6G evolves, wireless networks become essential for critical operations and\nenable innovative applications that demand seamless adaptation to dynamic\nenvironments and disruptions. Because these vital services require\nuninterrupted operation, their resilience to unforeseen disruptions is\nessential. However, implementing resilience necessitates rapid recovery\nprocedures, which operate in the finite blocklength (FBL) regime, where short\npackets and added error-correction overhead can severely degrade communication\nefficiency. Due to this performance loss, always attempting recovery can\nbackfire and result in worse outcomes than simply enduring the disruption under\nlonger blocklengths. In this work, we study these effects of FBL constraints\nwithin a resilience framework, incorporating reconfigurable intelligent\nsurfaces (RIS) to enhance adaptation capabilities. By actively shaping the\nwireless environment, RIS help counteract some of the performance losses caused\nby FBL, enabling more effective recovery from disruptions. Numerical results\nreveal two critical blocklength thresholds: the first enables full recovery\nfrom the FBL penalty, while the second, at a higher blocklength, allows the\nsystem to recover from both the FBL penalty and the initial disruption,\nyielding a significant improvement in resilience performance. Additionally, we\nshow that the number of RIS elements shifts these thresholds, enabling faster\nreconfiguration with shorter blocklengths and providing insights to the\ntrade-offs between rate, blocklength, and reconfiguration effort under FBL\nconditions.", "comment": "6 pages, 3 figures, submitted to European Wireless 2025. arXiv admin\n  note: text overlap with arXiv:2504.11589", "pdf_url": "http://arxiv.org/pdf/2506.21664v1", "categories": ["eess.SP", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21664v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21724", "title": "Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning", "authors": ["Remco F. Leijenaar", "Hamidreza Kasaei"], "summary": "Learning semantically meaningful representations from unstructured 3D point\nclouds remains a central challenge in computer vision, especially in the\nabsence of large-scale labeled datasets. While masked point modeling (MPM) is\nwidely used in self-supervised 3D learning, its reconstruction-based objective\ncan limit its ability to capture high-level semantics. We propose AsymDSD, an\nAsymmetric Dual Self-Distillation framework that unifies masked modeling and\ninvariance learning through prediction in the latent space rather than the\ninput space. AsymDSD builds on a joint embedding architecture and introduces\nseveral key design choices: an efficient asymmetric setup, disabling attention\nbetween masked queries to prevent shape leakage, multi-mask sampling, and a\npoint cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results\non ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k\nshapes, surpassing prior methods.", "comment": "for associated source code, see\n  https://github.com/RFLeijenaar/AsymDSD", "pdf_url": "http://arxiv.org/pdf/2506.21724v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21724v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22267", "title": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need", "authors": ["Junaid Ahmed Khan", "Hiari Pizzini Cavagna", "Andrea Proia", "Andrea Bartolini"], "summary": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2506.22267v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22267v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21804", "title": "Large-Scale Simulations of Turbulent Flows using Lattice Boltzmann Methods on Heterogeneous High Performance Computers", "authors": ["Adrian Kummerländer", "Fedor Bukreev", "Yuji Shimojima", "Shota Ito", "Mathias J. Krause"], "summary": "Current GPU-accelerated supercomputers promise to enable large-scale\nsimulations of turbulent flows. Lattice Boltzmann Methods (LBM) are\nparticularly well-suited to fulfilling this promise due to their intrinsic\ncompatibility with highly parallel execution on both SIMD CPUs and GPUs. A\nnovel LBM scheme for wall-modeled LES in complex geometries is described with a\nspecial focus on the efficient implementation in the open source LBM framework\nOpenLB. Detailed scalability results are provided for all HoreKa partitions,\nutilizing up to 128 nodes and covering problem sizes up to 18 billion cells.", "comment": "Annual report of LBRG's usage of the HoreKa supercomputer within the\n  scope of the NHR JARDS project CPE. Submitted to the HLRS Results and Review\n  Workshop 2025", "pdf_url": "http://arxiv.org/pdf/2506.21804v1", "categories": ["physics.comp-ph", "cs.CE", "cs.MS"], "cate": "physics.comp-ph", "url": "http://arxiv.org/abs/2506.21804v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21887", "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds", "authors": ["Edward Chen", "Sang T. Truong", "Natalie Dullerud", "Sanmi Koyejo", "Carlos Guestrin"], "summary": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21887v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21887v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22201", "title": "A Matlab-based Toolbox for Automatic EMT Modeling and Small-Signal Stability Analysis of Modern Power Systems", "authors": ["Josep Arevalo-Soler", "Dionysios Moutevelis", "Elia Mateu-Barriendos", "Onur Alican", "Carlos Collados-Rodriguez", "Marc Cheah-Mañe", "Eduardo Prieto-Araujo", "Oriol Gomis-Bellmunt"], "summary": "The intensive integration of power converters is changing the way that power\nsystems operate, leading to the emergence of new types of dynamic phenomena and\ninstabilities. At the same time, converters act as an interface between\ntraditional AC grids and their more recent DC counterparts, giving rise to\nhybrid AC/DC networks. These conditions increase the necessity for stability\nanalysis tools that can simultaneously account for the newly-introduced dynamic\nphenomena and can also be applied for the stability study of hybrid networks.\nThis paper presents a Matlab-based toolbox for small-signal analysis of hybrid\nAC/DC power systems considering electromagnetic-transient (EMT) models. The\ntoolbox allows the automatized modeling of the system from the input data and\noffers options for modal, impedance and passivity analyses. In the paper, the\nstructure and internal processes of the toolbox are duly discussed, together\nwith all its features, both main and complementary. Its capabilities for\nstability analysis are demonstrated via comprehensive case studies of\nconverter-based system of various size and topology.", "comment": "12 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.22201v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22201v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21803", "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21803v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21803v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22012", "title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction", "authors": ["Qi Gao", "Zhihao Chen", "Dong Zeng", "Junping Zhang", "Jianhua Ma", "Hongming Shan"], "summary": "The generalization of deep learning-based low-dose computed tomography (CT)\nreconstruction models to doses unseen in the training data is important and\nremains challenging. Previous efforts heavily rely on paired data to improve\nthe generalization performance and robustness through collecting either diverse\nCT data for re-training or a few test data for fine-tuning. Recently, diffusion\nmodels have shown promising and generalizable performance in low-dose CT (LDCT)\nreconstruction, however, they may produce unrealistic structures due to the CT\nimage noise deviating from Gaussian distribution and imprecise prior\ninformation from the guidance of noisy LDCT images. In this paper, we propose a\nnoise-inspired diffusion model for generalizable LDCT reconstruction, termed\nNEED, which tailors diffusion models for noise characteristics of each domain.\nFirst, we propose a novel shifted Poisson diffusion model to denoise projection\ndata, which aligns the diffusion process with the noise model in pre-log LDCT\nprojections. Second, we devise a doubly guided diffusion model to refine\nreconstructed images, which leverages LDCT images and initial reconstructions\nto more accurately locate prior information and enhance reconstruction\nfidelity. By cascading these two diffusion models for dual-domain\nreconstruction, our NEED requires only normal-dose data for training and can be\neffectively extended to various unseen dose levels during testing via a time\nstep matching strategy. Extensive qualitative, quantitative, and\nsegmentation-based evaluations on two datasets demonstrate that our NEED\nconsistently outperforms state-of-the-art methods in reconstruction and\ngeneralization performance. Source code is made available at\nhttps://github.com/qgao21/NEED.", "comment": "Accepted for publication in Medical Image Analysis, 2025", "pdf_url": "http://arxiv.org/pdf/2506.22012v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22012v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21926", "title": "Computing Maximum Cliques in Unit Disk Graphs", "authors": ["Anastasiia Tkachenko", "Haitao Wang"], "summary": "Given a set $P$ of $n$ points in the plane, the unit-disk graph $G(P)$ is a\ngraph with $P$ as its vertex set such that two points of $P$ have an edge if\ntheir Euclidean distance is at most $1$. We consider the problem of computing a\nmaximum clique in $G(P)$. The previously best algorithm for the problem runs in\n$O(n^{7/3+o(1)})$ time. We show that the problem can be solved in $O(n \\log n +\nn K^{4/3+o(1)})$ time, where $K$ is the maximum clique size. The algorithm is\nfaster than the previous one when $K=o(n)$. In addition, if $P$ is in convex\nposition, we give a randomized algorithm that runs in $O(n^{15/7+o(1)})=\nO(n^{2.143})$ worst-case time and the algorithm can compute a maximum clique\nwith high probability. For points in convex position, one special case we solve\nis when a point in the maximum clique is given; we present an $O(n^2\\log n)$\ntime (deterministic) algorithm for this special case.", "comment": "To appear in CCCG 2025", "pdf_url": "http://arxiv.org/pdf/2506.21926v1", "categories": ["cs.CG", "cs.DS"], "cate": "cs.CG", "url": "http://arxiv.org/abs/2506.21926v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2212.09525", "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "comment": "AAAI 2023", "pdf_url": "http://arxiv.org/pdf/2212.09525v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2212.09525v1", "date": "2022-12-19", "updated": "2022-12-19"}
{"id": "2506.21576", "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21576v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21576v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21560", "title": "Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning", "authors": ["Yifu Han", "Geo Zhang"], "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21560v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21560v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21601", "title": "Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization", "authors": ["Duong Bach"], "summary": "Multi-vector document retrieval systems, such as ColPali, excel in\nfine-grained matching for complex queries but incur significant storage and\ncomputational costs due to their reliance on high-dimensional patch embeddings\nand late-interaction scoring. To address these challenges, we propose\nHPC-ColPali, a Hierarchical Patch Compression framework that enhances the\nefficiency of ColPali while preserving its retrieval accuracy. Our approach\nintegrates three innovative techniques: (1) K-Means quantization, which\ncompresses patch embeddings into 1-byte centroid indices, achieving up to\n32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing\nVision-Language Model attention weights to retain only the top-$p\\%$ most\nsalient patches, reducing late-interaction computation by up to 60\\% with less\nthan 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices\ninto $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming\ndistance-based similarity search for resource-constrained environments.\nEvaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\%\nlower query latency under HNSW indexing while maintaining high retrieval\nprecision. When integrated into a Retrieval-Augmented Generation pipeline for\nlegal summarization, it reduces hallucination rates by 30\\% and halves\nend-to-end latency. These advancements establish HPC-ColPali as a scalable and\nefficient solution for multi-vector document retrieval across diverse\napplications. Code is available at https://github.com/DngBack/HPC-ColPali.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2506.21601v1", "categories": ["cs.IR", "cs.CV"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21601v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21969", "title": "Computing rough solutions of the KdV equation below ${\\bf L^2}$", "authors": ["Jiachuan Cao", "Buyang Li", "Yifei Wu", "Fangyan Yao"], "summary": "We establish a novel numerical and analytical framework for solving the\nKorteweg--de Vries (KdV) equation in the negative Sobolev spaces, where\nclassical numerical methods fail due to their reliance on high regularity and\ninability to control nonlinear interactions at low regularities. Numerical\nanalysis is established by combining a continuous reformulation of the\nnumerical scheme, the Bourgain-space estimates for the continuous\nreformulation, and a rescaling strategy that reduces the reformulated problem\nto a small initial value problem, which allow us to bridge a critical gap\nbetween numerical analysis and theoretical well-posedness by designing the\nfirst numerical method capable of solving the KdV equation in the negative\nSobolev spaces. The numerical scheme is proved to have nearly optimal-order\nconvergence with respect to the spatial degrees of freedom in the\n$H^{-\\frac{1}{2}}$ norm for initial data in $H^s$, with $-\\frac{1}{2} < s \\leq\n0$, a result unattainable by existing numerical methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21969v1", "categories": ["math.NA", "cs.NA", "65M12, 65M15, 65M70, 35Q53"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21969v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21744", "title": "Federated Item Response Theory Models", "authors": ["Biying Zhou", "Nanyu Luo", "Feng Ji"], "summary": "Item Response Theory (IRT) models have been widely used to estimate\nrespondents' latent abilities and calibrate items' difficulty. Traditional IRT\nestimation requires all individual raw response data to be centralized in one\nplace, thus potentially causing privacy issues. Federated learning is an\nemerging field in computer science and machine learning with added features of\nprivacy protection and distributed computing. To integrate the advances from\nfederated learning with modern psychometrics, we propose a novel framework,\nFederated Item Response Theory (IRT), to enable estimating traditional IRT\nmodels with additional privacy, allowing estimation in a distributed manner\nwithout losing estimation accuracy.\n  Our numerical experiments confirm that FedIRT achieves statistical accuracy\nsimilar to standard IRT estimation using popular R packages, while offering\ncritical advantages: privacy protection and reduced communication costs. We\nalso validate FedIRT's utility through a real-world exam dataset, demonstrating\nits effectiveness in realistic educational contexts. This new framework extends\nIRT's applicability to distributed settings, such as multi-school assessments,\nwithout sacrificing accuracy or security. To support practical adoption, we\nprovide an open-ource R package, FedIRT, implementing the framework for the\ntwo-parameter logistic (2PL) and partial credit models (PCM).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21744v1", "categories": ["cs.LG", "stat.AP", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21744v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22270", "title": "Public Service Algorithm: towards a transparent, explainable, and scalable content curation for news content based on editorial values", "authors": ["Ahmad Mel", "Sebastien Noir"], "summary": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22270v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22270v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22321", "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22321v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22321v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21972", "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21972v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21972v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21732", "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "authors": ["Ameya Salvi", "Venkat Krovi"], "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21732v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21732v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22370", "title": "Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny", "authors": ["Carolina Carreira", "Álvaro Silva", "Alexandre Abreu", "Alexandra Mendes"], "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22370v1", "categories": ["cs.SE", "cs.PL"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22370v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21962", "title": "AnyAni: An Interactive System with Generative AI for Animation Effect Creation and Code Understanding in Web Development", "authors": ["Tianrun Qiu", "Yuxin Ma"], "summary": "Generative AI assistants have been widely used in front-end programming.\nHowever, besides code writing, developers often encounter the need to generate\nanimation effects. As novices in creative design without the assistance of\nprofessional designers, developers typically face difficulties in describing,\ndesigning, and implementing desired animations. To address this issue, we\nconducted a formative study (N=6) to identify the challenges that code\ndevelopers face when dealing with animation design issues. Then, we introduce\nAnyAni, a human-AI collaborative system that supports front-end developers in\nthe ideation, manipulation, and implementation of animation effects. The system\ncombines the assistance of generative AI in creative design by adopting a\nnonlinear workflow for iterative animation development. In addition, developers\ncan understand and learn the code generated for implementing animations through\nvarious interactive methods. A user study (N=9) demonstrated the usability of\nAnyAni in animation effect creation support for developers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21962v1", "categories": ["cs.HC", "J.6"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21962v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22119", "title": "Harder, shorter, sharper, forward: A comparison of women's and men's elite football gameplay (2020-2025)", "authors": ["Rebecca Carstens", "Raj Deshpande", "Pau Esteve", "Nicolò Fidelibus", "Sara Linde Neven", "Ramona Ottow", "Lokamruth K. R.", "Paula Rodríguez-Sánchez", "Luca Santagata", "Javier M. Buldú", "Brennan Klein", "Maddalena Torricelli"], "summary": "Elite football is believed to have evolved in recent years, but systematic\nevidence for the pace and form of that change is sparse. Drawing on event-level\nrecords for 13,067 matches in ten top-tier men's and women's leagues in\nEngland, Spain, Germany, Italy, and the United States (2020-2025), we quantify\nmatch dynamics with two views: conventional performance statistics and\npitch-passing networks that track ball movement among a grid of pitch (field)\nregions. Between 2020 and 2025, average passing volume, pass accuracy, and the\npercent of passes made under pressure all rose. In general, the largest\nyear-on-year changes occurred in women's competitions. Network measures offer\nalternative but complementary perspectives on the changing gameplay in recent\nyears, normalized outreach in the pitch passing networks decreased, while the\naverage shortest path lengths increased, indicating a wider ball circulation.\nTogether, these indicators point to a sustained intensification of collective\nplay across contemporary professional football.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22119v1", "categories": ["physics.soc-ph", "cs.SI"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.22119v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22185", "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22185v1", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22185v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21936", "title": "Some more constructions of $n-$cycle permutation polynomials", "authors": ["Varsha Jarali", "Prasanna Poojary", "Vadiraja Bhatta G. R"], "summary": "$ n-$cycle permutation polynomials with small n have the advantage that their\ncompositional inverses are efficient in terms of implementation. These\npermutation polynomials have significant applications in cryptography and\ncoding theory. In this article, we propose criteria for the construction of $\nn-$cycle permutation using linearized polynomial $ L(x) $ for larger $ n $.\nFurthermore, we investigate and generalize certain novel forms of $ n-$cycle\npermutation polynomials. Finally, we demonstrate our approach by constructing\nexplicit $ n-$cycle permutation of the form $ L(x)+\\gamma h(Tr_{q^{m}/q}(x)) $,\nand $ G(x)+\\gamma f(x) $ with a Boolean function $ f(x) $. The polynomial $\nx^{d}+\\gamma f(x) $ with $ f(x) $ being a Boolean function is shown to be\nquadruple and quintuple permutation polynomials. Moreover, linear binomial\ntriple-cycle permutation polynomials are constructed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21936v1", "categories": ["math.RA", "cs.IT", "math.CO", "math.IT", "2010: 05A05 11T06"], "cate": "math.RA", "url": "http://arxiv.org/abs/2506.21936v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21731", "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis", "authors": ["Chenqiu Zhao", "Anup Basu"], "summary": "We propose two theoretical frameworks, the Mutually Exclusive Probability\nSpace (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential\nlimitation in probabilistic generative models; namely that learning global\ndistributions leads to memorization rather than generative behavior. MESP\nemerges from our rethinking of the Variational Autoencoder (VAE). We observe\nthat latent variable distributions in VAE exhibit overlap, which leads to an\noptimization conflict between the reconstruction loss and KL-divergence loss. A\nlower bound based on the overlap coefficient is proposed. We refer to this\nphenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary\nLatent Autoencoder (BL-AE) is proposed to encode images into binary latent\nrepresentations. These binary latents are used as the input to our\nAutoregressive Random Variable Model (ARVM), a modified autoregressive model\noutputting histograms. Our ARVM achieves competitive FID scores, outperforming\nstate-of-the-art methods on standard datasets. However, such scores reflect\nmemorization rather than generation. To address this issue, we propose the\nLocal Correlation Hypothesis (LCH), which posits that generative capability\narising from local correlations among latent variables. Comprehensive\nexperiments and discussions are conducted to validate our frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21731v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21731v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22180", "title": "Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study", "authors": ["Önder Gürcan"], "summary": "The industrial market continuously needs reliable solutions to secure\nautonomous systems. Especially as these systems become more complex and\ninterconnected, reliable security solutions are becoming increasingly\nimportant. One promising solution to tackle this challenge is using smart\ncontracts designed to meet contractual conditions, avoid malicious errors,\nsecure exchanges, and minimize the need for reliable intermediaries. However,\nsmart contracts are immutable. Moreover, there are different smart contract\nexecution architectures (namely Order-Execute and Execute-Order-Validate) that\nhave different throughputs. In this study, we developed an evaluation model for\nassessing the security of reliable smart contract execution. We then developed\na realistic smart contract enabled IoT energy case study. Finally, we simulate\nthe developed case study to evaluate several smart contract security\nvulnerabilities reported in the literature. Our results show that the\nExecute-Order-Validate architecture is more promising regarding reliability and\nsecurity.", "comment": "23 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.22180v1", "categories": ["cs.CR", "cs.DC"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22180v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21932", "title": "StructMG: A Fast and Scalable Structured Algebraic Multigrid", "authors": ["Yi Zong", "Peinan Yu", "Haopeng Huang", "Zhengding Hu", "Xinliang Wang", "Qin Wang", "Chensong Zhang", "Xiaowen Xu", "Jian Sun", "Yongxiao Zhou", "Wei Xue"], "summary": "Parallel multigrid is widely used as preconditioners in solving large-scale\nsparse linear systems. However, the current multigrid library still needs more\nsatisfactory performance for structured grid problems regarding speed and\nscalability. Based on the classical 'multigrid seesaw', we derive three\nnecessary principles for an efficient structured multigrid, which instructs our\ndesign and implementation of StructMG, a fast and scalable algebraic multigrid\nthat constructs hierarchical grids automatically. As a preconditioner, StructMG\ncan achieve both low cost per iteration and good convergence when solving\nlarge-scale linear systems with iterative methods in parallel. A stencil-based\ntriple-matrix product via symbolic derivation and code generation is proposed\nfor multi-dimensional Galerkin coarsening to reduce grid complexity, operator\ncomplexity, and implementation effort. A unified parallel framework of sparse\ntriangular solver is presented to achieve fast convergence and high parallel\nefficiency for smoothers, including dependence-preserving Gauss-Seidel and\nincomplete LU methods. Idealized and real-world problems from radiation\nhydrodynamics, petroleum reservoir simulation, numerical weather prediction,\nand solid mechanics, are evaluated on ARM and X86 platforms to show StructMG's\neffectiveness. In comparison to \\textit{hypre}'s structured and general\nmultigrid preconditioners, StructMG achieves the fastest time-to-solutions in\nall cases with average speedups of 15.5x, 5.5x, 6.7x, 7.3x over SMG, PFMG,\nSysPFMG, and BoomerAMG, respectively. StructMG also significantly improves\nstrong and weak scaling efficiencies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21932v1", "categories": ["math.NA", "cs.CE", "cs.NA", "cs.PF"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21932v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21996", "title": "AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms", "authors": ["Raphaël Boige", "Amine Boumaza", "Bruno Scherrer"], "summary": "Deterministic game-solving algorithms are conventionally analyzed in the\nlight of their average-case complexity against a distribution of random\ngame-trees, where leaf values are independently sampled from a fixed\ndistribution. This simplified model enables uncluttered mathematical analysis,\nrevealing two key properties: root value distributions asymptotically collapse\nto a single fixed value for finite-valued trees, and all reasonable algorithms\nachieve global optimality. However, these findings are artifacts of the model's\ndesign-its long criticized independence assumption strips games of structural\ncomplexity, producing trivial instances where no algorithm faces meaningful\nchallenges. To address this limitation, we introduce a new probabilistic model\nthat incrementally constructs game-trees using a fixed level-wise conditional\ndistribution. By enforcing ancestor dependency, a critical structural feature\nof real-world games, our framework generates problems with adjustable\ndifficulty while retaining some form of analytical tractability. For several\nalgorithms, including AlphaBeta and Scout, we derive recursive formulas\ncharacterizing their average-case complexities under this model. These allow us\nto rigorously compare algorithms on deep game-trees, where Monte-Carlo\nsimulations are no longer feasible. While asymptotically, all algorithms seem\nto converge to identical branching factor (a result analogous to those of\nindependence-based models), deep finite trees reveal stark differences:\nAlphaBeta incurs a significantly larger constant multiplicative factor compared\nto algorithms like Scout, leading to a substantial practical slowdown. Our\nframework sheds new light on classical game-solving algorithms, offering\nrigorous evidence and analytical tools to advance the understanding of these\nmethods under a more realistic, challenging, and yet tractable model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21996v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21996v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22404", "title": "Data-Driven Intrusion Detection in Vehicles: Integrating Unscented Kalman Filter (UKF) with Machine Learning", "authors": ["Shuhao Bian", "Milad Farsi", "Nasser L. Azad", "Chris Hobbs"], "summary": "In the realm of Cyber-Physical System (CPS), accurately identifying attacks\nwithout detailed knowledge of the system's parameters remains a major\nchallenge. When it comes to Advanced Driver Assistance Systems (ADAS),\nidentifying the parameters of vehicle dynamics could be impractical or\nprohibitively costly. To tackle this challenge, we propose a novel framework\nfor attack detection in vehicles that effectively addresses the uncertainty in\ntheir dynamics. Our method integrates the widely used Unscented Kalman Filter\n(UKF), a well-known technique for nonlinear state estimation in dynamic\nsystems, with machine learning algorithms. This combination eliminates the\nrequirement for precise vehicle modeling in the detection process, enhancing\nthe system's adaptability and accuracy. To validate the efficacy and\npracticality of our proposed framework, we conducted extensive comparative\nsimulations by introducing Denial of Service (DoS) attacks on the vehicle\nsystems' sensors and actuators.", "comment": "Accepted in Proceedings of the 21st International Conference on\n  Informatics in Control, Automation and Robotics - Volume 1: ICINCO; ISBN\n  978-989-758-717-7, SciTePress, pages 714-723. DOI: 10.5220/0013063900003822", "pdf_url": "http://arxiv.org/pdf/2506.22404v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22404v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21893", "title": "Improving Convergence for Semi-Federated Learning: An Energy-Efficient Approach by Manipulating Over-the-Air Distortion", "authors": ["Jingheng Zheng", "Hui Tian", "Wanli Ni", "Yang Tian", "Ping Zhang"], "summary": "In this paper, we propose a hybrid learning framework that combines federated\nand split learning, termed semi-federated learning (SemiFL), in which\nover-the-air computation is utilized for gradient aggregation. A key idea is to\nstrategically adjust the learning rate by manipulating over-the-air distortion\nfor improving SemiFL's convergence. Specifically, we intentionally amplify\namplitude distortion to increase the learning rate in the non-stable region,\nthereby accelerating convergence and reducing communication energy consumption.\nIn the stable region, we suppress noise perturbation to maintain a small\nlearning rate for improving SemiFL's final convergence. Theoretical results\ndemonstrate the antagonistic effects of over-the-air distortion in different\nregions, under both independent and identically distributed (i.i.d.) and\nnon-i.i.d. data settings. Then, we formulate two energy consumption\nminimization problems, one for each region, which implements a two-region mean\nsquare error threshold configuration scheme. Accordingly, we propose two\nresource allocation algorithms with closed-form solutions. Simulation results\nshow that under different network and data distribution conditions,\nstrategically manipulating over-the-air distortion can efficiently adjust the\nlearning rate to improve SemiFL's convergence. Moreover, energy consumption can\nbe reduced by using the proposed algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21893v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21893v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22041", "title": "Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning", "authors": ["Julia Machnio", "Sebastian Nørgaard Llambias", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "summary": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions.", "comment": "2nd Sorbonne-Heidelberg Workshop on AI in medicine: Machine Learning\n  for multi-modal data", "pdf_url": "http://arxiv.org/pdf/2506.22041v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22041v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21811", "title": "Revisiting Graph Analytics Benchmark", "authors": ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Xue Li", "Wenyuan Yu", "Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"], "summary": "The rise of graph analytics platforms has led to the development of various\nbenchmarks for evaluating and comparing platform performance. However, existing\nbenchmarks often fall short of fully assessing performance due to limitations\nin core algorithm selection, data generation processes (and the corresponding\nsynthetic datasets), as well as the neglect of API usability evaluation. To\naddress these shortcomings, we propose a novel graph analytics benchmark.\nFirst, we select eight core algorithms by extensively reviewing both academic\nand industrial settings. Second, we design an efficient and flexible data\ngenerator and produce eight new synthetic datasets as the default datasets for\nour benchmark. Lastly, we introduce a multi-level large language model\n(LLM)-based framework for API usability evaluation-the first of its kind in\ngraph analytics benchmarks. We conduct comprehensive experimental evaluations\non existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and\nG-thinker). The experimental results demonstrate the superiority of our\nproposed benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21811v1", "categories": ["cs.DB", "cs.GR"], "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.21811v1", "date": "2025-03-04", "updated": "2025-03-04"}
{"id": "2506.21577", "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21577v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21577v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21561", "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", "authors": ["Emilio Barkett", "Olivia Long", "Madhavendra Thakur"], "summary": "Despite their widespread use in fact-checking, moderation, and high-stakes\ndecision-making, large language models (LLMs) remain poorly understood as\njudges of truth. This study presents the largest evaluation to date of LLMs'\nveracity detection capabilities and the first analysis of these capabilities in\nreasoning models. We had eight LLMs make 4,800 veracity judgments across\nseveral prompts, comparing reasoning and non-reasoning models. We find that\nrates of truth-bias, or the likelihood to believe a statement is true,\nregardless of whether it is actually true, are lower in reasoning models than\nin non-reasoning models, but still higher than human benchmarks. Most\nconcerning, we identify sycophantic tendencies in several advanced models\n(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an\nasymmetry in detection accuracy, performing well in truth accuracy but poorly\nin deception accuracy. This suggests that capability advances alone do not\nresolve fundamental veracity detection challenges in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21561v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21561v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21604", "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "pdf_url": "http://arxiv.org/pdf/2506.21604v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21604v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21999", "title": "Do locking-free finite element schemes lock for holey Reissner-Mindlin plates with mixed boundary conditions?", "authors": ["Mark Ainsworth", "Charles Parker"], "summary": "We revisit finite element discretizations of the Reissner-Mindlin plate in\nthe case of non-simply connected (holey) domains with mixed boundary\nconditions. Guided by the de Rham complex, we develop conditions under which\nschemes deliver locking-free, optimal rates of convergence. We naturally\nrecover the typical assumptions arising for clamped, simply supported plates.\nMore importantly, we also see new conditions arise naturally from the presence\nof holes in the domain or in the case of mixed boundary conditions. We show\nthat, fortunately, many of the existing popularly used schemes do, in fact,\nsatisfy all of the conditions, and thus are locking-free.", "comment": "58 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21999v1", "categories": ["math.NA", "cs.NA", "65N30, 65N12, 74K20, 74S10"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21999v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21771", "title": "Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks", "authors": ["John Wesley Hostetter", "Min Chi"], "summary": "Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function\napproximations that perform as well as conventional neural architectures, but\ntheir knowledge is expressed as linguistic IF-THEN rules. Despite these\nadvantages, their systematic design process remains a challenge. Existing work\nwill often sequentially build NFNs by inefficiently isolating parametric and\nstructural identification, leading to a premature commitment to brittle and\nsubpar architecture. We propose a novel application-independent approach called\ngradient-based neuroplastic adaptation for the concurrent optimization of NFNs'\nparameters and structure. By recognizing that NFNs' parameters and structure\nshould be optimized simultaneously as they are deeply conjoined, settings\npreviously unapproachable for NFNs are now accessible, such as the online\nreinforcement learning of NFNs for vision-based tasks. The effectiveness of\nconcurrently optimizing NFNs is empirically shown as it is trained by online\nreinforcement learning to proficiently play challenging scenarios from a\nvision-based video game called DOOM.", "comment": "45 pages", "pdf_url": "http://arxiv.org/pdf/2506.21771v1", "categories": ["cs.LG", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21771v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21580", "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21580v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21580v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21555", "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "comment": "Accepted in Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21555v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21555v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21988", "title": "Unifying communication paradigms in delegated quantum computing", "authors": ["Fabian Wiesner", "Jens Eisert", "Anna Pappa"], "summary": "Delegated quantum computing (DQC) allows clients with low quantum\ncapabilities to outsource computations to a server hosting a quantum computer.\nThis process is typically envisioned within the measurement-based quantum\ncomputing framework, as it naturally facilitates blindness of inputs and\ncomputation. Hence, the overall process of setting up and conducting the\ncomputation encompasses a sequence of three stages: preparing the qubits,\nentangling the qubits to obtain the resource state, and measuring the qubits to\nrun the computation. There are two primary approaches to distributing these\nstages between the client and the server that impose different constraints on\ncryptographic techniques and experimental implementations. In the\nprepare-and-send setting, the client prepares the qubits and sends them to the\nserver, while in the receive-and-measure setting, the client receives the\nqubits from the server and measures them. Although these settings have been\nextensively studied independently, their interrelation and whether\nsetting-dependent theoretical constraints are inevitable remain unclear. By\nimplementing the key components of most DQC protocols in the respective missing\nsetting, we provide a method to build prospective protocols in both settings\nsimultaneously and to translate existing protocols from one setting into the\nother.", "comment": "8+1 pages, 3 figures. This work supersedes arXiv:2206.07469", "pdf_url": "http://arxiv.org/pdf/2506.21988v1", "categories": ["quant-ph", "cs.CR"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21988v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21853", "title": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface", "authors": ["Dewei Wang", "Chenjia Ba", "Chenhui Li", "Jiyuan Shi", "Yan Ding", "Chi Zhang", "Bin Zhao"], "summary": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks.", "comment": "17pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21853v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21853v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22390", "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee"], "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22390v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22390v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22066", "title": "Building Trustworthy Cognitive Monitoring for Safety-Critical Human Tasks: A Phased Methodological Approach", "authors": ["Maciej Grzeszczuk", "Grzegorz Pochwatko", "Barbara Karpowicz", "Stanisław Knapiński", "Wiesław Kopeć"], "summary": "Operators performing high-stakes, safety-critical tasks - such as air traffic\ncontrollers, surgeons, or mission control personnel - must maintain exceptional\ncognitive performance under variable and often stressful conditions. This paper\npresents a phased methodological approach to building cognitive monitoring\nsystems for such environments. By integrating insights from human factors\nresearch, simulation-based training, sensor technologies, and fundamental\npsychological principles, the proposed framework supports real-time performance\nassessment with minimum intrusion. The approach begins with simplified\nsimulations and evolves towards operational contexts. Key challenges addressed\ninclude variability in workload, the effects of fatigue and stress, thus the\nneed for adaptive monitoring for early warning support mechanisms. The\nmethodology aims to improve situational awareness, reduce human error, and\nsupport decision-making without undermining operator autonomy. Ultimately, the\nwork contributes to the development of resilient and transparent systems in\ndomains where human performance is critical to safety.", "comment": "11 pages, 5 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.22066v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22066v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22136", "title": "Characterization Of Diseases In Temporal Comorbidity Networks", "authors": ["Yuri Gardinazzi", "Roger Gonzaléz March", "Suprabhath Kalahasti", "Andrea Montaño Ramirez", "Matteo Neri", "Cicely Nguyen", "Giovanni Palermo", "Erik Weis", "Katharina Ledebur", "Elma Dervić"], "summary": "Comorbidity networks, which capture disease-disease co-occurrence usually\nbased on electronic health records, reveal structured patterns in how diseases\ncluster and progress across individuals. However, how these networks evolve\nacross different age groups and how this evolution relates to properties like\ndisease prevalence and mortality remains understudied. To address these issues,\nwe used publicly available comorbidity networks extracted from a comprehensive\ndataset of 45 million Austrian hospital stays from 1997 to 2014, covering 8.9\nmillion patients. These networks grow and become denser with age. We identified\ngroups of diseases that exhibit similar patterns of structural centrality\nthroughout the lifespan, revealing three dominant age-related components with\npeaks in early childhood, midlife, and late life. To uncover the drivers of\nthis structural change, we examined the relationship between prevalence and\ndegree. This allowed us to identify conditions that were disproportionately\nconnected to other diseases. Using betweenness centrality in combination with\nmortality data, we further identified high-mortality bridging diseases. Several\ndiseases show high connectivity relative to their prevalence, such as iron\ndeficiency anemia (D50) in children, nicotine dependence (F17), and lipoprotein\nmetabolism disorders (E78) in adults. We also highlight structurally central\ndiseases with high mortality that emerge at different life stages, including\ncancers (C group), liver cirrhosis (K74), subarachnoid hemorrhage (I60), and\nchronic kidney disease (N18). These findings underscore the importance of\ntargeting age-specific, network-central conditions with high mortality for\nprevention and integrated care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22136v1", "categories": ["physics.soc-ph", "cs.SI", "physics.med-ph"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.22136v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22323", "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America", "authors": ["Alessio Di Santo"], "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22323v1", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.OS", "cs.PL"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22323v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22106", "title": "Pinsker's inequality for adapted total variation", "authors": ["Mathias Beiglböck", "Markus Zona"], "summary": "Pinsker's classical inequality asserts that the total variation $TV(\\mu,\n\\nu)$ between two probability measures is bounded by $\\sqrt{ 2H(\\mu|\\nu)}$\nwhere $H$ denotes the relative entropy (or Kullback-Leibler divergence).\nConsidering the discrete metric, $TV$ can be seen as a Wasserstein distance and\nas such possesses an adapted variant $ATV$. Adapted Wasserstein distances have\ndistinct advantages over their classical counterparts when $\\mu, \\nu$ are the\nlaws of stochastic processes $(X_k)_{k=1}^n, (Y_k)_{k=1}^n$ and exhibit\nnumerous applications from stochastic control to machine learning. In this note\nwe observe that the adapted total variation distance $ATV$ satisfies the\nPinsker-type inequality $$ ATV(\\mu, \\nu)\\leq \\sqrt{n} \\sqrt{2 H(\\mu|\\nu)}.$$", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22106v1", "categories": ["math.PR", "cs.IT", "math.IT"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.22106v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21735", "title": "Equitable Federated Learning with NCA", "authors": ["Nick Lemke", "Mirko Konstantin", "Henry John Krumb", "John Kalkhof", "Jonathan Stieber", "Anirban Mukhopadhyay"], "summary": "Federated Learning (FL) is enabling collaborative model training across\ninstitutions without sharing sensitive patient data. This approach is\nparticularly valuable in low- and middle-income countries (LMICs), where access\nto trained medical professionals is limited. However, FL adoption in LMICs\nfaces significant barriers, including limited high-performance computing\nresources and unreliable internet connectivity. To address these challenges, we\nintroduce FedNCA, a novel FL system tailored for medical image segmentation\ntasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training\non low-cost edge devices, such as widely available smartphones, while\nminimizing communication costs. Additionally, our encryption-ready FedNCA\nproves to be suitable for compromised network communication. By overcoming\ninfrastructural and security challenges, FedNCA paves the way for inclusive,\nefficient, lightweight, and encryption-ready medical imaging solutions,\nfostering equitable healthcare advancements in resource-constrained regions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21735v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21735v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22185", "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22185v1", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22185v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22005", "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving", "authors": ["Naoto Onda", "Kazumi Kasaura", "Yuta Oriike", "Masaya Taniguchi", "Akiyoshi Sannai", "Sho Sonoda"], "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.", "comment": "15 pages, 4 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.22005v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22005v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22406", "title": "Economic Model Predictive Control with a Non-Fixed Reference Trajectory for Optimal Microgrid Dispatch", "authors": ["Avik Ghosh", "Adil Khurram", "Jan Kleissl", "Sonia Martinez"], "summary": "Economic Model Predictive Control (EMPC), instead of stabilizing a reference\ntrajectory/state in the objective function like a Tracking MPC, optimizes the\neconomic performance over the prediction horizon, making it attractive for\neconomical microgrid (MG) dispatch. However, the demand charge component in the\nmonthly electricity cost, make it difficult to be encapsulated in additive\nstage costs, and can make solutions violate the principle of optimality if\nnaively introduced in the objective function. Moreover, previous EMPC based\nworks mostly rely on a-priori knowledge of an optimal economic steady state or\noptimal periodic trajectory for performance guarantees, which are not useful or\npossibly don't exist respectively, for real-time economical MG dispatch where\nload/generation forecasts are known only 24-48 h in advance. This paper, first,\nproposes an EMPC formulation for a generic deterministic discrete non-linear\ntime varying system with hard state and input constraints, without any a-priori\nrequirements of an optimal economic steady state or optimal periodic\ntrajectory. It is proved that under mild assumptions on terminal cost and\nregion, the asymptotic average economic cost of the proposed method is no worse\nthan the asymptotic average economic cost of any other non-fixed arbitrary\nreference trajectory which is known only until the current time-step. The EMPC\nframework is then leveraged for optimal MG dispatch by showing that the problem\ncan be reformulated to satisfy the assumptions required for the asymptotic\nperformance guarantee. Realistic simulations at the Port of San Diego MG\ndemonstrated that the proposed method can also reduce monthly electricity costs\nin closed-loop with respect to reference trajectories generated by directly\noptimizing the electricity cost function over the prediction horizon or by\ntracking an ideal grid import curve in a majority of the cases.", "comment": "18 pages, 4 tables, Manuscript under review", "pdf_url": "http://arxiv.org/pdf/2506.22406v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22406v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21966", "title": "Movable Antennas-aided Wireless Energy Transfer for the Internet of Things", "authors": ["Osmel Martínez Rosabal", "Onel Alcaraz López", "Marco Di Renzo", "Richard Demo Souza", "Hirley Alves"], "summary": "Recent advancements in movable antennas (MAs) technology create new\nopportunities for 6G and beyond wireless systems. MAs are promising for radio\nfrequency wireless energy transfer because they can dynamically adjust antenna\npositions, improving energy efficiency and scalability. This work aims to\nminimize the power consumed by an analog beamforming power beacon equipped with\nindependently-controlled MAs (IMAs) for charging multiple single-antenna\ndevices. To this end, we enforce a minimum separation among antennas and a\nminimum received power at the devices. The resulting optimization problem is\nnonlinear and nonconvex due to interdependencies among the variables. To tackle\nthis, we propose a semidefinite program guided particle swarm optimization\n(SgPSO) algorithm where each particle represents an antenna configuration, and\nthe fitness function optimizes the corresponding power allocation. SgPSO is\nutilized for configuring the MAs largely outperforming fixed array\nimplementations, particularly with more antennas or devices. We also present an\nalternative implementation using uniformly-spaced MAs, whose performance\nclosely approaches that of the IMAs, with the gap widening only as the number\nof devices grows. We also examine how increasing the number of antennas\npromotes near-field conditions, which decrease as devices become more widely\ndistributed.", "comment": "7 pages, 5 figures, submitted to IEEE Transactions on Vehicular\n  Technology", "pdf_url": "http://arxiv.org/pdf/2506.21966v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21966v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22222", "title": "Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections", "authors": ["Hao Xu", "Ruth Lim", "Brian E. Chapman"], "summary": "Purpose: Aortic dissections are life-threatening cardiovascular conditions\nrequiring accurate segmentation of true lumen (TL), false lumen (FL), and false\nlumen thrombosis (FLT) from CTA images for effective management. Manual\nsegmentation is time-consuming and variable, necessitating automated solutions.\nMaterials and Methods: We developed four deep learning-based pipelines for Type\nB aortic dissection segmentation: a single-step model, a sequential model, a\nsequential multi-task model, and an ensemble model, utilizing 3D U-Net and\nSwin-UnetR architectures. A dataset of 100 retrospective CTA images was split\ninto training (n=80), validation (n=10), and testing (n=10). Performance was\nassessed using the Dice Coefficient and Hausdorff Distance. Results: Our\napproach achieved superior segmentation accuracy, with Dice Coefficients of\n0.91 $\\pm$ 0.07 for TL, 0.88 $\\pm$ 0.18 for FL, and 0.47 $\\pm$ 0.25 for FLT,\noutperforming Yao et al. (1), who reported 0.78 $\\pm$ 0.20, 0.68 $\\pm$ 0.18,\nand 0.25 $\\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide\naccurate segmentation of TBAD features, enabling derivation of morphological\nparameters for surveillance and treatment planning", "comment": "9 pages, 5 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.22222v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22222v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21845", "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "authors": ["Zhuodi Cai"], "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "pdf_url": "http://arxiv.org/pdf/2506.21845v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21845v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21613", "title": "ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech", "authors": ["Gautam Siddharth Kashyap", "Mohammad Anas Azeez", "Rafiq Ali", "Zohaib Hasan Siddiqui", "Jiechao Gao", "Usman Naseem"], "summary": "The increasing prevalence of child-targeted hate speech online underscores\nthe urgent need for specialized datasets to address this critical issue.\nExisting hate speech datasets lack agespecific annotations, fail to capture\nnuanced contexts, and overlook the unique emotional impact on children. To\nbridge this gap, we introduce ChildGuard1, a curated dataset derived from\nexisting corpora and enriched with child-specific annotations. ChildGuard\ncaptures diverse contexts of child-targeted hate speech, spanning age groups.\nWe benchmark existing state-of-the-art hate speech detection methods, including\nLarge Language Models (LLMs), and assess their effectiveness in detecting and\ncontextualizing child-targeted hate speech. To foster further research in this\narea, we publicly release ChildGuard, providing a robust foundation for\ndeveloping improved methods to detect and mitigate such harm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21613v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21613v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.21562", "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21562v1", "categories": ["cs.CL", "cs.AI", "cs.AR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21562v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21617", "title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems", "authors": ["Hiba Bederina", "Jill-Jênn Vie"], "summary": "The challenge of balancing user relevance and content diversity in\nrecommender systems is increasingly critical amid growing concerns about\ncontent homogeneity and reduced user engagement. In this work, we propose a\nnovel framework that leverages a multi-objective, contextual sequential\nsampling strategy. Item selection is guided by Bayesian updates that\ndynamically adjust scores to optimize diversity. The reward formulation\nintegrates multiple diversity metrics-including the log-determinant volume of a\ntuned similarity submatrix and ridge leverage scores-along with a diversity\ngain uncertainty term to address the exploration-exploitation trade-off. Both\nintra- and inter-batch diversity are modeled to promote serendipity and\nminimize redundancy. A dominance-based ranking procedure identifies\nPareto-optimal item sets, enabling adaptive and balanced selections at each\niteration. Experiments on a real-world dataset show that our approach\nsignificantly improves diversity without sacrificing relevance, demonstrating\nits potential to enhance user experience in large-scale recommendation\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21617v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21617v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22292", "title": "Scalable inference of large-scale random kronecker graphs via tensor decomposition and Einstein summation", "authors": ["Sanaa Khobizy"], "summary": "In this paper, we extend the analysis of random Kronecker graphs to\nmulti-dimensional networks represented as tensors, enabling a more detailed and\nnuanced understanding of complex network structures. We decompose the adjacency\ntensor of such networks into two components: a low-rank signal tensor that\ncaptures the essential network structure and a zero-mean noise tensor that\naccounts for random variations. Building on recent advancements in tensor\ndecomposition and random tensor theory, we introduce a generalized\ndenoise-and-solve framework that leverages the Einstein summation convention\nfor efficient tensor operations. This approach significantly reduces\ncomputational complexity while demonstrating strong performance in network\ninference tasks, providing a scalable and efficient solution for analyzing\nlarge-scale, multi-dimensional networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22292v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22292v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21782", "title": "M3PO: Massively Multi-Task Model-Based Policy Optimization", "authors": ["Aditya Narendra", "Dmitry Makarov", "Aleksandr Panov"], "summary": "We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a\nscalable model-based reinforcement learning (MBRL) framework designed to\naddress sample inefficiency in single-task settings and poor generalization in\nmulti-task domains. Existing model-based approaches like DreamerV3 rely on\npixel-level generative models that neglect control-centric representations,\nwhile model-free methods such as PPO suffer from high sample complexity and\nweak exploration. M3PO integrates an implicit world model, trained to predict\ntask outcomes without observation reconstruction, with a hybrid exploration\nstrategy that combines model-based planning and model-free uncertainty-driven\nbonuses. This eliminates the bias-variance trade-off in prior methods by using\ndiscrepancies between model-based and model-free value estimates to guide\nexploration, while maintaining stable policy updates through a trust-region\noptimizer. M3PO provides an efficient and robust alternative to existing\nmodel-based policy optimization approaches and achieves state-of-the-art\nperformance across multiple benchmarks.", "comment": "6 pages, 4 figures. Accepted at IEEE/RSJ IROS 2025. Full version,\n  including appendix and implementation details", "pdf_url": "http://arxiv.org/pdf/2506.21782v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21782v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21584", "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "authors": ["J. Koorndijk"], "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21584v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21584v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21576", "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21576v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21576v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.22089", "title": "Pseudo-Equilibria, or: How to Stop Worrying About Crypto and Just Analyze the Game", "authors": ["Alexandros Psomas", "Athina Terzoglou", "Yu Wei", "Vassilis Zikas"], "summary": "We consider the problem of a game theorist analyzing a game that uses\ncryptographic protocols. Ideally, a theorist abstracts protocols as ideal,\nimplementation-independent primitives, letting conclusions in the \"ideal world\"\ncarry over to the \"real world.\" This is crucial, since the game theorist\ncannot--and should not be expected to--handle full cryptographic complexity. In\ntoday's landscape, the rise of distributed ledgers makes a shared language\nbetween cryptography and game theory increasingly necessary.\n  The security of cryptographic protocols hinges on two types of assumptions:\nstate-of-the-world (e.g., \"factoring is hard\") and behavioral (e.g., \"honest\nmajority\"). We observe that for protocols relying on behavioral assumptions\n(e.g., ledgers), our goal is unattainable in full generality. For\nstate-of-the-world assumptions, we show that standard solution concepts, e.g.,\n($\\epsilon$-)Nash equilibria, are not robust to transfer from the ideal to the\nreal world.\n  We propose a new solution concept: the pseudo-Nash equilibrium. Informally, a\nprofile $s=(s_1,\\dots,s_n)$ is a pseudo-Nash equilibrium if, for any player $i$\nand deviation $s'_i$ with higher expected utility, $i$'s utility from $s_i$ is\n(computationally) indistinguishable from that of $s'_i$. Pseudo-Nash is simpler\nand more accessible to game theorists than prior notions addressing the\nmismatch between (asymptotic) cryptography and game theory. We prove that Nash\nequilibria in games with ideal, unbreakable cryptography correspond to\npseudo-Nash equilibria when ideal cryptography is instantiated with real\nprotocols (under state-of-the-world assumptions). Our translation is\nconceptually simpler and more general: it avoids tuning or restricting utility\nfunctions in the ideal game to fit quirks of cryptographic implementations.\nThus, pseudo-Nash lets us study game-theoretic and cryptographic aspects\nseparately and seamlessly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22089v1", "categories": ["cs.GT", "cs.CR"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.22089v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21860", "title": "Embodied Domain Adaptation for Object Detection", "authors": ["Xiangyu Shi", "Yanyuan Qiao", "Lingqiao Liu", "Feras Dayoub"], "summary": "Mobile robots rely on object detectors for perception and object localization\nin indoor environments. However, standard closed-set methods struggle to handle\nthe diverse objects and dynamic conditions encountered in real homes and labs.\nOpen-vocabulary object detection (OVOD), driven by Vision Language Models\n(VLMs), extends beyond fixed labels but still struggles with domain shifts in\nindoor environments. We introduce a Source-Free Domain Adaptation (SFDA)\napproach that adapts a pre-trained model without accessing source data. We\nrefine pseudo labels via temporal clustering, employ multi-scale threshold\nfusion, and apply a Mean Teacher framework with contrastive learning. Our\nEmbodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates\nadaptation under sequential changes in lighting, layout, and object diversity.\nOur experiments show significant gains in zero-shot detection performance and\nflexible adaptation to dynamic indoor conditions.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21860v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21860v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21718", "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "pdf_url": "http://arxiv.org/pdf/2506.21718v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21718v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22125", "title": "NoticeLight: Embracing Socio-Technical Asymmetry through Tangible Peripheral Robotic Embodiment in Hybrid Collaboration", "authors": ["Marie Altmann", "Kimberly Hegemann", "Ali Askari", "Vineetha Rallabandi", "Max Pascher", "Jens Gerken"], "summary": "Hybrid collaboration has become a fixture in modern workplaces, yet it\nintroduces persistent socio-technical asymmetries-especially disadvantaging\nremote participants, who struggle with presence disparity, reduced visibility,\nand limited non-verbal communication. Traditional solutions often seek to erase\nthese asymmetries, but recent research suggests embracing them as productive\ndesign constraints. In this context, we introduce NoticeLight: a tangible,\nperipheral robotic embodiment designed to augment hybrid meetings. NoticeLight\ntransforms remote participants' digital presence into ambient, physical signals\n-- such as mood dynamics, verbal contribution mosaics, and attention cues --\nwithin the co-located space. By abstracting group states into subtle light\npatterns, NoticeLight fosters peripheral awareness and balanced participation\nwithout disrupting meeting flow or demanding cognitive overload. This approach\naligns with emerging perspectives in human-robot synergy, positioning robots as\nmediators that reshape, rather than replicate, human presence. Our work thereby\nadvances the discourse on how robotic embodiments can empower equitable,\ndynamic collaboration in the workplace.", "comment": "Workshop on The Future of Human-Robot Synergy in Interactive\n  Environments: The Role of Robots at the Workplace at CHIWORK 2025, Amsterdam,\n  Netherlands", "pdf_url": "http://arxiv.org/pdf/2506.22125v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22125v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22190", "title": "dreaMLearning: Data Compression Assisted Machine Learning", "authors": ["Xiaobo Zhao", "Aaron Hurst", "Panagiotis Karras", "Daniel E. Lucani"], "summary": "Despite rapid advancements, machine learning, particularly deep learning, is\nhindered by the need for large amounts of labeled data to learn meaningful\npatterns without overfitting and immense demands for computation and storage,\nwhich motivate research into architectures that can achieve good performance\nwith fewer resources. This paper introduces dreaMLearning, a novel framework\nthat enables learning from compressed data without decompression, built upon\nEntropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless\ncompression method that consolidates information into a compact set of\nrepresentative samples. DreaMLearning accommodates a wide range of data types,\ntasks, and model architectures. Extensive experiments on regression and\nclassification tasks with tabular and image data demonstrate that dreaMLearning\naccelerates training by up to 8.8x, reduces memory usage by 10x, and cuts\nstorage by 42%, with a minimal impact on model performance. These advancements\nenhance diverse ML applications, including distributed and federated learning,\nand tinyML on resource-constrained edge devices, unlocking new possibilities\nfor efficient and scalable learning.", "comment": "18 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.22190v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22190v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21742", "title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning", "authors": ["Sirnam Swetha", "Rohit Gupta", "Parth Parag Kulkarni", "David G Shatwell", "Jeffrey A Chan Santiago", "Nyle Siddiqui", "Joseph Fioresi", "Mubarak Shah"], "summary": "Video QA has made significant strides by leveraging multimodal learning to\nalign visual and textual modalities. However, current benchmarks overwhelmingly\nfocus on questions answerable through explicit visual content - actions,\nobjects & events directly observable within individual frames or short clips.\nIn contrast, creative and cinematic videos - such as movies, TV shows, and\nnarrative-driven content - employ storytelling techniques that deliberately\nomit certain depictions, requiring viewers to infer motives, causality, and\nrelationships across discontinuous frames. Humans naturally excel at such\nimplicit reasoning, seamlessly integrating information across time and context\nto construct coherent narratives. Current VideoQA systems and benchmarks fail\nto capture this essential dimension of human-like understanding. To bridge this\ngap, we present ImplicitQA, a novel benchmark specifically designed to test\nmodels on implicit reasoning. It comprises 1K meticulously annotated QA pairs\nderived from 320+ high-quality creative video clips, systematically categorized\ninto key reasoning dimensions: lateral and vertical spatial reasoning, depth\nand proximity, viewpoint and visibility, motion and trajectory, causal and\nmotivational reasoning, social interactions, physical context, and inferred\ncounting. These annotations are deliberately challenging, crafted by authors\nensuring high-quality. Our extensive evaluations on leading VideoQA models\nreveals performance degradation, underscoring their reliance on surface-level\nvisual cues and highlighting the difficulty of implicit reasoning. Performance\nvariations across models further illustrate the complexity and diversity of the\nchallenges presented by ImplicitQA. By releasing both the dataset and our data\ncollection framework, we aim to stimulate further research and development in\nthe community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21742v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21742v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22056", "title": "Universal Retrieval for Multimodal Trajectory Modeling", "authors": ["Xuan Zhang", "Ziyan Jiang", "Rui Meng", "Yifei Leng", "Zhenbang Xiao", "Zora Zhiruo Wang", "Yanyi Shang", "Dehan Kong"], "summary": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.", "comment": "18 pages, 3 figures, accepted by Workshop on Computer-use Agents @\n  ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22056v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22056v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22410", "title": "Spherical Pendulum with Quad-Rotor Thrust Vectoring Actuation -- A Novel Mechatronics and Control Benchmark Platform", "authors": ["Yuchen Li", "Omar Curiel", "Sheng-Fan Wen", "Tsu-Chin Tsao"], "summary": "Motor-actuated pendulums have been established as arguably the most common\nlaboratory prototypes used in control system education because of the relevance\nto robot manipulator control in industry. Meanwhile, multi-rotor drones like\nquadcopters have become popular in industrial applications but have not been\nbroadly employed in control education laboratory. Platforms with pendulums and\nmulti-rotor copters present classical yet intriguing multi-degree of freedom\n(DoF) dynamics and coordinate systems for the control system investigation. In\nthis paper, we introduce a novel control platform in which a 2-DoF pendulum\ncapable of azimuth and elevation rotation is actuated through vectored thrust\ngenerated by a quadcopter. Designed as a benchmark for mechatronics and\nnonlinear control education and research, the system integrates detailed\nmechatronic implementation with different control strategies. Specifically, we\napply and compare small perturbation linearization (SPL), state feedback\nlinearization (SFL), and partial feedback linearization (PFL) to the nonlinear\nsystem dynamics. The performances are evaluated by time specifications of step\nresponse and Root-Mean-Square (RMS) error of trajectory tracking. The\nrobustness of the closed-loop system is validated under external disturbances,\nand both simulation and experimental results are presented to highlight the\nstrengths and limitations of the nonlinear model-based control approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22410v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22410v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21983", "title": "Learning-Based Hybrid Neural Receiver for 6G-V2X Communications", "authors": ["Osama Saleem", "Mohammed Alfaqawi", "Pierre Merdrignac", "Abdelaziz Bensrhair", "Soheyb Ribouh"], "summary": "Neural receiver models are proposed to jointly optimize multiple\nfunctionalities of wireless receivers; however, a comprehensive receiver model\nthat replaces the entire physical layer blocks has not yet been presented in\nthe literature. In this work, we introduce a novel hybrid neural receiver\n(H-NR) built on Transformer encoder blocks and Graph Neural Network (GNN), as\npart of an end-to-end wireless communication framework. In our communication\nframework, we assume vehicle to network (V2N) uplink scenario where information\nis transmitted by vehicle and received at the base station (BS). Our proposed\nH-NR model replace OFDM resource grid demapping, channel estimation, signal\nequalization, demodulation, and channel decoding. To test the adaptability of\nour proposed model on unseen conditions, we evaluate its performance for\nvarious scenarios, including a vehicle speed of range [0-60] km/h, a carrier\nfrequency of 5.9GHz, and a cluster delay line (CDL) channel model. Furthermore,\nwe assess the performance of our proposed H-NR on multimodal data, such as\nimages, audio, GPS, radar, and LiDAR, to examine its adaptability in real-world\nuse cases. The simulation results clearly demonstrate that our proposed model\noutperforms the state-of-the-art neural receiver by approximately 0.5 dB in\nterms of reconstruction and error correction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21983v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21983v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22226", "title": "Cardiovascular disease classification using radiomics and geometric features from cardiac CT", "authors": ["Ajay Mittal", "Raghav Mehta", "Omar Todd", "Philipp Seeböck", "Georg Langs", "Ben Glocker"], "summary": "Automatic detection and classification of Cardiovascular disease (CVD) from\nComputed Tomography (CT) images play an important part in facilitating\nbetter-informed clinical decisions. However, most of the recent deep learning\nbased methods either directly work on raw CT data or utilize it in pair with\nanatomical cardiac structure segmentation by training an end-to-end classifier.\nAs such, these approaches become much more difficult to interpret from a\nclinical perspective. To address this challenge, in this work, we break down\nthe CVD classification pipeline into three components: (i) image segmentation,\n(ii) image registration, and (iii) downstream CVD classification. Specifically,\nwe utilize the Atlas-ISTN framework and recent segmentation foundational models\nto generate anatomical structure segmentation and a normative healthy atlas.\nThese are further utilized to extract clinically interpretable radiomic\nfeatures as well as deformation field based geometric features (through atlas\nregistration) for CVD classification. Our experiments on the publicly available\nASOCA dataset show that utilizing these features leads to better CVD\nclassification accuracy (87.50\\%) when compared against classification model\ntrained directly on raw CT images (67.50\\%). Our code is publicly available:\nhttps://github.com/biomedia-mira/grc-net", "comment": "Under Review at STACOM 2025 with MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22226v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22226v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22319", "title": "Asymptotic analysis and design of shell-based thermal lattice metamaterials", "authors": ["Di Zhang", "Ligang Liu"], "summary": "We present a rigorous asymptotic analysis framework for investigating the\nthermal conductivity of shell lattice metamaterials, extending prior work from\nmechanical stiffness to heat transfer. Central to our analysis is a new metric,\nthe asymptotic directional conductivity (ADC), which captures the leading-order\ninfluence of the middle surface geometry on the effective thermal conductivity\nin the vanishing-thickness limit. A convergence theorem is established for\nevaluating ADC, along with a sharp upper bound and the necessary and sufficient\ncondition for achieving this bound. These results provide the first theoretical\njustification for the optimal thermal conductivity of triply periodic minimal\nsurfaces. Furthermore, we show that ADC yields a third-order approximation to\nthe effective conductivity of shell lattices at low volume fractions. To\nsupport practical design applications, we develop a discrete algorithm for\ncomputing and optimizing ADC over arbitrary periodic surfaces. Numerical\nresults confirm the theoretical predictions and demonstrate the robustness and\neffectiveness of the proposed optimization algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22319v1", "categories": ["math.AP", "cs.GR", "math-ph", "math.MP", "physics.comp-ph", "74Q15 (Primary) 35Q74, 74Q20, 74K25 (Secondary)", "I.3.5; J.2"], "cate": "math.AP", "url": "http://arxiv.org/abs/2506.22319v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21619", "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "summary": "Large-scale text-to-speech (TTS) models are typically categorized into\nautoregressive and non-autoregressive systems. Although autoregressive systems\nexhibit certain advantages in speech naturalness, their token-by-token\ngeneration mechanism makes it difficult to precisely control the duration of\nsynthesized speech. This is a key limitation in applications such as video\ndubbing that require strict audio-visual synchronization. This paper introduces\nIndexTTS2, which proposes a novel and autoregressive-model-friendly method for\nspeech duration control. The method supports two generation modes: one allows\nexplicit specification of the number of generated tokens for precise duration\ncontrol; the other does not require manual input and lets the model freely\ngenerate speech while preserving prosodic characteristics from the input\nprompt. Furthermore, IndexTTS2 achieves disentanglement between emotional\nexpression and speaker identity, enabling independent control of timbre and\nemotion. In the zero-shot setting, the model can perfectly reproduce the\nemotional characteristics of the input prompt. Users may also provide a\nseparate emotion prompt, even from a different speaker, allowing the model to\nreconstruct the target timbre while conveying the desired emotion. To enhance\nclarity during strong emotional expressions, we incorporate GPT latent\nrepresentations to improve speech stability. Meanwhile, to lower the barrier\nfor emotion control, we design a soft instruction mechanism based on textual\ndescriptions by fine-tuning Qwen3. This enables effective guidance of speech\ngeneration with desired emotional tendencies using natural language input.\nExperimental results demonstrate that IndexTTS2 outperforms existing\nstate-of-the-art zero-shot TTS models in word error rate, speaker similarity,\nand emotional fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21619v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21619v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21563", "title": "FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models", "authors": ["Kaiying Kevin Lin", "Hsiyu Chen", "Haopeng Zhang"], "summary": "While large language models (LLMs) have demonstrated impressive performance\nacross a wide range of natural language processing (NLP) tasks in high-resource\nlanguages, their capabilities in low-resource and minority languages remain\nsignificantly underexplored. Formosan languages -- a subgroup of Austronesian\nlanguages spoken in Taiwan -- are both linguistically rich and endangered,\nlargely due to the sociolinguistic dominance of Mandarin. In this work, we\nintroduce FORMOSANBENCH, the first benchmark for evaluating LLMs on\nlow-resource Austronesian languages. It covers three endangered Formosan\nlanguages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine\ntranslation, automatic speech recognition (ASR), and text summarization. We\nassess model performance in zero-shot, 10-shot, and fine-tuned settings using\nFORMOSANBENCH. Our results reveal a substantial performance gap between\nhigh-resource and Formosan languages. Existing LLMs consistently underperform\nacross all tasks, with 10-shot learning and fine-tuning offering only limited\nimprovements. These findings underscore the urgent need for more inclusive NLP\ntechnologies that can effectively support endangered and underrepresented\nlanguages. We release our datasets and code to facilitate future research in\nthis direction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21563v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21563v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21624", "title": "DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation", "authors": ["Blaž Škrlj", "Yonatan Karni", "Grega Gašperšič", "Blaž Mramor", "Yulia Stolin", "Martin Jakomin", "Jasna Urbančič", "Yuval Dishi", "Natalia Silberstein", "Ophir Friedler", "Assaf Klein"], "summary": "The Deep and Cross architecture (DCNv2) is a robust production baseline and\nis integral to numerous real-life recommender systems. Its inherent efficiency\nand ability to model interactions often result in models that are both simpler\nand highly competitive compared to more computationally demanding alternatives,\nsuch as Deep FFMs. In this work, we introduce three significant algorithmic\nimprovements to the DCNv2 architecture, detailing their formulation and\nbehavior at scale. The enhanced architecture we refer to as DCN^2 is actively\nused in a live recommender system, processing over 0.5 billion predictions per\nsecond across diverse use cases where it out-performed DCNv2, both offline and\nonline (ab tests). These improvements effectively address key limitations\nobserved in the DCNv2, including information loss in Cross layers, implicit\nmanagement of collisions through learnable lookup-level weights, and explicit\nmodeling of pairwise similarities with a custom layer that emulates FFMs'\nbehavior. The superior performance of DCN^2 is also demonstrated on four\npublicly available benchmark data sets.", "comment": "AdKDD 25", "pdf_url": "http://arxiv.org/pdf/2506.21624v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21624v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22312", "title": "An Alternative Finite Difference WENO-like Scheme with Physical Constraint Preservation for Divergence-Preserving Hyperbolic Systems", "authors": ["Dinshaw S. Balsara", "Deepak Bhoriya", "Chi-Wang Shu"], "summary": "Alternative finite difference Weighted Essentially Non-Oscillatory (AFD-WENO)\nschemes allow us to very efficiently update hyperbolic systems even in complex\ngeometries. Recent innovations in AFD-WENO methods allow us to treat hyperbolic\nsystem with non-conservative products almost as efficiently as conservation\nlaws. However, some PDE systems,like computational electrodynamics (CED) and\nmagnetohydrodynamics (MHD) and relativistic magnetohydrodynamics (RMHD), have\ninvolution constraints that require divergence-free or divergence-preserving\nevolution of vector fields. In such situations, a Yee-style collocation of\nvariables proves indispensable; and that collocation is retained in this work.\nIn previous works, only higher order finite volume discretization of such\ninvolution constrained systems was possible. In this work, we show that\nsubstantially more efficient AFD-WENO methods have been extended to encompass\ndivergence-preserving hyperbolic PDEs.\n  Our method retains the Yee-style collocation of normal components of...", "comment": "Accepted in Communications on Applied Mathematics and Computation", "pdf_url": "http://arxiv.org/pdf/2506.22312v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22312v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21788", "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data", "authors": ["Massimiliano Lupo Pasini", "Jong Youl Choi", "Pei Zhang", "Kshitij Mehta", "Rylie Weaver", "Ashwin M. Aji", "Karl W. Schulz", "Jorda Polo", "Prasanna Balaprakash"], "summary": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures.", "comment": "15 pages, 4 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.21788v1", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.atm-clus", "68T07, 68T09", "I.2; I.2.5; I.2.11"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21788v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21603", "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "authors": ["Yenisel Plasencia-Calaña"], "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21603v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21603v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21577", "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21577v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21577v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.22311", "title": "Reconstructing Intelligible Speech from the Pressure Sensor Data in HVACs", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Anomadarshi Barua"], "summary": "Pressure sensors are an integrated component of modern Heating, Ventilation,\nand Air Conditioning (HVAC) systems. As these pressure sensors operate within\nthe 0-10 Pa range, support high sampling frequencies of 0.5-2 kHz, and are\noften placed close to human proximity, they can be used to eavesdrop on\nconfidential conversation, since human speech has a similar audible range of\n0-10 Pa and a bandwidth of 4 kHz for intelligible quality. This paper presents\nWaLi, which reconstructs intelligible speech from the low-resolution and noisy\npressure sensor data by providing the following technical contributions: (i)\nWaLi reconstructs intelligible speech from a minimum of 0.5 kHz sampling\nfrequency of pressure sensors, whereas previous work can only detect hot\nwords/phrases. WaLi uses complex-valued conformer and Complex Global Attention\nBlock (CGAB) to capture inter-phoneme and intra-phoneme dependencies that exist\nin the low-resolution pressure sensor data. (ii) WaLi handles the transient\nnoise injected from HVAC fans and duct vibrations, by reconstructing both the\nclean magnitude and phase of the missing frequencies of the low-frequency\naliased components. Extensive measurement studies on real-world pressure\nsensors show an LSD of 1.24 and NISQA-MOS of 1.78 for 0.5 kHz to 8 kHz\nupsampling. We believe that such levels of accuracy pose a significant threat\nwhen viewed from a privacy perspective that has not been addressed before for\npressure sensors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22311v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22311v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21982", "title": "A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments", "authors": ["Akshay Jaitly", "Jack Cline", "Siavash Farzan"], "summary": "We propose a mixed-integer linear program (MILP) for multi-agent motion\nplanning that embeds Polytopic Action-based Motion Planning (PAAMP) into a\nsequence-then-solve pipeline. Region sequences confine each agent to adjacent\nconvex polytopes, while a big-M hyperplane model enforces inter-agent\nseparation. Collision constraints are applied only to agents sharing or\nneighboring a region, which reduces binary variables exponentially compared\nwith naive formulations. An L1 path-length-plus-acceleration cost yields smooth\ntrajectories. We prove finite-time convergence and demonstrate on\nrepresentative multi-agent scenarios with obstacles that our formulation\nproduces collision-free trajectories an order of magnitude faster than an\nunstructured MILP baseline.", "comment": "Accepted to 2025 IEEE International Conference on Automation Science\n  and Engineering (CASE 2025)", "pdf_url": "http://arxiv.org/pdf/2506.21982v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21982v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22231", "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education", "authors": ["Russell Beale"], "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22231v1", "categories": ["cs.HC", "cs.AI", "cs.CY", "K.3.1; K.3.2; K.6.0"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22231v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21770", "title": "Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images", "authors": ["Rishiraj Paul Chowdhury", "Nirmit Shekar Karkera"], "summary": "Glaucoma is a leading cause of irreversible blindness, but early detection\ncan significantly improve treatment outcomes. Traditional diagnostic methods\nare often invasive and require specialized equipment. In this work, we present\na deep learning pipeline using the EfficientNet-B0 architecture for glaucoma\ndetection from retinal fundus images. Unlike prior studies that rely on single\ndatasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,\nand RIM-ONE datasets to enhance generalization. Our experiments show that\nminimal preprocessing yields higher AUC-ROC compared to more complex\nenhancements, and our model demonstrates strong discriminative performance on\nunseen datasets. The proposed pipeline offers a reproducible and scalable\napproach to early glaucoma detection, supporting its potential clinical\nutility.", "comment": "13 pages, 6 figures, prepared for course CSCI 5922 at University of\n  Colorado Boulder. Code available upon request, dataset taken from Kaggle", "pdf_url": "http://arxiv.org/pdf/2506.21770v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21770v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22068", "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios", "authors": ["Shengyue Yao", "Runqing Guo", "Yangyang Qin", "Miangbing Meng", "Jipeng Cao", "Yilun Lin", "Yisheng Lv", "Fei-Yue Wang"], "summary": "With the deep penetration of Artificial Intelligence (AI) in the\ntransportation sector, intelligent cockpits, autonomous driving, and\nintelligent road networks are developing at an unprecedented pace. However, the\ndata ecosystems of these three key areas are increasingly fragmented and\nincompatible. Especially, existing testing methods rely on data stacking, fail\nto cover all edge cases, and lack flexibility. To address this issue, this\npaper introduces the concept of \"Query as Test\" (QaT). This concept shifts the\nfocus from rigid, prescripted test cases to flexible, on-demand logical queries\nagainst a unified data representation. Specifically, we identify the need for a\nfundamental improvement in data storage and representation, leading to our\nproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative\ndata framework based on Answer Set Programming (ASP), which uniformly\nrepresents heterogeneous multimodal data from the cockpit, vehicle, and road as\na collection of logical facts and rules. This approach not only achieves deep\nsemantic fusion of data, but also brings three core advantages: (1) supports\ncomplex and flexible semantic querying through logical reasoning; (2) provides\nnatural interpretability for decision-making processes; (3) allows for\non-demand data abstraction through logical rules, enabling fine-grained privacy\nprotection. We further elaborate on the QaT paradigm, transforming the\nfunctional validation and safety compliance checks of autonomous driving\nsystems into logical queries against the ESN database, significantly enhancing\nthe expressiveness and formal rigor of the testing. Finally, we introduce the\nconcept of \"Validation-Driven Development\" (VDD), which suggests to guide\ndevelopments by logical validation rather than quantitative testing in the era\nof Large Language Models, in order to accelerating the iteration and\ndevelopment process.", "comment": "Submitted to IEEE Transaction on Vehicular Technology", "pdf_url": "http://arxiv.org/pdf/2506.22068v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22068v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21648", "title": "Advanced System Engineering Approaches to Emerging Challenges in Planetary and Deep-Space Exploration", "authors": ["J. de Curtò", "Cristina LiCalzi", "Julien Tubiana Warin", "Jack Gehlert", "Brian Langbein", "Alexandre Gamboa", "Chris Sixbey", "William Maguire", "Santiago Fernández", "Álvaro Maestroarena", "Alex Brenchley", "Logan Maroclo", "Philemon Mercado", "Joshua DeJohn", "Cesar Velez", "Ethan Dahmus", "Taylor Steinys", "David Fritz", "I. de Zarzà"], "summary": "This paper presents innovative solutions to critical challenges in planetary\nand deep-space exploration electronics. We synthesize findings across diverse\nmission profiles, highlighting advances in: (1) MARTIAN positioning systems\nwith dual-frequency transmission to achieve $\\pm$1m horizontal accuracy; (2)\nartificial reef platforms for Titan's hydrocarbon seas utilizing specialized\nsensor arrays and multi-stage communication chains; (3) precision orbital\nrendezvous techniques demonstrating novel thermal protection solutions; (4)\nminiaturized CubeSat architectures for asteroid exploration with optimized\npower-to-mass ratios; and (5) next-generation power management systems for MARS\nrovers addressing dust accumulation challenges. These innovations represent\npromising directions for future space exploration technologies, particularly in\nenvironments where traditional Earth-based electronic solutions prove\ninadequate. The interdisciplinary nature of these developments highlights the\ncritical intersection of aerospace engineering, electrical engineering, and\nplanetary science in advancing human exploration capabilities beyond Earth\norbit.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21648v1", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.RO", "cs.SY", "eess.SY"], "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2506.21648v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22059", "title": "Hybrid Constellation Modulation for Symbol-Level Precoding in RIS-Enhanced MU-MISO Systems", "authors": ["Yupeng Zheng", "Yi Ma", "Rahim Tafazolli"], "summary": "The application of symbol-level precoding (SLP) in reconfigurable intelligent\nsurfaces (RIS) enhanced multi-user multiple-input single-output (MU-MISO)\nsystems faces two main challenges. First, the state-of-the-art joint reflecting\nand SLP optimization approach requires exhaustive enumeration of all possible\ntransmit symbol combinations, resulting in scalability issues as the modulation\norder and number of users increase. Second, conventional quadrature amplitude\nmodulation (QAM) exhibits strict constructive interference (CI) regions,\nlimiting its effectiveness for CI exploitation in SLP. To address these\nchallenges, this paper proposes a novel modulation scheme, termed\nhybrid-constellation modulation (HCM), which has a structure of superposed QAM\nand ASK sub-constellations (SCs). HCM extends the CI regions compared to QAM.\nAdditionally, a two-stage reflecting and SLP optimization method is developed\nto support HCM. The proposed methods are designed for practical RIS with\ndiscrete phase shifts and has good scalability. Simulation results show that\nHCM achieves up to 1.5 dB and 1 dB SER gains over QAM with modulation order 16\nand 64, respectively.", "comment": "This work has been accepted by IEEE SPAWC 2025", "pdf_url": "http://arxiv.org/pdf/2506.22059v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22059v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22280", "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "authors": ["Yuliang Huang", "Imraj Singh", "Thomas Joyce", "Kris Thielemans", "Jamie R. McClelland"], "summary": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion\nartifacts due to breathing. A common clinical approach mitigates this by\nsorting projections into respiratory phases and reconstructing images per\nphase, but this does not account for breathing variability. Dynamic CBCT\ninstead reconstructs images at each projection, capturing continuous motion\nwithout phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)\noffer powerful tools for modeling dynamic scenes, yet their application to\ndynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,\nuse implicit motion representations, which are computationally expensive. While\nexplicit low-rank motion models have been proposed, they lack spatial\nregularization, leading to inconsistencies in Gaussian motion. To address these\nlimitations, we introduce a free-form deformation (FFD)-based spatial basis\nfunction and a deformation-informed framework that enforces consistency by\ncoupling the temporal evolution of Gaussian's mean position, scale, and\nrotation under a unified deformation field. We evaluate our approach on six\nCBCT datasets, demonstrating superior image quality with a 6x speedup over\nHexPlane. These results highlight the potential of deformation-informed 4DGS\nfor efficient, motion-compensated CBCT reconstruction. The code is available at\nhttps://github.com/Yuliang-Huang/DIGS.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22280v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22280v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22426", "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22426v1", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22426v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21622", "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "summary": "Speech impairments caused by conditions such as cerebral palsy or genetic\ndisorders pose significant challenges for automatic speech recognition (ASR)\nsystems. Despite recent advances, ASR models like Whisper struggle with\nnon-normative speech due to limited training data and the difficulty of\ncollecting and annotating non-normative speech samples. In this work, we\npropose a practical and lightweight pipeline to personalize ASR models,\nformalizing the selection of words and enriching a small, speech-impaired\ndataset with semantic coherence. Applied to data from a child with a structural\nspeech impairment, our approach shows promising improvements in transcription\nquality, demonstrating the potential to reduce communication barriers for\nindividuals with atypical speech patterns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21622v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21622v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21564", "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing", "authors": ["Jiyan Liu", "Youzheng Liu", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21564v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21564v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21638", "title": "IRanker: Towards Ranking Foundation Model", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21638v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21638v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22273", "title": "Phase field approximation for Plateau's problem: a curve geodesic distance penalty approach", "authors": ["Matthieu Bonnivard", "Elie Bretin", "Antoine Lemenant", "Eve Machefert"], "summary": "This work focuses on a phase field approximation of Plateau's problem.\nInspired by Reifenberg's point of view, we introduce a model that combines the\nAmbrosio-Torterelli energy with a geodesic distance term, which can be\nconsidered as a generalization of the approach developed by Bonnivard, Lemenant\nand Santambrogio to approximate solutions to Steiner's problem. First, we\npresent a Gamma-convergence analysis of this model in the simple case of a\nsingle curve located on the edge of a cylinder. In a numerical section, we\ndetail the numerical optimisation schemes used to minimize this energy for\nnumerous examples, for which good approximations of solutions to Plateau's\nproblem are found.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22273v1", "categories": ["math.OC", "cs.NA", "math.AP", "math.NA"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22273v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21797", "title": "Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning", "authors": ["Peihao Wang", "Zhangyang Wang"], "summary": "We develop a theoretical framework that explains how discrete symbolic\nstructures can emerge naturally from continuous neural network training\ndynamics. By lifting neural parameters to a measure space and modeling training\nas Wasserstein gradient flow, we show that under geometric constraints, such as\ngroup invariance, the parameter measure $\\mu_t$ undergoes two concurrent\nphenomena: (1) a decoupling of the gradient flow into independent optimization\ntrajectories over some potential functions, and (2) a progressive contraction\non the degree of freedom. These potentials encode algebraic constraints\nrelevant to the task and act as ring homomorphisms under a commutative\nsemi-ring structure on the measure space. As training progresses, the network\ntransitions from a high-dimensional exploration to compositional\nrepresentations that comply with algebraic operations and exhibit a lower\ndegree of freedom. We further establish data scaling laws for realizing\nsymbolic tasks, linking representational capacity to the group invariance that\nfacilitates symbolic solutions. This framework charts a principled foundation\nfor understanding and designing neurosymbolic systems that integrate continuous\nlearning with discrete algebraic reasoning.", "comment": "International Conference on Neuro-symbolic Systems (NeuS), 2025", "pdf_url": "http://arxiv.org/pdf/2506.21797v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21797v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21606", "title": "Large Language Models as symbolic DNA of cultural dynamics", "authors": ["Parham Pourdavood", "Michael Jacob", "Terrence Deacon"], "summary": "This paper proposes a novel conceptualization of Large Language Models (LLMs)\nas externalized informational substrates that function analogously to DNA for\nhuman cultural dynamics. Rather than viewing LLMs as either autonomous\nintelligence or mere programmed mimicry, we argue they serve a broader role as\nrepositories that preserve compressed patterns of human symbolic\nexpression--\"fossils\" of meaningful dynamics that retain relational residues\nwithout their original living contexts. Crucially, these compressed patterns\nonly become meaningful through human reinterpretation, creating a recursive\nfeedback loop where they can be recombined and cycle back to ultimately\ncatalyze human creative processes. Through analysis of four universal\nfeatures--compression, decompression, externalization, and recursion--we\ndemonstrate that just as DNA emerged as a compressed and externalized medium\nfor preserving useful cellular dynamics without containing explicit reference\nto goal-directed physical processes, LLMs preserve useful regularities of human\nculture without containing understanding of embodied human experience.\nTherefore, we argue that LLMs' significance lies not in rivaling human\nintelligence, but in providing humanity a tool for self-reflection and playful\nhypothesis-generation in a low-stakes, simulated environment. This framework\npositions LLMs as tools for cultural evolvability, enabling humanity to\ngenerate novel hypotheses about itself while maintaining the human\ninterpretation necessary to ground these hypotheses in ongoing human aesthetics\nand norms.", "comment": "28 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.21606v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21606v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21613", "title": "ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech", "authors": ["Gautam Siddharth Kashyap", "Mohammad Anas Azeez", "Rafiq Ali", "Zohaib Hasan Siddiqui", "Jiechao Gao", "Usman Naseem"], "summary": "The increasing prevalence of child-targeted hate speech online underscores\nthe urgent need for specialized datasets to address this critical issue.\nExisting hate speech datasets lack agespecific annotations, fail to capture\nnuanced contexts, and overlook the unique emotional impact on children. To\nbridge this gap, we introduce ChildGuard1, a curated dataset derived from\nexisting corpora and enriched with child-specific annotations. ChildGuard\ncaptures diverse contexts of child-targeted hate speech, spanning age groups.\nWe benchmark existing state-of-the-art hate speech detection methods, including\nLarge Language Models (LLMs), and assess their effectiveness in detecting and\ncontextualizing child-targeted hate speech. To foster further research in this\narea, we publicly release ChildGuard, providing a robust foundation for\ndeveloping improved methods to detect and mitigate such harm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21613v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21613v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22423", "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks", "authors": ["Pritam Dash", "Ethan Chan", "Nathan P. Lawrence", "Karthik Pattabiraman"], "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22423v1", "categories": ["cs.LG", "cs.CR", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22423v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22028", "title": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies", "authors": ["Ossi Parikka", "Roel Pieters"], "summary": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC).", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). For further information, videos and\n  code, see https://github.com/ozzyuni/LMPVC", "pdf_url": "http://arxiv.org/pdf/2506.22028v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22028v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22379", "title": "How to Evaluate the Accuracy of Online and AI-Based Symptom Checkers: A Standardized Methodological Framework", "authors": ["Marvin Kopka", "Markus A. Feufel"], "summary": "Online and AI-based symptom checkers are applications that assist medical\nlaypeople in diagnosing their symptoms and determining which course of action\nto take. When evaluating these tools, previous studies primarily used an\napproach introduced a decade ago that lacked any type of quality control.\nNumerous studies have criticized this approach, and several empirical studies\nhave sought to improve specific aspects of evaluations. However, even after a\ndecade, a high-quality methodological framework for standardizing the\nevaluation of symptom checkers remains missing. This article synthesizes\nempirical studies to outline a framework for standardized evaluations based on\nrepresentative case selection, an externally and internally valid evaluation\ndesign, and metrics that increase cross-study comparability. This approach is\nbacked up by several open-access resources to facilitate implementation.\nUltimately, this approach should enhance the quality and comparability of\nfuture evaluations of online and AI-based symptom checkers to enable\nmeta-analyses and help stakeholders make more informed decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22379v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22379v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21785", "title": "Comparing Learning Paradigms for Egocentric Video Summarization", "authors": ["Daniel Wen"], "summary": "In this study, we investigate various computer vision paradigms - supervised\nlearning, unsupervised learning, and prompt fine-tuning - by assessing their\nability to understand and interpret egocentric video data. Specifically, we\nexamine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM\n(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned\npre-trained model), evaluating their effectiveness in video summarization. Our\nresults demonstrate that current state-of-the-art models perform less\neffectively on first-person videos compared to third-person videos,\nhighlighting the need for further advancements in the egocentric video domain.\nNotably, a prompt fine-tuned general-purpose GPT-4o model outperforms these\nspecialized models, emphasizing the limitations of existing approaches in\nadapting to the unique challenges of first-person perspectives. Although our\nevaluation is conducted on a small subset of egocentric videos from the\nEgo-Exo4D dataset due to resource constraints, the primary objective of this\nresearch is to provide a comprehensive proof-of-concept analysis aimed at\nadvancing the application of computer vision techniques to first-person videos.\nBy exploring novel methodologies and evaluating their potential, we aim to\ncontribute to the ongoing development of models capable of effectively\nprocessing and interpreting egocentric perspectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21785v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21785v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22183", "title": "A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety", "authors": ["Camille François", "Ludovic Péran", "Ayah Bdeir", "Nouha Dziri", "Will Hawkins", "Yacine Jernite", "Sayash Kapoor", "Juliet Shen", "Heidy Khlaaf", "Kevin Klyman", "Nik Marda", "Marie Pellat", "Deb Raji", "Divya Siddarth", "Aviya Skowron", "Joseph Spisak", "Madhulika Srikumar", "Victor Storchan", "Audrey Tang", "Jen Weedon"], "summary": "The rapid rise of open-weight and open-source foundation models is\nintensifying the obligation and reshaping the opportunity to make AI systems\nsafe. This paper reports outcomes from the Columbia Convening on AI Openness\nand Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme\ninvolving more than forty-five researchers, engineers, and policy leaders from\nacademia, industry, civil society, and government. Using a participatory,\nsolutions-oriented process, the working groups produced (i) a research agenda\nat the intersection of safety and open source AI; (ii) a mapping of existing\nand needed technical interventions and open source tools to safely and\nresponsibly deploy open foundation models across the AI development workflow;\nand (iii) a mapping of the content safety filter ecosystem with a proposed\nroadmap for future research and development. We find that openness --\nunderstood as transparent weights, interoperable tooling, and public governance\n-- can enhance safety by enabling independent scrutiny, decentralized\nmitigation, and culturally plural oversight. However, significant gaps persist:\nscarce multimodal and multilingual benchmarks, limited defenses against\nprompt-injection and compositional attacks in agentic systems, and insufficient\nparticipatory mechanisms for communities most affected by AI harms. The paper\nconcludes with a roadmap of five priority research directions, emphasizing\nparticipatory inputs, future-proof content filters, ecosystem-wide safety\ninfrastructure, rigorous agentic safeguards, and expanded harm taxonomies.\nThese recommendations informed the February 2025 French AI Action Summit and\nlay groundwork for an open, plural, and accountable AI safety discipline.", "comment": "Proceedings from the Columbia Convening on Openness in Artificial\n  Intelligence and AI Safety", "pdf_url": "http://arxiv.org/pdf/2506.22183v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22183v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21664", "title": "When Every Symbol Counts: Resilient Wireless Systems Under Finite Blocklength Constraints", "authors": ["Kevin Weinberger", "Aydin Sezgin"], "summary": "As 6G evolves, wireless networks become essential for critical operations and\nenable innovative applications that demand seamless adaptation to dynamic\nenvironments and disruptions. Because these vital services require\nuninterrupted operation, their resilience to unforeseen disruptions is\nessential. However, implementing resilience necessitates rapid recovery\nprocedures, which operate in the finite blocklength (FBL) regime, where short\npackets and added error-correction overhead can severely degrade communication\nefficiency. Due to this performance loss, always attempting recovery can\nbackfire and result in worse outcomes than simply enduring the disruption under\nlonger blocklengths. In this work, we study these effects of FBL constraints\nwithin a resilience framework, incorporating reconfigurable intelligent\nsurfaces (RIS) to enhance adaptation capabilities. By actively shaping the\nwireless environment, RIS help counteract some of the performance losses caused\nby FBL, enabling more effective recovery from disruptions. Numerical results\nreveal two critical blocklength thresholds: the first enables full recovery\nfrom the FBL penalty, while the second, at a higher blocklength, allows the\nsystem to recover from both the FBL penalty and the initial disruption,\nyielding a significant improvement in resilience performance. Additionally, we\nshow that the number of RIS elements shifts these thresholds, enabling faster\nreconfiguration with shorter blocklengths and providing insights to the\ntrade-offs between rate, blocklength, and reconfiguration effort under FBL\nconditions.", "comment": "6 pages, 3 figures, submitted to European Wireless 2025. arXiv admin\n  note: text overlap with arXiv:2504.11589", "pdf_url": "http://arxiv.org/pdf/2506.21664v1", "categories": ["eess.SP", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21664v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22082", "title": "Optimizing Indoor RIS-Aided Physical-Layer Security: A Codebook-Generation Methodology and Measurement-Based Analysis", "authors": ["Dimitris Kompostiotis", "Dimitris Vordonis", "Vassilis Paliouras", "George C. Alexandropoulos"], "summary": "Sixth-Generation (6G) wireless networks aim to support innovative\nInternet-of-Things (IoT) applications that demand faster and more secure data\ntransmission. While higher Open Systems Interconnection (OSI) layers employ\nmeasures like encryption and secure protocols to address data security,\nPhysical-Layer Security (PLS) focuses on preventing information leakage to\nEavesDroppers (EDs) and mitigating the effects of jammers and spoofing attacks.\nIn this context, the emerging technology of Reconfigurable Intelligent Surfaces\n(RISs) can play an instrumental role, enhancing PLS by intelligently reflecting\nelectromagnetic waves to benefit Legitimate Users (LUs) while obstructing EDs.\nThis paper presents practical indoor measurements to evaluate the capability of\nan RIS to enhance PLS, focusing on a varactor-based RIS technology designed for\nthe FR1 band at 3.55 GHz. A comparative analysis of state-of-the-art RIS-aided\nsecrecy optimization algorithms together with a novel approach designed in this\npaper, which relies on a newly generated RIS phase configuration codebook,\nhighlight the potential of RISs to improve both data rates for LUs as well as\nsecrecy against EDs in real-world indoor multipath environments. The results\nalso demonstrate the frequency selectivity of the RIS, proviging practical\ninsights on the optimization of the technology.", "comment": "7 pages, 3 figures, 2 tables Accepted for publication in the 2025\n  IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC), Istanbul, Turkey, September 1-4, 2025; to appear in\n  IEEE PIMRC 2025 proceedings. copyright 2025 IEEE. Personal use of this\n  material is permitted", "pdf_url": "http://arxiv.org/pdf/2506.22082v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22082v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22397", "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "summary": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.", "comment": "supplement pending, 4 figures, 10 pages + refs", "pdf_url": "http://arxiv.org/pdf/2506.22397v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22397v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21712", "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers", "authors": ["Tzu-Quan Lin", "Hsi-Chun Cheng", "Hung-yi Lee", "Hao Tang"], "summary": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21712v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21712v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21565", "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "authors": ["Takato Ueno", "Keito Inoshita"], "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21565v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21565v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21913", "title": "HyReC: Exploring Hybrid-based Retriever for Chinese", "authors": ["Zunran Wang", "Zheng Shenpeng", "Wang Shenglan", "Minghui Zhao", "Zhonghua Li"], "summary": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21913v1", "categories": ["cs.IR", "cs.CL"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21913v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22413", "title": "Physics-Informed Neural Networks: Bridging the Divide Between Conservative and Non-Conservative Equations", "authors": ["Arun Govind Neelan", "Ferdin Sagai Don Bosco", "Naveen Sagar Jarugumalli", "Suresh Balaji Vedarethinam"], "summary": "In the realm of computational fluid dynamics, traditional numerical methods,\nwhich heavily rely on discretization, typically necessitate the formulation of\npartial differential equations (PDEs) in conservative form to accurately\ncapture shocks and other discontinuities in compressible flows. Conversely,\nutilizing non-conservative forms often introduces significant errors near these\ndiscontinuities or results in smeared shocks. This dependency poses a\nconsiderable limitation, particularly as many PDEs encountered in complex\nphysical phenomena, such as multi-phase flows, are inherently non-conservative.\nThis inherent non-conservativity restricts the direct applicability of standard\nnumerical solvers designed for conservative forms. This work aims to thoroughly\ninvestigate the sensitivity of Physics-Informed Neural Networks (PINNs) to the\nchoice of PDE formulation (conservative vs. non-conservative) when solving\nproblems involving shocks and discontinuities. We have conducted this\ninvestigation across a range of benchmark problems, specifically the Burgers\nequation and both steady and unsteady Euler equations, to provide a\ncomprehensive understanding of PINNs capabilities in this critical area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22413v1", "categories": ["physics.flu-dyn", "cs.NA", "math.NA", "35L65, 35Q70, 65M70, 76N15, 68T07"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.22413v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21833", "title": "The Cost of Avoiding Backpropagation", "authors": ["Kunjal Panchal", "Sunav Choudhary", "Yuriy Brun", "Hui Guan"], "summary": "Forward-mode automatic differentiation (FmAD) and zero-order (ZO)\noptimization have been proposed as memory-efficient alternatives to\nbackpropagation (BP) for gradient computation, especially in low-resource\nsettings. However, their practical benefits remain unclear due to two key gaps:\na lack of comparison against memory-efficient BP variants, such as activation\ncheckpointing, and a lack of a unified theoretical analysis. This work presents\na comprehensive theoretical and empirical comparison of BP, FmAD, and ZO\nmethods. Our theoretical analysis shows that while FmAD, and ZO can reduce\nmemory usage, they incur significant costs in accuracy, convergence speed, and\ncomputation compared to BP with checkpointing. These drawbacks worsen with\nlarger models or constrained perturbation budgets. Empirical experiments on\nlarge language and vision-language models show that BP with checkpointing\noutperforms FmAD and ZO variants, including those enhanced with variance\nreduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and\n3.8x fewer computations at comparable memory usage. Our results highlight\nfundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as\nthe most effective strategy for model training under memory-constrained\nsettings. Our code is available at\nhttps://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21833v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21833v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21616", "title": "TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization", "authors": ["Chuanrui Hu", "Wei Hu", "Penghang Yu", "Hua Zhang", "Bing-Kun Bao"], "summary": "Open-domain Timeline Summarization (TLS) is crucial for monitoring the\nevolution of news topics. To identify changes in news topics, existing methods\ntypically employ general Large Language Models (LLMs) to summarize relevant\ntimestamps from retrieved news. While general LLMs demonstrate capabilities in\nzero-shot news summarization and timestamp localization, they struggle with\nassessing topic relevance and understanding topic evolution. Consequently, the\nsummarized information often includes irrelevant details or inaccurate\ntimestamps. To address these issues, we propose the first large Timeline\nIntelligence Model (TIM) for open-domain TLS, which is capable of effectively\nsummarizing open-domain timelines. Specifically, we begin by presenting a\nlarge-scale TLS dataset, comprising over 1,000 news topics and more than 3,000\nannotated TLS instances. Furthermore, we propose a progressive optimization\nstrategy, which gradually enhance summarization performance. It employs\ninstruction tuning to enhance summarization and topic-irrelevant information\nfiltering capabilities. Following this, it exploits a novel dual-alignment\nreward learning method that incorporates both semantic and temporal\nperspectives, thereby improving the understanding of topic evolution\nprinciples. Through this progressive optimization strategy, TIM demonstrates a\nrobust ability to summarize open-domain timelines. Extensive experiments in\nopen-domain demonstrate the effectiveness of our TIM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21616v1", "categories": ["cs.CL", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21616v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.21619", "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "summary": "Large-scale text-to-speech (TTS) models are typically categorized into\nautoregressive and non-autoregressive systems. Although autoregressive systems\nexhibit certain advantages in speech naturalness, their token-by-token\ngeneration mechanism makes it difficult to precisely control the duration of\nsynthesized speech. This is a key limitation in applications such as video\ndubbing that require strict audio-visual synchronization. This paper introduces\nIndexTTS2, which proposes a novel and autoregressive-model-friendly method for\nspeech duration control. The method supports two generation modes: one allows\nexplicit specification of the number of generated tokens for precise duration\ncontrol; the other does not require manual input and lets the model freely\ngenerate speech while preserving prosodic characteristics from the input\nprompt. Furthermore, IndexTTS2 achieves disentanglement between emotional\nexpression and speaker identity, enabling independent control of timbre and\nemotion. In the zero-shot setting, the model can perfectly reproduce the\nemotional characteristics of the input prompt. Users may also provide a\nseparate emotion prompt, even from a different speaker, allowing the model to\nreconstruct the target timbre while conveying the desired emotion. To enhance\nclarity during strong emotional expressions, we incorporate GPT latent\nrepresentations to improve speech stability. Meanwhile, to lower the barrier\nfor emotion control, we design a soft instruction mechanism based on textual\ndescriptions by fine-tuning Qwen3. This enables effective guidance of speech\ngeneration with desired emotional tendencies using natural language input.\nExperimental results demonstrate that IndexTTS2 outperforms existing\nstate-of-the-art zero-shot TTS models in word error rate, speaker similarity,\nand emotional fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21619v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21619v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22034", "title": "Multi-Robot Assembly of Deformable Linear Objects Using Multi-Modal Perception", "authors": ["Kejia Chen", "Celina Dettmering", "Florian Pachler", "Zhuo Liu", "Yue Zhang", "Tailai Cheng", "Jonas Dirr", "Zhenshan Bing", "Alois Knoll", "Rüdiger Daub"], "summary": "Industrial assembly of deformable linear objects (DLOs) such as cables offers\ngreat potential for many industries. However, DLOs pose several challenges for\nrobot-based automation due to the inherent complexity of deformation and,\nconsequentially, the difficulties in anticipating the behavior of DLOs in\ndynamic situations. Although existing studies have addressed isolated\nsubproblems like shape tracking, grasping, and shape control, there has been\nlimited exploration of integrated workflows that combine these individual\nprocesses. To address this gap, we propose an object-centric perception and\nplanning framework to achieve a comprehensive DLO assembly process throughout\nthe industrial value chain. The framework utilizes visual and tactile\ninformation to track the DLO's shape as well as contact state across different\nstages, which facilitates effective planning of robot actions. Our approach\nencompasses robot-based bin picking of DLOs from cluttered environments,\nfollowed by a coordinated handover to two additional robots that mount the DLOs\nonto designated fixtures. Real-world experiments employing a setup with\nmultiple robots demonstrate the effectiveness of the approach and its relevance\nto industrial scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22034v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22034v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2501.06184", "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06184v1", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2501.06184v1", "date": "2025-01-10", "updated": "2025-01-10"}
{"id": "2506.21813", "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "authors": ["Felix Holm", "Gözde Ünver", "Ghazal Ghazaei", "Nassir Navab"], "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21813v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21813v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22271", "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion", "authors": ["Samy Badreddine", "Emile van Krieken", "Luciano Serafini"], "summary": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22271v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22271v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21718", "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "pdf_url": "http://arxiv.org/pdf/2506.21718v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21718v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22252", "title": "On the Feasibility of Distributed Phase Synchronization for Coherent Signal Superposition", "authors": ["Alphan Sahin"], "summary": "In this study, we analyze the feasibility of distributed phase\nsynchronization for coherent signal superposition, a fundamental enabler for\nparadigms such as coherent over-the-air computation (OAC), distributed\nbeamforming, and interference alignment, under mobility and hardware\nimpairments. With the focus on coherent OAC, we introduce phase-coded pilots\n(PCPs), a strategy where the radios communicate with each other to eliminate\nthe round-trip phase change in the uplink (UL) and downlink (DL) to align the\nphase of the received symbol at a desired angle. In this study, considering a\ncarrier frequency offset (CFO)-resilient multi-user procedure, we derive the\nstatistics of the phase deviations to assess how fast the phase coherency\ndegrades. Our results show that residual CFO is a major factor determining the\nduration of phase coherency, in addition to the non-negligible effects of\nmobility and the number of nodes in the network. We also provide a\nproof-of-concept demonstration for coherent signal superposition by using\noff-the-shelf radios to demonstrate the feasibility of PCPs in practice.", "comment": "Submitted to IEEE for publication", "pdf_url": "http://arxiv.org/pdf/2506.22252v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22252v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22426", "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22426v1", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22426v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21921", "title": "Explainable anomaly detection for sound spectrograms using pooling statistics with quantile differences", "authors": ["Nicolas Thewes", "Philipp Steinhauer", "Patrick Trampert", "Markus Pauly", "Georg Schneider"], "summary": "Anomaly detection is the task of identifying rarely occurring (i.e. anormal\nor anomalous) samples that differ from almost all other samples in a dataset.\nAs the patterns of anormal samples are usually not known a priori, this task is\nhighly challenging. Consequently, anomaly detection lies between semi- and\nunsupervised learning. The detection of anomalies in sound data, often called\n'ASD' (Anomalous Sound Detection), is a sub-field that deals with the\nidentification of new and yet unknown effects in acoustic recordings. It is of\ngreat importance for various applications in Industry 4.0. Here, vibrational or\nacoustic data are typically obtained from standard sensor signals used for\npredictive maintenance. Examples cover machine condition monitoring or quality\nassurance to track the state of components or products. However, the use of\nintelligent algorithms remains a controversial topic. Management generally aims\nfor cost-reduction and automation, while quality and maintenance experts\nemphasize the need for human expertise and comprehensible solutions. In this\nwork, we present an anomaly detection approach specifically designed for\nspectrograms. The approach is based on statistical evaluations and is\ntheoretically motivated. In addition, it features intrinsic explainability,\nmaking it particularly suitable for applications in industrial settings. Thus,\nthis algorithm is of relevance for applications in which black-box algorithms\nare unwanted or unsuitable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21921v1", "categories": ["stat.AP", "cs.SD", "eess.AS", "stat.CO", "62", "G.3"], "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.21921v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21566", "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "authors": ["Arwa Arif"], "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "comment": "Preprint, 8 Pages", "pdf_url": "http://arxiv.org/pdf/2506.21566v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21566v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21931", "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21931v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21931v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22428", "title": "Augmented Lagrangian methods for infeasible convex optimization problems and diverging proximal-point algorithms", "authors": ["Roland Andrews", "Justin Carpentier", "Adrien Taylor"], "summary": "This work investigates the convergence behavior of augmented Lagrangian\nmethods (ALMs) when applied to convex optimization problems that may be\ninfeasible. ALMs are a popular class of algorithms for solving constrained\noptimization problems. We establish progressively stronger convergence results,\nranging from basic sequence convergence to precise convergence rates, under a\nhierarchy of assumptions. In particular, we demonstrate that, under mild\nassumptions, the sequences of iterates generated by ALMs converge to solutions\nof the ``closest feasible problem''.\n  This study leverages the classical relationship between ALMs and the\nproximal-point algorithm applied to the dual problem. A key technical\ncontribution is a set of concise results on the behavior of the proximal-point\nalgorithm when applied to functions that may not have minimizers. These results\npertain to its convergence in terms of its subgradients and of the values of\nthe convex conjugate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22428v1", "categories": ["math.OC", "cs.NA", "math.NA"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22428v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21844", "title": "Koopman operator-based discussion on partial observation in stochastic systems", "authors": ["Jun Ohkubo"], "summary": "It is sometimes difficult to achieve a complete observation for a full set of\nobservables, and partial observations are necessary. For deterministic systems,\nthe Mori-Zwanzig formalism provides a theoretical framework for handling\npartial observations. Recently, data-driven algorithms based on the Koopman\noperator theory have made significant progress, and there is a discussion to\nconnect the Mori-Zwanzig formalism with the Koopman operator theory. In this\nwork, we discuss the effects of partial observation in stochastic systems using\nthe Koopman operator theory. The discussion clarifies the importance of\ndistinguishing the state space and the function space in stochastic systems.\nEven in stochastic systems, the delay embedding technique is beneficial for\npartial observation, and several numerical experiments showed a power-law\nbehavior of the accuracy for the amplitude of the additive noise. We also\ndiscuss the relation between the exponent of the power-law behavior and the\neffects of partial observation.", "comment": "23 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21844v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21844v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21620", "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit", "authors": ["Daniele Cirulli", "Giulio Cimini", "Giovanni Palermo"], "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for\nnatural language generation, with applications spanning from content creation\nto social simulations. Their ability to mimic human interactions raises both\nopportunities and concerns, particularly in the context of politically relevant\nonline discussions. In this study, we evaluate the performance of LLMs in\nreplicating user-generated content within a real-world, divisive scenario:\nReddit conversations during the 2016 US Presidential election. In particular,\nwe conduct three different experiments, asking GPT-4 to generate comments by\nimpersonating either real or artificial partisan users. We analyze the\ngenerated comments in terms of political alignment, sentiment, and linguistic\nfeatures, comparing them against real user contributions and benchmarking\nagainst a null model. We find that GPT-4 is able to produce realistic comments,\nboth in favor of or against the candidate supported by the community, yet\ntending to create consensus more easily than dissent. In addition we show that\nreal and artificial comments are well separated in a semantically embedded\nspace, although they are indistinguishable by manual inspection. Our findings\nprovide insights on the potential use of LLMs to sneak into online discussions,\ninfluence political debate and shape political narratives, bearing broader\nimplications of AI-driven discourse manipulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21620v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21620v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21622", "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "summary": "Speech impairments caused by conditions such as cerebral palsy or genetic\ndisorders pose significant challenges for automatic speech recognition (ASR)\nsystems. Despite recent advances, ASR models like Whisper struggle with\nnon-normative speech due to limited training data and the difficulty of\ncollecting and annotating non-normative speech samples. In this work, we\npropose a practical and lightweight pipeline to personalize ASR models,\nformalizing the selection of words and enriching a small, speech-impaired\ndataset with semantic coherence. Applied to data from a child with a structural\nspeech impairment, our approach shows promising improvements in transcription\nquality, demonstrating the potential to reduce communication barriers for\nindividuals with atypical speech patterns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21622v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21622v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22087", "title": "An Introduction to Zero-Order Optimization Techniques for Robotics", "authors": ["Armand Jordana", "Jianghan Zhang", "Joseph Amigo", "Ludovic Righetti"], "summary": "Zero-order optimization techniques are becoming increasingly popular in\nrobotics due to their ability to handle non-differentiable functions and escape\nlocal minima. These advantages make them particularly useful for trajectory\noptimization and policy optimization. In this work, we propose a mathematical\ntutorial on random search. It offers a simple and unifying perspective for\nunderstanding a wide range of algorithms commonly used in robotics. Leveraging\nthis viewpoint, we classify many trajectory optimization methods under a common\nframework and derive novel competitive RL algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22087v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22087v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21582v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21582v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21826", "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "comment": "18 pages, accepted at ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.21826v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21826v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22276", "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates", "authors": ["Reuth Mirsky"], "summary": "Artificial intelligence has made remarkable strides in recent years,\nachieving superhuman performance across a wide range of tasks. Yet despite\nthese advances, most cooperative AI systems remain rigidly obedient, designed\nto follow human instructions without question and conform to user expectations,\neven when doing so may be counterproductive or unsafe. This paper argues for\nexpanding the agency of AI teammates to include \\textit{intelligent\ndisobedience}, empowering them to make meaningful and autonomous contributions\nwithin human-AI teams. It introduces a scale of AI agency levels and uses\nrepresentative examples to highlight the importance and growing necessity of\ntreating AI autonomy as an independent research focus in cooperative settings.\nThe paper then explores how intelligent disobedience manifests across different\nautonomy levels and concludes by proposing initial boundaries and\nconsiderations for studying disobedience as a core capability of artificial\nagents.", "comment": "Extended version of a paper accepted for publication in AI Magazine", "pdf_url": "http://arxiv.org/pdf/2506.22276v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22276v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21732", "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "authors": ["Ameya Salvi", "Venkat Krovi"], "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21732v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21732v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22277", "title": "A Self-scaled Approximate $\\ell_0$ Regularization Robust Model for Outlier Detection", "authors": ["Pengyang Song", "Jue Wang"], "summary": "Robust regression models in the presence of outliers have significant\npractical relevance in areas such as signal processing, financial econometrics,\nand energy management. Many existing robust regression methods, either grounded\nin statistical theory or sparse signal recovery, typically rely on the explicit\nor implicit assumption of outlier sparsity to filter anomalies and recover the\nunderlying signal or data. However, these methods often suffer from limited\nrobustness or high computational complexity, rendering them inefficient for\nlarge-scale problems. In this work, we propose a novel robust regression model\nbased on a Self-scaled Approximate l0 Regularization Model (SARM) scheme. By\nintroducing a self-scaling mechanism into the regularization term, the proposed\nmodel mitigates the negative impact of uneven or excessively large outlier\nmagnitudes on robustness. We also develop an alternating minimization algorithm\ngrounded in Proximal Operators and Block Coordinate Descent. We rigorously\nprove the algorithm convergence. Empirical comparisons with several\nstate-of-the-art robust regression methods demonstrate that SARM not only\nachieves superior robustness but also significantly improves computational\nefficiency. Motivated by both the theoretical error bound and empirical\nobservations, we further design a Two-Stage SARM (TSSARM) framework, which\nbetter utilizes sample information when the singular values of the design\nmatrix are widely spread, thereby enhancing robustness under certain\nconditions. Finally, we validate our approach on a real-world load forecasting\ntask. The experimental results show that our method substantially enhances the\nrobustness of load forecasting against adversarial data attacks, which is\nincreasingly critical in the era of heightened data security concerns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22277v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22277v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21851", "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "summary": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "comment": "IEEE International Conference on Systems, Man, and Cybernetics 2025.\n  (SMC), under review", "pdf_url": "http://arxiv.org/pdf/2506.21851v1", "categories": ["cs.CV", "cs.MM", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21851v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21990", "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "pdf_url": "http://arxiv.org/pdf/2506.21990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21990v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21567", "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21567v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21567v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21934", "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design", "authors": ["Najmeh Forouzandehmehr", "Reza Yousefi Maragheh", "Sriram Kollipara", "Kai Zhao", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21934v1", "categories": ["cs.IR", "cs.CV", "I.3.3; I.2.11; H.5.2"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21934v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21872", "title": "A Survey of Continual Reinforcement Learning", "authors": ["Chaofan Pan", "Xin Yang", "Yanhua Li", "Wei Wei", "Tianrui Li", "Bo An", "Jiye Liang"], "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for\nsolving sequential decision-making problems. Recent years have witnessed\nremarkable progress in this field due to the rapid development of deep neural\nnetworks. However, the success of RL currently relies on extensive training\ndata and computational resources. In addition, RL's limited ability to\ngeneralize across tasks restricts its applicability in dynamic and real-world\nenvironments. With the arisen of Continual Learning (CL), Continual\nReinforcement Learning (CRL) has emerged as a promising research direction to\naddress these limitations by enabling agents to learn continuously, adapt to\nnew tasks, and retain previously acquired knowledge. In this survey, we provide\na comprehensive examination of CRL, focusing on its core concepts, challenges,\nand methodologies. Firstly, we conduct a detailed review of existing works,\norganizing and analyzing their metrics, tasks, benchmarks, and scenario\nsettings. Secondly, we propose a new taxonomy of CRL methods, categorizing them\ninto four types from the perspective of knowledge storage and/or transfer.\nFinally, our analysis highlights the unique challenges of CRL and provides\npractical insights into future directions.", "comment": "This work has been submitted to the IEEE TPAMI", "pdf_url": "http://arxiv.org/pdf/2506.21872v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21872v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21723", "title": "Dynamic Bayesian Item Response Model with Decomposition (D-BIRD): Modeling Cohort and Individual Learning Over Time", "authors": ["Hansol Lee", "Jason B. Cho", "David S. Matteson", "Benjamin W. Domingue"], "summary": "We present D-BIRD, a Bayesian dynamic item response model for estimating\nstudent ability from sparse, longitudinal assessments. By decomposing ability\ninto a cohort trend and individual trajectory, D-BIRD supports interpretable\nmodeling of learning over time. We evaluate parameter recovery in simulation\nand demonstrate the model using real-world personalized learning data.", "comment": "Submitted to the NCME Special Conference: Artificial Intelligence in\n  Measurement and Education Conference (AIME-Con)", "pdf_url": "http://arxiv.org/pdf/2506.21723v1", "categories": ["stat.AP", "cs.CY", "stat.ME"], "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.21723v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21712", "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers", "authors": ["Tzu-Quan Lin", "Hsi-Chun Cheng", "Hung-yi Lee", "Hao Tang"], "summary": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21712v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21712v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22116", "title": "Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration", "authors": ["Noora Sassali", "Roel Pieters"], "summary": "Pointing gestures are a common interaction method used in Human-Robot\nCollaboration for various tasks, ranging from selecting targets to guiding\nindustrial processes. This study introduces a method for localizing pointed\ntargets within a planar workspace. The approach employs pose estimation, and a\nsimple geometric model based on shoulder-wrist extension to extract gesturing\ndata from an RGB-D stream. The study proposes a rigorous methodology and\ncomprehensive analysis for evaluating pointing gestures and target selection in\ntypical robotic tasks. In addition to evaluating tool accuracy, the tool is\nintegrated into a proof-of-concept robotic system, which includes object\ndetection, speech transcription, and speech synthesis to demonstrate the\nintegration of multiple modalities in a collaborative application. Finally, a\ndiscussion over tool limitations and performance is provided to understand its\nrole in multimodal robotic systems. All developments are available at:\nhttps://github.com/NMKsas/gesture_pointer.git.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). Preprint", "pdf_url": "http://arxiv.org/pdf/2506.22116v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22116v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21604", "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "pdf_url": "http://arxiv.org/pdf/2506.21604v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21604v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21832", "title": "TaleForge: Interactive Multimodal System for Personalized Story Creation", "authors": ["Minh-Loi Nguyen", "Quang-Khai Le", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Storytelling is a deeply personal and creative process, yet existing methods\noften treat users as passive consumers, offering generic plots with limited\npersonalization. This undermines engagement and immersion, especially where\nindividual style or appearance is crucial. We introduce TaleForge, a\npersonalized story-generation system that integrates large language models\n(LLMs) and text-to-image diffusion to embed users' facial images within both\nnarratives and illustrations. TaleForge features three interconnected modules:\nStory Generation, where LLMs create narratives and character descriptions from\nuser prompts; Personalized Image Generation, merging users' faces and outfit\nchoices into character illustrations; and Background Generation, creating scene\nbackdrops that incorporate personalized characters. A user study demonstrated\nheightened engagement and ownership when individuals appeared as protagonists.\nParticipants praised the system's real-time previews and intuitive controls,\nthough they requested finer narrative editing tools. TaleForge advances\nmultimodal storytelling by aligning personalized text and imagery to create\nimmersive, user-centric experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21832v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21832v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22309", "title": "Conceptual Topic Aggregation", "authors": ["Klara M. Gutekunst", "Dominik Dürrschnabel", "Johannes Hirth", "Gerd Stumme"], "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "pdf_url": "http://arxiv.org/pdf/2506.22309v1", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22309v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21982", "title": "A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments", "authors": ["Akshay Jaitly", "Jack Cline", "Siavash Farzan"], "summary": "We propose a mixed-integer linear program (MILP) for multi-agent motion\nplanning that embeds Polytopic Action-based Motion Planning (PAAMP) into a\nsequence-then-solve pipeline. Region sequences confine each agent to adjacent\nconvex polytopes, while a big-M hyperplane model enforces inter-agent\nseparation. Collision constraints are applied only to agents sharing or\nneighboring a region, which reduces binary variables exponentially compared\nwith naive formulations. An L1 path-length-plus-acceleration cost yields smooth\ntrajectories. We prove finite-time convergence and demonstrate on\nrepresentative multi-agent scenarios with obstacles that our formulation\nproduces collision-free trajectories an order of magnitude faster than an\nunstructured MILP baseline.", "comment": "Accepted to 2025 IEEE International Conference on Automation Science\n  and Engineering (CASE 2025)", "pdf_url": "http://arxiv.org/pdf/2506.21982v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21982v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22411", "title": "19.3 GHz Acoustic Filter with High Close-in Rejection in Tri-layer Thin-Film Lithium Niobate", "authors": ["Omar Barrera", "Sinwoo Cho", "Jack Kramer", "Vakhtang Chulukhadze", "Tzu-Hsuan Hsu", "Ruochen Lu"], "summary": "Acoustic filters are preferred front-end solutions at sub-6 GHz due to their\nsuperior frequency selectivity compared to electromagnetic (EM) counterparts.\nWith the ongoing development of 5G and the evolution toward 6G, there is a\ngrowing need to extend acoustic filter technologies into frequency range 3\n(FR3), which spans 7 to 24 GHz to accommodate emerging high-frequency bands.\nHowever, scaling acoustic filters beyond 10 GHz presents significant\nchallenges, as conventional platforms suffer from increased insertion loss (IL)\nand degraded out-of-band (OoB) rejection at higher frequencies. Recent\ninnovations have led to the emergence of periodically poled piezoelectric\nlithium niobate (P3F LN) laterally excited bulk acoustic resonators (XBARs),\noffering low-loss and high electromechanical coupling performance above 10 GHz.\nThis work presents the first tri-layer P3F LN filter operating at 19.3 GHz,\nachieving a low IL of 2.2 dB, a 3-dB fractional bandwidth (FBW) of 8.5%, and an\nimpressive 49 dB close in rejection. These results demonstrate strong potential\nfor integration into FR3 diplexers.", "comment": "4 Pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22411v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22411v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21900", "title": "TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments", "authors": ["Sheng Yun", "Jianhua Pei", "Ping Wang"], "summary": "The evolution toward 6G networks demands a fundamental shift from bit-centric\ntransmission to semantic-aware communication that emphasizes task-relevant\ninformation. This work introduces TOAST (Task-Oriented Adaptive Semantic\nTransmission), a unified framework designed to address the core challenge of\nmulti-task optimization in dynamic wireless environments through three\ncomplementary components. First, we formulate adaptive task balancing as a\nMarkov decision process, employing deep reinforcement learning to dynamically\nadjust the trade-off between image reconstruction fidelity and semantic\nclassification accuracy based on real-time channel conditions. Second, we\nintegrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our\nSwin Transformer-based joint source-channel coding architecture, enabling\nparameter-efficient fine-tuning that dramatically reduces adaptation overhead\nwhile maintaining full performance across diverse channel impairments including\nAdditive White Gaussian Noise (AWGN), fading, phase noise, and impulse\ninterference. Third, we incorporate an Elucidating diffusion model that\noperates in the latent space to restore features corrupted by channel noises,\nproviding substantial quality improvements compared to baseline approaches.\nExtensive experiments across multiple datasets demonstrate that TOAST achieves\nsuperior performance compared to baseline approaches, with significant\nimprovements in both classification accuracy and reconstruction quality at low\nSignal-to-Noise Ratio (SNR) conditions while maintaining robust performance\nacross all tested scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21900v1", "categories": ["cs.LG", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21900v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22023", "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.", "comment": "17 pages, 8 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.22023v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22023v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21568", "title": "Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion", "authors": ["Andrejs Sorstkins"], "summary": "Resource efficiency is a critical barrier to deploying large language models\n(LLMs) in edge and privacy-sensitive applications. This study evaluates the\nefficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)\nand Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion\nand 4 billion parameters, within the context of a privacy-first personal\nassistant. We implement short-term memory via MongoDB and long-term semantic\nstorage via Qdrant, orchestrated through FastAPI and LangChain, and expose the\nsystem through a React.js frontend. Across both model scales, RAG consistently\nreduces latency by up to 17\\% and eliminates factual hallucinations when\nresponding to user-specific and domain-specific queries. HyDE, by contrast,\nenhances semantic relevance--particularly for complex physics prompts--but\nincurs a 25--40\\% increase in response time and a non-negligible hallucination\nrate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that\nscaling yields marginal throughput gains for baseline and RAG pipelines, but\nmagnifies HyDE's computational overhead and variability. Our findings position\nRAG as the pragmatic choice for on-device personal assistants powered by\nsmall-scale LLMs.", "comment": "Technical report as part of research project", "pdf_url": "http://arxiv.org/pdf/2506.21568v1", "categories": ["cs.CL", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21568v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22026", "title": "Literature-Grounded Novelty Assessment of Scientific Ideas", "authors": ["Simra Shahid", "Marissa Radensky", "Raymond Fok", "Pao Siangliulue", "Daniel S. Weld", "Tom Hope"], "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22026v1", "categories": ["cs.IR", "cs.AI", "I.2; H.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22026v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21899", "title": "Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review", "authors": ["Amara Zuffer", "Michael Burke", "Mehrtash Harandi"], "summary": "The diversity of tasks and dynamic nature of reinforcement learning (RL)\nrequire RL agents to be able to learn sequentially and continuously, a learning\nparadigm known as continuous reinforcement learning. This survey reviews how\ncontinual learning transforms RL agents into dynamic continual learners. This\nenables RL agents to acquire and retain useful and reusable knowledge\nseamlessly. The paper delves into fundamental aspects of continual\nreinforcement learning, exploring key concepts, significant challenges, and\nnovel methodologies. Special emphasis is placed on recent advancements in\ncontinual reinforcement learning within robotics, along with a succinct\noverview of evaluation environments utilized in prominent research,\nfacilitating accessibility for newcomers to the field. The review concludes\nwith a discussion on limitations and promising future directions, providing\nvaluable insights for researchers and practitioners alike.", "comment": "65 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.21899v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21899v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21914", "title": "Consumer Beware! Exploring Data Brokers' CCPA Compliance", "authors": ["Elina van Kempen", "Isita Bagayatkar", "Pavel Frolikov", "Chloe Georgiou", "Gene Tsudik"], "summary": "Data brokers collect and sell the personal information of millions of\nindividuals, often without their knowledge or consent. The California Consumer\nPrivacy Act (CCPA) grants consumers the legal right to request access to, or\ndeletion of, their data. To facilitate these requests, California maintains an\nofficial registry of data brokers. However, the extent to which these entities\ncomply with the law is unclear.\n  This paper presents the first large-scale, systematic study of CCPA\ncompliance of all 543 officially registered data brokers. Data access requests\nwere manually submitted to each broker, followed by in-depth analyses of their\nresponses (or lack thereof). Above 40% failed to respond at all, in an apparent\nviolation of the CCPA. Data brokers that responded requested personal\ninformation as part of their identity verification process, including details\nthey had not previously collected. Paradoxically, this means that exercising\none's privacy rights under CCPA introduces new privacy risks.\n  Our findings reveal rampant non-compliance and lack of standardization of the\ndata access request process. These issues highlight an urgent need for stronger\nenforcement, clearer guidelines, and standardized, periodic compliance checks\nto enhance consumers' privacy protections and improve data broker\naccountability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21914v1", "categories": ["cs.CR", "cs.CY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21914v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21921", "title": "Explainable anomaly detection for sound spectrograms using pooling statistics with quantile differences", "authors": ["Nicolas Thewes", "Philipp Steinhauer", "Patrick Trampert", "Markus Pauly", "Georg Schneider"], "summary": "Anomaly detection is the task of identifying rarely occurring (i.e. anormal\nor anomalous) samples that differ from almost all other samples in a dataset.\nAs the patterns of anormal samples are usually not known a priori, this task is\nhighly challenging. Consequently, anomaly detection lies between semi- and\nunsupervised learning. The detection of anomalies in sound data, often called\n'ASD' (Anomalous Sound Detection), is a sub-field that deals with the\nidentification of new and yet unknown effects in acoustic recordings. It is of\ngreat importance for various applications in Industry 4.0. Here, vibrational or\nacoustic data are typically obtained from standard sensor signals used for\npredictive maintenance. Examples cover machine condition monitoring or quality\nassurance to track the state of components or products. However, the use of\nintelligent algorithms remains a controversial topic. Management generally aims\nfor cost-reduction and automation, while quality and maintenance experts\nemphasize the need for human expertise and comprehensible solutions. In this\nwork, we present an anomaly detection approach specifically designed for\nspectrograms. The approach is based on statistical evaluations and is\ntheoretically motivated. In addition, it features intrinsic explainability,\nmaking it particularly suitable for applications in industrial settings. Thus,\nthis algorithm is of relevance for applications in which black-box algorithms\nare unwanted or unsuitable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21921v1", "categories": ["stat.AP", "cs.SD", "eess.AS", "stat.CO", "62", "G.3"], "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.21921v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22170", "title": "RM-Dijkstra: A surface optimal path planning algorithm based on Riemannian metric", "authors": ["Yu Zhang", "Xiao-Song Yang"], "summary": "The Dijkstra algorithm is a classic path planning method, which operates in a\ndiscrete graph space to determine the shortest path from a specified source\npoint to a target node or all other nodes based on non-negative edge weights.\nNumerous studies have focused on the Dijkstra algorithm due to its potential\napplication. However, its application in surface path planning for mobile\nrobots remains largely unexplored. In this letter, a surface optimal path\nplanning algorithm called RM-Dijkstra is proposed, which is based on Riemannian\nmetric model. By constructing a new Riemannian metric on the 2D projection\nplane, the surface optimal path planning problem is therefore transformed into\na geometric problem on the 2D plane with new Riemannian metric. Induced by the\nstandard Euclidean metric on surface, the constructed new metric reflects\nenvironmental information of the robot and ensures that the projection map is\nan isometric immersion. By conducting a series of simulation tests, the\nexperimental results demonstrate that the RM-Dijkstra algorithm not only\neffectively solves the optimal path planning problem on surfaces, but also\noutperforms traditional path planning algorithms in terms of path accuracy and\nsmoothness, particularly in complex scenarios.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22170v1", "categories": ["cs.RO", "math.OC", "00A69, 93C85, 14H55", "I.2.9"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22170v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21819", "title": "SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge", "authors": ["Lena John", "Kheir Eddine Farfar", "Sören Auer", "Oliver Karras"], "summary": "Scientific publications, primarily digitized as PDFs, remain static and\nunstructured, limiting the accessibility and reusability of the contained\nknowledge. At best, scientific knowledge from publications is provided in\ntabular formats, which lack semantic context. A more flexible, structured, and\nsemantic representation is needed to make scientific knowledge understandable\nand processable by both humans and machines. We propose an evolution model of\nknowledge representation, inspired by the 5-star Linked Open Data (LOD) model,\nwith five stages and defined criteria to guide the stepwise transition from a\ndigital artifact, such as a PDF, to a semantic representation integrated in a\nknowledge graph (KG). Based on an exemplary workflow implementing the entire\nmodel, we developed a hybrid approach, called SciMantify, leveraging tabular\nformats of scientific knowledge, e.g., results from secondary studies, to\nsupport its evolving semantification. In the approach, humans and machines\ncollaborate closely by performing semantic annotation tasks (SATs) and refining\nthe results to progressively improve the semantic representation of scientific\nknowledge. We implemented the approach in the Open Research Knowledge Graph\n(ORKG), an established platform for improving the findability, accessibility,\ninteroperability, and reusability of scientific knowledge. A preliminary user\nexperiment showed that the approach simplifies the preprocessing of scientific\nknowledge, reduces the effort for the evolving semantification, and enhances\nthe knowledge representation through better alignment with the KG structures.", "comment": "Accepted at the 25th International Conference on Web Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2506.21819v1", "categories": ["cs.DL", "cs.AI", "cs.HC"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.21819v1", "date": "2025-04-14", "updated": "2025-04-14"}
{"id": "2506.21834", "title": "PrefPaint: Enhancing Image Inpainting through Expert Human Feedback", "authors": ["Duy-Bao Bui", "Hoang-Khang Nguyen", "Trung-Nghia Le"], "summary": "Inpainting, the process of filling missing or corrupted image parts, has\nbroad applications, including medical imaging. However, in specialized fields\nlike medical polyps imaging, where accuracy and reliability are critical,\ninpainting models can generate inaccurate images, leading to significant errors\nin medical diagnosis and treatment. To ensure reliability, medical images\nshould be annotated by experts like oncologists for effective model training.\nWe propose PrefPaint, an approach that incorporates human feedback into the\ntraining process of Stable Diffusion Inpainting, bypassing the need for\ncomputationally expensive reward models. In addition, we develop a web-based\ninterface streamlines training, fine-tuning, and inference. This interactive\ninterface provides a smooth and intuitive user experience, making it easier to\noffer feedback and manage the fine-tuning process. User study on various\ndomains shows that PrefPaint outperforms existing methods, reducing visual\ninconsistencies and improving image rendering, particularly in medical\ncontexts, where our model generates more realistic polyps images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21834v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21834v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22355", "title": "Embodied AI Agents: Modeling the World", "authors": ["Pascale Fung", "Yoram Bachrach", "Asli Celikyilmaz", "Kamalika Chaudhuri", "Delong Chen", "Willy Chung", "Emmanuel Dupoux", "Hervé Jégou", "Alessandro Lazaric", "Arjun Majumdar", "Andrea Madotto", "Franziska Meier", "Florian Metze", "Théo Moutakanni", "Juan Pino", "Basile Terver", "Joseph Tighe", "Jitendra Malik"], "summary": "This paper describes our research on AI agents embodied in visual, virtual or\nphysical forms, enabling them to interact with both users and their\nenvironments. These agents, which include virtual avatars, wearable devices,\nand robots, are designed to perceive, learn and act within their surroundings,\nwhich makes them more similar to how humans learn and interact with the\nenvironments as compared to disembodied agents. We propose that the development\nof world models is central to reasoning and planning of embodied AI agents,\nallowing these agents to understand and predict their environment, to\nunderstand user intentions and social contexts, thereby enhancing their ability\nto perform complex tasks autonomously. World modeling encompasses the\nintegration of multimodal perception, planning through reasoning for action and\ncontrol, and memory to create a comprehensive understanding of the physical\nworld. Beyond the physical world, we also propose to learn the mental world\nmodel of users to enable better human-agent collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22355v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22355v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22185", "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22185v1", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22185v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21828", "title": "Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification", "authors": ["Weitao Tang", "Johann Vargas-Calixto", "Nasim Katebi", "Robert Galinsky", "Gari D. Clifford", "Faezeh Marzbanrad"], "summary": "Fetal sleep is a relatively underexplored yet vital aspect of prenatal\nneurodevelopment. Understanding fetal sleep patterns could provide insights\ninto early brain maturation and help clinicians detect signs of neurological\ncompromise that arise due to fetal hypoxia or fetal growth restriction. This\nreview synthesizes over eight decades of research on the physiological\ncharacteristics, ontogeny, and regulation of fetal sleep. We compare\nsleep-state patterns in humans and large animal models, highlighting\nspecies-specific differences and the presence of sleep-state analogs. We review\nboth invasive techniques in animals and non-invasive modalities in humans.\nComputational methods for sleep-state classification are also examined,\nincluding rule-based approaches (with and without clustering-based\npreprocessing) and state-of-the-art deep learning techniques. Finally, we\ndiscuss how intrauterine conditions such as hypoxia and fetal growth\nrestriction can disrupt fetal sleep. This review provides a comprehensive\nfoundation for the development of objective, multimodal, and non-invasive fetal\nsleep monitoring technologies to support early diagnosis and intervention in\nprenatal care.", "comment": "Review article, 17 pages, 1 figure, 5 tables, submitted to Sleep\n  (under review)", "pdf_url": "http://arxiv.org/pdf/2506.21828v1", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.21828v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22216", "title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "summary": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement.", "comment": "6 pages, 8 figures, accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2506.22216v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22216v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22143", "title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "summary": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively.", "comment": "Accepted for IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2506.22143v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22143v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21569", "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA", "authors": ["Weihua Xiao", "Derek Ekberg", "Siddharth Garg", "Ramesh Karri"], "summary": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of\nhardware designs, but manually writing them from natural language property\ndescriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.\nRecent advances in large language models (LLMs) offer opportunities to automate\nthis translation. However, existing models still struggle with understanding\ndomain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we\npropose a customized retrieval-augmented generation (RAG) framework and a\nsynthetic fine-tuning dataset that together improve LLM's performance. To\nfurther improve lightweight models over NL2SVA, our fine-tuning dataset\nprovides prompt-guided explanations that teach LLMs the layer-by-layer\nconstruction process of concurrent SVAs, enabling supervised fine-tuning that\ngreatly improves syntax and functionality accuracy. To evaluate the performance\nof LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,\ncomprising 40 Verilog designs and 229 formally verified SVAs with detailed\nannotations. Experimental results show that our customized RAG framework\nincreases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,\nwhile Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and\nintegrated with HybridRetrieval achieves a 59.05% over the base Qwen model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21569v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21569v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22112", "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems", "authors": ["Wenzheng Shu", "Yanxiang Zeng", "Yongxiang Tang", "Teng Sha", "Ning Luo", "Yanhua Cheng", "Xialong Liu", "Fan Zhou", "Peng Jiang"], "summary": "Offline reinforcement learning (RL) has emerged as a prevalent and effective\nmethodology for real-world recommender systems, enabling learning policies from\nhistorical data and capturing user preferences. In offline RL, reward shaping\nencounters significant challenges, with past efforts to incorporate prior\nstrategies for uncertainty to improve world models or penalize underexplored\nstate-action pairs. Despite these efforts, a critical gap remains: the\nsimultaneous balancing of intrinsic biases in world models and the diversity of\npolicy recommendations. To address this limitation, we present an innovative\noffline RL framework termed Reallocated Reward for Recommender Systems (R3S).\nBy integrating inherent model uncertainty to tackle the intrinsic fluctuations\nin reward predictions, we boost diversity for decision-making to align with a\nmore interactive paradigm, incorporating extra penalizers with decay that deter\nactions leading to diminished state variety at both local and global scales.\nThe experimental results demonstrate that R3S improves the accuracy of world\nmodels and efficiently harmonizes the heterogeneous preferences of the users.", "comment": "Accepted in Companion Proceedings of the ACM Web Conference 2025", "pdf_url": "http://arxiv.org/pdf/2506.22112v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22112v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21900", "title": "TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments", "authors": ["Sheng Yun", "Jianhua Pei", "Ping Wang"], "summary": "The evolution toward 6G networks demands a fundamental shift from bit-centric\ntransmission to semantic-aware communication that emphasizes task-relevant\ninformation. This work introduces TOAST (Task-Oriented Adaptive Semantic\nTransmission), a unified framework designed to address the core challenge of\nmulti-task optimization in dynamic wireless environments through three\ncomplementary components. First, we formulate adaptive task balancing as a\nMarkov decision process, employing deep reinforcement learning to dynamically\nadjust the trade-off between image reconstruction fidelity and semantic\nclassification accuracy based on real-time channel conditions. Second, we\nintegrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our\nSwin Transformer-based joint source-channel coding architecture, enabling\nparameter-efficient fine-tuning that dramatically reduces adaptation overhead\nwhile maintaining full performance across diverse channel impairments including\nAdditive White Gaussian Noise (AWGN), fading, phase noise, and impulse\ninterference. Third, we incorporate an Elucidating diffusion model that\noperates in the latent space to restore features corrupted by channel noises,\nproviding substantial quality improvements compared to baseline approaches.\nExtensive experiments across multiple datasets demonstrate that TOAST achieves\nsuperior performance compared to baseline approaches, with significant\nimprovements in both classification accuracy and reconstruction quality at low\nSignal-to-Noise Ratio (SNR) conditions while maintaining robust performance\nacross all tested scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21900v1", "categories": ["cs.LG", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21900v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22098", "title": "Involvement drives complexity of language in online debates", "authors": ["Eleonora Amadori", "Daniele Cirulli", "Edoardo Di Martino", "Jacopo Nudo", "Maria Sahakyan", "Emanuele Sangiorgio", "Arnaldo Santoro", "Simon Zollo", "Alessandro Galeazzi", "Niccolò Di Marco"], "summary": "Language is a fundamental aspect of human societies, continuously evolving in\nresponse to various stimuli, including societal changes and intercultural\ninteractions. Technological advancements have profoundly transformed\ncommunication, with social media emerging as a pivotal force that merges\nentertainment-driven content with complex social dynamics. As these platforms\nreshape public discourse, analyzing the linguistic features of user-generated\ncontent is essential to understanding their broader societal impact. In this\npaper, we examine the linguistic complexity of content produced by influential\nusers on Twitter across three globally significant and contested topics:\nCOVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of\ntextual complexity, we assess how language use varies along four key\ndimensions: account type, political leaning, content reliability, and\nsentiment. Our analysis reveals significant differences across all four axes,\nincluding variations in language complexity between individuals and\norganizations, between profiles with sided versus moderate political views, and\nbetween those associated with higher versus lower reliability scores.\nAdditionally, profiles producing more negative and offensive content tend to\nuse more complex language, with users sharing similar political stances and\nreliability levels converging toward a common jargon. Our findings offer new\ninsights into the sociolinguistic dynamics of digital platforms and contribute\nto a deeper understanding of how language reflects ideological and social\nstructures in online spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22098v1", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22098v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22001", "title": "WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation", "authors": ["Lu Han", "Junqi Zhao", "Renhua Peng"], "summary": "Current multi-channel speech enhancement systems mainly adopt single-output\narchitecture, which face significant challenges in preserving spatio-temporal\nsignal integrity during multiple-input multiple-output (MIMO) processing. To\naddress this limitation, we propose a novel neural network, termed WTFormer,\nfor MIMO speech enhancement that leverages the multi-resolution characteristics\nof wavelet transform and multi-dimensional collaborative attention to\neffectively capture globally distributed spatial features, while using\nConformer for time-frequency modeling. A multi task loss strategy accompanying\nMUSIC algorithm is further proposed for optimization training to protect\nspatial information to the greatest extent. Experimental results on the\nLibriSpeech dataset show that WTFormer can achieve comparable denoising\nperformance to advanced systems while preserving more spatial information with\nonly 0.98M parameters.", "comment": "Accepted by Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2506.22001v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22001v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22174", "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research", "authors": ["Bavo Lesy", "Siemen Herremans", "Robin Kerstens", "Jan Steckel", "Walter Daems", "Siegfried Mercelis", "Ali Anwar"], "summary": "The transport industry has recently shown significant interest in unmanned\nsurface vehicles (USVs), specifically for port and inland waterway transport.\nThese systems can improve operational efficiency and safety, which is\nespecially relevant in the European Union, where initiatives such as the Green\nDeal are driving a shift towards increased use of inland waterways. At the same\ntime, a shortage of qualified personnel is accelerating the adoption of\nautonomous solutions. However, there is a notable lack of open-source,\nhigh-fidelity simulation frameworks and datasets for developing and evaluating\nsuch solutions. To address these challenges, we introduce AirSim For Surface\nVehicles (ASVSim), an open-source simulation framework specifically designed\nfor autonomous shipping research in inland and port environments. The framework\ncombines simulated vessel dynamics with marine sensor simulation capabilities,\nincluding radar and camera systems and supports the generation of synthetic\ndatasets for training computer vision models and reinforcement learning agents.\nBuilt upon Cosys-AirSim, ASVSim provides a comprehensive platform for\ndeveloping autonomous navigation algorithms and generating synthetic datasets.\nThe simulator supports research of both traditional control methods and deep\nlearning-based approaches. Through limited experiments, we demonstrate the\npotential of the simulator in these research areas. ASVSim is provided as an\nopen-source project under the MIT license, making autonomous navigation\nresearch accessible to a larger part of the ocean engineering community.", "comment": "14 Pages, 11 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22174v1", "categories": ["cs.RO", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22174v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21862", "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "comment": "21 pages, 4 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2506.21862v1", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21862v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21835", "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts", "authors": ["Xiaoqi Wang", "Clint Sebastian", "Wenbin He", "Liu Ren"], "summary": "The recent advancements in large foundation models have driven the success of\nopen-set image segmentation, a task focused on segmenting objects beyond\npredefined categories. Among various prompt types (such as points, boxes,\ntexts, and visual references), visual reference segmentation stands out for its\nunique flexibility and strong zero-shot capabilities. Recently, several\nSAM-based methods have made notable progress in this task by automatically\ngenerating prompts to guide SAM. However, these methods often generate prompts\nat object boundaries due to suboptimal prompt encoder, which results in\ninstability and reduced robustness. In this work, we introduce ProSAM, a simple\nbut effective method to address the stability challenges we identified in\nexisting SAM-based visual reference segmentation approaches. By learning a\nvariational prompt encoder to predict multivariate prompt distributions, ProSAM\navoids generating prompts that lie in unstable regions, overcoming the\ninstability caused by less robust prompts. Our approach consistently surpasses\nstate-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,\nproviding a more robust solution for visual reference segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21835v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21835v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22358", "title": "AI Model Passport: Data and System Traceability Framework for Transparent AI in Health", "authors": ["Varvara Kalokyri", "Nikolaos S. Tachos", "Charalampos N. Kalantzopoulos", "Stelios Sfakianakis", "Haridimos Kondylakis", "Dimitrios I. Zaridis", "Sara Colantonio", "Daniele Regge", "Nikolaos Papanikolaou", "The ProCAncer-I consortium", "Konstantinos Marias", "Dimitrios I. Fotiadis", "Manolis Tsiknakis"], "summary": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22358v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22358v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22293", "title": "The Effect of Network Topology on the Equilibria of Influence-Opinion Games", "authors": ["Yigit Ege Bayiz", "Arash Amini", "Radu Marculescu", "Ufuk Topcu"], "summary": "Online social networks exert a powerful influence on public opinion.\nAdversaries weaponize these networks to manipulate discourse, underscoring the\nneed for more resilient social networks. To this end, we investigate the impact\nof network connectivity on Stackelberg equilibria in a two-player game to shape\npublic opinion. We model opinion evolution as a repeated competitive\ninfluence-propagation process. Players iteratively inject \\textit{messages}\nthat diffuse until reaching a steady state, modeling the dispersion of two\ncompeting messages. Opinions then update according to the discounted sum of\nexposure to the messages. This bi-level model captures viral-media correlation\neffects omitted by standard opinion-dynamics models. To solve the resulting\nhigh-dimensional game, we propose a scalable, iterative algorithm based on\nlinear-quadratic regulators that approximates local feedback Stackelberg\nstrategies for players with limited cognition. We analyze how the network\ntopology shapes equilibrium outcomes through experiments on synthetic networks\nand real Facebook data. Our results identify structural characteristics that\nimprove a network's resilience to adversarial influence, guiding the design of\nmore resilient social networks.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.22293v1", "categories": ["cs.SI", "cs.SY", "eess.SY", "91D30, 91D10"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22293v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21884", "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "comment": "Paper accepted at ICCV 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2506.21884v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21884v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22237", "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.", "comment": "9 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.22237v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22237v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21570", "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21570v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21570v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22210", "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses", "authors": ["Weronika Łajewska", "Ivica Kostric", "Gabriel Iturra-Bocaz", "Mariam Arustashvili", "Krisztian Balog"], "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22210v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22210v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21937", "title": "HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "summary": "We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain\ntumor classification using MRI images. Trained on a dataset of 7,576 scans\ncovering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC\nintegrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized\nvia AdamW and a composite loss blending cross-entropy and attention\nconsistency.\n  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical\nbaseline (86.72%). It delivers higher precision and F1-scores, especially for\nglioma detection. t-SNE projections reveal enhanced feature separability in\nquantum space, and confusion matrices show lower misclassification. Attention\nmap analysis (Jaccard Index) confirms more accurate and focused tumor\nlocalization at high-confidence thresholds.\n  These results highlight the promise of quantum-enhanced models in medical\nimaging, advancing both diagnostic accuracy and interpretability for clinical\nbrain tumor assessment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21937v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21937v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22224", "title": "A Decade of News Forum Interactions: Threaded Conversations, Signed Votes, and Topical Tags", "authors": ["Emma Fraxanet", "Vicenç Gómez", "Andreas Kaltenbrunner", "Max Pellert"], "summary": "We present a large-scale, longitudinal dataset capturing user activity on the\nonline platform of DerStandard, a major Austrian newspaper. The dataset spans\nten years (2013-2022) and includes over 75 million user comments, more than 400\nmillion votes, and detailed metadata on articles and user interactions. It\nprovides structured conversation threads, explicit up- and downvotes of user\ncomments and editorial topic labels, enabling rich analyses of online discourse\nwhile preserving user privacy. To ensure this privacy, all persistent\nidentifiers are anonymized using salted hash functions, and the raw comment\ntexts are not publicly shared. Instead, we release pre-computed vector\nrepresentations derived from a state-of-the-art embedding model. The dataset\nsupports research on discussion dynamics, network structures, and semantic\nanalyses in the mid-resourced language German, offering a reusable resource\nacross computational social science and related fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22224v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22224v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22143", "title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "summary": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively.", "comment": "Accepted for IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2506.22143v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22143v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22176", "title": "KnotDLO: Toward Interpretable Knot Tying", "authors": ["Holly Dinkel", "Raghavendra Navaratna", "Jingyi Xiang", "Brian Coltin", "Trey Smith", "Timothy Bretl"], "summary": "This work presents KnotDLO, a method for one-handed Deformable Linear Object\n(DLO) knot tying that is robust to occlusion, repeatable for varying rope\ninitial configurations, interpretable for generating motion policies, and\nrequires no human demonstrations or training. Grasp and target waypoints for\nfuture DLO states are planned from the current DLO shape. Grasp poses are\ncomputed from indexing the tracked piecewise linear curve representing the DLO\nstate based on the current curve shape and are piecewise continuous. KnotDLO\ncomputes intermediate waypoints from the geometry of the current DLO state and\nthe desired next state. The system decouples visual reasoning from control. In\n16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an\noverhand knot from previously unseen configurations.", "comment": "4 pages, 5 figures, presented at the Workshop on 3D Visual\n  Representations for Manipulation at the 2023 IEEE International Conference on\n  Robotics and Automation in Yokohama, Japan. Video presentation\n  [https://youtu.be/mg30uCUtpOk]. Poster\n  [https://hollydinkel.github.io/assets/pdf/ICRA20243DVRM_poster.pdf] 3DVRM\n  Workshop [https://3d-manipulation-workshop.github.io/]", "pdf_url": "http://arxiv.org/pdf/2506.22176v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22176v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22111", "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "authors": ["Ruthvik Bokkasam", "Shankar Gangisetty", "A. H. Abdul Hafez", "C. V. Jawahar"], "summary": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22111v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22111v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21839", "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21839v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21839v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22419", "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22419v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22419v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21968", "title": "Multi-IRS Aided ISAC System: Multi-Path Exploitation Versus Reduction", "authors": ["Guangji Chen", "Qingqing Wu", "Shihang Lu", "Meng Hua", "Wen Chen"], "summary": "This paper investigates a multi-intelligent reflecting surface (IRS) aided\nintegrated sensing and communication (ISAC) system, where multiple IRSs are\nstrategically deployed not only to assist the communication from a\nmulti-antenna base station (BS) to a multi-antenna communication user (CU), but\nalso enable the sensing service for a point target in the non-line-of-sight\n(NLoS) region of the BS. First, we propose a hybrid multi-IRS architecture,\nwhich consists of several passive IRSs and one semi-passive IRS equipped with\nboth active sensors and reflecting elements. To be specific, the active sensors\nare exploited to receive the echo signals for estimating the target's angle\ninformation, and the multiple reflecting paths provided by multi-IRS are\nemployed to improve the degree of freedoms (DoFs) of communication. Under the\ngiven budget on the number of total IRSs elements, we theoretically show that\nincreasing the number of deployed IRSs is beneficial for improving DoFs of\nspatial multiplexing for communication while increasing the Cramer-Rao bound\n(CRB) of target estimation, which unveils a fundamental tradeoff between the\nsensing and communication performance. To characterize the rate-CRB tradeoff,\nwe study a rate maximization problem, by optimizing the BS transmit covariance\nmatrix, IRSs phase-shifts, and the number of deployed IRSs, subject to a\nmaximum CRB constraint. Analytical results reveal that the\ncommunication-oriented design becomes optimal when the total number of IRSs\nelements exceeds a certain threshold, wherein the relationships of the rate and\nCRB with the number of IRS elements/sensors, transmit power, and the number of\ndeployed IRSs are theoretically derived and demystified. Simulation results\nvalidate our theoretical findings and also demonstrate the superiority of our\nproposed designs over the benchmark schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21968v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21968v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22311", "title": "Reconstructing Intelligible Speech from the Pressure Sensor Data in HVACs", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Anomadarshi Barua"], "summary": "Pressure sensors are an integrated component of modern Heating, Ventilation,\nand Air Conditioning (HVAC) systems. As these pressure sensors operate within\nthe 0-10 Pa range, support high sampling frequencies of 0.5-2 kHz, and are\noften placed close to human proximity, they can be used to eavesdrop on\nconfidential conversation, since human speech has a similar audible range of\n0-10 Pa and a bandwidth of 4 kHz for intelligible quality. This paper presents\nWaLi, which reconstructs intelligible speech from the low-resolution and noisy\npressure sensor data by providing the following technical contributions: (i)\nWaLi reconstructs intelligible speech from a minimum of 0.5 kHz sampling\nfrequency of pressure sensors, whereas previous work can only detect hot\nwords/phrases. WaLi uses complex-valued conformer and Complex Global Attention\nBlock (CGAB) to capture inter-phoneme and intra-phoneme dependencies that exist\nin the low-resolution pressure sensor data. (ii) WaLi handles the transient\nnoise injected from HVAC fans and duct vibrations, by reconstructing both the\nclean magnitude and phase of the missing frequencies of the low-frequency\naliased components. Extensive measurement studies on real-world pressure\nsensors show an LSD of 1.24 and NISQA-MOS of 1.78 for 0.5 kHz to 8 kHz\nupsampling. We believe that such levels of accuracy pose a significant threat\nwhen viewed from a privacy perspective that has not been addressed before for\npressure sensors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22311v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22311v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21571", "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21571v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21571v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.22262", "title": "JointRank: Rank Large Set with Single Pass", "authors": ["Evgeny Dedov"], "summary": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank", "comment": "ICTIR'25 Accepted", "pdf_url": "http://arxiv.org/pdf/2506.22262v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22262v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21940", "title": "GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "summary": "Variational Quantum Algorithms (VQAs) offer potential for near-term quantum\nadvantage but face challenges from barren plateaus, where gradients vanish, and\npoorly conditioned optimization landscapes. We introduce GuiderNet, a\nmeta-learning framework that conditions Parameterized Quantum Circuits (PQCs)\nusing data-dependent parameter shifts aimed at minimizing the log condition\nnumber of the Fubini-Study metric tensor. Implemented as a classical neural\nnetwork, GuiderNet is meta-trained to guide PQC parameters into geometrically\nfavorable regions and is embedded within hybrid quantum-classical pipelines to\nsteer both initialization and adaptive modulation during training.\n  Applied to the Kaggle Diabetes classification task, GuiderNet reduces\ncumulative training loss by over 5x, improves test accuracy from 75.3% to\n98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also\nsuppresses gradient explosion and stabilizes parameter updates, enabling\nsmoother and more robust optimization. These results demonstrate that geometric\nmeta-conditioning can mitigate barren plateaus and ill-conditioning, providing\na scalable approach to enhance trainability and generalization in quantum\nmachine learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21940v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21940v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22231", "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education", "authors": ["Russell Beale"], "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22231v1", "categories": ["cs.HC", "cs.AI", "cs.CY", "K.3.1; K.3.2; K.6.0"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22231v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22364", "title": "Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass", "authors": ["Joe Johnson", "Phanender Chalasani", "Arnav Shah", "Ram L. Ray", "Muthukumar Bagavathiannan"], "summary": "Accurate weed management is essential for mitigating significant crop yield\nlosses, necessitating effective weed suppression strategies in agricultural\nsystems. Integrating cover crops (CC) offers multiple benefits, including soil\nerosion reduction, weed suppression, decreased nitrogen requirements, and\nenhanced carbon sequestration, all of which are closely tied to the aboveground\nbiomass (AGB) they produce. However, biomass production varies significantly\ndue to microsite variability, making accurate estimation and mapping essential\nfor identifying zones of poor weed suppression and optimizing targeted\nmanagement strategies. To address this challenge, developing a comprehensive CC\nmap, including its AGB distribution, will enable informed decision-making\nregarding weed control methods and optimal application rates. Manual visual\ninspection is impractical and labor-intensive, especially given the extensive\nfield size and the wide diversity and variation of weed species and sizes. In\nthis context, optical imagery and Light Detection and Ranging (LiDAR) data are\ntwo prominent sources with unique characteristics that enhance AGB estimation.\nThis study introduces a ground robot-mounted multimodal sensor system designed\nfor agricultural field mapping. The system integrates optical and LiDAR data,\nleveraging machine learning (ML) methods for data fusion to improve biomass\npredictions. The best ML-based model for dry AGB estimation achieved a\ncoefficient of determination value of 0.88, demonstrating robust performance in\ndiverse field conditions. This approach offers valuable insights for\nsite-specific management, enabling precise weed suppression strategies and\npromoting sustainable farming practices.", "comment": "Accepted in the Extended Abstract, The 22nd International Conference\n  on Ubiquitous Robots (UR 2025), Texas, USA", "pdf_url": "http://arxiv.org/pdf/2506.22364v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22364v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21843", "title": "3D-Telepathy: Reconstructing 3D Objects from EEG Signals", "authors": ["Yuxiang Ge", "Jionghao Cheng", "Ruiquan Ge", "Zhaojie Fang", "Gangyong Jia", "Xiang Wan", "Nannan Li", "Ahmed Elazab", "Changmiao Wang"], "summary": "Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds\nsignificant potential for applications in Brain-Computer Interfaces (BCIs) and\naiding individuals with communication disorders. Traditionally, efforts have\nfocused on converting brain activity into 2D images, neglecting the translation\nof EEG data into 3D objects. This limitation is noteworthy, as the human brain\ninherently processes three-dimensional spatial information regardless of\nwhether observing 2D images or the real world. The neural activities captured\nby EEG contain rich spatial information that is inevitably lost when\nreconstructing only 2D images, thus limiting its practical applications in BCI.\nThe transition from EEG data to 3D object reconstruction faces considerable\nobstacles. These include the presence of extensive noise within EEG signals and\na scarcity of datasets that include both EEG and 3D information, which\ncomplicates the extraction process of 3D visual data. Addressing this\nchallenging task, we propose an innovative EEG encoder architecture that\nintegrates a dual self-attention mechanism. We use a hybrid training strategy\nto train the EEG Encoder, which includes cross-attention, contrastive learning,\nand self-supervised learning techniques. Additionally, by employing stable\ndiffusion as a prior distribution and utilizing Variational Score Distillation\nto train a neural radiation field, we successfully generate 3D objects with\nsimilar content and structure from EEG data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21843v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21843v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2109.05721", "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "pdf_url": "http://arxiv.org/pdf/2109.05721v2", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2109.05721v2", "date": "2021-09-13", "updated": "2022-12-19"}
{"id": "2506.22000", "title": "Heterogeneous Massive MIMO: A Cost-Efficient Technique for Uniform Service in Cellular Networks", "authors": ["Wei Jiang", "Hans D. Schotten"], "summary": "Massive multi-input multi-output (MIMO) has evolved along two tracks:\ncellular and cell-free, each with unique advantages and limitations. The\ncellular approach suffers from worse user spectral efficiency at cell edges,\nwhereas the cell-free approach incurs high implementation costs due to a\nlarge-scale distributed infrastructure. This paper introduces a novel\nnetworking paradigm, termed heterogeneous massive MIMO (HmMIMO), which\nseamlessly integrates co-located and distributed antennas. Differing from two\nconventional paradigms, HmMIMO remains a base station with a large antenna\narray at the center of each cell, aided by distributed antennas deployed at\ncell edges. Our findings demonstrate that this paradigm achieves a favorable\ntrade-off between performance and implementation complexity.", "comment": "IEEE ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2506.22000v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22000v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22321", "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22321v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22321v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21572", "title": "Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling", "authors": ["Tianyu. Zou", "Shengwu. Xiong", "Ruilin. Yao", "Jirui. Huang", "Yi. Rong", "Yaxiong. Chen", "Shili. Xiong", "Cong. Wang"], "summary": "Evaluating multimodal large language models (MLLMs) remains a fundamental\nchallenge due to a lack of structured, interpretable, and theoretically\ngrounded benchmark designs. Existing benchmarks often adopt heuristic-based\ntask groupings with unclear cognitive targets, thus resulting in overlapping\nabilities, redundant indicators, and limited diagnostic power. In this work, we\npropose a novel framework for aligning MLLM benchmark based on Structural\nEquation Modeling (SEM) to analyze and quantify the internal validity,\ndimensional separability, and contribution of benchmark components. Motivated\nby the observed limitations of current designs, we further introduce a novel\ncapability hierarchy grounded in Piagets theory of cognitive development,\ndividing MLLM abilities into three hierarchical layers, i.e., Perception,\nMemory, and Reasoning. We reorganize existing MLLM benchmarks under the\nproposed framework and construct a new benchmark named Gold. Experimental\nresults demonstrate that the proposed benchmark exhibits stronger\ninterpretability, reduced indicator redundancy, and clearer cognitive\nconsistency compared to existing approaches.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21572v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21572v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.22303", "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation", "authors": ["Xinghe Cheng", "Zihan Zhang", "Jiapu Wang", "Liangda Fang", "Chaobo He", "Quanlong Guan", "Shirui Pan", "Weiqi Luo"], "summary": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (e.g., knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Many educational datasets do not explicitly provide\nprerequisite relationships between knowledge concepts, hindering the\napplication of current learning path recommendation methods. 2) Relying solely\non prerequisite relationships as the sole knowledge structure can impede\nlearning progress and negatively impact student outcomes. To address these\nchallenges, we propose a novel approach, Discrimination Learning Enhances\nLearning Path Recommendation (DLELP), which enhances learning path\nrecommendations by incorporating both prerequisite and similarity relationships\nbetween knowledge concepts. Specifically, we introduce a knowledge concept\nstructure graph generation module that adaptively constructs knowledge concept\nstructure graphs for different educational datasets, significantly improving\nthe generalizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) framework, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22303v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22303v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21952", "title": "Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications", "authors": ["Yangyang Wan", "Haotian Wang", "Xuhui Yu", "Jiageng Chen", "Xinyu Fan", "Zuyuan He"], "summary": "Distributed acoustic sensing (DAS) has attracted considerable attention\nacross various fields and artificial intelligence (AI) technology plays an\nimportant role in DAS applications to realize event recognition and denoising.\nExisting AI models require real-world data (RWD), whether labeled or not, for\ntraining, which is contradictory to the fact of limited available event data in\nreal-world scenarios. Here, a physics-informed DAS neural network paradigm is\nproposed, which does not need real-world events data for training. By\nphysically modeling target events and the constraints of real world and DAS\nsystem, physical functions are derived to train a generative network for\ngeneration of DAS events data. DAS debackground net is trained by using the\ngenerated DAS events data to eliminate background noise in DAS data. The\neffectiveness of the proposed paradigm is verified in event identification\napplication based on a public dataset of DAS spatiotemporal data and in belt\nconveyor fault monitoring application based on DAS time-frequency data, and\nachieved comparable or better performance than data-driven networks trained\nwith RWD. Owing to the introduction of physical information and capability of\nbackground noise removal, the paradigm demonstrates generalization in same\napplication on different sites. A fault diagnosis accuracy of 91.8% is achieved\nin belt conveyor field with networks which transferred from simulation test\nsite without any fault events data of test site and field for training. The\nproposed paradigm is a prospective solution to address significant obstacles of\ndata acquisition and intense noise in practical DAS applications and explore\nmore potential fields for DAS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21952v1", "categories": ["cs.LG", "physics.app-ph", "physics.optics"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21952v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22318", "title": "Agent-based modeling and the sociology of money: some suggestions for refining monetary theory using social simulation", "authors": ["Eduardo Coltre Ferraciolli", "Tanya V. Araújo"], "summary": "The institution of money can be seen as a foundational social mechanism,\nenabling communities to quantify collectively regulate economic processes.\nMoney can be said, indeed, to constitute the micro-macro link in economics.\nThis paper reviews influential views on the nature of money in economics and\nsociology, contrasting them to the relatively limited findings of recent\nagent-based models of \"the emergence of money\". Noting ample room for novel\ncombinations of sociological and formal methods to drive insight into the many\nroles played by money in the economy, we conclude by indicating research\ndirections in which we believe this combination can provide new answers to old\nquestions in monetary theory", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2506.22318v1", "categories": ["physics.soc-ph", "cs.CY"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.22318v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21648", "title": "Advanced System Engineering Approaches to Emerging Challenges in Planetary and Deep-Space Exploration", "authors": ["J. de Curtò", "Cristina LiCalzi", "Julien Tubiana Warin", "Jack Gehlert", "Brian Langbein", "Alexandre Gamboa", "Chris Sixbey", "William Maguire", "Santiago Fernández", "Álvaro Maestroarena", "Alex Brenchley", "Logan Maroclo", "Philemon Mercado", "Joshua DeJohn", "Cesar Velez", "Ethan Dahmus", "Taylor Steinys", "David Fritz", "I. de Zarzà"], "summary": "This paper presents innovative solutions to critical challenges in planetary\nand deep-space exploration electronics. We synthesize findings across diverse\nmission profiles, highlighting advances in: (1) MARTIAN positioning systems\nwith dual-frequency transmission to achieve $\\pm$1m horizontal accuracy; (2)\nartificial reef platforms for Titan's hydrocarbon seas utilizing specialized\nsensor arrays and multi-stage communication chains; (3) precision orbital\nrendezvous techniques demonstrating novel thermal protection solutions; (4)\nminiaturized CubeSat architectures for asteroid exploration with optimized\npower-to-mass ratios; and (5) next-generation power management systems for MARS\nrovers addressing dust accumulation challenges. These innovations represent\npromising directions for future space exploration technologies, particularly in\nenvironments where traditional Earth-based electronic solutions prove\ninadequate. The interdisciplinary nature of these developments highlights the\ncritical intersection of aerospace engineering, electrical engineering, and\nplanetary science in advancing human exploration capabilities beyond Earth\norbit.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21648v1", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.RO", "cs.SY", "eess.SY"], "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2506.21648v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21851", "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "summary": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "comment": "IEEE International Conference on Systems, Man, and Cybernetics 2025.\n  (SMC), under review", "pdf_url": "http://arxiv.org/pdf/2506.21851v1", "categories": ["cs.CV", "cs.MM", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21851v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2212.09525", "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "comment": "AAAI 2023", "pdf_url": "http://arxiv.org/pdf/2212.09525v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2212.09525v1", "date": "2022-12-19", "updated": "2022-12-19"}
{"id": "2506.22052", "title": "Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles", "authors": ["Nico Ostendorf", "Keno Garlichs", "Lars Wolf"], "summary": "V2X communication has become crucial for enhancing road safety, especially\nfor Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the\nincreasing number of devices communicating on the same channels will lead to\nsignificant channel load. To address this issue this study evaluates the\neffectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),\nfocusing specifically on cyclists. The objective of RM is to minimize the\ntransmission of redundant information. We conducted a simulation study using a\nurban scenario with a high bicycle density based on traffic data from Hannover,\nGermany. This study assessed the impact of RM on channel load, measured by\nChannel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in\nsimulation. To evaluate the accuracy and reliability of the RM mechanisms, we\nanalyzed the actual differences in position, speed, and heading between the ego\nVRU and the VRU, which was assumed to be redundant. Our findings indicate that\nwhile RM can reduce channel congestion, it also leads to a decrease in VPR. The\nanalysis of actual differences revealed that the RM mechanism standardized by\nETSI often uses outdated information, leading to significant discrepancies in\nposition, speed, and heading, which could result in dangerous situations. To\naddress these limitations, we propose an adapted RM mechanism that improves the\nbalance between reducing channel load and maintaining VRU awareness. The\nadapted approach shows a significant reduction in maximum CBR and a less\nsignificant decrease in VPR compared to the standardized RM. Moreover, it\ndemonstrates better performance in the actual differences in position, speed,\nand heading, thereby enhancing overall safety. Our results highlight the need\nfor further research to optimize RM techniques and ensure they effectively\nenhance V2X communication without compromising the safety of VRUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22052v1", "categories": ["cs.ET", "cs.NI", "eess.SP"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.22052v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21573", "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21573v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21573v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2506.22356", "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval", "authors": ["Kevin Duh", "Eugene Yang", "Orion Weller", "Andrew Yates", "Dawn Lawrie"], "summary": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for\nresearching the context of the question, filtering the returned results, and\ngenerating the final answer. The retrieval system was a ColBERT bi-encoder\narchitecture, which represents a passage with many dense tokens. Retrieval used\na local, compressed index of the FineWeb10-BT collection created with PLAID-X,\nusing a model fine-tuned for multilingual retrieval. Query generation from\ncontext was done with Qwen2.5-7B-Instruct, while filtering was accomplished\nwith m2-bert-80M-8k-retrieval. Up to nine passages were used as context to\ngenerate an answer using Falcon3-10B. This system placed 5th in the LiveRAG\nautomatic evaluation for correctness with a score of 1.07.", "comment": "5 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.22356v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22356v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21956", "title": "Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement", "authors": ["Hao Jiang", "Yongxiang Tang", "Yanxiang Zeng", "Pengjia Yuan", "Yanhua Cheng", "Teng Sha", "Xialong Liu", "Peng Jiang"], "summary": "In the realm of online advertising, advertisers partake in ad auctions to\nobtain advertising slots, frequently taking advantage of auto-bidding tools\nprovided by demand-side platforms. To improve the automation of these bidding\nsystems, we adopt generative models, namely the Decision Transformer (DT), to\ntackle the difficulties inherent in automated bidding. Applying the Decision\nTransformer to the auto-bidding task enables a unified approach to sequential\nmodeling, which efficiently overcomes short-sightedness by capturing long-term\ndependencies between past bidding actions and user behavior. Nevertheless,\nconventional DT has certain drawbacks: (1) DT necessitates a preset\nreturn-to-go (RTG) value before generating actions, which is not inherently\nproduced; (2) The policy learned by DT is restricted by its training data,\nwhich is consists of mixed-quality trajectories. To address these challenges,\nwe introduce the R* Decision Transformer (R* DT), developed in a three-step\nprocess: (1) R DT: Similar to traditional DT, R DT stores actions based on\nstate and RTG value, as well as memorizing the RTG for a given state using the\ntraining set; (2) R^ DT: We forecast the highest value (within the training\nset) of RTG for a given state, deriving a suboptimal policy based on the\ncurrent state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,\nwe generate trajectories and select those with high rewards (using a simulator)\nto augment our training dataset. This data enhancement has been shown to\nimprove the RTG of trajectories in the training data and gradually leads the\nsuboptimal policy towards optimality. Comprehensive tests on a publicly\navailable bidding dataset validate the R* DT's efficacy and highlight its\nsuperiority when dealing with mixed-quality trajectories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21956v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21956v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22323", "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America", "authors": ["Alessio Di Santo"], "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22323v1", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.OS", "cs.PL"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22323v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21693", "title": "The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Fredrik Törner", "Christian Berger"], "summary": "Developing autonomous driving (AD) systems is challenging due to the\ncomplexity of the systems and the need to assure their safe and reliable\noperation. The widely adopted approach of DevOps seems promising to support the\ncontinuous technological progress in AI and the demand for fast reaction to\nincidents, which necessitate continuous development, deployment, and\nmonitoring. We present a systematic literature review meant to identify,\nanalyse, and synthesise a broad range of existing literature related to usage\nof DevOps in autonomous driving development. Our results provide a structured\noverview of challenges and solutions, arising from applying DevOps to\nsafety-related AI-enabled functions. Our results indicate that there are still\nseveral open topics to be addressed to enable safe DevOps for the development\nof safe AD.", "comment": "Accepted for publication in the Journal of Systems and Software (JSS)", "pdf_url": "http://arxiv.org/pdf/2506.21693v1", "categories": ["cs.SE", "cs.RO"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21693v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21855", "title": "Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation", "authors": ["Jiho Choi", "Sang Jun Lee"], "summary": "In this paper, we propose a method that learns a general representation of\nperiodic signals from unlabeled facial videos by capturing subtle changes in\nskin tone over time. The proposed framework employs the video masked\nautoencoder to learn a high-dimensional spatio-temporal representation of the\nfacial region through self-supervised learning. Capturing quasi-periodic\nsignals in the video is crucial for remote photoplethysmography (rPPG)\nestimation. To account for signal periodicity, we apply frame masking in terms\nof video sampling, which allows the model to capture resampled quasi-periodic\nsignals during the pre-training stage. Moreover, the framework incorporates\nphysiological bandlimit constraints, leveraging the property that physiological\nsignals are sparse within their frequency bandwidth to provide pulse cues to\nthe model. The pre-trained encoder is then transferred to the rPPG task, where\nit is used to extract physiological signals from facial videos. We evaluate the\nproposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and\nV4V datasets. Our results demonstrate significant performance improvements,\nparticularly in challenging cross-dataset evaluations. Our code is available at\nhttps://github.com/ziiho08/Periodic-MAE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21855v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21855v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2412.15194", "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "authors": ["Qihao Zhao", "Yangyu Huang", "Tengchao Lv", "Lei Cui", "Qinzheng Sun", "Shaoguang Mao", "Xin Zhang", "Ying Xin", "Qiufeng Yin", "Scarlett Li", "Furu Wei"], "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.15194v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2412.15194v1", "date": "2024-12-19", "updated": "2024-12-19"}
{"id": "2506.22094", "title": "Nonlinear Power Amplifier-Resilient Cell-Free Massive MIMO: A Joint Optimization Approach", "authors": ["Wei Jiang", "Hans D. Schotten"], "summary": "This letter analyzes the effects of power amplifiers (PAs) on the downlink of\ncell-free massive MIMO systems. We model signal transmission incorporating\nnonlinear PA distortion and derive a unified spectral efficiency (SE)\nexpression applicable to arbitrary precoding schemes. To combat PA-induced\nperformance degradation, a joint optimization approach for user association and\nmax-min power control is proposed. Furthermore, a low-complexity alternative is\ndeveloped to approximate the joint optimization with reduced computational\noverhead. Simulations validate the analysis and demonstrate significant\nperformance gains of the proposed approaches over conventional techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22094v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.22094v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21574", "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "authors": ["Yicheng Mao", "Yang Zhao"], "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21574v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21574v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2506.22372", "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement", "authors": ["Maryam Mousavian", "Zahra Abbasiantaeb", "Mohammad Aliannejadi", "Fabio Crestani"], "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.", "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22372v1", "categories": ["cs.IR", "cs.CL"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22372v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21976", "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21976v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21976v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21697", "title": "Stochastic Neural Control Barrier Functions", "authors": ["Hongchao Zhang", "Manan Tayal", "Jackson Cox", "Pushpak Jagtap", "Shishir Kolathaya", "Andrew Clark"], "summary": "Control Barrier Functions (CBFs) are utilized to ensure the safety of control\nsystems. CBFs act as safety filters in order to provide safety guarantees\nwithout compromising system performance. These safety guarantees rely on the\nconstruction of valid CBFs. Due to their complexity, CBFs can be represented by\nneural networks, known as neural CBFs (NCBFs). Existing works on the\nverification of the NCBF focus on the synthesis and verification of NCBFs in\ndeterministic settings, leaving the stochastic NCBFs (SNCBFs) less studied. In\nthis work, we propose a verifiably safe synthesis for SNCBFs. We consider the\ncases of smooth SNCBFs with twice-differentiable activation functions and\nSNCBFs that utilize the Rectified Linear Unit or ReLU activation function. We\npropose a verification-free synthesis framework for smooth SNCBFs and a\nverification-in-the-loop synthesis framework for both smooth and ReLU SNCBFs.\nand we validate our frameworks in three cases, namely, the inverted pendulum,\nDarboux, and the unicycle model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21697v1", "categories": ["eess.SY", "cs.RO", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21697v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21857", "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21857v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21857v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2501.06184", "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06184v1", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2501.06184v1", "date": "2025-01-10", "updated": "2025-01-10"}
{"id": "2506.22122", "title": "In situ fine-tuning of in silico trained Optical Neural Networks", "authors": ["Gianluca Kosmella", "Ripalta Stabile", "Jaron Sanders"], "summary": "Optical Neural Networks (ONNs) promise significant advantages over\ntraditional electronic neural networks, including ultrafast computation, high\nbandwidth, and low energy consumption, by leveraging the intrinsic capabilities\nof photonics. However, training ONNs poses unique challenges, notably the\nreliance on simplified in silico models whose trained parameters must\nsubsequently be mapped to physical hardware. This process often introduces\ninaccuracies due to discrepancies between the idealized digital model and the\nphysical ONN implementation, particularly stemming from noise and fabrication\nimperfections.\n  In this paper, we analyze how noise misspecification during in silico\ntraining impacts ONN performance and we introduce Gradient-Informed Fine-Tuning\n(GIFT), a lightweight algorithm designed to mitigate this performance\ndegradation. GIFT uses gradient information derived from the noise structure of\nthe ONN to adapt pretrained parameters directly in situ, without requiring\nexpensive retraining or complex experimental setups. GIFT comes with formal\nconditions under which it improves ONN performance.\n  We also demonstrate the effectiveness of GIFT via simulation on a five-layer\nfeed forward ONN trained on the MNIST digit classification task. GIFT achieves\nup to $28\\%$ relative accuracy improvement compared to the baseline performance\nunder noise misspecification, without resorting to costly retraining. Overall,\nGIFT provides a practical solution for bridging the gap between simplified\ndigital models and real-world ONN implementations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22122v1", "categories": ["cs.NE", "cs.ET", "eess.SP"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.22122v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21575", "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Casper Hansen", "Julien Fauqueur"], "summary": "We propose STRuCT-LLM, a unified framework for training large language models\n(LLMs) to perform structured reasoning over both relational and\ngraph-structured data. Our approach jointly optimizes Text-to-SQL and\nText-to-Cypher tasks using reinforcement learning (RL) combined with\nChain-of-Thought (CoT) supervision. To support fine-grained optimization in\ngraph-based parsing, we introduce a topology-aware reward function based on\ngraph edit distance. Unlike prior work that treats relational and graph\nformalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL\nand Cypher to induce cross-formalism transfer, enabling SQL training to improve\nCypher performance and vice versa - even without shared schemas. Our largest\nmodel (QwQ-32B) achieves substantial relative improvements across tasks: on\nsemantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The\nmodel also demonstrates strong zero-shot generalization, improving performance\non downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA\n(CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results\ndemonstrate both the effectiveness of executable queries as scaffolds for\nstructured reasoning and the synergistic benefits of jointly training on SQL\nand Cypher (code available at https://github.com/bouv/STRuCT-LLM).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21575v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21575v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2109.05721", "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "pdf_url": "http://arxiv.org/pdf/2109.05721v2", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2109.05721v2", "date": "2021-09-13", "updated": "2022-12-19"}
{"id": "2506.21997", "title": "Binned semiparametric Bayesian networks", "authors": ["Rafael Sojo", "Javier Díaz-Rozo", "Concha Bielza", "Pedro Larrañaga"], "summary": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21997v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.1; G.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21997v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21782", "title": "M3PO: Massively Multi-Task Model-Based Policy Optimization", "authors": ["Aditya Narendra", "Dmitry Makarov", "Aleksandr Panov"], "summary": "We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a\nscalable model-based reinforcement learning (MBRL) framework designed to\naddress sample inefficiency in single-task settings and poor generalization in\nmulti-task domains. Existing model-based approaches like DreamerV3 rely on\npixel-level generative models that neglect control-centric representations,\nwhile model-free methods such as PPO suffer from high sample complexity and\nweak exploration. M3PO integrates an implicit world model, trained to predict\ntask outcomes without observation reconstruction, with a hybrid exploration\nstrategy that combines model-based planning and model-free uncertainty-driven\nbonuses. This eliminates the bias-variance trade-off in prior methods by using\ndiscrepancies between model-based and model-free value estimates to guide\nexploration, while maintaining stable policy updates through a trust-region\noptimizer. M3PO provides an efficient and robust alternative to existing\nmodel-based policy optimization approaches and achieves state-of-the-art\nperformance across multiple benchmarks.", "comment": "6 pages, 4 figures. Accepted at IEEE/RSJ IROS 2025. Full version,\n  including appendix and implementation details", "pdf_url": "http://arxiv.org/pdf/2506.21782v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21782v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21862", "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "comment": "21 pages, 4 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2506.21862v1", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21862v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.20893", "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20893v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20893v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22190", "title": "dreaMLearning: Data Compression Assisted Machine Learning", "authors": ["Xiaobo Zhao", "Aaron Hurst", "Panagiotis Karras", "Daniel E. Lucani"], "summary": "Despite rapid advancements, machine learning, particularly deep learning, is\nhindered by the need for large amounts of labeled data to learn meaningful\npatterns without overfitting and immense demands for computation and storage,\nwhich motivate research into architectures that can achieve good performance\nwith fewer resources. This paper introduces dreaMLearning, a novel framework\nthat enables learning from compressed data without decompression, built upon\nEntropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless\ncompression method that consolidates information into a compact set of\nrepresentative samples. DreaMLearning accommodates a wide range of data types,\ntasks, and model architectures. Extensive experiments on regression and\nclassification tasks with tabular and image data demonstrate that dreaMLearning\naccelerates training by up to 8.8x, reduces memory usage by 10x, and cuts\nstorage by 42%, with a minimal impact on model performance. These advancements\nenhance diverse ML applications, including distributed and federated learning,\nand tinyML on resource-constrained edge devices, unlocking new possibilities\nfor efficient and scalable learning.", "comment": "18 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.22190v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22190v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21576", "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21576v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21576v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2212.09525", "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "comment": "AAAI 2023", "pdf_url": "http://arxiv.org/pdf/2212.09525v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2212.09525v1", "date": "2022-12-19", "updated": "2022-12-19"}
{"id": "2506.22004", "title": "GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning", "authors": ["Mohammad Sabbaqi", "Riccardo Taormina", "Elvin Isufi"], "summary": "Inference tasks with time series over graphs are of importance in\napplications such as urban water networks, economics, and networked\nneuroscience. Addressing these tasks typically relies on identifying a\ncomputationally affordable model that jointly captures the graph-temporal\npatterns of the data. In this work, we propose a graph-aware state space model\nfor graph time series, where both the latent state and the observation equation\nare parametric graph-induced models with a limited number of parameters that\nneed to be learned. More specifically, we consider the state equation to follow\na stochastic partial differential equation driven by noise over the graphs\nedges accounting not only for potential edge uncertainties but also for\nincreasing the degrees of freedom in the latter in a tractable manner. The\ngraph structure conditioning of the noise dispersion allows the state variable\nto deviate from the stochastic process in certain neighborhoods. The\nobservation model is a sampled and graph-filtered version of the state\ncapturing multi-hop neighboring influence. The goal is to learn the parameters\nin both state and observation models from the partially observed data for\ndownstream tasks such as prediction and imputation. The model is inferred first\nthrough a maximum likelihood approach that provides theoretical tractability\nbut is limited in expressivity and scalability. To improve on the latter, we\nuse the state-space formulation to build a principled deep learning\narchitecture that jointly learns the parameters and tracks the state in an\nend-to-end manner in the spirit of Kalman neural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22004v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22004v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21885", "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "comment": "Accepted by IEEE IV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21885v1", "categories": ["cs.CV", "cs.MM", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21885v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21863", "title": "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling", "authors": ["Sungjune Park", "Yeongyun Kim", "Se Yeon Kim", "Yong Man Ro"], "summary": "Large Vision and Language Models (LVLMs) have shown strong performance across\nvarious vision-language tasks in natural image domains. However, their\napplication to remote sensing (RS) remains underexplored due to significant\ndomain differences in visual appearances, object scales, and semantics. These\ndiscrepancies hider the effective understanding of RS scenes, which contain\nrich, multi-level semantic information spanning from coarse-to-fine levels.\nHence, it limits the direct adaptation of existing LVLMs to RS imagery. To\naddress this gap, we propose a novel LVLM framework tailored for RS\nunderstanding, incorporating two core components: Semantic-augmented\nMulti-level Alignment and Semantic-aware Expert Modeling. First, to align\nmulti-level visual features, we introduce the retrieval-based Semantic\nAugmentation Module which enriches the visual features with relevant semantics\nacross fine-to-coarse levels (e.g., object- and scene-level information). It is\ndesigned to retrieve relevant semantic cues from a RS semantic knowledge\ndatabase, followed by aggregation of semantic cues with user query and\nmulti-level visual features, resulting in semantically enriched representation\nacross multiple levels. Second, for Semantic-aware Expert Modeling, we design\nsemantic experts, where each expert is responsible for processing semantic\nrepresentation at different levels separately. This enables hierarchical\nsemantic understanding from coarse to fine levels. Evaluations across multiple\nRS tasks-including scene classification and VQA, etc.-demonstrate that the\nproposed framework achieves consistent improvements across multiple semantic\nlevels. This highlights its capability and effectiveness in bridging the gap\nbetween general LVLMs and unique demands of RS-specific vision-language\nunderstanding.", "comment": "13 pages including reference pages, 7 tables, and 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21863v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21863v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21545v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21545v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22227", "title": "Unified Memcapacitor-Memristor Memory for Synaptic Weights and Neuron Temporal Dynamics", "authors": ["Simone D'Agostino", "Marco Massarotto", "Tristan Torchet", "Filippo Moro", "Niccolò Castellani", "Laurent Grenouillet", "Yann Beilliard", "David Esseni", "Melika Payvand", "Elisa Vianello"], "summary": "We present a fabricated and experimentally characterized memory stack that\nunifies memristive and memcapacitive behavior. Exploiting this dual\nfunctionality, we design a circuit enabling simultaneous control of spatial and\ntemporal dynamics in recurrent spiking neural networks (RSNNs). Hardware-aware\nsimulations highlight its promise for efficient neuromorphic processing.", "comment": "2 pages, accepted and discussed at Silicon Nanoelectronics Workshop\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22227v1", "categories": ["cs.ET", "cs.NE", "eess.SP"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.22227v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21577", "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21577v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21577v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21585", "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "pdf_url": "http://arxiv.org/pdf/2506.21585v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21585v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22008", "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Konrad Tollmar", "Andrew D. Bagdanov", "Linus Gisslén"], "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.", "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22008v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22008v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21976", "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21976v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21976v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21866", "title": "Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images", "authors": ["Yanguang Sun", "Jiexi Yan", "Jianjun Qian", "Chunyan Xu", "Jian Yang", "Lei Luo"], "summary": "Automatically segmenting objects from optical remote sensing images (ORSIs)\nis an important task. Most existing models are primarily based on either\nconvolutional or Transformer features, each offering distinct advantages.\nExploiting both advantages is valuable research, but it presents several\nchallenges, including the heterogeneity between the two types of features, high\ncomplexity, and large parameters of the model. However, these issues are often\noverlooked in existing the ORSIs methods, causing sub-optimal segmentation. For\nthat, we propose a novel Dual-Perspective United Transformer (DPU-Former) with\na unique structure designed to simultaneously integrate long-range dependencies\nand spatial details. In particular, we design the global-local mixed attention,\nwhich captures diverse information through two perspectives and introduces a\nFourier-space merging strategy to obviate deviations for efficient fusion.\nFurthermore, we present a gated linear feed-forward network to increase the\nexpressive ability. Additionally, we construct a DPU-Former decoder to\naggregate and strength features at different layers. Consequently, the\nDPU-Former model outperforms the state-of-the-art methods on multiple datasets.\nCode: https://github.com/CSYSI/DPU-Former.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21866v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21866v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21558", "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter Mühlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21558v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21558v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22404", "title": "Data-Driven Intrusion Detection in Vehicles: Integrating Unscented Kalman Filter (UKF) with Machine Learning", "authors": ["Shuhao Bian", "Milad Farsi", "Nasser L. Azad", "Chris Hobbs"], "summary": "In the realm of Cyber-Physical System (CPS), accurately identifying attacks\nwithout detailed knowledge of the system's parameters remains a major\nchallenge. When it comes to Advanced Driver Assistance Systems (ADAS),\nidentifying the parameters of vehicle dynamics could be impractical or\nprohibitively costly. To tackle this challenge, we propose a novel framework\nfor attack detection in vehicles that effectively addresses the uncertainty in\ntheir dynamics. Our method integrates the widely used Unscented Kalman Filter\n(UKF), a well-known technique for nonlinear state estimation in dynamic\nsystems, with machine learning algorithms. This combination eliminates the\nrequirement for precise vehicle modeling in the detection process, enhancing\nthe system's adaptability and accuracy. To validate the efficacy and\npracticality of our proposed framework, we conducted extensive comparative\nsimulations by introducing Denial of Service (DoS) attacks on the vehicle\nsystems' sensors and actuators.", "comment": "Accepted in Proceedings of the 21st International Conference on\n  Informatics in Control, Automation and Robotics - Volume 1: ICINCO; ISBN\n  978-989-758-717-7, SciTePress, pages 714-723. DOI: 10.5220/0013063900003822", "pdf_url": "http://arxiv.org/pdf/2506.22404v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22404v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21578", "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models", "authors": ["Andrew Maranhão Ventura D'addario"], "summary": "The evaluation of Large Language Models (LLMs) in healthcare has been\ndominated by physician-centric, English-language benchmarks, creating a\ndangerous illusion of competence that ignores the interprofessional nature of\npatient care. To provide a more holistic and realistic assessment, we introduce\nHealthQA-BR, the first large-scale, system-wide benchmark for\nPortuguese-speaking healthcare. Comprising 5,632 questions from Brazil's\nnational licensing and residency exams, it uniquely assesses knowledge not only\nin medicine and its specialties but also in nursing, dentistry, psychology,\nsocial work, and other allied health professions. We conducted a rigorous\nzero-shot evaluation of over 20 leading LLMs. Our results reveal that while\nstate-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),\nthis top-line score masks alarming, previously unmeasured deficiencies. A\ngranular analysis shows performance plummets from near-perfect in specialties\nlike Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most\nnotably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic\nissue observed across all models, demonstrating that high-level scores are\ninsufficient for safety validation. By publicly releasing HealthQA-BR and our\nevaluation suite, we provide a crucial tool to move beyond single-score\nevaluations and toward a more honest, granular audit of AI readiness for the\nentire healthcare team.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21578v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21578v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21596", "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nsuccess in vision--language tasks. However, their capacity to reason over\ncomplex, long lessons and intricate educational diagrams that cannot be\nrepresented as a single natural image remains largely untested. In this work,\nwe present the first evaluation of state-of-the-art MLLMs on the textbook\nquestion answering (TQA) task using the CK12-QA dataset. We assess the\nperformance of recent vision-language models, including LLaVA and LLaMA\n3.2-Vision, across various input configurations. Additionally, we introduce a\nlightweight multimodal retrieval-augmented generation (RAG) pipeline that\nintegrates both paragraphs and diagrams from the lesson into the prompt. Our\nresults demonstrate the influence of retrieved educational context on model\naccuracy and reasoning, while also revealing current limitations in handling\nquestion-context relationships and the potential for noise, pointing to key\ndirections for future research in multimodal AI-driven learning.", "comment": "7 Pages", "pdf_url": "http://arxiv.org/pdf/2506.21596v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21596v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22036", "title": "Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion", "authors": ["Ying Zhang", "Yu Zhao", "Xuhui Sui", "Baohang Zhou", "Xiangrui Cai", "Li Shen", "Xiaojie Yuan", "Dacheng Tao"], "summary": "With the increasing multimodal knowledge privatization requirements,\nmultimodal knowledge graphs in different institutes are usually decentralized,\nlacking of effective collaboration system with both stronger reasoning ability\nand transmission safety guarantees. In this paper, we propose the Federated\nMultimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over\nfederated MKGs for better predicting the missing links in clients without\nsharing sensitive knowledge. We propose a framework named MMFeD3-HidE for\naddressing multimodal uncertain unavailability and multimodal client\nheterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed\nHyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete\nmultimodal distributions from incomplete entity embeddings constrained by\navailable modalities. (2) Among clients, our proposed Multimodal FeDerated Dual\nDistillation (MMFeD3) transfers knowledge mutually between clients and the\nserver with logit and feature distillation to improve both global convergence\nand semantic consistency. We propose a FedMKGC benchmark for a comprehensive\nevaluation, consisting of a general FedMKGC backbone named MMFedE, datasets\nwith heterogeneous multimodal information, and three groups of constructed\nbaselines. Experiments conducted on our benchmark validate the effectiveness,\nsemantic consistency, and convergence robustness of MMFeD3-HidE.", "comment": "Submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.22036v1", "categories": ["cs.LG", "cs.MM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22036v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22191", "title": "Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints", "authors": ["Yuxin Cui", "Rui Song", "Yibin Li", "Max Q. -H. Meng", "Zhe Min"], "summary": "Robust and accurate 2D/3D registration, which aligns preoperative models with\nintraoperative images of the same anatomy, is crucial for successful\ninterventional navigation. To mitigate the challenge of a limited field of view\nin single-image intraoperative scenarios, multi-view 2D/3D registration is\nrequired by leveraging multiple intraoperative images. In this paper, we\npropose a novel multi-view 2D/3D rigid registration approach comprising two\nstages. In the first stage, a combined loss function is designed, incorporating\nboth the differences between predicted and ground-truth poses and the\ndissimilarities (e.g., normalized cross-correlation) between simulated and\nobserved intraoperative images. More importantly, additional cross-view\ntraining loss terms are introduced for both pose and image losses to explicitly\nenforce cross-view constraints. In the second stage, test-time optimization is\nperformed to refine the estimated poses from the coarse stage. Our method\nexploits the mutual constraints of multi-view projection poses to enhance the\nrobustness of the registration process. The proposed framework achieves a mean\ntarget registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from\nthe DeepFluoro dataset, demonstrating superior performance compared to\nstate-of-the-art registration algorithms.", "comment": "ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.22191v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22191v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21873", "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning", "authors": ["Tzu-Chun Chien", "Chieh-Kai Lin", "Shiang-Feng Tsai", "Ruei-Chi Lai", "Hung-Jen Chen", "Min Sun"], "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21873v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21873v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21560", "title": "Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning", "authors": ["Yifu Han", "Geo Zhang"], "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21560v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21560v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22426", "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22426v1", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22426v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21580", "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21580v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21580v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21597", "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering", "authors": ["Brandon Colelough", "Davis Bartels", "Dina Demner-Fushman"], "summary": "In this paper, we present an overview of ClinIQLink, a shared task,\ncollocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test\nlarge language models (LLMs) on medically-oriented question answering aimed at\nthe level of a General Practitioner. The challenge supplies 4,978\nexpert-verified, medical source-grounded question-answer pairs that cover seven\nformats: true/false, multiple choice, unordered list, short answer,\nshort-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled\nin Docker or Apptainer images, are executed on the CodaBench platform or the\nUniversity of Maryland's Zaratan cluster. An automated harness (Task 1) scores\nclosed-ended items by exact match and open-ended items with a three-tier\nembedding metric. A subsequent physician panel (Task 2) audits the top model\nresponses.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21597v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21597v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22039", "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22039v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22365", "title": "Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation", "authors": ["Tao Li", "Haozhe Lei", "Mingsheng Yin", "Yaqi Hu"], "summary": "When using reinforcement learning (RL) to tackle physical control tasks,\ninductive biases that encode physics priors can help improve sample efficiency\nduring training and enhance generalization in testing. However, the current\npractice of incorporating these helpful physics-informed inductive biases\ninevitably runs into significant manual labor and domain expertise, making them\nprohibitive for general users. This work explores a symbolic approach to\ndistill physics-informed inductive biases into RL agents, where the physics\npriors are expressed in a domain-specific language (DSL) that is human-readable\nand naturally explainable. Yet, the DSL priors do not translate directly into\nan implementable policy due to partial and noisy observations and additional\nphysical constraints in navigation tasks. To address this gap, we develop a\nphysics-informed program-guided RL (PiPRL) framework with applications to\nindoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic\nintegration, where a meta symbolic program receives semantically meaningful\nfeatures from a neural perception module, which form the bases for symbolic\nprogramming that encodes physics priors and guides the RL process of a\nlow-level neural controller. Extensive experiments demonstrate that PiPRL\nconsistently outperforms purely symbolic or neural policies and reduces\ntraining time by over 26% with the help of the program-based inductive biases.", "comment": "Spotlight paper at Reinforcement Learning Conference 2025, Workshop\n  on Inductive Biases in Reinforcement Learning", "pdf_url": "http://arxiv.org/pdf/2506.22365v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22365v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21883", "title": "GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification", "authors": ["Basudha Pal", "Sharif Amit Kamran", "Brendon Lutnick", "Molly Lucas", "Chaitanya Parmar", "Asha Patel Shah", "David Apfel", "Steven Fakharzadeh", "Lloyd Miller", "Gabriela Cula", "Kristopher Standish"], "summary": "Psoriasis (PsO) severity scoring is important for clinical trials but is\nhindered by inter-rater variability and the burden of in person clinical\nevaluation. Remote imaging using patient captured mobile photos offers\nscalability but introduces challenges, such as variation in lighting,\nbackground, and device quality that are often imperceptible to humans but can\nimpact model performance. These factors, along with inconsistencies in\ndermatologist annotations, reduce the reliability of automated severity\nscoring. We propose a framework to automatically flag problematic training\nimages that introduce spurious correlations which degrade model generalization,\nusing a gradient based interpretability approach. By tracing the gradients of\nmisclassified validation images, we detect training samples where model errors\nalign with inconsistently rated examples or are affected by subtle, nonclinical\nartifacts. We apply this method to a ConvNeXT based weakly supervised model\ndesigned to classify PsO severity from phone images. Removing 8.2% of flagged\nimages improves model AUC-ROC by 5% (85% to 90%) on a held out test set.\nCommonly, multiple annotators and an adjudication process ensure annotation\naccuracy, which is expensive and time consuming. Our method detects training\nimages with annotation inconsistencies, potentially removing the need for\nmanual review. When applied to a subset of training data rated by two\ndermatologists, the method identifies over 90% of cases with inter-rater\ndisagreement by reviewing only the top 30% of samples. This improves automated\nscoring for remote assessments, ensuring robustness despite data collection\nvariability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21883v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21883v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21561", "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", "authors": ["Emilio Barkett", "Olivia Long", "Madhavendra Thakur"], "summary": "Despite their widespread use in fact-checking, moderation, and high-stakes\ndecision-making, large language models (LLMs) remain poorly understood as\njudges of truth. This study presents the largest evaluation to date of LLMs'\nveracity detection capabilities and the first analysis of these capabilities in\nreasoning models. We had eight LLMs make 4,800 veracity judgments across\nseveral prompts, comparing reasoning and non-reasoning models. We find that\nrates of truth-bias, or the likelihood to believe a statement is true,\nregardless of whether it is actually true, are lower in reasoning models than\nin non-reasoning models, but still higher than human benchmarks. Most\nconcerning, we identify sycophantic tendencies in several advanced models\n(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an\nasymmetry in detection accuracy, performing well in truth accuracy but poorly\nin deception accuracy. This suggests that capability advances alone do not\nresolve fundamental veracity detection challenges in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21561v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21561v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21582v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21582v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21600", "title": "Structured Attention Matters to Multimodal LLMs in Document Understanding", "authors": ["Chang Liu", "Hongkai Chen", "Yujun Cai", "Hang Wu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "summary": "Document understanding remains a significant challenge for multimodal large\nlanguage models (MLLMs). While previous research has primarily focused on\nlocating evidence pages through precise multimodal queries, our work\ninvestigates a fundamental yet overlooked aspect: how input format influences\ndocument comprehension performance. Through systematic analysis, we discover\nthat raw OCR text often impairs rather than improves MLLMs' performance, which\nis a counterintuitive finding we attribute to attention dispersion and\nstructure loss. To further substantiate our hypothesis, we propose a novel\nstructure-preserving approach that encodes document elements using the LaTex\nparadigm, maintaining the hierarchical organization and spatial relationships\ncritical for comprehension. Our attention analysis reveals that structured text\ninduces structured attention patterns on both textual and visual content,\ndirecting models to focus on semantically meaningful regions while reducing\nattention waste. This approach significantly enhances MLLMs' document question\nanswering performance across diverse document types without requiring\narchitectural modifications or additional training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21600v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21600v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22049", "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22049v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22049v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22423", "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks", "authors": ["Pritam Dash", "Ethan Chan", "Nathan P. Lawrence", "Karthik Pattabiraman"], "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22423v1", "categories": ["cs.LG", "cs.CR", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22423v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21885", "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "comment": "Accepted by IEEE IV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21885v1", "categories": ["cs.CV", "cs.MM", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21885v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21562", "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21562v1", "categories": ["cs.CL", "cs.AI", "cs.AR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21562v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21583", "title": "Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "Ameer Hamza", "Ildar Batyrshin", "Grigori Sidorov"], "summary": "Hope is a positive emotional state involving the expectation of favorable\nfuture outcomes, while hope speech refers to communication that promotes\noptimism, resilience, and support, particularly in adverse contexts. Although\nhope speech detection has gained attention in Natural Language Processing\n(NLP), existing research mainly focuses on high-resource languages and\nstandardized scripts, often overlooking informal and underrepresented forms\nsuch as Roman Urdu. To the best of our knowledge, this is the first study to\naddress hope speech detection in code-mixed Roman Urdu by introducing a\ncarefully annotated dataset, thereby filling a critical gap in inclusive NLP\nresearch for low-resource, informal language varieties. This study makes four\nkey contributions: (1) it introduces the first multi-class annotated dataset\nfor Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,\nUnrealistic Hope, and Not Hope categories; (2) it explores the psychological\nfoundations of hope and analyzes its linguistic patterns in code-mixed Roman\nUrdu to inform dataset development; (3) it proposes a custom attention-based\ntransformer model optimized for the syntactic and semantic variability of Roman\nUrdu, evaluated using 5-fold cross-validation; and (4) it verifies the\nstatistical significance of performance gains using a t-test. The proposed\nmodel, XLM-R, achieves the best performance with a cross-validation score of\n0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%\nand 2.63% respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21583v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21583v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21612", "title": "AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning", "authors": ["Xiaobin Ren", "Xinyu Zhu", "Kaiqi Zhao"], "summary": "Currently, considerable strides have been achieved in Point-of-Interest (POI)\nembedding methodologies, driven by the emergence of novel POI tasks like\nrecommendation and classification. Despite the success of task-specific,\nend-to-end models in POI embedding, several challenges remain. These include\nthe need for more effective multi-context sampling strategies, insufficient\nexploration of multiple POI contexts, limited versatility, and inadequate\ngeneralization. To address these issues, we propose the AdaptGOT model, which\nintegrates both the (Adapt)ive representation learning technique and the\nGeographical-Co-Occurrence-Text (GOT) representation with a particular emphasis\non Geographical location, Co-Occurrence and Textual information. The AdaptGOT\nmodel comprises three key components: (1) contextual neighborhood generation,\nwhich integrates advanced mixed sampling techniques such as KNN, density-based,\nimportance-based, and category-aware strategies to capture complex contextual\nneighborhoods; (2) an advanced GOT representation enhanced by an attention\nmechanism, designed to derive high-quality, customized representations and\nefficiently capture complex interrelations between POIs; and (3) the MoE-based\nadaptive encoder-decoder architecture, which ensures topological consistency\nand enriches contextual representation by minimizing Jensen-Shannon divergence\nacross varying contexts. Experiments on two real-world datasets and multiple\nPOI tasks substantiate the superior performance of the proposed AdaptGOT model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21612v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21612v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22055", "title": "crypto price prediction using lstm+xgboost", "authors": ["Mehul Gautam"], "summary": "The volatility and complex dynamics of cryptocurrency markets present unique\nchallenges for accurate price forecasting. This research proposes a hybrid deep\nlearning and machine learning model that integrates Long Short-Term Memory\n(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency\nprice prediction. The LSTM component captures temporal dependencies in\nhistorical price data, while XGBoost enhances prediction by modeling nonlinear\nrelationships with auxiliary features such as sentiment scores and\nmacroeconomic indicators. The model is evaluated on historical datasets of\nBitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and\nlocalized exchange data. Comparative analysis using Mean Absolute Percentage\nError (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)\ndemonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone\nmodels and traditional forecasting methods. This study underscores the\npotential of hybrid architectures in financial forecasting and provides\ninsights into model adaptability across different cryptocurrencies and market\ncontexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22055v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22055v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21891", "title": "DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025", "authors": ["Umihiro Kamoto", "Tatsuya Ishibashi", "Noriyuki Kugo"], "summary": "In this report, we present the winning solution that achieved the 1st place\nin the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This\nchallenge evaluates the ability to generate accurate natural language answers\nto questions about diverse, real-world video clips. It uses the Complex Video\nReasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists\nof 214 unique videos and 2,400 question-answer pairs spanning 11 categories.\nOur method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative\nreasoning approach, in which each input question is semantically decomposed and\nsolved through stepwise reasoning and progressive inference. This enables our\nsystem to provide highly accurate and contextually appropriate answers to even\nthe most complex queries. Applied to the CVRR-ES benchmark, our approach\nachieves 81.44% accuracy on the test set, securing the top position among all\nparticipants. This report details our methodology and provides a comprehensive\nanalysis of the experimental results, demonstrating the effectiveness of our\niterative reasoning framework in achieving robust video question answering. The\ncode is available at https://github.com/PanasonicConnect/DIVE", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21891v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21891v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21564", "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing", "authors": ["Jiyan Liu", "Youzheng Liu", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21564v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21564v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21584", "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "authors": ["J. Koorndijk"], "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21584v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21584v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21615", "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "authors": ["Wenhao Li", "Hongkuan Zhang", "Hongwei Zhang", "Zhengxu Li", "Zengjie Dong", "Yafan Chen", "Niranjan Bidargaddi", "Hong Liu"], "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21615v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21615v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22084", "title": "Transformers are Graph Neural Networks", "authors": ["Chaitanya K. Joshi"], "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.", "comment": "This paper is a technical version of an article in The Gradient at\n  https://thegradient.pub/transformers-are-graph-neural-networks/", "pdf_url": "http://arxiv.org/pdf/2506.22084v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22084v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21892", "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation", "authors": ["Adam Goodge", "Xun Xu", "Bryan Hooi", "Wee Siong Ng", "Jingyi Liao", "Yongyi Su", "Xulei Yang"], "summary": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21892v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21892v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21566", "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "authors": ["Arwa Arif"], "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "comment": "Preprint, 8 Pages", "pdf_url": "http://arxiv.org/pdf/2506.21566v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21566v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21585", "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "pdf_url": "http://arxiv.org/pdf/2506.21585v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21585v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21625", "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents", "authors": ["Jiaxi Zhuang", "Kangning Li", "Jue Hou", "Mingjun Xu", "Zhifeng Gao", "Hengxing Cai"], "summary": "Extracting molecular structure-activity relationships (SARs) from scientific\nliterature and patents is essential for drug discovery and materials research.\nHowever, this task remains challenging due to heterogeneous document formats\nand limitations of existing methods. Specifically, rule-based approaches\nrelying on rigid templates fail to generalize across diverse document layouts,\nwhile general-purpose multimodal large language models (MLLMs) lack sufficient\naccuracy and reliability for specialized tasks, such as layout detection and\noptical chemical structure recognition (OCSR). To address these challenges, we\nintroduce DocSAR-200, a rigorously annotated benchmark of 200 scientific\ndocuments designed specifically for evaluating SAR extraction methods.\nAdditionally, we propose Doc2SAR, a novel synergistic framework that integrates\ndomain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).\nExtensive experiments demonstrate that Doc2SAR achieves state-of-the-art\nperformance across various document types, significantly outperforming leading\nend-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of\n80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR\ndemonstrates practical usability through efficient inference and is accompanied\nby a web app.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21625v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21625v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22095", "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Balázs Kulcsár"], "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).", "comment": "18 pages, 5 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22095v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22095v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21895", "title": "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning", "authors": ["Fangling Jiang", "Qi Li", "Weining Wang", "Gang Wang", "Bing Liu", "Zhenan Sun"], "summary": "Recently the emergence of novel presentation attacks has drawn increasing\nattention to face anti-spoofing. However, existing methods tend to memorize\ndata patterns from the training set, resulting in poor generalization to\nunknown attack types across different scenarios and limited interpretability.\nTo address these challenges, this paper presents a reinforcement\nfine-tuning-based face anti-spoofing method that stimulates the capabilities of\nmultimodal large language models to think and learn how to solve the\nanti-spoofing task itself, rather than relying on the memorization of\nauthenticity patterns. We design verifiable class consistent reward and\nreasoning consistent reward, and employ a GRPO-based optimization strategy to\nguide the model in exploring reasoning policies from multiple perspectives to\nmaximize expected rewards. As a result, through iterative trial-and-error\nlearning while retaining only high-reward trajectories, the model distills\nhighly generalizable decision-making rules from the extensive solution space to\neffectively address cross-domain face anti-spoofing tasks. Extensive\nexperimental results demonstrate that our method achieves state-of-the-art\ncross-domain generalization performance. It generalizes well to diverse unknown\nattack types in unseen target domains while providing interpretable reasoning\nfor its authenticity decisions without requiring labor-intensive textual\nannotations for training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21895v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21895v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21567", "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21567v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21567v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21586", "title": "Can Vision Language Models Understand Mimed Actions?", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21586v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21586v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22141", "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level", "authors": ["Iliass Ayaou", "Denis Cavallucci", "Hicham Chibane"], "summary": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22141v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22141v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22096", "title": "Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments", "authors": ["Tin Lai", "Farnaz Farid", "Yueyang Kuan", "Xintian Zhang"], "summary": "Detecting heavy metal pollution in soils and seaports is vital for regional\nenvironmental monitoring. The Pollution Load Index (PLI), an international\nstandard, is commonly used to assess heavy metal containment. However, the\nconventional PLI assessment involves laborious procedures and data analysis of\nsediment samples. To address this challenge, we propose a deep-learning-based\nmodel that simplifies the heavy metal assessment process. Our model tackles the\nissue of data scarcity in the water-sediment domain, which is traditionally\nplagued by challenges in data collection and varying standards across nations.\nBy leveraging transfer learning, we develop an accurate quantitative assessment\nmethod for predicting PLI. Our approach allows the transfer of learned features\nacross domains with different sets of features. We evaluate our model using\ndata from six major ports in New South Wales, Australia: Port Yamba, Port\nNewcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results\ndemonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute\nPercentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared\nto other models. Our model performance is up to 2 orders of magnitude than\nother baseline models. Our proposed model offers an innovative, accessible, and\ncost-effective approach to predicting water quality, benefiting marine life\nconservation, aquaculture, and industrial pollution monitoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22096v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22096v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21903", "title": "Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment", "authors": ["Dipayan Biswas", "Shishir Shah", "Jaspal Subhlok"], "summary": "Video is transforming education with online courses and recorded lectures\nsupplementing and replacing classroom teaching. Recent research has focused on\nenhancing information retrieval for video lectures with advanced navigation,\nsearchability, summarization, as well as question answering chatbots. Visual\nelements like tables, charts, and illustrations are central to comprehension,\nretention, and data presentation in lecture videos, yet their full potential\nfor improving access to video content remains underutilized. A major factor is\nthat accurate automatic detection of visual elements in a lecture video is\nchallenging; reasons include i) most visual elements, such as charts, graphs,\ntables, and illustrations, are artificially created and lack any standard\nstructure, and ii) coherent visual objects may lack clear boundaries and may be\ncomposed of connected text and visual components. Despite advancements in deep\nlearning based object detection, current models do not yield satisfactory\nperformance due to the unique nature of visual content in lectures and scarcity\nof annotated datasets. This paper reports on a transfer learning approach for\ndetecting visual elements in lecture video frames. A suite of state of the art\nobject detection models were evaluated for their performance on lecture video\ndatasets. YOLO emerged as the most promising model for this task. Subsequently\nYOLO was optimized for lecture video object detection with training on multiple\nbenchmark datasets and deploying a semi-supervised auto labeling strategy.\nResults evaluate the success of this approach, also in developing a general\nsolution to the problem of object detection in lecture videos. Paper\ncontributions include a publicly released benchmark of annotated lecture video\nframes, along with the source code to facilitate future research.", "comment": "This is an extended version of a paper accepted to MIPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21903v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21903v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21569", "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA", "authors": ["Weihua Xiao", "Derek Ekberg", "Siddharth Garg", "Ramesh Karri"], "summary": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of\nhardware designs, but manually writing them from natural language property\ndescriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.\nRecent advances in large language models (LLMs) offer opportunities to automate\nthis translation. However, existing models still struggle with understanding\ndomain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we\npropose a customized retrieval-augmented generation (RAG) framework and a\nsynthetic fine-tuning dataset that together improve LLM's performance. To\nfurther improve lightweight models over NL2SVA, our fine-tuning dataset\nprovides prompt-guided explanations that teach LLMs the layer-by-layer\nconstruction process of concurrent SVAs, enabling supervised fine-tuning that\ngreatly improves syntax and functionality accuracy. To evaluate the performance\nof LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,\ncomprising 40 Verilog designs and 229 formally verified SVAs with detailed\nannotations. Experimental results show that our customized RAG framework\nincreases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,\nwhile Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and\nintegrated with HybridRetrieval achieves a 59.05% over the base Qwen model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21569v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21569v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21587", "title": "Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?", "authors": ["Weihong Qi", "Fan Huang", "Jisun An", "Haewoon Kwak"], "summary": "This study evaluates the ability of DeepSeek, an open-source large language\nmodel (LLM), to simulate public opinions in comparison to LLMs developed by\nmajor tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,\nGPT-4o, and Llama-3.3 and utilizing survey data from the American National\nElection Studies (ANES) and the Zuobiao dataset of China, we assess these\nmodels' capacity to predict public opinions on social issues in both China and\nthe United States, highlighting their comparative capabilities between\ncountries. Our findings indicate that DeepSeek-V3 performs best in simulating\nU.S. opinions on the abortion issue compared to other topics such as climate\nchange, gun control, immigration, and services for same-sex couples, primarily\nbecause it more accurately simulates responses when provided with Democratic or\nliberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating\nopinions on foreign aid and individualism but shows limitations in modeling\nviews on capitalism, particularly failing to capture the stances of low-income\nand non-college-educated individuals. It does not exhibit significant\ndifferences from other models in simulating opinions on traditionalism and the\nfree market. Further analysis reveals that all LLMs exhibit the tendency to\novergeneralize a single perspective within demographic groups, often defaulting\nto consistent responses within groups. These findings highlight the need to\nmitigate cultural and demographic biases in LLM-driven public opinion modeling,\ncalling for approaches such as more inclusive training methodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21587v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21587v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22165", "title": "The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment", "authors": ["Lorenz Wendlinger", "Simon Alexander Nonn", "Abdullah Al Zubaer", "Michael Granitzer"], "summary": "Legal systems heavily rely on cross-citations of legal norms as well as\nprevious court decisions. Practitioners, novices and legal AI systems need\naccess to these relevant data to inform appraisals and judgments. We propose a\nGraph-Neural-Network (GNN) link prediction model that can identify Case-Law and\nCase-Case citations with high proficiency through fusion of semantic and\ntopological information. We introduce adapted relational graph convolutions\noperating on an extended and enriched version of the original citation graph\nthat allow the topological integration of semantic meta-information. This\nfurther improves prediction by 3.1 points of average precision and by 8.5\npoints in data sparsity as well as showing robust performance over time and in\nchallenging fully inductive prediction. Jointly learning and predicting case\nand norm citations achieves a large synergistic effect that improves case\ncitation prediction by up to 4.7 points, at almost doubled efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22165v1", "categories": ["cs.SI", "cs.IR"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22165v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22129", "title": "Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models", "authors": ["Anurag Panda", "Gaurav Kumar Yadav"], "summary": "In the aftermath of major earthquakes, evaluating structural and\ninfrastructural damage is vital for coordinating post-disaster response\nefforts. This includes assessing damage's extent and spatial distribution to\nprioritize rescue operations and resource allocation. Accurately estimating\ndamage grades to buildings post-earthquake is paramount for effective response\nand recovery, given the significant impact on lives and properties,\nunderscoring the urgency of streamlining relief fund allocation processes.\nPrevious studies have shown the effectiveness of multi-class classification,\nespecially XGBoost, along with other machine learning models and ensembling\nmethods, incorporating regularization to address class imbalance. One\nconsequence of class imbalance is that it may give rise to skewed models that\nundervalue minority classes and give preference to the majority class. This\nresearch deals with the problem of class imbalance with the help of the\nsynthetic minority oversampling technique (SMOTE). We delve into multiple\nmulti-class classification machine learning, deep learning models, and\nensembling methods to forecast structural damage grades. The study elucidates\nperformance determinants through comprehensive feature manipulation experiments\nand diverse training approaches. It identifies key factors contributing to\nseismic vulnerability while evaluating model performance using techniques like\nthe confusion matrix further to enhance understanding of the effectiveness of\nearthquake damage prediction.", "comment": "3rd International Conference on Applied Mathematics in Science and\n  Engineering", "pdf_url": "http://arxiv.org/pdf/2506.22129v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22129v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21905", "title": "RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network", "authors": ["Mingquan Liu"], "summary": "Fine Grained Visual Categorization (FGVC) remains a challenging task in\ncomputer vision due to subtle inter class differences and fragile feature\nrepresentations. Existing methods struggle in fine grained scenarios,\nespecially when labeled data is scarce. We propose a semi supervised method\ncombining Mamba based feature modeling, region attention, and Bayesian\nuncertainty. Our approach enhances local to global feature modeling while\nfocusing on key areas during learning. Bayesian inference selects high quality\npseudo labels for stability. Experiments show strong performance on FGVC\nbenchmarks with occlusions, demonstrating robustness when labeled data is\nlimited. Code is available at https://github.com/wxqnl/RAUM Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21905v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21905v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21570", "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21570v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21570v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.21588", "title": "Understanding Verbatim Memorization in LLMs Through Circuit Discovery", "authors": ["Ilya Lasy", "Peter Knees", "Stefan Woltran"], "summary": "Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of\ntraining data -- remain poorly understood. What exact part of the network\ndecides to retrieve a token that we would consider as start of memorization\nsequence? How exactly is the models' behaviour different when producing\nmemorized sentence vs non-memorized? In this work we approach these questions\nfrom mechanistic interpretability standpoint by utilizing transformer circuits\n-- the minimal computational subgraphs that perform specific functions within\nthe model. Through carefully constructed contrastive datasets, we identify\npoints where model generation diverges from memorized content and isolate the\nspecific circuits responsible for two distinct aspects of memorization. We find\nthat circuits that initiate memorization can also maintain it once started,\nwhile circuits that only maintain memorization cannot trigger its initiation.\nIntriguingly, memorization prevention mechanisms transfer robustly across\ndifferent text domains, while memorization induction appears more\ncontext-dependent.", "comment": "The First Workshop on Large Language Model Memorization @ ACL 2025,\n  Vienna, August 1st, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21588v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21588v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22186", "title": "Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems", "authors": ["Kaikai Zheng", "Dawei Shi", "Yang Shi", "Long Wang"], "summary": "Thompson sampling (TS) is an effective method to explore parametric\nuncertainties and can therefore be used for active learning-based controller\ndesign. However, TS relies on finite parametric representations, which limits\nits applicability to more general spaces, which are more commonly encountered\nin control system design. To address this issue, this work pro poses a\nparameterization method for control law learning using reproducing kernel\nHilbert spaces and designs a data-driven active learning control approach.\nSpecifically, the proposed method treats the control law as an element in a\nfunction space, allowing the design of control laws without imposing\nrestrictions on the system structure or the form of the controller. A TS\nframework is proposed in this work to explore potential optimal control laws,\nand the convergence guarantees are further provided for the learning process.\nTheoretical analysis shows that the proposed method learns the relationship\nbetween control laws and closed-loop performance metrics at an exponential\nrate, and the upper bound of control regret is also derived. Numerical\nexperiments on controlling unknown nonlinear systems validate the effectiveness\nof the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22186v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22186v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21909", "title": "CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability", "authors": ["Justin Reinman", "Sunwoong Choi"], "summary": "CERBERUS is a synthetic benchmark designed to help train and evaluate AI\nmodels for detecting cracks and other defects in infrastructure. It includes a\ncrack image generator and realistic 3D inspection scenarios built in Unity. The\nbenchmark features two types of setups: a simple Fly-By wall inspection and a\nmore complex Underpass scene with lighting and geometry challenges. We tested a\npopular object detection model (YOLO) using different combinations of synthetic\nand real crack data. Results show that combining synthetic and real data\nimproves performance on real-world images. CERBERUS provides a flexible,\nrepeatable way to test defect detection systems and supports future research in\nautomated infrastructure inspection. CERBERUS is publicly available at\nhttps://github.com/justinreinman/Cerberus-Defect-Generator.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21909v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21909v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21571", "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21571v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21571v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.21589", "title": "A General Method for Detecting Information Generated by Large Language Models", "authors": ["Minjia Mao", "Dongjun Wei", "Xiao Fang", "Michael Chau"], "summary": "The proliferation of large language models (LLMs) has significantly\ntransformed the digital information landscape, making it increasingly\nchallenging to distinguish between human-written and LLM-generated content.\nDetecting LLM-generated information is essential for preserving trust on\ndigital platforms (e.g., social media and e-commerce sites) and preventing the\nspread of misinformation, a topic that has garnered significant attention in IS\nresearch. However, current detection methods, which primarily focus on\nidentifying content generated by specific LLMs in known domains, face\nchallenges in generalizing to new (i.e., unseen) LLMs and domains. This\nlimitation reduces their effectiveness in real-world applications, where the\nnumber of LLMs is rapidly multiplying and content spans a vast array of\ndomains. In response, we introduce a general LLM detector (GLD) that combines a\ntwin memory networks design and a theory-guided detection generalization module\nto detect LLM-generated information across unseen LLMs and domains. Using\nreal-world datasets, we conduct extensive empirical evaluations and case\nstudies to demonstrate the superiority of GLD over state-of-the-art detection\nmethods. The study has important academic and practical implications for\ndigital platforms and LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21589v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21589v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22189", "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sjölund"], "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22189v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22189v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21912", "title": "Generating Attribute-Aware Human Motions from Textual Prompt", "authors": ["Xinghan Wang", "Kun Xu", "Fei Li", "Cao Sheng", "Jiazhong Yu", "Yadong Mu"], "summary": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21912v1", "categories": ["cs.CV", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21912v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21573", "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21573v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21573v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2506.21590", "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "authors": ["Junqi Jiang", "Tom Bewley", "Salim I. Amoukou", "Francesco Leofante", "Antonio Rago", "Saumitra Mishra", "Francesca Toni"], "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21590v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21590v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22190", "title": "dreaMLearning: Data Compression Assisted Machine Learning", "authors": ["Xiaobo Zhao", "Aaron Hurst", "Panagiotis Karras", "Daniel E. Lucani"], "summary": "Despite rapid advancements, machine learning, particularly deep learning, is\nhindered by the need for large amounts of labeled data to learn meaningful\npatterns without overfitting and immense demands for computation and storage,\nwhich motivate research into architectures that can achieve good performance\nwith fewer resources. This paper introduces dreaMLearning, a novel framework\nthat enables learning from compressed data without decompression, built upon\nEntropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless\ncompression method that consolidates information into a compact set of\nrepresentative samples. DreaMLearning accommodates a wide range of data types,\ntasks, and model architectures. Extensive experiments on regression and\nclassification tasks with tabular and image data demonstrate that dreaMLearning\naccelerates training by up to 8.8x, reduces memory usage by 10x, and cuts\nstorage by 42%, with a minimal impact on model performance. These advancements\nenhance diverse ML applications, including distributed and federated learning,\nand tinyML on resource-constrained edge devices, unlocking new possibilities\nfor efficient and scalable learning.", "comment": "18 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.22190v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22190v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21920", "title": "SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition", "authors": ["Nam Quan Nguyen", "Xuan Phong Pham", "Tuan-Anh Tran"], "summary": "The automated reconstruction of the logical arrangement of tables from image\ndata, termed Table Structure Recognition (TSR), is fundamental for semantic\ndata extraction. Recently, researchers have explored a wide range of techniques\nto tackle this problem, demonstrating significant progress. Each table is a set\nof vertical and horizontal separators. Following this realization, we present\nSepFormer, which integrates the split-and-merge paradigm into a single step\nthrough separator regression with a DETR-style architecture, improving speed\nand robustness. SepFormer is a coarse-to-fine approach that predicts table\nseparators from single-line to line-strip separators with a stack of two\ntransformer decoders. In the coarse-grained stage, the model learns to\ngradually refine single-line segments through decoder layers with additional\nangle loss. At the end of the fine-grained stage, the model predicts line-strip\nseparators by refining sampled points from each single-line segment. Our\nSepFormer can run on average at 25.6 FPS while achieving comparable performance\nwith state-of-the-art methods on several benchmark datasets, including SciTSR,\nPubTabNet, WTW, and iFLYTAB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21920v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21920v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21574", "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "authors": ["Yicheng Mao", "Yang Zhao"], "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21574v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21574v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2506.21591", "title": "FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning", "authors": ["Shaoyu Dou", "Yutian Shen", "Mofan Chen", "Zixuan Wang", "Jiajie Xu", "Qi Guo", "Kailai Shao", "Chao Chen", "Haixiang Hu", "Haibo Shi", "Min Min", "Liwen Zhang"], "summary": "Large Language Models (LLMs) demonstrate significant potential but face\nchallenges in complex financial reasoning tasks requiring both domain knowledge\nand sophisticated reasoning. Current evaluation benchmarks often fall short by\nnot decoupling these capabilities indicators from single task performance and\nlack root cause analysis for task failure. To address this, we introduce\nFinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'\nknowledge and reasoning abilities independently, proposing distinct knowledge\nscore and reasoning score metrics. Inspired by cognitive science, we further\npropose a cognitive score based on Bloom's taxonomy to analyze capabilities in\nreasoning tasks across different cognitive levels. We also release a new\nopen-source Chinese financial reasoning dataset covering 22 subfields to\nsupport reproducible research and further advancements in financial reasoning.\nOur experimental results reveal that LLM reasoning ability and higher-order\ncognitive ability are the core factors influencing reasoning accuracy. We also\nspecifically find that even top models still face a bottleneck with knowledge\napplication. Furthermore, our analysis shows that specialized financial LLMs\ngenerally lag behind the top general large models across multiple metrics.", "comment": "Submitted to EMNLP 2025, 27 pages, 20 figures", "pdf_url": "http://arxiv.org/pdf/2506.21591v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21591v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22199", "title": "REDELEX: A Framework for Relational Deep Learning Exploration", "authors": ["Jakub Peleška", "Gustav Šír"], "summary": "Relational databases (RDBs) are widely regarded as the gold standard for\nstoring structured information. Consequently, predictive tasks leveraging this\ndata format hold significant application promise. Recently, Relational Deep\nLearning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized\nas graph structures, enabling the application of various graph neural\narchitectures to effectively address these tasks. However, given its novelty,\nthere is a lack of analysis into the relationships between the performance of\nvarious RDL models and the characteristics of the underlying RDBs.\n  In this study, we present REDELEX$-$a comprehensive exploration framework for\nevaluating RDL models of varying complexity on the most diverse collection of\nover 70 RDBs, which we make available to the community. Benchmarked alongside\nkey representatives of classic methods, we confirm the generally superior\nperformance of RDL while providing insights into the main factors shaping\nperformance, including model complexity, database sizes and their structural\nproperties.", "comment": "Accepted to ECMLPKDD 2025 at Porto, Portugal", "pdf_url": "http://arxiv.org/pdf/2506.22199v1", "categories": ["cs.LG", "cs.DB"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22199v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21923", "title": "ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction", "authors": ["Juming Xiong", "Ruining Deng", "Jialin Yue", "Siqi Lu", "Junlin Guo", "Marilyn Lionts", "Tianyuan Yao", "Can Cui", "Junchao Zhu", "Chongyu Qu", "Mengmeng Yin", "Haichun Yang", "Yuankai Huo"], "summary": "Histological analysis plays a crucial role in understanding tissue structure\nand pathology. While recent advancements in registration methods have improved\n2D histological analysis, they often struggle to preserve critical 3D spatial\nrelationships, limiting their utility in both clinical and research\napplications. Specifically, constructing accurate 3D models from 2D slices\nremains challenging due to tissue deformation, sectioning artifacts,\nvariability in imaging techniques, and inconsistent illumination. Deep\nlearning-based registration methods have demonstrated improved performance but\nsuffer from limited generalizability and require large-scale training data. In\ncontrast, non-deep-learning approaches offer better generalizability but often\ncompromise on accuracy. In this study, we introduced ZeroReg3D, a novel\nzero-shot registration pipeline tailored for accurate 3D reconstruction from\nserial histological sections. By combining zero-shot deep learning-based\nkeypoint matching with optimization-based affine and non-rigid registration\ntechniques, ZeroReg3D effectively addresses critical challenges such as tissue\ndeformation, sectioning artifacts, staining variability, and inconsistent\nillumination without requiring retraining or fine-tuning. The code has been\nmade publicly available at https://github.com/hrlblab/ZeroReg3D", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21923v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21923v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21575", "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Casper Hansen", "Julien Fauqueur"], "summary": "We propose STRuCT-LLM, a unified framework for training large language models\n(LLMs) to perform structured reasoning over both relational and\ngraph-structured data. Our approach jointly optimizes Text-to-SQL and\nText-to-Cypher tasks using reinforcement learning (RL) combined with\nChain-of-Thought (CoT) supervision. To support fine-grained optimization in\ngraph-based parsing, we introduce a topology-aware reward function based on\ngraph edit distance. Unlike prior work that treats relational and graph\nformalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL\nand Cypher to induce cross-formalism transfer, enabling SQL training to improve\nCypher performance and vice versa - even without shared schemas. Our largest\nmodel (QwQ-32B) achieves substantial relative improvements across tasks: on\nsemantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The\nmodel also demonstrates strong zero-shot generalization, improving performance\non downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA\n(CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results\ndemonstrate both the effectiveness of executable queries as scaffolds for\nstructured reasoning and the synergistic benefits of jointly training on SQL\nand Cypher (code available at https://github.com/bouv/STRuCT-LLM).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21575v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21575v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2506.21592", "title": "SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition", "authors": ["Tinh Nguyen", "Minh Khue Phan Tran"], "summary": "Sign language recognition is crucial for individuals with hearing impairments\nto break communication barriers. However, previous approaches have had to\nchoose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had\nproblems with vanishing gradients and high computational costs. Despite\nimproving performance, transformer-based methods were not commonly used. This\nstudy presents a new novel SLR approach that overcomes the challenge of\nindependently extracting meaningful information from the x and y coordinates of\nskeleton sequences, which traditional models often treat as inseparable. By\nutilizing an encoder-decoder of BART architecture, the model independently\nencodes the x and y coordinates, while Cross-Attention ensures their\ninterrelation is maintained. With only 749,888 parameters, the model achieves\n96.04% accuracy on the LSA-64 dataset, significantly outperforming previous\nmodels with over one million parameters. The model also demonstrates excellent\nperformance and generalization across WLASL and ASL-Citizen datasets. Ablation\nstudies underscore the importance of coordinate projection, normalization, and\nusing multiple skeleton components for boosting model efficacy. This study\noffers a reliable and effective approach for sign language recognition, with\nstrong potential for enhancing accessibility tools for the deaf and hard of\nhearing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21592v1", "categories": ["cs.CL", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21592v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22200", "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework", "authors": ["Chen Wang", "Lai Wei", "Yanzhi Zhang", "Chenyang Shao", "Zedong Dan", "Weiran Huang", "Yue Wang", "Yuzhi Zhang"], "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22200v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22200v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21924", "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding", "authors": ["Zhao Jin", "Rong-Cheng Tu", "Jingyi Liao", "Wenhao Sun", "Xiao Luo", "Shunyu Liu", "Dacheng Tao"], "summary": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21924v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21924v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21576", "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21576v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21576v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21594", "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training", "authors": ["Ahmed M. Adly", "Mostafa Samy", "Amr Fawzy"], "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21594v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21594v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22253", "title": "Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence", "authors": ["Shunta Nonaga", "Koji Tabata", "Yuta Mizuno", "Tamiki Komatsuzaki"], "summary": "Decision making under uncertain environments in the maximization of expected\nreward while minimizing its risk is one of the ubiquitous problems in many\nsubjects. Here, we introduce a novel problem setting in stochastic bandit\noptimization that jointly addresses two critical aspects of decision-making:\nmaximizing expected reward and minimizing associated uncertainty, quantified\nvia the mean-variance(MV) criterion. Unlike traditional bandit formulations\nthat focus solely on expected returns, our objective is to efficiently and\naccurately identify the Pareto-optimal set of arms that strikes the best\ntrade-off between expected performance and risk. We propose a unified\nmeta-algorithmic framework capable of operating under both fixed-confidence and\nfixed-budget regimes, achieved through adaptive design of confidence intervals\ntailored to each scenario using the same sample exploration strategy. We\nprovide theoretical guarantees on the correctness of the returned solutions in\nboth settings. To complement this theoretical analysis, we conduct extensive\nempirical evaluations across synthetic benchmarks, demonstrating that our\napproach outperforms existing methods in terms of both accuracy and sample\nefficiency, highlighting its broad applicability to risk-aware decision-making\ntasks in uncertain environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22253v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22253v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21925", "title": "Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images", "authors": ["Liu Yang", "Huiyu Duan", "Jiarui Wang", "Jing Liu", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Patrick Le Callet"], "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) techniques, AI generated images (AIGIs) have attracted widespread\nattention, among which AI generated omnidirectional images (AIGODIs) hold\nsignificant potential for Virtual Reality (VR) and Augmented Reality (AR)\napplications. AI generated omnidirectional images exhibit unique quality\nissues, however, research on the quality assessment and optimization of\nAI-generated omnidirectional images is still lacking. To this end, this work\nfirst studies the quality assessment and distortion-aware saliency prediction\nproblems for AIGODIs, and further presents a corresponding optimization\nprocess. Specifically, we first establish a comprehensive database to reflect\nhuman feedback for AI-generated omnidirectionals, termed OHF2024, which\nincludes both subjective quality ratings evaluated from three perspectives and\ndistortion-aware salient regions. Based on the constructed OHF2024 database, we\npropose two models with shared encoders based on the BLIP-2 model to evaluate\nthe human visual experience and predict distortion-aware saliency for\nAI-generated omnidirectional images, which are named as BLIP2OIQA and\nBLIP2OISal, respectively. Finally, based on the proposed models, we present an\nautomatic optimization process that utilizes the predicted visual experience\nscores and distortion regions to further enhance the visual quality of an\nAI-generated omnidirectional image. Extensive experiments show that our\nBLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in\nthe human visual experience evaluation task and the distortion-aware saliency\nprediction task for AI generated omnidirectional images, and can be effectively\nused in the optimization process. The database and codes will be released on\nhttps://github.com/IntMeGroup/AIGCOIQA to facilitate future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21925v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21925v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21577", "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.21577v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21577v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21595", "title": "Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources", "authors": ["Jinpyo Kim", "Gyeongje Cho", "Chanwoo Park", "Jongwon Park", "Jongmin Kim", "Yeonkyoun So", "Jaejin Lee"], "summary": "Since state-of-the-art LLMs often underperform in languages other than\nEnglish or Chinese, improving the capability of LLMs in new languages has\nbecome an essential task. Moreover, LLMs' entire end-to-end training process\nremains largely unknown to the public due to proprietary reasons, technical\ncomplexity, inconsistent documentation, and ethical considerations. The\ncomplete picture remains a closely guarded secret within the industry. This\npaper presents methods to adapt an existing English-based LLM to Korean in a\nlow-budget scenario. We describe the entire end-to-end process: collecting\nKorean datasets, preprocessing the data, training the model, creating\ndownstream benchmarks, and conducting evaluations. The evaluation results\nindicate that our method can effectively and cost-efficiently add new language\ncapabilities to existing LLMs. Our new bilingual models, Thunder-LLM and\nThunder-LLM-Ins, achieve superior Korean performance compared to\nstate-of-the-art models while utilizing minimal data and computational\nresources. We share our comprehensive experience and make the code publicly\navailable.", "comment": "Submitted to ARR 2025 May cycle", "pdf_url": "http://arxiv.org/pdf/2506.21595v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21595v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22255", "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "authors": ["Maciej Stefaniak", "Michał Krutul", "Jan Małaśnicki", "Maciej Pióro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22255v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22255v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21945", "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images", "authors": ["Naftaly Wambugu", "Ruisheng Wang", "Bo Guo", "Tianshu Yu", "Sheng Xu", "Mohammed Elhassan"], "summary": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21945v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21945v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21578", "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models", "authors": ["Andrew Maranhão Ventura D'addario"], "summary": "The evaluation of Large Language Models (LLMs) in healthcare has been\ndominated by physician-centric, English-language benchmarks, creating a\ndangerous illusion of competence that ignores the interprofessional nature of\npatient care. To provide a more holistic and realistic assessment, we introduce\nHealthQA-BR, the first large-scale, system-wide benchmark for\nPortuguese-speaking healthcare. Comprising 5,632 questions from Brazil's\nnational licensing and residency exams, it uniquely assesses knowledge not only\nin medicine and its specialties but also in nursing, dentistry, psychology,\nsocial work, and other allied health professions. We conducted a rigorous\nzero-shot evaluation of over 20 leading LLMs. Our results reveal that while\nstate-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),\nthis top-line score masks alarming, previously unmeasured deficiencies. A\ngranular analysis shows performance plummets from near-perfect in specialties\nlike Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most\nnotably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic\nissue observed across all models, demonstrating that high-level scores are\ninsufficient for safety validation. By publicly releasing HealthQA-BR and our\nevaluation suite, we provide a crucial tool to move beyond single-score\nevaluations and toward a more honest, granular audit of AI readiness for the\nentire healthcare team.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21578v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21578v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21596", "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nsuccess in vision--language tasks. However, their capacity to reason over\ncomplex, long lessons and intricate educational diagrams that cannot be\nrepresented as a single natural image remains largely untested. In this work,\nwe present the first evaluation of state-of-the-art MLLMs on the textbook\nquestion answering (TQA) task using the CK12-QA dataset. We assess the\nperformance of recent vision-language models, including LLaVA and LLaMA\n3.2-Vision, across various input configurations. Additionally, we introduce a\nlightweight multimodal retrieval-augmented generation (RAG) pipeline that\nintegrates both paragraphs and diagrams from the lesson into the prompt. Our\nresults demonstrate the influence of retrieved educational context on model\naccuracy and reasoning, while also revealing current limitations in handling\nquestion-context relationships and the potential for noise, pointing to key\ndirections for future research in multimodal AI-driven learning.", "comment": "7 Pages", "pdf_url": "http://arxiv.org/pdf/2506.21596v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21596v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22295", "title": "Score-Based Model for Low-Rank Tensor Recovery", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "summary": "Low-rank tensor decompositions (TDs) provide an effective framework for\nmultiway data analysis. Traditional TD methods rely on predefined structural\nassumptions, such as CP or Tucker decompositions. From a probabilistic\nperspective, these can be viewed as using Dirac delta distributions to model\nthe relationships between shared factors and the low-rank tensor. However, such\nprior knowledge is rarely available in practical scenarios, particularly\nregarding the optimal rank structure and contraction rules. The optimization\nprocedures based on fixed contraction rules are complex, and approximations\nmade during these processes often lead to accuracy loss. To address this issue,\nwe propose a score-based model that eliminates the need for predefined\nstructural or distributional assumptions, enabling the learning of\ncompatibility between tensors and shared factors. Specifically, a neural\nnetwork is designed to learn the energy function, which is optimized via score\nmatching to capture the gradient of the joint log-probability of tensor entries\nand shared factors. Our method allows for modeling structures and distributions\nbeyond the Dirac delta assumption. Moreover, integrating the block coordinate\ndescent (BCD) algorithm with the proposed smooth regularization enables the\nmodel to perform both tensor completion and denoising. Experimental results\ndemonstrate significant performance improvements across various tensor types,\nincluding sparse and continuous-time tensors, as well as visual data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22295v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22295v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21957", "title": "Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding", "authors": ["Yixin Zha", "Chuxin Wang", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Point cloud understanding aims to acquire robust and general feature\nrepresentations from unlabeled data. Masked point modeling-based methods have\nrecently shown significant performance across various downstream tasks. These\npre-training methods rely on random masking strategies to establish the\nperception of point clouds by restoring corrupted point cloud inputs, which\nleads to the failure of capturing reasonable semantic relationships by the\nself-supervised models. To address this issue, we propose Semantic Masked\nAutoencoder, which comprises two main components: a prototype-based component\nsemantic modeling module and a component semantic-enhanced masking strategy.\nSpecifically, in the component semantic modeling module, we design a component\nsemantic guidance mechanism to direct a set of learnable prototypes in\ncapturing the semantics of different components from objects. Leveraging these\nprototypes, we develop a component semantic-enhanced masking strategy that\naddresses the limitations of random masking in effectively covering complete\ncomponent structures. Furthermore, we introduce a component semantic-enhanced\nprompt-tuning strategy, which further leverages these prototypes to improve the\nperformance of pre-trained models in downstream tasks. Extensive experiments\nconducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart\ndemonstrate the effectiveness of our proposed modules.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21957v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21957v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21579", "title": "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation", "authors": ["Yingzhi He", "Xiaohao Liu", "An Zhang", "Yunshan Ma", "Tat-Seng Chua"], "summary": "Sequential recommendation aims to predict users' future interactions by\nmodeling collaborative filtering (CF) signals from historical behaviors of\nsimilar users or items. Traditional sequential recommenders predominantly rely\non ID-based embeddings, which capture CF signals through high-order\nco-occurrence patterns. However, these embeddings depend solely on past\ninteractions, lacking transferable knowledge to generalize to unseen domains.\nRecent advances in large language models (LLMs) have motivated text-based\nrecommendation approaches that derive item representations from textual\ndescriptions. While these methods enhance generalization, they fail to encode\nCF signals-i.e., latent item correlations and preference patterns-crucial for\neffective recommendation. We argue that an ideal embedding model should\nseamlessly integrate CF signals with rich semantic representations to improve\nboth in-domain and out-of-domain recommendation performance.\n  To this end, we propose LLM2Rec, a novel embedding model tailored for\nsequential recommendation, integrating the rich semantic understanding of LLMs\nwith CF awareness. Our approach follows a two-stage training framework: (1)\nCollaborative Supervised Fine-tuning, which adapts LLMs to infer item\nrelationships based on historical interactions, and (2) Item-level Embedding\nModeling, which refines these specialized LLMs into structured item embedding\nmodels that encode both semantic and collaborative information. Extensive\nexperiments on real-world datasets demonstrate that LLM2Rec effectively\nimproves recommendation quality across both in-domain and out-of-domain\nsettings. Our findings highlight the potential of leveraging LLMs to build more\nrobust, generalizable embedding models for sequential recommendation. Our codes\nare available at https://github.com/HappyPointer/LLM2Rec.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.21579v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21579v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21597", "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering", "authors": ["Brandon Colelough", "Davis Bartels", "Dina Demner-Fushman"], "summary": "In this paper, we present an overview of ClinIQLink, a shared task,\ncollocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test\nlarge language models (LLMs) on medically-oriented question answering aimed at\nthe level of a General Practitioner. The challenge supplies 4,978\nexpert-verified, medical source-grounded question-answer pairs that cover seven\nformats: true/false, multiple choice, unordered list, short answer,\nshort-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled\nin Docker or Apptainer images, are executed on the CodaBench platform or the\nUniversity of Maryland's Zaratan cluster. An automated harness (Task 1) scores\nclosed-ended items by exact match and open-ended items with a three-tier\nembedding metric. A subsequent physician panel (Task 2) audits the top model\nresponses.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21597v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21597v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22299", "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks", "authors": ["Tao Liu", "Longlong Lin", "Yunfeng Yu", "Xi Ou", "Youan Zhang", "Zhiqiu Ye", "Tao Jia"], "summary": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.", "comment": "icmr", "pdf_url": "http://arxiv.org/pdf/2506.22299v1", "categories": ["cs.LG", "cs.AI", "I.2"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22299v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21975", "title": "TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models", "authors": ["Meng Yu", "Te Cui", "Qitong Chu", "Wenjie Song", "Yi Yang", "Yufeng Yue"], "summary": "Reliable semantic segmentation of open environments is essential for\nintelligent systems, yet significant problems remain: 1) Existing RGB-T\nsemantic segmentation models mainly rely on low-level visual features and lack\nhigh-level textual information, which struggle with accurate segmentation when\ncategories share similar visual characteristics. 2) While SAM excels in\ninstance-level segmentation, integrating it with thermal images and text is\nhindered by modality heterogeneity and computational inefficiency. To address\nthese, we propose TASeg, a text-aware RGB-T segmentation framework by using\nLow-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation\nmodels. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the\nimage encoder, which effectively merges features from multiple visual\nmodalities while freezing SAM's original transformer blocks. Additionally, we\nincorporate CLIP-generated text embeddings in the mask decoder to enable\nsemantic alignment, which further rectifies the classification error and\nimproves the semantic understanding accuracy. Experimental results across\ndiverse datasets demonstrate that our method achieves superior performance in\nchallenging scenarios with fewer trainable parameters.", "comment": "6 pages, accepted for publication in lEEE/RSJ international\n  Conference on Intelligent Robots and Systems (lROS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.21975v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21975v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21580", "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21580v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21580v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21600", "title": "Structured Attention Matters to Multimodal LLMs in Document Understanding", "authors": ["Chang Liu", "Hongkai Chen", "Yujun Cai", "Hang Wu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "summary": "Document understanding remains a significant challenge for multimodal large\nlanguage models (MLLMs). While previous research has primarily focused on\nlocating evidence pages through precise multimodal queries, our work\ninvestigates a fundamental yet overlooked aspect: how input format influences\ndocument comprehension performance. Through systematic analysis, we discover\nthat raw OCR text often impairs rather than improves MLLMs' performance, which\nis a counterintuitive finding we attribute to attention dispersion and\nstructure loss. To further substantiate our hypothesis, we propose a novel\nstructure-preserving approach that encodes document elements using the LaTex\nparadigm, maintaining the hierarchical organization and spatial relationships\ncritical for comprehension. Our attention analysis reveals that structured text\ninduces structured attention patterns on both textual and visual content,\ndirecting models to focus on semantically meaningful regions while reducing\nattention waste. This approach significantly enhances MLLMs' document question\nanswering performance across diverse document types without requiring\narchitectural modifications or additional training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21600v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21600v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22301", "title": "Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling", "authors": ["Takumi Okuo", "Shinnosuke Matsuo", "Shota Harada", "Kiyohito Tanaka", "Ryoma Bise"], "summary": "Domain shift is a significant challenge in machine learning, particularly in\nmedical applications where data distributions differ across institutions due to\nvariations in data collection practices, equipment, and procedures. This can\ndegrade performance when models trained on source domain data are applied to\nthe target domain. Domain adaptation methods have been widely studied to\naddress this issue, but most struggle when class proportions between the source\nand target domains differ. In this paper, we propose a weakly-supervised domain\nadaptation method that leverages class proportion information from the target\ndomain, which is often accessible in medical datasets through prior knowledge\nor statistical reports. Our method assigns pseudo-labels to the unlabeled\ntarget data based on class proportion (called proportion-constrained\npseudo-labeling), improving performance without the need for additional\nannotations. Experiments on two endoscopic datasets demonstrate that our method\noutperforms semi-supervised domain adaptation techniques, even when 5% of the\ntarget domain is labeled. Additionally, the experimental results with noisy\nproportion labels highlight the robustness of our method, further demonstrating\nits effectiveness in real-world application scenarios.", "comment": "Accepted at IJCNN2025", "pdf_url": "http://arxiv.org/pdf/2506.22301v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22301v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21980", "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "authors": ["Biao Wang", "Wenwen Li"], "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "comment": "7 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21980v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21980v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21581", "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21581v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21581v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.21602", "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models", "authors": ["Xiaoyan Feng", "He Zhang", "Yanjun Zhang", "Leo Yu Zhang", "Shirui Pan"], "summary": "Recent advances in Large Language Models (LLMs) have raised urgent concerns\nabout LLM-generated text authenticity, prompting regulatory demands for\nreliable identification mechanisms. Although watermarking offers a promising\nsolution, existing approaches struggle to simultaneously achieve three critical\nrequirements: text quality preservation, model-agnostic detection, and message\nembedding capacity, which are crucial for practical implementation. To achieve\nthese goals, the key challenge lies in balancing the trade-off between text\nquality preservation and message embedding capacity. To address this challenge,\nwe propose BiMark, a novel watermarking framework that achieves these\nrequirements through three key innovations: (1) a bit-flip unbiased reweighting\nmechanism enabling model-agnostic detection, (2) a multilayer architecture\nenhancing detectability without compromising generation quality, and (3) an\ninformation encoding approach supporting multi-bit watermarking. Through\ntheoretical analysis and extensive experiments, we validate that, compared to\nstate-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%\nhigher extraction rates for short texts while maintaining text quality\nindicated by lower perplexity, and performs comparably to non-watermarked text\non downstream tasks such as summarization and translation.", "comment": "This paper is accepted by International Conference on Machine\n  Learning (ICML) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21602v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21602v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22304", "title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "authors": ["Erkan Turan", "Aristotelis Siozopoulos", "Maks Ovsjanikov"], "summary": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22304v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22304v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22007", "title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Soumajit Majumder", "Ziyuan Liu", "Gitta Kutyniok", "Abhinav Valada"], "summary": "We address the problem of generating long-horizon videos for robotic\nmanipulation tasks. Text-to-video diffusion models have made significant\nprogress in photorealism, language understanding, and motion generation but\nstruggle with long-horizon robotic tasks. Recent works use video diffusion\nmodels for high-quality simulation data and predictive rollouts in robot\nplanning. However, these works predict short sequences of the robot achieving\none task and employ an autoregressive paradigm to extend to the long horizon,\nleading to error accumulations in the generated video and in the execution. To\novercome these limitations, we propose a novel pipeline that bypasses the need\nfor autoregressive generation. We achieve this through a threefold\ncontribution: 1) we first decompose the high-level goals into smaller atomic\ntasks and generate keyframes aligned with these instructions. A second\ndiffusion model then interpolates between each of the two generated frames,\nachieving the long-horizon video. 2) We propose a semantics preserving\nattention module to maintain consistency between the keyframes. 3) We design a\nlightweight policy model to regress the robot joint states from generated\nvideos. Our approach achieves state-of-the-art results on two benchmarks in\nvideo quality and consistency while outperforming previous policy models on\nlong-horizon tasks.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22007v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22007v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21582v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21582v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21603", "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "authors": ["Yenisel Plasencia-Calaña"], "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21603v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21603v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22331", "title": "Less Greedy Equivalence Search", "authors": ["Adiba Ejaz", "Elias Bareinboim"], "summary": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.", "comment": "35 total pages. 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.22331v1", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22331v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22015", "title": "Towards Universal & Efficient Model Compression via Exponential Torque Pruning", "authors": ["Sarthak Ketanbhai Modi", "Lim Zi Pong", "Shourya Kuchhal", "Yoshi Cao", "Yupeng Cheng", "Teo Yon Shin", "Lin Shang-Wei", "Zhiming Li"], "summary": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22015v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22015v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21583", "title": "Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "Ameer Hamza", "Ildar Batyrshin", "Grigori Sidorov"], "summary": "Hope is a positive emotional state involving the expectation of favorable\nfuture outcomes, while hope speech refers to communication that promotes\noptimism, resilience, and support, particularly in adverse contexts. Although\nhope speech detection has gained attention in Natural Language Processing\n(NLP), existing research mainly focuses on high-resource languages and\nstandardized scripts, often overlooking informal and underrepresented forms\nsuch as Roman Urdu. To the best of our knowledge, this is the first study to\naddress hope speech detection in code-mixed Roman Urdu by introducing a\ncarefully annotated dataset, thereby filling a critical gap in inclusive NLP\nresearch for low-resource, informal language varieties. This study makes four\nkey contributions: (1) it introduces the first multi-class annotated dataset\nfor Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,\nUnrealistic Hope, and Not Hope categories; (2) it explores the psychological\nfoundations of hope and analyzes its linguistic patterns in code-mixed Roman\nUrdu to inform dataset development; (3) it proposes a custom attention-based\ntransformer model optimized for the syntactic and semantic variability of Roman\nUrdu, evaluated using 5-fold cross-validation; and (4) it verifies the\nstatistical significance of performance gains using a t-test. The proposed\nmodel, XLM-R, achieves the best performance with a cross-validation score of\n0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%\nand 2.63% respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21583v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21583v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21605", "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents", "authors": ["Haoran Tan", "Zeyu Zhang", "Chen Ma", "Xu Chen", "Quanyu Dai", "Zhenhua Dong"], "summary": "Recent works have highlighted the significance of memory mechanisms in\nLLM-based agents, which enable them to store observed information and adapt to\ndynamic environments. However, evaluating their memory capabilities still\nremains challenges. Previous evaluations are commonly limited by the diversity\nof memory levels and interactive scenarios. They also lack comprehensive\nmetrics to reflect the memory capabilities from multiple aspects. To address\nthese problems, in this paper, we construct a more comprehensive dataset and\nbenchmark to evaluate the memory capability of LLM-based agents. Our dataset\nincorporates factual memory and reflective memory as different levels, and\nproposes participation and observation as various interactive scenarios. Based\non our dataset, we present a benchmark, named MemBench, to evaluate the memory\ncapability of LLM-based agents from multiple aspects, including their\neffectiveness, efficiency, and capacity. To benefit the research community, we\nrelease our dataset and project at https://github.com/import-myself/Membench.", "comment": "17 pages, 5 figures. Accepted by ACL 2025 findings", "pdf_url": "http://arxiv.org/pdf/2506.21605v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21605v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22342", "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis", "authors": ["Zihan Guan", "Zhiyuan Zhao", "Fengwei Tian", "Dung Nguyen", "Payel Bhattacharjee", "Ravi Tandon", "B. Aditya Prakash", "Anil Vullikanti"], "summary": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.", "comment": "17 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22342v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22342v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22022", "title": "Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision", "authors": ["Zhanyi Lu", "Yue Zhou"], "summary": "Facial stylization aims to transform facial images into appealing,\nhigh-quality stylized portraits, with the critical challenge of accurately\nlearning the target style while maintaining content consistency with the\noriginal image. Although previous StyleGAN-based methods have made significant\nadvancements, the generated results still suffer from artifacts or insufficient\nfidelity to the source image. We argue that these issues stem from neglecting\nsemantic shift of the generator during stylization. Therefore, we propose a\nfacial stylization method that integrates semantic preservation constraint and\npseudo-paired supervision to enhance the content correspondence and improve the\nstylization effect. Additionally, we develop a methodology for creating\nmulti-level pseudo-paired datasets to implement supervisory constraint.\nFurthermore, building upon our facial stylization framework, we achieve more\nflexible multimodal and reference-guided stylization without complex network\narchitecture designs or additional training. Experimental results demonstrate\nthat our approach produces high-fidelity, aesthetically pleasing facial style\ntransfer that surpasses previous methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22022v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22022v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21584", "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "authors": ["J. Koorndijk"], "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21584v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21584v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21606", "title": "Large Language Models as symbolic DNA of cultural dynamics", "authors": ["Parham Pourdavood", "Michael Jacob", "Terrence Deacon"], "summary": "This paper proposes a novel conceptualization of Large Language Models (LLMs)\nas externalized informational substrates that function analogously to DNA for\nhuman cultural dynamics. Rather than viewing LLMs as either autonomous\nintelligence or mere programmed mimicry, we argue they serve a broader role as\nrepositories that preserve compressed patterns of human symbolic\nexpression--\"fossils\" of meaningful dynamics that retain relational residues\nwithout their original living contexts. Crucially, these compressed patterns\nonly become meaningful through human reinterpretation, creating a recursive\nfeedback loop where they can be recombined and cycle back to ultimately\ncatalyze human creative processes. Through analysis of four universal\nfeatures--compression, decompression, externalization, and recursion--we\ndemonstrate that just as DNA emerged as a compressed and externalized medium\nfor preserving useful cellular dynamics without containing explicit reference\nto goal-directed physical processes, LLMs preserve useful regularities of human\nculture without containing understanding of embodied human experience.\nTherefore, we argue that LLMs' significance lies not in rivaling human\nintelligence, but in providing humanity a tool for self-reflection and playful\nhypothesis-generation in a low-stakes, simulated environment. This framework\npositions LLMs as tools for cultural evolvability, enabling humanity to\ngenerate novel hypotheses about itself while maintaining the human\ninterpretation necessary to ground these hypotheses in ongoing human aesthetics\nand norms.", "comment": "28 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.21606v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21606v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22365", "title": "Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation", "authors": ["Tao Li", "Haozhe Lei", "Mingsheng Yin", "Yaqi Hu"], "summary": "When using reinforcement learning (RL) to tackle physical control tasks,\ninductive biases that encode physics priors can help improve sample efficiency\nduring training and enhance generalization in testing. However, the current\npractice of incorporating these helpful physics-informed inductive biases\ninevitably runs into significant manual labor and domain expertise, making them\nprohibitive for general users. This work explores a symbolic approach to\ndistill physics-informed inductive biases into RL agents, where the physics\npriors are expressed in a domain-specific language (DSL) that is human-readable\nand naturally explainable. Yet, the DSL priors do not translate directly into\nan implementable policy due to partial and noisy observations and additional\nphysical constraints in navigation tasks. To address this gap, we develop a\nphysics-informed program-guided RL (PiPRL) framework with applications to\nindoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic\nintegration, where a meta symbolic program receives semantically meaningful\nfeatures from a neural perception module, which form the bases for symbolic\nprogramming that encodes physics priors and guides the RL process of a\nlow-level neural controller. Extensive experiments demonstrate that PiPRL\nconsistently outperforms purely symbolic or neural policies and reduces\ntraining time by over 26% with the help of the program-based inductive biases.", "comment": "Spotlight paper at Reinforcement Learning Conference 2025, Workshop\n  on Inductive Biases in Reinforcement Learning", "pdf_url": "http://arxiv.org/pdf/2506.22365v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22365v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22027", "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "authors": ["Han Wang", "Shengyang Li", "Jian Yang", "Yuxuan Liu", "Yixuan Lv", "Zhuang Zhou"], "summary": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22027v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22027v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21586", "title": "Can Vision Language Models Understand Mimed Actions?", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21586v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21586v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21607", "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21607v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21607v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22374", "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "summary": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.", "comment": "13 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.22374v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22374v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22032", "title": "Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen\nclasses using supervision from only seen classes. Beyond adaptation-based\nmethods, distillation-based approaches transfer vision-language alignment of\nvision-language model, e.g., CLIP, to segmentation models. However, such\nknowledge transfer remains challenging due to: (1) the difficulty of aligning\nvision-based features with the textual space, which requires combining spatial\nprecision with vision-language alignment; and (2) the semantic gap between\nCLIP's global representations and the local, fine-grained features of\nsegmentation models. To address challenge (1), we propose Chimera-Seg, which\nintegrates a segmentation backbone as the body and a CLIP-based semantic head\nas the head, like the Chimera in Greek mythology, combining spatial precision\nwith vision-language alignment. Specifically, Chimera-Seg comprises a trainable\nsegmentation model and a CLIP Semantic Head (CSH), which maps dense features\ninto the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed\nprojection layers from the CLIP visual encoder, along with lightweight\ntrainable components. The partial module from CLIP visual encoder, paired with\nthe segmentation model, retains segmentation capability while easing the\nmapping to CLIP's semantic space. To address challenge (2), we propose\nSelective Global Distillation (SGD), which distills knowledge from dense\nfeatures exhibiting high similarity to the CLIP CLS token, while gradually\nreducing the number of features used for alignment as training progresses.\nBesides, we also use a Semantic Alignment Module (SAM) to further align dense\nvisual features with semantic embeddings extracted from the frozen CLIP text\nencoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in\nhIoU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22032v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22032v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21596", "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nsuccess in vision--language tasks. However, their capacity to reason over\ncomplex, long lessons and intricate educational diagrams that cannot be\nrepresented as a single natural image remains largely untested. In this work,\nwe present the first evaluation of state-of-the-art MLLMs on the textbook\nquestion answering (TQA) task using the CK12-QA dataset. We assess the\nperformance of recent vision-language models, including LLaVA and LLaMA\n3.2-Vision, across various input configurations. Additionally, we introduce a\nlightweight multimodal retrieval-augmented generation (RAG) pipeline that\nintegrates both paragraphs and diagrams from the lesson into the prompt. Our\nresults demonstrate the influence of retrieved educational context on model\naccuracy and reasoning, while also revealing current limitations in handling\nquestion-context relationships and the potential for noise, pointing to key\ndirections for future research in multimodal AI-driven learning.", "comment": "7 Pages", "pdf_url": "http://arxiv.org/pdf/2506.21596v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21596v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.21608", "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2", "authors": ["Yasmine Bouamra", "Bruno Yun", "Alexandre Poisson", "Frédéric Armetta"], "summary": "The automatic generation of SysML v2 models represents a major challenge in\nthe engineering of complex systems, particularly due to the scarcity of\nlearning corpora and complex syntax. We present SysTemp, a system aimed at\nfacilitating and improving the creation of SysML v2 models from natural\nlanguage specifications. It is based on a multi-agent system, including a\ntemplate generator that structures the generation process. We discuss the\nadvantages and challenges of this system through an evaluation, highlighting\nits potential to improve the quality of the generations in SysML v2 modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21608v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21608v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22376", "title": "Probabilistic Optimality for Inference-time Scaling", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22376v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22376v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22044", "title": "Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field", "authors": ["Hong Nie", "Fuyuan Cao", "Lu Chen", "Fengxin Chen", "Yuefeng Zou", "Jun Yu"], "summary": "Reconstruction and rendering-based talking head synthesis methods achieve\nhigh-quality results with strong identity preservation but are limited by their\ndependence on identity-specific models. Each new identity requires training\nfrom scratch, incurring high computational costs and reduced scalability\ncompared to generative model-based approaches. To overcome this limitation, we\npropose FIAG, a novel 3D speaking head synthesis framework that enables\nefficient identity-specific adaptation using only a few training footage. FIAG\nincorporates Global Gaussian Field, which supports the representation of\nmultiple identities within a shared field, and Universal Motion Field, which\ncaptures the common motion dynamics across diverse identities. Benefiting from\nthe shared facial structure information encoded in the Global Gaussian Field\nand the general motion priors learned in the motion field, our framework\nenables rapid adaptation from canonical identity representations to specific\nones with minimal data. Extensive comparative and ablation experiments\ndemonstrate that our method outperforms existing state-of-the-art approaches,\nvalidating both the effectiveness and generalizability of the proposed\nframework. Code is available at: \\textit{https://github.com/gme-hong/FIAG}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22044v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22044v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21597", "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering", "authors": ["Brandon Colelough", "Davis Bartels", "Dina Demner-Fushman"], "summary": "In this paper, we present an overview of ClinIQLink, a shared task,\ncollocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test\nlarge language models (LLMs) on medically-oriented question answering aimed at\nthe level of a General Practitioner. The challenge supplies 4,978\nexpert-verified, medical source-grounded question-answer pairs that cover seven\nformats: true/false, multiple choice, unordered list, short answer,\nshort-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled\nin Docker or Apptainer images, are executed on the CodaBench platform or the\nUniversity of Maryland's Zaratan cluster. An automated harness (Task 1) scores\nclosed-ended items by exact match and open-ended items with a three-tier\nembedding metric. A subsequent physician panel (Task 2) audits the top model\nresponses.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21597v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21597v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.21609", "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models", "authors": ["Junhao Liu", "Zhenhao Xu", "Yuxin Fang", "Yichuan Chen", "Zuobin Ying", "Wenhan Chang"], "summary": "Recently, there have been notable advancements in large language models\n(LLMs), demonstrating their growing abilities in complex reasoning. However,\nexisting research largely overlooks a thorough and systematic comparison of\nthese models' reasoning processes and outputs, particularly regarding their\nself-reflection pattern (also termed \"Aha moment\") and the interconnections\nacross diverse domains. This paper proposes a novel framework for analyzing the\nreasoning characteristics of four cutting-edge large reasoning models (GPT-o1,\nDeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge\nparadigm. Our approach connects their internal thinking processes with their\nfinal outputs. A diverse dataset consists of real-world scenario-based\nquestions covering logical deduction, causal inference, and multi-step\nproblem-solving. Additionally, a set of metrics is put forward to assess both\nthe coherence of reasoning and the accuracy of the outputs. The research\nresults uncover various patterns of how these models balance exploration and\nexploitation, deal with problems, and reach conclusions during the reasoning\nprocess. Through quantitative and qualitative comparisons, disparities among\nthese models are identified in aspects such as the depth of reasoning, the\nreliance on intermediate steps, and the degree of similarity between their\nthinking processes and output patterns and those of GPT-o1. This work offers\nvaluable insights into the trade-off between computational efficiency and\nreasoning robustness and provides practical recommendations for enhancing model\ndesign and evaluation in practical applications. We publicly release our\nproject at: https://github.com/ChangWenhan/FromThinking2Output", "comment": "18 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21609v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21609v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22389", "title": "Towards Distributed Neural Architectures", "authors": ["Aditya Cowsik", "Tianyu He", "Andrey Gromov"], "summary": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.", "comment": "36 pages, 25 figures", "pdf_url": "http://arxiv.org/pdf/2506.22389v1", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22389v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22063", "title": "EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode", "authors": ["Durgesh K. Singh", "Ahcene Boubekki", "Qing Cao", "Svein Arne Aase", "Robert Jenssen", "Michael Kampffmeyer"], "summary": "Linear measurements of the left ventricle (LV) in the Parasternal Long Axis\n(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.\nThese involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular\nto the LV axis near the mitral valve tips. Manual placement is time-consuming\nand error-prone, while existing deep learning methods often misalign landmarks,\ncausing inaccurate measurements. We propose a novel framework that enhances LV\nmeasurement accuracy by enforcing straight-line constraints. A landmark\ndetector is trained on Anatomical M-Mode (AMM) images, computed in real time\nfrom B-mode videos, then transformed back to B-mode space. This approach\naddresses misalignment and reduces measurement errors. Experiments show\nimproved accuracy over standard B-mode methods, and the framework generalizes\nwell across network architectures. Our semi-automatic design includes a\nhuman-in-the-loop step where the user only places the SL, simplifying\ninteraction while preserving alignment flexibility and clinical relevance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22063v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22063v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21599", "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barthélemy", "Tomasz Bednarz", "Flora D. Salim"], "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21599v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21599v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21611", "title": "Does Multimodality Lead to Better Time Series Forecasting?", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21611v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21611v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22393", "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis", "authors": ["YongKyung Oh", "Alex Bui"], "summary": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22393v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22393v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22065", "title": "MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation", "authors": ["Dechao Meng", "Steven Xiao", "Xindi Zhang", "Guangyuan Wang", "Peng Zhang", "Qi Wang", "Bang Zhang", "Liefeng Bo"], "summary": "Audio-driven portrait animation, which synthesizes realistic videos from\nreference images using audio signals, faces significant challenges in real-time\ngeneration of high-fidelity, temporally coherent animations. While recent\ndiffusion-based methods improve generation quality by integrating audio into\ndenoising processes, their reliance on frame-by-frame UNet architectures\nintroduces prohibitive latency and struggles with temporal consistency. This\npaper introduces MirrorMe, a real-time, controllable framework built on the LTX\nvideo model, a diffusion transformer that compresses video spatially and\ntemporally for efficient latent space denoising. To address LTX's trade-offs\nbetween compression and semantic fidelity, we propose three innovations: 1. A\nreference identity injection mechanism via VAE-encoded image concatenation and\nself-attention, ensuring identity consistency; 2. A causal audio encoder and\nadapter tailored to LTX's temporal structure, enabling precise audio-expression\nsynchronization; and 3. A progressive training strategy combining close-up\nfacial training, half-body synthesis with facial masking, and hand pose\nintegration for enhanced gesture control. Extensive experiments on the EMTD\nBenchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,\nlip-sync accuracy, and temporal stability.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22065v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22065v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21600", "title": "Structured Attention Matters to Multimodal LLMs in Document Understanding", "authors": ["Chang Liu", "Hongkai Chen", "Yujun Cai", "Hang Wu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "summary": "Document understanding remains a significant challenge for multimodal large\nlanguage models (MLLMs). While previous research has primarily focused on\nlocating evidence pages through precise multimodal queries, our work\ninvestigates a fundamental yet overlooked aspect: how input format influences\ndocument comprehension performance. Through systematic analysis, we discover\nthat raw OCR text often impairs rather than improves MLLMs' performance, which\nis a counterintuitive finding we attribute to attention dispersion and\nstructure loss. To further substantiate our hypothesis, we propose a novel\nstructure-preserving approach that encodes document elements using the LaTex\nparadigm, maintaining the hierarchical organization and spatial relationships\ncritical for comprehension. Our attention analysis reveals that structured text\ninduces structured attention patterns on both textual and visual content,\ndirecting models to focus on semantically meaningful regions while reducing\nattention waste. This approach significantly enhances MLLMs' document question\nanswering performance across diverse document types without requiring\narchitectural modifications or additional training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21600v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21600v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21612", "title": "AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning", "authors": ["Xiaobin Ren", "Xinyu Zhu", "Kaiqi Zhao"], "summary": "Currently, considerable strides have been achieved in Point-of-Interest (POI)\nembedding methodologies, driven by the emergence of novel POI tasks like\nrecommendation and classification. Despite the success of task-specific,\nend-to-end models in POI embedding, several challenges remain. These include\nthe need for more effective multi-context sampling strategies, insufficient\nexploration of multiple POI contexts, limited versatility, and inadequate\ngeneralization. To address these issues, we propose the AdaptGOT model, which\nintegrates both the (Adapt)ive representation learning technique and the\nGeographical-Co-Occurrence-Text (GOT) representation with a particular emphasis\non Geographical location, Co-Occurrence and Textual information. The AdaptGOT\nmodel comprises three key components: (1) contextual neighborhood generation,\nwhich integrates advanced mixed sampling techniques such as KNN, density-based,\nimportance-based, and category-aware strategies to capture complex contextual\nneighborhoods; (2) an advanced GOT representation enhanced by an attention\nmechanism, designed to derive high-quality, customized representations and\nefficiently capture complex interrelations between POIs; and (3) the MoE-based\nadaptive encoder-decoder architecture, which ensures topological consistency\nand enriches contextual representation by minimizing Jensen-Shannon divergence\nacross varying contexts. Experiments on two real-world datasets and multiple\nPOI tasks substantiate the superior performance of the proposed AdaptGOT model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21612v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21612v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22401", "title": "Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL", "authors": ["Tong Yang", "Bo Dai", "Lin Xiao", "Yuejie Chi"], "summary": "Online reinforcement learning (RL) with complex function approximations such\nas transformers and deep neural networks plays a significant role in the modern\npractice of artificial intelligence. Despite its popularity and importance,\nbalancing the fundamental trade-off between exploration and exploitation\nremains a long-standing challenge; in particular, we are still in lack of\nefficient and practical schemes that are backed by theoretical performance\nguarantees. Motivated by recent developments in exploration via optimistic\nregularization, this paper provides an interpretation of the principle of\noptimism through the lens of primal-dual optimization. From this fresh\nperspective, we set forth a new value-incentivized actor-critic (VAC) method,\nwhich optimizes a single easy-to-optimize objective integrating exploration and\nexploitation -- it promotes state-action and policy estimates that are both\nconsistent with collected data transitions and result in higher value\nfunctions. Theoretically, the proposed VAC method has near-optimal regret\nguarantees under linear Markov decision processes (MDPs) in both finite-horizon\nand infinite-horizon settings, which can be extended to the general function\napproximation setting under appropriate assumptions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22401v1", "categories": ["cs.LG", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22401v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22069", "title": "Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras", "authors": ["Petr Hruby", "Marc Pollefeys"], "summary": "We propose a novel approach for estimating the relative pose between rolling\nshutter cameras using the intersections of line projections with a single\nscanline per image. This allows pose estimation without explicitly modeling\ncamera motion. Alternatively, scanlines can be selected within a single image,\nenabling single-view relative pose estimation for scanlines of rolling shutter\ncameras. Our approach is designed as a foundational building block for rolling\nshutter structure-from-motion (SfM), where no motion model is required, and\neach scanline's pose can be computed independently. % We classify minimal\nsolvers for this problem in both generic and specialized settings, including\ncases with parallel lines and known gravity direction, assuming known\nintrinsics and no lens distortion. Furthermore, we develop minimal solvers for\nthe parallel-lines scenario, both with and without gravity priors, by\nleveraging connections between this problem and the estimation of 2D structure\nfrom 1D cameras. % Experiments on rolling shutter images from the Fastec\ndataset demonstrate the feasibility of our approach for initializing rolling\nshutter SfM, highlighting its potential for further development. % The code\nwill be made publicly available.", "comment": "ICCV 2025, 15 pages, 5 figures, 12 tables", "pdf_url": "http://arxiv.org/pdf/2506.22069v1", "categories": ["cs.CV", "68T45", "I.4.5"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22069v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21602", "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models", "authors": ["Xiaoyan Feng", "He Zhang", "Yanjun Zhang", "Leo Yu Zhang", "Shirui Pan"], "summary": "Recent advances in Large Language Models (LLMs) have raised urgent concerns\nabout LLM-generated text authenticity, prompting regulatory demands for\nreliable identification mechanisms. Although watermarking offers a promising\nsolution, existing approaches struggle to simultaneously achieve three critical\nrequirements: text quality preservation, model-agnostic detection, and message\nembedding capacity, which are crucial for practical implementation. To achieve\nthese goals, the key challenge lies in balancing the trade-off between text\nquality preservation and message embedding capacity. To address this challenge,\nwe propose BiMark, a novel watermarking framework that achieves these\nrequirements through three key innovations: (1) a bit-flip unbiased reweighting\nmechanism enabling model-agnostic detection, (2) a multilayer architecture\nenhancing detectability without compromising generation quality, and (3) an\ninformation encoding approach supporting multi-bit watermarking. Through\ntheoretical analysis and extensive experiments, we validate that, compared to\nstate-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%\nhigher extraction rates for short texts while maintaining text quality\nindicated by lower perplexity, and performs comparably to non-watermarked text\non downstream tasks such as summarization and translation.", "comment": "This paper is accepted by International Conference on Machine\n  Learning (ICML) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21602v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21602v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21613", "title": "ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech", "authors": ["Gautam Siddharth Kashyap", "Mohammad Anas Azeez", "Rafiq Ali", "Zohaib Hasan Siddiqui", "Jiechao Gao", "Usman Naseem"], "summary": "The increasing prevalence of child-targeted hate speech online underscores\nthe urgent need for specialized datasets to address this critical issue.\nExisting hate speech datasets lack agespecific annotations, fail to capture\nnuanced contexts, and overlook the unique emotional impact on children. To\nbridge this gap, we introduce ChildGuard1, a curated dataset derived from\nexisting corpora and enriched with child-specific annotations. ChildGuard\ncaptures diverse contexts of child-targeted hate speech, spanning age groups.\nWe benchmark existing state-of-the-art hate speech detection methods, including\nLarge Language Models (LLMs), and assess their effectiveness in detecting and\ncontextualizing child-targeted hate speech. To foster further research in this\narea, we publicly release ChildGuard, providing a robust foundation for\ndeveloping improved methods to detect and mitigate such harm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21613v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21613v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22423", "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks", "authors": ["Pritam Dash", "Ethan Chan", "Nathan P. Lawrence", "Karthik Pattabiraman"], "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22423v1", "categories": ["cs.LG", "cs.CR", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22423v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22075", "title": "Reasoning in machine vision: learning to think fast and slow", "authors": ["Shaheer U. Saeed", "Yipei Wang", "Veeru Kasivisvanathan", "Brian R. Davidson", "Matthew J. Clarkson", "Yipeng Hu", "Daniel C. Alexander"], "summary": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22075v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22075v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21604", "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "pdf_url": "http://arxiv.org/pdf/2506.21604v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21604v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21614", "title": "LastingBench: Defend Benchmarks Against Knowledge Leakage", "authors": ["Yixiong Fang", "Tianran Sun", "Yuling Shi", "Min Wang", "Xiaodong Gu"], "summary": "The increasing complexity of large language models (LLMs) raises concerns\nabout their ability to \"cheat\" on standard Question Answering (QA) benchmarks\nby memorizing task-specific data. This undermines the validity of benchmark\nevaluations, as they no longer reflect genuine model capabilities but instead\nthe effects of data leakage. While prior work has focused on detecting such\nleakage, little attention has been given to mitigating its impact and\npreserving the long-term utility of benchmarks. In this paper, we introduce\nLastingBench, a novel framework designed to continuously reinforce and\nsafeguard existing benchmarks against knowledge leakage. LastingBench\nidentifies leakage points in the context through perturbation, then rewrites\nthe leakage points to counterfactual ones-disrupting memorization while\npreserving the benchmark's original evaluative intent. Evaluations of\nstate-of-the-art QA benchmarks show significant performance gaps, highlighting\nthe efficacy of LastingBench in reducing memorization effects. LastingBench\noffers a practical and scalable solution to ensure benchmark robustness over\ntime, promoting fairer and more interpretable evaluations of LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21614v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21614v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22427", "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings", "authors": ["Randeep Bhatia", "Nikos Papadis", "Murali Kodialam", "TV Lakshman", "Sayak Chakrabarty"], "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.", "comment": "31 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22427v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22427v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22078", "title": "Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction", "authors": ["Pei-Kai Huanga", "Ya-Ting Chan", "Kuan-Wen Chen", "Yen-Chun Chou", "Shih-Yu Yang", "Chiou-Ting Hsu"], "summary": "Many remote Heart Rate (HR) measurement methods focus on estimating remote\nphotoplethysmography (rPPG) signals from video clips lasting around 10 seconds\nbut often overlook the need for HR estimation from ultra-short video clips. In\nthis paper, we aim to accurately measure HR from ultra-short 2-second video\nclips by specifically addressing two key challenges. First, to overcome the\nlimited number of heartbeat cycles in ultra-short video clips, we propose an\neffective periodicity-guided rPPG estimation method that enforces consistent\nperiodicity between rPPG signals estimated from ultra-short clips and their\nmuch longer ground truth signals. Next, to mitigate estimation inaccuracies due\nto spectral leakage, we propose including a generator to reconstruct longer\nrPPG signals from ultra-short ones while preserving their periodic consistency\nto enable more accurate HR measurement. Extensive experiments on four rPPG\nestimation benchmark datasets demonstrate that our proposed method not only\naccurately measures HR from ultra-short video clips but also outperform\nprevious rPPG estimation techniques to achieve state-of-the-art performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22078v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22078v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21605", "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents", "authors": ["Haoran Tan", "Zeyu Zhang", "Chen Ma", "Xu Chen", "Quanyu Dai", "Zhenhua Dong"], "summary": "Recent works have highlighted the significance of memory mechanisms in\nLLM-based agents, which enable them to store observed information and adapt to\ndynamic environments. However, evaluating their memory capabilities still\nremains challenges. Previous evaluations are commonly limited by the diversity\nof memory levels and interactive scenarios. They also lack comprehensive\nmetrics to reflect the memory capabilities from multiple aspects. To address\nthese problems, in this paper, we construct a more comprehensive dataset and\nbenchmark to evaluate the memory capability of LLM-based agents. Our dataset\nincorporates factual memory and reflective memory as different levels, and\nproposes participation and observation as various interactive scenarios. Based\non our dataset, we present a benchmark, named MemBench, to evaluate the memory\ncapability of LLM-based agents from multiple aspects, including their\neffectiveness, efficiency, and capacity. To benefit the research community, we\nrelease our dataset and project at https://github.com/import-myself/Membench.", "comment": "17 pages, 5 figures. Accepted by ACL 2025 findings", "pdf_url": "http://arxiv.org/pdf/2506.21605v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21605v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21615", "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "authors": ["Wenhao Li", "Hongkuan Zhang", "Hongwei Zhang", "Zhengxu Li", "Zengjie Dong", "Yafan Chen", "Niranjan Bidargaddi", "Hong Liu"], "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21615v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21615v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2109.05721", "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "pdf_url": "http://arxiv.org/pdf/2109.05721v2", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2109.05721v2", "date": "2021-09-13", "updated": "2022-12-19"}
{"id": "2506.22099", "title": "BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting", "authors": ["Zipei Ma", "Junzhe Jiang", "Yurui Chen", "Li Zhang"], "summary": "The realistic reconstruction of street scenes is critical for developing\nreal-world simulators in autonomous driving. Most existing methods rely on\nobject pose annotations, using these poses to reconstruct dynamic objects and\nmove them during the rendering process. This dependence on high-precision\nobject annotations limits large-scale and extensive scene reconstruction. To\naddress this challenge, we propose B\\'ezier curve Gaussian splatting\n(B\\'ezierGS), which represents the motion trajectories of dynamic objects using\nlearnable B\\'ezier curves. This approach fully leverages the temporal\ninformation of dynamic objects and, through learnable curve modeling,\nautomatically corrects pose errors. By introducing additional supervision on\ndynamic object rendering and inter-curve consistency constraints, we achieve\nreasonable and accurate separation and reconstruction of scene elements.\nExtensive experiments on the Waymo Open Dataset and the nuPlan benchmark\ndemonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both\ndynamic and static scene components reconstruction and novel view synthesis.", "comment": "Accepted at ICCV 2025, Project Page:\n  https://github.com/fudan-zvg/BezierGS", "pdf_url": "http://arxiv.org/pdf/2506.22099v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22099v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21606", "title": "Large Language Models as symbolic DNA of cultural dynamics", "authors": ["Parham Pourdavood", "Michael Jacob", "Terrence Deacon"], "summary": "This paper proposes a novel conceptualization of Large Language Models (LLMs)\nas externalized informational substrates that function analogously to DNA for\nhuman cultural dynamics. Rather than viewing LLMs as either autonomous\nintelligence or mere programmed mimicry, we argue they serve a broader role as\nrepositories that preserve compressed patterns of human symbolic\nexpression--\"fossils\" of meaningful dynamics that retain relational residues\nwithout their original living contexts. Crucially, these compressed patterns\nonly become meaningful through human reinterpretation, creating a recursive\nfeedback loop where they can be recombined and cycle back to ultimately\ncatalyze human creative processes. Through analysis of four universal\nfeatures--compression, decompression, externalization, and recursion--we\ndemonstrate that just as DNA emerged as a compressed and externalized medium\nfor preserving useful cellular dynamics without containing explicit reference\nto goal-directed physical processes, LLMs preserve useful regularities of human\nculture without containing understanding of embodied human experience.\nTherefore, we argue that LLMs' significance lies not in rivaling human\nintelligence, but in providing humanity a tool for self-reflection and playful\nhypothesis-generation in a low-stakes, simulated environment. This framework\npositions LLMs as tools for cultural evolvability, enabling humanity to\ngenerate novel hypotheses about itself while maintaining the human\ninterpretation necessary to ground these hypotheses in ongoing human aesthetics\nand norms.", "comment": "28 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.21606v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21606v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21616", "title": "TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization", "authors": ["Chuanrui Hu", "Wei Hu", "Penghang Yu", "Hua Zhang", "Bing-Kun Bao"], "summary": "Open-domain Timeline Summarization (TLS) is crucial for monitoring the\nevolution of news topics. To identify changes in news topics, existing methods\ntypically employ general Large Language Models (LLMs) to summarize relevant\ntimestamps from retrieved news. While general LLMs demonstrate capabilities in\nzero-shot news summarization and timestamp localization, they struggle with\nassessing topic relevance and understanding topic evolution. Consequently, the\nsummarized information often includes irrelevant details or inaccurate\ntimestamps. To address these issues, we propose the first large Timeline\nIntelligence Model (TIM) for open-domain TLS, which is capable of effectively\nsummarizing open-domain timelines. Specifically, we begin by presenting a\nlarge-scale TLS dataset, comprising over 1,000 news topics and more than 3,000\nannotated TLS instances. Furthermore, we propose a progressive optimization\nstrategy, which gradually enhance summarization performance. It employs\ninstruction tuning to enhance summarization and topic-irrelevant information\nfiltering capabilities. Following this, it exploits a novel dual-alignment\nreward learning method that incorporates both semantic and temporal\nperspectives, thereby improving the understanding of topic evolution\nprinciples. Through this progressive optimization strategy, TIM demonstrates a\nrobust ability to summarize open-domain timelines. Extensive experiments in\nopen-domain demonstrate the effectiveness of our TIM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21616v1", "categories": ["cs.CL", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21616v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2212.09525", "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "comment": "AAAI 2023", "pdf_url": "http://arxiv.org/pdf/2212.09525v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2212.09525v1", "date": "2022-12-19", "updated": "2022-12-19"}
{"id": "2506.22101", "title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "authors": ["Hyeongji Kim", "Stine Hansen", "Michael Kampffmeyer"], "summary": "Common prototype-based medical image few-shot segmentation (FSS) methods\nmodel foreground and background classes using class-specific prototypes.\nHowever, given the high variability of the background, a more promising\ndirection is to focus solely on foreground modeling, treating the background as\nan anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key\nlimitations: dependence on a single prototype per class, a focus on binary\nclassification, and fixed thresholds that fail to adapt to patient and organ\nvariability. To address these shortcomings, we propose the Tied Prototype Model\n(TPM), a principled reformulation of ADNet with tied prototype locations for\nforeground and background distributions. Building on its probabilistic\nfoundation, TPM naturally extends to multiple prototypes and multi-class\nsegmentation while effectively separating non-typical background features.\nNotably, both extensions lead to improved segmentation accuracy. Finally, we\nleverage naturally occurring class priors to define an ideal target for\nadaptive thresholds, boosting segmentation performance. Taken together, TPM\nprovides a fresh perspective on prototype-based FSS for medical image\nsegmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.", "comment": "Submitted version (MICCAI). Accepted at MICCAI 2025. The code repo\n  will be made publicly available soon", "pdf_url": "http://arxiv.org/pdf/2506.22101v1", "categories": ["cs.CV", "cs.LG", "stat.ML"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22101v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21607", "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21607v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21607v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21618", "title": "TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge", "authors": ["Zhiyuan Zhang", "Xiaosong Jia", "Guanyu Chen", "Qifeng Li", "Junchi Yan"], "summary": "In this technical report, we introduce TrajTok, a trajectory tokenizer for\ndiscrete next-token-prediction based behavior generation models, which combines\ndata-driven and rule-based methods with better coverage, symmetry and\nrobustness, along with a spatial-aware label smoothing method for cross-entropy\nloss. We adopt the tokenizer and loss for the SMART model and reach a superior\nperformance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge\n2025. We will open-source the code in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21618v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21618v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2412.15194", "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "authors": ["Qihao Zhao", "Yangyu Huang", "Tengchao Lv", "Lei Cui", "Qinzheng Sun", "Shaoguang Mao", "Xin Zhang", "Ying Xin", "Qiufeng Yin", "Scarlett Li", "Furu Wei"], "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.15194v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2412.15194v1", "date": "2024-12-19", "updated": "2024-12-19"}
{"id": "2506.22111", "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "authors": ["Ruthvik Bokkasam", "Shankar Gangisetty", "A. H. Abdul Hafez", "C. V. Jawahar"], "summary": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22111v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22111v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21608", "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2", "authors": ["Yasmine Bouamra", "Bruno Yun", "Alexandre Poisson", "Frédéric Armetta"], "summary": "The automatic generation of SysML v2 models represents a major challenge in\nthe engineering of complex systems, particularly due to the scarcity of\nlearning corpora and complex syntax. We present SysTemp, a system aimed at\nfacilitating and improving the creation of SysML v2 models from natural\nlanguage specifications. It is based on a multi-agent system, including a\ntemplate generator that structures the generation process. We discuss the\nadvantages and challenges of this system through an evaluation, highlighting\nits potential to improve the quality of the generations in SysML v2 modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21608v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21608v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21619", "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "summary": "Large-scale text-to-speech (TTS) models are typically categorized into\nautoregressive and non-autoregressive systems. Although autoregressive systems\nexhibit certain advantages in speech naturalness, their token-by-token\ngeneration mechanism makes it difficult to precisely control the duration of\nsynthesized speech. This is a key limitation in applications such as video\ndubbing that require strict audio-visual synchronization. This paper introduces\nIndexTTS2, which proposes a novel and autoregressive-model-friendly method for\nspeech duration control. The method supports two generation modes: one allows\nexplicit specification of the number of generated tokens for precise duration\ncontrol; the other does not require manual input and lets the model freely\ngenerate speech while preserving prosodic characteristics from the input\nprompt. Furthermore, IndexTTS2 achieves disentanglement between emotional\nexpression and speaker identity, enabling independent control of timbre and\nemotion. In the zero-shot setting, the model can perfectly reproduce the\nemotional characteristics of the input prompt. Users may also provide a\nseparate emotion prompt, even from a different speaker, allowing the model to\nreconstruct the target timbre while conveying the desired emotion. To enhance\nclarity during strong emotional expressions, we incorporate GPT latent\nrepresentations to improve speech stability. Meanwhile, to lower the barrier\nfor emotion control, we design a soft instruction mechanism based on textual\ndescriptions by fine-tuning Qwen3. This enables effective guidance of speech\ngeneration with desired emotional tendencies using natural language input.\nExperimental results demonstrate that IndexTTS2 outperforms existing\nstate-of-the-art zero-shot TTS models in word error rate, speaker similarity,\nand emotional fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21619v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21619v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21545v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21545v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22118", "title": "Pipe Reconstruction from Point Cloud Data", "authors": ["Antje Alex", "Jannis Stoppe"], "summary": "Accurate digital twins of industrial assets, such as ships and offshore\nplatforms, rely on the precise reconstruction of complex pipe networks.\nHowever, manual modelling of pipes from laser scan data is a time-consuming and\nlabor-intensive process. This paper presents a pipeline for automated pipe\nreconstruction from incomplete laser scan data. The approach estimates a\nskeleton curve using Laplacian-based contraction, followed by curve elongation.\nThe skeleton axis is then recentred using a rolling sphere technique combined\nwith 2D circle fitting, and refined with a 3D smoothing step. This enables the\ndetermination of pipe properties, including radius, length and orientation, and\nfacilitates the creation of detailed 3D models of complex pipe networks. By\nautomating pipe reconstruction, this approach supports the development of\ndigital twins, allowing for rapid and accurate modeling while reducing costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22118v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22118v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21609", "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models", "authors": ["Junhao Liu", "Zhenhao Xu", "Yuxin Fang", "Yichuan Chen", "Zuobin Ying", "Wenhan Chang"], "summary": "Recently, there have been notable advancements in large language models\n(LLMs), demonstrating their growing abilities in complex reasoning. However,\nexisting research largely overlooks a thorough and systematic comparison of\nthese models' reasoning processes and outputs, particularly regarding their\nself-reflection pattern (also termed \"Aha moment\") and the interconnections\nacross diverse domains. This paper proposes a novel framework for analyzing the\nreasoning characteristics of four cutting-edge large reasoning models (GPT-o1,\nDeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge\nparadigm. Our approach connects their internal thinking processes with their\nfinal outputs. A diverse dataset consists of real-world scenario-based\nquestions covering logical deduction, causal inference, and multi-step\nproblem-solving. Additionally, a set of metrics is put forward to assess both\nthe coherence of reasoning and the accuracy of the outputs. The research\nresults uncover various patterns of how these models balance exploration and\nexploitation, deal with problems, and reach conclusions during the reasoning\nprocess. Through quantitative and qualitative comparisons, disparities among\nthese models are identified in aspects such as the depth of reasoning, the\nreliance on intermediate steps, and the degree of similarity between their\nthinking processes and output patterns and those of GPT-o1. This work offers\nvaluable insights into the trade-off between computational efficiency and\nreasoning robustness and provides practical recommendations for enhancing model\ndesign and evaluation in practical applications. We publicly release our\nproject at: https://github.com/ChangWenhan/FromThinking2Output", "comment": "18 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21609v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21609v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21620", "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit", "authors": ["Daniele Cirulli", "Giulio Cimini", "Giovanni Palermo"], "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for\nnatural language generation, with applications spanning from content creation\nto social simulations. Their ability to mimic human interactions raises both\nopportunities and concerns, particularly in the context of politically relevant\nonline discussions. In this study, we evaluate the performance of LLMs in\nreplicating user-generated content within a real-world, divisive scenario:\nReddit conversations during the 2016 US Presidential election. In particular,\nwe conduct three different experiments, asking GPT-4 to generate comments by\nimpersonating either real or artificial partisan users. We analyze the\ngenerated comments in terms of political alignment, sentiment, and linguistic\nfeatures, comparing them against real user contributions and benchmarking\nagainst a null model. We find that GPT-4 is able to produce realistic comments,\nboth in favor of or against the candidate supported by the community, yet\ntending to create consensus more easily than dissent. In addition we show that\nreal and artificial comments are well separated in a semantically embedded\nspace, although they are indistinguishable by manual inspection. Our findings\nprovide insights on the potential use of LLMs to sneak into online discussions,\ninfluence political debate and shape political narratives, bearing broader\nimplications of AI-driven discourse manipulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21620v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21620v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21558", "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter Mühlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21558v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21558v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22134", "title": "Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "summary": "Higher-order tensors are well-suited for representing multi-dimensional data,\nsuch as color images and videos. Low-rank tensor representation has become\nessential in machine learning and computer vision, but existing methods like\nTucker decomposition offer flexibility at the expense of interpretability. In\ncontrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more\nnatural and interpretable tensor structure, obtaining sparse solutions remains\nchallenging. Leveraging the rich properties of CP decomposition, we propose a\nCP-based low-rank tensor function parameterized by neural networks for implicit\nneural representation (CP-INR). This approach enables continuous data\nrepresentation beyond structured grids, fully exploiting the non-linearity of\ntensor data with theoretical guarantees on excess risk bounds. To achieve a\nsparse CP decomposition, we introduce a variational form of the Schatten-p\nquasi-norm and prove its relationship to multilinear rank minimization. For\nsmoothness, we propose a regularization term based on the spectral norm of the\nJacobian and Hutchinson's trace estimator. Our proposed smoothness\nregularization is SVD-free and avoids explicit chain rule derivations. It can\nserve as an alternative to Total Variation (TV) regularization in image\ndenoising tasks and is naturally applicable to continuous data. Extensive\nexperiments on multi-dimensional data recovery tasks, including image\ninpainting, denoising, and point cloud upsampling, demonstrate the superiority\nand versatility of our method compared to state-of-the-art approaches.", "comment": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "pdf_url": "http://arxiv.org/pdf/2506.22134v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22134v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21611", "title": "Does Multimodality Lead to Better Time Series Forecasting?", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21611v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21611v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.21621", "title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs", "authors": ["Jasper Dekoninck", "Ivo Petrov", "Kristian Minchev", "Mislav Balunovic", "Martin Vechev", "Miroslav Marinov", "Maria Drencheva", "Lyuba Konova", "Milen Shumanov", "Kaloyan Tsvetkov", "Nikolay Drenchev", "Lazar Todorov", "Kalina Nikolova", "Nikolay Georgiev", "Vanesa Kalinkova", "Margulan Ismoldayev"], "summary": "In recent months, large language models (LLMs) have made significant progress\nin mathematical proof generation, but further advancement is hindered by the\nlack of a large-scale, high-quality dataset of human-evaluated proofs. While\nexpensive to create, such a dataset is essential for driving improvements in\ntraining and enabling a rigorous analysis of proof generation capabilities. In\nthis work, we present the Open Proof Corpus (OPC), a dataset comprising over\n5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was\nspecifically designed for broad applicability and downstream usage in proof\ngeneration research and is the first to include a substantial number of\ncorrect, LLM-generated solutions to problems from prestigious mathematics\ncompetitions such as the USAMO and IMO. Using the OPC, we explore critical\nquestions in automated proof generation: (1) the performance gap between\nnatural language and formal proof generation, (2) the discrepancy between\nfinal-answer accuracy and full-proof validity, and (3) the impact of best-of-n\nselection on proof quality. Finally, to showcase the utility of the OPC, we\nfinetune an 8B-parameter model on the dataset, obtaining a model that performs\non par with the best model, Gemini-2.5-Pro, on the task of evaluating proof\ncorrectness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21621v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21621v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21565", "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "authors": ["Takato Ueno", "Keito Inoshita"], "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21565v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21565v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22139", "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs", "authors": ["Shaojie Zhang", "Jiahui Yang", "Jianqin Yin", "Zhenbo Luo", "Jian Luan"], "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22139v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22139v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21612", "title": "AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning", "authors": ["Xiaobin Ren", "Xinyu Zhu", "Kaiqi Zhao"], "summary": "Currently, considerable strides have been achieved in Point-of-Interest (POI)\nembedding methodologies, driven by the emergence of novel POI tasks like\nrecommendation and classification. Despite the success of task-specific,\nend-to-end models in POI embedding, several challenges remain. These include\nthe need for more effective multi-context sampling strategies, insufficient\nexploration of multiple POI contexts, limited versatility, and inadequate\ngeneralization. To address these issues, we propose the AdaptGOT model, which\nintegrates both the (Adapt)ive representation learning technique and the\nGeographical-Co-Occurrence-Text (GOT) representation with a particular emphasis\non Geographical location, Co-Occurrence and Textual information. The AdaptGOT\nmodel comprises three key components: (1) contextual neighborhood generation,\nwhich integrates advanced mixed sampling techniques such as KNN, density-based,\nimportance-based, and category-aware strategies to capture complex contextual\nneighborhoods; (2) an advanced GOT representation enhanced by an attention\nmechanism, designed to derive high-quality, customized representations and\nefficiently capture complex interrelations between POIs; and (3) the MoE-based\nadaptive encoder-decoder architecture, which ensures topological consistency\nand enriches contextual representation by minimizing Jensen-Shannon divergence\nacross varying contexts. Experiments on two real-world datasets and multiple\nPOI tasks substantiate the superior performance of the proposed AdaptGOT model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21612v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21612v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.21622", "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "summary": "Speech impairments caused by conditions such as cerebral palsy or genetic\ndisorders pose significant challenges for automatic speech recognition (ASR)\nsystems. Despite recent advances, ASR models like Whisper struggle with\nnon-normative speech due to limited training data and the difficulty of\ncollecting and annotating non-normative speech samples. In this work, we\npropose a practical and lightweight pipeline to personalize ASR models,\nformalizing the selection of words and enriching a small, speech-impaired\ndataset with semantic coherence. Applied to data from a child with a structural\nspeech impairment, our approach shows promising improvements in transcription\nquality, demonstrating the potential to reduce communication barriers for\nindividuals with atypical speech patterns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21622v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21622v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21566", "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "authors": ["Arwa Arif"], "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "comment": "Preprint, 8 Pages", "pdf_url": "http://arxiv.org/pdf/2506.21566v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21566v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22146", "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "authors": ["Amirmohammad Izadi", "Mohammad Ali Banayeeanzade", "Fatemeh Askari", "Ali Rahimiakbar", "Mohammad Mahdi Vahedi", "Hosein Hasani", "Mahdieh Soleymani Baghshah"], "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22146v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22146v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21614", "title": "LastingBench: Defend Benchmarks Against Knowledge Leakage", "authors": ["Yixiong Fang", "Tianran Sun", "Yuling Shi", "Min Wang", "Xiaodong Gu"], "summary": "The increasing complexity of large language models (LLMs) raises concerns\nabout their ability to \"cheat\" on standard Question Answering (QA) benchmarks\nby memorizing task-specific data. This undermines the validity of benchmark\nevaluations, as they no longer reflect genuine model capabilities but instead\nthe effects of data leakage. While prior work has focused on detecting such\nleakage, little attention has been given to mitigating its impact and\npreserving the long-term utility of benchmarks. In this paper, we introduce\nLastingBench, a novel framework designed to continuously reinforce and\nsafeguard existing benchmarks against knowledge leakage. LastingBench\nidentifies leakage points in the context through perturbation, then rewrites\nthe leakage points to counterfactual ones-disrupting memorization while\npreserving the benchmark's original evaluative intent. Evaluations of\nstate-of-the-art QA benchmarks show significant performance gaps, highlighting\nthe efficacy of LastingBench in reducing memorization effects. LastingBench\noffers a practical and scalable solution to ensure benchmark robustness over\ntime, promoting fairer and more interpretable evaluations of LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21614v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21614v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.21623", "title": "Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints", "authors": ["Peiheng Gao", "Chen Yang", "Ning Sun", "Ričardas Zitikis"], "summary": "Machine learning (ML) has significantly advanced text classification by\nenabling automated understanding and categorization of complex, unstructured\ntextual data. However, accurately capturing nuanced linguistic patterns and\ncontextual variations inherent in natural language, particularly within\nconsumer complaints, remains a challenge. This study addresses these issues by\nincorporating human-experience-trained algorithms that effectively recognize\nsubtle semantic differences crucial for assessing consumer relief eligibility.\nFurthermore, we propose integrating synthetic data generation methods that\nutilize expert evaluations of generative adversarial networks and are refined\nthrough expert annotations. By combining expert-trained classifiers with\nhigh-quality synthetic data, our research seeks to significantly enhance\nmachine learning classifier performance, reduce dataset acquisition costs, and\nimprove overall evaluation metrics and robustness in text classification tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21623v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21623v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21567", "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21567v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21567v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22149", "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models", "authors": ["Ronald Fecso", "José Morano", "Ursula Schmidt-Erfurth", "Hrvoje Bogunović"], "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.", "comment": "Accepted for presentation at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22149v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22149v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21615", "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "authors": ["Wenhao Li", "Hongkuan Zhang", "Hongwei Zhang", "Zhengxu Li", "Zengjie Dong", "Yafan Chen", "Niranjan Bidargaddi", "Hong Liu"], "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21615v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21615v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.21625", "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents", "authors": ["Jiaxi Zhuang", "Kangning Li", "Jue Hou", "Mingjun Xu", "Zhifeng Gao", "Hengxing Cai"], "summary": "Extracting molecular structure-activity relationships (SARs) from scientific\nliterature and patents is essential for drug discovery and materials research.\nHowever, this task remains challenging due to heterogeneous document formats\nand limitations of existing methods. Specifically, rule-based approaches\nrelying on rigid templates fail to generalize across diverse document layouts,\nwhile general-purpose multimodal large language models (MLLMs) lack sufficient\naccuracy and reliability for specialized tasks, such as layout detection and\noptical chemical structure recognition (OCSR). To address these challenges, we\nintroduce DocSAR-200, a rigorously annotated benchmark of 200 scientific\ndocuments designed specifically for evaluating SAR extraction methods.\nAdditionally, we propose Doc2SAR, a novel synergistic framework that integrates\ndomain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).\nExtensive experiments demonstrate that Doc2SAR achieves state-of-the-art\nperformance across various document types, significantly outperforming leading\nend-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of\n80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR\ndemonstrates practical usability through efficient inference and is accompanied\nby a web app.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21625v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21625v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21570", "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21570v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21570v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22161", "title": "Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection", "authors": ["Taijin Zhao", "Heqian Qiu", "Yu Dai", "Lanxiao Wang", "Fanman Meng", "Qingbo Wu", "Hongliang Li"], "summary": "Few-shot object detection (FSOD) aims to detect objects with limited samples\nfor novel classes, while relying on abundant data for base classes. Existing\nFSOD approaches, predominantly built on the Faster R-CNN detector, entangle\nobjectness recognition and foreground classification within shared feature\nspaces. This paradigm inherently establishes class-specific objectness criteria\nand suffers from unrepresentative novel class samples. To resolve this\nlimitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization\nframework. First, UOFS decouples the feature space into two orthogonal\ncomponents, where magnitude encodes objectness and angle encodes\nclassification. This decoupling enables transferring class-agnostic objectness\nknowledge from base classes to novel classes. Moreover, implementing the\ndisentanglement requires careful attention to two challenges: (1) Base set\nimages contain unlabeled foreground instances, causing confusion between\npotential novel class instances and backgrounds. (2) Angular optimization\ndepends exclusively on base class foreground instances, inducing overfitting of\nangular distributions to base classes. To address these challenges, we propose\na Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure\nbackground base set by removing unlabeled instances in original images to\nprovide unbiased magnitude-based objectness supervision. (2) Incorporating\nunlabeled foreground instances in the original base set into angular\noptimization to enhance distribution uniformity. Additionally, we propose a\nSpatial-wise Attention Disentanglement and Association (SADA) module to address\ntask conflicts between class-agnostic and class-specific tasks. Experiments\ndemonstrate that our method significantly outperforms existing approaches based\non entangled feature spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22161v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22161v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21617", "title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems", "authors": ["Hiba Bederina", "Jill-Jênn Vie"], "summary": "The challenge of balancing user relevance and content diversity in\nrecommender systems is increasingly critical amid growing concerns about\ncontent homogeneity and reduced user engagement. In this work, we propose a\nnovel framework that leverages a multi-objective, contextual sequential\nsampling strategy. Item selection is guided by Bayesian updates that\ndynamically adjust scores to optimize diversity. The reward formulation\nintegrates multiple diversity metrics-including the log-determinant volume of a\ntuned similarity submatrix and ridge leverage scores-along with a diversity\ngain uncertainty term to address the exploration-exploitation trade-off. Both\nintra- and inter-batch diversity are modeled to promote serendipity and\nminimize redundancy. A dominance-based ranking procedure identifies\nPareto-optimal item sets, enabling adaptive and balanced selections at each\niteration. Experiments on a real-world dataset show that our approach\nsignificantly improves diversity without sacrificing relevance, demonstrating\nits potential to enhance user experience in large-scale recommendation\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21617v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21617v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.21682", "title": "Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations", "authors": ["Li Zhou", "Hao Jiang", "Junjie Li", "Zefeng Zhao", "Feng Jiang", "Wenyu Chen", "Haizhou Li"], "summary": "Explicit structural information has been proven to be encoded by Graph Neural\nNetworks (GNNs), serving as auxiliary knowledge to enhance model capabilities\nand improve performance in downstream NLP tasks. However, recent studies\nindicate that GNNs fail to fully utilize structural information, whereas\nMulti-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms\ninherent to GNNs, exhibit a surprising ability in structure-aware tasks.\nMotivated by these findings, this paper introduces a comprehensive probing\nframework from an information-theoretic perspective. The framework is designed\nto systematically assess the role of explicit structural modeling in enhancing\nlanguage model (LM) representations and to investigate the potential of MLPs as\nefficient and scalable alternatives to GNNs. We extend traditional probing\nclassifiers by incorporating a control module that allows for selective use of\neither the full GNN model or its decoupled components, specifically, the\nmessage-passing and feature-transformation operations.This modular approach\nisolates and assesses the individual contributions of these operations,\navoiding confounding effects from the complete GNN architecture. Using the Edge\nProbing Suite, a diagnostic tool for evaluating the linguistic knowledge\nencoded in LMs, we find that MLPs, when used as feature-transformation modules,\nconsistently improve the linguistic knowledge captured in LM representations\nacross different architectures. They effectively encode both syntactic and\nsemantic patterns. Similarly, GNNs that incorporate feature-transformation\noperations show beneficial effects. In contrast, models that rely solely on\nmessage-passing operations tend to underperform, often leading to negative\nimpacts on probing task performance.", "comment": "Graph Neural Networks, Multi-Layer Perceptrons, Explicit Structural\n  Modeling, Probing Classifier", "pdf_url": "http://arxiv.org/pdf/2506.21682v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21682v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21573", "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21573v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21573v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2506.22179", "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition", "authors": ["Wenhan Wu", "Zhishuai Guo", "Chen Chen", "Hongfei Xue", "Aidong Lu"], "summary": "Zero-shot skeleton-based action recognition aims to develop models capable of\nidentifying actions beyond the categories encountered during training. Previous\napproaches have primarily focused on aligning visual and semantic\nrepresentations but often overlooked the importance of fine-grained action\npatterns in the semantic space (e.g., the hand movements in drinking water and\nbrushing teeth). To address these limitations, we propose a Frequency-Semantic\nEnhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic\nrepresentation learning with frequency decomposition. FS-VAE consists of three\nkey components: 1) a frequency-based enhancement module with high- and\nlow-frequency adjustments to enrich the skeletal semantics learning and improve\nthe robustness of zero-shot action recognition; 2) a semantic-based action\ndescription with multilevel alignment to capture both local details and global\ncorrespondence, effectively bridging the semantic gap and compensating for the\ninherent loss of information in skeleton sequences; 3) a calibrated\ncross-alignment loss that enables valid skeleton-text pairs to counterbalance\nambiguous ones, mitigating discrepancies and ambiguities in skeleton and text\nfeatures, thereby ensuring robust alignment. Evaluations on the benchmarks\ndemonstrate the effectiveness of our approach, validating that\nfrequency-enhanced semantic features enable robust differentiation of visually\nand semantically similar action clusters, improving zero-shot action\nrecognition.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22179v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22179v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21618", "title": "TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge", "authors": ["Zhiyuan Zhang", "Xiaosong Jia", "Guanyu Chen", "Qifeng Li", "Junchi Yan"], "summary": "In this technical report, we introduce TrajTok, a trajectory tokenizer for\ndiscrete next-token-prediction based behavior generation models, which combines\ndata-driven and rule-based methods with better coverage, symmetry and\nrobustness, along with a spatial-aware label smoothing method for cross-entropy\nloss. We adopt the tokenizer and loss for the SMART model and reach a superior\nperformance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge\n2025. We will open-source the code in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21618v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21618v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21686", "title": "ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages", "authors": ["Swastika Kundu", "Autoshi Ibrahim", "Mithila Rahman", "Tanvir Ahmed"], "summary": "Sentiment analysis for regional dialects of Bangla remains an underexplored\narea due to linguistic diversity and limited annotated data. This paper\nintroduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences\nmanually translated from standard Bangla into four major regional dialects\nMymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly\nfeatures political and religious content, reflecting the contemporary socio\npolitical landscape of Bangladesh, alongside neutral texts to maintain balance.\nEach sentence is annotated using a dual annotation scheme: multiclass thematic\nlabeling categorizes sentences as Political, Religious, or Neutral, and\nmultilabel emotion annotation assigns one or more emotions from Anger,\nContempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native\ntranslators conducted the translation and annotation, with quality assurance\nperformed via Cohens Kappa inter annotator agreement, achieving strong\nconsistency across dialects. The dataset was further refined through systematic\nchecks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a\ncritical gap in resources for sentiment analysis in low resource Bangla\ndialects, enabling more accurate and context aware natural language processing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21686v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21686v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21581", "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21581v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21581v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.22191", "title": "Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints", "authors": ["Yuxin Cui", "Rui Song", "Yibin Li", "Max Q. -H. Meng", "Zhe Min"], "summary": "Robust and accurate 2D/3D registration, which aligns preoperative models with\nintraoperative images of the same anatomy, is crucial for successful\ninterventional navigation. To mitigate the challenge of a limited field of view\nin single-image intraoperative scenarios, multi-view 2D/3D registration is\nrequired by leveraging multiple intraoperative images. In this paper, we\npropose a novel multi-view 2D/3D rigid registration approach comprising two\nstages. In the first stage, a combined loss function is designed, incorporating\nboth the differences between predicted and ground-truth poses and the\ndissimilarities (e.g., normalized cross-correlation) between simulated and\nobserved intraoperative images. More importantly, additional cross-view\ntraining loss terms are introduced for both pose and image losses to explicitly\nenforce cross-view constraints. In the second stage, test-time optimization is\nperformed to refine the estimated poses from the coarse stage. Our method\nexploits the mutual constraints of multi-view projection poses to enhance the\nrobustness of the registration process. The proposed framework achieves a mean\ntarget registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from\nthe DeepFluoro dataset, demonstrating superior performance compared to\nstate-of-the-art registration algorithms.", "comment": "ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.22191v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22191v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21619", "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "summary": "Large-scale text-to-speech (TTS) models are typically categorized into\nautoregressive and non-autoregressive systems. Although autoregressive systems\nexhibit certain advantages in speech naturalness, their token-by-token\ngeneration mechanism makes it difficult to precisely control the duration of\nsynthesized speech. This is a key limitation in applications such as video\ndubbing that require strict audio-visual synchronization. This paper introduces\nIndexTTS2, which proposes a novel and autoregressive-model-friendly method for\nspeech duration control. The method supports two generation modes: one allows\nexplicit specification of the number of generated tokens for precise duration\ncontrol; the other does not require manual input and lets the model freely\ngenerate speech while preserving prosodic characteristics from the input\nprompt. Furthermore, IndexTTS2 achieves disentanglement between emotional\nexpression and speaker identity, enabling independent control of timbre and\nemotion. In the zero-shot setting, the model can perfectly reproduce the\nemotional characteristics of the input prompt. Users may also provide a\nseparate emotion prompt, even from a different speaker, allowing the model to\nreconstruct the target timbre while conveying the desired emotion. To enhance\nclarity during strong emotional expressions, we incorporate GPT latent\nrepresentations to improve speech stability. Meanwhile, to lower the barrier\nfor emotion control, we design a soft instruction mechanism based on textual\ndescriptions by fine-tuning Qwen3. This enables effective guidance of speech\ngeneration with desired emotional tendencies using natural language input.\nExperimental results demonstrate that IndexTTS2 outperforms existing\nstate-of-the-art zero-shot TTS models in word error rate, speaker similarity,\nand emotional fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21619v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21619v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21712", "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers", "authors": ["Tzu-Quan Lin", "Hsi-Chun Cheng", "Hung-yi Lee", "Hao Tang"], "summary": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21712v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21712v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21585", "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "pdf_url": "http://arxiv.org/pdf/2506.21585v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21585v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22216", "title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "summary": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement.", "comment": "6 pages, 8 figures, accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2506.22216v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22216v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21620", "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit", "authors": ["Daniele Cirulli", "Giulio Cimini", "Giovanni Palermo"], "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for\nnatural language generation, with applications spanning from content creation\nto social simulations. Their ability to mimic human interactions raises both\nopportunities and concerns, particularly in the context of politically relevant\nonline discussions. In this study, we evaluate the performance of LLMs in\nreplicating user-generated content within a real-world, divisive scenario:\nReddit conversations during the 2016 US Presidential election. In particular,\nwe conduct three different experiments, asking GPT-4 to generate comments by\nimpersonating either real or artificial partisan users. We analyze the\ngenerated comments in terms of political alignment, sentiment, and linguistic\nfeatures, comparing them against real user contributions and benchmarking\nagainst a null model. We find that GPT-4 is able to produce realistic comments,\nboth in favor of or against the candidate supported by the community, yet\ntending to create consensus more easily than dissent. In addition we show that\nreal and artificial comments are well separated in a semantically embedded\nspace, although they are indistinguishable by manual inspection. Our findings\nprovide insights on the potential use of LLMs to sneak into online discussions,\ninfluence political debate and shape political narratives, bearing broader\nimplications of AI-driven discourse manipulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21620v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21620v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21745", "title": "(Fact) Check Your Bias", "authors": ["Eivind Morris Bakke", "Nora Winger Heggelund"], "summary": "Automatic fact verification systems increasingly rely on large language\nmodels (LLMs). We investigate how parametric knowledge biases in these models\naffect fact-checking outcomes of the HerO system (baseline for FEVER-25). We\nexamine how the system is affected by: (1) potential bias in Llama 3.1's\nparametric knowledge and (2) intentionally injected bias. When prompted\ndirectly to perform fact-verification, Llama 3.1 labels nearly half the claims\nas \"Not Enough Evidence\". Using only its parametric knowledge it is able to\nreach a verdict on the remaining half of the claims. In the second experiment,\nwe prompt the model to generate supporting, refuting, or neutral fact-checking\ndocuments. These prompts significantly influence retrieval outcomes, with\napproximately 50\\% of retrieved evidence being unique to each perspective.\nNotably, the model sometimes refuses to generate supporting documents for\nclaims it believes to be false, creating an inherent negative bias. Despite\ndifferences in retrieved evidence, final verdict predictions show stability\nacross prompting strategies. The code is available at:\nhttps://github.com/eibakke/FEVER-8-Shared-Task", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21745v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21745v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21590", "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "authors": ["Junqi Jiang", "Tom Bewley", "Salim I. Amoukou", "Francesco Leofante", "Antonio Rago", "Saumitra Mishra", "Francesca Toni"], "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21590v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21590v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22241", "title": "Boosting Classification with Quantum-Inspired Augmentations", "authors": ["Matthias Tschöpe", "Vitor Fortes Rey", "Sogo Pierre Sanon", "Paul Lukowicz", "Nikolaos Palaiodimopoulos", "Maximilian Kiefer-Emmanouilidis"], "summary": "Understanding the impact of small quantum gate perturbations, which are\ncommon in quantum digital devices but absent in classical computers, is crucial\nfor identifying potential advantages in quantum machine learning. While these\nperturbations are typically seen as detrimental to quantum computation, they\ncan actually enhance performance by serving as a natural source of data\naugmentation. Additionally, they can often be efficiently simulated on\nclassical hardware, enabling quantum-inspired approaches to improve classical\nmachine learning methods. In this paper, we investigate random Bloch sphere\nrotations, which are fundamental SU(2) transformations, as a simple yet\neffective quantum-inspired data augmentation technique. Unlike conventional\naugmentations such as flipping, rotating, or cropping, quantum transformations\nlack intuitive spatial interpretations, making their application to tasks like\nimage classification less straightforward. While common quantum augmentation\nmethods rely on applying quantum models or trainable quanvolutional layers to\nclassical datasets, we focus on the direct application of small-angle Bloch\nrotations and their effect on classical data. Using the large-scale ImageNet\ndataset, we demonstrate that our quantum-inspired augmentation method improves\nimage classification performance, increasing Top-1 accuracy by 3%, Top-5\naccuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard\nclassical augmentation methods. Finally, we examine the use of stronger unitary\naugmentations. Although these transformations preserve information in\nprinciple, they result in visually unrecognizable images with potential\napplications for privacy computations. However, we show that our augmentation\napproach and simple SU(2) transformations do not enhance differential privacy\nand discuss the implications of this limitation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22241v1", "categories": ["cs.CV", "cond-mat.dis-nn", "cs.LG", "quant-ph"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22241v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21621", "title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs", "authors": ["Jasper Dekoninck", "Ivo Petrov", "Kristian Minchev", "Mislav Balunovic", "Martin Vechev", "Miroslav Marinov", "Maria Drencheva", "Lyuba Konova", "Milen Shumanov", "Kaloyan Tsvetkov", "Nikolay Drenchev", "Lazar Todorov", "Kalina Nikolova", "Nikolay Georgiev", "Vanesa Kalinkova", "Margulan Ismoldayev"], "summary": "In recent months, large language models (LLMs) have made significant progress\nin mathematical proof generation, but further advancement is hindered by the\nlack of a large-scale, high-quality dataset of human-evaluated proofs. While\nexpensive to create, such a dataset is essential for driving improvements in\ntraining and enabling a rigorous analysis of proof generation capabilities. In\nthis work, we present the Open Proof Corpus (OPC), a dataset comprising over\n5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was\nspecifically designed for broad applicability and downstream usage in proof\ngeneration research and is the first to include a substantial number of\ncorrect, LLM-generated solutions to problems from prestigious mathematics\ncompetitions such as the USAMO and IMO. Using the OPC, we explore critical\nquestions in automated proof generation: (1) the performance gap between\nnatural language and formal proof generation, (2) the discrepancy between\nfinal-answer accuracy and full-proof validity, and (3) the impact of best-of-n\nselection on proof quality. Finally, to showcase the utility of the OPC, we\nfinetune an 8B-parameter model on the dataset, obtaining a model that performs\non par with the best model, Gemini-2.5-Pro, on the task of evaluating proof\ncorrectness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21621v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21621v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21783", "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models", "authors": ["Alexandru Dumitru", "V Venktesh", "Adam Jatowt", "Avishek Anand"], "summary": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.", "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages", "pdf_url": "http://arxiv.org/pdf/2506.21783v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21783v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21599", "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barthélemy", "Tomasz Bednarz", "Flora D. Salim"], "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21599v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21599v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22242", "title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration", "authors": ["Jiahui Zhang", "Yurui Chen", "Yueming Xu", "Ze Huang", "Yanpeng Zhou", "Yu-Jie Yuan", "Xinyue Cai", "Guowei Huang", "Xingyue Quan", "Hang Xu", "Li Zhang"], "summary": "Leveraging diverse robotic data for pretraining remains a critical challenge.\nExisting methods typically model the dataset's action distribution using simple\nobservations as inputs. However, these inputs are often incomplete, resulting\nin a dispersed conditional action distribution-an issue we refer to as\ncoordinate system chaos and state chaos. This inconsistency significantly\nhampers pretraining efficiency. To address this, we propose 4D-VLA, a novel\napproach that effectively integrates 4D information into the input to mitigate\nthese sources of chaos. Our model introduces depth and temporal information\ninto visual features with sequential RGB-D inputs, aligning the coordinate\nsystems of the robot and the scene. This alignment endows the model with strong\nspatiotemporal reasoning capabilities while minimizing training overhead.\nAdditionally, we introduce memory bank sampling, a frame sampling strategy\ndesigned to extract informative frames from historical images, further\nimproving effectiveness and efficiency. Experimental results demonstrate that\nour pretraining method and architectural components substantially enhance model\nperformance. In both simulated and real-world experiments, our model achieves a\nsignificant increase in success rate over OpenVLA. To further assess spatial\nperception and generalization to novel views, we introduce MV-Bench, a\nmulti-view simulation benchmark. Our model consistently outperforms existing\nmethods, demonstrating stronger spatial understanding and adaptability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22242v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22242v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21622", "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "summary": "Speech impairments caused by conditions such as cerebral palsy or genetic\ndisorders pose significant challenges for automatic speech recognition (ASR)\nsystems. Despite recent advances, ASR models like Whisper struggle with\nnon-normative speech due to limited training data and the difficulty of\ncollecting and annotating non-normative speech samples. In this work, we\npropose a practical and lightweight pipeline to personalize ASR models,\nformalizing the selection of words and enriching a small, speech-impaired\ndataset with semantic coherence. Applied to data from a child with a structural\nspeech impairment, our approach shows promising improvements in transcription\nquality, demonstrating the potential to reduce communication barriers for\nindividuals with atypical speech patterns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21622v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21622v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.21795", "title": "Offensive Language Detection on Social Media Using XLNet", "authors": ["Reem Alothman", "Hafida Benhidour", "Said Kerrache"], "summary": "The widespread use of text-based communication on social media-through chats,\ncomments, and microblogs-has improved user interaction but has also led to an\nincrease in offensive content, including hate speech, racism, and other forms\nof abuse. Due to the enormous volume of user-generated content, manual\nmoderation is impractical, which creates a need for automated systems that can\ndetect offensive language. Deep learning models, particularly those using\ntransfer learning, have demonstrated significant success in understanding\nnatural language through large-scale pretraining. In this study, we propose an\nautomatic offensive language detection model based on XLNet, a generalized\nautoregressive pretraining method, and compare its performance with BERT\n(Bidirectional Encoder Representations from Transformers), which is a widely\nused baseline in natural language processing (NLP). Both models are evaluated\nusing the Offensive Language Identification Dataset (OLID), a benchmark Twitter\ndataset that includes hierarchical annotations. Our experimental results show\nthat XLNet outperforms BERT in detecting offensive content and in categorizing\nthe types of offenses, while BERT performs slightly better in identifying the\ntargets of the offenses. Additionally, we find that oversampling and\nundersampling strategies are effective in addressing class imbalance and\nimproving classification performance. These findings highlight the potential of\ntransfer learning and XLNet-based architectures to create robust systems for\ndetecting offensive language on social media platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21795v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21795v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21603", "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "authors": ["Yenisel Plasencia-Calaña"], "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21603v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21603v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22246", "title": "EAMamba: Efficient All-Around Vision State Space Model for Image Restoration", "authors": ["Yu-Cheng Lin", "Yu-Syuan Xu", "Hao-Wei Chen", "Hsien-Kai Kuo", "Chun-Yi Lee"], "summary": "Image restoration is a key task in low-level computer vision that aims to\nreconstruct high-quality images from degraded inputs. The emergence of Vision\nMamba, which draws inspiration from the advanced state space model Mamba, marks\na significant advancement in this field. Vision Mamba demonstrates excellence\nin modeling long-range dependencies with linear complexity, a crucial advantage\nfor image restoration tasks. Despite its strengths, Vision Mamba encounters\nchallenges in low-level vision tasks, including computational complexity that\nscales with the number of scanning sequences and local pixel forgetting. To\naddress these limitations, this study introduces Efficient All-Around Mamba\n(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan\nModule (MHSSM) with an all-around scanning mechanism. MHSSM efficiently\naggregates multiple scanning sequences, which avoids increases in computational\ncomplexity and parameter count. The all-around scanning strategy implements\nmultiple patterns to capture holistic information and resolves the local pixel\nforgetting issue. Our experimental evaluations validate these innovations\nacross several restoration tasks, including super resolution, denoising,\ndeblurring, and dehazing. The results validate that EAMamba achieves a\nsignificant 31-89% reduction in FLOPs while maintaining favorable performance\ncompared to existing low-level Vision Mamba methods.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22246v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22246v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21625", "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents", "authors": ["Jiaxi Zhuang", "Kangning Li", "Jue Hou", "Mingjun Xu", "Zhifeng Gao", "Hengxing Cai"], "summary": "Extracting molecular structure-activity relationships (SARs) from scientific\nliterature and patents is essential for drug discovery and materials research.\nHowever, this task remains challenging due to heterogeneous document formats\nand limitations of existing methods. Specifically, rule-based approaches\nrelying on rigid templates fail to generalize across diverse document layouts,\nwhile general-purpose multimodal large language models (MLLMs) lack sufficient\naccuracy and reliability for specialized tasks, such as layout detection and\noptical chemical structure recognition (OCSR). To address these challenges, we\nintroduce DocSAR-200, a rigorously annotated benchmark of 200 scientific\ndocuments designed specifically for evaluating SAR extraction methods.\nAdditionally, we propose Doc2SAR, a novel synergistic framework that integrates\ndomain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).\nExtensive experiments demonstrate that Doc2SAR achieves state-of-the-art\nperformance across various document types, significantly outperforming leading\nend-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of\n80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR\ndemonstrates practical usability through efficient inference and is accompanied\nby a web app.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21625v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21625v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21808", "title": "A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence", "authors": ["Jonathan St-Onge", "Ashley M. A. Fehr", "Carter Ward", "Calla G. Beauregard", "Michael V. Arnold", "Samuel F. Rosenblatt", "Benjamin Cooley", "Christopher M. Danforth", "Peter Sheridan Dodds"], "summary": "Describing and comparing complex systems requires principled, theoretically\ngrounded tools. Built around the phenomenon of type turbulence,\nallotaxonographs provide map-and-list visual comparisons of pairs of\nheavy-tailed distributions. Allotaxonographs are designed to accommodate a wide\nrange of instruments including rank- and probability-turbulence divergences,\nJenson-Shannon divergence, and generalized entropy divergences. Here, we\ndescribe a suite of programmatic tools for rendering allotaxonographs for\nrank-turbulence divergence in Matlab, Javascript, and Python, all of which have\ndifferent use cases.", "comment": "4 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21808v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21808v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21604", "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "pdf_url": "http://arxiv.org/pdf/2506.21604v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21604v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22274", "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication", "authors": ["Filippo Merlo", "Ece Takmaz", "Wenkai Chen", "Albert Gatt"], "summary": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22274v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22274v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21627", "title": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models", "authors": ["Shiyi Wang", "Wenbo Li", "Yiteng Chen", "Qingyao Wu", "Huiping Zhuang"], "summary": "Developing a general robot manipulation system capable of performing a wide\nrange of tasks in complex, dynamic, and unstructured real-world environments\nhas long been a challenging task. It is widely recognized that achieving\nhuman-like efficiency and robustness manipulation requires the robotic brain to\nintegrate a comprehensive set of functions, such as task planning, policy\ngeneration, anomaly monitoring and handling, and long-term memory, achieving\nhigh-efficiency operation across all functions. Vision-Language Models (VLMs),\npretrained on massive multimodal data, have acquired rich world knowledge,\nexhibiting exceptional scene understanding and multimodal reasoning\ncapabilities. However, existing methods typically focus on realizing only a\nsingle function or a subset of functions within the robotic brain, without\nintegrating them into a unified cognitive architecture. Inspired by a\ndivide-and-conquer strategy and the architecture of the human brain, we propose\nFrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that\nachieves both comprehensive functionality and high operational efficiency. Our\nframework includes a suite of components, decoupling a part of key functions\nfrom frequent VLM calls, striking an optimal balance between functional\ncompleteness and system efficiency. Specifically, we map task planning, policy\ngeneration, memory management, and low-level interfacing to the cortex,\ncerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and\ndesign efficient coordination mechanisms for the modules. We conducted\ncomprehensive experiments in both simulation and real-world robotic\nenvironments, demonstrating that our method offers significant advantages in\nanomaly detection and handling, long-term memory, operational efficiency, and\nstability -- all without requiring any fine-tuning or retraining.", "comment": "15 pages, 4 figures, under review of NeurIPS", "pdf_url": "http://arxiv.org/pdf/2506.21627v1", "categories": ["cs.RO", "cs.AI", "F.4.3; I.2.9"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21627v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21812", "title": "Towards Transparent AI: A Survey on Explainable Large Language Models", "authors": ["Avash Palikhe", "Zhenyu Yu", "Zichong Wang", "Wenbin Zhang"], "summary": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21812v1", "categories": ["cs.CL", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21812v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21607", "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21607v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21607v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22283", "title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment", "authors": ["Rui Xu", "Yunke Wang", "Yong Luo", "Bo Du"], "summary": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22283v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22283v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21628", "title": "Ark: An Open-source Python-based Framework for Robot Learning", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21628v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21628v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21817", "title": "Exploring the Structure of AI-Induced Language Change in Scientific English", "authors": ["Riley Galpin", "Bryce Anderson", "Tom S. Juzek"], "summary": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language.", "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table.\n  Licensed under CC BY-NC-SA 4.0", "pdf_url": "http://arxiv.org/pdf/2506.21817v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.1"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21817v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21611", "title": "Does Multimodality Lead to Better Time Series Forecasting?", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21611v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21611v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22291", "title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation", "authors": ["Mengqi Zhou", "Xipeng Wang", "Yuxi Wang", "Zhaoxiang Zhang"], "summary": "Generating realistic 3D indoor scenes from user inputs remains a challenging\nproblem in computer vision and graphics, requiring careful balance of geometric\nconsistency, spatial relationships, and visual realism. While neural generation\nmethods often produce repetitive elements due to limited global spatial\nreasoning, procedural approaches can leverage constraints for controllable\ngeneration but struggle with multi-constraint scenarios. When constraints\nbecome numerous, object collisions frequently occur, forcing the removal of\nfurniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline\nthat converts real images, sketches, or text descriptions into coherent 3D\nindoor scenes. Our approach combines a scene generation pipeline with a\nconstraint-driven optimization framework. The pipeline first extracts\nhigh-level scene information from user inputs and organizes it into a\nstructured format containing room type, furniture items, and spatial relations.\nIt then constructs a spatial relationship network to represent furniture\narrangements and generates an optimized placement sequence using a\nheuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.\nTo handle complex multi-constraint scenarios, we introduce a unified constraint\nrepresentation that processes both formal specifications and natural language\ninputs, enabling flexible constraint-oriented adjustments through a\ncomprehensive action space design. Additionally, we propose a Conflict-Aware\nPositioning Strategy (CAPS) that dynamically adjusts placement weights to\nminimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms\nexisting methods in generating realistic, semantically coherent, and visually\nappealing room layouts across diverse input modalities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22291v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22291v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21635", "title": "AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing", "authors": ["Haiping Yang", "Huaxing Liu", "Wei Wu", "Zuohui Chen", "Ning Wu"], "summary": "Unmanned aerial vehicles (UAVs) are increasingly employed in diverse\napplications such as land surveying, material transport, and environmental\nmonitoring. Following missions like data collection or inspection, UAVs must\nland safely at docking stations for storage or recharging, which is an\nessential requirement for ensuring operational continuity. However, accurate\nlanding remains challenging due to factors like GPS signal interference. To\naddress this issue, we propose a deviation warning system for UAV landings,\npowered by a novel vision-based model called AeroLite-MDNet. This model\nintegrates a multiscale fusion module for robust cross-scale object detection\nand incorporates a segmentation branch for efficient orientation estimation. We\nintroduce a new evaluation metric, Average Warning Delay (AWD), to quantify the\nsystem's sensitivity to landing deviations. Furthermore, we contribute a new\ndataset, UAVLandData, which captures real-world landing deviation scenarios to\nsupport training and evaluation. Experimental results show that our system\nachieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\\%,\ndemonstrating its effectiveness in enhancing UAV landing reliability. Code will\nbe available at https://github.com/ITTTTTI/Maskyolo.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21635v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21635v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.21840", "title": "PARSI: Persian Authorship Recognition via Stylometric Integration", "authors": ["Kourosh Shahnazari", "Mohammadali Keshtparvar", "Seyed Moein Ayyoubzadeh"], "summary": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21840v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21840v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21623", "title": "Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints", "authors": ["Peiheng Gao", "Chen Yang", "Ning Sun", "Ričardas Zitikis"], "summary": "Machine learning (ML) has significantly advanced text classification by\nenabling automated understanding and categorization of complex, unstructured\ntextual data. However, accurately capturing nuanced linguistic patterns and\ncontextual variations inherent in natural language, particularly within\nconsumer complaints, remains a challenge. This study addresses these issues by\nincorporating human-experience-trained algorithms that effectively recognize\nsubtle semantic differences crucial for assessing consumer relief eligibility.\nFurthermore, we propose integrating synthetic data generation methods that\nutilize expert evaluations of generative adversarial networks and are refined\nthrough expert annotations. By combining expert-trained classifiers with\nhigh-quality synthetic data, our research seeks to significantly enhance\nmachine learning classifier performance, reduce dataset acquisition costs, and\nimprove overall evaluation metrics and robustness in text classification tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21623v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21623v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22298", "title": "OutDreamer: Video Outpainting with a Diffusion Transformer", "authors": ["Linhao Zhong", "Fan Li", "Yi Huang", "Jianzhuang Liu", "Renjing Pei", "Fenglong Song"], "summary": "Video outpainting is a challenging task that generates new video content by\nextending beyond the boundaries of an original input video, requiring both\ntemporal and spatial consistency. Many state-of-the-art methods utilize latent\ndiffusion models with U-Net backbones but still struggle to achieve high\nquality and adaptability in generated content. Diffusion transformers (DiTs)\nhave emerged as a promising alternative because of their superior performance.\nWe introduce OutDreamer, a DiT-based video outpainting framework comprising two\nmain components: an efficient video control branch and a conditional\noutpainting branch. The efficient video control branch effectively extracts\nmasked video information, while the conditional outpainting branch generates\nmissing content based on these extracted conditions. Additionally, we propose a\nmask-driven self-attention layer that dynamically integrates the given mask\ninformation, further enhancing the model's adaptability to outpainting tasks.\nFurthermore, we introduce a latent alignment loss to maintain overall\nconsistency both within and between frames. For long video outpainting, we\nemploy a cross-video-clip refiner to iteratively generate missing content,\nensuring temporal consistency across video clips. Extensive evaluations\ndemonstrate that our zero-shot OutDreamer outperforms state-of-the-art\nzero-shot methods on widely recognized benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22298v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22298v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21638", "title": "IRanker: Towards Ranking Foundation Model", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21638v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21638v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.21848", "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification", "authors": ["Duo Zhang", "Junyi Mo"], "summary": "Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21848v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21848v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21624", "title": "DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation", "authors": ["Blaž Škrlj", "Yonatan Karni", "Grega Gašperšič", "Blaž Mramor", "Yulia Stolin", "Martin Jakomin", "Jasna Urbančič", "Yuval Dishi", "Natalia Silberstein", "Ophir Friedler", "Assaf Klein"], "summary": "The Deep and Cross architecture (DCNv2) is a robust production baseline and\nis integral to numerous real-life recommender systems. Its inherent efficiency\nand ability to model interactions often result in models that are both simpler\nand highly competitive compared to more computationally demanding alternatives,\nsuch as Deep FFMs. In this work, we introduce three significant algorithmic\nimprovements to the DCNv2 architecture, detailing their formulation and\nbehavior at scale. The enhanced architecture we refer to as DCN^2 is actively\nused in a live recommender system, processing over 0.5 billion predictions per\nsecond across diverse use cases where it out-performed DCNv2, both offline and\nonline (ab tests). These improvements effectively address key limitations\nobserved in the DCNv2, including information loss in Cross layers, implicit\nmanagement of collisions through learnable lookup-level weights, and explicit\nmodeling of pairwise similarities with a custom layer that emulates FFMs'\nbehavior. The superior performance of DCN^2 is also demonstrated on four\npublicly available benchmark data sets.", "comment": "AdKDD 25", "pdf_url": "http://arxiv.org/pdf/2506.21624v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21624v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22336", "title": "MatChA: Cross-Algorithm Matching with Feature Augmentation", "authors": ["Paula Carbó Cubero", "Alberto Jaenal Gálvez", "André Mateus", "José Araújo", "Patric Jensfelt"], "summary": "State-of-the-art methods fail to solve visual localization in scenarios where\ndifferent devices use different sparse feature extraction algorithms to obtain\nkeypoints and their corresponding descriptors. Translating feature descriptors\nis enough to enable matching. However, performance is drastically reduced in\ncross-feature detector cases, because current solutions assume common\nkeypoints. This means that the same detector has to be used, which is rarely\nthe case in practice when different descriptors are used. The low repeatability\nof keypoints, in addition to non-discriminatory and non-distinctive\ndescriptors, make the identification of true correspondences extremely\nchallenging. We present the first method tackling this problem, which performs\nfeature descriptor augmentation targeting cross-detector feature matching, and\nthen feature translation to a latent space. We show that our method\nsignificantly improves image matching and visual localization in the\ncross-feature scenario and evaluate the proposed method on several benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22336v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22336v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21655", "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21655v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21655v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21849", "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models", "authors": ["Quan Xiao", "Debarun Bhattacharjya", "Balaji Ganesan", "Radu Marinescu", "Katsiaryna Mirylenka", "Nhan H Pham", "Michael Glass", "Junkyu Lee"], "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.", "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21849v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21849v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21628", "title": "Ark: An Open-source Python-based Framework for Robot Learning", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21628v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21628v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22338", "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "authors": ["Luigi Russo", "Deodato Tapete", "Silvia Liberata Ullo", "Paolo Gamba"], "summary": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.", "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted\n  to IEEE Journal of Selected Topics in Applied Earth Observations and Remote\n  Sensing", "pdf_url": "http://arxiv.org/pdf/2506.22338v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22338v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21718", "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "pdf_url": "http://arxiv.org/pdf/2506.21718v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21718v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21861", "title": "Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models", "authors": ["Taiga Someya", "Ryo Yoshida", "Hitomi Yanaka", "Yohei Oseki"], "summary": "Recent work has demonstrated that neural language models encode syntactic\nstructures in their internal representations, yet the derivations by which\nthese structures are constructed across layers remain poorly understood. In\nthis paper, we propose Derivational Probing to investigate how micro-syntactic\nstructures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,\nthe relationship between the root verbs and their direct dependents) are\nconstructed as word embeddings propagate upward across layers. Our experiments\non BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge\nin lower layers and are gradually integrated into a coherent macro-syntactic\nstructure in higher layers. Furthermore, a targeted evaluation on subject-verb\nnumber agreement shows that the timing of constructing macro-syntactic\nstructures is critical for downstream performance, suggesting an optimal timing\nfor integrating global syntactic information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21861v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21861v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21630", "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "comment": "8 pages, 9 figures, 2025 IJCNN", "pdf_url": "http://arxiv.org/pdf/2506.21630v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21630v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22347", "title": "Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults", "authors": ["Hans Geißner", "Christian Rathgeb"], "summary": "This paper analyses and addresses the performance gap in the fuzzy\nvault-based \\ac{BCS}. We identify unstable error correction capabilities, which\nare caused by variable feature set sizes and their influence on similarity\nthresholds, as a key source of performance degradation. This issue is further\ncompounded by information loss introduced through feature type transformations.\nTo address both problems, we propose a novel feature quantization method based\non \\it{equal frequent intervals}. This method guarantees fixed feature set\nsizes and supports training-free adaptation to any number of intervals. The\nproposed approach significantly reduces the performance gap introduced by\ntemplate protection. Additionally, it integrates seamlessly with existing\nsystems to minimize the negative effects of feature transformation. Experiments\non state-of-the-art face, fingerprint, and iris recognition systems confirm\nthat only minimal performance degradation remains, demonstrating the\neffectiveness of the method across major biometric modalities.", "comment": "10 pages, 4 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2506.22347v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22347v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21722", "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration", "authors": ["Xin Lu", "Xueyang Fu", "Jie Xiao", "Zihao Fan", "Yurui Zhu", "Zheng-Jun Zha"], "summary": "While diffusion models demonstrate strong generative capabilities in image\nrestoration (IR) tasks, their complex architectures and iterative processes\nlimit their practical application compared to mainstream reconstruction-based\ngeneral ordinary IR networks. Existing approaches primarily focus on optimizing\nnetwork architecture and diffusion paths but overlook the integration of the\ndiffusion training paradigm within general ordinary IR frameworks. To address\nthese challenges, this paper elucidates key principles for adapting the\ndiffusion training paradigm to general IR training through systematic analysis\nof time-step dependencies, network hierarchies, noise-level relationships, and\nmulti-restoration task correlations, proposing a new IR framework supported by\ndiffusion-based training. To enable IR networks to simultaneously restore\nimages and model generative representations, we introduce a series of\nregularization strategies that align diffusion objectives with IR tasks,\nimproving generalization in single-task scenarios. Furthermore, recognizing\nthat diffusion-based generation exerts varying influences across different IR\ntasks, we develop an incremental training paradigm and task-specific adaptors,\nfurther enhancing performance in multi-task unified IR. Experiments demonstrate\nthat our method significantly improves the generalization of IR networks in\nsingle-task IR and achieves superior performance in multi-task unified IR.\nNotably, the proposed framework can be seamlessly integrated into existing\ngeneral IR architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21722v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21722v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21864", "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE", "authors": ["Hang Shao", "Heting Gao", "Yunhang Shen", "Jiawei Chen", "Lijiang Li", "Zuwei Long", "Bo Tong", "Ke Li", "Xing Sun"], "summary": "Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2506.21864v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21864v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21638", "title": "IRanker: Towards Ranking Foundation Model", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21638v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21638v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22360", "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "pdf_url": "http://arxiv.org/pdf/2506.22360v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22360v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21727", "title": "Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions", "authors": ["Yasushi Kawase", "Bodhayan Roy", "Mohammad Azharuddin Sanpui"], "summary": "This paper explores the fair allocation of indivisible items in a\nmultidimensional setting, motivated by the need to address fairness in complex\nenvironments where agents assess bundles according to multiple criteria. Such\nmultidimensional settings are not merely of theoretical interest but are\ncentral to many real-world applications. For example, cloud computing resources\nare evaluated based on multiple criteria such as CPU cores, memory, and network\nbandwidth. In such cases, traditional one dimensional fairness notions fail to\ncapture fairness across multiple attributes. To address these challenges, we\nstudy two relaxed variants of envy-freeness: weak simultaneously envy-free up\nto c goods (weak sEFc) and strong simultaneously envy-free up to c goods\n(strong sEFc), which accommodate the multidimensionality of agents'\npreferences. Under the weak notion, for every pair of agents and for each\ndimension, any perceived envy can be eliminated by removing, if necessary, a\ndifferent set of goods from the envied agent's allocation. In contrast, the\nstrong version requires selecting a single set of goods whose removal from the\nenvied bundle simultaneously eliminates envy in every dimension. We provide\nupper and lower bounds on the relaxation parameter c that guarantee the\nexistence of weak or strong sEFc allocations, where these bounds are\nindependent of the total number of items. In addition, we present algorithms\nfor checking whether a weak or strong sEFc allocation exists. Moreover, we\nestablish NP-hardness results for checking the existence of weak sEF1 and\nstrong sEF1 allocations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21727v1", "categories": ["cs.GT", "cs.AI"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.21727v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21875", "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation", "authors": ["Jian Zhang", "Linhao Zhang", "Bokai Lei", "Chuhan Wu", "Wei Jia", "Xiao Zhou"], "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21875v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21875v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21681", "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation", "authors": ["Hakan Çapuk", "Andrew Bond", "Muhammed Burak Kızıl", "Emir Göçen", "Erkut Erdem", "Aykut Erdem"], "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21681v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21681v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22375", "title": "Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation", "authors": ["Tiankai Chen", "Yushu Li", "Adam Goodge", "Fei Teng", "Xulei Yang", "Tianrui Li", "Xun Xu"], "summary": "Out-of-distribution (OOD) detection in 3D point cloud data remains a\nchallenge, particularly in applications where safe and robust perception is\ncritical. While existing OOD detection methods have shown progress for 2D image\ndata, extending these to 3D environments involves unique obstacles. This paper\nintroduces a training-free framework that leverages Vision-Language Models\n(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph\nbased on class prototypes and testing data, we exploit the data manifold\nstructure to enhancing the effectiveness of VLMs for 3D OOD detection. We\npropose a novel Graph Score Propagation (GSP) method that incorporates prompt\nclustering and self-training negative prompting to improve OOD scoring with\nVLM. Our method is also adaptable to few-shot scenarios, providing options for\npractical applications. We demonstrate that GSP consistently outperforms\nstate-of-the-art methods across synthetic and real-world datasets 3D point\ncloud OOD detection.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22375v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22375v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21731", "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis", "authors": ["Chenqiu Zhao", "Anup Basu"], "summary": "We propose two theoretical frameworks, the Mutually Exclusive Probability\nSpace (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential\nlimitation in probabilistic generative models; namely that learning global\ndistributions leads to memorization rather than generative behavior. MESP\nemerges from our rethinking of the Variational Autoencoder (VAE). We observe\nthat latent variable distributions in VAE exhibit overlap, which leads to an\noptimization conflict between the reconstruction loss and KL-divergence loss. A\nlower bound based on the overlap coefficient is proposed. We refer to this\nphenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary\nLatent Autoencoder (BL-AE) is proposed to encode images into binary latent\nrepresentations. These binary latents are used as the input to our\nAutoregressive Random Variable Model (ARVM), a modified autoregressive model\noutputting histograms. Our ARVM achieves competitive FID scores, outperforming\nstate-of-the-art methods on standard datasets. However, such scores reflect\nmemorization rather than generation. To address this issue, we propose the\nLocal Correlation Hypothesis (LCH), which posits that generative capability\narising from local correlations among latent variables. Comprehensive\nexperiments and discussions are conducted to validate our frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21731v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21731v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21876", "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation", "authors": ["Qiyue Gao", "Xinyu Pi", "Kevin Liu", "Junrong Chen", "Ruolan Yang", "Xinqi Huang", "Xinyu Fang", "Lu Sun", "Gautham Kishore", "Bo Ai", "Stone Tao", "Mengyang Liu", "Jiaxi Yang", "Chao-Jung Lai", "Chuanyang Jin", "Jiannan Xiang", "Benhao Huang", "Zeming Chen", "David Danks", "Hao Su", "Tianmin Shu", "Ziqiao Ma", "Lianhui Qin", "Zhiting Hu"], "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.", "comment": "ACL 2025 (Findings)", "pdf_url": "http://arxiv.org/pdf/2506.21876v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21876v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21686", "title": "ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages", "authors": ["Swastika Kundu", "Autoshi Ibrahim", "Mithila Rahman", "Tanvir Ahmed"], "summary": "Sentiment analysis for regional dialects of Bangla remains an underexplored\narea due to linguistic diversity and limited annotated data. This paper\nintroduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences\nmanually translated from standard Bangla into four major regional dialects\nMymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly\nfeatures political and religious content, reflecting the contemporary socio\npolitical landscape of Bangladesh, alongside neutral texts to maintain balance.\nEach sentence is annotated using a dual annotation scheme: multiclass thematic\nlabeling categorizes sentences as Political, Religious, or Neutral, and\nmultilabel emotion annotation assigns one or more emotions from Anger,\nContempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native\ntranslators conducted the translation and annotation, with quality assurance\nperformed via Cohens Kappa inter annotator agreement, achieving strong\nconsistency across dialects. The dataset was further refined through systematic\nchecks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a\ncritical gap in resources for sentiment analysis in low resource Bangla\ndialects, enabling more accurate and context aware natural language processing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21686v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21686v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22385", "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22385v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22385v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21732", "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "authors": ["Ameya Salvi", "Venkat Krovi"], "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21732v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21732v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21881", "title": "A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs", "authors": ["Sean Kim", "Hyuhng Joon Kim"], "summary": "As large language models (LLMs) are increasingly deployed across diverse\nlinguistic and cultural contexts, understanding their behavior in both factual\nand disputable scenarios is essential, especially when their outputs may shape\npublic opinion or reinforce dominant narratives. In this paper, we define two\ntypes of bias in LLMs: model bias (bias stemming from model training) and\ninference bias (bias induced by the language of the query), through a two-phase\nevaluation. Phase 1 evaluates LLMs on factual questions where a single\nverifiable answer exists, assessing whether models maintain consistency across\ndifferent query languages. Phase 2 expands the scope by probing geopolitically\nsensitive disputes, where responses may reflect culturally embedded or\nideologically aligned perspectives. We construct a manually curated dataset\nspanning both factual and disputable QA, across four languages and question\ntypes. The results show that Phase 1 exhibits query language induced alignment,\nwhile Phase 2 reflects an interplay between the model's training context and\nquery language. This paper offers a structured framework for evaluating LLM\nbehavior across neutral and sensitive topics, providing insights for future LLM\ndeployment and culturally aware evaluation practices in multilingual contexts.", "comment": "This paper is accepted to ACL Student Research Workshop (SRW) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21881v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21881v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21720", "title": "CaloHadronic: a diffusion model for the generation of hadronic showers", "authors": ["Thorsten Buss", "Frank Gaede", "Gregor Kasieczka", "Anatolii Korol", "Katja Krüger", "Peter McKeown", "Martina Mozzanica"], "summary": "Simulating showers of particles in highly-granular calorimeters is a key\nfrontier in the application of machine learning to particle physics. Achieving\nhigh accuracy and speed with generative machine learning models can enable them\nto augment traditional simulations and alleviate a major computing constraint.\nRecent developments have shown how diffusion based generative shower simulation\napproaches that do not rely on a fixed structure, but instead generate\ngeometry-independent point clouds, are very efficient. We present a\ntransformer-based extension to previous architectures which were developed for\nsimulating electromagnetic showers in the highly granular electromagnetic\ncalorimeter of the International Large Detector, ILD. The attention mechanism\nnow allows us to generate complex hadronic showers with more pronounced\nsubstructure across both the electromagnetic and hadronic calorimeters. This is\nthe first time that machine learning methods are used to holistically generate\nshowers across the electromagnetic and hadronic calorimeter in highly granular\nimaging calorimeter systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21720v1", "categories": ["physics.ins-det", "cs.LG", "hep-ex", "hep-ph", "physics.data-an"], "cate": "physics.ins-det", "url": "http://arxiv.org/abs/2506.21720v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22395", "title": "Test-Time Consistency in Vision Language Models", "authors": ["Shih-Han Chou", "Shivam Chandhok", "James J. Little", "Leonid Sigal"], "summary": "Vision-Language Models (VLMs) have achieved impressive performance across a\nwide range of multimodal tasks, yet they often exhibit inconsistent behavior\nwhen faced with semantically equivalent inputs, undermining their reliability\nand robustness. Recent benchmarks, such as MM-R3, highlight that even\nstate-of-the-art VLMs can produce divergent predictions across semantically\nequivalent inputs, despite maintaining high average accuracy. Prior work\naddresses this issue by modifying model architectures or conducting large-scale\nfine-tuning on curated datasets. In contrast, we propose a simple and effective\ntest-time consistency framework that enhances semantic consistency without\nsupervised re-training. Our method is entirely post-hoc, model-agnostic, and\napplicable to any VLM with access to its weights. Given a single test point, we\nenforce consistent predictions via two complementary objectives: (i) a\nCross-Entropy Agreement Loss that aligns predictive distributions across\nsemantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that\ndraws outputs toward a self-averaged consensus. Our method is plug-and-play and\nleverages information from a single test input itself to improve consistency.\nExperiments on the MM-R3 benchmark show that our framework yields substantial\ngains in consistency across state-of-the-art models, establishing a new\ndirection for inference-time adaptation in multimodal learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22395v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22395v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21783", "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models", "authors": ["Alexandru Dumitru", "V Venktesh", "Adam Jatowt", "Avishek Anand"], "summary": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.", "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages", "pdf_url": "http://arxiv.org/pdf/2506.21783v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21783v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21910", "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers", "authors": ["Ernie Chang", "Yang Li", "Patrick Huber", "David Kant", "Yangyang Shi", "Vikas Chandra"], "summary": "In language model training, it is desirable to equip models with capabilities\nfrom various tasks. However, it is not clear how to directly obtain the right\ndata mixtures for these capabilities as the relationship between data and tasks\nis difficult to be modeled. In this work, we observe that checkpoint models\nexhibit emerging capabilities at different points in the training trajectory.\nOften, the training process saves checkpoints as artifacts that are\nunder-utilized as a source of in-training data signals. We identify these\nartifact models based on their respective capabilities on the benchmarks and\nleverage them as data mixers by using their aggregated first-order influence\napproximation over source data. We demonstrated on eight reasoning benchmarks\nthat the proposed framework shows significant improvements in the pretraining\nsetting, with performance improvements of up to 1.93%. Overall, this shows the\npotential of checkpoint models to enhance data quality and optimize data\nmixtures.", "comment": "Accepted at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21910v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21910v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21732", "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "authors": ["Ameya Salvi", "Venkat Krovi"], "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21732v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21732v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22432", "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy", "authors": ["Yuhao Liu", "Tengfei Wang", "Fang Liu", "Zhenwei Wang", "Rynson W. H. Lau"], "summary": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22432v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22432v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21785", "title": "Comparing Learning Paradigms for Egocentric Video Summarization", "authors": ["Daniel Wen"], "summary": "In this study, we investigate various computer vision paradigms - supervised\nlearning, unsupervised learning, and prompt fine-tuning - by assessing their\nability to understand and interpret egocentric video data. Specifically, we\nexamine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM\n(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned\npre-trained model), evaluating their effectiveness in video summarization. Our\nresults demonstrate that current state-of-the-art models perform less\neffectively on first-person videos compared to third-person videos,\nhighlighting the need for further advancements in the egocentric video domain.\nNotably, a prompt fine-tuned general-purpose GPT-4o model outperforms these\nspecialized models, emphasizing the limitations of existing approaches in\nadapting to the unique challenges of first-person perspectives. Although our\nevaluation is conducted on a small subset of egocentric videos from the\nEgo-Exo4D dataset due to resource constraints, the primary objective of this\nresearch is to provide a comprehensive proof-of-concept analysis aimed at\nadvancing the application of computer vision techniques to first-person videos.\nBy exploring novel methodologies and evaluating their potential, we aim to\ncontribute to the ongoing development of models capable of effectively\nprocessing and interpreting egocentric perspectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21785v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21785v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21961", "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory", "authors": ["Junho Myung", "Yeon Su Park", "Sunwoo Kim", "Shin Yoo", "Alice Oh"], "summary": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please.", "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21961v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21961v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21734", "title": "Hierarchical Reasoning Model", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21734v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21734v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22433", "title": "WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields", "authors": ["Sadra Safadoust", "Fabio Tosi", "Fatma Güney", "Matteo Poggi"], "summary": "We introduce WarpRF, a training-free general-purpose framework for\nquantifying the uncertainty of radiance fields. Built upon the assumption that\nphotometric and geometric consistency should hold among images rendered by an\naccurate model, WarpRF quantifies its underlying uncertainty from an unseen\npoint of view by leveraging backward warping across viewpoints, projecting\nreliable renderings to the unseen viewpoint and measuring the consistency with\nimages rendered there. WarpRF is simple and inexpensive, does not require any\ntraining, and can be applied to any radiance field implementation for free.\nWarpRF excels at both uncertainty quantification and downstream tasks, e.g.,\nactive view selection and active mapping, outperforming any existing method\ntailored to specific frameworks.", "comment": "Project page: https://kuis-ai.github.io/WarpRF/", "pdf_url": "http://arxiv.org/pdf/2506.22433v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22433v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21788", "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data", "authors": ["Massimiliano Lupo Pasini", "Jong Youl Choi", "Pei Zhang", "Kshitij Mehta", "Rylie Weaver", "Ashwin M. Aji", "Karl W. Schulz", "Jorda Polo", "Prasanna Balaprakash"], "summary": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures.", "comment": "15 pages, 4 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.21788v1", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.atm-clus", "68T07, 68T09", "I.2; I.2.5; I.2.11"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21788v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21967", "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21967v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21967v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21739", "title": "Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19", "authors": ["Felipe Rogério Pimentel", "Rafael Gustavo Alves"], "summary": "Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use\nthe Finite Impulse Response (FIR) linear system filtering method to track and\npredict the number of people infected and recovered from COVID-19, in a\npandemic context in which there was still no vaccine and the only way to avoid\ncontagion was isolation. To estimate the coefficients of these FIR filters,\nChen et al. used machine learning methods through a classical optimization\nproblem with regularization (ridge regression). These estimated coefficients\nare called ridge coefficients. The epidemic mathematical model adopted by these\nresearchers to formulate the FIR filters is the time-dependent discrete SIR. In\nthis paper, we propose a small modification to the algorithm of Chen et al. to\nobtain the ridge coefficients. We then used this modified algorithm to track\nand predict the number of people infected and recovered from COVID-19 in the\nstate of Minas Gerais/Brazil, within a prediction window, during the initial\nperiod of the pandemic. We also compare the predicted data with the respective\nreal data to check how good the approximation is. In the modified algorithm, we\nset values for the FIR filter orders and for the regularization parameters,\nboth different from the respective values defined by Chen et al. in their\nalgorithm. In this context, the numerical results obtained by the modified\nalgorithm in some simulations present better approximation errors compared to\nthe respective approximation errors presented by the algorithm of Chen et al.", "comment": "14 pages, 3 figures, 3 tables, and 2 algorithms", "pdf_url": "http://arxiv.org/pdf/2506.21739v1", "categories": ["stat.ML", "cs.LG", "math.OC", "92B05, 92-10, 65K05, 37M99, 49"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21739v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22434", "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "authors": ["Xi Chen", "Mingkang Zhu", "Shaoteng Liu", "Xiaoyang Wu", "Xiaogang Xu", "Yu Liu", "Xiang Bai", "Hengshuang Zhao"], "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22434v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22434v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21796", "title": "Demonstrating Interoperable Channel State Feedback Compression with Machine Learning", "authors": ["Dani Korpi", "Rachel Wang", "Jerry Wang", "Abdelrahman Ibrahim", "Carl Nuzman", "Runxin Wang", "Kursat Rasim Mestav", "Dustin Zhang", "Iraj Saniee", "Shawn Winston", "Gordana Pavlovic", "Wei Ding", "William J. Hillery", "Chenxi Hao", "Ram Thirunagari", "Jung Chang", "Jeehyun Kim", "Bartek Kozicki", "Dragan Samardzija", "Taesang Yoo", "Andreas Maeder", "Tingfang Ji", "Harish Viswanathan"], "summary": "Neural network-based compression and decompression of channel state feedback\nhas been one of the most widely studied applications of machine learning (ML)\nin wireless networks. Various simulation-based studies have shown that ML-based\nfeedback compression can result in reduced overhead and more accurate channel\ninformation. However, to the best of our knowledge, there are no real-life\nproofs of concepts demonstrating the benefits of ML-based channel feedback\ncompression in a practical setting, where the user equipment (UE) and base\nstation have no access to each others' ML models. In this paper, we present a\nnovel approach for training interoperable compression and decompression ML\nmodels in a confidential manner, and demonstrate the accuracy of the ensuing\nmodels using prototype UEs and base stations. The performance of the ML-based\nchannel feedback is measured both in terms of the accuracy of the reconstructed\nchannel information and achieved downlink throughput gains when using the\nchannel information for beamforming. The reported measurement results\ndemonstrate that it is possible to develop an accurate ML-based channel\nfeedback link without having to share ML models between device and network\nvendors. These results pave the way for a practical implementation of ML-based\nchannel feedback in commercial 6G networks.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.21796v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21796v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21972", "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21972v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21972v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21741", "title": "Critically-Damped Higher-Order Langevin Dynamics", "authors": ["Benjamin Sterling", "Chad Gueli", "Mónica F. Bugallo"], "summary": "Denoising Diffusion Probabilistic Models represent an entirely new class of\ngenerative AI methods that have yet to be fully explored. Critical damping has\nbeen successfully introduced in Critically-Damped Langevin Dynamics (CLD) and\nCritically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been\napplied to dynamics of arbitrary order. The proposed line of work generalizes\nHigher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion\nmethod, by introducing the concept of critical damping from systems analysis.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.21741v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21741v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21586", "title": "Can Vision Language Models Understand Mimed Actions?", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21586v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21586v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.21803", "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21803v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21803v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21974", "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism", "authors": ["Simon Münker", "Nils Schwager", "Achim Rettinger"], "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.", "comment": "11 pages, 1 figure, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21974v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21974v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21743", "title": "Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting", "authors": ["Jinpai Zhao", "Albert Cerrone", "Eirik Valseth", "Leendert Westerink", "Clint Dawson"], "summary": "Storm surge forecasting plays a crucial role in coastal disaster\npreparedness, yet existing machine learning approaches often suffer from\nlimited spatial resolution, reliance on coastal station data, and poor\ngeneralization. Moreover, many prior models operate directly on unstructured\nspatial data, making them incompatible with modern deep learning architectures.\nIn this work, we introduce a novel approach that projects unstructured water\nelevation fields onto structured Red Green Blue (RGB)-encoded image\nrepresentations, enabling the application of Convolutional Long Short Term\nMemory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our\nmodel further integrates ground-truth wind fields as dynamic conditioning\nsignals and topo-bathymetry as a static input, capturing physically meaningful\ndrivers of surge evolution. Evaluated on a large-scale dataset of synthetic\nstorms in the Gulf of Mexico, our method demonstrates robust 48-hour\nforecasting performance across multiple regions along the Texas coast and\nexhibits strong spatial extensibility to other coastal areas. By combining\nstructured representation, physically grounded forcings, and scalable deep\nlearning, this study advances the frontier of storm surge forecasting in\nusability, adaptability, and interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21743v1", "categories": ["cs.CE", "cs.LG"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21743v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21592", "title": "SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition", "authors": ["Tinh Nguyen", "Minh Khue Phan Tran"], "summary": "Sign language recognition is crucial for individuals with hearing impairments\nto break communication barriers. However, previous approaches have had to\nchoose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had\nproblems with vanishing gradients and high computational costs. Despite\nimproving performance, transformer-based methods were not commonly used. This\nstudy presents a new novel SLR approach that overcomes the challenge of\nindependently extracting meaningful information from the x and y coordinates of\nskeleton sequences, which traditional models often treat as inseparable. By\nutilizing an encoder-decoder of BART architecture, the model independently\nencodes the x and y coordinates, while Cross-Attention ensures their\ninterrelation is maintained. With only 749,888 parameters, the model achieves\n96.04% accuracy on the LSA-64 dataset, significantly outperforming previous\nmodels with over one million parameters. The model also demonstrates excellent\nperformance and generalization across WLASL and ASL-Citizen datasets. Ablation\nstudies underscore the importance of coordinate projection, normalization, and\nusing multiple skeleton components for boosting model efficacy. This study\noffers a reliable and effective approach for sign language recognition, with\nstrong potential for enhancing accessibility tools for the deaf and hard of\nhearing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21592v1", "categories": ["cs.CL", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21592v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.21813", "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "authors": ["Felix Holm", "Gözde Ünver", "Ghazal Ghazaei", "Nassir Navab"], "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21813v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21813v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21990", "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "pdf_url": "http://arxiv.org/pdf/2506.21990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21990v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21748", "title": "Inverse Design of Diffractive Metasurfaces Using Diffusion Models", "authors": ["Liav Hen", "Erez Yosef", "Dan Raviv", "Raja Giryes", "Jacob Scheuer"], "summary": "Metasurfaces are ultra-thin optical elements composed of engineered\nsub-wavelength structures that enable precise control of light. Their inverse\ndesign - determining a geometry that yields a desired optical response - is\nchallenging due to the complex, nonlinear relationship between structure and\noptical properties. This often requires expert tuning, is prone to local\nminima, and involves significant computational overhead. In this work, we\naddress these challenges by integrating the generative capabilities of\ndiffusion models into computational design workflows. Using an RCWA simulator,\nwe generate training data consisting of metasurface geometries and their\ncorresponding far-field scattering patterns. We then train a conditional\ndiffusion model to predict meta-atom geometry and height from a target spatial\npower distribution at a specified wavelength, sampled from a continuous\nsupported band. Once trained, the model can generate metasurfaces with low\nerror, either directly using RCWA-guided posterior sampling or by serving as an\ninitializer for traditional optimization methods. We demonstrate our approach\non the design of a spatially uniform intensity splitter and a polarization beam\nsplitter, both produced with low error in under 30 minutes. To support further\nresearch in data-driven metasurface design, we publicly release our code and\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21748v1", "categories": ["physics.optics", "cs.CV", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.21748v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21601", "title": "Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization", "authors": ["Duong Bach"], "summary": "Multi-vector document retrieval systems, such as ColPali, excel in\nfine-grained matching for complex queries but incur significant storage and\ncomputational costs due to their reliance on high-dimensional patch embeddings\nand late-interaction scoring. To address these challenges, we propose\nHPC-ColPali, a Hierarchical Patch Compression framework that enhances the\nefficiency of ColPali while preserving its retrieval accuracy. Our approach\nintegrates three innovative techniques: (1) K-Means quantization, which\ncompresses patch embeddings into 1-byte centroid indices, achieving up to\n32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing\nVision-Language Model attention weights to retain only the top-$p\\%$ most\nsalient patches, reducing late-interaction computation by up to 60\\% with less\nthan 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices\ninto $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming\ndistance-based similarity search for resource-constrained environments.\nEvaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\%\nlower query latency under HNSW indexing while maintaining high retrieval\nprecision. When integrated into a Retrieval-Augmented Generation pipeline for\nlegal summarization, it reduces hallucination rates by 30\\% and halves\nend-to-end latency. These advancements establish HPC-ColPali as a scalable and\nefficient solution for multi-vector document retrieval across diverse\napplications. Code is available at https://github.com/DngBack/HPC-ColPali.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2506.21601v1", "categories": ["cs.IR", "cs.CV"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21601v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21817", "title": "Exploring the Structure of AI-Induced Language Change in Scientific English", "authors": ["Riley Galpin", "Bryce Anderson", "Tom S. Juzek"], "summary": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language.", "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table.\n  Licensed under CC BY-NC-SA 4.0", "pdf_url": "http://arxiv.org/pdf/2506.21817v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.1"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21817v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22038", "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation", "authors": ["Delu Kong", "Lieve Macken"], "summary": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.", "comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology", "pdf_url": "http://arxiv.org/pdf/2506.22038v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22038v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21757", "title": "TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics", "authors": ["Tianrong Chen", "Huangjie Zheng", "David Berthelot", "Jiatao Gu", "Josh Susskind", "Shuangfei Zhai"], "summary": "Diffusion models have demonstrated exceptional capabilities in generating\nhigh-fidelity images but typically suffer from inefficient sampling. Many\nsolver designs and noise scheduling strategies have been proposed to\ndramatically improve sampling speeds. In this paper, we introduce a new\nsampling method that is up to $186\\%$ faster than the current state of the art\nsolver for comparative FID on ImageNet512. This new sampling method is\ntraining-free and uses an ordinary differential equation (ODE) solver. The key\nto our method resides in using higher-dimensional initial noise, allowing to\nproduce more detailed samples with less function evaluations from existing\npretrained diffusion models. In addition, by design our solver allows to\ncontrol the level of detail through a simple hyper-parameter at no extra\ncomputational cost. We present how our approach leverages momentum dynamics by\nestablishing a fundamental equivalence between momentum diffusion models and\nconventional diffusion models with respect to their training paradigms.\nMoreover, we observe the use of higher-dimensional noise naturally exhibits\ncharacteristics similar to stochastic differential equations (SDEs). Finally,\nwe demonstrate strong performances on a set of representative pretrained\ndiffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover\nmodels in both pixel and latent spaces, as well as class and text conditional\nsettings. The code is available at https://github.com/apple/ml-tada.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21757v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21757v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21604", "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "pdf_url": "http://arxiv.org/pdf/2506.21604v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21604v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.21819", "title": "SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge", "authors": ["Lena John", "Kheir Eddine Farfar", "Sören Auer", "Oliver Karras"], "summary": "Scientific publications, primarily digitized as PDFs, remain static and\nunstructured, limiting the accessibility and reusability of the contained\nknowledge. At best, scientific knowledge from publications is provided in\ntabular formats, which lack semantic context. A more flexible, structured, and\nsemantic representation is needed to make scientific knowledge understandable\nand processable by both humans and machines. We propose an evolution model of\nknowledge representation, inspired by the 5-star Linked Open Data (LOD) model,\nwith five stages and defined criteria to guide the stepwise transition from a\ndigital artifact, such as a PDF, to a semantic representation integrated in a\nknowledge graph (KG). Based on an exemplary workflow implementing the entire\nmodel, we developed a hybrid approach, called SciMantify, leveraging tabular\nformats of scientific knowledge, e.g., results from secondary studies, to\nsupport its evolving semantification. In the approach, humans and machines\ncollaborate closely by performing semantic annotation tasks (SATs) and refining\nthe results to progressively improve the semantic representation of scientific\nknowledge. We implemented the approach in the Open Research Knowledge Graph\n(ORKG), an established platform for improving the findability, accessibility,\ninteroperability, and reusability of scientific knowledge. A preliminary user\nexperiment showed that the approach simplifies the preprocessing of scientific\nknowledge, reduces the effort for the evolving semantification, and enhances\nthe knowledge representation through better alignment with the KG structures.", "comment": "Accepted at the 25th International Conference on Web Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2506.21819v1", "categories": ["cs.DL", "cs.AI", "cs.HC"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.21819v1", "date": "2025-04-14", "updated": "2025-04-14"}
{"id": "2506.22050", "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs", "authors": ["Delu Kong", "Lieve Macken"], "summary": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.", "comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology", "pdf_url": "http://arxiv.org/pdf/2506.22050v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22050v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21770", "title": "Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images", "authors": ["Rishiraj Paul Chowdhury", "Nirmit Shekar Karkera"], "summary": "Glaucoma is a leading cause of irreversible blindness, but early detection\ncan significantly improve treatment outcomes. Traditional diagnostic methods\nare often invasive and require specialized equipment. In this work, we present\na deep learning pipeline using the EfficientNet-B0 architecture for glaucoma\ndetection from retinal fundus images. Unlike prior studies that rely on single\ndatasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,\nand RIM-ONE datasets to enhance generalization. Our experiments show that\nminimal preprocessing yields higher AUC-ROC compared to more complex\nenhancements, and our model demonstrates strong discriminative performance on\nunseen datasets. The proposed pipeline offers a reproducible and scalable\napproach to early glaucoma detection, supporting its potential clinical\nutility.", "comment": "13 pages, 6 figures, prepared for course CSCI 5922 at University of\n  Colorado Boulder. Code available upon request, dataset taken from Kaggle", "pdf_url": "http://arxiv.org/pdf/2506.21770v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21770v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21630", "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "comment": "8 pages, 9 figures, 2025 IJCNN", "pdf_url": "http://arxiv.org/pdf/2506.21630v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21630v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.21826", "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "comment": "18 pages, accepted at ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.21826v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21826v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22058", "title": "Lost at the Beginning of Reasoning", "authors": ["Baohao Liao", "Xinyi Chen", "Sara Rajaee", "Yuhui Xu", "Christian Herold", "Anders Søgaard", "Maarten de Rijke", "Christof Monz"], "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs.", "comment": "9 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.22058v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22058v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21772", "title": "Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search", "authors": ["Noé Lallouet", "Tristan Cazenave", "Cyrille Enderli", "Stéphanie Gourdin"], "summary": "Recent research works establish deep neural networks as high performing tools\nfor radar target detection, especially on challenging environments (presence of\nclutter or interferences, multi-target scenarii...). However, the usually large\ncomputational complexity of these networks is one of the factors preventing\nthem from being widely implemented in embedded radar systems. We propose to\ninvestigate novel neural architecture search (NAS) methods, based on\nMonte-Carlo Tree Search (MCTS), for finding neural networks achieving the\nrequired detection performance and striving towards a lower computational\ncomplexity. We evaluate the searched architectures on endoclutter radar\nsignals, in order to compare their respective performance metrics and\ngeneralization properties. A novel network satisfying the required detection\nprobability while being significantly lighter than the expert-designed baseline\nis proposed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21772v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21772v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21635", "title": "AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing", "authors": ["Haiping Yang", "Huaxing Liu", "Wei Wu", "Zuohui Chen", "Ning Wu"], "summary": "Unmanned aerial vehicles (UAVs) are increasingly employed in diverse\napplications such as land surveying, material transport, and environmental\nmonitoring. Following missions like data collection or inspection, UAVs must\nland safely at docking stations for storage or recharging, which is an\nessential requirement for ensuring operational continuity. However, accurate\nlanding remains challenging due to factors like GPS signal interference. To\naddress this issue, we propose a deviation warning system for UAV landings,\npowered by a novel vision-based model called AeroLite-MDNet. This model\nintegrates a multiscale fusion module for robust cross-scale object detection\nand incorporates a segmentation branch for efficient orientation estimation. We\nintroduce a new evaluation metric, Average Warning Delay (AWD), to quantify the\nsystem's sensitivity to landing deviations. Furthermore, we contribute a new\ndataset, UAVLandData, which captures real-world landing deviation scenarios to\nsupport training and evaluation. Experimental results show that our system\nachieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\\%,\ndemonstrating its effectiveness in enhancing UAV landing reliability. Code will\nbe available at https://github.com/ITTTTTI/Maskyolo.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21635v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21635v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.21840", "title": "PARSI: Persian Authorship Recognition via Stylometric Integration", "authors": ["Kourosh Shahnazari", "Mohammadali Keshtparvar", "Seyed Moein Ayyoubzadeh"], "summary": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21840v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21840v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22062", "title": "MDC-R: The Minecraft Dialogue Corpus with Reference", "authors": ["Chris Madge", "Maris Camilleri", "Paloma Carretero Garcia", "Mladen Karan", "Juexi Shao", "Prashant Jayannavar", "Julian Hough", "Benjamin Roth", "Massimo Poesio"], "summary": "We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a\nnew language resource that supplements the original Minecraft Dialogue Corpus\n(MDC) with expert annotations of anaphoric and deictic reference. MDC's\ntask-orientated, multi-turn, situated dialogue in a dynamic environment has\nmotivated multiple annotation efforts, owing to the interesting linguistic\nphenomena that this setting gives rise to. We believe it can serve as a\nvaluable resource when annotated with reference, too. Here, we discuss our\nmethod of annotation and the resulting corpus, and provide both a quantitative\nand a qualitative analysis of the data. Furthermore, we carry out a short\nexperiment demonstrating the usefulness of our corpus for referring expression\ncomprehension.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22062v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22062v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21795", "title": "Offensive Language Detection on Social Media Using XLNet", "authors": ["Reem Alothman", "Hafida Benhidour", "Said Kerrache"], "summary": "The widespread use of text-based communication on social media-through chats,\ncomments, and microblogs-has improved user interaction but has also led to an\nincrease in offensive content, including hate speech, racism, and other forms\nof abuse. Due to the enormous volume of user-generated content, manual\nmoderation is impractical, which creates a need for automated systems that can\ndetect offensive language. Deep learning models, particularly those using\ntransfer learning, have demonstrated significant success in understanding\nnatural language through large-scale pretraining. In this study, we propose an\nautomatic offensive language detection model based on XLNet, a generalized\nautoregressive pretraining method, and compare its performance with BERT\n(Bidirectional Encoder Representations from Transformers), which is a widely\nused baseline in natural language processing (NLP). Both models are evaluated\nusing the Offensive Language Identification Dataset (OLID), a benchmark Twitter\ndataset that includes hierarchical annotations. Our experimental results show\nthat XLNet outperforms BERT in detecting offensive content and in categorizing\nthe types of offenses, while BERT performs slightly better in identifying the\ntargets of the offenses. Additionally, we find that oversampling and\nundersampling strategies are effective in addressing class imbalance and\nimproving classification performance. These findings highlight the potential of\ntransfer learning and XLNet-based architectures to create robust systems for\ndetecting offensive language on social media platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21795v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21795v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21655", "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21655v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21655v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21845", "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "authors": ["Zhuodi Cai"], "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "pdf_url": "http://arxiv.org/pdf/2506.21845v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21845v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22098", "title": "Involvement drives complexity of language in online debates", "authors": ["Eleonora Amadori", "Daniele Cirulli", "Edoardo Di Martino", "Jacopo Nudo", "Maria Sahakyan", "Emanuele Sangiorgio", "Arnaldo Santoro", "Simon Zollo", "Alessandro Galeazzi", "Niccolò Di Marco"], "summary": "Language is a fundamental aspect of human societies, continuously evolving in\nresponse to various stimuli, including societal changes and intercultural\ninteractions. Technological advancements have profoundly transformed\ncommunication, with social media emerging as a pivotal force that merges\nentertainment-driven content with complex social dynamics. As these platforms\nreshape public discourse, analyzing the linguistic features of user-generated\ncontent is essential to understanding their broader societal impact. In this\npaper, we examine the linguistic complexity of content produced by influential\nusers on Twitter across three globally significant and contested topics:\nCOVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of\ntextual complexity, we assess how language use varies along four key\ndimensions: account type, political leaning, content reliability, and\nsentiment. Our analysis reveals significant differences across all four axes,\nincluding variations in language complexity between individuals and\norganizations, between profiles with sided versus moderate political views, and\nbetween those associated with higher versus lower reliability scores.\nAdditionally, profiles producing more negative and offensive content tend to\nuse more complex language, with users sharing similar political stances and\nreliability levels converging toward a common jargon. Our findings offer new\ninsights into the sociolinguistic dynamics of digital platforms and contribute\nto a deeper understanding of how language reflects ideological and social\nstructures in online spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22098v1", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22098v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21802", "title": "Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction", "authors": ["Johan Hallberg Szabadváry", "Tuwe Löfström", "Ulf Johansson", "Cecilia Sönströd", "Ernst Ahlberg", "Lars Carlsson"], "summary": "Machine learning (ML) models always make a prediction, even when they are\nlikely to be wrong. This causes problems in practical applications, as we do\nnot know if we should trust a prediction. ML with reject option addresses this\nissue by abstaining from making a prediction if it is likely to be incorrect.\nIn this work, we formalise the approach to ML with reject option in binary\nclassification, deriving theoretical guarantees on the resulting error rate.\nThis is achieved through conformal prediction (CP), which produce prediction\nsets with distribution-free validity guarantees. In binary classification, CP\ncan output prediction sets containing exactly one, two or no labels. By\naccepting only the singleton predictions, we turn CP into a binary classifier\nwith reject option.\n  Here, CP is formally put in the framework of predicting with reject option.\nWe state and prove the resulting error rate, and give finite sample estimates.\nNumerical examples provide illustrations of derived error rate through several\ndifferent conformal prediction settings, ranging from full conformal prediction\nto offline batch inductive conformal prediction. The former has a direct link\nto sharp validity guarantees, whereas the latter is more fuzzy in terms of\nvalidity guarantees but can be used in practice. Error-reject curves illustrate\nthe trade-off between error rate and reject rate, and can serve to aid a user\nto set an acceptable error rate or reject rate in practice.", "comment": "20 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21802v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21802v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21680", "title": "PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors", "authors": ["Sai Sri Teja", "Sreevidya Chintalapati", "Vinayak Gupta", "Mukund Varma T", "Haejoon Lee", "Aswin Sankaranarayanan", "Kaushik Mitra"], "summary": "Advances in 3D reconstruction using neural rendering have enabled\nhigh-quality 3D capture. However, they often fail when the input imagery is\ncorrupted by motion blur, due to fast motion of the camera or the objects in\nthe scene. This work advances neural rendering techniques in such scenarios by\nusing single-photon avalanche diode (SPAD) arrays, an emerging sensing\ntechnology capable of sensing images at extremely high speeds. However, the use\nof SPADs presents its own set of unique challenges in the form of binary\nimages, that are driven by stochastic photon arrivals. To address this, we\nintroduce PhotonSplat, a framework designed to reconstruct 3D scenes directly\nfrom SPAD binary images, effectively navigating the noise vs. blur trade-off.\nOur approach incorporates a novel 3D spatial filtering technique to reduce\nnoise in the renderings. The framework also supports both no-reference using\ngenerative priors and reference-based colorization from a single blurry image,\nenabling downstream applications such as segmentation, object detection and\nappearance editing tasks. Additionally, we extend our method to incorporate\ndynamic scene representations, making it suitable for scenes with moving\nobjects. We further contribute PhotonScenes, a real-world multi-view dataset\ncaptured with the SPAD sensors.", "comment": "Accepted at the International Conference on Computational\n  Photography(ICCP) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21680v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21680v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21849", "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models", "authors": ["Quan Xiao", "Debarun Bhattacharjya", "Balaji Ganesan", "Radu Marinescu", "Katsiaryna Mirylenka", "Nhan H Pham", "Michael Glass", "Junkyu Lee"], "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.", "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21849v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21849v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22105", "title": "Identifying a Circuit for Verb Conjugation in GPT-2", "authors": ["David Demitri Africa"], "summary": "I implement a procedure to isolate and interpret the sub-network (or\n\"circuit\") responsible for subject-verb agreement in GPT-2 Small. In this\nstudy, the model is given prompts where the subject is either singular (e.g.\n\"Alice\") or plural (e.g. \"Alice and Bob\"), and the task is to correctly predict\nthe appropriate verb form (\"walks\" for singular subjects, \"walk\" for plural\nsubjects). Using a series of techniques-including performance verification\nautomatic circuit discovery via direct path patching, and direct logit\nattribution- I isolate a candidate circuit that contributes significantly to\nthe model's correct verb conjugation. The results suggest that only a small\nfraction of the network's component-token pairs is needed to achieve near-model\nperformance on the base task but substantially more for more complex settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22105v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22105v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21803", "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21803v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21803v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.21714", "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have\nbeen studied using the unified theoretical framework. Although such models can\ngenerate high-quality data points from a noise distribution, the sampling\ndemands multiple iterations to solve an ordinary differential equation (ODE)\nwith high computational complexity. Most existing methods focus on reducing the\nnumber of time steps during the sampling process to improve efficiency. In this\nwork, we explore a complementary direction in which the quality-complexity\ntradeoff can be dynamically controlled in terms of time steps and in the length\nof the neural network. We achieve this by rewiring the blocks in the\ntransformer-based architecture to solve an inner discretized ODE w.r.t. its\nlength. Then, we employ time- and length-wise consistency terms during flow\nmatching training, and as a result, the sampling can be performed with an\narbitrary number of time steps and transformer blocks. Unlike others, our\n$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in\ntime dimension and decreases both latency and memory usage. Compared to the\nprevious state of the art, image generation experiments on CelebA-HQ and\nImageNet show a latency reduction of up to $3\\times$ in the most efficient\nsampling mode, and a FID score improvement of up to $3.5$ points for\nhigh-quality sampling. We release our code and model weights with fully\nreproducible experiments.", "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "pdf_url": "http://arxiv.org/pdf/2506.21714v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21714v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21857", "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21857v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21857v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22141", "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level", "authors": ["Iliass Ayaou", "Denis Cavallucci", "Hicham Chibane"], "summary": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22141v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22141v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21813", "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "authors": ["Felix Holm", "Gözde Ünver", "Ghazal Ghazaei", "Nassir Navab"], "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21813v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21813v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21732", "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "authors": ["Ameya Salvi", "Venkat Krovi"], "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21732v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21732v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21862", "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "comment": "21 pages, 4 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2506.21862v1", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21862v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22143", "title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "summary": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively.", "comment": "Accepted for IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2506.22143v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22143v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21815", "title": "Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning", "authors": ["Augustine Twumasi", "Prokash Chandra Roy", "Zixun Li", "Soumya Shouvik Bhattacharjee", "Zhengtao Gan"], "summary": "Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing\ntechnology for producing intricate metal components with exceptional accuracy.\nA key challenge in L-PBF is the formation of complex microstructures affecting\nproduct quality. We propose a physics-guided, machine-learning approach to\noptimize scan paths for desired microstructure outcomes, such as equiaxed\ngrains. We utilized a phase-field method (PFM) to model crystalline grain\nstructure evolution. To reduce computational costs, we trained a surrogate\nmachine learning model, a 3D U-Net convolutional neural network, using\nsingle-track phase-field simulations with various laser powers to predict\ncrystalline grain orientations based on initial microstructure and thermal\nhistory. We investigated three scanning strategies across various hatch\nspacings within a square domain, achieving a two-orders-of-magnitude speedup\nusing the surrogate model. To reduce trial and error in designing laser scan\ntoolpaths, we used deep reinforcement learning (DRL) to generate optimized scan\npaths for target microstructure. Results from three cases demonstrate the DRL\napproach's effectiveness. We integrated the surrogate 3D U-Net model into our\nDRL environment to accelerate the reinforcement learning training process. The\nreward function minimizes both aspect ratio and grain volume of the predicted\nmicrostructure from the agent's scan path. The reinforcement learning algorithm\nwas benchmarked against conventional zigzag approach for smaller and larger\ndomains, showing machine learning methods' potential to enhance microstructure\ncontrol and computational efficiency in L-PBF optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21815v1", "categories": ["cs.CE", "cs.LG", "math.OC"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21815v1", "date": "2025-04-12", "updated": "2025-04-12"}
{"id": "2506.21748", "title": "Inverse Design of Diffractive Metasurfaces Using Diffusion Models", "authors": ["Liav Hen", "Erez Yosef", "Dan Raviv", "Raja Giryes", "Jacob Scheuer"], "summary": "Metasurfaces are ultra-thin optical elements composed of engineered\nsub-wavelength structures that enable precise control of light. Their inverse\ndesign - determining a geometry that yields a desired optical response - is\nchallenging due to the complex, nonlinear relationship between structure and\noptical properties. This often requires expert tuning, is prone to local\nminima, and involves significant computational overhead. In this work, we\naddress these challenges by integrating the generative capabilities of\ndiffusion models into computational design workflows. Using an RCWA simulator,\nwe generate training data consisting of metasurface geometries and their\ncorresponding far-field scattering patterns. We then train a conditional\ndiffusion model to predict meta-atom geometry and height from a target spatial\npower distribution at a specified wavelength, sampled from a continuous\nsupported band. Once trained, the model can generate metasurfaces with low\nerror, either directly using RCWA-guided posterior sampling or by serving as an\ninitializer for traditional optimization methods. We demonstrate our approach\non the design of a spatially uniform intensity splitter and a polarization beam\nsplitter, both produced with low error in under 30 minutes. To support further\nresearch in data-driven metasurface design, we publicly release our code and\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21748v1", "categories": ["physics.optics", "cs.CV", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.21748v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21864", "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE", "authors": ["Hang Shao", "Heting Gao", "Yunhang Shen", "Jiawei Chen", "Lijiang Li", "Zuwei Long", "Bo Tong", "Ke Li", "Xing Sun"], "summary": "Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2506.21864v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21864v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22157", "title": "Training Language Model to Critique for Better Refinement", "authors": ["Tianshu Yu", "Chao Xiang", "Mingchuan Yang", "Pei Ke", "Bosi Wen", "Cunxiang Wang", "Jiale Cheng", "Li Zhang", "Xinyu Mu", "Chuxiong Sun", "Minlie Huang"], "summary": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.", "comment": "Accepted to ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.22157v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22157v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21826", "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "comment": "18 pages, accepted at ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.21826v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21826v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21765", "title": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker", "authors": ["Qi Li", "Shaheer U. Saeed", "Yuliang Huang", "Mingyuan Luo", "Zhongnuo Yan", "Jiongquan Chen", "Xin Yang", "Dong Ni", "Nektarios Winter", "Phuc Nguyen", "Lucas Steinberger", "Caelan Haney", "Yuan Zhao", "Mingjie Jiang", "Bowen Ren", "SiYeoul Lee", "Seonho Kim", "MinKyung Seo", "MinWoo Kim", "Yimeng Dou", "Zhiwei Zhang", "Yin Li", "Tomy Varghese", "Dean C. Barratt", "Matthew J. Clarkson", "Tom Vercauteren", "Yipeng Hu"], "summary": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes\nfrom sequences of 2D ultrasound images without relying on external tracking\nsystems, offering a low-cost, portable, and widely deployable alternative for\nvolumetric imaging. However, it presents significant challenges, including\naccurate inter-frame motion estimation, minimisation of drift accumulation over\nlong sequences, and generalisability across scanning protocols. The TUS-REC2024\nChallenge was established to benchmark and accelerate progress in trackerless\n3D ultrasound reconstruction by providing a publicly available dataset for the\nfirst time, along with a baseline model and evaluation framework. The Challenge\nattracted over 43 registered teams, of which 6 teams submitted 21 valid\ndockerized solutions. Submitted methods spanned a wide range of algorithmic\napproaches, including recurrent models, registration-driven volume refinement,\nattention, and physics-informed models. This paper presents an overview of the\nChallenge design, summarises the key characteristics of the dataset, provides a\nconcise literature review, introduces the technical details of the underlying\nmethodology working with tracked freehand ultrasound data, and offers a\ncomparative analysis of submitted methods across multiple evaluation metrics.\nThe results highlight both the progress and current limitations of\nstate-of-the-art approaches in this domain, and inform directions for future\nresearch. The data, evaluation code, and baseline are publicly available to\nfacilitate ongoing development and reproducibility. As a live and evolving\nbenchmark, this Challenge is designed to be continuously developed and\nimproved. The Challenge was held at MICCAI 2024 and will be organised again at\nMICCAI 2025, reflecting its growing impact and the sustained commitment to\nadvancing this field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21765v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21765v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21872", "title": "A Survey of Continual Reinforcement Learning", "authors": ["Chaofan Pan", "Xin Yang", "Yanhua Li", "Wei Wei", "Tianrui Li", "Bo An", "Jiye Liang"], "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for\nsolving sequential decision-making problems. Recent years have witnessed\nremarkable progress in this field due to the rapid development of deep neural\nnetworks. However, the success of RL currently relies on extensive training\ndata and computational resources. In addition, RL's limited ability to\ngeneralize across tasks restricts its applicability in dynamic and real-world\nenvironments. With the arisen of Continual Learning (CL), Continual\nReinforcement Learning (CRL) has emerged as a promising research direction to\naddress these limitations by enabling agents to learn continuously, adapt to\nnew tasks, and retain previously acquired knowledge. In this survey, we provide\na comprehensive examination of CRL, focusing on its core concepts, challenges,\nand methodologies. Firstly, we conduct a detailed review of existing works,\norganizing and analyzing their metrics, tasks, benchmarks, and scenario\nsettings. Secondly, we propose a new taxonomy of CRL methods, categorizing them\ninto four types from the perspective of knowledge storage and/or transfer.\nFinally, our analysis highlights the unique challenges of CRL and provides\npractical insights into future directions.", "comment": "This work has been submitted to the IEEE TPAMI", "pdf_url": "http://arxiv.org/pdf/2506.21872v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21872v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22232", "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs", "authors": ["Patrick Haller", "Jannis Vamvas", "Rico Sennrich", "Lena A. Jäger"], "summary": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.22232v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22232v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21828", "title": "Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification", "authors": ["Weitao Tang", "Johann Vargas-Calixto", "Nasim Katebi", "Robert Galinsky", "Gari D. Clifford", "Faezeh Marzbanrad"], "summary": "Fetal sleep is a relatively underexplored yet vital aspect of prenatal\nneurodevelopment. Understanding fetal sleep patterns could provide insights\ninto early brain maturation and help clinicians detect signs of neurological\ncompromise that arise due to fetal hypoxia or fetal growth restriction. This\nreview synthesizes over eight decades of research on the physiological\ncharacteristics, ontogeny, and regulation of fetal sleep. We compare\nsleep-state patterns in humans and large animal models, highlighting\nspecies-specific differences and the presence of sleep-state analogs. We review\nboth invasive techniques in animals and non-invasive modalities in humans.\nComputational methods for sleep-state classification are also examined,\nincluding rule-based approaches (with and without clustering-based\npreprocessing) and state-of-the-art deep learning techniques. Finally, we\ndiscuss how intrauterine conditions such as hypoxia and fetal growth\nrestriction can disrupt fetal sleep. This review provides a comprehensive\nfoundation for the development of objective, multimodal, and non-invasive fetal\nsleep monitoring technologies to support early diagnosis and intervention in\nprenatal care.", "comment": "Review article, 17 pages, 1 figure, 5 tables, submitted to Sleep\n  (under review)", "pdf_url": "http://arxiv.org/pdf/2506.21828v1", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.21828v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21812", "title": "Towards Transparent AI: A Survey on Explainable Large Language Models", "authors": ["Avash Palikhe", "Zhenyu Yu", "Zichong Wang", "Wenbin Zhang"], "summary": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21812v1", "categories": ["cs.CL", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21812v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21873", "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning", "authors": ["Tzu-Chun Chien", "Chieh-Kai Lin", "Shiang-Feng Tsai", "Ruei-Chi Lai", "Hung-Jen Chen", "Min Sun"], "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21873v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21873v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22305", "title": "Detection of Personal Data in Structured Datasets Using a Large Language Model", "authors": ["Albert Agisha Ntwali", "Luca Rück", "Martin Heckmann"], "summary": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.22305v1", "categories": ["cs.CL", "I.5.4; I.2.7; H.3.1"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22305v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21842", "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses", "authors": ["Archisman Ghosh", "Satwik Kundu", "Swaroop Ghosh"], "summary": "Quantum Machine Learning (QML) integrates quantum computing with classical\nmachine learning, primarily to solve classification, regression and generative\ntasks. However, its rapid development raises critical security challenges in\nthe Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines\nadversarial threats unique to QML systems, focusing on vulnerabilities in\ncloud-based deployments, hybrid architectures, and quantum generative models.\nKey attack vectors include model stealing via transpilation or output\nextraction, data poisoning through quantum-specific perturbations, reverse\nengineering of proprietary variational quantum circuits, and backdoor attacks.\nAdversaries exploit noise-prone quantum hardware and insufficiently secured\nQML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,\nand functionality. Defense mechanisms leverage quantum properties to counter\nthese threats. Noise signatures from training hardware act as non-invasive\nwatermarks, while hardware-aware obfuscation techniques and ensemble strategies\ndisrupt cloning attempts. Emerging solutions also adapt classical adversarial\ntraining and differential privacy to quantum settings, addressing\nvulnerabilities in quantum neural networks and generative architectures.\nHowever, securing QML requires addressing open challenges such as balancing\nnoise levels for reliability and security, mitigating cross-platform attacks,\nand developing quantum-classical trust frameworks. This chapter summarizes\nrecent advances in attacks and defenses, offering a roadmap for researchers and\npractitioners to build robust, trustworthy QML systems resilient to evolving\nadversarial landscapes.", "comment": "23 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21842v1", "categories": ["quant-ph", "cs.CR", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21842v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21860", "title": "Embodied Domain Adaptation for Object Detection", "authors": ["Xiangyu Shi", "Yanyuan Qiao", "Lingqiao Liu", "Feras Dayoub"], "summary": "Mobile robots rely on object detectors for perception and object localization\nin indoor environments. However, standard closed-set methods struggle to handle\nthe diverse objects and dynamic conditions encountered in real homes and labs.\nOpen-vocabulary object detection (OVOD), driven by Vision Language Models\n(VLMs), extends beyond fixed labels but still struggles with domain shifts in\nindoor environments. We introduce a Source-Free Domain Adaptation (SFDA)\napproach that adapts a pre-trained model without accessing source data. We\nrefine pseudo labels via temporal clustering, employ multi-scale threshold\nfusion, and apply a Mean Teacher framework with contrastive learning. Our\nEmbodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates\nadaptation under sequential changes in lighting, layout, and object diversity.\nOur experiments show significant gains in zero-shot detection performance and\nflexible adaptation to dynamic indoor conditions.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21860v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21860v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21874", "title": "On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling", "authors": ["Stanley Wu", "Ronik Bhaskar", "Anna Yoo Jeong Ha", "Shawn Shan", "Haitao Zheng", "Ben Y. Zhao"], "summary": "Today's text-to-image generative models are trained on millions of images\nsourced from the Internet, each paired with a detailed caption produced by\nVision-Language Models (VLMs). This part of the training pipeline is critical\nfor supplying the models with large volumes of high-quality image-caption pairs\nduring training. However, recent work suggests that VLMs are vulnerable to\nstealthy adversarial attacks, where adversarial perturbations are added to\nimages to mislead the VLMs into producing incorrect captions.\n  In this paper, we explore the feasibility of adversarial mislabeling attacks\non VLMs as a mechanism to poisoning training pipelines for text-to-image\nmodels. Our experiments demonstrate that VLMs are highly vulnerable to\nadversarial perturbations, allowing attackers to produce benign-looking images\nthat are consistently miscaptioned by the VLM models. This has the effect of\ninjecting strong \"dirty-label\" poison samples into the training pipeline for\ntext-to-image models, successfully altering their behavior with a small number\nof poisoned samples. We find that while potential defenses can be effective,\nthey can be targeted and circumvented by adaptive attackers. This suggests a\ncat-and-mouse game that is likely to reduce the quality of training data and\nincrease the cost of text-to-image model development. Finally, we demonstrate\nthe real-world effectiveness of these attacks, achieving high attack success\n(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex\nAI and Microsoft Azure).", "comment": "ACM Conference on Computer and Communications Security 2025", "pdf_url": "http://arxiv.org/pdf/2506.21874v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21874v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22316", "title": "Evaluating Scoring Bias in LLM-as-a-Judge", "authors": ["Qingquan Li", "Shaoyu Dou", "Kailai Shao", "Chao Chen", "Haixiang Hu"], "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22316v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22316v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21849", "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models", "authors": ["Quan Xiao", "Debarun Bhattacharjya", "Balaji Ganesan", "Radu Marinescu", "Katsiaryna Mirylenka", "Nhan H Pham", "Michael Glass", "Junkyu Lee"], "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.", "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "pdf_url": "http://arxiv.org/pdf/2506.21849v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21849v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21876", "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation", "authors": ["Qiyue Gao", "Xinyu Pi", "Kevin Liu", "Junrong Chen", "Ruolan Yang", "Xinqi Huang", "Xinyu Fang", "Lu Sun", "Gautham Kishore", "Bo Ai", "Stone Tao", "Mengyang Liu", "Jiaxi Yang", "Chao-Jung Lai", "Chuanyang Jin", "Jiannan Xiang", "Benhao Huang", "Zeming Chen", "David Danks", "Hao Su", "Tianmin Shu", "Ziqiao Ma", "Lianhui Qin", "Zhiting Hu"], "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.", "comment": "ACL 2025 (Findings)", "pdf_url": "http://arxiv.org/pdf/2506.21876v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21876v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21876", "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation", "authors": ["Qiyue Gao", "Xinyu Pi", "Kevin Liu", "Junrong Chen", "Ruolan Yang", "Xinqi Huang", "Xinyu Fang", "Lu Sun", "Gautham Kishore", "Bo Ai", "Stone Tao", "Mengyang Liu", "Jiaxi Yang", "Chao-Jung Lai", "Chuanyang Jin", "Jiannan Xiang", "Benhao Huang", "Zeming Chen", "David Danks", "Hao Su", "Tianmin Shu", "Ziqiao Ma", "Lianhui Qin", "Zhiting Hu"], "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.", "comment": "ACL 2025 (Findings)", "pdf_url": "http://arxiv.org/pdf/2506.21876v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21876v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22366", "title": "Why Are Parsing Actions for Understanding Message Hierarchies Not Random?", "authors": ["Daichi Kato", "Ryo Ueda", "Yusuke Miyao"], "summary": "If humans understood language by randomly selecting parsing actions, it might\nhave been necessary to construct a robust symbolic system capable of being\ninterpreted under any hierarchical structure. However, human parsing strategies\ndo not seem to follow such a random pattern. Why is that the case? In fact, a\nprevious study on emergent communication using models with hierarchical biases\nhave reported that agents adopting random parsing\nstrategies$\\unicode{x2013}$ones that deviate significantly from human language\ncomprehension$\\unicode{x2013}$can achieve high communication accuracy. In this\nstudy, we investigate this issue by making two simple and natural modifications\nto the experimental setup: (I) we use more complex inputs that have\nhierarchical structures, such that random parsing makes semantic interpretation\nmore difficult, and (II) we incorporate a surprisal-related term, which is\nknown to influence the order of words and characters in natural language, into\nthe objective function. With these changes, we evaluate whether agents\nemploying random parsing strategies still maintain high communication accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22366v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22366v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21857", "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21857v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21857v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21880", "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer", "authors": ["Yuansheng Li", "Yunhao Zou", "Linwei Chen", "Ying Fu"], "summary": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for\nlarge-scale remote sensing tasks due to its advantages in flux and spectral\nresolution. However, IHI is susceptible to complex errors arising from imaging\nsteps, and its quality is limited by existing signal processing-based\nreconstruction algorithms. Two key challenges hinder performance enhancement:\n1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific\ndegradation components through learning-based methods. To address these\nchallenges, we propose a novel IHI reconstruction pipeline. First, based on\nimaging physics and radiometric calibration data, we establish a simplified yet\naccurate IHI degradation model and a parameter estimation method. This model\nenables the synthesis of realistic IHI training datasets from hyperspectral\nimages (HSIs), bridging the gap between IHI reconstruction and deep learning.\nSecond, we design the Interferometric Hyperspectral Reconstruction Unfolding\nTransformer (IHRUT), which achieves effective spectral correction and detail\nrestoration through a stripe-pattern enhancement mechanism and a\nspatial-spectral transformer architecture. Experimental results demonstrate the\nsuperior performance and generalization capability of our method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21880v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21880v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21884", "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "comment": "Paper accepted at ICCV 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2506.21884v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21884v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22396", "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization", "authors": ["Danush Khanna", "Aditya Kumar Guru", "Srivarshinee Sridhar", "Zidan Ahmed", "Rubhav Bahirwani", "Meetu Malhotra", "Vinija Jain", "Aman Chadha", "Amitava Das", "Kripabandhu Ghosh"], "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).", "comment": "Preprint. Under submission", "pdf_url": "http://arxiv.org/pdf/2506.22396v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22396v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21884", "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "comment": "Paper accepted at ICCV 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2506.21884v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21884v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21884", "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "comment": "Paper accepted at ICCV 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2506.21884v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21884v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21892", "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation", "authors": ["Adam Goodge", "Xun Xu", "Bryan Hooi", "Wee Siong Ng", "Jingyi Liao", "Yongyi Su", "Xulei Yang"], "summary": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21892v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21892v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22402", "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach", "authors": ["Petr Pechman", "Milan Straka", "Jana Straková", "Jakub Náplava"], "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec.", "comment": "Accepted to TSD 2025", "pdf_url": "http://arxiv.org/pdf/2506.22402v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22402v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21887", "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds", "authors": ["Edward Chen", "Sang T. Truong", "Natalie Dullerud", "Sanmi Koyejo", "Carlos Guestrin"], "summary": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21887v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21887v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21934", "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design", "authors": ["Najmeh Forouzandehmehr", "Reza Yousefi Maragheh", "Sriram Kollipara", "Kai Zhao", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21934v1", "categories": ["cs.IR", "cs.CV", "I.3.3; I.2.11; H.5.2"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21934v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21931", "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21931v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21931v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22403", "title": "HyperCLOVA X THINK Technical Report", "authors": ["NAVER Cloud HyperCLOVA X Team"], "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.", "comment": "49 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.22403v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22403v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21894", "title": "Thompson Sampling in Function Spaces via Neural Operators", "authors": ["Rafael Oliveira", "Xuesong Wang", "Kian Ming A. Chai", "Edwin V. Bonilla"], "summary": "We propose an extension of Thompson sampling to optimization problems over\nfunction spaces where the objective is a known functional of an unknown\noperator's output. We assume that functional evaluations are inexpensive, while\nqueries to the operator (such as running a high-fidelity simulator) are costly.\nOur algorithm employs a sample-then-optimize approach using neural operator\nsurrogates. This strategy avoids explicit uncertainty quantification by\ntreating trained neural operators as approximate samples from a Gaussian\nprocess. We provide novel theoretical convergence guarantees, based on Gaussian\nprocesses in the infinite-dimensional setting, under minimal assumptions. We\nbenchmark our method against existing baselines on functional optimization\ntasks involving partial differential equations and other nonlinear\noperator-driven phenomena, demonstrating improved sample efficiency and\ncompetitive performance.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.21894v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21894v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21976", "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21976v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21976v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21945", "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images", "authors": ["Naftaly Wambugu", "Ruisheng Wang", "Bo Guo", "Tianshu Yu", "Sheng Xu", "Mohammed Elhassan"], "summary": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21945v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21945v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22405", "title": "Sequential Diagnosis with Language Models", "authors": ["Harsha Nori", "Mayank Daswani", "Christopher Kelly", "Scott Lundberg", "Marco Tulio Ribeiro", "Marc Wilson", "Xiaoxuan Liu", "Viknesh Sounderajah", "Jonathan Carlson", "Matthew P Lungren", "Bay Gross", "Peter Hames", "Mustafa Suleyman", "Dominic King", "Eric Horvitz"], "summary": "Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care.", "comment": "23 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.22405v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22405v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21933", "title": "Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion", "authors": ["Yifan Xue", "Ruihuai Liang", "Bo Yang", "Xuelin Cao", "Zhiwen Yu", "Mérouane Debbah", "Chau Yuen"], "summary": "With the rapid development of the low-altitude economy, air-ground integrated\nmulti-access edge computing (MEC) systems are facing increasing demands for\nreal-time and intelligent task scheduling. In such systems, task offloading and\nresource allocation encounter multiple challenges, including node\nheterogeneity, unstable communication links, and dynamic task variations. To\naddress these issues, this paper constructs a three-layer heterogeneous MEC\nsystem architecture for low-altitude economic networks, encompassing aerial and\nground users as well as edge servers. The system is systematically modeled from\nthe perspectives of communication channels, computational costs, and constraint\nconditions, and the joint optimization problem of offloading decisions and\nresource allocation is uniformly abstracted into a graph-structured modeling\ntask. On this basis, we propose a graph attention diffusion-based solution\ngenerator (GADSG). This method integrates the contextual awareness of graph\nattention networks with the solution distribution learning capability of\ndiffusion models, enabling joint modeling and optimization of discrete\noffloading variables and continuous resource allocation variables within a\nhigh-dimensional latent space. We construct multiple simulation datasets with\nvarying scales and topologies. Extensive experiments demonstrate that the\nproposed GADSG model significantly outperforms existing baseline methods in\nterms of optimization performance, robustness, and generalization across task\nstructures, showing strong potential for efficient task scheduling in dynamic\nand complex low-altitude economic network environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21933v1", "categories": ["cs.NI", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.21933v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21977", "title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression", "authors": ["Tianyu Zhang", "Xin Luo", "Li Li", "Dong Liu"], "summary": "Diffusion-based image compression has shown remarkable potential for\nachieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high\nrealism, by leveraging the generative priors of large pre-trained text-to-image\ndiffusion models. However, current approaches require a large number of\ndenoising steps at the decoder to generate realistic results under extreme\nbitrate constraints, limiting their application in real-time compression\nscenarios. Additionally, these methods often sacrifice reconstruction fidelity,\nas diffusion models typically fail to guarantee pixel-level consistency. To\naddress these challenges, we introduce StableCodec, which enables one-step\ndiffusion for high-fidelity and high-realism extreme image compression with\nimproved coding efficiency. To achieve ultra-low bitrates, we first develop an\nefficient Deep Compression Latent Codec to transmit a noisy latent\nrepresentation for a single-step denoising process. We then propose a\nDual-Branch Coding Structure, consisting of a pair of auxiliary encoder and\ndecoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end\noptimization with joint bitrate and pixel-level constraints. Extensive\nexperiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that\nStableCodec outperforms existing methods in terms of FID, KID and DISTS by a\nsignificant margin, even at bitrates as low as 0.005 bits per pixel, while\nmaintaining strong fidelity. Additionally, StableCodec achieves inference\nspeeds comparable to mainstream transform coding schemes. All source code are\navailable at https://github.com/LuizScarlet/StableCodec.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21977v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21977v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21964", "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics", "authors": ["Michael A. Riegler", "Kristoffer Herland Hellton", "Vajira Thambawita", "Hugo L. Hammer"], "summary": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21964v1", "categories": ["stat.ME", "cs.AI", "cs.CL"], "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.21964v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21656", "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.", "comment": "29 pages", "pdf_url": "http://arxiv.org/pdf/2506.21656v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21656v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21946", "title": "Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling", "authors": ["Till Wenke"], "summary": "Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded\nsystematic study due to its informal nature. This paper presents and analyzes\nthe largest known structured dataset of hitchhiking rides, comprising over\n63,000 entries collected over nearly two decades through platforms associated\nwith hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced\ncontributions, the dataset captures key spatiotemporal and strategic aspects of\nhitchhiking. This work documents the dataset's origins, evolution, and\ncommunity-driven maintenance, highlighting its Europe-centric distribution,\nseasonal patterns, and reliance on a small number of highly active\ncontributors. Through exploratory analyses, I examine waiting times, user\nbehavior, and comment metadata, shedding light on the lived realities of\nhitchhikers. While the dataset has inherent biases and limitations - such as\ndemographic skew and unverifiable entries it offers a rare and valuable window\ninto an alternative form of mobility. I conclude by outlining future directions\nfor enriching the dataset and advancing research on hitchhiking as both a\ntransportation practice and cultural phenomenon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21946v1", "categories": ["cs.CY", "cs.LG"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21946v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22012", "title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction", "authors": ["Qi Gao", "Zhihao Chen", "Dong Zeng", "Junping Zhang", "Jianhua Ma", "Hongming Shan"], "summary": "The generalization of deep learning-based low-dose computed tomography (CT)\nreconstruction models to doses unseen in the training data is important and\nremains challenging. Previous efforts heavily rely on paired data to improve\nthe generalization performance and robustness through collecting either diverse\nCT data for re-training or a few test data for fine-tuning. Recently, diffusion\nmodels have shown promising and generalizable performance in low-dose CT (LDCT)\nreconstruction, however, they may produce unrealistic structures due to the CT\nimage noise deviating from Gaussian distribution and imprecise prior\ninformation from the guidance of noisy LDCT images. In this paper, we propose a\nnoise-inspired diffusion model for generalizable LDCT reconstruction, termed\nNEED, which tailors diffusion models for noise characteristics of each domain.\nFirst, we propose a novel shifted Poisson diffusion model to denoise projection\ndata, which aligns the diffusion process with the noise model in pre-log LDCT\nprojections. Second, we devise a doubly guided diffusion model to refine\nreconstructed images, which leverages LDCT images and initial reconstructions\nto more accurately locate prior information and enhance reconstruction\nfidelity. By cascading these two diffusion models for dual-domain\nreconstruction, our NEED requires only normal-dose data for training and can be\neffectively extended to various unseen dose levels during testing via a time\nstep matching strategy. Extensive qualitative, quantitative, and\nsegmentation-based evaluations on two datasets demonstrate that our NEED\nconsistently outperforms state-of-the-art methods in reconstruction and\ngeneralization performance. Source code is made available at\nhttps://github.com/qgao21/NEED.", "comment": "Accepted for publication in Medical Image Analysis, 2025", "pdf_url": "http://arxiv.org/pdf/2506.22012v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22012v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21972", "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21972v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21972v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21805", "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21805v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21805v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21967", "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21967v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21967v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22041", "title": "Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning", "authors": ["Julia Machnio", "Sebastian Nørgaard Llambias", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "summary": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions.", "comment": "2nd Sorbonne-Heidelberg Workshop on AI in medicine: Machine Learning\n  for multi-modal data", "pdf_url": "http://arxiv.org/pdf/2506.22041v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22041v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21976", "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21976v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21976v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21825", "title": "Exploring the change in scientific readability following the release of ChatGPT", "authors": ["Abdulkareem Alsudais"], "summary": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21825v1", "categories": ["cs.CY", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.21825v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.21972", "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21972v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21972v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22116", "title": "Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration", "authors": ["Noora Sassali", "Roel Pieters"], "summary": "Pointing gestures are a common interaction method used in Human-Robot\nCollaboration for various tasks, ranging from selecting targets to guiding\nindustrial processes. This study introduces a method for localizing pointed\ntargets within a planar workspace. The approach employs pose estimation, and a\nsimple geometric model based on shoulder-wrist extension to extract gesturing\ndata from an RGB-D stream. The study proposes a rigorous methodology and\ncomprehensive analysis for evaluating pointing gestures and target selection in\ntypical robotic tasks. In addition to evaluating tool accuracy, the tool is\nintegrated into a proof-of-concept robotic system, which includes object\ndetection, speech transcription, and speech synthesis to demonstrate the\nintegration of multiple modalities in a collaborative application. Finally, a\ndiscussion over tool limitations and performance is provided to understand its\nrole in multimodal robotic systems. All developments are available at:\nhttps://github.com/NMKsas/gesture_pointer.git.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). Preprint", "pdf_url": "http://arxiv.org/pdf/2506.22116v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22116v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21990", "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "pdf_url": "http://arxiv.org/pdf/2506.21990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21990v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21839", "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21839v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21839v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21990", "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "pdf_url": "http://arxiv.org/pdf/2506.21990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21990v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22156", "title": "Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction", "authors": ["Mattia Ricchi", "Fabrizio Alfonsi", "Camilla Marella", "Marco Barbieri", "Alessandra Retico", "Leonardo Brizi", "Alessandro Gabrielli", "Claudia Testa"], "summary": "Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging\ntechnique that provides multi-parametric maps with a single acquisition. Neural\nNetworks (NNs) accelerate reconstruction but require significant resources for\ntraining. We propose an FPGA-based NN for real-time brain parameter\nreconstruction from MRF data. Training the NN takes an estimated 200 seconds,\nsignificantly faster than standard CPU-based training, which can be up to 250\ntimes slower. This method could enable real-time brain analysis on mobile\ndevices, revolutionizing clinical decision-making and telemedicine.", "comment": "8 pages, 2 figures, to be published in conference proceedings of SDPS\n  2024: 2024 International Conference of the Society for Design and Process\n  Science on Advances and Challenges of Applying AI/GenAI in Design and Process\n  Science", "pdf_url": "http://arxiv.org/pdf/2506.22156v1", "categories": ["cs.AR", "cs.CV", "physics.ins-det"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.22156v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21997", "title": "Binned semiparametric Bayesian networks", "authors": ["Rafael Sojo", "Javier Díaz-Rozo", "Concha Bielza", "Pedro Larrañaga"], "summary": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21997v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.1; G.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21997v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21845", "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "authors": ["Zhuodi Cai"], "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "pdf_url": "http://arxiv.org/pdf/2506.21845v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21845v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22101", "title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "authors": ["Hyeongji Kim", "Stine Hansen", "Michael Kampffmeyer"], "summary": "Common prototype-based medical image few-shot segmentation (FSS) methods\nmodel foreground and background classes using class-specific prototypes.\nHowever, given the high variability of the background, a more promising\ndirection is to focus solely on foreground modeling, treating the background as\nan anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key\nlimitations: dependence on a single prototype per class, a focus on binary\nclassification, and fixed thresholds that fail to adapt to patient and organ\nvariability. To address these shortcomings, we propose the Tied Prototype Model\n(TPM), a principled reformulation of ADNet with tied prototype locations for\nforeground and background distributions. Building on its probabilistic\nfoundation, TPM naturally extends to multiple prototypes and multi-class\nsegmentation while effectively separating non-typical background features.\nNotably, both extensions lead to improved segmentation accuracy. Finally, we\nleverage naturally occurring class priors to define an ideal target for\nadaptive thresholds, boosting segmentation performance. Taken together, TPM\nprovides a fresh perspective on prototype-based FSS for medical image\nsegmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.", "comment": "Submitted version (MICCAI). Accepted at MICCAI 2025. The code repo\n  will be made publicly available soon", "pdf_url": "http://arxiv.org/pdf/2506.22101v1", "categories": ["cs.CV", "cs.LG", "stat.ML"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22101v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22176", "title": "KnotDLO: Toward Interpretable Knot Tying", "authors": ["Holly Dinkel", "Raghavendra Navaratna", "Jingyi Xiang", "Brian Coltin", "Trey Smith", "Timothy Bretl"], "summary": "This work presents KnotDLO, a method for one-handed Deformable Linear Object\n(DLO) knot tying that is robust to occlusion, repeatable for varying rope\ninitial configurations, interpretable for generating motion policies, and\nrequires no human demonstrations or training. Grasp and target waypoints for\nfuture DLO states are planned from the current DLO shape. Grasp poses are\ncomputed from indexing the tracked piecewise linear curve representing the DLO\nstate based on the current curve shape and are piecewise continuous. KnotDLO\ncomputes intermediate waypoints from the geometry of the current DLO state and\nthe desired next state. The system decouples visual reasoning from control. In\n16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an\noverhand knot from previously unseen configurations.", "comment": "4 pages, 5 figures, presented at the Workshop on 3D Visual\n  Representations for Manipulation at the 2023 IEEE International Conference on\n  Robotics and Automation in Yokohama, Japan. Video presentation\n  [https://youtu.be/mg30uCUtpOk]. Poster\n  [https://hollydinkel.github.io/assets/pdf/ICRA20243DVRM_poster.pdf] 3DVRM\n  Workshop [https://3d-manipulation-workshop.github.io/]", "pdf_url": "http://arxiv.org/pdf/2506.22176v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22176v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22008", "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Konrad Tollmar", "Andrew D. Bagdanov", "Linus Gisslén"], "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.", "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22008v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22008v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21865", "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture", "authors": ["Haofeng Wang", "Yilin Guo", "Zehao Li", "Tong Yue", "Yizong Wang", "Enci Zhang", "Rongqun Lin", "Feng Gao", "Shiqi Wang", "Siwei Ma"], "summary": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights.", "comment": "IEEE International Conference on Multimedia and Expo Workshop,\n  2025.(Accepted)", "pdf_url": "http://arxiv.org/pdf/2506.21865v1", "categories": ["cs.MM", "cs.CL"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.21865v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22105", "title": "Identifying a Circuit for Verb Conjugation in GPT-2", "authors": ["David Demitri Africa"], "summary": "I implement a procedure to isolate and interpret the sub-network (or\n\"circuit\") responsible for subject-verb agreement in GPT-2 Small. In this\nstudy, the model is given prompts where the subject is either singular (e.g.\n\"Alice\") or plural (e.g. \"Alice and Bob\"), and the task is to correctly predict\nthe appropriate verb form (\"walks\" for singular subjects, \"walk\" for plural\nsubjects). Using a series of techniques-including performance verification\nautomatic circuit discovery via direct path patching, and direct logit\nattribution- I isolate a candidate circuit that contributes significantly to\nthe model's correct verb conjugation. The results suggest that only a small\nfraction of the network's component-token pairs is needed to achieve near-model\nperformance on the base task but substantially more for more complex settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22105v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22105v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22222", "title": "Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections", "authors": ["Hao Xu", "Ruth Lim", "Brian E. Chapman"], "summary": "Purpose: Aortic dissections are life-threatening cardiovascular conditions\nrequiring accurate segmentation of true lumen (TL), false lumen (FL), and false\nlumen thrombosis (FLT) from CTA images for effective management. Manual\nsegmentation is time-consuming and variable, necessitating automated solutions.\nMaterials and Methods: We developed four deep learning-based pipelines for Type\nB aortic dissection segmentation: a single-step model, a sequential model, a\nsequential multi-task model, and an ensemble model, utilizing 3D U-Net and\nSwin-UnetR architectures. A dataset of 100 retrospective CTA images was split\ninto training (n=80), validation (n=10), and testing (n=10). Performance was\nassessed using the Dice Coefficient and Hausdorff Distance. Results: Our\napproach achieved superior segmentation accuracy, with Dice Coefficients of\n0.91 $\\pm$ 0.07 for TL, 0.88 $\\pm$ 0.18 for FL, and 0.47 $\\pm$ 0.25 for FLT,\noutperforming Yao et al. (1), who reported 0.78 $\\pm$ 0.20, 0.68 $\\pm$ 0.18,\nand 0.25 $\\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide\naccurate segmentation of TBAD features, enabling derivation of morphological\nparameters for surveillance and treatment planning", "comment": "9 pages, 5 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.22222v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22222v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22026", "title": "Literature-Grounded Novelty Assessment of Scientific Ideas", "authors": ["Simra Shahid", "Marissa Radensky", "Raymond Fok", "Pao Siangliulue", "Daniel S. Weld", "Tom Hope"], "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22026v1", "categories": ["cs.IR", "cs.AI", "I.2; H.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22026v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21913", "title": "HyReC: Exploring Hybrid-based Retriever for Chinese", "authors": ["Zunran Wang", "Zheng Shenpeng", "Wang Shenglan", "Minghui Zhao", "Zhonghua Li"], "summary": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21913v1", "categories": ["cs.IR", "cs.CL"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21913v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22146", "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "authors": ["Amirmohammad Izadi", "Mohammad Ali Banayeeanzade", "Fatemeh Askari", "Ali Rahimiakbar", "Mohammad Mahdi Vahedi", "Hosein Hasani", "Mahdieh Soleymani Baghshah"], "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22146v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22146v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22226", "title": "Cardiovascular disease classification using radiomics and geometric features from cardiac CT", "authors": ["Ajay Mittal", "Raghav Mehta", "Omar Todd", "Philipp Seeböck", "Georg Langs", "Ben Glocker"], "summary": "Automatic detection and classification of Cardiovascular disease (CVD) from\nComputed Tomography (CT) images play an important part in facilitating\nbetter-informed clinical decisions. However, most of the recent deep learning\nbased methods either directly work on raw CT data or utilize it in pair with\nanatomical cardiac structure segmentation by training an end-to-end classifier.\nAs such, these approaches become much more difficult to interpret from a\nclinical perspective. To address this challenge, in this work, we break down\nthe CVD classification pipeline into three components: (i) image segmentation,\n(ii) image registration, and (iii) downstream CVD classification. Specifically,\nwe utilize the Atlas-ISTN framework and recent segmentation foundational models\nto generate anatomical structure segmentation and a normative healthy atlas.\nThese are further utilized to extract clinically interpretable radiomic\nfeatures as well as deformation field based geometric features (through atlas\nregistration) for CVD classification. Our experiments on the publicly available\nASOCA dataset show that utilizing these features leads to better CVD\nclassification accuracy (87.50\\%) when compared against classification model\ntrained directly on raw CT images (67.50\\%). Our code is publicly available:\nhttps://github.com/biomedia-mira/grc-net", "comment": "Under Review at STACOM 2025 with MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22226v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22226v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22039", "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22039v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21931", "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21931v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21931v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22174", "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research", "authors": ["Bavo Lesy", "Siemen Herremans", "Robin Kerstens", "Jan Steckel", "Walter Daems", "Siegfried Mercelis", "Ali Anwar"], "summary": "The transport industry has recently shown significant interest in unmanned\nsurface vehicles (USVs), specifically for port and inland waterway transport.\nThese systems can improve operational efficiency and safety, which is\nespecially relevant in the European Union, where initiatives such as the Green\nDeal are driving a shift towards increased use of inland waterways. At the same\ntime, a shortage of qualified personnel is accelerating the adoption of\nautonomous solutions. However, there is a notable lack of open-source,\nhigh-fidelity simulation frameworks and datasets for developing and evaluating\nsuch solutions. To address these challenges, we introduce AirSim For Surface\nVehicles (ASVSim), an open-source simulation framework specifically designed\nfor autonomous shipping research in inland and port environments. The framework\ncombines simulated vessel dynamics with marine sensor simulation capabilities,\nincluding radar and camera systems and supports the generation of synthetic\ndatasets for training computer vision models and reinforcement learning agents.\nBuilt upon Cosys-AirSim, ASVSim provides a comprehensive platform for\ndeveloping autonomous navigation algorithms and generating synthetic datasets.\nThe simulator supports research of both traditional control methods and deep\nlearning-based approaches. Through limited experiments, we demonstrate the\npotential of the simulator in these research areas. ASVSim is provided as an\nopen-source project under the MIT license, making autonomous navigation\nresearch accessible to a larger part of the ocean engineering community.", "comment": "14 Pages, 11 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22174v1", "categories": ["cs.RO", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22174v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22280", "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "authors": ["Yuliang Huang", "Imraj Singh", "Thomas Joyce", "Kris Thielemans", "Jamie R. McClelland"], "summary": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion\nartifacts due to breathing. A common clinical approach mitigates this by\nsorting projections into respiratory phases and reconstructing images per\nphase, but this does not account for breathing variability. Dynamic CBCT\ninstead reconstructs images at each projection, capturing continuous motion\nwithout phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)\noffer powerful tools for modeling dynamic scenes, yet their application to\ndynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,\nuse implicit motion representations, which are computationally expensive. While\nexplicit low-rank motion models have been proposed, they lack spatial\nregularization, leading to inconsistencies in Gaussian motion. To address these\nlimitations, we introduce a free-form deformation (FFD)-based spatial basis\nfunction and a deformation-informed framework that enforces consistency by\ncoupling the temporal evolution of Gaussian's mean position, scale, and\nrotation under a unified deformation field. We evaluate our approach on six\nCBCT datasets, demonstrating superior image quality with a 6x speedup over\nHexPlane. These results highlight the potential of deformation-informed 4DGS\nfor efficient, motion-compensated CBCT reconstruction. The code is available at\nhttps://github.com/Yuliang-Huang/DIGS.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22280v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22280v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22084", "title": "Transformers are Graph Neural Networks", "authors": ["Chaitanya K. Joshi"], "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.", "comment": "This paper is a technical version of an article in The Gradient at\n  https://thegradient.pub/transformers-are-graph-neural-networks/", "pdf_url": "http://arxiv.org/pdf/2506.22084v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22084v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.21964", "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics", "authors": ["Michael A. Riegler", "Kristoffer Herland Hellton", "Vajira Thambawita", "Hugo L. Hammer"], "summary": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21964v1", "categories": ["stat.ME", "cs.AI", "cs.CL"], "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.21964v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22204", "title": "Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport", "authors": ["Gurjeet Sangra Singh", "Maciej Falkiewicz", "Alexandros Kalousis"], "summary": "Physics phenomena are often described by ordinary and/or partial differential\nequations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,\nmany real-world systems are described only approximately with missing or\nunknown terms in the equations. This makes the distribution of the physics\nmodel differ from the true data-generating process (DGP). Using limited and\nunpaired data between DGP observations and the imperfect model simulations, we\ninvestigate this particular setting by completing the known-physics model,\ncombining theory-driven models and data-driven to describe the shifted\ndistribution involved in the DGP. We present a novel hybrid generative model\napproach combining deep grey-box modelling with Optimal Transport (OT) methods\nto enhance incomplete physics models. Our method implements OT maps in data\nspace while maintaining minimal source distribution distortion, demonstrating\nsuperior performance in resolving the unpaired problem and ensuring correct\nusage of physics parameters. Unlike black-box alternatives, our approach\nleverages physics-based inductive biases to accurately learn system dynamics\nwhile preserving interpretability through its domain knowledge foundation.\nExperimental results validate our method's effectiveness in both generation\ntasks and model transparency, offering detailed insights into learned physics\ndynamics.", "comment": "Workshop paper at ICLR 2025 (XAI4Science Workshop)", "pdf_url": "http://arxiv.org/pdf/2506.22204v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22204v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22304", "title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "authors": ["Erkan Turan", "Aristotelis Siozopoulos", "Maks Ovsjanikov"], "summary": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22304v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22304v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22095", "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Balázs Kulcsár"], "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).", "comment": "18 pages, 5 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22095v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22095v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22023", "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.", "comment": "17 pages, 8 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.22023v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22023v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22228", "title": "Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings", "authors": ["Rong Ma", "Xi Li", "Jingyuan Hu", "Bin Yu"], "summary": "Single-cell sequencing is revolutionizing biology by enabling detailed\ninvestigations of cell-state transitions. Many biological processes unfold\nalong continuous trajectories, yet it remains challenging to extract smooth,\nlow-dimensional representations from inherently noisy, high-dimensional\nsingle-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,\nare widely used to embed high-dimensional single-cell data into low dimensions.\nBut they often introduce undesirable distortions, resulting in misleading\ninterpretations. Existing evaluation methods for NE algorithms primarily focus\non separating discrete cell types rather than capturing continuous cell-state\ntransitions, while dynamic modeling approaches rely on strong assumptions about\ncellular processes and specialized data. To address these challenges, we build\non the Predictability-Computability-Stability (PCS) framework for reliable and\nreproducible data-driven discoveries. First, we systematically evaluate popular\nNE algorithms through empirical analysis, simulation, and theory, and reveal\ntheir key shortcomings, such as artifacts and instability. We then introduce\nNESS, a principled and interpretable machine learning approach to improve NE\nrepresentations by leveraging algorithmic stability and to enable robust\ninference of smooth biological structures. NESS offers useful concepts,\nquantitative stability metrics, and efficient computational workflows to\nuncover developmental trajectories and cell-state transitions in single-cell\ndata. Finally, we apply NESS to six single-cell datasets, spanning pluripotent\nstem cell differentiation, organoid development, and multiple tissue-specific\nlineage trajectories. Across these diverse contexts, NESS consistently yields\nuseful biological insights, such as identification of transitional and stable\ncell states and quantification of transcriptional dynamics during development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22228v1", "categories": ["stat.ML", "cs.LG", "q-bio.GN", "stat.AP"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22228v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22340", "title": "QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks", "authors": ["Yannick Werner", "Akash Malemath", "Mengxi Liu", "Vitor Fortes Rey", "Nikolaos Palaiodimopoulos", "Paul Lukowicz", "Maximilian Kiefer-Emmanouilidis"], "summary": "Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold\nrepresentation theorem (KAR), have demonstrated promising capabilities in\nexpressing complex functions with fewer neurons. This is achieved by\nimplementing learnable parameters on the edges instead of on the nodes, unlike\ntraditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs\npotential in quantum machine learning has not yet been well explored. In this\nwork, we present an implementation of these KAN architectures in both hybrid\nand fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt\nthe KAN transfer using pre-trained residual functions, thereby exploiting the\nrepresentational power of parametrized quantum circuits. In the hybrid model we\ncombine classical KAN components with quantum subroutines, while the fully\nquantum version the entire architecture of the residual function is translated\nto a quantum model. We demonstrate the feasibility, interpretability and\nperformance of the proposed Quantum KAN (QuKAN) architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22340v1", "categories": ["quant-ph", "cs.CV", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22340v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22146", "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "authors": ["Amirmohammad Izadi", "Mohammad Ali Banayeeanzade", "Fatemeh Askari", "Ali Rahimiakbar", "Mohammad Mahdi Vahedi", "Hosein Hasani", "Mahdieh Soleymani Baghshah"], "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22146v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22146v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22049", "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22049v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22049v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22236", "title": "A Plea for History and Philosophy of Statistics and Machine Learning", "authors": ["Hanti Lin"], "summary": "The integration of the history and philosophy of statistics was initiated at\nleast by Hacking (1965) and advanced by Mayo (1996), but it has not received\nsustained follow-up. Yet such integration is more urgent than ever, as the\nrecent success of artificial intelligence has been driven largely by machine\nlearning -- a field historically developed alongside statistics. Today, the\nboundary between statistics and machine learning is increasingly blurred. What\nwe now need is integration, twice over: of history and philosophy, and of the\nfield they engage -- statistics and machine learning. I present a case study of\na philosophical idea in machine learning (and in formal epistemology) whose\nroot can be traced back to an often under-appreciated insight in Neyman and\nPearson's 1936 work (a follow-up to their 1933 classic). This leads to the\narticulation of a foundational assumption -- largely implicit in, but shared\nby, the practices of frequentist statistics and machine learning -- which I\ncall achievabilism. Another integration also emerges at the level of\nmethodology, combining two ends of the philosophy of science spectrum: history\nand philosophy of science on the one hand, and formal epistemology on the other\nhand.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22236v1", "categories": ["stat.OT", "cs.LG"], "cate": "stat.OT", "url": "http://arxiv.org/abs/2506.22236v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22397", "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "summary": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.", "comment": "supplement pending, 4 figures, 10 pages + refs", "pdf_url": "http://arxiv.org/pdf/2506.22397v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22397v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22179", "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition", "authors": ["Wenhan Wu", "Zhishuai Guo", "Chen Chen", "Hongfei Xue", "Aidong Lu"], "summary": "Zero-shot skeleton-based action recognition aims to develop models capable of\nidentifying actions beyond the categories encountered during training. Previous\napproaches have primarily focused on aligning visual and semantic\nrepresentations but often overlooked the importance of fine-grained action\npatterns in the semantic space (e.g., the hand movements in drinking water and\nbrushing teeth). To address these limitations, we propose a Frequency-Semantic\nEnhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic\nrepresentation learning with frequency decomposition. FS-VAE consists of three\nkey components: 1) a frequency-based enhancement module with high- and\nlow-frequency adjustments to enrich the skeletal semantics learning and improve\nthe robustness of zero-shot action recognition; 2) a semantic-based action\ndescription with multilevel alignment to capture both local details and global\ncorrespondence, effectively bridging the semantic gap and compensating for the\ninherent loss of information in skeleton sequences; 3) a calibrated\ncross-alignment loss that enables valid skeleton-text pairs to counterbalance\nambiguous ones, mitigating discrepancies and ambiguities in skeleton and text\nfeatures, thereby ensuring robust alignment. Evaluations on the benchmarks\ndemonstrate the effectiveness of our approach, validating that\nfrequency-enhanced semantic features enable robust differentiation of visually\nand semantically similar action clusters, improving zero-shot action\nrecognition.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22179v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22179v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22189", "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sjölund"], "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22189v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22189v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22241", "title": "Boosting Classification with Quantum-Inspired Augmentations", "authors": ["Matthias Tschöpe", "Vitor Fortes Rey", "Sogo Pierre Sanon", "Paul Lukowicz", "Nikolaos Palaiodimopoulos", "Maximilian Kiefer-Emmanouilidis"], "summary": "Understanding the impact of small quantum gate perturbations, which are\ncommon in quantum digital devices but absent in classical computers, is crucial\nfor identifying potential advantages in quantum machine learning. While these\nperturbations are typically seen as detrimental to quantum computation, they\ncan actually enhance performance by serving as a natural source of data\naugmentation. Additionally, they can often be efficiently simulated on\nclassical hardware, enabling quantum-inspired approaches to improve classical\nmachine learning methods. In this paper, we investigate random Bloch sphere\nrotations, which are fundamental SU(2) transformations, as a simple yet\neffective quantum-inspired data augmentation technique. Unlike conventional\naugmentations such as flipping, rotating, or cropping, quantum transformations\nlack intuitive spatial interpretations, making their application to tasks like\nimage classification less straightforward. While common quantum augmentation\nmethods rely on applying quantum models or trainable quanvolutional layers to\nclassical datasets, we focus on the direct application of small-angle Bloch\nrotations and their effect on classical data. Using the large-scale ImageNet\ndataset, we demonstrate that our quantum-inspired augmentation method improves\nimage classification performance, increasing Top-1 accuracy by 3%, Top-5\naccuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard\nclassical augmentation methods. Finally, we examine the use of stronger unitary\naugmentations. Although these transformations preserve information in\nprinciple, they result in visually unrecognizable images with potential\napplications for privacy computations. However, we show that our augmentation\napproach and simple SU(2) transformations do not enhance differential privacy\nand discuss the implications of this limitation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22241v1", "categories": ["cs.CV", "cond-mat.dis-nn", "cs.LG", "quant-ph"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22241v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22426", "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22426v1", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22426v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22185", "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22185v1", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22185v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22237", "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.", "comment": "9 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.22237v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22237v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22271", "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion", "authors": ["Samy Badreddine", "Emile van Krieken", "Luciano Serafini"], "summary": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22271v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22271v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22200", "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework", "authors": ["Chen Wang", "Lai Wei", "Yanzhi Zhang", "Chenyang Shao", "Zedong Dan", "Weiran Huang", "Yue Wang", "Yuzhi Zhang"], "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22200v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22200v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22255", "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "authors": ["Maciej Stefaniak", "Michał Krutul", "Jan Małaśnicki", "Maciej Pióro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22255v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22255v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22309", "title": "Conceptual Topic Aggregation", "authors": ["Klara M. Gutekunst", "Dominik Dürrschnabel", "Johannes Hirth", "Gerd Stumme"], "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "pdf_url": "http://arxiv.org/pdf/2506.22309v1", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22309v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22231", "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education", "authors": ["Russell Beale"], "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22231v1", "categories": ["cs.HC", "cs.AI", "cs.CY", "K.3.1; K.3.2; K.6.0"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22231v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22274", "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication", "authors": ["Filippo Merlo", "Ece Takmaz", "Wenkai Chen", "Albert Gatt"], "summary": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22274v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22274v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22335", "title": "Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability", "authors": ["Osama Ahmed", "Felix Tennie", "Luca Magri"], "summary": "We show that recurrent quantum reservoir computers (QRCs) and their\nrecurrence-free architectures (RF-QRCs) are robust tools for learning and\nforecasting chaotic dynamics from time-series data. First, we formulate and\ninterpret quantum reservoir computers as coupled dynamical systems, where the\nreservoir acts as a response system driven by training data; in other words,\nquantum reservoir computers are generalized-synchronization (GS) systems.\nSecond, we show that quantum reservoir computers can learn chaotic dynamics and\ntheir invariant properties, such as Lyapunov spectra, attractor dimensions, and\ngeometric properties such as the covariant Lyapunov vectors. This analysis is\nenabled by deriving the Jacobian of the quantum reservoir update. Third, by\nleveraging tools from generalized synchronization, we provide a method for\ndesigning robust quantum reservoir computers. We propose the criterion\n$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We\nanalytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we\nanalyze the effect of simulated noise. We find that dissipation from noise\nenhances the robustness of quantum reservoir computers. Numerical verifications\non systems of different dimensions support our conclusions. This work opens\nopportunities for designing robust quantum machines for chaotic time series\nforecasting on near-term quantum hardware.", "comment": "28 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.22335v1", "categories": ["quant-ph", "cs.LG", "nlin.CD"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22335v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22255", "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "authors": ["Maciej Stefaniak", "Michał Krutul", "Jan Małaśnicki", "Maciej Pióro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22255v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22255v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22309", "title": "Conceptual Topic Aggregation", "authors": ["Klara M. Gutekunst", "Dominik Dürrschnabel", "Johannes Hirth", "Gerd Stumme"], "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "pdf_url": "http://arxiv.org/pdf/2506.22309v1", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22309v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22340", "title": "QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks", "authors": ["Yannick Werner", "Akash Malemath", "Mengxi Liu", "Vitor Fortes Rey", "Nikolaos Palaiodimopoulos", "Paul Lukowicz", "Maximilian Kiefer-Emmanouilidis"], "summary": "Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold\nrepresentation theorem (KAR), have demonstrated promising capabilities in\nexpressing complex functions with fewer neurons. This is achieved by\nimplementing learnable parameters on the edges instead of on the nodes, unlike\ntraditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs\npotential in quantum machine learning has not yet been well explored. In this\nwork, we present an implementation of these KAN architectures in both hybrid\nand fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt\nthe KAN transfer using pre-trained residual functions, thereby exploiting the\nrepresentational power of parametrized quantum circuits. In the hybrid model we\ncombine classical KAN components with quantum subroutines, while the fully\nquantum version the entire architecture of the residual function is translated\nto a quantum model. We demonstrate the feasibility, interpretability and\nperformance of the proposed Quantum KAN (QuKAN) architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22340v1", "categories": ["quant-ph", "cs.CV", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22340v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22291", "title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation", "authors": ["Mengqi Zhou", "Xipeng Wang", "Yuxi Wang", "Zhaoxiang Zhang"], "summary": "Generating realistic 3D indoor scenes from user inputs remains a challenging\nproblem in computer vision and graphics, requiring careful balance of geometric\nconsistency, spatial relationships, and visual realism. While neural generation\nmethods often produce repetitive elements due to limited global spatial\nreasoning, procedural approaches can leverage constraints for controllable\ngeneration but struggle with multi-constraint scenarios. When constraints\nbecome numerous, object collisions frequently occur, forcing the removal of\nfurniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline\nthat converts real images, sketches, or text descriptions into coherent 3D\nindoor scenes. Our approach combines a scene generation pipeline with a\nconstraint-driven optimization framework. The pipeline first extracts\nhigh-level scene information from user inputs and organizes it into a\nstructured format containing room type, furniture items, and spatial relations.\nIt then constructs a spatial relationship network to represent furniture\narrangements and generates an optimized placement sequence using a\nheuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.\nTo handle complex multi-constraint scenarios, we introduce a unified constraint\nrepresentation that processes both formal specifications and natural language\ninputs, enabling flexible constraint-oriented adjustments through a\ncomprehensive action space design. Additionally, we propose a Conflict-Aware\nPositioning Strategy (CAPS) that dynamically adjusts placement weights to\nminimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms\nexisting methods in generating realistic, semantically coherent, and visually\nappealing room layouts across diverse input modalities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22291v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22291v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22343", "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts", "authors": ["Xiang Li", "Garrett Wen", "Weiqing He", "Jiayuan Wu", "Qi Long", "Weijie J. Su"], "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22343v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22343v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22343", "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts", "authors": ["Xiang Li", "Garrett Wen", "Weiqing He", "Jiayuan Wu", "Qi Long", "Weijie J. Su"], "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22343v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22343v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22299", "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks", "authors": ["Tao Liu", "Longlong Lin", "Yunfeng Yu", "Xi Ou", "Youan Zhang", "Zhiqiu Ye", "Tao Jia"], "summary": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.", "comment": "icmr", "pdf_url": "http://arxiv.org/pdf/2506.22299v1", "categories": ["cs.LG", "cs.AI", "I.2"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22299v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22372", "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement", "authors": ["Maryam Mousavian", "Zahra Abbasiantaeb", "Mohammad Aliannejadi", "Fabio Crestani"], "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.", "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22372v1", "categories": ["cs.IR", "cs.CL"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22372v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22360", "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "pdf_url": "http://arxiv.org/pdf/2506.22360v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22360v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22321", "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22321v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22321v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22376", "title": "Probabilistic Optimality for Inference-time Scaling", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22376v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22376v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22362", "title": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding", "authors": ["Yang Yang", "Yunpeng Li", "George Sung", "Shao-Fu Shih", "Craig Dooley", "Alessio Centazzo", "Ramanan Rajeswaran"], "summary": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22362v1", "categories": ["eess.AS", "cs.LG"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22362v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22331", "title": "Less Greedy Equivalence Search", "authors": ["Adiba Ejaz", "Elias Bareinboim"], "summary": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.", "comment": "35 total pages. 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.22331v1", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22331v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22385", "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22385v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22385v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22419", "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22419v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22419v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22338", "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "authors": ["Luigi Russo", "Deodato Tapete", "Silvia Liberata Ullo", "Paolo Gamba"], "summary": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.", "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted\n  to IEEE Journal of Selected Topics in Applied Earth Observations and Remote\n  Sensing", "pdf_url": "http://arxiv.org/pdf/2506.22338v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22338v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22419", "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22419v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22419v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22429", "title": "Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks", "authors": ["David Holzmüller", "Max Schölpple"], "summary": "While the theory of deep learning has made some progress in recent years,\nmuch of it is limited to the ReLU activation function. In particular, while the\nneural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)\nhave given theoreticians tractable limiting cases of fully connected neural\nnetworks, their properties for most activation functions except for powers of\nthe ReLU function are poorly understood. Our main contribution is to provide a\nmore general characterization of the RKHS of these kernels for typical\nactivation functions whose only non-smoothness is at zero, such as SELU, ELU,\nor LeakyReLU. Our analysis also covers a broad set of special cases such as\nmissing biases, two-layer networks, or polynomial activations. Our results show\nthat a broad class of not infinitely smooth activations generate equivalent\nRKHSs at different network depths, while polynomial activations generate\nnon-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP\nsample paths, characterizing the smoothness of infinitely wide neural networks\nat initialization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22429v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22429v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22342", "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis", "authors": ["Zihan Guan", "Zhiyuan Zhao", "Fengwei Tian", "Dung Nguyen", "Payel Bhattacharjee", "Ravi Tandon", "B. Aditya Prakash", "Anil Vullikanti"], "summary": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.", "comment": "17 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22342v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22342v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22359", "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models", "authors": ["Viswanath Kumarskandpriya", "Abdulhalim Dandoush", "Abbas Bradai", "Ali Belgacem"], "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22359v1", "categories": ["cs.NI", "cs.AI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22359v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22360", "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "pdf_url": "http://arxiv.org/pdf/2506.22360v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22360v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22374", "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "summary": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.", "comment": "13 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.22374v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22374v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22376", "title": "Probabilistic Optimality for Inference-time Scaling", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22376v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22376v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22385", "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22385v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22385v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22389", "title": "Towards Distributed Neural Architectures", "authors": ["Aditya Cowsik", "Tianyu He", "Andrey Gromov"], "summary": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.", "comment": "36 pages, 25 figures", "pdf_url": "http://arxiv.org/pdf/2506.22389v1", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22389v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22393", "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis", "authors": ["YongKyung Oh", "Alex Bui"], "summary": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22393v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22393v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22396", "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization", "authors": ["Danush Khanna", "Aditya Kumar Guru", "Srivarshinee Sridhar", "Zidan Ahmed", "Rubhav Bahirwani", "Meetu Malhotra", "Vinija Jain", "Aman Chadha", "Amitava Das", "Kripabandhu Ghosh"], "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).", "comment": "Preprint. Under submission", "pdf_url": "http://arxiv.org/pdf/2506.22396v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22396v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22397", "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "summary": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.", "comment": "supplement pending, 4 figures, 10 pages + refs", "pdf_url": "http://arxiv.org/pdf/2506.22397v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22397v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22403", "title": "HyperCLOVA X THINK Technical Report", "authors": ["NAVER Cloud HyperCLOVA X Team"], "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.", "comment": "49 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.22403v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22403v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22427", "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings", "authors": ["Randeep Bhatia", "Nikos Papadis", "Murali Kodialam", "TV Lakshman", "Sayak Chakrabarty"], "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.", "comment": "31 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22427v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22427v1", "date": "2025-06-27", "updated": "2025-06-27"}
