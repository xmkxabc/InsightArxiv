{"id": "2506.02034", "pdf": "https://arxiv.org/pdf/2506.02034", "abs": "https://arxiv.org/abs/2506.02034", "authors": ["Ignacio Arretche", "Mohammad Tanver Hossain", "Ramdas Tiwari", "Abbie Kim", "Mya G. Mills", "Connor D. Armstrong", "Jacob J. Lessard", "Sameh H. Tawfick", "Randy H. Ewoldt"], "title": "High-throughput viscometry via machine-learning from videos of inverted vials", "categories": ["cs.GR"], "comment": null, "summary": "Although the inverted vial test has been widely used as a qualitative method\nfor estimating fluid viscosity, quantitative rheological characterization has\nremained limited due to its complex, uncontrolled flow - driven by gravity,\nsurface tension, inertia, and initial conditions. Here, we present a computer\nvision (CV) viscometer that automates the inverted vial test and enables\nquantitative viscosity inference across nearly five orders of magnitude\n(0.01-1000 Pas), without requiring direct velocity field measurements. The\nsystem simultaneously inverts multiple vials and records videos of the evolving\nfluid, which are fed into a neural network that approximates the inverse\nfunction from visual features and known fluid density. Despite the complex,\nmulti-regime flow within the vial, our approach achieves relative errors below\n25%, improving to 15% for viscosities above 0.1 Pas. When tested on\nnon-Newtonian polymer solutions, the method reliably estimates zero-shear\nviscosity as long as viscoelastic or shear-thinning behaviors remain negligible\nwithin the flow regime. Moreover, high standard deviations in the inferred\nvalues may serve as a proxy for identifying fluids with strong non-Newtonian\nbehavior. The CV viscometer requires only one camera and one motor, is\ncontactless and low-cost, and can be easily integrated into high-throughput\nexperimental automated and manual workflows. Transcending traditional\ncharacterization paradigms, our method leverages uncontrolled flows and visual\nfeatures to achieve simplicity and scalability, enabling high-throughput\nviscosity inference that can meet the growing demand of data-driven material\nmodels while remaining accessible to lower resource environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7c98\u5ea6\u8ba1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5012\u7f6e\u74f6\u6d4b\u8bd5\u5b9e\u73b0\u5b9a\u91cf\u7c98\u5ea6\u6d4b\u91cf\uff0c\u8986\u76d6\u8303\u56f4\u5e7f\u4e14\u6210\u672c\u4f4e\u3002", "motivation": "\u4f20\u7edf\u5012\u7f6e\u74f6\u6d4b\u8bd5\u56e0\u590d\u6742\u6d41\u52a8\u96be\u4ee5\u5b9a\u91cf\u6d4b\u91cf\u7c98\u5ea6\uff0c\u9700\u5f00\u53d1\u7b80\u5355\u3001\u9ad8\u541e\u5410\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u795e\u7ecf\u7f51\u7edc\u4ece\u5012\u7f6e\u74f6\u4e2d\u6d41\u4f53\u6d41\u52a8\u7684\u89c6\u89c9\u7279\u5f81\u63a8\u65ad\u7c98\u5ea6\u3002", "result": "\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e25%\uff0c\u5bf9\u975e\u725b\u987f\u6d41\u4f53\u4e5f\u80fd\u53ef\u9760\u4f30\u8ba1\u96f6\u526a\u5207\u7c98\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u9ad8\u901a\u91cf\u5b9e\u9a8c\u548c\u8d44\u6e90\u6709\u9650\u73af\u5883\u3002"}}
{"id": "2506.02219", "pdf": "https://arxiv.org/pdf/2506.02219", "abs": "https://arxiv.org/abs/2506.02219", "authors": ["Abhishek Madan", "Nicholas Sharp", "Francis Williams", "Ken Museth", "David I. W. Levin"], "title": "Stochastic Barnes-Hut Approximation for Fast Summation on the GPU", "categories": ["cs.GR"], "comment": "11 pages, 9 figures. To appear in ACM SIGGRAPH 2025", "summary": "We present a novel stochastic version of the Barnes-Hut approximation.\nRegarding the level-of-detail (LOD) family of approximations as control\nvariates, we construct an unbiased estimator of the kernel sum being\napproximated. Through several examples in graphics applications such as winding\nnumber computation and smooth distance evaluation, we demonstrate that our\nmethod is well-suited for GPU computation, capable of outperforming a\nGPU-optimized implementation of the deterministic Barnes-Hut approximation by\nachieving equal median error in up to 9.4x less time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673aBarnes-Hut\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LOD\u8fd1\u4f3c\u4f5c\u4e3a\u63a7\u5236\u53d8\u91cf\uff0c\u6784\u5efa\u4e86\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u5728\u56fe\u5f62\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cGPU\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u6539\u8fdbBarnes-Hut\u8fd1\u4f3c\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728GPU\u8ba1\u7b97\u73af\u5883\u4e0b\u3002", "method": "\u5c06LOD\u8fd1\u4f3c\u4f5c\u4e3a\u63a7\u5236\u53d8\u91cf\uff0c\u6784\u5efa\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u5e76\u4f18\u5316GPU\u5b9e\u73b0\u3002", "result": "\u5728\u56fe\u5f62\u5e94\u7528\u4e2d\uff0c\u5982\u73af\u7ed5\u6570\u8ba1\u7b97\u548c\u5e73\u6ed1\u8ddd\u79bb\u8bc4\u4f30\uff0c\u6027\u80fd\u4f18\u4e8e\u786e\u5b9a\u6027Barnes-Hut\u8fd1\u4f3c\uff0c\u901f\u5ea6\u5feb9.4\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728GPU\u8ba1\u7b97\u4e2d\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u9002\u7528\u4e8e\u56fe\u5f62\u5e94\u7528\u3002"}}
{"id": "2506.02620", "pdf": "https://arxiv.org/pdf/2506.02620", "abs": "https://arxiv.org/abs/2506.02620", "authors": ["Dongyu Yan", "Leyi Wu", "Jiantao Lin", "Luozhou Wang", "Tianshuo Xu", "Zhifei Chen", "Zhen Yang", "Lie Xu", "Shunsi Zhang", "Yingcong Chen"], "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 10 figures in main paper, 10 pages, 12 figures in\n  supplementary", "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce \\textbf{FlexPainter},\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.", "AI": {"tldr": "FlexPainter\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7eb9\u7406\u751f\u6210\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u5f15\u5bfc\u548c\u4e00\u81f4\u6027\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eb9\u7406\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728\u7eb9\u7406\u751f\u6210\u4e2d\u5b58\u5728\u63a7\u5236\u7075\u6d3b\u6027\u4e0d\u8db3\u3001\u591a\u89c6\u89d2\u56fe\u50cf\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u751f\u6210\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u5171\u4eab\u6761\u4ef6\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u8f93\u5165\u7075\u6d3b\u805a\u5408\uff1b\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u7684CFG\u65b9\u6cd5\u5206\u89e3\u7ed3\u6784\u548c\u98ce\u683c\u4fe1\u606f\uff1b\u5229\u75283D\u77e5\u8bc6\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u89c6\u56fe\u540c\u6b65\u548c\u81ea\u9002\u5e94\u52a0\u6743\u6a21\u5757\u786e\u4fdd\u4e00\u81f4\u6027\uff1b\u7ed3\u54083D\u611f\u77e5\u7eb9\u7406\u8865\u5168\u548c\u589e\u5f3a\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlexPainter\u5728\u7075\u6d3b\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlexPainter\u901a\u8fc7\u591a\u6a21\u6001\u5f15\u5bfc\u548c\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u4e3a\u9ad8\u8d28\u91cf\u7eb9\u7406\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02774", "pdf": "https://arxiv.org/pdf/2506.02774", "abs": "https://arxiv.org/abs/2506.02774", "authors": ["Zheng Liu", "He Zhu", "Xinyang Li", "Yirun Wang", "Yujiao Shi", "Wei Li", "Jingwen Leng", "Minyi Guo", "Yu Feng"], "title": "Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone", "categories": ["cs.GR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D\nscene rendering. However, rendering city-scale 3DGS scenes on mobile devices,\ne.g., your smartphones, remains a significant challenge due to the limited\nresources on mobile devices. A natural solution is to offload computation to\nthe cloud; however, naively streaming rendered frames from the cloud to the\nclient introduces high latency and requires bandwidth far beyond the capacity\nof current wireless networks.\n  In this paper, we propose an effective solution to enable city-scale 3DGS\nrendering on mobile devices. Our key insight is that, under normal user motion,\nthe number of newly visible Gaussians per second remains roughly constant.\nLeveraging this, we stream only the necessary Gaussians to the client.\nSpecifically, on the cloud side, we propose asynchronous level-of-detail search\nto identify the necessary Gaussians for the client. On the client side, we\naccelerate rendering via a lookup table-based rasterization. Combined with\nholistic runtime optimizations, our system can deliver low-latency, city-scale\n3DGS rendering on mobile devices. Compared to existing solutions, Voyager\nachieves over 100$\\times$ reduction on data transfer and up to 8.9$\\times$\nspeedup while retaining comparable rendering quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u57ce\u5e02\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6d41\u5f0f\u4f20\u8f93\u5fc5\u8981\u7684\u9ad8\u65af\u5143\u7d20\u548c\u4f18\u5316\u6e32\u67d3\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u4f20\u8f93\u548c\u5ef6\u8fdf\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u96be\u4ee5\u76f4\u63a5\u6e32\u67d3\u57ce\u5e02\u89c4\u6a21\u76843D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\uff0c\u800c\u4f20\u7edf\u7684\u4e91\u7aef\u6e32\u67d3\u65b9\u6848\u56e0\u9ad8\u5ef6\u8fdf\u548c\u5e26\u5bbd\u9700\u6c42\u8fc7\u5927\u800c\u4e0d\u9002\u7528\u3002", "method": "\u5229\u7528\u7528\u6237\u8fd0\u52a8\u65f6\u65b0\u53ef\u89c1\u9ad8\u65af\u5143\u7d20\u6570\u91cf\u6052\u5b9a\u7684\u7279\u70b9\uff0c\u4e91\u7aef\u5f02\u6b65\u641c\u7d22\u5fc5\u8981\u7684\u9ad8\u65af\u5143\u7d20\uff0c\u5ba2\u6237\u7aef\u901a\u8fc7\u67e5\u627e\u8868\u52a0\u901f\u6e32\u67d3\uff0c\u5e76\u7ed3\u5408\u8fd0\u884c\u65f6\u4f18\u5316\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\uff0c\u6570\u636e\u4f20\u8f93\u51cf\u5c11100\u500d\u4ee5\u4e0a\uff0c\u901f\u5ea6\u63d0\u53478.9\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6848\u6210\u529f\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u57ce\u5e02\u89c4\u6a21\u76843D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u3002"}}
{"id": "2506.02010", "pdf": "https://arxiv.org/pdf/2506.02010", "abs": "https://arxiv.org/abs/2506.02010", "authors": ["Zehua Liu", "Xiaolou Li", "Chen Chen", "Lantian Li", "Dong Wang"], "title": "CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "to be published in INTERSPEECH 2025", "summary": "This paper presents the second Chinese Continuous Visual Speech Recognition\nChallenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in\nChinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The\nchallenge evaluates two test scenarios: reading in recording studios and\nInternet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC\n2023, which involves CN-CVS for training and CNVSRC-Single/Multi for\ndevelopment and evaluation. However, CNVSRC 2024 introduced two key\nimprovements: (1) a stronger baseline system, and (2) an additional dataset,\nCN-CVS2-P1, for open tracks to improve data volume and diversity. The new\nchallenge has demonstrated several important innovations in data preprocessing,\nfeature extraction, model design, and training strategies, further pushing the\nstate-of-the-art in Chinese LVC-VSR. More details and resources are available\nat the official website.", "AI": {"tldr": "CNVSRC 2024\u6311\u6218\u8d5b\u5728CNVSRC 2023\u57fa\u7840\u4e0a\u63a8\u8fdb\u4e2d\u6587\u5927\u8bcd\u6c47\u91cf\u8fde\u7eed\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\uff0c\u5f15\u5165\u66f4\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u548c\u65b0\u589e\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u6570\u636e\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u63a8\u52a8\u4e2d\u6587\u5927\u8bcd\u6c47\u91cf\u8fde\u7eed\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08LVC-VSR\uff09\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528\u76f8\u540c\u6570\u636e\u96c6\uff08CN-CVS\u8bad\u7ec3\uff0cCNVSRC-Single/Multi\u8bc4\u4f30\uff09\uff0c\u65b0\u589e\u6570\u636e\u96c6CN-CVS2-P1\uff0c\u6539\u8fdb\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u3001\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u5728\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u3001\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u4e0a\u7684\u521b\u65b0\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86LVC-VSR\u7684\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "CNVSRC 2024\u901a\u8fc7\u6539\u8fdb\u548c\u65b0\u589e\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u4e2d\u6587LVC-VSR\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.01961", "pdf": "https://arxiv.org/pdf/2506.01961", "abs": "https://arxiv.org/abs/2506.01961", "authors": ["Jinzhu Yang"], "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System", "categories": ["cs.CL"], "comment": null, "summary": "This study is dedicated to exploring the application of prompt learning\nmethods to advance Named Entity Recognition (NER) within the medical domain. In\nrecent years, the emergence of large-scale models has driven significant\nprogress in NER tasks, particularly with the introduction of the BioBERT\nlanguage model, which has greatly enhanced NER capabilities in medical texts.\nOur research introduces the Prompt-bioMRC model, which integrates both hard\ntemplate and soft prompt designs aimed at refining the precision and efficiency\nof medical entity recognition. Through extensive experimentation across diverse\nmedical datasets, our findings consistently demonstrate that our approach\nsurpasses traditional models. This enhancement not only validates the efficacy\nof our methodology but also highlights its potential to provide reliable\ntechnological support for applications like intelligent diagnosis systems. By\nleveraging advanced NER techniques, this study contributes to advancing\nautomated medical data processing, facilitating more accurate medical\ninformation extraction, and supporting efficient healthcare decision-making\nprocesses.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPrompt-bioMRC\u6a21\u578b\uff0c\u7ed3\u5408\u786c\u6a21\u677f\u548c\u8f6f\u63d0\u793a\u8bbe\u8ba1\uff0c\u63d0\u5347\u533b\u7597\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u4e3a\u667a\u80fd\u8bca\u65ad\u7cfb\u7edf\u7b49\u5e94\u7528\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "motivation": "\u63a2\u7d22\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u7597\u9886\u57dfNER\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u533b\u5b66\u6587\u672c\u5b9e\u4f53\u8bc6\u522b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPrompt-bioMRC\u6a21\u578b\uff0c\u7ed3\u5408\u786c\u6a21\u677f\u548c\u8f6f\u63d0\u793a\u8bbe\u8ba1\uff0c\u4f18\u5316\u533b\u5b66\u5b9e\u4f53\u8bc6\u522b\u3002", "result": "\u5728\u591a\u79cd\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u7597\u6570\u636e\u81ea\u52a8\u5316\u5904\u7406\u548c\u9ad8\u6548\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2506.02794", "pdf": "https://arxiv.org/pdf/2506.02794", "abs": "https://arxiv.org/abs/2506.02794", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "summary": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.", "AI": {"tldr": "PhysGaia\u662f\u4e00\u4e2a\u4e13\u4e3a\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\uff08DyNVS\uff09\u8bbe\u8ba1\u7684\u7269\u7406\u611f\u77e5\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u5bf9\u8c61\u548c\u975e\u7ed3\u6784\u5316\u7269\u7406\u73b0\u8c61\uff0c\u652f\u6301\u7269\u7406\u611f\u77e5\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u903c\u771f\u91cd\u5efa\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u4ea4\u4e92\u7684\u5168\u9762\u652f\u6301\uff0cPhysGaia\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u7684\u7269\u7406\u6c42\u89e3\u5668\u751f\u6210\u4e25\u683c\u9075\u5faa\u7269\u7406\u89c4\u5f8b\u7684\u573a\u666f\uff0c\u5e76\u63d0\u4f9b3D\u7c92\u5b50\u8f68\u8ff9\u548c\u7269\u7406\u53c2\u6570\u7b49\u771f\u5b9e\u6570\u636e\u3002", "result": "PhysGaia\u652f\u6301\u5b9a\u91cf\u8bc4\u4f30\u7269\u7406\u5efa\u6a21\uff0c\u5e76\u4e3a\u524d\u6cbfDyNVS\u6a21\u578b\u63d0\u4f9b\u96c6\u6210\u7ba1\u9053\u3002", "conclusion": "PhysGaia\u5c06\u63a8\u52a8\u52a8\u6001\u89c6\u89d2\u5408\u6210\u3001\u7269\u7406\u573a\u666f\u7406\u89e3\u548c\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u7406\u6a21\u62df\u7684\u7ed3\u5408\u7814\u7a76\u3002"}}
{"id": "2506.02011", "pdf": "https://arxiv.org/pdf/2506.02011", "abs": "https://arxiv.org/abs/2506.02011", "authors": ["Minjae Lee", "Minhyuk Seo", "Tingyu Qu", "Tinne Tuytelaars", "Jonghyun Choi"], "title": "OASIS: Online Sample Selection for Continual Visual Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "In continual visual instruction tuning (CVIT) scenarios, where multi-modal\ndata continuously arrive in an online streaming manner, training delays from\nlarge-scale data significantly hinder real-time adaptation. While existing data\nselection strategies reduce training overheads, they rely on pre-trained\nreference models, which are impractical in CVIT setups due to unknown future\ndata. Recent reference model-free online sample selection methods address this\nissue but typically select a fixed number of samples per batch (e.g., top-k),\ncausing them to suffer from distribution shifts where informativeness varies\nacross batches. To address these limitations, we propose OASIS, an adaptive\nonline sample selection approach for CVIT that: (1) dynamically adjusts\nselected samples per batch based on relative inter-batch informativeness, and\n(2) minimizes redundancy of selected samples through iterative selection score\nupdates. Empirical results across various MLLMs, such as LLaVA-1.5 and\nQwen-VL-2.5, show that OASIS achieves comparable performance to full-data\ntraining using only 25% of the data and outperforms the state-of-the-art.", "AI": {"tldr": "OASIS\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5728\u7ebf\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff08CVIT\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u6279\u6837\u672c\u6570\u91cf\u4ee5\u51cf\u5c11\u5197\u4f59\u5e76\u9002\u5e94\u5206\u5e03\u53d8\u5316\uff0c\u4ec5\u970025%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u5728CVIT\u573a\u666f\u4e2d\uff0c\u591a\u6a21\u6001\u6570\u636e\u6301\u7eed\u5728\u7ebf\u5230\u8fbe\uff0c\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u53c2\u8003\u6a21\u578b\u6216\u56fa\u5b9a\u6837\u672c\u6570\u91cf\uff0c\u65e0\u6cd5\u9002\u5e94\u5206\u5e03\u53d8\u5316\u3002", "method": "\u63d0\u51faOASIS\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u6279\u6837\u672c\u6570\u91cf\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u9009\u62e9\u5206\u6570\u6700\u5c0f\u5316\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOASIS\u4ec5\u752825%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u8bad\u7ec3\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OASIS\u4e3aCVIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6837\u672c\u9009\u62e9\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u5f00\u9500\u3002"}}
{"id": "2506.01992", "pdf": "https://arxiv.org/pdf/2506.01992", "abs": "https://arxiv.org/abs/2506.01992", "authors": ["Lukas Rauch", "Moritz Wirth", "Denis Huseljic", "Marek Herde", "Bernhard Sick", "Matthias A\u00dfenmacher"], "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "under review @NeurIPS2025", "summary": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5d4c\u5165\u6539\u8fdb\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u7684\u5b9e\u7528\u6027\uff0c\u53d1\u73b0\u5d4c\u5165\u8d28\u91cf\u548c\u67e5\u8be2\u7b56\u7565\u9009\u62e9\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u901a\u8fc7\u5229\u7528\u51bb\u7ed3\u7684LLM\u5d4c\u5165\uff0c\u51cf\u5c11\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u4e2d\u8fed\u4ee3\u5fae\u8c03\u5927\u578b\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528MTEB\u6392\u884c\u699c\u4e2d\u7684\u4e94\u4e2a\u9ad8\u6027\u80fd\u6a21\u578b\u548c\u4e24\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u5341\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u5d4c\u5165\u8d28\u91cf\u5bf9AL\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6837\u6027\u91c7\u6837\u4e0e\u9ad8\u8d28\u91cf\u5d4c\u5165\u534f\u540c\u63d0\u5347\u65e9\u671fAL\u6027\u80fd\uff1b\u67e5\u8be2\u7b56\u7565\u9009\u62e9\u5bf9\u5d4c\u5165\u8d28\u91cf\u654f\u611f\uff0cBadge\u7b56\u7565\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "AL\u7b56\u7565\u9700\u7ed3\u5408\u5d4c\u5165\u8d28\u91cf\u548c\u4efb\u52a1\u7279\u6027\u8fdb\u884c\u4e0a\u4e0b\u6587\u8bc4\u4f30\u3002"}}
{"id": "2506.02895", "pdf": "https://arxiv.org/pdf/2506.02895", "abs": "https://arxiv.org/abs/2506.02895", "authors": ["Ahmad AlMughrabi", "Umair Haroon", "Ricardo Marques", "Petia Radeva"], "title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for dietary monitoring, medical\nnutrition management, and food intake analysis. Existing 3D Food Volume\nestimation methods accurately compute the food volume but lack for food\nportions selection. We present VolTex, a framework that improves \\change{the\nfood object selection} in food volume estimation. Allowing users to specify a\ntarget food item via text input to be segmented, our method enables the precise\nselection of specific food objects in real-world scenes. The segmented object\nis then reconstructed using the Neural Surface Reconstruction method to\ngenerate high-fidelity 3D meshes for volume computation. Extensive evaluations\non the MetaFood3D dataset demonstrate the effectiveness of our approach in\nisolating and reconstructing food items for accurate volume estimation. The\nsource code is accessible at https://github.com/GCVCG/VolTex.", "AI": {"tldr": "VolTex\u6846\u67b6\u901a\u8fc7\u6587\u672c\u8f93\u5165\u5b9e\u73b0\u7cbe\u51c6\u98df\u7269\u5206\u5272\uff0c\u7ed3\u5408\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u751f\u6210\u9ad8\u4fdd\u771f3D\u7f51\u683c\uff0c\u63d0\u5347\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u67093D\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u98df\u7269\u90e8\u5206\u7684\u7cbe\u51c6\u9009\u62e9\uff0cVolTex\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7528\u6237\u901a\u8fc7\u6587\u672c\u8f93\u5165\u6307\u5b9a\u76ee\u6807\u98df\u7269\uff0c\u5229\u7528\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u751f\u62103D\u7f51\u683c\u8fdb\u884c\u4f53\u79ef\u8ba1\u7b97\u3002", "result": "\u5728MetaFood3D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86VolTex\u5728\u98df\u7269\u5206\u5272\u548c\u4f53\u79ef\u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "VolTex\u901a\u8fc7\u6539\u8fdb\u98df\u7269\u5bf9\u8c61\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2506.02012", "pdf": "https://arxiv.org/pdf/2506.02012", "abs": "https://arxiv.org/abs/2506.02012", "authors": ["Zehua Liu", "Xiaolou Li", "Li Guo", "Lantian Li", "Dong Wang"], "title": "Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Visual Speech Recognition (VSR) transcribes speech by analyzing lip\nmovements. Recently, Large Language Models (LLMs) have been integrated into VSR\nsystems, leading to notable performance improvements. However, the potential of\nLLMs has not been extensively studied, and how to effectively utilize LLMs in\nVSR tasks remains unexplored. This paper systematically explores how to better\nleverage LLMs for VSR tasks and provides three key contributions: (1) Scaling\nTest: We study how the LLM size affects VSR performance, confirming a scaling\nlaw in the VSR task. (2) Context-Aware Decoding: We add contextual text to\nguide the LLM decoding, improving recognition accuracy. (3) Iterative\nPolishing: We propose iteratively refining LLM outputs, progressively reducing\nrecognition errors. Extensive experiments demonstrate that by these designs,\nthe great potential of LLMs can be largely harnessed, leading to significant\nVSR performance improvement.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08VSR\uff09\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e09\u9879\u5173\u952e\u8d21\u732e\uff1a\u89c4\u6a21\u6d4b\u8bd5\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5df2\u88ab\u6574\u5408\u5230VSR\u7cfb\u7edf\u4e2d\u5e76\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5176\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528LLMs\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u89c4\u6a21\u6d4b\u8bd5\u7814\u7a76LLM\u5927\u5c0f\u5bf9VSR\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u4ee5\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u9010\u6b65\u51cf\u5c11\u8bc6\u522b\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u663e\u8457\u91ca\u653eLLMs\u7684\u6f5c\u529b\uff0c\u5927\u5e45\u63d0\u5347VSR\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86LLMs\u5728VSR\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000", "abs": "https://arxiv.org/abs/2506.02000", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale.", "AI": {"tldr": "NovelHopQA\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u572864k-128k\u6807\u8bb0\u7684\u957f\u6587\u672c\u4e2d\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u8054\u5408\u8bc4\u4f30\u8fd9\u4e24\u65b9\u9762\u7684\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e83\u90e8\u5c0f\u8bf4\u7684\u957f\u6587\u672c\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u5173\u952e\u8bcd\u5f15\u5bfc\u7684\u7ba1\u9053\u751f\u6210\u591a\u8df3\u95ee\u9898\u94fe\uff0c\u5e76\u8bc4\u4f306\u79cd\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u968f\u7740\u8df3\u6570\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u6a21\u578b\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u89c4\u6a21\u4e0d\u80fd\u4fdd\u8bc1\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "NovelHopQA\u4e3a\u957f\u6587\u672c\u591a\u8df3\u63a8\u7406\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2506.03004", "pdf": "https://arxiv.org/pdf/2506.03004", "abs": "https://arxiv.org/abs/2506.03004", "authors": ["Junyu Liu", "R. Kenny Jones", "Daniel Ritchie"], "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.", "AI": {"tldr": "PartComposer\u662f\u4e00\u4e2a\u4ece\u5355\u56fe\u50cf\u793a\u4f8b\u5b66\u4e60\u90e8\u5206\u7ea7\u6982\u5ff5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u636e\u5408\u6210\u548c\u6700\u5927\u5316\u4e92\u4fe1\u606f\u5b9e\u73b0\u5f3a\u89e3\u8026\u548c\u53ef\u63a7\u7ec4\u5408\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u7ec6\u7c92\u5ea6\u6982\u5ff5\u6216\u9700\u8981\u5927\u91cf\u6570\u636e\u8f93\u5165\uff0cPartComposer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u591a\u6837\u90e8\u5206\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9884\u6d4b\u5668\u6700\u5927\u5316\u53bb\u566a\u6f5c\u5728\u4e0e\u7ed3\u6784\u5316\u6982\u5ff5\u4ee3\u7801\u7684\u4e92\u4fe1\u606f\u3002", "result": "\u65b9\u6cd5\u5728\u89e3\u8026\u548c\u53ef\u63a7\u7ec4\u5408\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u540c\u7c7b\u548c\u8de8\u7c7b\u522b\u57fa\u7ebf\u3002", "conclusion": "PartComposer\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u90e8\u5206\u7ea7\u6982\u5ff5\u5b66\u4e60\u548c\u7ec4\u5408\u3002"}}
{"id": "2506.02014", "pdf": "https://arxiv.org/pdf/2506.02014", "abs": "https://arxiv.org/abs/2506.02014", "authors": ["Wang Mengjie", "Zhu Huiping", "Li Jian", "Shi Wenxiu", "Zhang Song"], "title": "Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of autonomous and assisted driving technologies, higher\ndemands are placed on the ability to understand complex driving scenarios.\nMultimodal general large models have emerged as a solution for this challenge.\nHowever, applying these models in vertical domains involves difficulties such\nas data collection, model training, and deployment optimization. This paper\nproposes a comprehensive method for optimizing multimodal models in driving\nscenarios, including cone detection, traffic light recognition, speed limit\nrecommendation, and intersection alerts. The method covers key aspects such as\ndynamic prompt optimization, dataset construction, model training, and\ndeployment. Specifically, the dynamic prompt optimization adjusts the prompts\nbased on the input image content to focus on objects affecting the ego vehicle,\nenhancing the model's task-specific focus and judgment capabilities. The\ndataset is constructed by combining real and synthetic data to create a\nhigh-quality and diverse multimodal training dataset, improving the model's\ngeneralization in complex driving environments. In model training, advanced\ntechniques like knowledge distillation, dynamic fine-tuning, and quantization\nare integrated to reduce storage and computational costs while boosting\nperformance. Experimental results show that this systematic optimization method\nnot only significantly improves the model's accuracy in key tasks but also\nachieves efficient resource utilization, providing strong support for the\npractical application of driving scenario perception technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u5e94\u7528\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u52a8\u6001\u63d0\u793a\u4f18\u5316\u3001\u6570\u636e\u96c6\u6784\u5efa\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u90e8\u7f72\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7406\u89e3\u590d\u6742\u9a7e\u9a76\u573a\u666f\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u4f18\u5316\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u52a8\u6001\u63d0\u793a\u4f18\u5316\uff08\u6839\u636e\u8f93\u5165\u56fe\u50cf\u8c03\u6574\u63d0\u793a\uff09\u3001\u6570\u636e\u96c6\u6784\u5efa\uff08\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\uff09\u3001\u6a21\u578b\u8bad\u7ec3\uff08\u77e5\u8bc6\u84b8\u998f\u3001\u52a8\u6001\u5fae\u8c03\u3001\u91cf\u5316\uff09\u548c\u90e8\u7f72\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9a7e\u9a76\u573a\u666f\u611f\u77e5\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005", "abs": "https://arxiv.org/abs/2506.02005", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408mBERT\u3001\u53cc\u5411LSTM\u548c\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982Konkani\uff09\u4e2d\u6bd4\u55bb\u8bed\u8a00\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6bd4\u55bb\u8bed\u8a00\u5bf9NLP\u7cfb\u7edf\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6a21\u578b\uff08mBERT+\u53cc\u5411LSTM+\u7ebf\u6027\u5206\u7c7b\u5668\uff09\uff0c\u5e76\u91c7\u7528\u68af\u5ea6\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\u3002", "result": "\u6bd4\u55bb\u5206\u7c7b\u51c6\u786e\u738778%\uff0c\u4e60\u8bed\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u738783%\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9ad8\u6548NLP\u5de5\u5177\u6784\u5efa\u6709\u6548\u3002"}}
{"id": "2506.03118", "pdf": "https://arxiv.org/pdf/2506.03118", "abs": "https://arxiv.org/abs/2506.03118", "authors": ["Zhiyuan Yu", "Zhe Li", "Hujun Bao", "Can Yang", "Xiaowei Zhou"], "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Track). Project page:\n  https://zju3dv.github.io/humanram/", "summary": "3D human reconstruction and animation are long-standing topics in computer\ngraphics and vision. However, existing methods typically rely on sophisticated\ndense-view capture and/or time-consuming per-subject optimization procedures.\nTo address these limitations, we propose HumanRAM, a novel feed-forward\napproach for generalizable human reconstruction and animation from monocular or\nsparse human images. Our approach integrates human reconstruction and animation\ninto a unified framework by introducing explicit pose conditions, parameterized\nby a shared SMPL-X neural texture, into transformer-based large reconstruction\nmodels (LRM). Given monocular or sparse input images with associated camera\nparameters and SMPL-X poses, our model employs scalable transformers and a\nDPT-based decoder to synthesize realistic human renderings under novel\nviewpoints and novel poses. By leveraging the explicit pose conditions, our\nmodel simultaneously enables high-quality human reconstruction and\nhigh-fidelity pose-controlled animation. Experiments show that HumanRAM\nsignificantly surpasses previous methods in terms of reconstruction accuracy,\nanimation fidelity, and generalization performance on real-world datasets.\nVideo results are available at https://zju3dv.github.io/humanram/.", "AI": {"tldr": "HumanRAM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u6216\u7a00\u758f\u56fe\u50cf\u7684\u53ef\u63a8\u5e7f\u4eba\u4f53\u91cd\u5efa\u4e0e\u52a8\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u59ff\u6001\u6761\u4ef6\u548c\u5171\u4eabSMPL-X\u795e\u7ecf\u7eb9\u7406\uff0c\u7ed3\u5408Transformer\u548cDPT\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u4e0e\u52a8\u753b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u89c6\u56fe\u6355\u83b7\u6216\u8017\u65f6\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002HumanRAM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u663e\u5f0f\u59ff\u6001\u6761\u4ef6\uff08SMPL-X\u795e\u7ecf\u7eb9\u7406\uff09\u548cTransformer\u6a21\u578b\uff0c\u5229\u7528DPT\u89e3\u7801\u5668\u5408\u6210\u65b0\u89c6\u89d2\u548c\u65b0\u59ff\u6001\u4e0b\u7684\u4eba\u4f53\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHumanRAM\u5728\u91cd\u5efa\u7cbe\u5ea6\u3001\u52a8\u753b\u903c\u771f\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HumanRAM\u4e3a\u5355\u76ee\u6216\u7a00\u758f\u56fe\u50cf\u7684\u4eba\u4f53\u91cd\u5efa\u4e0e\u52a8\u753b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02015", "pdf": "https://arxiv.org/pdf/2506.02015", "abs": "https://arxiv.org/abs/2506.02015", "authors": ["Yoonjin Oh", "Yongjin Kim", "Hyomin Kim", "Donghwan Chi", "Sungwoong Kim"], "title": "Object-centric Self-improving Preference Optimization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly improved both image understanding and generation capabilities.\nDespite these improvements, MLLMs still struggle with fine-grained visual\ncomprehension, particularly in text-to-image generation tasks. While preference\noptimization methods have been explored to address these limitations in image\nunderstanding tasks, their application to image generation remains largely\nunderexplored. To address this gap, we propose an Object-centric Self-improving\nPreference Optimization (OSPO) framework designed for text-to-image generation\nby MLLMs. OSPO leverages the intrinsic reasoning abilities of MLLMs without\nrequiring any external datasets or models. OSPO emphasizes the importance of\nhigh-quality preference pair data, which is critical for effective preference\noptimization. To achieve this, it introduces a self-improving mechanism that\nautonomously constructs object-level contrastive preference pairs through\nobject-centric prompt perturbation, densification and VQA scoring. This process\neliminates ambiguous or disproportionate variations commonly found in naively\ngenerated preference pairs, thereby enhancing the effectiveness of preference\noptimization. We validate OSPO on three representative compositional\ntext-to-image benchmarks, demonstrating substantial performance gains over\nbaseline models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOSPO\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u4e3b\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\u6570\u636e\u5b9e\u73b0\u4f18\u5316\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\uff08\u5c24\u5176\u662f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff09\u4e2d\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5728\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e2d\u5df2\u6709\u63a2\u7d22\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faOSPO\u6846\u67b6\uff0c\u5229\u7528MLLMs\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u6a21\u578b\u3002\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u63d0\u793a\u6270\u52a8\u3001\u5bc6\u96c6\u5316\u548cVQA\u8bc4\u5206\uff0c\u81ea\u4e3b\u6784\u5efa\u5bf9\u8c61\u7ea7\u5bf9\u6bd4\u504f\u597d\u5bf9\uff0c\u6d88\u9664\u6a21\u7cca\u6216\u4e0d\u5747\u8861\u7684\u53d8\u4f53\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86OSPO\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "OSPO\u901a\u8fc7\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\u6570\u636e\u7684\u81ea\u4e3b\u6784\u5efa\uff0c\u6709\u6548\u63d0\u5347\u4e86MLLMs\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u586b\u8865\u4e86\u504f\u597d\u4f18\u5316\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.02018", "pdf": "https://arxiv.org/pdf/2506.02018", "abs": "https://arxiv.org/abs/2506.02018", "authors": ["Christopher Lee L\u00fcbbers"], "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 11 figures. Master's thesis, University of Goettingen,\n  December 2025. Code: https://github.com/cluebbers/dpo-rlhf-paraphrase-types.\n  Models:\n  https://huggingface.co/collections/cluebbers/enhancing-paraphrase-type-generation-673ca8d75dfe2ce962a48ac0", "summary": "Paraphrasing re-expresses meaning to enhance applications like text\nsimplification, machine translation, and question-answering. Specific\nparaphrase types facilitate accurate semantic analysis and robust language\nmodels. However, existing paraphrase-type generation methods often misalign\nwith human preferences due to reliance on automated metrics and limited\nhuman-annotated training data, obscuring crucial aspects of semantic fidelity\nand linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type\ndataset and integrating Direct Preference Optimization (DPO) to align model\noutputs directly with human judgments. DPO-based training increases\nparaphrase-type generation accuracy by 3 percentage points over a supervised\nbaseline and raises human preference ratings by 7 percentage points. A newly\ncreated human-annotated dataset supports more rigorous future evaluations.\nAdditionally, a paraphrase-type detection model achieves F1 scores of 0.91 for\naddition/deletion, 0.78 for same polarity substitution, and 0.70 for\npunctuation changes.\n  These findings demonstrate that preference data and DPO training produce more\nreliable, semantically accurate paraphrases, enabling downstream applications\nsuch as improved summarization and more robust question-answering. The PTD\nmodel surpasses automated metrics and provides a more reliable framework for\nevaluating paraphrase quality, advancing paraphrase-type research toward\nricher, user-aligned language generation and establishing a stronger foundation\nfor future evaluations grounded in human-centric criteria.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6392\u540d\u6570\u636e\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6539\u5199\u7c7b\u578b\u7684\u751f\u6210\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\uff0c\u540c\u65f6\u521b\u5efa\u4e86\u65b0\u7684\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6539\u5199\u7c7b\u578b\u751f\u6210\u65b9\u6cd5\u56e0\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\u548c\u6709\u9650\u7684\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\uff0c\u5e38\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u8bed\u8a00\u8f6c\u6362\u7684\u5173\u952e\u65b9\u9762\u88ab\u5ffd\u89c6\u3002", "method": "\u5229\u7528\u4eba\u7c7b\u6392\u540d\u7684\u6539\u5199\u7c7b\u578b\u6570\u636e\u96c6\uff0c\u5e76\u6574\u5408DPO\u6280\u672f\uff0c\u76f4\u63a5\u5bf9\u9f50\u6a21\u578b\u8f93\u51fa\u4e0e\u4eba\u7c7b\u5224\u65ad\u3002", "result": "DPO\u8bad\u7ec3\u5c06\u6539\u5199\u7c7b\u578b\u751f\u6210\u51c6\u786e\u6027\u63d0\u9ad8\u4e863\u4e2a\u767e\u5206\u70b9\uff0c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u63d0\u9ad8\u4e867\u4e2a\u767e\u5206\u70b9\uff1b\u6539\u5199\u7c7b\u578b\u68c0\u6d4b\u6a21\u578b\u7684F1\u5206\u6570\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u504f\u597d\u6570\u636e\u548cDPO\u8bad\u7ec3\u80fd\u751f\u6210\u66f4\u53ef\u9760\u3001\u8bed\u4e49\u51c6\u786e\u7684\u6539\u5199\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\uff08\u5982\u6458\u8981\u548c\u95ee\u7b54\uff09\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u63a8\u52a8\u6539\u5199\u7c7b\u578b\u7814\u7a76\u5411\u66f4\u7528\u6237\u5bf9\u9f50\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86EyeNavGS\uff0c\u9996\u4e2a\u516c\u5f00\u76846-DoF\u5bfc\u822a\u6570\u636e\u96c6\uff0c\u5305\u542b46\u540d\u53c2\u4e0e\u8005\u572812\u4e2a\u771f\u5b9e\u4e16\u754c3DGS\u573a\u666f\u4e2d\u7684\u5bfc\u822a\u6570\u636e\uff0c\u7528\u4e8e\u652f\u63016-DoF\u89c6\u53e3\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u6d41\u5a92\u4f53\u7b49\u7814\u7a76\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u9ad8\u4fdd\u771f3DGS\u573a\u666f\u7684\u771f\u5b9e\u7528\u6237\u5bfc\u822a\u6570\u636e\uff0c\u9650\u5236\u4e86\u76f8\u5173\u5e94\u7528\u5f00\u53d1\u548c\u6e32\u67d3\u6027\u80fd\u4f18\u5316\u3002", "method": "\u4f7f\u7528Meta Quest Pro\u5934\u663e\u8bb0\u5f5546\u540d\u53c2\u4e0e\u8005\u572812\u4e2a3DGS\u573a\u666f\u4e2d\u7684\u5934\u90e8\u59ff\u6001\u548c\u773c\u52a8\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u573a\u666f\u521d\u59cb\u5316\u548c\u6570\u636e\u540e\u5904\u7406\u3002", "result": "\u53d1\u5e03\u4e86EyeNavGS\u6570\u636e\u96c6\u53ca\u914d\u5957\u5f00\u6e90\u5de5\u5177\uff0c\u652f\u63016-DoF\u5bfc\u822a\u7814\u7a76\u3002", "conclusion": "EyeNavGS\u4e3a3DGS\u573a\u666f\u7684\u89c6\u53e3\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u6d41\u5a92\u4f53\u7b49\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2506.02016", "pdf": "https://arxiv.org/pdf/2506.02016", "abs": "https://arxiv.org/abs/2506.02016", "authors": ["Nuolin Sun", "Linyuan Wang", "Dongyang Li", "Bin Yan", "Lei Li"], "title": "Are classical deep neural networks weakly adversarially robust?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial attacks have received increasing attention and it has been widely\nrecognized that classical DNNs have weak adversarial robustness. The most\ncommonly used adversarial defense method, adversarial training, improves the\nadversarial accuracy of DNNs by generating adversarial examples and retraining\nthe model. However, adversarial training requires a significant computational\noverhead. In this paper, inspired by existing studies focusing on the\nclustering properties of DNN output features at each layer and the Progressive\nFeedforward Collapse phenomenon, we propose a method for adversarial example\ndetection and image recognition that uses layer-wise features to construct\nfeature paths and computes the correlation between the examples feature paths\nand the class-centered feature paths. Experimental results show that the\nrecognition method achieves 82.77% clean accuracy and 44.17% adversarial\naccuracy on the ResNet-20 with PFC. Compared to the adversarial training method\nwith 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits\na trade-off without relying on computationally expensive defense strategies.\nFurthermore, on the standard ResNet-18, our method maintains this advantage\nwith respective metrics of 80.01% and 46.1%. This result reveals inherent\nadversarial robustness in DNNs, challenging the conventional understanding of\nthe weak adversarial robustness in DNNs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c42\u7279\u5f81\u8def\u5f84\u7684\u5bf9\u6297\u6837\u672c\u68c0\u6d4b\u4e0e\u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\uff0c\u907f\u514d\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u5bf9\u6297\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edfDNN\u5bf9\u6297\u9c81\u68d2\u6027\u5f31\uff0c\u5bf9\u6297\u8bad\u7ec3\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5c42\u7279\u5f81\u6784\u5efa\u7279\u5f81\u8def\u5f84\uff0c\u8ba1\u7b97\u5176\u4e0e\u7c7b\u4e2d\u5fc3\u7279\u5f81\u8def\u5f84\u7684\u76f8\u5173\u6027\u3002", "result": "\u5728ResNet-20\u4e0a\uff0c\u5e72\u51c0\u51c6\u786e\u738782.77%\uff0c\u5bf9\u6297\u51c6\u786e\u738744.17%\uff1bResNet-18\u4e0a\u5206\u522b\u4e3a80.01%\u548c46.1%\u3002", "conclusion": "\u65b9\u6cd5\u63ed\u793aDNN\u56fa\u6709\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u77e5\uff0c\u63d0\u4f9b\u9ad8\u6548\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019", "abs": "https://arxiv.org/abs/2506.02019", "authors": ["E Fan", "Weizong Wang", "Tianhan Zhang"], "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking", "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for scientific and\nengineering advancements but is limited by operational complexity and the need\nfor extensive expertise. This paper presents ChatCFD, a large language\nmodel-driven pipeline that automates CFD workflows within the OpenFOAM\nframework. It enables users to configure and execute complex simulations from\nnatural language prompts or published literature with minimal expertise. The\ninnovation is its structured approach to database construction, configuration\nvalidation, and error reflection, integrating CFD and OpenFOAM knowledge with\ngeneral language models to improve accuracy and adaptability. Validation shows\nChatCFD can autonomously reproduce published CFD results, handling complex,\nunseen configurations beyond basic examples, a task challenging for general\nlanguage models.", "AI": {"tldr": "ChatCFD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316CFD\u5de5\u4f5c\u6d41\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6216\u6587\u732e\u914d\u7f6e\u6a21\u62df\uff0c\u964d\u4f4e\u4e86\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "motivation": "CFD\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u64cd\u4f5c\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0cChatCFD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528OpenFOAM\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u5e93\u6784\u5efa\u3001\u914d\u7f6e\u9a8c\u8bc1\u548c\u9519\u8bef\u53cd\u601d\uff0c\u5c06CFD\u77e5\u8bc6\u4e0e\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u3002", "result": "\u9a8c\u8bc1\u8868\u660eChatCFD\u80fd\u81ea\u4e3b\u590d\u73b0\u590d\u6742CFD\u7ed3\u679c\uff0c\u8d85\u8d8a\u901a\u7528\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "conclusion": "ChatCFD\u4e3aCFD\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6613\u7528\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u975e\u4e13\u5bb6\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.02661", "pdf": "https://arxiv.org/pdf/2506.02661", "abs": "https://arxiv.org/abs/2506.02661", "authors": ["Mingyang Huang", "Peng Zhang", "Bang Zhang"], "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": "12 pages, 5 figures", "summary": "Generating long-term, coherent, and realistic music-conditioned dance\nsequences remains a challenging task in human motion synthesis. Existing\napproaches exhibit critical limitations: motion graph methods rely on fixed\ntemplate libraries, restricting creative generation; diffusion models, while\ncapable of producing novel motions, often lack temporal coherence and musical\nalignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a\nhybrid framework that integrates Retrieval-Augmented Generation (RAG) with\ndiffusion-based refinement to enable high-quality, musically coherent dance\ngeneration for arbitrary long-term music inputs. Our method introduces three\ncore innovations: (1) A cross-modal contrastive learning architecture that\naligns heterogeneous music and dance representations in a shared latent space,\nestablishing unsupervised semantic correspondence without paired data; (2) An\noptimized motion graph system for efficient retrieval and seamless\nconcatenation of motion segments, ensuring realism and temporal coherence\nacross long sequences; (3) A multi-condition diffusion model that jointly\nconditions on raw music signals and contrastive features to enhance motion\nquality and global synchronization. Extensive experiments demonstrate that\nMotionRAG-Diff achieves state-of-the-art performance in motion quality,\ndiversity, and music-motion synchronization accuracy. This work establishes a\nnew paradigm for music-driven dance generation by synergizing retrieval-based\ntemplate fidelity with diffusion-based creative enhancement.", "AI": {"tldr": "MotionRAG-Diff\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u8d28\u91cf\u3001\u97f3\u4e50\u8fde\u8d2f\u7684\u821e\u8e48\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u8fde\u8d2f\u6027\u548c\u97f3\u4e50\u5bf9\u9f50\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u821e\u8e48\u751f\u6210\u65b9\u6cd5\uff08\u5982\u8fd0\u52a8\u56fe\u548c\u6269\u6563\u6a21\u578b\uff09\u5728\u957f\u671f\u8fde\u8d2f\u6027\u548c\u97f3\u4e50\u5bf9\u9f50\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "1. \u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u97f3\u4e50\u4e0e\u821e\u8e48\u8868\u793a\uff1b2. \u4f18\u5316\u8fd0\u52a8\u56fe\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u4e0e\u62fc\u63a5\uff1b3. \u591a\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8054\u5408\u4f18\u5316\u97f3\u4e50\u4fe1\u53f7\u4e0e\u5bf9\u6bd4\u7279\u5f81\u3002", "result": "MotionRAG-Diff\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u97f3\u4e50-\u8fd0\u52a8\u540c\u6b65\u51c6\u786e\u6027\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u6a21\u677f\u7684\u4fdd\u771f\u5ea6\u4e0e\u6269\u6563\u6a21\u578b\u7684\u521b\u9020\u529b\uff0c\u4e3a\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.02017", "pdf": "https://arxiv.org/pdf/2506.02017", "abs": "https://arxiv.org/abs/2506.02017", "authors": ["Camilla Quaresmini", "Giacomo Zanotti"], "title": "Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Automatic Gender Recognition (AGR) systems are an increasingly widespread\napplication in the Machine Learning (ML) landscape. While these systems are\ntypically understood as detecting gender, they often classify datapoints based\non observable features correlated at best with either male or female sex. In\naddition to questionable binary assumptions, from an epistemological point of\nview, this is problematic for two reasons. First, there exists a gap between\nthe categories the system is meant to predict (woman versus man) and those onto\nwhich their output reasonably maps (female versus male). What is more, gender\ncannot be inferred on the basis of such observable features. This makes AGR\ntools often unreliable, especially in the case of non-binary and gender\nnon-conforming people. We suggest a theoretical and practical rethinking of AGR\nsystems. To begin, distinctions are made between sex, gender, and gender\nexpression. Then, we build upon the observation that, unlike algorithmic\nmisgendering, human-human misgendering is open to the possibility of\nre-evaluation and correction. We suggest that analogous dynamics should be\nrecreated in AGR, giving users the possibility to correct the system's output.\nWhile implementing such a feedback mechanism could be regarded as diminishing\nthe system's autonomy, it represents a way to significantly increase fairness\nlevels in AGR. This is consistent with the conceptual change of paradigm that\nwe advocate for AGR systems, which should be understood as tools respecting\nindividuals' rights and capabilities of self-expression and determination.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u6027\u522b\u8bc6\u522b\uff08AGR\uff09\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u91cd\u65b0\u601d\u8003\u5176\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u6846\u67b6\uff0c\u5efa\u8bae\u901a\u8fc7\u7528\u6237\u53cd\u9988\u673a\u5236\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "motivation": "AGR\u7cfb\u7edf\u901a\u5e38\u57fa\u4e8e\u6027\u522b\u4e8c\u5143\u5047\u8bbe\uff0c\u4e14\u5206\u7c7b\u7ed3\u679c\u4e0e\u6027\u522b\u8868\u8fbe\u5b58\u5728\u5dee\u8ddd\uff0c\u5bf9\u975e\u4e8c\u5143\u6027\u522b\u8005\u4e0d\u53cb\u597d\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u533a\u5206\u6027\u522b\u3001\u6027\u522b\u8868\u8fbe\u4e0e\u751f\u7406\u6027\u522b\uff0c\u63d0\u51fa\u901a\u8fc7\u7528\u6237\u53cd\u9988\u673a\u5236\u4fee\u6b63\u7cfb\u7edf\u8f93\u51fa\u3002", "result": "\u53cd\u9988\u673a\u5236\u867d\u964d\u4f4e\u7cfb\u7edf\u81ea\u4e3b\u6027\uff0c\u4f46\u80fd\u663e\u8457\u63d0\u5347AGR\u7684\u516c\u5e73\u6027\u3002", "conclusion": "AGR\u7cfb\u7edf\u5e94\u5c0a\u91cd\u4e2a\u4f53\u6743\u5229\u4e0e\u81ea\u6211\u8868\u8fbe\uff0c\u5efa\u8bae\u5c06\u5176\u89c6\u4e3a\u652f\u6301\u5de5\u5177\u800c\u975e\u7edd\u5bf9\u5206\u7c7b\u5668\u3002"}}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037", "abs": "https://arxiv.org/abs/2506.02037", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "title": "FinS-Pilot: A Benchmark for Online Financial System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.", "AI": {"tldr": "FinS-Pilot\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u7ebf\u91d1\u878d\u5e94\u7528RAG\u7cfb\u7edf\u8bc4\u4f30\u7684\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4fdd\u5bc6\u548c\u52a8\u6001\u6570\u636e\u6574\u5408\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9e\u91d1\u878d\u52a9\u624b\u4ea4\u4e92\u6784\u5efa\u3002", "motivation": "\u73b0\u6709\u91d1\u878dRAG\u57fa\u51c6\u56e0\u6570\u636e\u4fdd\u5bc6\u548c\u7f3a\u4e4f\u52a8\u6001\u6570\u636e\u6574\u5408\u53d7\u9650\uff0c\u9700\u4e13\u95e8\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u91d1\u878d\u52a9\u624b\u4ea4\u4e92\uff0c\u6574\u5408\u5b9e\u65f6API\u6570\u636e\u548c\u7ed3\u6784\u5316\u6587\u672c\uff0c\u901a\u8fc7\u610f\u56fe\u5206\u7c7b\u6846\u67b6\u7ec4\u7ec7\u3002", "result": "FinS-Pilot\u80fd\u5168\u9762\u8bc4\u4f30\u91d1\u878d\u52a9\u624b\u5904\u7406\u9759\u6001\u77e5\u8bc6\u548c\u5b9e\u65f6\u5e02\u573a\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u9002\u5408\u91d1\u878d\u5e94\u7528\u7684LLM\u3002", "conclusion": "FinS-Pilot\u586b\u8865\u4e86\u91d1\u878d\u9886\u57df\u4e13\u7528\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.03099", "pdf": "https://arxiv.org/pdf/2506.03099", "abs": "https://arxiv.org/abs/2506.03099", "authors": ["Chetwin Low", "Weimin Wang"], "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models", "categories": ["cs.SD", "cs.AI", "cs.GR"], "comment": null, "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/", "AI": {"tldr": "TalkingMachines\u662f\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u5316\u4e3a\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u751f\u6210\u5668\uff0c\u7ed3\u5408\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u7136\u5bf9\u8bdd\u4f53\u9a8c\u3002", "motivation": "\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u7684\u7cfb\u7edf\uff0c\u63d0\u5347\u5bf9\u8bdd\u4f53\u9a8c\u7684\u81ea\u7136\u6027\u548c\u6548\u7387\u3002", "method": "1. \u5c06\u9884\u8bad\u7ec3\u7684SOTA\u56fe\u50cf\u5230\u89c6\u9891DiT\u6a21\u578b\u9002\u914d\u4e3a\u97f3\u9891\u9a71\u52a8\u768418B\u53c2\u6570\u89d2\u8272\u751f\u6210\u6a21\u578b\uff1b2. \u901a\u8fc7\u975e\u5bf9\u79f0\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u65e0\u9650\u89c6\u9891\u6d41\u751f\u6210\uff1b3. \u8bbe\u8ba1\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7ba1\u9053\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86TalkingMachines\u6846\u67b6\uff0c\u652f\u6301\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u751f\u6210\u3002", "conclusion": "TalkingMachines\u901a\u8fc7\u6280\u672f\u521b\u65b0\u548c\u5de5\u7a0b\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u89d2\u8272\u52a8\u753b\u751f\u6210\uff0c\u4e3a\u5bf9\u8bdd\u4f53\u9a8c\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.02020", "pdf": "https://arxiv.org/pdf/2506.02020", "abs": "https://arxiv.org/abs/2506.02020", "authors": ["Youze Xue", "Dian Li", "Gang Liu"], "title": "Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the rapid advancement of multi-modal large language models (MLLMs) in\nrecent years, the foundational Contrastive Language-Image Pretraining (CLIP)\nframework has been successfully extended to MLLMs, enabling more powerful and\nuniversal multi-modal embeddings for a wide range of retrieval tasks. Despite\nthese developments, the core contrastive learning paradigm remains largely\nunchanged from CLIP-style models to MLLMs. Within this framework, the effective\nmining of hard negative samples continues to be a critical factor for enhancing\nperformance. Prior works have introduced both offline and online strategies for\nhard negative mining to improve the efficiency of contrastive learning. While\nthese approaches have led to improved multi-modal embeddings, the specific\ncontribution of each hard negative sample to the learning process has not been\nthoroughly investigated. In this work, we conduct a detailed analysis of the\ngradients of the info-NCE loss with respect to the query, positive, and\nnegative samples, elucidating the role of hard negatives in updating model\nparameters. Building upon this analysis, we propose to explicitly amplify the\ngradients associated with hard negative samples, thereby encouraging the model\nto learn more discriminative embeddings. Our multi-modal embedding model,\ntrained with the proposed Explicit Gradient Amplifier and based on the\nLLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the\nMMEB benchmark compared to previous methods utilizing the same MLLM backbone.\nFurthermore, when integrated with our self-developed MLLM, QQMM, our approach\nattains the top rank on the MMEB leaderboard. Code and models are released on\nhttps://github.com/QQ-MM/QQMM-embed.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u786c\u8d1f\u6837\u672c\u5bf9\u5bf9\u6bd4\u5b66\u4e60\u7684\u8d21\u732e\uff0c\u63d0\u51fa\u663e\u5f0f\u68af\u5ea6\u653e\u5927\u5668\u4ee5\u589e\u5f3a\u786c\u8d1f\u6837\u672c\u7684\u68af\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u5d4c\u5165\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1CLIP\u6846\u67b6\u5df2\u6210\u529f\u6269\u5c55\u5230MLLMs\uff0c\u4f46\u786c\u8d1f\u6837\u672c\u7684\u5177\u4f53\u8d21\u732e\u672a\u88ab\u6df1\u5165\u7814\u7a76\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u5206\u6790info-NCE\u635f\u5931\u7684\u68af\u5ea6\uff0c\u63d0\u51fa\u663e\u5f0f\u68af\u5ea6\u653e\u5927\u5668\uff0c\u589e\u5f3a\u786c\u8d1f\u6837\u672c\u7684\u68af\u5ea6\uff0c\u57fa\u4e8eLLaVA-OneVision-7B\u67b6\u6784\u8bad\u7ec3\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u3002", "result": "\u5728MMEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u7ed3\u5408\u81ea\u7814MLLM\uff08QQMM\uff09\u540e\u767b\u4e0aMMEB\u6392\u884c\u699c\u9996\u4f4d\u3002", "conclusion": "\u663e\u5f0f\u68af\u5ea6\u653e\u5927\u5668\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02041", "pdf": "https://arxiv.org/pdf/2506.02041", "abs": "https://arxiv.org/abs/2506.02041", "authors": ["Duzhen Zhang", "Yong Ren", "Zhong-Zhi Li", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Zhilong Ji", "Jinfeng Bai"], "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal\nLarge Language Models (MLLMs) to continually align with human intent across\nsequential tasks. Existing approaches often rely on the Mixture-of-Experts\n(MoE) LoRA framework to preserve previous instruction alignments. However,\nthese methods are prone to Catastrophic Forgetting (CF), as they aggregate all\nLoRA blocks via simple summation, which compromises performance over time. In\nthis paper, we identify a critical parameter inefficiency in the MoELoRA\nframework within the MCIT context. Based on this insight, we propose\nBranchLoRA, an asymmetric framework to enhance both efficiency and performance.\nTo mitigate CF, we introduce a flexible tuning-freezing mechanism within\nBranchLoRA, enabling branches to specialize in intra-task knowledge while\nfostering inter-task collaboration. Moreover, we incrementally incorporate\ntask-specific routers to ensure an optimal branch distribution over time,\nrather than favoring the most recent task. To streamline inference, we\nintroduce a task selector that automatically routes test inputs to the\nappropriate router without requiring task identity. Extensive experiments on\nthe latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms\nMoELoRA and maintains its superiority across various MLLM sizes.", "AI": {"tldr": "BranchLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u5bf9\u79f0\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u8c03\u4f18-\u51bb\u7ed3\u673a\u5236\u548c\u4efb\u52a1\u7279\u5b9a\u8def\u7531\u5668\uff0c\u89e3\u51b3\u4e86MCIT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982MoELoRA\uff09\u5728\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u56e0\u7b80\u5355\u805a\u5408LoRA\u5757\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\uff0cBranchLoRA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faBranchLoRA\u6846\u67b6\uff0c\u91c7\u7528\u8c03\u4f18-\u51bb\u7ed3\u673a\u5236\u548c\u4efb\u52a1\u7279\u5b9a\u8def\u7531\u5668\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u9009\u62e9\u5668\u4ee5\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728MCIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBranchLoRA\u663e\u8457\u4f18\u4e8eMoELoRA\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u7684MLLM\u4e2d\u4fdd\u6301\u4f18\u52bf\u3002", "conclusion": "BranchLoRA\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u6548\u7387\u95ee\u9898\uff0c\u4e3aMCIT\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02021", "pdf": "https://arxiv.org/pdf/2506.02021", "abs": "https://arxiv.org/abs/2506.02021", "authors": ["Yinjie Zhao", "Heng Zhao", "Bihan Wen", "Yew-Soon Ong", "Joey Tianyi Zhou"], "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of vision tasks and the scaling on datasets and\nmodels, redundancy reduction in vision datasets has become a key area of\nresearch. To address this issue, dataset distillation (DD) has emerged as a\npromising approach to generating highly compact synthetic datasets with\nsignificantly less redundancy while preserving essential information. However,\nwhile DD has been extensively studied for image datasets, DD on video datasets\nremains underexplored. Video datasets present unique challenges due to the\npresence of temporal information and varying levels of redundancy across\ndifferent classes. Existing DD approaches assume a uniform level of temporal\nredundancy across all different video semantics, which limits their\neffectiveness on video datasets. In this work, we propose Dynamic-Aware Video\nDistillation (DAViD), a Reinforcement Learning (RL) approach to predict the\noptimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop\nreward function is proposed to update the RL agent policy. To the best of our\nknowledge, this is the first study to introduce adaptive temporal resolution\nbased on video semantics in video dataset distillation. Our approach\nsignificantly outperforms existing DD methods, demonstrating substantial\nimprovements in performance. This work paves the way for future research on\nmore efficient and semantic-adaptive video dataset distillation research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u611f\u77e5\u89c6\u9891\u84b8\u998f\u65b9\u6cd5\uff08DAViD\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u7684\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u548c\u5197\u4f59\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u89c6\u9891\u8bed\u4e49\u5177\u6709\u7edf\u4e00\u7684\u65f6\u95f4\u5197\u4f59\uff0c\u9650\u5236\u4e86\u6548\u679c\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u9884\u6d4b\u5408\u6210\u89c6\u9891\u7684\u6700\u4f73\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5e76\u63d0\u51fa\u6559\u5e08\u5faa\u73af\u5956\u52b1\u51fd\u6570\u66f4\u65b0RL\u4ee3\u7406\u7b56\u7565\u3002", "result": "DAViD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u66f4\u9ad8\u6548\u548c\u8bed\u4e49\u81ea\u9002\u5e94\u7684\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.02058", "pdf": "https://arxiv.org/pdf/2506.02058", "abs": "https://arxiv.org/abs/2506.02058", "authors": ["Xiang Li", "Jiayi Xin", "Qi Long", "Weijie J. Su"], "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKnowSum\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u672a\u89c2\u5bdf\u5230\u7684\u77e5\u8bc6\u6765\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u63ed\u793a\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u4e86\u5927\u91cf\u6f5c\u5728\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u53cd\u6620LLMs\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u5c24\u5176\u662f\u672a\u89c2\u5bdf\u5230\u7684\u77e5\u8bc6\u90e8\u5206\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u5f15\u5165KnowSum\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u63a8\u89c2\u5bdf\u5230\u7684\u77e5\u8bc6\u5b9e\u4f8b\u9891\u7387\u6765\u4f30\u8ba1\u672a\u89c2\u5bdf\u5230\u7684\u77e5\u8bc6\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKnowSum\u5728\u4f30\u8ba1\u603b\u77e5\u8bc6\u3001\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u6548\u679c\u548c\u6d4b\u91cf\u8f93\u51fa\u591a\u6837\u6027\u65b9\u9762\u6709\u6548\uff0c\u4e14\u663e\u8457\u6539\u53d8\u4e86\u5e38\u89c1LLMs\u7684\u6392\u540d\u3002", "conclusion": "KnowSum\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u672a\u89c2\u5bdf\u77e5\u8bc6\u5bf9\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.02022", "pdf": "https://arxiv.org/pdf/2506.02022", "abs": "https://arxiv.org/abs/2506.02022", "authors": ["Aditya Kanade", "Tanuja Ganu"], "title": "Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show reasoning promise, yet their\nvisual perception is a critical bottleneck. Strikingly, MLLMs can produce\ncorrect answers even while misinterpreting crucial visual elements, masking\nthese underlying failures. Our preliminary study on a joint\nperception-reasoning dataset revealed that for one leading MLLM, 29% of its\ncorrect answers to reasoning questions still exhibited visual perception\nerrors. To systematically address this, we introduce \"Do You See Me\", a\nscalable benchmark with 1,758 images and 2,612 questions. It spans seven\nhuman-psychology inspired subtasks in 2D and 3D, featuring controllable\ncomplexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading\nclosed-source and 5 major open-source models reveal a stark deficit: humans\nachieve 96.49% accuracy, while top MLLMs average below 50%. This performance\ngap widens rapidly with increased task complexity (e.g., from 12% to 45% in the\nvisual form constancy subtask). Further analysis into the root causes suggests\nthat failures stem from challenges like misallocated visual attention and the\ninstability of internal representations for fine-grained details, especially at\nor below encoder patch resolution. This underscores an urgent need for MLLMs\nwith truly robust visual perception. The benchmark dataset, source code and\nevaluation scripts are available at https://github.com/microsoft/Do-You-See-Me.", "AI": {"tldr": "MLLMs\u5b58\u5728\u89c6\u89c9\u611f\u77e5\u7f3a\u9677\uff0c\u5373\u4f7f\u7b54\u6848\u6b63\u786e\u4e5f\u53ef\u80fd\u8bef\u89e3\u5173\u952e\u89c6\u89c9\u5143\u7d20\u3002\u7814\u7a76\u63d0\u51fa\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0MLLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u63ed\u793aMLLMs\u5728\u89c6\u89c9\u611f\u77e5\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63a8\u52a8\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002", "method": "\u521b\u5efa\u5305\u542b1,758\u5f20\u56fe\u50cf\u548c2,612\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d67\u4e2a\u5b50\u4efb\u52a1\uff0c\u8bc4\u4f30MLLMs\u7684\u89c6\u89c9\u80fd\u529b\u3002", "result": "\u4eba\u7c7b\u51c6\u786e\u738796.49%\uff0c\u800c\u9876\u7ea7MLLMs\u5e73\u5747\u4f4e\u4e8e50%\uff0c\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u5dee\u8ddd\u66f4\u5927\u3002", "conclusion": "MLLMs\u9700\u6539\u8fdb\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u7c92\u5ea6\u7ec6\u8282\u5904\u7406\u4e0a\u3002"}}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126", "abs": "https://arxiv.org/abs/2506.02126", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u7406\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u548c\u6570\u5b66\u9886\u57df\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u51fa\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\u548c\u900f\u660e\u5ea6\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5c06\u63a8\u7406\u8f68\u8ff9\u5206\u89e3\u4e3a\u77e5\u8bc6\u548c\u63a8\u7406\u4e24\u90e8\u5206\uff0c\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\uff08\u77e5\u8bc6\u6307\u6570\u548c\u4fe1\u606f\u589e\u76ca\uff09\uff0c\u5e76\u5206\u6790SFT\u548cRL\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u533b\u5b66\u548c\u6570\u5b66\u9886\u57df\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0SFT\u63d0\u9ad8\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4f46\u964d\u4f4e\u63a8\u7406\u8d28\u91cf\uff0c\u800cRL\u5728\u533b\u5b66\u9886\u57df\u901a\u8fc7\u4fee\u526a\u4e0d\u51c6\u786e\u77e5\u8bc6\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SFT\u5728\u533b\u5b66\u9886\u57df\u4e0d\u53ef\u6216\u7f3a\uff0c\u800cRL\u80fd\u4f18\u5316\u63a8\u7406\u8def\u5f84\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.02095", "pdf": "https://arxiv.org/pdf/2506.02095", "abs": "https://arxiv.org/abs/2506.02095", "authors": ["Hyojin Bahng", "Caroline Chan", "Fredo Durand", "Phillip Isola"], "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning alignment between language and vision is a fundamental challenge,\nespecially as multimodal data becomes increasingly detailed and complex.\nExisting methods often rely on collecting human or AI preferences, which can be\ncostly and time-intensive. We propose an alternative approach that leverages\ncycle consistency as a supervisory signal. Given an image and generated text,\nwe map the text back to image space using a text-to-image model and compute the\nsimilarity between the original image and its reconstruction. Analogously, for\ntext-to-image generation, we measure the textual similarity between an input\ncaption and its reconstruction through the cycle. We use the cycle consistency\nscore to rank candidates and construct a preference dataset of 866K comparison\npairs. The reward model trained on our dataset outperforms state-of-the-art\nalignment metrics on detailed captioning, with superior inference-time\nscalability when used as a verifier for Best-of-N sampling. Furthermore,\nperforming DPO and Diffusion DPO using our dataset enhances performance across\na wide range of vision-language tasks and text-to-image generation. Our\ndataset, model, and code are at https://cyclereward.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5faa\u73af\u4e00\u81f4\u6027\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u6587\u672c\u7684\u53cc\u5411\u6620\u5c04\u8ba1\u7b97\u76f8\u4f3c\u6027\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b866K\u6bd4\u8f83\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6216AI\u504f\u597d\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u8bed\u8a00\u4e0e\u89c6\u89c9\u7684\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u6587\u672c\u7684\u53cc\u5411\u6620\u5c04\u8ba1\u7b97\u5faa\u73af\u4e00\u81f4\u6027\u5f97\u5206\uff0c\u7528\u4e8e\u6392\u5e8f\u5019\u9009\u5e76\u6784\u5efa\u504f\u597d\u6570\u636e\u96c6\u3002", "result": "\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u5728\u8be6\u7ec6\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u5bf9\u9f50\u6307\u6807\uff0c\u5e76\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u5faa\u73af\u4e00\u81f4\u6027\u662f\u4e00\u79cd\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132", "abs": "https://arxiv.org/abs/2506.02132", "authors": ["Michael Li", "Nishant Subramani"], "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u3001GPT-2\u3001Pythia\u7b49\uff09\u5982\u4f55\u7f16\u7801\u8bcd\u6c47\u548c\u5f62\u6001\u4fe1\u606f\uff0c\u53d1\u73b0\u8bcd\u6c47\u4fe1\u606f\u5728\u65e9\u671f\u5c42\u7ebf\u6027\u96c6\u4e2d\uff0c\u540e\u671f\u5c42\u975e\u7ebf\u6027\u96c6\u4e2d\uff0c\u800c\u5f62\u6001\u4fe1\u606f\u5728\u5404\u5c42\u5747\u7ebf\u6027\u53ef\u5206\u79bb\u3002", "motivation": "\u7406\u89e3\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u8bed\u8a00\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u8bcd\u6c47\u548c\u5f62\u6001\u5b66\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u5206\u6790\u6a21\u578b\u5404\u5c42\u6fc0\u6d3b\u4ee5\u9884\u6d4b\u8bcd\u6c47\u8bcd\u6839\u548c\u5f62\u6001\u7279\u5f81\u3002", "result": "\u8bcd\u6c47\u4fe1\u606f\u5728\u65e9\u671f\u5c42\u7ebf\u6027\u96c6\u4e2d\uff0c\u540e\u671f\u5c42\u975e\u7ebf\u6027\u96c6\u4e2d\uff1b\u5f62\u6001\u4fe1\u606f\u5728\u5404\u5c42\u5747\u7ebf\u6027\u53ef\u5206\u79bb\u4e14\u901a\u8fc7\u62bd\u8c61\u5316\u7f16\u7801\u3002", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u4e0d\u540c\uff0c\u4f46\u8bed\u8a00\u4fe1\u606f\u7684\u7ec4\u7ec7\u65b9\u5f0f\u76f8\u4f3c\uff0c\u8868\u660e\u8fd9\u4e9b\u7279\u6027\u53ef\u80fd\u662f\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u3002"}}
{"id": "2506.02112", "pdf": "https://arxiv.org/pdf/2506.02112", "abs": "https://arxiv.org/abs/2506.02112", "authors": ["Xuweiyi Chen", "Tian Xia", "Sihan Xu", "Jianing Yang", "Joyce Chai", "Zezhou Cheng"], "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://uva-computer-vision-lab.github.io/sab3r/", "summary": "We introduce a new task, Map and Locate, which unifies the traditionally\ndistinct objectives of open-vocabulary segmentation - detecting and segmenting\nobject instances based on natural language queries - and 3D reconstruction, the\nprocess of estimating a scene's 3D structure from visual inputs. Specifically,\nMap and Locate involves generating a point cloud from an unposed video and\nsegmenting object instances based on open-vocabulary queries. This task serves\nas a critical step toward real-world embodied AI applications and introduces a\npractical task that bridges reconstruction, recognition and reorganization. To\ntackle this task, we introduce a simple yet effective baseline, which we denote\nas SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer\nvision, and incorporates a lightweight distillation strategy. This method\ntransfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP\nand DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary\nfrozen networks, our model generates per-pixel semantic features and constructs\ncohesive point maps in a single forward pass. Compared to separately deploying\nMASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the\nMap and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic\nsegmentation and 3D tasks to comprehensively validate its effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u4efb\u52a1Map and Locate\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e0e3D\u91cd\u5efa\uff0c\u5e76\u4ecb\u7ecd\u57fa\u7ebf\u65b9\u6cd5SAB3R\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7edf\u4e00\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u548c3D\u91cd\u5efa\u4efb\u52a1\uff0c\u63a8\u52a8\u5177\u8eabAI\u5e94\u7528\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eMASt3R\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u84b8\u998f\u7b56\u7565\uff0c\u5c062D\u89c6\u89c9\u4e3b\u5e72\u7279\u5f81\u8fc1\u79fb\u81f33D\u6a21\u578b\uff0c\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u751f\u6210\u8bed\u4e49\u7279\u5f81\u548c\u70b9\u4e91\u3002", "result": "SAB3R\u5728Map and Locate\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eMASt3R\u548cCLIP\u7ec4\u5408\uff0c\u5e76\u57282D\u548c3D\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "conclusion": "SAB3R\u4e3a\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e0e3D\u91cd\u5efa\u7684\u7edf\u4e00\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.02147", "pdf": "https://arxiv.org/pdf/2506.02147", "abs": "https://arxiv.org/abs/2506.02147", "authors": ["Joshua Rozner", "Leonie Weissweiler", "Cory Shain"], "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning", "categories": ["cs.CL"], "comment": null, "summary": "Construction grammar posits that children acquire constructions (form-meaning\npairings) from the statistics of their environment. Recent work supports this\nhypothesis by showing sensitivity to constructions in pretrained language\nmodels (PLMs), including one recent study (Rozner et al., 2025) demonstrating\nthat constructions shape the PLM's output distribution. However, models under\nstudy have generally been trained on developmentally implausible amounts of\ndata, casting doubt on their relevance to human language learning. Here we use\nRozner et al.'s methods to evaluate constructional learning in models from the\n2024 BabyLM challenge. Our results show that even when trained on\ndevelopmentally plausible quantities of data, models represent diverse\nconstructions, even hard cases that are superficially indistinguishable. We\nfurther find correlational evidence that constructional performance may be\nfunctionally relevant: models that better represent constructions perform\nbetter on the BabyLM benchmarks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u91cf\u7b26\u5408\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u7684\u53d1\u5c55\u6c34\u5e73\uff0c\u8bed\u8a00\u6a21\u578b\u4ecd\u80fd\u5b66\u4e60\u5e76\u8868\u73b0\u591a\u6837\u5316\u7684\u6784\u5f0f\uff0c\u4e14\u6784\u5f0f\u8868\u73b0\u4e0e\u6a21\u578b\u6027\u80fd\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u5408\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u6570\u636e\u91cf\u7684\u60c5\u51b5\u4e0b\u662f\u5426\u4ecd\u80fd\u5b66\u4e60\u6784\u5f0f\uff0c\u5e76\u9a8c\u8bc1\u6784\u5f0f\u5b66\u4e60\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528Rozner\u7b49\u4eba\u7684\u65b9\u6cd5\u8bc4\u4f30BabyLM\u6311\u6218\u8d5b\u4e2d\u6a21\u578b\u7684\u6784\u5f0f\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u591a\u6837\u5316\u7684\u6784\u5f0f\uff0c\u5305\u62ec\u8868\u9762\u96be\u4ee5\u533a\u5206\u7684\u6848\u4f8b\uff0c\u4e14\u6784\u5f0f\u8868\u73b0\u4e0eBabyLM\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "\u6784\u5f0f\u5b66\u4e60\u5728\u5f00\u53d1\u5408\u7406\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u5177\u6709\u529f\u80fd\u76f8\u5173\u6027\uff0c\u652f\u6301\u6784\u5f0f\u8bed\u6cd5\u7406\u8bba\u3002"}}
{"id": "2506.02150", "pdf": "https://arxiv.org/pdf/2506.02150", "abs": "https://arxiv.org/abs/2506.02150", "authors": ["Stefano Fogarollo", "Gregor Laimer", "Reto Bale", "Matthias Harders"], "title": "Implicit Deformable Medical Image Registration with Learnable Kernels", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025 Provisional Accept", "summary": "Deformable medical image registration is an essential task in\ncomputer-assisted interventions. This problem is particularly relevant to\noncological treatments, where precise image alignment is necessary for tracking\ntumor growth, assessing treatment response, and ensuring accurate delivery of\ntherapies. Recent AI methods can outperform traditional techniques in accuracy\nand speed, yet they often produce unreliable deformations that limit their\nclinical adoption. In this work, we address this challenge and introduce a\nnovel implicit registration framework that can predict accurate and reliable\ndeformations. Our insight is to reformulate image registration as a signal\nreconstruction problem: we learn a kernel function that can recover the dense\ndisplacement field from sparse keypoint correspondences. We integrate our\nmethod in a novel hierarchical architecture, and estimate the displacement\nfield in a coarse-to-fine manner. Our formulation also allows for efficient\nrefinement at test time, permitting clinicians to easily adjust registrations\nwhen needed. We validate our method on challenging intra-patient thoracic and\nabdominal zero-shot registration tasks, using public and internal datasets from\nthe local University Hospital. Our method not only shows competitive accuracy\nto state-of-the-art approaches, but also bridges the generalization gap between\nimplicit and explicit registration techniques. In particular, our method\ngenerates deformations that better preserve anatomical relationships and\nmatches the performance of specialized commercial systems, underscoring its\npotential for clinical adoption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9690\u5f0f\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u5173\u952e\u70b9\u5bf9\u5e94\u91cd\u5efa\u5bc6\u96c6\u4f4d\u79fb\u573a\uff0c\u63d0\u9ad8\u4e86\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u5728\u80bf\u7624\u6cbb\u7597\u7b49\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u65b9\u6cd5\u5e38\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u53d8\u5f62\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u5c06\u56fe\u50cf\u914d\u51c6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u53f7\u91cd\u5efa\u95ee\u9898\uff0c\u5b66\u4e60\u6838\u51fd\u6570\u4ece\u7a00\u758f\u5173\u952e\u70b9\u6062\u590d\u5bc6\u96c6\u4f4d\u79fb\u573a\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u67b6\u6784\u8fdb\u884c\u7c97\u5230\u7ec6\u7684\u4f30\u8ba1\u3002", "result": "\u5728\u80f8\u90e8\u548c\u8179\u90e8\u96f6\u6837\u672c\u914d\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u66f4\u7b26\u5408\u89e3\u5256\u5173\u7cfb\u7684\u53d8\u5f62\uff0c\u6027\u80fd\u63a5\u8fd1\u4e13\u4e1a\u5546\u4e1a\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u8fd8\u7f29\u5c0f\u4e86\u9690\u5f0f\u4e0e\u663e\u5f0f\u914d\u51c6\u6280\u672f\u7684\u6cdb\u5316\u5dee\u8ddd\uff0c\u5177\u6709\u4e34\u5e8a\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2506.02157", "pdf": "https://arxiv.org/pdf/2506.02157", "abs": "https://arxiv.org/abs/2506.02157", "authors": ["Amir Hussein", "Cihan Xiao", "Matthew Wiesner", "Dan Povey", "Leibny Paola Garcia", "Sanjeev Khudanpur"], "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems.", "AI": {"tldr": "HENT-SRT\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u8f6c\u5bfc\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u81ea\u84b8\u998f\u6280\u672f\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8f6c\u5bfc\u5668\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u5b58\u5728\u8bcd\u5e8f\u95ee\u9898\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faHENT-SRT\u6846\u67b6\uff0c\u5206\u89e3ASR\u548c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u91c7\u7528\u81ea\u84b8\u998f\u548cCTC\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u4f18\u5316\u7f16\u7801\u5668\u548c\u9884\u6d4b\u5668\u7ed3\u6784\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u666e\u901a\u8bdd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u7f29\u5c0f\u4e0eAED\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "HENT-SRT\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u8f6c\u5bfc\u5668\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.02161", "pdf": "https://arxiv.org/pdf/2506.02161", "abs": "https://arxiv.org/abs/2506.02161", "authors": ["Xinyu Wei", "Jinrui Zhang", "Zeqing Wang", "Hongyang Wei", "Zhen Guo", "Lei Zhang"], "title": "TIIF-Bench: How Does Your T2I Model Follow Your Instructions?", "categories": ["cs.CV"], "comment": "23 pages, 12 figures, 11 tables", "summary": "The rapid advancements of Text-to-Image (T2I) models have ushered in a new\nphase of AI-generated content, marked by their growing ability to interpret and\nfollow user instructions. However, existing T2I model evaluation benchmarks\nfall short in limited prompt diversity and complexity, as well as coarse\nevaluation metrics, making it difficult to evaluate the fine-grained alignment\nperformance between textual instructions and generated images. In this paper,\nwe present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming\nto systematically assess T2I models' ability in interpreting and following\nintricate textual instructions. TIIF-Bench comprises a set of 5000 prompts\norganized along multiple dimensions, which are categorized into three levels of\ndifficulties and complexities. To rigorously evaluate model robustness to\nvarying prompt lengths, we provide a short and a long version for each prompt\nwith identical core semantics. Two critical attributes, i.e., text rendering\nand style control, are introduced to evaluate the precision of text synthesis\nand the aesthetic coherence of T2I models. In addition, we collect 100\nhigh-quality designer level prompts that encompass various scenarios to\ncomprehensively assess model performance. Leveraging the world knowledge\nencoded in large vision language models, we propose a novel computable\nframework to discern subtle variations in T2I model outputs. Through meticulous\nbenchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and\ncons of current T2I models and reveal the limitations of current T2I\nbenchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/.", "AI": {"tldr": "TIIF-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u63d0\u793a\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5bf9\u6587\u672c\u6307\u4ee4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u5728\u63d0\u793a\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u4e0a\u4e0d\u8db3\uff0c\u4e14\u8bc4\u4ef7\u6307\u6807\u7c97\u7cd9\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6587\u672c\u6307\u4ee4\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6027\u80fd\u3002", "method": "TIIF-Bench\u5305\u542b5000\u4e2a\u6309\u591a\u7ef4\u5ea6\u7ec4\u7ec7\u7684\u63d0\u793a\uff0c\u5206\u4e3a\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u63d0\u4f9b\u957f\u77ed\u7248\u672c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5bf9\u63d0\u793a\u957f\u5ea6\u7684\u9c81\u68d2\u6027\u3002\u5f15\u5165\u6587\u672c\u6e32\u67d3\u548c\u98ce\u683c\u63a7\u5236\u4e24\u4e2a\u5173\u952e\u5c5e\u6027\uff0c\u5e76\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u53ef\u8ba1\u7b97\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u4e3b\u6d41T2I\u6a21\u578b\u7684\u7ec6\u81f4\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "TIIF-Bench\u4e3aT2I\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6a21\u578b\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2506.02172", "pdf": "https://arxiv.org/pdf/2506.02172", "abs": "https://arxiv.org/abs/2506.02172", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli", "Andre Martins", "Giuseppe Attanasio"], "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Recent studies on interpreting the hidden states of speech models have shown\ntheir ability to capture speaker-specific features, including gender. Does this\nfinding also hold for speech translation (ST) models? If so, what are the\nimplications for the speaker's gender assignment in translation? We address\nthese questions from an interpretability perspective, using probing methods to\nassess gender encoding across diverse ST models. Results on three language\ndirections (English-French/Italian/Spanish) indicate that while traditional\nencoder-decoder models capture gender information, newer architectures --\nintegrating a speech encoder with a machine translation system via adapters --\ndo not. We also demonstrate that low gender encoding capabilities result in\nsystems' tendency toward a masculine default, a translation bias that is more\npronounced in newer architectures.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u662f\u5426\u6355\u6349\u8bf4\u8bdd\u8005\u6027\u522b\u7279\u5f81\u53ca\u5176\u5bf9\u7ffb\u8bd1\u4e2d\u6027\u522b\u5206\u914d\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u662f\u5426\u50cf\u8bed\u97f3\u6a21\u578b\u4e00\u6837\u80fd\u6355\u6349\u6027\u522b\u7279\u5f81\uff0c\u4ee5\u53ca\u8fd9\u5bf9\u7ffb\u8bd1\u4e2d\u6027\u522b\u5206\u914d\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u63a2\u6d4b\u65b9\u6cd5\u8bc4\u4f30\u4e0d\u540c\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u4e2d\u7684\u6027\u522b\u7f16\u7801\u80fd\u529b\u3002", "result": "\u4f20\u7edf\u7f16\u7801-\u89e3\u7801\u6a21\u578b\u80fd\u6355\u6349\u6027\u522b\u4fe1\u606f\uff0c\u4f46\u65b0\u67b6\u6784\uff08\u901a\u8fc7\u9002\u914d\u5668\u6574\u5408\u8bed\u97f3\u7f16\u7801\u5668\u548c\u7ffb\u8bd1\u7cfb\u7edf\uff09\u4e0d\u80fd\u3002\u6027\u522b\u7f16\u7801\u80fd\u529b\u4f4e\u7684\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u7537\u6027\u9ed8\u8ba4\u7ffb\u8bd1\u3002", "conclusion": "\u65b0\u67b6\u6784\u7684\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u5728\u6027\u522b\u7f16\u7801\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u4e2d\u66f4\u660e\u663e\u7684\u7537\u6027\u9ed8\u8ba4\u504f\u89c1\u3002"}}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08DVC\uff09\u6765\u6bd4\u8f83\u6a21\u578b\u4e0e\u7334\u5b50\u5927\u8111\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u7b56\u7565\u76f8\u4f3c\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u4e0e\u7334\u5b50\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u4f4e\u4e8e\u6a21\u578b\u4e4b\u95f4\u6216\u7334\u5b50\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4e14\u968f\u7740\u6a21\u578b\u6027\u80fd\u63d0\u5347\u53cd\u800c\u964d\u4f4e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5148\u524d\u5173\u4e8e\u5927\u8111\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u5f81\u76f8\u4f3c\u6027\u7684\u4e89\u8bae\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u4e13\u6ce8\u4e8e\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u6bd4\u8f83\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u51b3\u7b56\u53d8\u91cf\u76f8\u5173\uff08DVC\uff09\u65b9\u6cd5\uff0c\u91cf\u5316\u5206\u7c7b\u4efb\u52a1\u4e2d\u89e3\u7801\u51b3\u7b56\u7684\u76f8\u5173\u6027\uff0c\u8bc4\u4f30\u6a21\u578b\u4e0e\u7334\u5b50V4/IT\u8bb0\u5f55\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u4e4b\u95f4\u4e0e\u7334\u5b50\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u76f8\u5f53\uff0c\u4f46\u6a21\u578b\u4e0e\u7334\u5b50\u7684\u76f8\u4f3c\u6027\u8f83\u4f4e\u4e14\u968f\u6a21\u578b\u6027\u80fd\u63d0\u5347\u800c\u964d\u4f4e\uff1b\u5bf9\u6297\u8bad\u7ec3\u548c\u66f4\u5927\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u672a\u80fd\u63d0\u5347\u6a21\u578b\u4e0e\u7334\u5b50\u7684\u76f8\u4f3c\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u7334\u5b50V4/IT\u4e0e\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u5728\u4efb\u52a1\u76f8\u5173\u8868\u5f81\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5dee\u5f02\u3002"}}
{"id": "2506.02175", "pdf": "https://arxiv.org/pdf/2506.02175", "abs": "https://arxiv.org/abs/2506.02175", "authors": ["Salman Rahman", "Sheriff Issaka", "Ashima Suvarna", "Genglin Liu", "James Shiffer", "Jaeyoung Lee", "Md Rizwan Parvez", "Hamid Palangi", "Shi Feng", "Nanyun Peng", "Yejin Choi", "Julian Michael", "Liwei Jiang", "Saadia Gabriel"], "title": "AI Debate Aids Assessment of Controversial Claims", "categories": ["cs.CL"], "comment": null, "summary": "As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics like public health\nwhere factual accuracy directly impacts well-being. Scalable Oversight aims to\nensure AI truthfulness by enabling humans to supervise systems that may exceed\nhuman capabilities--yet humans themselves hold different beliefs and biases\nthat impair their judgment. We study whether AI debate can guide biased judges\ntoward the truth by having two AI systems debate opposing sides of\ncontroversial COVID-19 factuality claims where people hold strong prior\nbeliefs. We conduct two studies: one with human judges holding either\nmainstream or skeptical beliefs evaluating factuality claims through\nAI-assisted debate or consultancy protocols, and a second examining the same\nproblem with personalized AI judges designed to mimic these different human\nbelief systems. In our human study, we find that debate-where two AI advisor\nsystems present opposing evidence-based arguments-consistently improves\njudgment accuracy and confidence calibration, outperforming consultancy with a\nsingle-advisor system by 10% overall. The improvement is most significant for\njudges with mainstream beliefs (+15.2% accuracy), though debate also helps\nskeptical judges who initially misjudge claims move toward accurate views\n(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like\npersonas achieve even higher accuracy (78.5%) than human judges (70.1%) and\ndefault AI judges without personas (69.8%), suggesting their potential for\nsupervising frontier AI models. These findings highlight AI debate as a\npromising path toward scalable, bias-resilient oversight--leveraging both\ndiverse human and AI judgments to move closer to truth in contested domains.", "AI": {"tldr": "AI\u8fa9\u8bba\u80fd\u5e2e\u52a9\u51cf\u5c11\u504f\u89c1\u5e76\u63d0\u9ad8\u5224\u65ad\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u4e89\u8bae\u6027\u8bdd\u9898\u5982COVID-19\u4e8b\u5b9e\u6027\u4e0a\u3002", "motivation": "\u968f\u7740AI\u5f71\u54cd\u529b\u589e\u5f3a\uff0c\u5176\u53ef\u80fd\u653e\u5927\u9519\u8bef\u4fe1\u606f\u5e76\u52a0\u6df1\u793e\u4f1a\u5206\u6b67\uff0c\u5c24\u5176\u662f\u5728\u516c\u5171\u5065\u5eb7\u7b49\u5173\u952e\u9886\u57df\u3002", "method": "\u901a\u8fc7AI\u8fa9\u8bba\uff08\u4e24\u4e2aAI\u7cfb\u7edf\u8fa9\u8bba\u5bf9\u7acb\u89c2\u70b9\uff09\u548c\u54a8\u8be2\uff08\u5355\u4e00AI\u7cfb\u7edf\u63d0\u4f9b\u5efa\u8bae\uff09\u4e24\u79cd\u65b9\u5f0f\uff0c\u7814\u7a76\u4eba\u7c7b\u548cAI\u6cd5\u5b98\u5bf9COVID-19\u4e8b\u5b9e\u6027\u5224\u65ad\u7684\u5f71\u54cd\u3002", "result": "\u8fa9\u8bba\u663e\u8457\u63d0\u9ad8\u5224\u65ad\u51c6\u786e\u6027\uff08\u4e3b\u6d41\u4fe1\u5ff5\u8005\u63d0\u534715.2%\uff0c\u6000\u7591\u8005\u63d0\u53474.7%\uff09\uff0cAI\u6cd5\u5b98\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u6cd5\u5b98\uff0878.5% vs. 70.1%\uff09\u3002", "conclusion": "AI\u8fa9\u8bba\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u7ed3\u5408\u591a\u6837\u7684\u4eba\u7c7b\u548cAI\u5224\u65ad\uff0c\u63d0\u5347\u4e89\u8bae\u9886\u57df\u7684\u771f\u76f8\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2506.02167", "pdf": "https://arxiv.org/pdf/2506.02167", "abs": "https://arxiv.org/abs/2506.02167", "authors": ["Aditi Tiwari", "Farzaneh Masoud", "Dac Trong Nguyen", "Jill Kraft", "Heng Ji", "Klara Nahrstedt"], "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 9 figures, 6 tables", "summary": "Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset.", "AI": {"tldr": "Fire360\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6d88\u9632\u573a\u666f\u4e2d\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b228\u4e2a360\u5ea6\u89c6\u9891\uff0c\u652f\u6301\u4e94\u9879\u4efb\u52a1\uff0c\u65e8\u5728\u63d0\u5347AI\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u5728\u53ef\u9760\u6027\u8981\u6c42\u9ad8\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6d88\u9632\u5458\u56e0\u60c5\u5883\u611f\u77e5\u95ee\u9898\u53d7\u4f24\u7684\u60c5\u51b5\u9891\u53d1\uff0c\u9700\u8981\u6539\u8fdbAI\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u80fd\u529b\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u6761\u4ef6\u4e0b\u7684\u6d88\u9632\u8bad\u7ec3\u89c6\u9891\uff0c\u6807\u6ce8\u4e86\u52a8\u4f5c\u3001\u7269\u4f53\u4f4d\u7f6e\u548c\u9000\u5316\u5143\u6570\u636e\uff0c\u652f\u6301\u4e94\u9879\u4efb\u52a1\u6d4b\u8bd5\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u5728TOR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0883.5%\uff09\uff0c\u800cGPT-4o\u7b49\u6a21\u578b\u8868\u73b0\u8f83\u5dee\uff0c\u66b4\u9732\u4e86\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u7f3a\u9677\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03Fire360\u53ca\u5176\u8bc4\u4f30\u5957\u4ef6\uff0c\u65e8\u5728\u63a8\u52a8AI\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u8bb0\u5fc6\u3001\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\u3002"}}
{"id": "2506.02181", "pdf": "https://arxiv.org/pdf/2506.02181", "abs": "https://arxiv.org/abs/2506.02181", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Mauro Cettolo", "Luisa Bentivogli"], "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Despite significant advances in ASR, the specific acoustic cues models rely\non remain unclear. Prior studies have examined such cues on a limited set of\nphonemes and outdated models. In this work, we apply a feature attribution\ntechnique to identify the relevant acoustic cues for a modern Conformer-based\nASR system. By analyzing plosives, fricatives, and vowels, we assess how\nfeature attributions align with their acoustic properties in the time and\nfrequency domains, also essential for human speech perception. Our findings\nshow that the ASR model relies on vowels' full time spans, particularly their\nfirst two formants, with greater saliency in male speech. It also better\ncaptures the spectral characteristics of sibilant fricatives than non-sibilants\nand prioritizes the release phase in plosives, especially burst\ncharacteristics. These insights enhance the interpretability of ASR models and\nhighlight areas for future research to uncover potential gaps in model\nrobustness.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u6280\u672f\u5206\u6790\u4e86\u73b0\u4ee3Conformer-based ASR\u7cfb\u7edf\u4f9d\u8d56\u7684\u58f0\u5b66\u7ebf\u7d22\uff0c\u53d1\u73b0\u5176\u5bf9\u5143\u97f3\u3001\u6469\u64e6\u97f3\u548c\u7206\u7834\u97f3\u7684\u58f0\u5b66\u7279\u6027\u6709\u7279\u5b9a\u504f\u597d\u3002", "motivation": "\u5c3d\u7ba1ASR\u6280\u672f\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6a21\u578b\u4f9d\u8d56\u7684\u5177\u4f53\u58f0\u5b66\u7ebf\u7d22\u4ecd\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5e94\u7528\u7279\u5f81\u5f52\u56e0\u6280\u672f\uff0c\u5206\u6790\u5143\u97f3\u3001\u6469\u64e6\u97f3\u548c\u7206\u7834\u97f3\u7684\u58f0\u5b66\u7279\u6027\u5728\u65f6\u95f4\u548c\u9891\u7387\u57df\u4e2d\u7684\u8868\u73b0\u3002", "result": "ASR\u6a21\u578b\u66f4\u4f9d\u8d56\u5143\u97f3\u7684\u5168\u65f6\u7a0b\uff08\u5c24\u5176\u662f\u524d\u4e24\u4e2a\u5171\u632f\u5cf0\uff09\u3001\u6469\u64e6\u97f3\u7684\u9891\u8c31\u7279\u6027\uff08\u5c24\u5176\u662f\u549d\u97f3\uff09\uff0c\u4ee5\u53ca\u7206\u7834\u97f3\u7684\u91ca\u653e\u9636\u6bb5\uff08\u5c24\u5176\u662f\u7206\u7834\u7279\u6027\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u5347\u4e86ASR\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4ee5\u53d1\u73b0\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6f5c\u5728\u4e0d\u8db3\u3002"}}
{"id": "2506.02221", "pdf": "https://arxiv.org/pdf/2506.02221", "abs": "https://arxiv.org/abs/2506.02221", "authors": ["Johannes Schusterbauer", "Ming Gui", "Frank Fundel", "Bj\u00f6rn Ommer"], "title": "Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR 2025", "summary": "Diffusion models have revolutionized generative tasks through high-fidelity\noutputs, yet flow matching (FM) offers faster inference and empirical\nperformance gains. However, current foundation FM models are computationally\nprohibitive for finetuning, while diffusion models like Stable Diffusion\nbenefit from efficient architectures and ecosystem support. This work addresses\nthe critical challenge of efficiently transferring knowledge from pre-trained\ndiffusion models to flow matching. We propose Diff2Flow, a novel framework that\nsystematically bridges diffusion and FM paradigms by rescaling timesteps,\naligning interpolants, and deriving FM-compatible velocity fields from\ndiffusion predictions. This alignment enables direct and efficient FM\nfinetuning of diffusion priors with no extra computation overhead. Our\nexperiments demonstrate that Diff2Flow outperforms na\\\"ive FM and diffusion\nfinetuning particularly under parameter-efficient constraints, while achieving\nsuperior or competitive performance across diverse downstream tasks compared to\nstate-of-the-art methods. We will release our code at\nhttps://github.com/CompVis/diff2flow.", "AI": {"tldr": "Diff2Flow\u6846\u67b6\u901a\u8fc7\u65f6\u95f4\u6b65\u91cd\u7f29\u653e\u3001\u5bf9\u9f50\u63d2\u503c\u548c\u4ece\u6269\u6563\u9884\u6d4b\u4e2d\u5bfc\u51faFM\u517c\u5bb9\u901f\u5ea6\u573a\uff0c\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u77e5\u8bc6\u9ad8\u6548\u8fc1\u79fb\u5230\u6d41\u5339\u914d\uff08FM\uff09\u4e2d\uff0c\u5b9e\u73b0\u76f4\u63a5\u4e14\u9ad8\u6548\u7684FM\u5fae\u8c03\u3002", "motivation": "\u5f53\u524d\u57fa\u7840FM\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5177\u6709\u9ad8\u6548\u67b6\u6784\u548c\u751f\u6001\u7cfb\u7edf\u652f\u6301\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u6269\u6563\u6a21\u578b\u7684\u77e5\u8bc6\u9ad8\u6548\u8fc1\u79fb\u5230FM\u4e2d\u3002", "method": "\u63d0\u51faDiff2Flow\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6b65\u91cd\u7f29\u653e\u3001\u5bf9\u9f50\u63d2\u503c\u548c\u4ece\u6269\u6563\u9884\u6d4b\u4e2d\u5bfc\u51faFM\u517c\u5bb9\u901f\u5ea6\u573a\uff0c\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u5230FM\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "Diff2Flow\u5728\u53c2\u6570\u9ad8\u6548\u7ea6\u675f\u4e0b\u4f18\u4e8e\u6734\u7d20FM\u548c\u6269\u6563\u5fae\u8c03\uff0c\u5e76\u5728\u591a\u6837\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "Diff2Flow\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5230FM\u7684\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548FM\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02204", "pdf": "https://arxiv.org/pdf/2506.02204", "abs": "https://arxiv.org/abs/2506.02204", "authors": ["Lindia Tjuatja", "Graham Neubig"], "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Language model evaluation is a daunting task: prompts are brittle,\ncorpus-level perplexities are vague, and the choice of benchmarks are endless.\nFinding examples that show meaningful, generalizable differences between two\nLMs is crucial to understanding where one model succeeds and another fails. Can\nthis process be done automatically? In this work, we propose methodology for\nautomated comparison of language models that uses performance-aware contextual\nembeddings to find fine-grained features of text where one LM outperforms\nanother. Our method, which we name BehaviorBox, extracts coherent features that\ndemonstrate differences with respect to the ease of generation between two LMs.\nSpecifically, BehaviorBox finds features that describe groups of words in\nfine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\"\nand \"exclamation marks after emotional statements\", where one model outperforms\nanother within a particular datatset. We apply BehaviorBox to compare models\nthat vary in size, model family, and post-training, and enumerate insights into\nspecific contexts that illustrate meaningful differences in performance which\ncannot be found by measures such as corpus-level perplexity alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBehaviorBox\u7684\u81ea\u52a8\u6bd4\u8f83\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6027\u80fd\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u627e\u5230\u6a21\u578b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5dee\u5f02\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4efb\u52a1\u590d\u6742\u4e14\u6a21\u7cca\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u56f0\u60d1\u5ea6\u96be\u4ee5\u6355\u6349\u6a21\u578b\u95f4\u7684\u5177\u4f53\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u53d1\u73b0\u6a21\u578b\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u6027\u80fd\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6280\u672f\uff0c\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\uff08\u5982\u7279\u5b9a\u77ed\u8bed\u6216\u6807\u70b9\u7b26\u53f7\u7684\u4f7f\u7528\uff09\uff0c\u6bd4\u8f83\u4e24\u4e2a\u6a21\u578b\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "BehaviorBox\u6210\u529f\u8bc6\u522b\u51fa\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4f8b\u5982\u6761\u4ef6\u53e5\u6216\u60c5\u611f\u8868\u8fbe\u4e2d\u7684\u8868\u73b0\uff0c\u8fd9\u4e9b\u5dee\u5f02\u65e0\u6cd5\u901a\u8fc7\u4f20\u7edf\u56f0\u60d1\u5ea6\u6307\u6807\u53d1\u73b0\u3002", "conclusion": "BehaviorBox\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u7ec6\u7c92\u5ea6\u7684\u65b9\u6cd5\u6765\u6bd4\u8f83\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7684\u5177\u4f53\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229", "abs": "https://arxiv.org/abs/2506.02229", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u80ce\u76d8\u75c5\u7406\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u548c\u52a0\u901f\u63d0\u5347\u90e8\u7f72\u80fd\u529b\u3002", "motivation": "\u80ce\u76d8\u75c5\u7406\u68c0\u67e5\u5bf9\u68c0\u6d4b\u548c\u51cf\u8f7b\u5206\u5a29\u76f8\u5173\u5065\u5eb7\u98ce\u9669\u6709\u6548\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\uff1a1\uff09\u6587\u672c\u951a\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\uff08VLCD\uff09\uff1b2\uff09\u4f7f\u7528\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u65e0\u76d1\u7763\u9884\u84b8\u998f\u4ee5\u4f18\u5316\u521d\u59cb\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u6559\u5e08\u6a21\u578b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u8d28\u91cf\u56fe\u50cf\uff0c\u63d0\u5347\u4e86\u6548\u7387\u548c\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "VLCD\u63d0\u9ad8\u4e86\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u90e8\u7f72\u6027\uff0c\u4f7fAI\u533b\u7597\u89e3\u51b3\u65b9\u6848\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u66f4\u6613\u666e\u53ca\u3002"}}
{"id": "2506.02212", "pdf": "https://arxiv.org/pdf/2506.02212", "abs": "https://arxiv.org/abs/2506.02212", "authors": ["Ella Rannon", "David Burstein"], "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics", "categories": ["cs.CL", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Natural Language Processing (NLP) has transformed various fields beyond\nlinguistics by applying techniques originally developed for human language to\nthe analysis of biological sequences. This review explores the application of\nNLP methods to biological sequence data, focusing on genomics, transcriptomics,\nand proteomics. We examine how various NLP methods, from classic approaches\nlike word2vec to advanced models employing transformers and hyena operators,\nare being adapted to analyze DNA, RNA, protein sequences, and entire genomes.\nThe review also examines tokenization strategies and model architectures,\nevaluating their strengths, limitations, and suitability for different\nbiological tasks. We further cover recent advances in NLP applications for\nbiological data, such as structure prediction, gene expression, and\nevolutionary analysis, highlighting the potential of these methods for\nextracting meaningful insights from large-scale genomic data. As language\nmodels continue to advance, their integration into bioinformatics holds immense\npromise for advancing our understanding of biological processes in all domains\nof life.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u65b9\u6cd5\u5728\u751f\u7269\u5e8f\u5217\u6570\u636e\uff08\u5982\u57fa\u56e0\u7ec4\u5b66\u3001\u8f6c\u5f55\u7ec4\u5b66\u548c\u86cb\u767d\u8d28\u7ec4\u5b66\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u4ece\u7ecf\u5178\u65b9\u6cd5\u5230\u5148\u8fdb\u6a21\u578b\u7684\u9002\u5e94\u6027\u53ca\u5176\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22NLP\u6280\u672f\u5728\u751f\u7269\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u63a8\u52a8\u5bf9\u751f\u7269\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "method": "\u7efc\u8ff0\u4e86NLP\u65b9\u6cd5\uff08\u5982word2vec\u3001transformer\u548chyena\u7b97\u5b50\uff09\u5728DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u4f18\u7f3a\u70b9\u3002", "result": "NLP\u65b9\u6cd5\u5728\u7ed3\u6784\u9884\u6d4b\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u8fdb\u5316\u5206\u6790\u7b49\u751f\u7269\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "NLP\u4e0e\u751f\u7269\u4fe1\u606f\u5b66\u7684\u7ed3\u5408\u6709\u671b\u4e3a\u751f\u547d\u79d1\u5b66\u9886\u57df\u5e26\u6765\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2506.02244", "pdf": "https://arxiv.org/pdf/2506.02244", "abs": "https://arxiv.org/abs/2506.02244", "authors": ["Bowen Xue", "Giuseppe Claudio Guarnera", "Shuang Zhao", "Zahra Montazeri"], "title": "Motion aware video generative model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based video generation have yielded\nunprecedented quality in visual content and semantic coherence. However,\ncurrent approaches predominantly rely on statistical learning from vast\ndatasets without explicitly modeling the underlying physics of motion,\nresulting in subtle yet perceptible non-physical artifacts that diminish the\nrealism of generated videos. This paper introduces a physics-informed frequency\ndomain approach to enhance the physical plausibility of generated videos. We\nfirst conduct a systematic analysis of the frequency-domain characteristics of\ndiverse physical motions (translation, rotation, scaling), revealing that each\nmotion type exhibits distinctive and identifiable spectral signatures. Building\non this theoretical foundation, we propose two complementary components: (1) a\nphysical motion loss function that quantifies and optimizes the conformity of\ngenerated videos to ideal frequency-domain motion patterns, and (2) a frequency\ndomain enhancement module that progressively learns to adjust video features to\nconform to physical motion constraints while preserving original network\nfunctionality through a zero-initialization strategy. Experiments across\nmultiple video diffusion architectures demonstrate that our approach\nsignificantly enhances motion quality and physical plausibility without\ncompromising visual quality or semantic alignment. Our frequency-domain\nphysical motion framework generalizes effectively across different video\ngeneration architectures, offering a principled approach to incorporating\nphysical constraints into deep learning-based video synthesis pipelines. This\nwork seeks to establish connections between data-driven models and\nphysics-based motion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u9891\u7387\u57df\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6269\u6563\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u5408\u7406\u6027\uff0c\u901a\u8fc7\u5206\u6790\u8fd0\u52a8\u7c7b\u578b\u7684\u9891\u8c31\u7279\u5f81\u5e76\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\u548c\u589e\u5f3a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u89c6\u9891\u7684\u8fd0\u52a8\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7edf\u8ba1\u5b66\u4e60\uff0c\u672a\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u7269\u7406\u7279\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u89c6\u9891\u5b58\u5728\u975e\u7269\u7406\u4f2a\u5f71\uff0c\u5f71\u54cd\u771f\u5b9e\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9891\u7387\u57df\u7269\u7406\u5efa\u6a21\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "method": "1. \u5206\u6790\u4e0d\u540c\u7269\u7406\u8fd0\u52a8\uff08\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\u7684\u9891\u7387\u57df\u7279\u5f81\uff1b2. \u63d0\u51fa\u7269\u7406\u8fd0\u52a8\u635f\u5931\u51fd\u6570\u4f18\u5316\u89c6\u9891\u9891\u8c31\uff1b3. \u8bbe\u8ba1\u9891\u7387\u57df\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u96f6\u521d\u59cb\u5316\u7b56\u7565\u8c03\u6574\u89c6\u9891\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u8fd0\u52a8\u8d28\u91cf\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u6216\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9891\u7387\u57df\u7269\u7406\u8fd0\u52a8\u6846\u67b6\u4e3a\u6df1\u5ea6\u5b66\u4e60\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u7269\u7406\u7ea6\u675f\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u8fde\u63a5\u4e86\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4e0e\u7269\u7406\u8fd0\u52a8\u6a21\u578b\u3002"}}
{"id": "2506.02239", "pdf": "https://arxiv.org/pdf/2506.02239", "abs": "https://arxiv.org/abs/2506.02239", "authors": ["Sofoklis Kakouros"], "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u91cd\u8981\u6027\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u5173\u952e\u8bed\u97f3\u6bb5\uff0c\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6574\u53e5\u6216\u957f\u8bed\u97f3\u6bb5\u4e0a\u8ba1\u7b97\u58f0\u5b66\u7279\u5f81\uff0c\u53ef\u80fd\u5ffd\u7565\u7ec6\u7c92\u5ea6\u53d8\u5316\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u91cd\u8981\u6027\u7b5b\u9009\u5173\u952e\u6bb5\u4ee5\u63d0\u9ad8\u8bc6\u522b\u6548\u679c\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u8bcd\u8bed\u4fe1\u606f\u91cf\uff0c\u7b5b\u9009\u5173\u952e\u8bed\u97f3\u6bb5\uff0c\u5e76\u4ec5\u5728\u8fd9\u4e9b\u6bb5\u4e0a\u8ba1\u7b97\u58f0\u5b66\u7279\u5f81\uff08\u5982\u80fd\u91cf\u3001F0\u7b49\uff09\u53ca\u5176\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u8bcd\u8bed\u4fe1\u606f\u91cf\u7b5b\u9009\u7684\u8bed\u97f3\u6bb5\u7279\u5f81\u8ba1\u7b97\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u91cd\u8981\u6027\u7b5b\u9009\u8bed\u97f3\u6bb5\u80fd\u6709\u6548\u63d0\u5347\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e3a\u8bed\u97f3\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02247", "pdf": "https://arxiv.org/pdf/2506.02247", "abs": "https://arxiv.org/abs/2506.02247", "authors": ["Yu Wang", "Juhyung Ha", "David J. Crandall"], "title": "PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss", "categories": ["cs.CV"], "comment": "4 pages, 1 figure, and 1 table", "summary": "Active speaker detection (ASD) in egocentric videos presents unique\nchallenges due to unstable viewpoints, motion blur, and off-screen speech\nsources - conditions under which traditional visual-centric methods degrade\nsignificantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with\nRegularization Network), an effective model that integrates a partially frozen\nWhisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly\nfuse cross-modal cues. To counteract modality imbalance, we introduce an\ninter-modal alignment loss that synchronizes audio and visual representations,\nenabling more consistent convergence across modalities. Without relying on\nmulti-speaker context or ideal frontal views, PAIR-Net achieves\nstate-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP,\nsurpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results\nhighlight the value of pretrained audio priors and alignment-based fusion for\nrobust ASD under real-world egocentric conditions.", "AI": {"tldr": "PAIR-Net\u7ed3\u5408\u9884\u8bad\u7ec3\u7684Whisper\u97f3\u9891\u7f16\u7801\u5668\u548c\u5fae\u8c03\u7684AV-HuBERT\u89c6\u89c9\u9aa8\u5e72\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u635f\u5931\u63d0\u5347\u4e3b\u52a8\u8bf4\u8bdd\u8005\u68c0\u6d4b\u6027\u80fd\uff0c\u5728Ego4D\u57fa\u51c6\u4e0a\u8fbe\u523076.6% mAP\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u5728\u89c6\u89d2\u4e0d\u7a33\u5b9a\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u5c4f\u5e55\u5916\u8bed\u97f3\u7b49\u73b0\u5b9e\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408\u90e8\u5206\u51bb\u7ed3\u7684Whisper\u97f3\u9891\u7f16\u7801\u5668\u548c\u5fae\u8c03\u7684AV-HuBERT\u89c6\u89c9\u9aa8\u5e72\uff0c\u5f15\u5165\u8de8\u6a21\u6001\u5bf9\u9f50\u635f\u5931\u4ee5\u540c\u6b65\u97f3\u9891\u548c\u89c6\u89c9\u8868\u5f81\u3002", "result": "\u5728Ego4D ASD\u57fa\u51c6\u4e0a\u8fbe\u523076.6% mAP\uff0c\u4f18\u4e8eLoCoNet\u548cSTHG\u3002", "conclusion": "\u9884\u8bad\u7ec3\u97f3\u9891\u5148\u9a8c\u548c\u5bf9\u9f50\u878d\u5408\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e0b\u5bf9ASD\u5177\u6709\u663e\u8457\u4ef7\u503c\u3002"}}
{"id": "2506.02264", "pdf": "https://arxiv.org/pdf/2506.02264", "abs": "https://arxiv.org/abs/2506.02264", "authors": ["Radin Shayanfar", "Chu Fei Luo", "Rohan Bhambhoria", "Samuel Dahan", "Xiaodan Zhu"], "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment", "categories": ["cs.CL"], "comment": null, "summary": "It is often challenging to teach specialized, unseen tasks to dialogue\nsystems due to the high cost of expert knowledge, training data, and high\ntechnical difficulty. To support domain-specific applications - such as law,\nmedicine, or finance - it is essential to build frameworks that enable\nnon-technical experts to define, test, and refine system behaviour with minimal\neffort. Achieving this requires cross-disciplinary collaboration between\ndevelopers and domain specialists. In this work, we introduce a novel\nframework, CoDial (Code for Dialogue), that converts expert knowledge,\nrepresented as a novel structured heterogeneous graph, into executable\nconversation logic. CoDial can be easily implemented in existing guardrailing\nlanguages, such as Colang, to enable interpretable, modifiable, and true\nzero-shot specification of task-oriented dialogue systems. Empirically, CoDial\nachieves state-of-the-art performance on the STAR dataset for inference-based\nmodels and is competitive with similar baselines on the well-known MultiWOZ\ndataset. We also demonstrate CoDial's iterative improvement via manual and\nLLM-aided feedback, making it a practical tool for expert-guided alignment of\nLLMs in high-stakes domains.", "AI": {"tldr": "CoDial\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5f02\u6784\u56fe\u5c06\u4e13\u5bb6\u77e5\u8bc6\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u5bf9\u8bdd\u903b\u8f91\uff0c\u652f\u6301\u975e\u6280\u672f\u4e13\u5bb6\u8f7b\u677e\u5b9a\u4e49\u548c\u4f18\u5316\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5e76\u5728STAR\u548cMultiWOZ\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u4e2d\u4e13\u5bb6\u77e5\u8bc6\u6210\u672c\u9ad8\u3001\u6280\u672f\u96be\u5ea6\u5927\u7684\u95ee\u9898\uff0c\u652f\u6301\u975e\u6280\u672f\u4e13\u5bb6\u53c2\u4e0e\u7cfb\u7edf\u5b9a\u4e49\u4e0e\u4f18\u5316\u3002", "method": "\u63d0\u51faCoDial\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u5f02\u6784\u56fe\u8868\u793a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5e76\u96c6\u6210\u5230\u73b0\u6709\u5bf9\u8bdd\u8bed\u8a00\uff08\u5982Colang\uff09\u4e2d\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u3002", "result": "\u5728STAR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728MultiWOZ\u6570\u636e\u96c6\u4e0a\u4e0e\u57fa\u7ebf\u6a21\u578b\u7ade\u4e89\uff0c\u652f\u6301\u901a\u8fc7\u4eba\u5de5\u548cLLM\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u3002", "conclusion": "CoDial\u662f\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u652f\u6301\u4e13\u5bb6\u5f15\u5bfc\u7684\u9ad8\u98ce\u9669\u9886\u57dfLLM\u5bf9\u9f50\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fee\u6539\u6027\u3002"}}
{"id": "2506.02265", "pdf": "https://arxiv.org/pdf/2506.02265", "abs": "https://arxiv.org/abs/2506.02265", "authors": ["Samuel Li", "Pujith Kachana", "Prajwal Chidananda", "Saurabh Nair", "Yasutaka Furukawa", "Matthew Brown"], "title": "Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Estimating agent pose and 3D scene structure from multi-camera rigs is a\ncentral task in embodied AI applications such as autonomous driving. Recent\nlearned approaches such as DUSt3R have shown impressive performance in\nmultiview settings. However, these models treat images as unstructured\ncollections, limiting effectiveness in scenarios where frames are captured from\nsynchronized rigs with known or inferable structure.\n  To this end, we introduce Rig3R, a generalization of prior multiview\nreconstruction models that incorporates rig structure when available, and\nlearns to infer it when not. Rig3R conditions on optional rig metadata\nincluding camera ID, time, and rig poses to develop a rig-aware latent space\nthat remains robust to missing information. It jointly predicts pointmaps and\ntwo types of raymaps: a pose raymap relative to a global frame, and a rig\nraymap relative to a rig-centric frame consistent across time. Rig raymaps\nallow the model to infer rig structure directly from input images when metadata\nis missing.\n  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose\nestimation, and rig discovery, outperforming both traditional and learned\nmethods by 17-45% mAA across diverse real-world rig datasets, all in a single\nforward pass without post-processing or iterative refinement.", "AI": {"tldr": "Rig3R\u662f\u4e00\u79cd\u591a\u89c6\u89d2\u91cd\u5efa\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6216\u63a8\u65ad\u76f8\u673a\u8bbe\u5907\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u8bbe\u5907\u7ed3\u6784\u53d1\u73b0\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982DUSt3R\uff09\u5c06\u56fe\u50cf\u89c6\u4e3a\u65e0\u7ed3\u6784\u96c6\u5408\uff0c\u9650\u5236\u4e86\u5728\u5df2\u77e5\u6216\u53ef\u63a8\u65ad\u8bbe\u5907\u7ed3\u6784\u573a\u666f\u4e2d\u7684\u6548\u679c\u3002Rig3R\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Rig3R\u5229\u7528\u8bbe\u5907\u5143\u6570\u636e\uff08\u5982\u76f8\u673aID\u3001\u65f6\u95f4\u548c\u59ff\u6001\uff09\u6784\u5efa\u8bbe\u5907\u611f\u77e5\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u8054\u5408\u9884\u6d4b\u70b9\u56fe\u548c\u4e24\u79cd\u5c04\u7ebf\u56fe\uff08\u5168\u5c40\u548c\u8bbe\u5907\u4e2d\u5fc3\uff09\u3002", "result": "Rig3R\u57283D\u91cd\u5efa\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u8bbe\u5907\u53d1\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u6027\u80fd\u63d0\u534717-45% mAA\uff0c\u4e14\u65e0\u9700\u540e\u5904\u7406\u3002", "conclusion": "Rig3R\u901a\u8fc7\u8bbe\u5907\u611f\u77e5\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u91cd\u5efa\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8bbe\u5907\u7ed3\u6784\u4fe1\u606f\u7f3a\u5931\u65f6\u4ecd\u80fd\u7a33\u5065\u5de5\u4f5c\u3002"}}
{"id": "2506.02279", "pdf": "https://arxiv.org/pdf/2506.02279", "abs": "https://arxiv.org/abs/2506.02279", "authors": ["Wenzheng Zhang", "Xi Victoria Lin", "Karl Stratos", "Wen-tau Yih", "Mingda Chen"], "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance.", "AI": {"tldr": "ImpRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u67e5\u8be2\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5c06\u68c0\u7d22\u548c\u751f\u6210\u5206\u79bb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002ImpRAG\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6a21\u578b\u6d88\u9664\u663e\u5f0f\u67e5\u8be2\u7684\u9700\u6c42\u3002", "method": "ImpRAG\u5c06\u9884\u8bad\u7ec3\u7684\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u4e13\u7528\u5c42\u7ec4\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406\u8fc7\u7a0b\u540c\u65f6\u4f18\u5316\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\uff0c\u4f7f\u7528\u76f8\u540c\u53c2\u6570\u548c\u524d\u5411\u4f20\u64ad\u3002", "result": "\u57288\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\uff0cImpRAG\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u7cbe\u786e\u5339\u914d\u5206\u6570\u63d0\u5347\u4e863.6-11.5\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ImpRAG\u901a\u8fc7\u5e73\u8861\u68c0\u7d22\u548c\u751f\u6210\u53c2\u6570\uff0c\u5229\u7528\u751f\u6210\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u68c0\u7d22\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.02291", "pdf": "https://arxiv.org/pdf/2506.02291", "abs": "https://arxiv.org/abs/2506.02291", "authors": ["Cristian-Ioan Blaga", "Paul Suganthan", "Sahil Dua", "Krishna Srinivasan", "Enrique Alfonseca", "Peter Dornbach", "Tom Duerig", "Imed Zitouni", "Zhe Dong"], "title": "Entity Image and Mixed-Modal Image Retrieval Datasets", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Despite advances in multimodal learning, challenging benchmarks for\nmixed-modal image retrieval that combines visual and textual information are\nlacking. This paper introduces a novel benchmark to rigorously evaluate image\nretrieval that demands deep cross-modal contextual understanding. We present\ntwo new datasets: the Entity Image Dataset (EI), providing canonical images for\nWikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived\nfrom the WIT dataset. The MMIR benchmark features two challenging query types\nrequiring models to ground textual descriptions in the context of provided\nvisual entities: single entity-image queries (one entity image with descriptive\ntext) and multi-entity-image queries (multiple entity images with relational\ntext). We empirically validate the benchmark's utility as both a training\ncorpus and an evaluation set for mixed-modal retrieval. The quality of both\ndatasets is further affirmed through crowd-sourced human annotations. The\ndatasets are accessible through the GitHub page:\nhttps://github.com/google-research-datasets/wit-retrieval.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u56fe\u50cf\u68c0\u7d22\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6570\u636e\u96c6\uff08EI\u548cMMIR\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u4e0e\u6587\u672c\u7ed3\u5408\u7684\u6df1\u5ea6\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u7684\u6df7\u5408\u6a21\u6001\u56fe\u50cf\u68c0\u7d22\u7684\u6311\u6218\u6027\u57fa\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff1aEI\uff08\u63d0\u4f9bWikipedia\u5b9e\u4f53\u7684\u89c4\u8303\u56fe\u50cf\uff09\u548cMMIR\uff08\u6e90\u81eaWIT\u6570\u636e\u96c6\uff09\uff0c\u652f\u6301\u5355\u5b9e\u4f53\u56fe\u50cf\u67e5\u8be2\u548c\u591a\u5b9e\u4f53\u56fe\u50cf\u67e5\u8be2\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u57fa\u51c6\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u5f97\u5230\u4f17\u5305\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\u8d28\u91cf\u786e\u8ba4\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\u4e3a\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d44\u6e90\u3002"}}
{"id": "2506.02283", "pdf": "https://arxiv.org/pdf/2506.02283", "abs": "https://arxiv.org/abs/2506.02283", "authors": ["Sofoklis Kakouros", "Haoyu Chen"], "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This study examines the prosodic characteristics associated with winning and\nlosing in post-match tennis interviews. Additionally, this research explores\nthe potential to classify match outcomes solely based on post-match interview\nrecordings using prosodic features and self-supervised learning (SSL)\nrepresentations. By analyzing prosodic elements such as pitch and intensity,\nalongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine\nwhether an athlete has won or lost their match. Traditional acoustic features\nand deep speech representations are extracted from the data, and machine\nlearning classifiers are employed to distinguish between winning and losing\nplayers. Results indicate that SSL representations effectively differentiate\nbetween winning and losing outcomes, capturing subtle speech patterns linked to\nemotional states. At the same time, prosodic cues -- such as pitch variability\n-- remain strong indicators of victory.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u7f51\u7403\u8d5b\u540e\u91c7\u8bbf\u7684\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u548c\u5f3a\u5ea6\uff09\u53ca\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08\u5982Wav2Vec 2.0\u548cHuBERT\uff09\uff0c\u63a2\u7d22\u4ec5\u51ed\u91c7\u8bbf\u5f55\u97f3\u5206\u7c7b\u6bd4\u8d5b\u80dc\u8d1f\u7684\u53ef\u884c\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u6709\u6548\u533a\u5206\u80dc\u8d1f\uff0c\u800c\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u53d8\u5316\uff09\u4ecd\u662f\u80dc\u5229\u7684\u5f3a\u6307\u6807\u3002", "motivation": "\u63a2\u7d22\u8d5b\u540e\u91c7\u8bbf\u4e2d\u97f5\u5f8b\u7279\u5f81\u4e0e\u6bd4\u8d5b\u7ed3\u679c\u7684\u5173\u7cfb\uff0c\u5e76\u5c1d\u8bd5\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u80dc\u8d1f\uff0c\u4ee5\u63ed\u793a\u60c5\u7eea\u72b6\u6001\u4e0e\u8bed\u97f3\u6a21\u5f0f\u7684\u5173\u8054\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u58f0\u5b66\u7279\u5f81\u548c\u6df1\u5ea6\u8bed\u97f3\u8868\u5f81\uff08\u5982Wav2Vec 2.0\u548cHuBERT\uff09\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u533a\u5206\u80dc\u8d1f\u3002", "result": "\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u6709\u6548\u533a\u5206\u80dc\u8d1f\uff0c\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u53d8\u5316\uff09\u662f\u80dc\u5229\u7684\u5f3a\u6307\u6807\u3002", "conclusion": "\u8d5b\u540e\u91c7\u8bbf\u7684\u97f5\u5f8b\u7279\u5f81\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u53ef\u7528\u4e8e\u5206\u7c7b\u6bd4\u8d5b\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u60c5\u7eea\u72b6\u6001\u4e0e\u8bed\u97f3\u6a21\u5f0f\u7684\u5173\u8054\u3002"}}
{"id": "2506.02294", "pdf": "https://arxiv.org/pdf/2506.02294", "abs": "https://arxiv.org/abs/2506.02294", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u865a\u5047\u7279\u5f81\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u9c81\u68d2\u7684\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u4e5f\u80fd\u5bf9\u8fd9\u4e9b\u672a\u77e5\u7684\u865a\u5047\u7279\u5f81\u5177\u6709\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6559\u5e08\u4e0e\u5b66\u751f\u6a21\u578b\u7684\u5206\u6b67\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\u3002", "result": "\u5728CelebA\u3001SpuCo Birds\u548c\u865a\u5047ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6700\u5dee\u7ec4\u548c\u5e73\u5747\u7ec4\u51c6\u786e\u7387\uff0c\u4ee5\u53ca\u865a\u5047mAUC\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u6570\u636e\u589e\u5f3a\u57fa\u7ebf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.02298", "pdf": "https://arxiv.org/pdf/2506.02298", "abs": "https://arxiv.org/abs/2506.02298", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LAM Simulator framework for agentic data generation", "summary": "Large Action Models (LAMs) for AI Agents offer incredible potential but face\nchallenges due to the need for high-quality training data, especially for\nmulti-steps tasks that involve planning, executing tool calls, and responding\nto feedback. To address these issues, we present LAM SIMULATOR, a comprehensive\nframework designed for online exploration of agentic tasks with high-quality\nfeedback. Our framework features a dynamic task query generator, an extensive\ncollection of tools, and an interactive environment where Large Language Model\n(LLM) Agents can call tools and receive real-time feedback. This setup enables\nLLM Agents to explore and solve tasks autonomously, facilitating the discovery\nof multiple approaches to tackle any given task. The resulting action\ntrajectory data are then used to create high-quality training datasets for\nLAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,\nhighlight the effectiveness of LAM SIMULATOR: models trained with\nself-generated datasets using our framework achieve significant performance\ngains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR\nrequires minimal human input during dataset creation, highlighting LAM\nSIMULATOR's efficiency and effectiveness in speeding up development of AI\nagents.", "AI": {"tldr": "LAM SIMULATOR\u662f\u4e00\u4e2a\u4e3aAI\u4ee3\u7406\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u751f\u6210\u3001\u5de5\u5177\u8c03\u7528\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u52a8\u4f5c\u6a21\u578b\uff08LAMs\uff09\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u9762\u4e34\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u6570\u636e\u5e76\u63d0\u5347\u4ee3\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faLAM SIMULATOR\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u4efb\u52a1\u67e5\u8be2\u751f\u6210\u5668\u3001\u5de5\u5177\u5e93\u548c\u4ea4\u4e92\u73af\u5883\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u5e76\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728ToolBench\u548cCRMArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u81ea\u751f\u6210\u6570\u636e\u7684\u6a21\u578b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe49.3%\u3002", "conclusion": "LAM SIMULATOR\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\uff0c\u52a0\u901f\u4e86AI\u4ee3\u7406\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.02295", "pdf": "https://arxiv.org/pdf/2506.02295", "abs": "https://arxiv.org/abs/2506.02295", "authors": ["Ahmed Wasfy", "Omer Nacar", "Abdelakreem Elkhateb", "Mahmoud Reda", "Omar Elshehy", "Adel Ammar", "Wadii Boulila"], "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The inherent complexities of Arabic script; its cursive nature, diacritical\nmarks (tashkeel), and varied typography, pose persistent challenges for Optical\nCharacter Recognition (OCR). We present Qari-OCR, a series of vision-language\nmodels derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic\nthrough iterative fine-tuning on specialized synthetic datasets. Our leading\nmodel, QARI v0.2, establishes a new open-source state-of-the-art with a Word\nError Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score\nof 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling\nof tashkeel, diverse fonts, and document layouts, alongside impressive\nperformance on low-resolution images. Further explorations (QARI v0.3) showcase\nstrong potential for structural document understanding and handwritten text.\nThis work delivers a marked improvement in Arabic OCR accuracy and efficiency,\nwith all models and datasets released to foster further research.", "AI": {"tldr": "Qari-OCR\u662f\u4e00\u7cfb\u5217\u57fa\u4e8eQwen2-VL-2B-Instruct\u4f18\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u963f\u62c9\u4f2f\u8bedOCR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u811a\u672c\u7684\u590d\u6742\u6027\uff08\u5982\u8fde\u5199\u3001\u53d8\u97f3\u7b26\u53f7\u548c\u591a\u6837\u5b57\u4f53\uff09\u5bf9OCR\u6280\u672f\u63d0\u51fa\u4e86\u6301\u7eed\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u4e13\u4e1a\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f18\u5316Qwen2-VL-2B-Instruct\u6a21\u578b\uff0c\u5f00\u53d1\u4e86Qari-OCR\u7cfb\u5217\u6a21\u578b\u3002", "result": "QARI v0.2\u5728\u53d8\u97f3\u7b26\u53f7\u4e30\u5bcc\u7684\u6587\u672c\u4e0a\u5b9e\u73b0\u4e86WER 0.160\u3001CER 0.061\u548cBLEU 0.737\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Qari-OCR\u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u8bedOCR\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5f00\u6e90\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.02302", "pdf": "https://arxiv.org/pdf/2506.02302", "abs": "https://arxiv.org/abs/2506.02302", "authors": ["Russell Scheinberg", "Ameeta Agrawal", "Amber Shore", "So Young Lee"], "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Findings", "summary": "Large language models (LLMs) can explain grammatical rules, yet they often\nfail to apply those rules when judging sentence acceptability. We present\n\"grammar prompting\", an explain-then-process paradigm: a large LLM first\nproduces a concise explanation of the relevant syntactic phenomenon, then that\nexplanation is fed back as additional context to the target model -- either an\nLLM or a smaller language model (SLM) -- before deciding which sentence of a\nminimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian\nRuBLiMP benchmarks, this simple prompt design yields substantial improvements\nover strong baselines across many syntactic phenomena. Feeding an LLM's\nmetalinguistic explanation back to the target model bridges the gap between\nknowing a rule and using it. On SLMs, grammar prompting alone trims the average\nLLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by\n56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,\nlanguage-agnostic cue lets low-cost SLMs approach frontier-LLM performance in\nmultilingual settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u6cd5\u63d0\u793a\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5148\u89e3\u91ca\u8bed\u6cd5\u89c4\u5219\uff0c\u518d\u5c06\u89e3\u91ca\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u76ee\u6807\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5224\u65ad\u53e5\u5b50\u53ef\u63a5\u53d7\u6027\u65f6\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u89e3\u91ca\u8bed\u6cd5\u89c4\u5219\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5374\u96be\u4ee5\u6b63\u786e\u8fd0\u7528\u8fd9\u4e9b\u89c4\u5219\u3002", "method": "\u91c7\u7528\u201c\u89e3\u91ca-\u5904\u7406\u201d\u8303\u5f0f\uff0c\u5373LLM\u5148\u751f\u6210\u8bed\u6cd5\u73b0\u8c61\u7684\u7b80\u6d01\u89e3\u91ca\uff0c\u518d\u5c06\u89e3\u91ca\u4f5c\u4e3a\u989d\u5916\u4e0a\u4e0b\u6587\u8f93\u5165\u76ee\u6807\u6a21\u578b\uff08LLM\u6216\u5c0f\u578b\u8bed\u8a00\u6a21\u578bSLM\uff09\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86LLM\u4e0eSLM\u4e4b\u95f4\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "conclusion": "\u8bed\u6cd5\u63d0\u793a\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u8bed\u8a00\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5e2e\u52a9\u4f4e\u6210\u672cSLM\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u63a5\u8fd1\u524d\u6cbfLLM\u7684\u6027\u80fd\u3002"}}
{"id": "2506.02327", "pdf": "https://arxiv.org/pdf/2506.02327", "abs": "https://arxiv.org/abs/2506.02327", "authors": ["Yijun Yang", "Zhao-Yang Wang", "Qiuping Liu", "Shuwen Sun", "Kang Wang", "Rama Chellappa", "Zongwei Zhou", "Alan Yuille", "Lei Zhu", "Yu-Dong Zhang", "Jieneng Chen"], "title": "Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning", "categories": ["cs.CV"], "comment": null, "summary": "Providing effective treatment and making informed clinical decisions are\nessential goals of modern medicine and clinical care. We are interested in\nsimulating disease dynamics for clinical decision-making, leveraging recent\nadvances in large generative models. To this end, we introduce the Medical\nWorld Model (MeWM), the first world model in medicine that visually predicts\nfuture disease states based on clinical decisions. MeWM comprises (i)\nvision-language models to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates action plans, such as\nclinical treatments, while the dynamics model simulates tumor progression or\nregression under given treatment conditions. Building on this, we propose the\ninverse dynamics model that applies survival analysis to the simulated\npost-treatment tumor, enabling the evaluation of treatment efficacy and the\nselection of the optimal clinical action plan. As a result, the proposed MeWM\nsimulates disease dynamics by synthesizing post-treatment tumors, with\nstate-of-the-art specificity in Turing tests evaluated by radiologists.\nSimultaneously, its inverse dynamics model outperforms medical-specialized GPTs\nin optimizing individualized treatment protocols across all metrics. Notably,\nMeWM improves clinical decision-making for interventional physicians, boosting\nF1-score in selecting the optimal TACE protocol by 13%, paving the way for\nfuture integration of medical world models as the second readers.", "AI": {"tldr": "MeWM\u662f\u4e00\u79cd\u533b\u5b66\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u9884\u6d4b\u75be\u75c5\u672a\u6765\u72b6\u6001\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u751f\u5b58\u5206\u6790\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u73b0\u4ee3\u533b\u5b66\u9700\u8981\u6709\u6548\u7684\u6cbb\u7597\u548c\u4e34\u5e8a\u51b3\u7b56\uff0cMeWM\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6a21\u62df\u75be\u75c5\u52a8\u6001\uff0c\u8f85\u52a9\u533b\u751f\u9009\u62e9\u6700\u4f73\u6cbb\u7597\u65b9\u6848\u3002", "method": "MeWM\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u7b56\u7565\u6a21\u578b\uff09\u548c\u80bf\u7624\u751f\u6210\u6a21\u578b\uff08\u52a8\u6001\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u9006\u52a8\u6001\u6a21\u578b\u8bc4\u4f30\u6cbb\u7597\u6548\u679c\u5e76\u4f18\u5316\u6cbb\u7597\u65b9\u6848\u3002", "result": "MeWM\u5728\u751f\u6210\u80bf\u7624\u56fe\u50cf\u7684\u7279\u5f02\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u4f18\u5316\u4e2a\u4f53\u5316\u6cbb\u7597\u65b9\u6848\u4e0a\u8d85\u8d8a\u533b\u5b66\u4e13\u7528GPT\uff0c\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u7684F1\u5206\u657013%\u3002", "conclusion": "MeWM\u4e3a\u533b\u5b66\u4e16\u754c\u6a21\u578b\u7684\u672a\u6765\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u53ef\u4f5c\u4e3a\u7b2c\u4e8c\u8bfb\u8005\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2506.02321", "pdf": "https://arxiv.org/pdf/2506.02321", "abs": "https://arxiv.org/abs/2506.02321", "authors": ["Pegah Alipoormolabashi", "Ajay Patel", "Niranjan Balasubramanian"], "title": "Quantifying Misattribution Unfairness in Authorship Attribution", "categories": ["cs.CL"], "comment": null, "summary": "Authorship misattribution can have profound consequences in real life. In\nforensic settings simply being considered as one of the potential authors of an\nevidential piece of text or communication can result in undesirable scrutiny.\nThis raises a fairness question: Is every author in the candidate pool at equal\nrisk of misattribution? Standard evaluation measures for authorship attribution\nsystems do not explicitly account for this notion of fairness. We introduce a\nsimple measure, Misattribution Unfairness Index (MAUIk), which is based on how\noften authors are ranked in the top k for texts they did not write. Using this\nmeasure we quantify the unfairness of five models on two different datasets.\nAll models exhibit high levels of unfairness with increased risks for some\nauthors. Furthermore, we find that this unfairness relates to how the models\nembed the authors as vectors in the latent search space. In particular, we\nobserve that the risk of misattribution is higher for authors closer to the\ncentroid (or center) of the embedded authors in the haystack. These results\nindicate the potential for harm and the need for communicating with and\ncalibrating end users on misattribution risk when building and providing such\nmodels for downstream use.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u4f5c\u8005\u5f52\u5c5e\u4e0d\u516c\u5e73\u6027\u7684\u6307\u6807MAUIk\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f5c\u8005\u88ab\u9519\u8bef\u5f52\u56e0\u7684\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u663e\u8457\u4e0d\u516c\u5e73\u6027\uff0c\u67d0\u4e9b\u4f5c\u8005\u98ce\u9669\u66f4\u9ad8\u3002", "motivation": "\u5728\u6cd5\u533b\u7b49\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u4f5c\u8005\u5f52\u5c5e\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u8003\u8651\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165Misattribution Unfairness Index (MAUIk)\uff0c\u57fa\u4e8e\u4f5c\u8005\u5728\u672a\u64b0\u5199\u6587\u672c\u4e2d\u88ab\u6392\u5728\u524dk\u4f4d\u7684\u9891\u7387\uff0c\u8bc4\u4f30\u4e94\u79cd\u6a21\u578b\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u516c\u5e73\u6027\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e0d\u516c\u5e73\u6027\uff0c\u90e8\u5206\u4f5c\u8005\u98ce\u9669\u66f4\u9ad8\uff1b\u4e0d\u516c\u5e73\u6027\u4e0e\u6a21\u578b\u5728\u6f5c\u5728\u641c\u7d22\u7a7a\u95f4\u4e2d\u5bf9\u4f5c\u8005\u7684\u5411\u91cf\u5d4c\u5165\u65b9\u5f0f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u53ef\u80fd\u5e26\u6765\u7684\u6f5c\u5728\u5371\u5bb3\uff0c\u5efa\u8bae\u5728\u6a21\u578b\u6784\u5efa\u548c\u4f7f\u7528\u4e2d\u4e0e\u7ec8\u7aef\u7528\u6237\u6c9f\u901a\u5e76\u6821\u51c6\u8bef\u5f52\u98ce\u9669\u3002"}}
{"id": "2506.02334", "pdf": "https://arxiv.org/pdf/2506.02334", "abs": "https://arxiv.org/abs/2506.02334", "authors": ["Duo Liu", "Zhiquan Tan", "Linglan Zhao", "Zhongqiang Zhang", "Xiangzhong Fang", "Weiran Huang"], "title": "Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization", "categories": ["cs.CV"], "comment": "ICML2025 Poster", "summary": "Generalized Category Discovery (GCD) aims to identify unlabeled samples by\nleveraging the base knowledge from labeled ones, where the unlabeled set\nconsists of both base and novel classes. Since clustering methods are\ntime-consuming at inference, parametric-based approaches have become more\npopular. However, recent parametric-based methods suffer from inferior base\ndiscrimination due to unreliable self-supervision. To address this issue, we\npropose a Reciprocal Learning Framework (RLF) that introduces an auxiliary\nbranch devoted to base classification. During training, the main branch filters\nthe pseudo-base samples to the auxiliary branch. In response, the auxiliary\nbranch provides more reliable soft labels for the main branch, leading to a\nvirtuous cycle. Furthermore, we introduce Class-wise Distribution\nRegularization (CDR) to mitigate the learning bias towards base classes. CDR\nessentially increases the prediction confidence of the unlabeled data and\nboosts the novel class performance. Combined with both components, our proposed\nmethod, RLCD, achieves superior performance in all classes with negligible\nextra computation. Comprehensive experiments across seven GCD datasets validate\nits superiority. Our codes are available at https://github.com/APORduo/RLCD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRLF\u7684\u4e92\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u5206\u652f\u548c\u7c7b\u5206\u5e03\u6b63\u5219\u5316\uff08CDR\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u57fa\u7840\u7c7b\u522b\u8bc6\u522b\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u7c7b\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u4e2d\u56e0\u4e0d\u53ef\u9760\u7684\u81ea\u76d1\u7763\u5bfc\u81f4\u57fa\u7840\u7c7b\u522b\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e92\u5b66\u4e60\u6846\u67b6\uff08RLF\uff09\uff0c\u5305\u542b\u4e3b\u5206\u652f\u548c\u8f85\u52a9\u5206\u652f\uff0c\u4e3b\u5206\u652f\u7b5b\u9009\u4f2a\u57fa\u7840\u6837\u672c\u7ed9\u8f85\u52a9\u5206\u652f\uff0c\u8f85\u52a9\u5206\u652f\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8f6f\u6807\u7b7e\u3002\u540c\u65f6\u5f15\u5165\u7c7b\u5206\u5e03\u6b63\u5219\u5316\uff08CDR\uff09\u4ee5\u51cf\u5c11\u5bf9\u57fa\u7840\u7c7b\u522b\u7684\u5b66\u4e60\u504f\u5dee\u3002", "result": "\u5728\u4e03\u4e2aGCD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRLCD\u65b9\u6cd5\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "RLF\u548cCDR\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86GCD\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u53c2\u6570\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02326", "pdf": "https://arxiv.org/pdf/2506.02326", "abs": "https://arxiv.org/abs/2506.02326", "authors": ["Berk Atil", "Namrata Sureddy", "Rebecca J. Passonneau"], "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.", "AI": {"tldr": "TRuST\u662f\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u6bd2\u6027\u68c0\u6d4b\uff0c\u5305\u542b\u6bd2\u6027\u3001\u76ee\u6807\u793e\u4f1a\u7fa4\u4f53\u548c\u6bd2\u6027\u7247\u6bb5\u6807\u7b7e\u3002\u7814\u7a76\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u5728\u6bd2\u6027\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\uff0c\u4f46\u67d0\u4e9b\u793e\u4f1a\u7fa4\u4f53\u8868\u73b0\u4ecd\u4e0d\u4f73\u3002", "motivation": "\u5728\u7ebf\u5185\u5bb9\u4e2d\u7684\u6bd2\u6027\u95ee\u9898\u5bf9\u5fc3\u7406\u548c\u793e\u4f1a\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u73b0\u6709\u6570\u636e\u96c6\u521b\u5efaTRuST\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u76ee\u6807\u7fa4\u4f53\u6807\u7b7e\uff0c\u5e76\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6bd2\u6027\u68c0\u6d4b\u3001\u76ee\u6807\u7fa4\u4f53\u8bc6\u522b\u548c\u6bd2\u6027\u7247\u6bb5\u63d0\u53d6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\uff0c\u4f46\u5bf9\u67d0\u4e9b\u793e\u4f1a\u7fa4\u4f53\u6548\u679c\u4e0d\u4f73\uff1b\u63a8\u7406\u80fd\u529b\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u4e0a\u8f83\u5f31\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u6bd2\u6027\u68c0\u6d4b\u6a21\u578b\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7279\u5b9a\u793e\u4f1a\u7fa4\u4f53\u3002"}}
{"id": "2506.02354", "pdf": "https://arxiv.org/pdf/2506.02354", "abs": "https://arxiv.org/abs/2506.02354", "authors": ["Junjie Li", "Nan Zhang", "Xiaoyang Qu", "Kai Lu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Object Navigation (ObjectNav) is a fundamental task in embodied artificial\nintelligence. Although significant progress has been made in semantic map\nconstruction and target direction prediction in current research, redundant\nexploration and exploration failures remain inevitable. A critical but\nunderexplored direction is the timely termination of exploration to overcome\nthese challenges. We observe a diminishing marginal effect between exploration\nsteps and exploration rates and analyze the cost-benefit relationship of\nexploration. Inspired by this, we propose RATE-Nav, a Region-Aware\nTermination-Enhanced method. It includes a geometric predictive region\nsegmentation algorithm and region-Based exploration estimation algorithm for\nexploration rate calculation. By leveraging the visual question answering\ncapabilities of visual language models (VLMs) and exploration rates enables\nefficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of\n31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav\nshows approximately 10% improvement over previous zero-shot methods.", "AI": {"tldr": "RATE-Nav\u662f\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u7ec8\u6b62\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u9884\u6d4b\u533a\u57df\u5206\u5272\u548c\u533a\u57df\u63a2\u7d22\u4f30\u8ba1\u7b97\u6cd5\u4f18\u5316\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u76ee\u6807\u5bfc\u822a\u7814\u7a76\u4e2d\uff0c\u5197\u4f59\u63a2\u7d22\u548c\u63a2\u7d22\u5931\u8d25\u95ee\u9898\u7a81\u51fa\uff0c\u63a2\u7d22\u7ec8\u6b62\u7684\u65f6\u673a\u662f\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u65b9\u5411\u3002", "method": "\u63d0\u51faRATE-Nav\u65b9\u6cd5\uff0c\u7ed3\u5408\u51e0\u4f55\u9884\u6d4b\u533a\u57df\u5206\u5272\u7b97\u6cd5\u548c\u533a\u57df\u63a2\u7d22\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5b9e\u73b0\u9ad8\u6548\u7ec8\u6b62\u3002", "result": "\u5728HM3D\u6570\u636e\u96c6\u4e0a\u6210\u529f\u7387\u4e3a67.8%\uff0cSPL\u4e3a31.3%\uff1b\u5728MP3D\u6570\u636e\u96c6\u4e0a\u6bd4\u96f6\u6837\u672c\u65b9\u6cd5\u63d0\u5347\u7ea610%\u3002", "conclusion": "RATE-Nav\u901a\u8fc7\u4f18\u5316\u63a2\u7d22\u7ec8\u6b62\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338", "abs": "https://arxiv.org/abs/2506.02338", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7acb\u6784\u5efa\u957f\u94fe\u63a8\u7406\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u94fe\u63a8\u7406LLM\u751f\u6210Long CoT Collection\uff0c\u9a8c\u8bc1\u5176\u8d28\u91cf\u63a5\u8fd1R1\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u51cf\u5c11\u5bf9\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982R1\uff09\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u72ec\u7acbLRM\u53d1\u5c55\u3002", "method": "\u5229\u7528\u77ed\u94fe\u63a8\u7406LLM\u6784\u5efaLong CoT Collection\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u7ba1\u9053\u6280\u672f\u5f15\u5165\u65b0\u63a8\u7406\u7b56\u7565\u3002", "result": "\u6570\u636e\u96c6\u8d28\u91cf\u63a5\u8fd1R1\uff0c\u8bad\u7ec3\u540e\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff082-3\u500d\u589e\u76ca\uff09\u3002", "conclusion": "\u72ec\u7acb\u6784\u5efa\u7684\u957f\u94fe\u63a8\u7406\u6570\u636e\u96c6\u53ef\u884c\u4e14\u6709\u6548\uff0c\u4e3aLRM\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.02356", "pdf": "https://arxiv.org/pdf/2506.02356", "abs": "https://arxiv.org/abs/2506.02356", "authors": ["Woojeong Jin", "Seongchan Kim", "Seungryong Kim"], "title": "InterRVOS: Interaction-aware Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring video object segmentation aims to segment the object in a video\ncorresponding to a given natural language expression. While prior works have\nexplored various referring scenarios, including motion-centric or\nmulti-instance expressions, most approaches still focus on localizing a single\ntarget object in isolation. However, in comprehensive video understanding, an\nobject's role is often defined by its interactions with other entities, which\nare largely overlooked in existing datasets and models. In this work, we\nintroduce Interaction-aware referring video object sgementation (InterRVOS), a\nnew task that requires segmenting both actor and target entities involved in an\ninteraction. Each interactoin is described through a pair of complementary\nexpressions from different semantic perspectives, enabling fine-grained\nmodeling of inter-object relationships. To tackle this task, we propose\nInterRVOS-8K, the large-scale and automatically constructed dataset containing\ndiverse interaction-aware expressions with corresponding masks, including\nchallenging cases such as motion-only multi-instance expressions. We also\npresent a baseline architecture, ReVIOSa, designed to handle actor-target\nsegmentation from a single expression, achieving strong performance in both\nstandard and interaction-focused settings. Furthermore, we introduce an\nactor-target-aware evalaution setting that enables a more targeted assessment\nof interaction understanding. Experimental results demonstrate that our\napproach outperforms prior methods in modeling complex object interactions for\nreferring video object segmentation task, establishing a strong foundation for\nfuture research in interaction-centric video understanding. Our project page is\navailable at\n\\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1InterRVOS\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u4e2d\u5bf9\u8c61\u95f4\u7684\u4ea4\u4e92\u5206\u5272\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6InterRVOS-8K\u548c\u57fa\u7ebf\u6a21\u578bReVIOSa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u76ee\u6807\u5bf9\u8c61\u7684\u5206\u5272\uff0c\u5ffd\u89c6\u4e86\u5bf9\u8c61\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u800c\u4ea4\u4e92\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faInterRVOS\u4efb\u52a1\uff0c\u6784\u5efa\u6570\u636e\u96c6InterRVOS-8K\uff0c\u8bbe\u8ba1\u57fa\u7ebf\u6a21\u578bReVIOSa\u5904\u7406\u4ea4\u4e92\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5bf9\u8c61\u4ea4\u4e92\u5efa\u6a21\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u4ea4\u4e92\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.02347", "pdf": "https://arxiv.org/pdf/2506.02347", "abs": "https://arxiv.org/abs/2506.02347", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStoryteller\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8eSVO\u4e09\u5143\u7ec4\u7684\u8282\u70b9\u7ed3\u6784\u548c\u52a8\u6001\u6a21\u5757\uff08STORYLINE\u4e0eNEKG\uff09\u63d0\u5347\u81ea\u52a8\u751f\u6210\u6545\u4e8b\u7684\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u6545\u4e8b\u4f53\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u57fa\u4e8eSVO\u4e09\u5143\u7ec4\u7684\u8282\u70b9\u7ed3\u6784\uff0c\u5e76\u6574\u5408\u52a8\u6001\u6a21\u5757STORYLINE\u548cNEKG\uff0c\u6301\u7eed\u4ea4\u4e92\u4f18\u5316\u6545\u4e8b\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStoryteller\u5728\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u4e2d\u5e73\u5747\u80dc\u7387\u8fbe84.33%\uff0c\u4e14\u5728\u521b\u610f\u3001\u8fde\u8d2f\u6027\u3001\u5438\u5f15\u529b\u548c\u76f8\u5173\u6027\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Storyteller\u901a\u8fc7\u7cfb\u7edf\u6027\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u7684\u8d28\u91cf\uff0c\u4e3aAI\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.02358", "pdf": "https://arxiv.org/pdf/2506.02358", "abs": "https://arxiv.org/abs/2506.02358", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Sun"], "title": "RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The classification of the type of road surface (RSC) aims to utilize pavement\nfeatures to identify the roughness, wet and dry conditions, and material\ninformation of the road surface. Due to its ability to effectively enhance road\nsafety and traffic management, it has received widespread attention in recent\nyears. In autonomous driving, accurate RSC allows vehicles to better understand\nthe road environment, adjust driving strategies, and ensure a safer and more\nefficient driving experience. For a long time, vision-based RSC has been\nfavored. However, existing visual classification methods have overlooked the\nexploration of fine-grained classification of pavement types (such as similar\npavement textures). In this work, we propose a pure vision-based fine-grained\nRSC method for autonomous driving scenarios, which fuses local and global\nfeature information through the stacking of convolutional and transformer\nmodules. We further explore the stacking strategies of local and global feature\nextraction modules to find the optimal feature extraction strategy. In\naddition, since fine-grained tasks also face the challenge of relatively large\nintra-class differences and relatively small inter-class differences, we\npropose a Foreground-Background Module (FBM) that effectively extracts\nfine-grained context features of the pavement, enhancing the classification\nability for complex pavements. Experiments conducted on a large-scale pavement\ndataset containing one million samples and a simplified dataset reorganized\nfrom this dataset achieved Top-1 classification accuracies of 92.52% and\n96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods.\nThese results demonstrate that RoadFormer outperforms existing methods in RSC\ntasks, providing significant progress in improving the reliability of pavement\nperception in autonomous driving systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u7ec6\u7c92\u5ea6\u9053\u8def\u8868\u9762\u5206\u7c7b\u65b9\u6cd5\uff08RoadFormer\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u548cTransformer\u6a21\u5757\u63d0\u53d6\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u524d\u666f-\u80cc\u666f\u6a21\u5757\uff08FBM\uff09\u63d0\u5347\u5206\u7c7b\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9053\u8def\u8868\u9762\u5206\u7c7b\uff08RSC\uff09\u5bf9\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u4ea4\u901a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff08\u5982\u76f8\u4f3c\u7eb9\u7406\u7684\u8def\u9762\uff09\u3002", "method": "\u63d0\u51fa\u7eaf\u89c6\u89c9\u7684\u7ec6\u7c92\u5ea6RSC\u65b9\u6cd5\uff0c\u878d\u5408\u5377\u79ef\u548cTransformer\u6a21\u5757\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1FBM\u6a21\u5757\u589e\u5f3a\u590d\u6742\u8def\u9762\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0aTop-1\u51c6\u786e\u7387\u8fbe92.52%\u548c96.50%\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53475.69%\u81f312.84%\u3002", "conclusion": "RoadFormer\u5728RSC\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8def\u9762\u611f\u77e5\u53ef\u9760\u6027\u3002"}}
{"id": "2506.02350", "pdf": "https://arxiv.org/pdf/2506.02350", "abs": "https://arxiv.org/abs/2506.02350", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTruthOverTricks\u8bc4\u4f30\u8303\u5f0f\uff0c\u63ed\u793a\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51faSMF\u6570\u636e\u589e\u5f3a\u6846\u67b6\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\u5e38\u4f9d\u8d56\u4e0e\u8bad\u7ec3\u6570\u636e\u76f8\u5173\u7684\u8868\u9762\u7ebf\u7d22\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\uff0c\u4e14\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faTruthOverTricks\u8bc4\u4f30\u8303\u5f0f\uff0c\u5206\u7c7b\u8868\u9762\u7ebf\u7d22\u884c\u4e3a\uff0c\u8bc4\u4f30\u4e03\u79cd\u68c0\u6d4b\u5668\uff0c\u5e76\u5f00\u53d1SMF\u6846\u67b6\uff08\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\uff09\u3002", "result": "\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u81ea\u7136\u548c\u5bf9\u6297\u6027\u8868\u9762\u7ebf\u7d22\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0cSMF\u572816\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "SMF\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u51cf\u5c11\u5bf9\u8868\u9762\u7ebf\u7d22\u7684\u4f9d\u8d56\uff0c\u4fc3\u8fdb\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5668\u7684\u5f00\u53d1\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.02359", "pdf": "https://arxiv.org/pdf/2506.02359", "abs": "https://arxiv.org/abs/2506.02359", "authors": ["Brent A. Griffin", "Manushree Gangwar", "Jacob Sela", "Jason J. Corso"], "title": "Auto-Labeling Data for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Great labels make great models. However, traditional labeling approaches for\ntasks like object detection have substantial costs at scale. Furthermore,\nalternatives to fully-supervised object detection either lose functionality or\nrequire larger models with prohibitive computational costs for inference at\nscale. To that end, this paper addresses the problem of training standard\nobject detection models without any ground truth labels. Instead, we configure\npreviously-trained vision-language foundation models to generate\napplication-specific pseudo \"ground truth\" labels. These auto-generated labels\ndirectly integrate with existing model training frameworks, and we subsequently\ntrain lightweight detection models that are computationally efficient. In this\nway, we avoid the costs of traditional labeling, leverage the knowledge of\nvision-language models, and keep the efficiency of lightweight models for\npractical application. We perform exhaustive experiments across multiple\nlabeling configurations, downstream inference models, and datasets to establish\nbest practices and set an extensive auto-labeling benchmark. From our results,\nwe find that our approach is a viable alternative to standard labeling in that\nit maintains competitive performance on multiple datasets and substantially\nreduces labeling time and costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u6807\u7b7e\u7684\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4ece\u800c\u964d\u4f4e\u6210\u672c\u5e76\u4fdd\u6301\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7269\u4f53\u68c0\u6d4b\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u800c\u66ff\u4ee3\u65b9\u6cd5\u8981\u4e48\u529f\u80fd\u53d7\u9650\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u914d\u7f6e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u63a5\u8fd1\u4f20\u7edf\u6807\u6ce8\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6807\u6ce8\u65f6\u95f4\u548c\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u517c\u5177\u6027\u80fd\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "AI": {"tldr": "DIAMOND\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u68d2\u7403\u9ad8\u5149\u65f6\u523b\u6458\u8981\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u4f53\u80b2\u5206\u6790\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982WPA\u6392\u540d\u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u4e8b\u4ef6\u68c0\u6d4b\uff09\u7f3a\u4e4f\u6218\u7565\u6df1\u5ea6\u548c\u6545\u4e8b\u6027\uff0c\u800c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "DIAMOND\u7ed3\u5408\u4e86\u68d2\u7403\u7edf\u8ba1\u7279\u5f81\uff08\u5982Win Expectancy\u3001WPA\u548cLeverage Index\uff09\u4e0eLLM\u6a21\u5757\uff0c\u91cf\u5316\u6bd4\u8d5b\u91cd\u8981\u6027\u5e76\u589e\u5f3a\u4e0a\u4e0b\u6587\u53d9\u4e8b\u4ef7\u503c\u3002", "result": "\u5728\u97e9\u56fd\u68d2\u7403\u8054\u76df\u7684\u6d4b\u8bd5\u4e2d\uff0cDIAMOND\u7684F1\u5206\u6570\u4ece42.9%\uff08\u4ec5WPA\uff09\u63d0\u5347\u81f384.8%\uff0c\u4f18\u4e8e\u5546\u4e1a\u548c\u7edf\u8ba1\u57fa\u7ebf\u3002", "conclusion": "DIAMOND\u5c55\u793a\u4e86\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u6846\u67b6\u5728\u4f53\u80b2\u4e8b\u4ef6\u6458\u8981\u4e2d\u7684\u6f5c\u529b\uff0c\u5c3d\u7ba1\u89c4\u6a21\u6709\u9650\u3002"}}
{"id": "2506.02364", "pdf": "https://arxiv.org/pdf/2506.02364", "abs": "https://arxiv.org/abs/2506.02364", "authors": ["Liang Li", "Jianli Zhao", "Sheng Fang", "Siyu Chen", "Hui Sun"], "title": "A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer", "categories": ["cs.CV"], "comment": "11 pages,6 figures", "summary": "Hyperspectral images (HSIs) are often degraded by complex mixed noise during\nacquisition and transmission, making effective denoising essential for\nsubsequent analysis. Recent hybrid approaches that bridge model-driven and\ndata-driven paradigms have shown great promise. However, most of these\napproaches lack effective alternation between different priors or modules,\nresulting in loosely coupled regularization and insufficient exploitation of\ntheir complementary strengths. Inspired by tensor robust principal component\nanalysis (TRPCA), we propose a novel deep unfolding network (DU-TRPCA) that\nenforces stage-wise alternation between two tightly integrated modules:\nlow-rank and sparse. The low-rank module employs thresholded tensor singular\nvalue decomposition (t-SVD), providing a widely adopted convex surrogate for\ntensor low-rankness and has been demonstrated to effectively capture the global\nspatial-spectral structure of HSIs. The Top-K sparse transformer module\nadaptively imposes sparse constraints, directly matching the sparse\nregularization in TRPCA and enabling effective removal of localized outliers\nand complex noise. This tightly coupled architecture preserves the stage-wise\nalternation between low-rank approximation and sparse refinement inherent in\nTRPCA, while enhancing representational capacity through attention mechanisms.\nExtensive experiments on synthetic and real-world HSIs demonstrate that\nDU-TRPCA surpasses state-of-the-art methods under severe mixed noise, while\noffering interpretability benefits and stable denoising dynamics inspired by\niterative optimization. Code is available at\nhttps://github.com/liangli97/TRPCA-Deep-Unfolding-HSI-Denoising.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08TRPCA\uff09\u7684\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\uff08DU-TRPCA\uff09\uff0c\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u7684\u4f4e\u79e9\u548c\u7a00\u758f\u6a21\u5757\uff0c\u6709\u6548\u53bb\u9664\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u7684\u6df7\u5408\u566a\u58f0\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSIs\uff09\u5728\u91c7\u96c6\u548c\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u5e38\u53d7\u590d\u6742\u6df7\u5408\u566a\u58f0\u5f71\u54cd\uff0c\u73b0\u6709\u6df7\u5408\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0d\u540c\u5148\u9a8c\u6216\u6a21\u5757\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u7ed3\u5408\u4f4e\u79e9\u6a21\u5757\uff08\u9608\u503c\u5f20\u91cf\u5947\u5f02\u503c\u5206\u89e3\uff09\u548c\u7a00\u758f\u6a21\u5757\uff08Top-K\u7a00\u758f\u53d8\u6362\u5668\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\u5b9e\u73b0\u7d27\u5bc6\u8026\u5408\u7684\u4ea4\u66ff\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eHSIs\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDU-TRPCA\u5728\u4e25\u91cd\u6df7\u5408\u566a\u58f0\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u53bb\u566a\u52a8\u6001\u3002", "conclusion": "DU-TRPCA\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u7684\u4f4e\u79e9\u548c\u7a00\u758f\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u53bb\u566a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u4e86TRPCA\u7684\u8fed\u4ee3\u4f18\u5316\u4f18\u52bf\u3002"}}
{"id": "2506.02372", "pdf": "https://arxiv.org/pdf/2506.02372", "abs": "https://arxiv.org/abs/2506.02372", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions.", "AI": {"tldr": "AnswerCarefully\u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u65e5\u672cLLM\u8f93\u51fa\u5b89\u5168\u6027\u548c\u9002\u5f53\u6027\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b1800\u4e2a\u9700\u7279\u522b\u5173\u6ce8\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u591a\u79cd\u98ce\u9669\u7c7b\u522b\u3002\u901a\u8fc7\u5fae\u8c03\u65e5\u672cLLM\uff0c\u63d0\u5347\u4e86\u8f93\u51fa\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u56de\u7b54\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u65e5\u672cLLM\u8f93\u51fa\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6587\u5316\u9002\u5f53\u6027\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u82f1\u8bed\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "method": "\u624b\u52a8\u521b\u5efa\u53cd\u6620\u65e5\u672c\u793e\u4f1a\u6587\u5316\u80cc\u666f\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u65e5\u672cLLM\u3002", "result": "\u5fae\u8c03\u540eLLM\u8f93\u51fa\u5b89\u5168\u6027\u63d0\u5347\uff0c\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u56de\u7b54\u7684\u5b9e\u7528\u6027\uff1b12\u4e2a\u65e5\u672cLLM\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "AnswerCarefully\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u65e5\u672cLLM\u7684\u5b89\u5168\u6027\uff0c\u5e76\u652f\u6301\u8de8\u8bed\u8a00\u548c\u5730\u533a\u7684\u7c7b\u4f3c\u6570\u636e\u96c6\u5f00\u53d1\u3002"}}
{"id": "2506.02366", "pdf": "https://arxiv.org/pdf/2506.02366", "abs": "https://arxiv.org/abs/2506.02366", "authors": ["Qin Xie", "Qinghua Zhang", "Shuyin Xia"], "title": "Approximate Borderline Sampling using Granular-Ball for Classification Tasks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data sampling enhances classifier efficiency and robustness through data\ncompression and quality improvement. Recently, the sampling method based on\ngranular-ball (GB) has shown promising performance in generality and noisy\nclassification tasks. However, some limitations remain, including the absence\nof borderline sampling strategies and issues with class boundary blurring or\nshrinking due to overlap between GBs. In this paper, an approximate borderline\nsampling method using GBs is proposed for classification tasks. First, a\nrestricted diffusion-based GB generation (RD-GBG) method is proposed, which\nprevents GB overlaps by constrained expansion, preserving precise geometric\nrepresentation of GBs via redefined ones. Second, based on the concept of\nheterogeneous nearest neighbor, a GB-based approximate borderline sampling\n(GBABS) method is proposed, which is the first general sampling method capable\nof both borderline sampling and improving the quality of class noise datasets.\nAdditionally, since RD-GBG incorporates noise detection and GBABS focuses on\nborderline samples, GBABS performs outstandingly on class noise datasets\nwithout the need for an optimal purity threshold. Experimental results\ndemonstrate that the proposed methods outperform the GB-based sampling method\nand several representative sampling methods. Our source code is publicly\navailable at https://github.com/CherylTse/GBABS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5ea6\u7403\uff08GB\uff09\u7684\u8fd1\u4f3c\u8fb9\u754c\u91c7\u6837\u65b9\u6cd5\uff08GBABS\uff09\uff0c\u901a\u8fc7\u9650\u5236\u6269\u6563\u751f\u6210GB\uff08RD-GBG\uff09\u907f\u514d\u91cd\u53e0\uff0c\u5e76\u7ed3\u5408\u5f02\u6784\u6700\u8fd1\u90bb\u6982\u5ff5\u8fdb\u884c\u8fb9\u754c\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u4efb\u52a1\u4e2d\u566a\u58f0\u6570\u636e\u7684\u5904\u7406\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGB\u7684\u91c7\u6837\u65b9\u6cd5\u5b58\u5728\u8fb9\u754c\u91c7\u6837\u7b56\u7565\u7f3a\u5931\u53caGB\u91cd\u53e0\u5bfc\u81f4\u7684\u7c7b\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "1. \u63d0\u51faRD-GBG\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d7\u9650\u6269\u6563\u751f\u6210GB\u4ee5\u907f\u514d\u91cd\u53e0\uff1b2. \u57fa\u4e8e\u5f02\u6784\u6700\u8fd1\u90bb\u6982\u5ff5\u8bbe\u8ba1GBABS\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8fb9\u754c\u91c7\u6837\u5e76\u63d0\u5347\u566a\u58f0\u6570\u636e\u96c6\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGBABS\u5728\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u6700\u4f18\u7eaf\u5ea6\u9608\u503c\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709GB\u91c7\u6837\u65b9\u6cd5\u53ca\u5176\u4ed6\u4ee3\u8868\u6027\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "GBABS\u662f\u4e00\u79cd\u901a\u7528\u91c7\u6837\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u8fb9\u754c\u91c7\u6837\u5e76\u63d0\u5347\u566a\u58f0\u6570\u636e\u8d28\u91cf\uff0c\u4e3a\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02378", "pdf": "https://arxiv.org/pdf/2506.02378", "abs": "https://arxiv.org/abs/2506.02378", "authors": ["Ukyo Honda", "Tatsushi Oka"], "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55X-ICL\u7684\u6846\u67b6X\u00b2-ICL\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u6807\u7b7e\u7684\u89e3\u91ca\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709ICL\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cX-ICL\u901a\u8fc7\u5f15\u5165\u89e3\u91ca\u63d0\u5347\u4e86\u53ef\u9760\u6027\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u6269\u5c55X-ICL\uff0c\u7cfb\u7edf\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u6807\u7b7e\u7684\u89e3\u91ca\uff08X\u00b2-ICL\uff09\uff0c\u4ee5\u652f\u6301\u66f4\u5168\u9762\u7684\u51b3\u7b56\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cX\u00b2-ICL\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "X\u00b2-ICL\u662f\u4e00\u79cd\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.02367", "pdf": "https://arxiv.org/pdf/2506.02367", "abs": "https://arxiv.org/abs/2506.02367", "authors": ["Jiayi Su", "Dequan Jin"], "title": "ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery", "categories": ["cs.CV", "68T07", "I.5.1"], "comment": "22 pages, 3 figures", "summary": "Generalized category discovery (GCD) is a highly popular task in open-world\nrecognition, aiming to identify unknown class samples using known class data.\nBy leveraging pre-training, meta-training, and fine-tuning, ViT achieves\nexcellent few-shot learning capabilities. Its MLP head is a feedforward\nnetwork, trained synchronously with the entire network in the same process,\nincreasing the training cost and difficulty without fully leveraging the power\nof the feature extractor. This paper proposes a new architecture by replacing\nthe MLP head with a neural field-based one. We first present a new static\nneural field function to describe the activity distribution of the neural field\nand then use two static neural field functions to build an efficient few-shot\nclassifier. This neural field-based (NF) classifier consists of two coupled\nstatic neural fields. It stores the feature information of support samples by\nits elementary field, the known categories by its high-level field, and the\ncategory information of support samples by its cross-field connections. We\nreplace the MLP head with the proposed NF classifier, resulting in a novel\narchitecture ViTNF, and simplify the three-stage training mode by pre-training\nthe feature extractor on source tasks and training the NF classifier with\nsupport samples in meta-testing separately, significantly reducing ViT's demand\nfor training samples and the difficulty of model training. To enhance the\nmodel's capability in identifying new categories, we provide an effective\nalgorithm to determine the lateral interaction scale of the elementary field.\nExperimental results demonstrate that our model surpasses existing\nstate-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard\nCars, achieving dramatic accuracy improvements of 19\\% and 16\\% in new and all\nclasses, respectively, indicating a notable advantage in GCD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u5206\u7c7b\u5668\uff08NF\uff09\uff0c\u66ff\u6362ViT\u4e2d\u7684MLP\u5934\uff0c\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfViT\u7684MLP\u5934\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u672a\u5145\u5206\u5229\u7528\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u529b\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u4efb\u52a1\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u9759\u6001\u795e\u7ecf\u573a\u51fd\u6570\u6784\u5efaNF\u5206\u7c7b\u5668\uff0c\u66ff\u4ee3MLP\u5934\uff0c\u5f62\u6210ViTNF\u67b6\u6784\uff0c\u5e76\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u65b0\u7c7b\u548c\u5168\u90e8\u7c7b\u51c6\u786e\u7387\u5206\u522b\u63d0\u534719%\u548c16%\u3002", "conclusion": "ViTNF\u5728GCD\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8bad\u7ec3\u66f4\u9ad8\u6548\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2506.02391", "pdf": "https://arxiv.org/pdf/2506.02391", "abs": "https://arxiv.org/abs/2506.02391", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 findings", "summary": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u540c\u673a\u5236Consultant Decoding (CD)\uff0c\u901a\u8fc7\u6539\u8fdb\u9a8c\u8bc1\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709Speculative Decoding (SD)\u7684\u9ad8\u62d2\u7edd\u7387\u5bfc\u81f4\u9700\u8981\u91cd\u590d\u8c03\u7528LLM\u9a8c\u8bc1\u8349\u6848\u6807\u8bb0\uff0c\u5f71\u54cd\u4e86\u6574\u4f53\u6548\u7387\u3002", "method": "CD\u91c7\u7528\u57fa\u4e8eLLM\u8ba1\u7b97\u7684\u6807\u8bb0\u7ea7\u4f3c\u7136\u6027\u9a8c\u8bc1\u5019\u9009\u8349\u6848\uff0c\u800c\u975eSD\u7684\u91cd\u8981\u6027\u91c7\u6837\u6307\u6807\u3002", "result": "CD\u5b9e\u73b0\u4e86\u63a8\u7406\u901f\u5ea6\u6700\u9ad82.5\u500d\u7684\u63d0\u5347\uff0c\u751f\u6210\u8d28\u91cf\u63a5\u8fd1\u76ee\u6807\u6a21\u578b\u7684100%\uff0c\u4e14\u5927\u5e45\u964d\u4f4e\u4e86\u5bf9\u5927\u578b\u76ee\u6807\u6a21\u578b\u7684\u8c03\u7528\u9891\u7387\u3002", "conclusion": "CD\u4e0d\u4ec5\u8d85\u8d8a\u4e86SD\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u8fd8\u5728\u9ad8\u8981\u6c42\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.02382", "pdf": "https://arxiv.org/pdf/2506.02382", "abs": "https://arxiv.org/abs/2506.02382", "authors": ["Seulgi Kim", "Ghazal Kaviani", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "Multi-level and Multi-modal Action Anticipation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Action anticipation, the task of predicting future actions from partially\nobserved videos, is crucial for advancing intelligent systems. Unlike action\nrecognition, which operates on fully observed videos, action anticipation must\nhandle incomplete information. Hence, it requires temporal reasoning, and\ninherent uncertainty handling. While recent advances have been made,\ntraditional methods often focus solely on visual modalities, neglecting the\npotential of integrating multiple sources of information. Drawing inspiration\nfrom human behavior, we introduce \\textit{Multi-level and Multi-modal Action\nAnticipation (m\\&m-Ant)}, a novel multi-modal action anticipation approach that\ncombines both visual and textual cues, while explicitly modeling hierarchical\nsemantic information for more accurate predictions. To address the challenge of\ninaccurate coarse action labels, we propose a fine-grained label generator\npaired with a specialized temporal consistency loss function to optimize\nperformance. Extensive experiments on widely used datasets, including\nBreakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,\nachieving state-of-the-art results with an average anticipation accuracy\nimprovement of 3.08\\% over existing methods. This work underscores the\npotential of multi-modal and hierarchical modeling in advancing action\nanticipation and establishes a new benchmark for future research in the field.\nOur code is available at: https://github.com/olivesgatech/mM-ant.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u548c\u591a\u5c42\u6b21\u7684\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\uff08m&m-Ant\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6807\u7b7e\u751f\u6210\u5668\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\u4f18\u5316\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e863.08%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u9700\u8981\u5904\u7406\u4e0d\u5b8c\u5168\u4fe1\u606f\uff0c\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89c6\u89c9\u6a21\u6001\uff0c\u5ffd\u7565\u4e86\u591a\u6e90\u4fe1\u606f\u6574\u5408\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fam&m-Ant\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\uff0c\u5e76\u5f15\u5165\u7ec6\u7c92\u5ea6\u6807\u7b7e\u751f\u6210\u5668\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728Breakfast\u300150 Salads\u548cDARai\u7b49\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e863.08%\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u548c\u5c42\u6b21\u5efa\u6a21\u5728\u52a8\u4f5c\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404", "abs": "https://arxiv.org/abs/2506.02404", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "AI": {"tldr": "GraphRAG-Bench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9886\u57df\u7279\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30GraphRAG\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5f25\u8865\u73b0\u6709\u8bc4\u4f30\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dGraphRAG\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "method": "\u8bbe\u8ba1\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3001\u591a\u6837\u5316\u7684\u4efb\u52a1\u8986\u76d6\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u4e5d\u79cdGraphRAG\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86GraphRAG-Bench\u5728\u91cf\u5316\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "GraphRAG-Bench\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5173\u4e8e\u56fe\u67b6\u6784\u3001\u68c0\u7d22\u6548\u679c\u548c\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u89c1\u89e3\u548c\u884c\u52a8\u6307\u5357\u3002"}}
{"id": "2506.02393", "pdf": "https://arxiv.org/pdf/2506.02393", "abs": "https://arxiv.org/abs/2506.02393", "authors": ["Yongxian Liu", "Boyang Li", "Ting Liu", "Zaiping Lin", "Wei An"], "title": "RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection is a challenging task due to its unique\ncharacteristics (e.g., small, dim, shapeless and changeable). Recently\npublished CNN-based methods have achieved promising performance with heavy\nfeature extraction and fusion modules. To achieve efficient and effective\ndetection, we propose a recurrent reusable-convolution attention network\n(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net\nincorporates reusable-convolution block (RuCB) in a recurrent manner without\nintroducing extra parameters. With the help of the repetitive iteration in\nRuCB, the high-level information of small targets in the deep layers can be\nwell maintained and further refined. Then, a dual interactive attention\naggregation module (DIAAM) is proposed to promote the mutual enhancement and\nfusion of refined information. In this way, RRCA-Net can both achieve\nhigh-level feature refinement and enhance the correlation of contextual\ninformation between adjacent layers. Moreover, to achieve steady convergence,\nwe design a target characteristic inspired loss function (DpT-k loss) by\nintegrating physical and mathematical constraints. Experimental results on\nthree benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate\nthat our RRCA-Net can achieve comparable performance to the state-of-the-art\nmethods while maintaining a small number of parameters, and act as a plug and\nplay module to introduce consistent performance improvement for several popular\nIRSTD methods. Our code will be available at https://github.com/yongxianLiu/\nsoon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRRCA-Net\u7684\u5faa\u73af\u53ef\u91cd\u7528\u5377\u79ef\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u53ef\u91cd\u7528\u5377\u79ef\u5757\u548c\u53cc\u4ea4\u4e92\u6ce8\u610f\u529b\u805a\u5408\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u4e14\u53c2\u6570\u5c11\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u56e0\u76ee\u6807\u5c0f\u3001\u6697\u3001\u5f62\u72b6\u591a\u53d8\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709CNN\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u6a21\u5757\uff0c\u6548\u7387\u4e0d\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e86RRCA-Net\uff0c\u5305\u542b\u53ef\u91cd\u7528\u5377\u79ef\u5757\uff08RuCB\uff09\u548c\u53cc\u4ea4\u4e92\u6ce8\u610f\u529b\u805a\u5408\u6a21\u5757\uff08DIAAM\uff09\uff0c\u901a\u8fc7\u5faa\u73af\u8fed\u4ee3\u548c\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u53c2\u6570\u8f83\u5c11\uff0c\u5e76\u80fd\u4f5c\u4e3a\u63d2\u4ef6\u63d0\u5347\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "RRCA-Net\u901a\u8fc7\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u9ad8\u6027\u80fd\u4e0e\u4f4e\u53c2\u6570\u9700\u6c42\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02412", "pdf": "https://arxiv.org/pdf/2506.02412", "abs": "https://arxiv.org/abs/2506.02412", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Industry Track", "summary": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels.", "AI": {"tldr": "SingaKids\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u5bf9\u8bdd\u5f0f\u8bed\u8a00\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u7247\u63cf\u8ff0\u4efb\u52a1\u548c\u6c89\u6d78\u5f0f\u8bbe\u8ba1\u63d0\u5347\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u89e3\u51b3\u591a\u8bed\u8a00\u548c\u6587\u5316\u9002\u5e94\u6027\u4ee5\u53ca\u513f\u7ae5\u53cb\u597d\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "\u6574\u5408\u5bc6\u96c6\u56fe\u50cf\u63cf\u8ff0\u3001\u591a\u8bed\u8a00\u5bf9\u8bdd\u4ea4\u4e92\u3001\u8bed\u97f3\u7406\u89e3\u548c\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u4f18\u5316\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cSingaKids\u5bf9\u4e0d\u540c\u6c34\u5e73\u7684\u5b66\u4e60\u8005\u5747\u6709\u6548\u3002", "conclusion": "SingaKids\u4e3a\u591a\u8bed\u8a00\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5bf9\u8bdd\u5f0f\u6559\u5b66\u5de5\u5177\u3002"}}
{"id": "2506.02395", "pdf": "https://arxiv.org/pdf/2506.02395", "abs": "https://arxiv.org/abs/2506.02395", "authors": ["Xiaofeng Cong", "Yu-Xin Zhang", "Haoran Wei", "Yeying Jin", "Junming Hou", "Jie Gui", "Jing Zhang", "Dacheng Tao"], "title": "The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception", "categories": ["cs.CV"], "comment": null, "summary": "While nighttime image dehazing has been extensively studied, converting\nnighttime hazy images to daytime-equivalent brightness remains largely\nunaddressed. Existing methods face two critical limitations: (1) datasets\noverlook the brightness relationship between day and night, resulting in the\nbrightness mapping being inconsistent with the real world during image\nsynthesis; and (2) models do not explicitly incorporate daytime brightness\nknowledge, limiting their ability to reconstruct realistic lighting. To address\nthese challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND)\nframework, which excels in both data synthesis and lighting reconstruction. Our\napproach starts with a data synthesis pipeline that simulates severe\ndistortions while enforcing brightness consistency between synthetic and\nreal-world scenes, providing a strong foundation for learning night-to-day\nbrightness mapping. Next, we propose a restoration model that integrates a\npre-trained diffusion model guided by a brightness perception network. This\ndesign harnesses the diffusion model's generative ability while adapting it to\nnighttime dehazing through brightness-aware optimization. Experiments validate\nour dataset's utility and the model's superior performance in joint haze\nremoval and brightness mapping.", "AI": {"tldr": "DiffND\u6846\u67b6\u901a\u8fc7\u6570\u636e\u5408\u6210\u548c\u4eae\u5ea6\u611f\u77e5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u591c\u95f4\u56fe\u50cf\u53bb\u96fe\u4e2d\u4eae\u5ea6\u6620\u5c04\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u5149\u7167\u91cd\u5efa\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591c\u95f4\u56fe\u50cf\u53bb\u96fe\u4e2d\u5ffd\u7565\u4e86\u663c\u591c\u4eae\u5ea6\u5173\u7cfb\uff0c\u5bfc\u81f4\u5408\u6210\u56fe\u50cf\u4eae\u5ea6\u4e0e\u771f\u5b9e\u4e16\u754c\u4e0d\u4e00\u81f4\uff0c\u4e14\u6a21\u578b\u7f3a\u4e4f\u5bf9\u767d\u5929\u4eae\u5ea6\u7684\u663e\u5f0f\u77e5\u8bc6\u3002", "method": "\u63d0\u51faDiffND\u6846\u67b6\uff0c\u5305\u62ec\u4eae\u5ea6\u4e00\u81f4\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u4eae\u5ea6\u611f\u77e5\u4f18\u5316\u6062\u590d\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u548c\u6a21\u578b\u5728\u8054\u5408\u53bb\u96fe\u53ca\u4eae\u5ea6\u6620\u5c04\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DiffND\u5728\u591c\u95f4\u56fe\u50cf\u53bb\u96fe\u548c\u4eae\u5ea6\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.02425", "pdf": "https://arxiv.org/pdf/2506.02425", "abs": "https://arxiv.org/abs/2506.02425", "authors": ["Tairan Liu"], "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Textbooks play a critical role in shaping children's understanding of the\nworld. While previous studies have identified gender inequality in individual\ncountries' textbooks, few have examined the issue cross-culturally. This study\napplies natural language processing methods to quantify gender inequality in\nEnglish textbooks from 22 countries across 7 cultural spheres. Metrics include\ncharacter count, firstness (which gender is mentioned first), and TF-IDF word\nassociations by gender. The analysis also identifies gender patterns in proper\nnames appearing in TF-IDF word lists, tests whether large language models can\ndistinguish between gendered word lists, and uses GloVe embeddings to examine\nhow closely keywords associate with each gender. Results show consistent\noverrepresentation of male characters in terms of count, firstness, and named\nentities. All regions exhibit gender inequality, with the Latin cultural sphere\nshowing the least disparity.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\u91cf\u5316\u4e8622\u4e2a\u56fd\u5bb6\u82f1\u8bed\u6559\u79d1\u4e66\u4e2d\u7684\u6027\u522b\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u53d1\u73b0\u7537\u6027\u89d2\u8272\u5728\u6570\u91cf\u3001\u63d0\u53ca\u987a\u5e8f\u548c\u547d\u540d\u5b9e\u4f53\u4e0a\u666e\u904d\u88ab\u8fc7\u5ea6\u4ee3\u8868\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u5355\u4e2a\u56fd\u5bb6\u6559\u79d1\u4e66\u4e2d\u7684\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u4f46\u7f3a\u4e4f\u8de8\u6587\u5316\u6bd4\u8f83\uff0c\u672c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5305\u62ec\u5b57\u7b26\u8ba1\u6570\u3001\u9996\u6b21\u63d0\u53ca\u987a\u5e8f\u3001TF-IDF\u8bcd\u5173\u8054\u3001\u547d\u540d\u5b9e\u4f53\u5206\u6790\u3001\u5927\u8bed\u8a00\u6a21\u578b\u6d4b\u8bd5\u548cGloVe\u5d4c\u5165\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6240\u6709\u5730\u533a\u5747\u5b58\u5728\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u7537\u6027\u89d2\u8272\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u88ab\u8fc7\u5ea6\u4ee3\u8868\uff0c\u62c9\u4e01\u6587\u5316\u5708\u7684\u4e0d\u5e73\u7b49\u7a0b\u5ea6\u6700\u4f4e\u3002", "conclusion": "\u6559\u79d1\u4e66\u4e2d\u7684\u6027\u522b\u4e0d\u5e73\u7b49\u662f\u4e00\u4e2a\u666e\u904d\u73b0\u8c61\uff0c\u8de8\u6587\u5316\u7814\u7a76\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u7406\u89e3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2506.02396", "pdf": "https://arxiv.org/pdf/2506.02396", "abs": "https://arxiv.org/abs/2506.02396", "authors": ["Longyu Yang", "Ping Hu", "Shangbo Yuan", "Lu Zhang", "Jun Liu", "Hengtao Shen", "Xiaofeng Zhu"], "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather", "categories": ["cs.CV"], "comment": null, "summary": "Existing LiDAR semantic segmentation models often suffer from decreased\naccuracy when exposed to adverse weather conditions. Recent methods addressing\nthis issue focus on enhancing training data through weather simulation or\nuniversal augmentation techniques. However, few works have studied the negative\nimpacts caused by the heterogeneous domain shifts in the geometric structure\nand reflectance intensity of point clouds. In this paper, we delve into this\nchallenge and address it with a novel Geometry-Reflectance Collaboration (GRC)\nframework that explicitly separates feature extraction for geometry and\nreflectance. Specifically, GRC employs a dual-branch architecture designed to\nindependently process geometric and reflectance features initially, thereby\ncapitalizing on their distinct characteristic. Then, GRC adopts a robust\nmulti-level feature collaboration module to suppress redundant and unreliable\ninformation from both branches. Consequently, without complex simulation or\naugmentation, our method effectively extracts intrinsic information about the\nscene while suppressing interference, thus achieving better robustness and\ngeneralization in adverse weather conditions. We demonstrate the effectiveness\nof GRC through comprehensive experiments on challenging benchmarks, showing\nthat our method outperforms previous approaches and establishes new\nstate-of-the-art results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRC\u7684\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u51e0\u4f55\u548c\u53cd\u5c04\u7279\u5f81\u63d0\u53d6\uff0c\u63d0\u5347LiDAR\u8bed\u4e49\u5206\u5272\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LiDAR\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5c11\u6709\u7814\u7a76\u5173\u6ce8\u70b9\u4e91\u51e0\u4f55\u7ed3\u6784\u548c\u53cd\u5c04\u5f3a\u5ea6\u7684\u5f02\u8d28\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "GRC\u6846\u67b6\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u5206\u522b\u5904\u7406\u51e0\u4f55\u548c\u53cd\u5c04\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u7279\u5f81\u534f\u4f5c\u6a21\u5757\u6291\u5236\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGRC\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u6a21\u62df\u6216\u589e\u5f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GRC\u6846\u67b6\u901a\u8fc7\u51e0\u4f55-\u53cd\u5c04\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u8d28\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002"}}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426", "abs": "https://arxiv.org/abs/2506.02426", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e09\u79cdAI\u4ee3\u7406\u67b6\u6784\u5728\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4f18\u4e8e\u6807\u51c6\u5c11\u6837\u672c\u63d0\u793a\uff0c\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4fe1\u606f\u63d0\u53d6\u4e2d\u5b9e\u4f53\u5173\u7cfb\u5206\u7c7b\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u5173\u7cfb\u7ed3\u6784\u4e0b\u7684\u6311\u6218\u3002", "method": "\u5206\u6790\u4e86\u4e09\u79cd\u67b6\u6784\uff1a\u81ea\u53cd\u81ea\u8bc4\u4f30\u3001\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u793a\u4f8b\u751f\u6210\uff0c\u5e76\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u591a\u9886\u57df\u548c\u6a21\u578b\u540e\u7aef\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u591a\u667a\u80fd\u4f53\u534f\u8c03\u8868\u73b0\u6700\u4f73\uff0c\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u4e3a\u57fa\u4e8eLLM\u7684\u7ed3\u6784\u5316\u5173\u7cfb\u63d0\u53d6\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2506.02405", "pdf": "https://arxiv.org/pdf/2506.02405", "abs": "https://arxiv.org/abs/2506.02405", "authors": ["Zhiya Tan", "Xin Zhang", "Joey Tianyi Zhou"], "title": "Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "As generative techniques become increasingly accessible, authentic visuals\nare frequently subjected to iterative alterations by various individuals\nemploying a variety of tools. Currently, to avoid misinformation and ensure\naccountability, a lot of research on detection and attribution is emerging.\nAlthough these methods demonstrate promise in single-stage manipulation\nscenarios, they fall short when addressing complex real-world iterative\nmanipulation. In this paper, we are the first, to the best of our knowledge, to\nsystematically model this real-world challenge and introduce a novel method to\nsolve it. We define a task called \"Modelship Attribution\", which aims to trace\nthe evolution of manipulated images by identifying the generative models\ninvolved and reconstructing the sequence of edits they performed. To\nrealistically simulate this scenario, we utilize three generative models,\nStyleMapGAN, DiffSwap, and FacePartsSwap, that sequentially modify distinct\nregions of the same image. This process leads to the creation of the first\nmodelship dataset, comprising 83,700 images (16,740 images*5). Given that later\nedits often overwrite the fingerprints of earlier models, the focus shifts from\nextracting blended fingerprints to characterizing each model's distinctive\nediting patterns. To tackle this challenge, we introduce the modelship\nattribution transformer (MAT), a purpose-built framework designed to\neffectively recognize and attribute the contributions of various models within\ncomplex, multi-stage manipulation workflows. Through extensive experiments and\ncomparative analysis with other related methods, our results, including\ncomprehensive ablation studies, demonstrate that the proposed approach is a\nhighly effective solution for modelship attribution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u8fed\u4ee3\u56fe\u50cf\u7be1\u6539\u7684\u6eaf\u6e90\u95ee\u9898\uff0c\u9996\u6b21\u7cfb\u7edf\u5efa\u6a21\u5e76\u5b9a\u4e49\u4e86\u201c\u6a21\u578b\u5f52\u5c5e\u201d\u4efb\u52a1\uff0c\u901a\u8fc7\u8bc6\u522b\u751f\u6210\u6a21\u578b\u548c\u91cd\u6784\u7f16\u8f91\u5e8f\u5217\u6765\u8ffd\u8e2a\u56fe\u50cf\u7be1\u6539\u8fc7\u7a0b\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6280\u672f\u7684\u666e\u53ca\uff0c\u56fe\u50cf\u7be1\u6539\u53d8\u5f97\u590d\u6742\u4e14\u96be\u4ee5\u6eaf\u6e90\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u9636\u6bb5\u7be1\u6539\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u8fed\u4ee3\u7be1\u6539\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff08StyleMapGAN\u3001DiffSwap\u3001FacePartsSwap\uff09\u6a21\u62df\u591a\u9636\u6bb5\u7be1\u6539\uff0c\u6784\u5efa\u9996\u4e2a\u6a21\u578b\u5f52\u5c5e\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u6a21\u578b\u5f52\u5c5e\u53d8\u6362\u5668\uff08MAT\uff09\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAT\u5728\u591a\u9636\u6bb5\u7be1\u6539\u6eaf\u6e90\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "MAT\u4e3a\u89e3\u51b3\u590d\u6742\u8fed\u4ee3\u7be1\u6539\u7684\u6eaf\u6e90\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.02431", "pdf": "https://arxiv.org/pdf/2506.02431", "abs": "https://arxiv.org/abs/2506.02431", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Emotions are a fundamental facet of human experience, varying across\nindividuals, cultural contexts, and nationalities. Given the recent success of\nLarge Language Models (LLMs) as role-playing agents, we examine whether LLMs\nexhibit emotional stereotypes when assigned nationality-specific personas.\nSpecifically, we investigate how different countries are represented in\npre-trained LLMs through emotion attributions and whether these attributions\nalign with cultural norms. Our analysis reveals significant nationality-based\ndifferences, with emotions such as shame, fear, and joy being\ndisproportionately assigned across regions. Furthermore, we observe notable\nmisalignment between LLM-generated and human emotional responses, particularly\nfor negative emotions, highlighting the presence of reductive and potentially\nbiased stereotypes in LLM outputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u626e\u6f14\u4e0d\u540c\u56fd\u7c4d\u89d2\u8272\u65f6\u8868\u73b0\u51fa\u60c5\u611f\u523b\u677f\u5370\u8c61\uff0c\u4e0e\u4eba\u7c7b\u60c5\u611f\u53cd\u5e94\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u6a21\u62df\u4e0d\u540c\u56fd\u7c4d\u89d2\u8272\u65f6\u662f\u5426\u8868\u73b0\u51fa\u60c5\u611f\u523b\u677f\u5370\u8c61\uff0c\u5e76\u9a8c\u8bc1\u5176\u60c5\u611f\u5206\u914d\u662f\u5426\u7b26\u5408\u6587\u5316\u89c4\u8303\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3LLMs\u5bf9\u4e0d\u540c\u56fd\u5bb6\u89d2\u8272\u7684\u60c5\u611f\u5206\u914d\uff0c\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u60c5\u611f\u53cd\u5e94\u7684\u5dee\u5f02\u3002", "result": "LLMs\u5728\u60c5\u611f\u5206\u914d\u4e0a\u5b58\u5728\u663e\u8457\u7684\u56fd\u7c4d\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u8d1f\u9762\u60c5\u611f\uff08\u5982\u7f9e\u803b\u3001\u6050\u60e7\uff09\u7684\u5206\u914d\u4e0e\u4eba\u7c7b\u53cd\u5e94\u4e0d\u4e00\u81f4\u3002", "conclusion": "LLMs\u8f93\u51fa\u4e2d\u5b58\u5728\u7b80\u5316\u548c\u6f5c\u5728\u504f\u89c1\u7684\u60c5\u611f\u523b\u677f\u5370\u8c61\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u51cf\u5c11\u6587\u5316\u504f\u89c1\u3002"}}
{"id": "2506.02408", "pdf": "https://arxiv.org/pdf/2506.02408", "abs": "https://arxiv.org/abs/2506.02408", "authors": ["Wenhao Tang", "Rong Qin", "Heng Fang", "Fengtao Zhou", "Hao Chen", "Xiang Li", "Ming-Ming Cheng"], "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained encoders for offline feature extraction followed by multiple\ninstance learning (MIL) aggregators have become the dominant paradigm in\ncomputational pathology (CPath), benefiting cancer diagnosis and prognosis.\nHowever, performance limitations arise from the absence of encoder fine-tuning\nfor downstream tasks and disjoint optimization with MIL. While slide-level\nsupervised end-to-end (E2E) learning is an intuitive solution to this issue, it\nfaces challenges such as high computational demands and suboptimal results.\nThese limitations motivate us to revisit E2E learning. We argue that prior work\nneglects inherent E2E optimization challenges, leading to performance\ndisparities compared to traditional two-stage methods. In this paper, we\npioneer the elucidation of optimization challenge caused by sparse-attention\nMIL and propose a novel MIL called ABMILX. It mitigates this problem through\nglobal correlation-based attention refinement and multi-head mechanisms. With\nthe efficient multi-scale random patch sampling strategy, an E2E trained ResNet\nwith ABMILX surpasses SOTA foundation models under the two-stage paradigm\nacross multiple challenging benchmarks, while remaining computationally\nefficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath\nand calls for greater research focus in this area. The code is\nhttps://github.com/DearCaat/E2E-WSI-ABMILX.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aABMILX\u7684\u65b0\u578b\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u76f8\u5173\u6027\u6ce8\u610f\u529b\u4f18\u5316\u548c\u591a\u5934\u673a\u5236\u89e3\u51b3\u4e86\u7a00\u758f\u6ce8\u610f\u529bMIL\u7684\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u968f\u673a\u8865\u4e01\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u5982\u7f16\u7801\u5668\u672a\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u4ee5\u53ca\u4e0eMIL\u7684\u5206\u79bb\u4f18\u5316\u3002\u7aef\u5230\u7aef\u5b66\u4e60\u867d\u76f4\u89c2\u4f46\u9762\u4e34\u8ba1\u7b97\u91cf\u5927\u548c\u7ed3\u679c\u6b21\u4f18\u7684\u6311\u6218\uff0c\u56e0\u6b64\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u5e76\u4f18\u5316\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faABMILX\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u76f8\u5173\u6027\u6ce8\u610f\u529b\u4f18\u5316\u548c\u591a\u5934\u673a\u5236\u89e3\u51b3\u7a00\u758f\u6ce8\u610f\u529bMIL\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u5c3a\u5ea6\u968f\u673a\u8865\u4e01\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cABMILX\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff08<10 RTX3090\u5c0f\u65f6\uff09\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u547c\u5401\u66f4\u591a\u7814\u7a76\u5173\u6ce8\u8fd9\u4e00\u9886\u57df\u3002"}}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442", "abs": "https://arxiv.org/abs/2506.02442", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u957f\u5c3e\u5206\u5e03\uff08\u52a0\u5bc6\uff09\u6587\u672c\u4e0a\u7684\u884c\u4e3a\u53ca\u5176\u5b89\u5168\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u89e3\u5bc6\u80fd\u529b\u4e0b\u53ef\u80fd\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u7814\u7a76LLM\u5728\u957f\u5c3e\u52a0\u5bc6\u6587\u672c\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5065\u58ee\u7684\u5b89\u5168\u673a\u5236\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u4e8c\u7ef4\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff08\u6307\u4ee4\u62d2\u7edd\u548c\u751f\u6210\u5b89\u5168\u6027\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u5728\u89e3\u5bc6\u80fd\u529b\u4e0b\u7684\u5b89\u5168\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5177\u5907\u89e3\u5bc6\u80fd\u529b\u7684\u6a21\u578b\u53ef\u80fd\u53d7\u5230\u4e0d\u5339\u914d\u6cdb\u5316\u653b\u51fb\uff0c\u5bfc\u81f4\u5b89\u5168\u673a\u5236\u5931\u6548\u6216\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3LLM\u5728\u957f\u5c3e\u6587\u672c\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5b89\u5168\u673a\u5236\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2506.02419", "pdf": "https://arxiv.org/pdf/2506.02419", "abs": "https://arxiv.org/abs/2506.02419", "authors": ["Nurislam Tursynbek", "Hastings Greer", "Basar Demir", "Marc Niethammer"], "title": "Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Diffusion models, while trained for image generation, have emerged as\npowerful foundational feature extractors for downstream tasks. We find that\noff-the-shelf diffusion models, trained exclusively to generate natural RGB\nimages, can identify semantically meaningful correspondences in medical images.\nBuilding on this observation, we propose to leverage diffusion model features\nas a similarity measure to guide deformable image registration networks. We\nshow that common intensity-based similarity losses often fail in challenging\nscenarios, such as when certain anatomies are visible in one image but absent\nin another, leading to anatomically inaccurate alignments. In contrast, our\nmethod identifies true semantic correspondences, aligning meaningful structures\nwhile disregarding those not present across images. We demonstrate superior\nperformance of our approach on two tasks: multimodal 2D registration (DXA to\nX-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted\nMRI). Code: https://github.com/uncbiag/dgir", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u88ab\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u914d\u51c6\uff0c\u901a\u8fc7\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5ea6\u7684\u914d\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5ea6\u7684\u914d\u51c6\u65b9\u6cd5\u5728\u89e3\u5256\u7ed3\u6784\u4e0d\u4e00\u81f4\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u6269\u6563\u6a21\u578b\u80fd\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u63d0\u53d6\u7279\u5f81\u4f5c\u4e3a\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u6307\u5bfc\u53ef\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u7f51\u7edc\u3002", "result": "\u57282D\u591a\u6a21\u6001\uff08DXA\u5230X\u5c04\u7ebf\uff09\u548c3D\u5355\u6a21\u6001\uff08\u8111\u63d0\u53d6\u4e0e\u975e\u8111\u63d0\u53d6MRI\uff09\u914d\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u7279\u5f81\u4f5c\u4e3a\u76f8\u4f3c\u6027\u5ea6\u91cf\u80fd\u663e\u8457\u63d0\u5347\u914d\u51c6\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u89e3\u5256\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449", "abs": "https://arxiv.org/abs/2506.02449", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86IP-Dialog\u57fa\u51c6\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7528\u6237\u80cc\u666f\u63a8\u65ad\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u4ece\u5bf9\u8bdd\u4e2d\u63a8\u65ad\u7528\u6237\u80cc\u666f\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u4e14\u4f20\u7edf\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u548c\u8d44\u6e90\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u6784\u5efaIP-Dialog\u57fa\u51c6\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6db5\u76d610\u4e2a\u4efb\u52a1\u548c12\u79cd\u7528\u6237\u5c5e\u6027\u7c7b\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4e2d\u63a8\u7406\u8def\u5f84\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.02433", "pdf": "https://arxiv.org/pdf/2506.02433", "abs": "https://arxiv.org/abs/2506.02433", "authors": ["Weiheng Yao", "Xuhang Chen", "Shuqiang Wang"], "title": "Empowering Functional Neuroimaging: A Pre-trained Generative Framework for Unified Representation of Neural Signals", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal functional neuroimaging enables systematic analysis of brain\nmechanisms and provides discriminative representations for brain-computer\ninterface (BCI) decoding. However, its acquisition is constrained by high costs\nand feasibility limitations. Moreover, underrepresentation of specific groups\nundermines fairness of BCI decoding model. To address these challenges, we\npropose a unified representation framework for multimodal functional\nneuroimaging via generative artificial intelligence (AI). By mapping multimodal\nfunctional neuroimaging into a unified representation space, the proposed\nframework is capable of generating data for acquisition-constrained modalities\nand underrepresented groups. Experiments show that the framework can generate\ndata consistent with real brain activity patterns, provide insights into brain\nmechanisms, and improve performance on downstream tasks. More importantly, it\ncan enhance model fairness by augmenting data for underrepresented groups.\nOverall, the framework offers a new paradigm for decreasing the cost of\nacquiring multimodal functional neuroimages and enhancing the fairness of BCI\ndecoding models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u751f\u6210AI\u7684\u591a\u6a21\u6001\u529f\u80fd\u795e\u7ecf\u5f71\u50cf\u7edf\u4e00\u8868\u793a\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u529f\u80fd\u795e\u7ecf\u5f71\u50cf\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u516c\u5e73\u6027\u95ee\u9898\uff0c\u9700\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u751f\u6210AI\u5c06\u591a\u6a21\u6001\u529f\u80fd\u795e\u7ecf\u5f71\u50cf\u6620\u5c04\u5230\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\uff0c\u751f\u6210\u53d7\u9650\u6a21\u6001\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7fa4\u4f53\u7684\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6846\u67b6\u80fd\u751f\u6210\u771f\u5b9e\u8111\u6d3b\u52a8\u6a21\u5f0f\u6570\u636e\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u589e\u5f3a\u6a21\u578b\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u964d\u4f4e\u591a\u6a21\u6001\u529f\u80fd\u795e\u7ecf\u5f71\u50cf\u83b7\u53d6\u6210\u672c\u548c\u63d0\u5347BCI\u89e3\u7801\u6a21\u578b\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.02454", "pdf": "https://arxiv.org/pdf/2506.02454", "abs": "https://arxiv.org/abs/2506.02454", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages", "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFDV\u7684\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u65b9\u6cd5\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u548c\u751f\u6210\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u53ef\u89c6\u5316\u5185\u5bb9\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u6846\u67b6Multimodal DeepResearcher\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u4e3b\u8981\u751f\u6210\u7eaf\u6587\u672c\u5185\u5bb9\uff0c\u800c\u6587\u672c\u4e0e\u53ef\u89c6\u5316\u7ed3\u5408\u7684\u81ea\u52a8\u5316\u751f\u6210\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u5e26\u6765\u4e86\u8bbe\u8ba1\u548c\u6574\u5408\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86FDV\u4f5c\u4e3a\u56fe\u8868\u7684\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e86Multimodal DeepResearcher\u6846\u67b6\uff0c\u5206\u4e3a\u7814\u7a76\u3001\u6587\u672c\u5316\u793a\u4f8b\u62a5\u544a\u3001\u89c4\u5212\u548c\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u56db\u4e2a\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMultimodal DeepResearcher\u5728Claude 3.7 Sonnet\u6a21\u578b\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa82%\u7684\u80dc\u7387\u3002", "conclusion": "FDV\u548cMultimodal DeepResearcher\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u4e0e\u6587\u672c\u7684\u6574\u5408\u8d28\u91cf\u3002"}}
{"id": "2506.02439", "pdf": "https://arxiv.org/pdf/2506.02439", "abs": "https://arxiv.org/abs/2506.02439", "authors": ["Shuang Li", "Jiaxu Leng", "Changjiang Kuang", "Mingpi Tan", "Xinbo Gao"], "title": "Video-Level Language-Driven Video-Based Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by IEEE TIFS", "summary": "Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to\nmatch pedestrian sequences across modalities by extracting modality-invariant\nsequence-level features. As a high-level semantic representation, language\nprovides a consistent description of pedestrian characteristics in both\ninfrared and visible modalities. Leveraging the Contrastive Language-Image\nPre-training (CLIP) model to generate video-level language prompts and guide\nthe learning of modality-invariant sequence-level features is theoretically\nfeasible. However, the challenge of generating and utilizing modality-shared\nvideo-level language prompts to address modality gaps remains a critical\nproblem. To address this problem, we propose a simple yet powerful framework,\nvideo-level language-driven VVI-ReID (VLD), which consists of two core modules:\ninvariant-modality language prompting (IMLP) and spatial-temporal prompting\n(STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the\nprompt learner to effectively generate modality-shared text prompts and align\nthem with visual features from different modalities in CLIP's multimodal space,\nthereby mitigating modality differences. Additionally, STP models\nspatiotemporal information through two submodules, the spatial-temporal hub\n(STH) and spatial-temporal aggregation (STA), which further enhance IMLP by\nincorporating spatiotemporal information into text prompts. The STH aggregates\nand diffuses spatiotemporal information into the [CLS] token of each frame\nacross the vision transformer (ViT) layers, whereas STA introduces dedicated\nidentity-level loss and specialized multihead attention to ensure that the STH\nfocuses on identity-relevant spatiotemporal feature aggregation. The VLD\nframework achieves state-of-the-art results on two VVI-ReID benchmarks. The\ncode will be released at https://github.com/Visuang/VLD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7ea7\u8bed\u8a00\u9a71\u52a8\u7684VVI-ReID\u6846\u67b6\uff08VLD\uff09\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u6001\u5171\u4eab\u7684\u6587\u672c\u63d0\u793a\u5e76\u7ed3\u5408\u65f6\u7a7a\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u7ea7\u8bed\u8a00\u63d0\u793a\u751f\u6210\u4e0e\u5229\u7528\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u4ee5\u63d0\u53d6\u6a21\u6001\u4e0d\u53d8\u7684\u5e8f\u5217\u7ea7\u7279\u5f81\u3002", "method": "\u63d0\u51faVLD\u6846\u67b6\uff0c\u5305\u542b\u4e0d\u53d8\u6a21\u6001\u8bed\u8a00\u63d0\u793a\uff08IMLP\uff09\u548c\u65f6\u7a7a\u63d0\u793a\uff08STP\uff09\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u751f\u6210\u6a21\u6001\u5171\u4eab\u6587\u672c\u63d0\u793a\u548c\u5efa\u6a21\u65f6\u7a7a\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2aVVI-ReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "VLD\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u63d0\u793a\u548c\u65f6\u7a7a\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2506.02460", "pdf": "https://arxiv.org/pdf/2506.02460", "abs": "https://arxiv.org/abs/2506.02460", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMidPO\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u65b9\u6cd5\u4f18\u5316\u5b89\u5168\u6027\u4e0e\u5e2e\u52a9\u6027\u7684\u5e73\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u5e2e\u52a9\u6027\u7684\u540c\u65f6\u589e\u5f3a\u5b89\u5168\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u4e0e\u5e2e\u52a9\u6027\u5e73\u8861\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "MidPO\u9996\u5148\u5c06\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u72ec\u7acb\u7684\u5b89\u5168\u6027\u548c\u5e2e\u52a9\u6027\u4e13\u5bb6\uff0c\u518d\u901a\u8fc7MoE\u6846\u67b6\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\u81ea\u9002\u5e94\u5e73\u8861\u4e24\u8005\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMidPO\u5728\u5b89\u5168\u6027\u548c\u5e2e\u52a9\u6027\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MidPO\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b89\u5168\u6027\u4e0e\u5e2e\u52a9\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02444", "pdf": "https://arxiv.org/pdf/2506.02444", "abs": "https://arxiv.org/abs/2506.02444", "authors": ["Lingwei Dang", "Ruizhi Shao", "Hongwen Zhang", "Wei Min", "Yebin Liu", "Qingyao Wu"], "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at\n\\href{https://github.com/Droliven}{https://github.com/Droliven}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u52a8\u6001\u7ea6\u675f\u7684\u540c\u6b65\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u751f\u6210\u624b-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u89c6\u9891\u548c\u8fd0\u52a8\uff0c\u6d88\u9664\u4e86\u5bf9\u9884\u5b9a\u4e493D\u6a21\u578b\u548c\u663e\u5f0f\u59ff\u6001\u6307\u5bfc\u7684\u4f9d\u8d56\u3002", "motivation": "\u5f53\u524dHOI\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e493D\u6a21\u578b\u548c\u5b9e\u9a8c\u5ba4\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u727a\u7272\u7269\u7406\u5408\u7406\u6027\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u89c6\u89c9\u548c\u52a8\u6001\u7ea6\u675f\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u6001\u81ea\u9002\u5e94\u8c03\u5236\u5bf9\u9f50\u7279\u5f81\uff0c\u7ed3\u54083D\u5168\u6ce8\u610f\u529b\u5efa\u6a21\u6a21\u6001\u95f4\u4f9d\u8d56\uff1b\u63d0\u51fa\u89c6\u89c9\u611f\u77e5\u76843D\u4ea4\u4e92\u6269\u6563\u6a21\u578b\uff0c\u5f62\u6210\u95ed\u73af\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u52a8\u6001\u5408\u7406\u7684HOI\u5e8f\u5217\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6cdb\u5316\u80fd\u529b\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891-\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2506.02461", "pdf": "https://arxiv.org/pdf/2506.02461", "abs": "https://arxiv.org/abs/2506.02461", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Sch\u00fctze", "Simon See", "Yangqiu Song"], "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts.", "AI": {"tldr": "XToM\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u4e0a\u5b58\u5728\u8bed\u8a00\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u5bf9LLM\u5fc3\u7406\u7406\u8bba\u7684\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u82f1\u8bed\uff0c\u5ffd\u7565\u4e86\u8bed\u8a00\u591a\u6837\u6027\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76LLM\u662f\u5426\u5177\u5907\u8de8\u8bed\u8a00\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86XToM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e94\u79cd\u8bed\u8a00\u548c\u591a\u6837\u5316\u7684\u4efb\u52a1\u573a\u666f\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\uff08\u5982DeepSeek R1\uff09\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u591a\u8bed\u8a00\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u4e0a\u5b58\u5728\u8bed\u8a00\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "LLM\u5728\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u7406\u8bba\u7684\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2506.02448", "pdf": "https://arxiv.org/pdf/2506.02448", "abs": "https://arxiv.org/abs/2506.02448", "authors": ["Baoyu Liang", "Qile Su", "Shoutai Zhu", "Yuchen Liang", "Chao Tong"], "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the significant impact of visual events on human cognition,\nunderstanding events in videos remains a challenging task for AI due to their\ncomplex structures, semantic hierarchies, and dynamic evolution. To address\nthis, we propose the task of video event understanding that extracts event\nscripts and makes predictions with these scripts from videos. To support this\ntask, we introduce VidEvent, a large-scale dataset containing over 23,000\nwell-labeled events, featuring detailed event structures, broad hierarchies,\nand logical relations extracted from movie recap videos. The dataset was\ncreated through a meticulous annotation process, ensuring high-quality and\nreliable event data. We also provide comprehensive baseline models offering\ndetailed descriptions of their architecture and performance metrics. These\nmodels serve as benchmarks for future research, facilitating comparisons and\nimprovements. Our analysis of VidEvent and the baseline models highlights the\ndataset's potential to advance video event understanding and encourages the\nexploration of innovative algorithms and models. The dataset and related\nresources are publicly available at www.videvent.top.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u4efb\u52a1\uff0c\u5e76\u53d1\u5e03\u4e86VidEvent\u6570\u636e\u96c6\uff0c\u5305\u542b23,000\u591a\u4e2a\u6807\u6ce8\u4e8b\u4ef6\uff0c\u652f\u6301\u4e8b\u4ef6\u811a\u672c\u63d0\u53d6\u4e0e\u9884\u6d4b\u3002", "motivation": "\u89c6\u9891\u4e8b\u4ef6\u7684\u590d\u6742\u7ed3\u6784\u548c\u52a8\u6001\u6f14\u53d8\u5bf9AI\u7406\u89e3\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u652f\u6301\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u6807\u6ce8\u7684\u7535\u5f71\u56de\u987e\u89c6\u9891\u6784\u5efaVidEvent\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u57fa\u7ebf\u6a21\u578b\u67b6\u6784\u4e0e\u6027\u80fd\u6307\u6807\u3002", "result": "VidEvent\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\u4e3a\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\uff0c\u63a8\u52a8\u7b97\u6cd5\u521b\u65b0\u3002", "conclusion": "VidEvent\u6709\u671b\u63a8\u52a8\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u7814\u7a76\uff0c\u6570\u636e\u96c6\u548c\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.02478", "pdf": "https://arxiv.org/pdf/2506.02478", "abs": "https://arxiv.org/abs/2506.02478", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "categories": ["cs.CL"], "comment": "12 pages, 11 figures", "summary": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5FroM\uff0c\u901a\u8fc7Frobenius\u8303\u6570\u76f4\u63a5\u6d4b\u91cf\u6a21\u578b\u53c2\u6570\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u4e2d\u6613\u53d7\u4efb\u52a1\u5e72\u6270\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faFroM\u65b9\u6cd5\uff0c\u5229\u7528Frobenius\u8303\u6570\u76f4\u63a5\u6d4b\u91cf\u53c2\u6570\uff0c\u5f15\u5165\u8d85\u53c2\u6570\u63a7\u5236\u5408\u5e76\u8fc7\u7a0b\u3002", "result": "FroM\u5728\u591a\u79cd\u5fae\u8c03\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u8f7b\u4efb\u52a1\u5e72\u6270\u3002", "conclusion": "FroM\u4e3a\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02452", "pdf": "https://arxiv.org/pdf/2506.02452", "abs": "https://arxiv.org/abs/2506.02452", "authors": ["Wenshuo Chen", "Kuimou Yu", "Haozhe Jia", "Kaishen Yuan", "Bowen Tian", "Songning Lai", "Hongru Xiao", "Erhang Zhang", "Lei Wang", "Yutao Yue"], "title": "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models advance text-to-motion generation, their static\nsemantic conditioning ignores temporal-frequency demands: early denoising\nrequires structural semantics for motion foundations while later stages need\nlocalized details for text alignment. This mismatch mirrors biological\nmorphogenesis where developmental phases demand distinct genetic programs.\nInspired by epigenetic regulation governing morphological specialization, we\npropose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture.\nANT orchestrates semantic granularity through: **(i) Semantic Temporally\nAdaptive (STA) Module:** Automatically partitions denoising into low-frequency\nstructural planning and high-frequency refinement via spectral analysis. **(ii)\nDynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts\nconditional to unconditional ratio enhancing efficiency while maintaining\nfidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text\ninfluence with phase requirements. Extensive experiments show that ANT can be\napplied to various baselines, significantly improving model performance, and\nachieving state-of-the-art semantic alignment on StableMoFusion.", "AI": {"tldr": "ANT\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u795e\u7ecf\u65f6\u95f4\u611f\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bed\u4e49\u7c92\u5ea6\u548c\u6761\u4ef6\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4e2d\u5ffd\u7565\u4e86\u65f6\u95f4\u9891\u7387\u9700\u6c42\uff0c\u5bfc\u81f4\u65e9\u671f\u53bb\u566a\u9700\u8981\u7ed3\u6784\u8bed\u4e49\u800c\u540e\u671f\u9700\u8981\u5c40\u90e8\u7ec6\u8282\uff0c\u4e0e\u751f\u7269\u5f62\u6001\u53d1\u751f\u4e2d\u7684\u9636\u6bb5\u9700\u6c42\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faANT\u67b6\u6784\uff0c\u5305\u62ec\u8bed\u4e49\u65f6\u95f4\u81ea\u9002\u5e94\u6a21\u5757\uff08STA\uff09\u3001\u52a8\u6001\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u8c03\u5ea6\uff08DCFG\uff09\u548c\u65f6\u95f4\u8bed\u4e49\u91cd\u52a0\u6743\uff0c\u4ee5\u52a8\u6001\u8c03\u6574\u8bed\u4e49\u7c92\u5ea6\u548c\u6761\u4ef6\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cANT\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728StableMoFusion\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "ANT\u901a\u8fc7\u6a21\u62df\u751f\u7269\u5f62\u6001\u53d1\u751f\u7684\u8c03\u63a7\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u9891\u7387\u9700\u6c42\u95ee\u9898\u3002"}}
{"id": "2506.02480", "pdf": "https://arxiv.org/pdf/2506.02480", "abs": "https://arxiv.org/abs/2506.02480", "authors": ["Yifan Duan", "Yihong Tang", "Kehai Chen", "Liqiang Nie", "Min Zhang"], "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities", "categories": ["cs.CL"], "comment": null, "summary": "High-quality prompts are crucial for eliciting outstanding performance from\nlarge language models (LLMs) on complex tasks. Existing research has explored\nmodel-driven strategies for prompt optimization. However, these methods often\nsuffer from high computational overhead or require strong optimization\ncapabilities from the model itself, which limits their broad applicability.To\naddress these challenges, we propose ORPP (Optimized Role-Playing Prompt),a\nframework that enhances model performance by optimizing and generating\nrole-playing prompts. The core idea of ORPP is to confine the prompt search\nspace to role-playing scenarios, thereby fully activating the model's intrinsic\ncapabilities through carefully crafted, high-quality role-playing prompts.\nSpecifically, ORPP first performs iterative optimization on a small subset of\ntraining samples to generate high-quality role-playing prompts. Then,\nleveraging the model's few-shot learning capability, it transfers the\noptimization experience to efficiently generate suitable prompts for the\nremaining samples.Our experimental results show that ORPP not only matches but\nin most cases surpasses existing mainstream prompt optimization methods in\nterms of performance. Notably, ORPP demonstrates superior \"plug-and-play\"\ncapability. In most cases, it can be integrated with various other prompt\nmethods and further enhance their effectiveness.", "AI": {"tldr": "ORPP\uff08\u4f18\u5316\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff09\u662f\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u548c\u751f\u6210\u89d2\u8272\u626e\u6f14\u63d0\u793a\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u63d0\u793a\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u5728\u89d2\u8272\u626e\u6f14\u573a\u666f\u4e2d\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u9ad8\u6216\u4f9d\u8d56\u6a21\u578b\u4f18\u5316\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "ORPP\u901a\u8fc7\u5728\u5c0f\u6837\u672c\u4e0a\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff0c\u5e76\u5229\u7528\u6a21\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u5c06\u4f18\u5316\u7ecf\u9a8c\u8fc1\u79fb\u5230\u5176\u4ed6\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eORPP\u5728\u6027\u80fd\u4e0a\u4e0d\u4ec5\u5339\u914d\u4e14\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5e76\u5177\u5907\u51fa\u8272\u7684\u5373\u63d2\u5373\u7528\u80fd\u529b\u3002", "conclusion": "ORPP\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5177\u5907\u5e7f\u6cdb\u517c\u5bb9\u6027\u3002"}}
{"id": "2506.02453", "pdf": "https://arxiv.org/pdf/2506.02453", "abs": "https://arxiv.org/abs/2506.02453", "authors": ["Kunyu Wang", "Xueyang Fu", "Yunfei Bao", "Chengjie Ge", "Chengzhi Cao", "Wei Zhai", "Zheng-Jun Zha"], "title": "PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to online adapt a pre-trained\nmodel to changing environments during inference. Most existing methods focus on\nexploiting target data, while overlooking another crucial source of\ninformation, the pre-trained weights, which encode underutilized\ndomain-invariant priors. This paper takes the geometric attributes of\npre-trained weights as a starting point, systematically analyzing three key\ncomponents: magnitude, absolute angle, and pairwise angular structure. We find\nthat the pairwise angular structure remains stable across diverse corrupted\ndomains and encodes domain-invariant semantic information, suggesting it should\nbe preserved during adaptation. Based on this insight, we propose PAID\n(Pairwise Angular-Invariant Decomposition), a prior-driven CTTA method that\ndecomposes weight into magnitude and direction, and introduces a learnable\northogonal matrix via Householder reflections to globally rotate direction\nwhile preserving the pairwise angular structure. During adaptation, only the\nmagnitudes and the orthogonal matrices are updated. PAID achieves consistent\nimprovements over recent SOTA methods on four widely used CTTA benchmarks,\ndemonstrating that preserving pairwise angular structure offers a simple yet\neffective principle for CTTA.", "AI": {"tldr": "PAID\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u6210\u5bf9\u89d2\u5ea6\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6301\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u9884\u8bad\u7ec3\u6743\u91cd\u4e2d\u7684\u9886\u57df\u4e0d\u53d8\u5148\u9a8c\u4fe1\u606f\uff0c\u800cPAID\u5229\u7528\u51e0\u4f55\u5c5e\u6027\uff08\u5982\u6210\u5bf9\u89d2\u5ea6\u7ed3\u6784\uff09\u6765\u6539\u8fdb\u9002\u5e94\u6548\u679c\u3002", "method": "PAID\u5c06\u6743\u91cd\u5206\u89e3\u4e3a\u5e45\u5ea6\u548c\u65b9\u5411\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6b63\u4ea4\u77e9\u9635\u4ee5\u5168\u5c40\u65cb\u8f6c\u65b9\u5411\uff0c\u540c\u65f6\u4fdd\u7559\u6210\u5bf9\u89d2\u5ea6\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2aCTTA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAID\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4fdd\u7559\u6210\u5bf9\u89d2\u5ea6\u7ed3\u6784\u662fCTTA\u4e2d\u7b80\u5355\u800c\u6709\u6548\u7684\u539f\u5219\u3002"}}
{"id": "2506.02481", "pdf": "https://arxiv.org/pdf/2506.02481", "abs": "https://arxiv.org/abs/2506.02481", "authors": ["Inderjeet Nair", "Lu Wang"], "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0f\u6d4b\u8bd5\u4e2d\u63a8\u65ad\u7684LLM\u4ef7\u503c\u504f\u597d\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u4e14\u957f\u5f62\u5f0f\u751f\u6210\u8bbe\u7f6e\u95f4\u7684\u504f\u597d\u4e00\u81f4\u6027\u4e5f\u8f83\u4f4e\u3002\u5bf9\u9f50\u4ec5\u7565\u5fae\u63d0\u5347\u4ef7\u503c\u8868\u8fbe\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7d22\u77ed\u5f62\u5f0f\u6d4b\u8bd5\u4e0e\u957f\u5f62\u5f0f\u5b9e\u9645\u5e94\u7528\u4e2dLLM\u4ef7\u503c\u504f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6bd4\u8f83\u4e94\u79cdLLM\uff08llama3-8b\u7b49\uff09\u5728\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0f\u54cd\u5e94\u4e2d\u7684\u4ef7\u503c\u504f\u597d\uff0c\u5206\u6790\u53c2\u6570\u6570\u91cf\u548c\u751f\u6210\u5c5e\u6027\u5bf9\u504f\u597d\u7684\u5f71\u54cd\u3002", "result": "\u77ed\u5f62\u5f0f\u4e0e\u957f\u5f62\u5f0f\u504f\u597d\u76f8\u5173\u6027\u5f31\uff1b\u957f\u5f62\u5f0f\u751f\u6210\u8bbe\u7f6e\u95f4\u4e00\u81f4\u6027\u4f4e\uff1b\u5bf9\u9f50\u6548\u679c\u6709\u9650\uff1b\u8bba\u8bc1\u7279\u5f02\u6027\u4e0e\u504f\u597d\u5f3a\u5ea6\u8d1f\u76f8\u5173\uff0c\u573a\u666f\u8986\u76d6\u4e0e\u504f\u597d\u6b63\u76f8\u5173\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u4ee5\u786e\u4fddLLM\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u4ef7\u503c\u8868\u8fbe\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.02459", "pdf": "https://arxiv.org/pdf/2506.02459", "abs": "https://arxiv.org/abs/2506.02459", "authors": ["Martin JJ. Bucher", "Iro Armeni"], "title": "ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment", "categories": ["cs.CV", "I.2.10; I.2.7"], "comment": "20 pages, 17 figures (incl. appendix)", "summary": "Scene synthesis and editing has emerged as a promising direction in computer\ngraphics. Current trained approaches for 3D indoor scenes either oversimplify\nobject semantics through one-hot class encodings (e.g., 'chair' or 'table'),\nrequire masked diffusion for editing, ignore room boundaries, or rely on floor\nplan renderings that fail to capture complex layouts. In contrast, LLM-based\nmethods enable richer semantics via natural language (e.g., 'modern studio with\nlight wood furniture') but do not support editing, remain limited to\nrectangular layouts or rely on weak spatial reasoning from implicit world\nmodels. We introduce ReSpace, a generative framework for text-driven 3D indoor\nscene synthesis and editing using autoregressive language models. Our approach\nfeatures a compact structured scene representation with explicit room\nboundaries that frames scene editing as a next-token prediction task. We\nleverage a dual-stage training approach combining supervised fine-tuning and\npreference alignment, enabling a specially trained language model for object\naddition that accounts for user instructions, spatial geometry, object\nsemantics, and scene-level composition. For scene editing, we employ a\nzero-shot LLM to handle object removal and prompts for addition. We further\nintroduce a novel voxelization-based evaluation that captures fine-grained\ngeometry beyond 3D bounding boxes. Experimental results surpass\nstate-of-the-art on object addition while maintaining competitive results on\nfull scene synthesis.", "AI": {"tldr": "ReSpace\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u672c\u9a71\u52a8\u76843D\u5ba4\u5185\u573a\u666f\u5408\u6210\u4e0e\u7f16\u8f91\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u573a\u666f\u8868\u793a\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u8bed\u4e49\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u5ba4\u5185\u573a\u666f\u5408\u6210\u4e0e\u7f16\u8f91\u4e2d\u5b58\u5728\u8bed\u4e49\u7b80\u5316\u3001\u7f16\u8f91\u53d7\u9650\u6216\u7a7a\u95f4\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0cReSpace\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u7ed3\u6784\u5316\u8868\u793a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u7684\u7ed3\u6784\u5316\u573a\u666f\u8868\u793a\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u4e0e\u504f\u597d\u5bf9\u9f50\uff09\uff0c\u5229\u7528\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5bf9\u8c61\u6dfb\u52a0\uff0c\u96f6-shot LLM\u5904\u7406\u5bf9\u8c61\u79fb\u9664\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReSpace\u5728\u5bf9\u8c61\u6dfb\u52a0\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5728\u5b8c\u6574\u573a\u666f\u5408\u6210\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "ReSpace\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4e3a3D\u573a\u666f\u5408\u6210\u4e0e\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02483", "pdf": "https://arxiv.org/pdf/2506.02483", "abs": "https://arxiv.org/abs/2506.02483", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal"], "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks", "categories": ["cs.CL"], "comment": "Accepted at 19th Conference on Neurosymbolic Learning and Reasoning\n  (NeSy 2025)", "summary": "Large language models (LLMs) often struggle to perform multi-target reasoning\nin long-context scenarios where relevant information is scattered across\nextensive documents. To address this challenge, we introduce NeuroSymbolic\nAugmented Reasoning (NSAR), which combines the benefits of neural and symbolic\nreasoning during inference. NSAR explicitly extracts symbolic facts from text\nand generates executable Python code to handle complex reasoning steps. Through\nextensive experiments across seven languages and diverse context lengths, we\ndemonstrate that NSAR significantly outperforms both a vanilla RAG baseline and\nadvanced prompting strategies in accurately identifying and synthesizing\nmultiple pieces of information. Our results highlight the effectiveness of\ncombining explicit symbolic operations with neural inference for robust,\ninterpretable, and scalable reasoning in multilingual settings.", "AI": {"tldr": "NSAR\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u4e2d\u591a\u76ee\u6807\u63a8\u7406\u7684\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u7b26\u53f7\u4e8b\u5b9e\u5e76\u751f\u6210Python\u4ee3\u7801\uff0c\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u63a8\u7406\u3002", "result": "\u5728\u4e03\u79cd\u8bed\u8a00\u548c\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cNSAR\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u795e\u7ecf\u4e0e\u7b26\u53f7\u63a8\u7406\u7ed3\u5408\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.02462", "pdf": "https://arxiv.org/pdf/2506.02462", "abs": "https://arxiv.org/abs/2506.02462", "authors": ["Kunyu Wang", "Xueyang Fu", "Xin Lu", "Chengjie Ge", "Chengzhi Cao", "Wei Zhai", "Zheng-Jun Zha"], "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning", "categories": ["cs.CV"], "comment": "Accepted as CVPR 2025 oral paper", "summary": "Continual test-time adaptive object detection (CTTA-OD) aims to online adapt\na source pre-trained detector to ever-changing environments during inference\nunder continuous domain shifts. Most existing CTTA-OD methods prioritize\neffectiveness while overlooking computational efficiency, which is crucial for\nresource-constrained scenarios. In this paper, we propose an efficient CTTA-OD\nmethod via pruning. Our motivation stems from the observation that not all\nlearned source features are beneficial; certain domain-sensitive feature\nchannels can adversely affect target domain performance. Inspired by this, we\nintroduce a sensitivity-guided channel pruning strategy that quantifies each\nchannel based on its sensitivity to domain discrepancies at both image and\ninstance levels. We apply weighted sparsity regularization to selectively\nsuppress and prune these sensitive channels, focusing adaptation efforts on\ninvariant ones. Additionally, we introduce a stochastic channel reactivation\nmechanism to restore pruned channels, enabling recovery of potentially useful\nfeatures and mitigating the risks of early pruning. Extensive experiments on\nthree benchmarks show that our method achieves superior adaptation performance\nwhile reducing computational overhead by 12% in FLOPs compared to the recent\nSOTA method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u526a\u679d\u7684\u9ad8\u6548\u6301\u7eed\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u654f\u611f\u5ea6\u5f15\u5bfc\u7684\u901a\u9053\u526a\u679d\u7b56\u7565\u548c\u968f\u673a\u901a\u9053\u91cd\u65b0\u6fc0\u6d3b\u673a\u5236\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c2\u5bdf\u5230\u5e76\u975e\u6240\u6709\u6e90\u7279\u5f81\u90fd\u6709\u76ca\uff0c\u67d0\u4e9b\u57df\u654f\u611f\u7279\u5f81\u901a\u9053\u53ef\u80fd\u5bf9\u76ee\u6807\u57df\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u6b64\u63d0\u51fa\u526a\u679d\u7b56\u7565\u3002", "method": "\u91c7\u7528\u654f\u611f\u5ea6\u5f15\u5bfc\u7684\u901a\u9053\u526a\u679d\u7b56\u7565\uff0c\u91cf\u5316\u901a\u9053\u5bf9\u57df\u5dee\u5f02\u7684\u654f\u611f\u5ea6\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u7a00\u758f\u6b63\u5219\u5316\u9009\u62e9\u6027\u526a\u679d\uff1b\u5f15\u5165\u968f\u673a\u901a\u9053\u91cd\u65b0\u6fc0\u6d3b\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c1112%\uff08FLOPs\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u9002\u5e94\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Minos-Corpus\u6570\u636e\u96c6\u548cMinos\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u7684\u8bc4\u4f30\uff0c\u7ed3\u5408\u4eba\u7c7b\u548cGPT\u6570\u636e\uff0c\u5728T2I\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5bf9T2I\u751f\u6210\u4efb\u52a1\u7684\u8bc4\u4f30\u80fd\u529b\u548c\u5927\u89c4\u6a21\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e\u7684\u7ed3\u5408\u3002", "method": "\u63d0\u51faMinos-Corpus\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4eba\u7c7b\u548cGPT\u6570\u636e\uff1b\u91c7\u7528Data Selection and Balance\u3001Mix-SFT\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528DPO\u5f00\u53d1Minos\u6a21\u578b\u3002", "result": "Minos\u5728T2I\u4efb\u52a1\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6240\u6709\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u8fbe\u5230\u540c\u7c7b\u6a21\u578b\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e\u548c\u8054\u5408\u8bad\u7ec3I2T\u4e0eT2I\u4efb\u52a1\u6570\u636e\u5bf9\u63d0\u5347\u8bc4\u4f30\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.02472", "pdf": "https://arxiv.org/pdf/2506.02472", "abs": "https://arxiv.org/abs/2506.02472", "authors": ["Halil Ismail Helvaci", "Justin Philip Huber", "Jihye Bae", "Sen-ching Samson Cheung"], "title": "HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation", "categories": ["cs.CV"], "comment": null, "summary": "Stroke rehabilitation often demands precise tracking of patient movements to\nmonitor progress, with complexities of rehabilitation exercises presenting two\ncritical challenges: fine-grained and sub-second (under one-second) action\ndetection. In this work, we propose the High Resolution Temporal Transformer\n(HRTR), to time-localize and classify high-resolution (fine-grained),\nsub-second actions in a single-stage transformer, eliminating the need for\nmulti-stage methods and post-processing. Without any refinements, HRTR\noutperforms state-of-the-art systems on both stroke related and general\ndatasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on\nStrokeRehab IMU, and 88.4 on 50Salads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHRTR\u7684\u5355\u9636\u6bb5Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u3001\u4e9a\u79d2\u7ea7\u52a8\u4f5c\u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e2d\u98ce\u5eb7\u590d\u9700\u8981\u7cbe\u786e\u8ddf\u8e2a\u60a3\u8005\u52a8\u4f5c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u3001\u4e9a\u79d2\u7ea7\u7684\u52a8\u4f5c\u68c0\u6d4b\u3002", "method": "\u63d0\u51faHRTR\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u9636\u6bb5Transformer\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u3001\u4e9a\u79d2\u7ea7\u52a8\u4f5c\u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u65e0\u9700\u591a\u9636\u6bb5\u65b9\u6cd5\u6216\u540e\u5904\u7406\u3002", "result": "HRTR\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cEdit Score\u5206\u522b\u4e3a70.1\uff08StrokeRehab Video\uff09\u300169.4\uff08StrokeRehab IMU\uff09\u548c88.4\uff0850Salads\uff09\u3002", "conclusion": "HRTR\u5728\u52a8\u4f5c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e2d\u98ce\u5eb7\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02503", "pdf": "https://arxiv.org/pdf/2506.02503", "abs": "https://arxiv.org/abs/2506.02503", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub.", "AI": {"tldr": "KARE-RAG\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3001\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\u548c\u5bf9\u6bd4\u6570\u636e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u566a\u58f0\u5185\u5bb9\u548c\u7ea0\u6b63\u5173\u952e\u9519\u8bef\u65b9\u9762\u3002", "motivation": "\u5c3d\u7ba1RAG\u6269\u5c55\u4e86LLMs\u7684\u77e5\u8bc6\u6765\u6e90\uff0c\u4f46\u68c0\u7d22\u6587\u6863\u4e2d\u7684\u566a\u58f0\u5bfc\u81f4\u4e8b\u5b9e\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u9700\u8981\u589e\u5f3a\u6a21\u578b\u5904\u7406\u566a\u58f0\u5185\u5bb9\u7684\u80fd\u529b\u3002", "method": "KARE-RAG\u5f15\u5165\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3001DDPO\u8bad\u7ec3\u76ee\u6807\u548c\u5bf9\u6bd4\u6570\u636e\u751f\u6210\uff0c\u4f18\u5316\u77e5\u8bc6\u5229\u7528\u548c\u9519\u8bef\u7ea0\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKARE-RAG\u663e\u8457\u63d0\u5347\u4e86RAG\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u5305\u62ec\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u4efb\u52a1\uff0c\u4e14\u6570\u636e\u6548\u7387\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u5904\u7406\u68c0\u7d22\u5185\u5bb9\u7684\u65b9\u5f0f\uff0cKARE-RAG\u4e3aRAG\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e14\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.02473", "pdf": "https://arxiv.org/pdf/2506.02473", "abs": "https://arxiv.org/abs/2506.02473", "authors": ["Xinran Nicole Han", "Ko Nishino", "Todd Zickler"], "title": "Generative Perception of Shape and Material from Differential Motion", "categories": ["cs.CV"], "comment": null, "summary": "Perceiving the shape and material of an object from a single image is\ninherently ambiguous, especially when lighting is unknown and unconstrained.\nDespite this, humans can often disentangle shape and material, and when they\nare uncertain, they often move their head slightly or rotate the object to help\nresolve the ambiguities. Inspired by this behavior, we introduce a novel\nconditional denoising-diffusion model that generates samples of\nshape-and-material maps from a short video of an object undergoing differential\nmotions. Our parameter-efficient architecture allows training directly in\npixel-space, and it generates many disentangled attributes of an object\nsimultaneously. Trained on a modest number of synthetic object-motion videos\nwith supervision on shape and material, the model exhibits compelling emergent\nbehavior: For static observations, it produces diverse, multimodal predictions\nof plausible shape-and-material maps that capture the inherent ambiguities; and\nwhen objects move, the distributions quickly converge to more accurate\nexplanations. The model also produces high-quality shape-and-material estimates\nfor less ambiguous, real-world objects. By moving beyond single-view to\ncontinuous motion observations, our work suggests a generative perception\napproach for improving visual reasoning in physically-embodied systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u65f6\u89c6\u9891\u4e2d\u7684\u7269\u4f53\u8fd0\u52a8\u4fe1\u606f\u751f\u6210\u5f62\u72b6\u548c\u6750\u8d28\u7684\u591a\u6a21\u6001\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u5355\u89c6\u89d2\u4e0b\u7684\u611f\u77e5\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5fae\u5c0f\u8fd0\u52a8\u6216\u65cb\u8f6c\u7269\u4f53\u6765\u6d88\u9664\u5f62\u72b6\u548c\u6750\u8d28\u611f\u77e5\u7684\u6a21\u7cca\u6027\uff0c\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528\u8fd0\u52a8\u4fe1\u606f\u63d0\u5347\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u67b6\u6784\uff0c\u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u8bad\u7ec3\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u7269\u4f53\u7684\u591a\u5c5e\u6027\u89e3\u8026\u8868\u793a\u3002", "result": "\u6a21\u578b\u5728\u9759\u6001\u89c2\u6d4b\u4e0b\u751f\u6210\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u9884\u6d4b\uff0c\u8fd0\u52a8\u65f6\u5feb\u901f\u6536\u655b\u5230\u66f4\u51c6\u786e\u7684\u89e3\u91ca\uff0c\u5e76\u5728\u771f\u5b9e\u7269\u4f53\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u901a\u8fc7\u8fde\u7eed\u8fd0\u52a8\u89c2\u6d4b\uff0c\u8be5\u751f\u6210\u5f0f\u611f\u77e5\u65b9\u6cd5\u4e3a\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02510", "pdf": "https://arxiv.org/pdf/2506.02510", "abs": "https://arxiv.org/abs/2506.02510", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL-2025", "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u884c\u4e1a\u3001\u591a\u4efb\u52a1\u7684\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u57fa\u51c6M\u00b3FinMeeting\uff0c\u586b\u8865\u4e86\u73b0\u6709\u91d1\u878d\u8bc4\u6d4b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u8bc4\u6d4b\u4e3b\u8981\u4f9d\u8d56\u65b0\u95fb\u6216\u62a5\u544a\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u91d1\u878d\u4f1a\u8bae\u52a8\u6001\uff0c\u9700\u66f4\u5168\u9762\u7684\u8bc4\u6d4b\u5de5\u5177\u3002", "method": "\u6784\u5efa\u652f\u6301\u82f1\u4e2d\u65e5\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u8986\u76d6GICS\u884c\u4e1a\u5206\u7c7b\uff0c\u5305\u542b\u6458\u8981\u3001QA\u5bf9\u63d0\u53d6\u548c\u95ee\u7b54\u4e09\u9879\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5148\u8fdb\u7684\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u4e0a\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "M\u00b3FinMeeting\u80fd\u6709\u6548\u8bc4\u4f30LLMs\u5728\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u4e0a\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u6a21\u578b\u8fdb\u6b65\u3002"}}
{"id": "2506.02477", "pdf": "https://arxiv.org/pdf/2506.02477", "abs": "https://arxiv.org/abs/2506.02477", "authors": ["Kunyu Wang", "Xueyang Fu", "Chengzhi Cao", "Chengjie Ge", "Wei Zhai", "Zheng-Jun Zha"], "title": "Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay", "categories": ["cs.CV"], "comment": null, "summary": "Current image de-raining methods primarily learn from a limited dataset,\nleading to inadequate performance in varied real-world rainy conditions. To\ntackle this, we introduce a new framework that enables networks to\nprogressively expand their de-raining knowledge base by tapping into a growing\npool of datasets, significantly boosting their adaptability. Drawing\ninspiration from the human brain's ability to continuously absorb and\ngeneralize from ongoing experiences, our approach borrow the mechanism of the\ncomplementary learning system. Specifically, we first deploy Generative\nAdversarial Networks (GANs) to capture and retain the unique features of new\ndata, mirroring the hippocampus's role in learning and memory. Then, the\nde-raining network is trained with both existing and GAN-synthesized data,\nmimicking the process of hippocampal replay and interleaved learning.\nFurthermore, we employ knowledge distillation with the replayed data to\nreplicate the synergy between the neocortex's activity patterns triggered by\nhippocampal replays and the pre-existing neocortical knowledge. This\ncomprehensive framework empowers the de-raining network to amass knowledge from\nvarious datasets, continually enhancing its performance on previously unseen\nrainy scenes. Our testing on three benchmark de-raining networks confirms the\nframework's effectiveness. It not only facilitates continuous knowledge\naccumulation across six datasets but also surpasses state-of-the-art methods in\ngeneralizing to new real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u6b65\u6269\u5c55\u6570\u636e\u96c6\u63d0\u5347\u56fe\u50cf\u53bb\u96e8\u7f51\u7edc\u7684\u9002\u5e94\u6027\uff0c\u7ed3\u5408GAN\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u6a21\u62df\u4eba\u8111\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u96c6\u6709\u9650\uff0c\u5728\u591a\u6837\u5316\u771f\u5b9e\u96e8\u5929\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u63d0\u5347\u7f51\u7edc\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528GAN\u6355\u83b7\u65b0\u6570\u636e\u7279\u5f81\uff0c\u6a21\u62df\u6d77\u9a6c\u4f53\u5b66\u4e60\uff1b\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u6a21\u62df\u65b0\u76ae\u8d28\u4e0e\u6d77\u9a6c\u4f53\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u7f51\u7edc\u4e0a\u6d4b\u8bd5\uff0c\u6846\u67b6\u80fd\u6301\u7eed\u79ef\u7d2f\u77e5\u8bc6\uff0c\u5e76\u5728\u65b0\u573a\u666f\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u53bb\u96e8\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02515", "pdf": "https://arxiv.org/pdf/2506.02515", "abs": "https://arxiv.org/abs/2506.02515", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "AI": {"tldr": "FinChain\u662f\u9996\u4e2a\u7528\u4e8e\u9a8c\u8bc1\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u91d1\u878d\u63a8\u7406\u7684\u7b26\u53f7\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u4efb\u52a1\u6570\u636e\u96c6\uff08\u5982FinQA\u548cConvFinQA\uff09\u4ec5\u76d1\u7763\u6700\u7ec8\u6570\u503c\u7b54\u6848\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "FinChain\u6db5\u76d612\u4e2a\u91d1\u878d\u9886\u57df\u768454\u4e2a\u4e3b\u9898\uff0c\u6bcf\u4e2a\u4e3b\u9898\u63d0\u4f9b5\u79cd\u53c2\u6570\u5316\u6a21\u677f\uff0c\u5305\u542b\u53ef\u6267\u884c\u7684Python\u8ddf\u8e2a\uff0c\u652f\u6301\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002\u540c\u65f6\u63d0\u51faChainEval\u6307\u6807\uff0c\u81ea\u52a8\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u548c\u4e2d\u95f4\u63a8\u7406\u3002", "result": "\u6d4b\u8bd530\u4e2aLLM\u540e\u53d1\u73b0\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u591a\u6b65\u91d1\u878d\u63a8\u7406\u4e0a\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "FinChain\u4e3a\u91d1\u878d\u63a8\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.02488", "pdf": "https://arxiv.org/pdf/2506.02488", "abs": "https://arxiv.org/abs/2506.02488", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.", "AI": {"tldr": "Flexiffusion\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684NAS\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u5408\u751f\u6210\u6b65\u9aa4\u7c7b\u578b\u548c\u5f15\u5165\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6307\u6807rFID\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u56e0\u591a\u6b65\u63a8\u7406\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709NAS\u65b9\u6cd5\u53d7\u9650\u4e8e\u91cd\u8bad\u7ec3\u9700\u6c42\u3001\u641c\u7d22\u590d\u6742\u6027\u548c\u7f13\u6162\u8bc4\u4f30\u3002", "method": "\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u7075\u6d3b\u6bb5\uff0c\u52a8\u6001\u7ec4\u5408\u5b8c\u6574\u8ba1\u7b97\u3001\u7f13\u5b58\u91cd\u7528\u548c\u8df3\u8fc7\u8ba1\u7b97\u4e09\u79cd\u6b65\u9aa4\u7c7b\u578b\uff0c\u5e76\u5f15\u5165rFID\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b02\u500d\u4ee5\u4e0a\u52a0\u901f\uff0cFID\u9000\u5316\u4f4e\u4e8e5%\uff0cStable Diffusion\u4e0a\u8fbe\u52305.1\u500d\u52a0\u901f\u3002", "conclusion": "Flexiffusion\u4e3a\u9ad8\u6548\u641c\u7d22\u9ad8\u901f\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u65e0\u9700\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.02519", "pdf": "https://arxiv.org/pdf/2506.02519", "abs": "https://arxiv.org/abs/2506.02519", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate).", "AI": {"tldr": "COLLATE\u6846\u67b6\u901a\u8fc7\u591a\u6837\u5316\u751f\u6210\u548c\u504f\u597d\u4f18\u5316\u63d0\u5347\u5c0f\u578bLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u56e0\u7248\u6743\u548c\u6cd5\u5f8b\u95ee\u9898\u65e0\u6cd5\u4f7f\u7528\u5927\u578bLLM\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u72ec\u7acb\u63a8\u7406\u80fd\u529b\u3002", "method": "COLLATE\u901a\u8fc7\u591a\u6837\u5316\u751f\u6210\u5019\u9009\u7b54\u6848\uff0c\u5e76\u5229\u7528\u504f\u597d\u4f18\u5316\u9009\u62e9\u6700\u4f73\u7b54\u6848\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "COLLATE\u4e3a\u5c0f\u578bLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u72ec\u7acb\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u6848\u3002"}}
{"id": "2506.02492", "pdf": "https://arxiv.org/pdf/2506.02492", "abs": "https://arxiv.org/abs/2506.02492", "authors": ["Yuanpeng He", "Lijian Li", "Tianxiang Zhan", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Co-Evidential Fusion with Information Volume for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Although existing semi-supervised image segmentation methods have achieved\ngood performance, they cannot effectively utilize multiple sources of\nvoxel-level uncertainty for targeted learning. Therefore, we propose two main\nimprovements. First, we introduce a novel pignistic co-evidential fusion\nstrategy using generalized evidential deep learning, extended by traditional\nD-S evidence theory, to obtain a more precise uncertainty measure for each\nvoxel in medical samples. This assists the model in learning mixed labeled\ninformation and establishing semantic associations between labeled and\nunlabeled data. Second, we introduce the concept of information volume of mass\nfunction (IVUM) to evaluate the constructed evidence, implementing two\nevidential learning schemes. One optimizes evidential deep learning by\ncombining the information volume of the mass function with original uncertainty\nmeasures. The other integrates the learning pattern based on the co-evidential\nfusion strategy, using IVUM to design a new optimization objective. Experiments\non four datasets demonstrate the competitive performance of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u7b56\u7565\u548c\u4fe1\u606f\u91cf\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6e90\u4f53\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u9488\u5bf9\u6027\u5b66\u4e60\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u7684pignistic\u5171\u8bc1\u636e\u878d\u5408\u7b56\u7565\uff1b2. \u5f15\u5165\u4fe1\u606f\u91cf\u8bc4\u4f30\uff08IVUM\uff09\u8bbe\u8ba1\u4e24\u79cd\u8bc1\u636e\u5b66\u4e60\u65b9\u6848\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u548c\u4f18\u5316\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5206\u5272\u6548\u679c\u3002"}}
{"id": "2506.02527", "pdf": "https://arxiv.org/pdf/2506.02527", "abs": "https://arxiv.org/abs/2506.02527", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "6 pages, accepted at GENNEXT@SIGIR25", "summary": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u52a0\u6743\u91c7\u6837\u5fae\u8c03\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u9ad8\u8d28\u91cf\u77e5\u8bc6\u5e93\u8d44\u6e90\u7a00\u7f3a\u4e14\u8bed\u8a00\u6709\u9650\uff0c\u9700\u6709\u6548\u5d4c\u5165\u6a21\u578b\u5b9e\u73b0\u8de8\u8bed\u8a00\u77e5\u8bc6\u5171\u4eab\u3002", "method": "\u91c7\u7528\u52a0\u6743\u91c7\u6837\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u5fae\u8c03\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u3002", "result": "\u52a0\u6743\u91c7\u6837\u7b56\u7565\u5728MRR\u548cRecall@3\u4e0a\u5206\u522b\u63d0\u534731.03%\u548c33.98%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bed\u8a00\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u4ee3\u7801\u6df7\u5408\u573a\u666f\u3002"}}
{"id": "2506.02493", "pdf": "https://arxiv.org/pdf/2506.02493", "abs": "https://arxiv.org/abs/2506.02493", "authors": ["Jiachen Liu", "Rui Yu", "Sili Chen", "Sharon X. Huang", "Hengkai Guo"], "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlighted Paper", "summary": "3D plane reconstruction from a single image is a crucial yet challenging\ntopic in 3D computer vision. Previous state-of-the-art (SOTA) methods have\nfocused on training their system on a single dataset from either indoor or\noutdoor domain, limiting their generalizability across diverse testing data. In\nthis work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based\nmodel targeting zero-shot 3D plane detection and reconstruction from a single\nimage, over diverse domains and environments. To enable data-driven models\nacross multiple domains, we have curated a large-scale planar benchmark,\ncomprising over 14 datasets and 560,000 high-resolution, dense planar\nannotations for diverse indoor and outdoor scenes. To address the challenge of\nachieving desirable planar geometry on multi-dataset training, we propose to\ndisentangle the representation of plane normal and offset, and employ an\nexemplar-guided, classification-then-regression paradigm to learn plane and\noffset respectively. Additionally, we employ advanced backbones as image\nencoder, and present an effective pixel-geometry-enhanced plane embedding\nmodule to further facilitate planar reconstruction. Extensive experiments\nacross multiple zero-shot evaluation datasets have demonstrated that our\napproach significantly outperforms previous methods on both reconstruction\naccuracy and generalizability, especially over in-the-wild data. Our code and\ndata are available at: https://github.com/jcliu0428/ZeroPlane.", "AI": {"tldr": "ZeroPlane\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c3D\u5e73\u9762\u68c0\u6d4b\u548c\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002ZeroPlane\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u5e73\u9762\u6cd5\u7ebf\u548c\u504f\u79fb\u8868\u793a\uff0c\u91c7\u7528\u5206\u7c7b-\u56de\u5f52\u8303\u5f0f\u5b66\u4e60\u5e73\u9762\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u9ad8\u7ea7\u9aa8\u5e72\u7f51\u7edc\u548c\u50cf\u7d20\u51e0\u4f55\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u96f6\u6837\u672c\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\uff0cZeroPlane\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u91ce\u5916\u6570\u636e\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ZeroPlane\u901a\u8fc7\u591a\u57df\u8bad\u7ec3\u548c\u51e0\u4f55\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u76843D\u5e73\u9762\u91cd\u5efa\u3002"}}
{"id": "2506.02532", "pdf": "https://arxiv.org/pdf/2506.02532", "abs": "https://arxiv.org/abs/2506.02532", "authors": ["Jinu Lee", "Sagnik Mukherjee", "Dilek Hakkani-Tur", "Julia Hockenmaier"], "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces", "categories": ["cs.CL"], "comment": "10 pages, 6 figures. ArgMining 2025 Workshop (Non-archival) @ ACL\n  2025", "summary": "Large reasoning models (LRMs) generate complex reasoning traces with\nplanning, reflection, verification, and backtracking. In this work, we\nintroduce ReasoningFlow, a unified schema for analyzing the semantic structures\nof these complex traces. ReasoningFlow parses traces into directed acyclic\ngraphs, enabling the characterization of distinct reasoning patterns as\nsubgraph structures. This human-interpretable representation offers promising\napplications in understanding, evaluating, and enhancing the reasoning\nprocesses of LRMs.", "AI": {"tldr": "ReasoningFlow\u662f\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u590d\u6742\u63a8\u7406\u8f68\u8ff9\uff0c\u5c06\u5176\u89e3\u6790\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u4ece\u800c\u8bc6\u522b\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u590d\u6742\u4e14\u591a\u6837\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5206\u6790\u548c\u7406\u89e3\u8fd9\u4e9b\u8f68\u8ff9\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u63d0\u51faReasoningFlow\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u89e3\u6790\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5e76\u901a\u8fc7\u5b50\u56fe\u7ed3\u6784\u8868\u5f81\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "result": "ReasoningFlow\u63d0\u4f9b\u4e86\u4e00\u79cd\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "ReasoningFlow\u4e3a\u5206\u6790\u548c\u6539\u8fdb\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2506.02497", "pdf": "https://arxiv.org/pdf/2506.02497", "abs": "https://arxiv.org/abs/2506.02497", "authors": ["Jiahao Chen", "Hangjie Yuan", "Yichen Qian", "Jingyun Liang", "Jiazheng Xing", "Pengwei Liu", "Weihua Chen", "Fan Wang", "Bing Su"], "title": "LumosFlow: Motion-Guided Long Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLumosFlow\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8fd0\u52a8\u6307\u5bfc\u6539\u8fdb\u957f\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u65f6\u95f4\u91cd\u590d\u548c\u4e0d\u81ea\u7136\u8fc7\u6e21\u95ee\u9898\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\u5728\u5a31\u4e50\u548c\u6a21\u62df\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u5438\u5f15\u529b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528LMTV-DM\u751f\u6210\u5927\u8fd0\u52a8\u95f4\u9694\u7684\u5173\u952e\u5e27\uff0c\u5e76\u901a\u8fc7LOF-DM\u548cMotionControlNet\u5206\u89e3\u4e2d\u95f4\u5e27\u63d2\u503c\u4e3a\u8fd0\u52a8\u751f\u6210\u548c\u540e\u5904\u7406\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u5177\u6709\u4e00\u81f4\u8fd0\u52a8\u548c\u5916\u89c2\u7684\u957f\u89c6\u9891\uff0c\u5b9e\u73b0\u4e8615\u500d\u7684\u63d2\u503c\u6548\u679c\u3002", "conclusion": "LumosFlow\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u8fde\u7eed\u6027\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533", "abs": "https://arxiv.org/abs/2506.02533", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u95ee\u9898\u53ca\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u63d0\u5347\u8ba8\u8bba\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u653f\u6cbb\u8ba8\u8bba\u9010\u6e10\u6570\u5b57\u5316\uff0c\u9ad8\u8d28\u91cf\u7684\u5728\u7ebf\u8ba8\u8bba\u548c\u53c2\u4e0e\u8fc7\u7a0b\u5bf9\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e73\u53f0\u8bbe\u8ba1\u5f71\u54cd\u8ba8\u8bba\u8d28\u91cf\u3002", "method": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u5e76\u89e3\u51b3\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u63d0\u5347\u8ba8\u8bba\u8d28\u91cf\u548c\u4fc3\u8fdb\u6df1\u601d\u719f\u8651\u4ea4\u6d41\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u6539\u5584\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u7684\u8bbe\u8ba1\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.02528", "pdf": "https://arxiv.org/pdf/2506.02528", "abs": "https://arxiv.org/abs/2506.02528", "authors": ["Yan Gong", "Yiren Song", "Yicheng Li", "Chenglin Li", "Yin Zhang"], "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u65b0\u65b9\u6cd5RelationAdapter\uff0c\u5229\u7528\u6e90-\u76ee\u6807\u56fe\u50cf\u5bf9\u63d0\u53d6\u7f16\u8f91\u610f\u56fe\uff0c\u5e76\u5f15\u5165Relation252K\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5355\u53c2\u8003\u65b9\u6cd5\u5728\u975e\u521a\u6027\u53d8\u6362\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u4ee5\u652f\u6301\u591a\u6837\u5316\u7f16\u8f91\u4efb\u52a1\u3002", "method": "\u63d0\u51faRelationAdapter\u6a21\u5757\uff0c\u7ed3\u5408DiT\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u6355\u6349\u548c\u5e94\u7528\u89c6\u89c9\u53d8\u6362\u3002", "result": "RelationAdapter\u663e\u8457\u63d0\u5347\u7f16\u8f91\u610f\u56fe\u7684\u7406\u89e3\u548c\u8f6c\u79fb\u80fd\u529b\uff0c\u751f\u6210\u8d28\u91cf\u548c\u7f16\u8f91\u6027\u80fd\u5747\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "RelationAdapter\u4e3a\u89c6\u89c9\u63d0\u793a\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536", "abs": "https://arxiv.org/abs/2506.02536", "authors": ["Xin Liu", "Lu Wang"], "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u6b65\u9aa4\uff0c60%\u7684\u63a8\u7406\u6b65\u9aa4\u540e\u5373\u53ef\u7a33\u5b9a\u8f93\u51fa\u7b54\u6848\u3002\u63d0\u51fa\u4e86\u4e09\u79cd\u9ad8\u6548\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "Chain-of-thought\uff08CoT\uff09\u63d0\u793a\u867d\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bfc\u81f4\u5197\u957f\u8f93\u51fa\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff0c\u63a8\u6d4b\u8bb8\u591a\u63a8\u7406\u6b65\u9aa4\u53ef\u80fd\u4e0d\u5fc5\u8981\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u786e\u5b9a\u6700\u5c0f\u5fc5\u8981\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u7b56\u7565\uff1a\u57fa\u4e8e\u7b54\u6848\u4e00\u81f4\u6027\u7684\u65e9\u671f\u505c\u6b62\u3001\u589e\u5f3a\u7ed3\u675f\u4fe1\u53f7\u751f\u6210\u6982\u7387\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5185\u90e8\u6fc0\u6d3b\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e94\u4e2a\u5f00\u6e90LLMs\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u4e14\u51c6\u786e\u6027\u51e0\u4e4e\u65e0\u4e0b\u964d\uff0c\u5176\u4e2dAnswer Consistency\u5728NaturalQuestions\u4e0a\u51cf\u5c1140%\u4ee4\u724c\u5e76\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.02534", "pdf": "https://arxiv.org/pdf/2506.02534", "abs": "https://arxiv.org/abs/2506.02534", "authors": ["Sining Chen", "Yilei Shi", "Xiao Xiang Zhu"], "title": "Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels", "categories": ["cs.CV"], "comment": null, "summary": "Monocular height estimation is considered the most efficient and\ncost-effective means of 3D perception in remote sensing, and it has attracted\nmuch attention since the emergence of deep learning. While training neural\nnetworks requires a large amount of data, data with perfect labels are scarce\nand only available within developed regions. The trained models therefore lack\ngeneralizability, which limits the potential for large-scale application of\nexisting methods. We tackle this problem for the first time, by introducing\ndata with imperfect labels into training pixel-wise height estimation networks,\nincluding labels that are incomplete, inexact, and inaccurate compared to\nhigh-quality labels. We propose an ensemble-based pipeline compatible with any\nmonocular height estimation network. Taking the challenges of noisy labels,\ndomain shift, and long-tailed distribution of height values into consideration,\nwe carefully design the architecture and loss functions to leverage the\ninformation concealed in imperfect labels using weak supervision through\nbalanced soft losses and ordinal constraints. We conduct extensive experiments\non two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).\nThe results indicate that the proposed pipeline outperforms baselines by\nachieving more balanced performance across various domains, leading to\nimprovements of average root mean square errors up to 22.94 %, and 18.62 % on\nDFC23 and GBH, respectively. The efficacy of each design component is validated\nthrough ablation studies. Code is available at\nhttps://github.com/zhu-xlab/weakim2h.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u6570\u636e\u8bad\u7ec3\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u7b7e\u6570\u636e\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u7a00\u7f3a\u4e14\u96c6\u4e2d\u5728\u53d1\u8fbe\u5730\u533a\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8bba\u6587\u9996\u6b21\u5c1d\u8bd5\u5229\u7528\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u6570\u636e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u517c\u5bb9\u4efb\u4f55\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u901a\u8fc7\u5e73\u8861\u8f6f\u635f\u5931\u548c\u5e8f\u6570\u7ea6\u675f\uff0c\u5904\u7406\u566a\u58f0\u6807\u7b7e\u3001\u57df\u504f\u79fb\u548c\u9ad8\u5ea6\u503c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002", "result": "\u5728DFC23\u548cGBH\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8622.94%\u548c18.62%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u6570\u636e\uff0c\u8bba\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee\u9ad8\u5ea6\u4f30\u8ba1\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544", "abs": "https://arxiv.org/abs/2506.02544", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "AI": {"tldr": "CoRe-MMRAG \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u6d41\u7a0b\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u53c2\u6570\u5316\u4e0e\u68c0\u7d22\u77e5\u8bc6\u4e0d\u4e00\u81f4\uff08PRKI\uff09\u53ca\u89c6\u89c9\u4e0e\u6587\u672c\u77e5\u8bc6\u4e0d\u4e00\u81f4\uff08VTKI\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u56db\u9636\u6bb5\u6d41\u7a0b\uff1a\u751f\u6210\u5185\u90e8\u54cd\u5e94\u3001\u9009\u62e9\u591a\u6a21\u6001\u8bc1\u636e\u3001\u751f\u6210\u5916\u90e8\u54cd\u5e94\u3001\u6574\u5408\u751f\u6210\u53ef\u9760\u7b54\u6848\uff0c\u5e76\u91c7\u7528\u4e13\u95e8\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728 KB-VQA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347 5.6% \u548c 9.3%\u3002", "conclusion": "CoRe-MMRAG \u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.02535", "pdf": "https://arxiv.org/pdf/2506.02535", "abs": "https://arxiv.org/abs/2506.02535", "authors": ["Juntong Li", "Lingwei Dang", "Yukun Su", "Yun Hao", "Qingxin Xiao", "Yongwei Nie", "Qingyao Wu"], "title": "MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection (VAD) methods based on reconstruction or prediction\nface two critical challenges: (1) strong generalization capability often\nresults in accurate reconstruction or prediction of abnormal events, making it\ndifficult to distinguish normal from abnormal patterns; (2) reliance only on\nlow-level appearance and motion cues limits their ability to identify\nhigh-level semantic in abnormal events from complex scenes. To address these\nlimitations, we propose a novel VAD framework with two key innovations. First,\nto suppress excessive generalization, we introduce the Sparse Feature Filtering\nModule (SFFM) that employs bottleneck filters to dynamically and adaptively\nremove abnormal information from features. Unlike traditional memory modules,\nit does not need to memorize the normal prototypes across the training dataset.\nFurther, we design the Mixture of Experts (MoE) architecture for SFFM. Each\nexpert is responsible for extracting specialized principal features during\nrunning time, and different experts are selectively activated to ensure the\ndiversity of the learned principal features. Second, to overcome the neglect of\nsemantics in existing methods, we integrate a Vision-Language Model (VLM) to\ngenerate textual descriptions for video clips, enabling comprehensive joint\nmodeling of semantic, appearance, and motion cues. Additionally, we enforce\nmodality consistency through semantic similarity constraints and motion\nframe-difference contrastive loss. Extensive experiments on multiple public\ndatasets validate the effectiveness of our multimodal joint modeling framework\nand sparse feature filtering paradigm. Project page at\nhttps://qzfm.github.io/sfn_vad_project_page/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u7279\u5f81\u8fc7\u6ee4\u6a21\u5757\uff08SFFM\uff09\u548c\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u6291\u5236\u8fc7\u5ea6\u6cdb\u5316\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91cd\u5efa\u6216\u9884\u6d4b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u6cdb\u5316\u548c\u5ffd\u89c6\u9ad8\u5c42\u8bed\u4e49\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSFFM\u52a8\u6001\u8fc7\u6ee4\u5f02\u5e38\u4fe1\u606f\uff0cMoE\u67b6\u6784\u63d0\u53d6\u591a\u6837\u5316\u7279\u5f81\uff1b\u7ed3\u5408VLM\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u8054\u5408\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u7279\u5f81\u8fc7\u6ee4\u548c\u591a\u6a21\u6001\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.02561", "pdf": "https://arxiv.org/pdf/2506.02561", "abs": "https://arxiv.org/abs/2506.02561", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "title": "Pruning General Large Language Models into Customized Expert Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCus-Prun\u7684\u81ea\u5b9a\u4e49\u526a\u679d\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u5927\u578b\u901a\u7528\u8bed\u8a00\u6a21\u578b\u526a\u679d\u4e3a\u5c0f\u578b\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\uff0c\u65e0\u9700\u540e\u8bad\u7ec3\uff0c\u4e14\u5728\u4e13\u5bb6\u548c\u901a\u7528\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u4fdd\u7559\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u9700\u8981\u540e\u8bad\u7ec3\u3002", "method": "Cus-Prun\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u5e76\u526a\u679d\u8bed\u8a00\u3001\u9886\u57df\u548c\u4efb\u52a1\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u65e0\u5173\u795e\u7ecf\u5143\uff0c\u751f\u6210\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCus-Prun\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e13\u5bb6\u548c\u901a\u7528\u80fd\u529b\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "Cus-Prun\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u540e\u8bad\u7ec3\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u4e0b\u6e38\u573a\u666f\u7684\u4e13\u5bb6\u6a21\u578b\u3002"}}
{"id": "2506.02537", "pdf": "https://arxiv.org/pdf/2506.02537", "abs": "https://arxiv.org/abs/2506.02537", "authors": ["Hao Yan", "Handong Zheng", "Hao Wang", "Liang Yin", "Xingchen Liu", "Zhenbiao Cao", "Xinxing Su", "Zihao Chen", "Jihao Wu", "Minghui Liao", "Chao Weng", "Wei Chen", "Yuliang Liu", "Xiang Bai"], "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Recent strides in multimodal large language models (MLLMs) have significantly\nadvanced their performance in many reasoning tasks. However, Abstract Visual\nReasoning (AVR) remains a critical challenge, primarily due to limitations in\nperceiving abstract graphics. To tackle this issue, we investigate the\nbottlenecks in current MLLMs and synthesize training data to improve their\nabstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,\nfeaturing tasks meticulously constructed to assess models' reasoning capacities\nacross five core dimensions and two high-level reasoning categories. Second, we\nintroduce the Perceptual Riddle Synthesizer (PRS), an automated framework for\ngenerating riddles with fine-grained perceptual descriptions. PRS not only\ngenerates valuable training data for abstract graphics but also provides\nfine-grained perceptual description, crucially allowing for supervision over\nintermediate reasoning stages and thereby improving both training efficacy and\nmodel interpretability. Our extensive experimental results on VisuRiddles\nempirically validate that fine-grained visual perception is the principal\nbottleneck and our synthesis framework markedly enhances the performance of\ncontemporary MLLMs on these challenging tasks. Our code and dataset will be\nreleased at https://github.com/yh-hust/VisuRiddles", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VisuRiddles\u57fa\u51c6\u548cPRS\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\uff08AVR\uff09\u4e2d\u5b58\u5728\u611f\u77e5\u62bd\u8c61\u56fe\u5f62\u7684\u5c40\u9650\u6027\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "1. \u63d0\u51faVisuRiddles\u57fa\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u4e94\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u548c\u4e24\u7c7b\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff1b2. \u5f00\u53d1PRS\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u5e26\u6709\u7ec6\u7c92\u5ea6\u611f\u77e5\u63cf\u8ff0\u7684\u8c1c\u9898\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u76d1\u7763\u4e2d\u95f4\u63a8\u7406\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u662f\u4e3b\u8981\u74f6\u9888\uff0cPRS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "PRS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2506.02573", "pdf": "https://arxiv.org/pdf/2506.02573", "abs": "https://arxiv.org/abs/2506.02573", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "categories": ["cs.CL"], "comment": "25 pages", "summary": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased.", "AI": {"tldr": "IndoSafety\u662f\u9996\u4e2a\u9488\u5bf9\u5370\u5c3c\u8bed\u5883\u7684\u9ad8\u8d28\u91cf\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684\u5b89\u5168\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e94\u79cd\u8bed\u8a00\u53d8\u4f53\uff0c\u53d1\u73b0\u73b0\u6709\u5370\u5c3c\u4e2d\u5fc3LLM\u5e38\u751f\u6210\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u800c\u57fa\u4e8eIndoSafety\u7684\u5fae\u8c03\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u7814\u7a76\u5370\u5c3c\u8bed\u5883\u4e0b\u533a\u57df\u7279\u5b9aLLM\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u56e0\u6587\u5316\u591a\u6837\u6027\u5bf9\u672c\u5730\u89c4\u8303\u7684\u654f\u611f\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6269\u5c55\u73b0\u6709\u5b89\u5168\u6846\u67b6\uff0c\u6784\u5efa\u5370\u5c3c\u793e\u4f1a\u6587\u5316\u80cc\u666f\u7684\u5206\u7c7b\u6cd5\uff0c\u521b\u5efaIndoSafety\u6570\u636e\u96c6\uff0c\u5e76\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u8bc4\u4f30\u3002", "result": "\u73b0\u6709\u5370\u5c3c\u4e2d\u5fc3LLM\u5728\u53e3\u8bed\u548c\u672c\u5730\u8bed\u8a00\u73af\u5883\u4e2d\u5e38\u751f\u6210\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u5fae\u8c03\u540e\u5b89\u5168\u6027\u663e\u8457\u63d0\u5347\u4e14\u4efb\u52a1\u6027\u80fd\u4fdd\u6301\u3002", "conclusion": "\u5f3a\u8c03\u6587\u5316\u57fa\u7840\u5b89\u5168\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u8d1f\u8d23\u4efbLLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8df5\u6b65\u9aa4\u3002"}}
{"id": "2506.02547", "pdf": "https://arxiv.org/pdf/2506.02547", "abs": "https://arxiv.org/abs/2506.02547", "authors": ["Andreu Girbau-Xalabarder", "Jun Nagata", "Shinichi Sumiyoshi"], "title": "Probabilistic Online Event Downsampling", "categories": ["cs.CV", "cs.ET"], "comment": "Accepted at CVPR 2025 Event-Vision workshop", "summary": "Event cameras capture scene changes asynchronously on a per-pixel basis,\nenabling extremely high temporal resolution. However, this advantage comes at\nthe cost of high bandwidth, memory, and computational demands. To address this,\nprior work has explored event downsampling, but most approaches rely on fixed\nheuristics or threshold-based strategies, limiting their adaptability. Instead,\nwe propose a probabilistic framework, POLED, that models event importance\nthrough an event-importance probability density function (ePDF), which can be\narbitrarily defined and adapted to different applications. Our approach\noperates in a purely online setting, estimating event importance on-the-fly\nfrom raw event streams, enabling scene-specific adaptation. Additionally, we\nintroduce zero-shot event downsampling, where downsampled events must remain\nusable for models trained on the original event stream, without task-specific\nadaptation. We design a contour-preserving ePDF that prioritizes structurally\nimportant events and evaluate our method across four datasets and tasks--object\nclassification, image interpolation, surface normal estimation, and object\ndetection--demonstrating that intelligent sampling is crucial for maintaining\nperformance under event-budget constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOLED\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u91cd\u8981\u6027\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff08ePDF\uff09\u52a8\u6001\u4f30\u8ba1\u4e8b\u4ef6\u91cd\u8981\u6027\uff0c\u5b9e\u73b0\u573a\u666f\u81ea\u9002\u5e94\u7684\u5728\u7ebf\u4e8b\u4ef6\u964d\u91c7\u6837\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u5e26\u6765\u4e86\u9ad8\u5e26\u5bbd\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u73b0\u6709\u964d\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPOLED\u6846\u67b6\uff0c\u5229\u7528\u53ef\u81ea\u5b9a\u4e49\u7684ePDF\u5728\u7ebf\u4f30\u8ba1\u4e8b\u4ef6\u91cd\u8981\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4fdd\u7559\u8f6e\u5ed3\u7684ePDF\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\uff08\u5206\u7c7b\u3001\u56fe\u50cf\u63d2\u503c\u3001\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u3001\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u9a8c\u8bc1\u4e86\u667a\u80fd\u91c7\u6837\u5bf9\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "conclusion": "POLED\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u4e8b\u4ef6\u91cd\u8981\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u964d\u91c7\u6837\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.02584", "pdf": "https://arxiv.org/pdf/2506.02584", "abs": "https://arxiv.org/abs/2506.02584", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5206\u6790\u97f5\u5f8b\u7684\u58f0\u5b66\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u9884\u6d4b\u5c40\u90e8\u548c\u957f\u671f\u7ed3\u6784\uff0c\u5982\u60c5\u611f\u8bc6\u522b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7279\u5f81\u3002", "motivation": "\u63a2\u8ba8\u97f5\u5f8b\uff08\u5982\u8bed\u8c03\u3001\u8282\u594f\u548c\u54cd\u5ea6\uff09\u5728\u72ec\u7acb\u4e8e\u8bcd\u6c47\u5185\u5bb9\u65f6\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u8d21\u732e\u7a0b\u5ea6\u3002", "method": "\u63d0\u51faMasked Prosody Model\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5206\u6790\u97f5\u5f8b\u7684\u58f0\u5b66\u76f8\u5173\u6027\uff0c\u5e76\u8fdb\u884c\u611f\u77e5\u6807\u7b7e\u9884\u6d4b\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u5c40\u90e8\u548c\u957f\u671f\u7ed3\u6784\uff08\u5982\u60c5\u611f\u8bc6\u522b\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7279\u5f81\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8bad\u7ec3\u76ee\u6807\u65f6\u95f4\u5c3a\u5ea6\u5bf9\u6355\u6349\u590d\u6742\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u5176\u7f16\u7801\u7684\u7ed3\u6784\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.02550", "pdf": "https://arxiv.org/pdf/2506.02550", "abs": "https://arxiv.org/abs/2506.02550", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025", "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eEgo4D\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\uff0c\u53d6\u5f97\u4e86CVPR 2025\u6311\u6218\u8d5b\u7684\u7b2c\u4e00\u540d\u3002", "motivation": "\u53d7\u57fa\u7840\u6a21\u578b\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u89c6\u89c9\u7279\u5f81\u63d0\u53d6\uff1b2) \u4f7f\u7528Transformer\u9884\u6d4b\u52a8\u8bcd\u548c\u540d\u8bcd\uff1b3) \u901a\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u672a\u6765\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728CVPR 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u5efa\u7acb\u4e86\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u65b0\u6807\u6746\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.02589", "pdf": "https://arxiv.org/pdf/2506.02589", "abs": "https://arxiv.org/abs/2506.02589", "authors": ["Maria Levchenko"], "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "comment": null, "summary": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4fc4\u8bed\u6587\u5316\u65b0\u95fb\u4e2d\u4eba\u540d\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u7684\u6311\u6218\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u53d1\u73b0GPT-4o\u5728\u7279\u5b9a\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff08F1=0.93\uff09\uff0cGPT-4\u7cbe\u5ea6\u6700\u9ad8\uff080.99\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u5728\u4fc4\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e2d\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u89e3\u51b3\u4fc4\u8bed\u6587\u5316\u65b0\u95fb\u4e2d\u4eba\u540dNER\u7684\u6311\u6218\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f7f\u7528SPbLitGuide\u6570\u636e\u96c6\uff081999-2019\u5e74\u5723\u5f7c\u5f97\u5821\u6587\u5316\u6d3b\u52a8\u516c\u544a\uff09\uff0c\u6bd4\u8f83\u4e86DeepPavlov\u3001RoBERTa\u3001SpaCy\u7b49\u4f20\u7edf\u6a21\u578b\u4e0eGPT-3.5\u3001GPT-4\u3001GPT-4o\u7b49LLM\u7684\u8868\u73b0\u3002", "result": "GPT-4o\u5728JSON\u63d0\u793a\u4e0bF1=0.93\uff0cGPT-4\u7cbe\u5ea60.99\uff1b\u540e\u7eedGPT-4.1\uff082025\u5e74\uff09F1=0.94\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86LLM\u5728\u4fc4\u8bedNER\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662fGPT\u7cfb\u5217\u6a21\u578b\u7684\u5feb\u901f\u8fdb\u6b65\u548c\u7b80\u5316\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2506.02555", "pdf": "https://arxiv.org/pdf/2506.02555", "abs": "https://arxiv.org/abs/2506.02555", "authors": ["Zhitao Zeng", "Zhu Zhuo", "Xiaojun Jia", "Erli Zhang", "Junde Wu", "Jiaan Zhang", "Yuxuan Wang", "Chang Han Low", "Jian Jiang", "Zilong Zheng", "Xiaochun Cao", "Yutong Ban", "Qi Dou", "Yang Liu", "Yueming Jin"], "title": "SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence", "categories": ["cs.CV", "68T45", "I.2.10"], "comment": "29 pages, 5 figures", "summary": "Foundation models have achieved transformative success across biomedical\ndomains by enabling holistic understanding of multimodal data. However, their\napplication in surgery remains underexplored. Surgical intelligence presents\nunique challenges - requiring surgical visual perception, temporal analysis,\nand reasoning. Existing general-purpose vision-language models fail to address\nthese needs due to insufficient domain-specific supervision and the lack of a\nlarge-scale high-quality surgical database. To bridge this gap, we propose\nSurgVLM, one of the first large vision-language foundation models for surgical\nintelligence, where this single universal model can tackle versatile surgical\ntasks. To enable this, we construct a large-scale multimodal surgical database,\nSurgVLM-DB, comprising over 1.81 million frames with 7.79 million\nconversations, spanning more than 16 surgical types and 18 anatomical\nstructures. We unify and reorganize 23 public datasets across 10 surgical\ntasks, followed by standardizing labels and doing hierarchical vision-language\nalignment to facilitate comprehensive coverage of gradually finer-grained\nsurgical tasks, from visual perception, temporal analysis, to high-level\nreasoning. Building upon this comprehensive dataset, we propose SurgVLM, which\nis built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical\ntasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for\nmethod evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets\nin surgical domain, covering several crucial downstream tasks. Based on\nSurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:\nSurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive\ncomparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,\nQwen2.5-Max).", "AI": {"tldr": "SurgVLM\u662f\u4e00\u79cd\u9488\u5bf9\u624b\u672f\u667a\u80fd\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u591a\u6a21\u6001\u624b\u672f\u6570\u636e\u5e93SurgVLM-DB\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u624b\u672f\u9886\u57df\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u624b\u672f\u667a\u80fd\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u624b\u672f\u89c6\u89c9\u611f\u77e5\u3001\u65f6\u95f4\u5206\u6790\u548c\u63a8\u7406\uff0c\u73b0\u6709\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u76d1\u7763\u548c\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u5e93\u800c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u6784\u5efaSurgVLM-DB\u6570\u636e\u5e93\uff08180\u4e07\u5e27\u56fe\u50cf\u548c779\u4e07\u5bf9\u8bdd\uff09\uff0c\u7edf\u4e0023\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u5f00\u53d1SurgVLM\u6a21\u578b\uff08\u57fa\u4e8eQwen2.5-VL\uff09\u3002", "result": "SurgVLM\u5728SurgVLM-Bench\u4e0a\u8bc4\u4f30\uff0c\u5e76\u4e0e14\u79cd\u4e3b\u6d41\u5546\u4e1aVLM\uff08\u5982GPT-4o\u3001Gemini 2.0 Flash\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SurgVLM\u586b\u8865\u4e86\u624b\u672f\u667a\u80fd\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u4efb\u52a1\u624b\u672f\u667a\u80fd\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591", "abs": "https://arxiv.org/abs/2506.02591", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glava\u0161", "Fabian David Schmidt", "Katharina von der Wense"], "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u6d4b\u91cf\u7cfb\u7edf\uff08\u5982\u8d27\u5e01\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u9ed8\u8ba4\u4f7f\u7528\u6570\u636e\u4e2d\u5360\u4e3b\u5bfc\u7684\u6d4b\u91cf\u7cfb\u7edf\uff0c\u4e14\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u63a8\u7406\u65b9\u6cd5\u867d\u53ef\u7f13\u89e3\u95ee\u9898\u4f46\u589e\u52a0\u6210\u672c\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u80fd\u8de8\u6587\u5316\u80cc\u666f\u63d0\u4f9b\u51c6\u786e\u7684\u6d4b\u91cf\u7cfb\u7edf\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u65b0\u7f16\u8bd1\u7684\u6570\u636e\u96c6\u6d4b\u8bd5\u4e03\u79cd\u5f00\u6e90LLMs\uff0c\u7814\u7a76\u5176\u9ed8\u8ba4\u6d4b\u91cf\u7cfb\u7edf\u3001\u6027\u80fd\u5dee\u5f02\u53ca\u63a8\u7406\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "LLMs\u9ed8\u8ba4\u4f7f\u7528\u6570\u636e\u4e3b\u5bfc\u7684\u6d4b\u91cf\u7cfb\u7edf\uff0c\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u63a8\u7406\u65b9\u6cd5\u53ef\u90e8\u5206\u89e3\u51b3\u95ee\u9898\u4f46\u589e\u52a0\u6210\u672c\u3002", "conclusion": "LLMs\u5728\u8de8\u6d4b\u91cf\u7cfb\u7edf\u5e94\u7528\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u4f18\u5316\u4ee5\u652f\u6301\u591a\u6837\u5316\u7528\u6237\u3002"}}
{"id": "2506.02557", "pdf": "https://arxiv.org/pdf/2506.02557", "abs": "https://arxiv.org/abs/2506.02557", "authors": ["Shizhan Gong", "Yankai Jiang", "Qi Dou", "Farzan Farnia"], "title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Vision-language models, such as CLIP, have achieved significant success in\naligning visual and textual representations, becoming essential components of\nmany multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.\nHowever, numerous studies have identified CLIP's limited fine-grained\nperception as a critical drawback, leading to substantial failures in\ndownstream MLLMs. In contrast, vision-centric foundation models like DINOv2\ndemonstrate remarkable capabilities in capturing fine details from images. In\nthis work, we propose a novel kernel-based method to align CLIP's visual\nrepresentation with that of DINOv2, ensuring that the resulting embeddings\nmaintain compatibility with text embeddings while enhancing perceptual\ncapabilities. Our alignment objective is designed for efficient stochastic\noptimization. Following this image-only alignment fine-tuning, the visual\nencoder retains compatibility with the frozen text encoder and exhibits\nsignificant improvements in zero-shot object recognition, fine-grained spatial\nreasoning, and localization. By integrating the aligned visual encoder,\ndownstream MLLMs also demonstrate enhanced performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u65b9\u6cd5\uff0c\u5c06CLIP\u7684\u89c6\u89c9\u8868\u793a\u4e0eDINOv2\u5bf9\u9f50\uff0c\u4ee5\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u5e76\u4fdd\u6301\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u517c\u5bb9\u6027\u3002", "motivation": "CLIP\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e0b\u6e38\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6027\u80fd\uff0c\u800cDINOv2\u5728\u6355\u6349\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u91c7\u7528\u6838\u65b9\u6cd5\u5bf9\u9f50CLIP\u548cDINOv2\u7684\u89c6\u89c9\u8868\u793a\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u968f\u673a\u4f18\u5316\u76ee\u6807\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5bf9\u9f50\u540e\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5728\u96f6\u6837\u672c\u7269\u4f53\u8bc6\u522b\u3001\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u4e0b\u6e38MLLMs\u6027\u80fd\u4e5f\u5f97\u5230\u589e\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86CLIP\u548cDINOv2\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.02592", "pdf": "https://arxiv.org/pdf/2506.02592", "abs": "https://arxiv.org/abs/2506.02592", "authors": ["Zhi-Yuan Chen", "Hao Wang", "Xinyu Zhang", "Enrui Hu", "Yankai Lin"], "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5DBG\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8bc4\u5224\u8005\u65f6\u7684\u81ea\u6211\u504f\u597d\u504f\u5dee\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56e0\u54cd\u5e94\u8d28\u91cf\u6df7\u6dc6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6d4b\u91cfLLMs\u7684\u81ea\u6211\u504f\u597d\u504f\u5dee\u65f6\uff0c\u65e0\u6cd5\u533a\u5206\u504f\u5dee\u4e0e\u54cd\u5e94\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u51c6\u786e\u3002", "method": "\u5f15\u5165\u9ec4\u91d1\u8bc4\u5224\u4f5c\u4e3a\u54cd\u5e94\u8d28\u91cf\u7684\u771f\u5b9e\u6807\u51c6\uff0c\u63d0\u51faDBG\u8bc4\u5206\uff0c\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u81ea\u8bc4\u5206\u4e0e\u9ec4\u91d1\u8bc4\u5224\u7684\u5dee\u5f02\u6765\u8861\u91cf\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DBG\u8bc4\u5206\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u7248\u672c\u3001\u5927\u5c0f\u3001\u63a8\u7406\u80fd\u529b\u7b49\u56e0\u7d20\u5bf9\u81ea\u6211\u504f\u597d\u504f\u5dee\u7684\u5f71\u54cd\u3002", "conclusion": "DBG\u8bc4\u5206\u80fd\u66f4\u51c6\u786e\u5730\u6d4b\u91cf\u81ea\u6211\u504f\u597d\u504f\u5dee\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f71\u54cd\u504f\u5dee\u7684\u56e0\u7d20\u53ca\u5176\u6f5c\u5728\u673a\u5236\u3002"}}
{"id": "2506.02560", "pdf": "https://arxiv.org/pdf/2506.02560", "abs": "https://arxiv.org/abs/2506.02560", "authors": ["Zixiang Li", "Haoyu Wang", "Wei Wang", "Chuangchuang Tan", "Yunchao Wei", "Yao Zhao"], "title": "DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in image generation and\nediting tasks. Inversion within these models aims to recover the latent noise\nrepresentation for a real or generated image, enabling reconstruction, editing,\nand other downstream tasks. However, to date, most inversion approaches suffer\nfrom an intrinsic trade-off between reconstruction accuracy and editing\nflexibility. This limitation arises from the difficulty of maintaining both\nsemantic alignment and structural consistency during the inversion process. In\nthis work, we introduce Dual-Conditional Inversion (DCI), a novel framework\nthat jointly conditions on the source prompt and reference image to guide the\ninversion process. Specifically, DCI formulates the inversion process as a\ndual-condition fixed-point optimization problem, minimizing both the latent\nnoise gap and the reconstruction error under the joint guidance. This design\nanchors the inversion trajectory in both semantic and visual space, leading to\nmore accurate and editable latent representations. Our novel setup brings new\nunderstanding to the inversion process. Extensive experiments demonstrate that\nDCI achieves state-of-the-art performance across multiple editing tasks,\nsignificantly improving both reconstruction quality and editing precision.\nFurthermore, we also demonstrate that our method achieves strong results in\nreconstruction tasks, implying a degree of robustness and generalizability\napproaching the ultimate goal of the inversion process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDual-Conditional Inversion\uff08DCI\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6761\u4ef6\u4f18\u5316\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u91cd\u5efa\u7cbe\u5ea6\u4e0e\u7f16\u8f91\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7684\u53cd\u6f14\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u77db\u76fe\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "DCI\u901a\u8fc7\u53cc\u6761\u4ef6\u56fa\u5b9a\u70b9\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u6e90\u63d0\u793a\u548c\u53c2\u8003\u56fe\u50cf\u6307\u5bfc\u53cd\u6f14\u8fc7\u7a0b\uff0c\u6700\u5c0f\u5316\u6f5c\u5728\u566a\u58f0\u5dee\u8ddd\u548c\u91cd\u5efa\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDCI\u5728\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u7f16\u8f91\u7cbe\u5ea6\u3002", "conclusion": "DCI\u4e3a\u53cd\u6f14\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7406\u89e3\uff0c\u5e76\u5728\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.02596", "pdf": "https://arxiv.org/pdf/2506.02596", "abs": "https://arxiv.org/abs/2506.02596", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4f53\u88c1\u7684\u4e2d\u6587\u5199\u4f5c\u57fa\u51c6\u6d4b\u8bd5\uff08\\benchName\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56db\u79cd\u4e3b\u8981\u6587\u4f53\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86\u7ec6\u7c92\u5ea6\u7684\u8bc4\u5206\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u4e2d\u6587\u5199\u4f5c\u8bc4\u4f30\u4e2d\u5ffd\u89c6\u4e86\u7ed3\u6784\u548c\u4fee\u8f9e\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u6587\u4f53\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b728\u4e2a\u771f\u5b9e\u4e16\u754c\u63d0\u793a\u7684\u591a\u4f53\u88c1\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ec6\u7c92\u5ea6\u3001\u5206\u5c42\u6b21\u7684\u8bc4\u5206\u6846\u67b6\u3002", "result": "\u5bf915\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u6587\u4f53\u548c\u6307\u4ee4\u7c7b\u578b\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\\benchName \u65e8\u5728\u63a8\u52a8\u57fa\u4e8eLLM\u7684\u4e2d\u6587\u5199\u4f5c\u8bc4\u4f30\uff0c\u5e76\u542f\u53d1\u672a\u6765\u5728\u6559\u80b2\u573a\u666f\u4e2d\u6539\u8fdb\u6587\u7ae0\u751f\u6210\u7684\u7814\u7a76\u3002"}}
{"id": "2506.02571", "pdf": "https://arxiv.org/pdf/2506.02571", "abs": "https://arxiv.org/abs/2506.02571", "authors": ["Abhishek Vivekanandan", "Christian Hubschneider", "J. Marius Z\u00f6llner"], "title": "Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories", "categories": ["cs.CV"], "comment": "Submitted for peer review", "summary": "The ability to retrieve semantically and directionally similar short-range\ntrajectories with both accuracy and efficiency is foundational for downstream\napplications such as motion forecasting and autonomous navigation. However,\nprevailing approaches often depend on computationally intensive heuristics or\nlatent anchor representations that lack interpretability and controllability.\nIn this work, we propose a novel framework for learning fixed-dimensional\nembeddings for short trajectories by leveraging a Transformer encoder trained\nwith a contrastive triplet loss that emphasize the importance of discriminative\nfeature spaces for trajectory data. We analyze the influence of Cosine and\nFFT-based similarity metrics within the contrastive learning paradigm, with a\nfocus on capturing the nuanced directional intent that characterizes short-term\nmaneuvers. Our empirical evaluation on the Argoverse 2 dataset demonstrates\nthat embeddings shaped by Cosine similarity objectives yield superior\nclustering of trajectories by both semantic and directional attributes,\noutperforming FFT-based baselines in retrieval tasks. Notably, we show that\ncompact Transformer architectures, even with low-dimensional embeddings (e.g.,\n16 dimensions, but qualitatively down to 4), achieve a compelling balance\nbetween retrieval performance (minADE, minFDE) and computational overhead,\naligning with the growing demand for scalable and interpretable motion priors\nin real-time systems. The resulting embeddings provide a compact, semantically\nmeaningful, and efficient representation of trajectory data, offering a robust\nalternative to heuristic similarity measures and paving the way for more\ntransparent and controllable motion forecasting pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u4e09\u5143\u7ec4\u635f\u5931\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u77ed\u8f68\u8ff9\u7684\u56fa\u5b9a\u7ef4\u5ea6\u5d4c\u5165\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u542f\u53d1\u5f0f\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u6f5c\u5728\u951a\u8868\u793a\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e0b\u6e38\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528Transformer\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u4e09\u5143\u7ec4\u635f\u5931\uff0c\u7ed3\u5408Cosine\u548cFFT\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5b66\u4e60\u77ed\u8f68\u8ff9\u7684\u5d4c\u5165\u8868\u793a\u3002", "result": "\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\uff0cCosine\u76f8\u4f3c\u6027\u76ee\u6807\u5728\u8f68\u8ff9\u805a\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eFFT\u57fa\u7ebf\uff0c\u4e14\u4f4e\u7ef4\u5d4c\u5165\uff08\u598216\u7ef4\uff09\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u5f00\u9500\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u8bed\u4e49\u660e\u786e\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u8868\u793a\uff0c\u4e3a\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02627", "pdf": "https://arxiv.org/pdf/2506.02627", "abs": "https://arxiv.org/abs/2506.02627", "authors": ["\u00d6mer Tarik \u00d6zyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5fae\u8c03OpenAI\u7684Whisper\u6a21\u578b\u5bf9\u4e94\u79cd\u4e3b\u8981\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6548\u679c\uff0c\u53d1\u73b0\u5c11\u91cf\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\u5fae\u8c03\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u800c\u65b9\u8a00\u6570\u636e\u6c60\u5316\u6a21\u578b\u8868\u73b0\u4e0e\u65b9\u8a00\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u5546\u4e1a\u963f\u62c9\u4f2f\u8bedASR\u7cfb\u7edf\u5728\u5904\u7406\u65b9\u8a00\u8bed\u97f3\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u548c\u6570\u636e\u6c60\u5316\u63d0\u5347\u65b9\u8a00\u8bc6\u522b\u6548\u679c\u3002", "method": "\u4f7f\u7528Mozilla Common Voice\u7684MSA\u6570\u636e\u548cMASC\u6570\u636e\u96c6\u7684\u65b9\u8a00\u6570\u636e\uff0c\u8bc4\u4f30MSA\u5fae\u8c03\u6570\u636e\u91cf\u3001MSA\u9884\u8bad\u7ec3\u6548\u679c\u4ee5\u53ca\u65b9\u8a00\u4e13\u7528\u4e0e\u6c60\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5c11\u91cfMSA\u5fae\u8c03\u6570\u636e\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff1bMSA\u9884\u8bad\u7ec3\u6548\u679c\u6709\u9650\uff1b\u65b9\u8a00\u6c60\u5316\u6a21\u578b\u4e0e\u4e13\u7528\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u65b9\u8a00\u6570\u636e\u6c60\u5316\u5728\u5e73\u8861\u6761\u4ef6\u4e0b\u53ef\u7f13\u89e3\u4f4e\u8d44\u6e90ASR\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e14\u6027\u80fd\u635f\u5931\u8f83\u5c0f\u3002"}}
{"id": "2506.02587", "pdf": "https://arxiv.org/pdf/2506.02587", "abs": "https://arxiv.org/abs/2506.02587", "authors": ["Weiduo Yuan", "Jerry Li", "Justin Yue", "Divyank Shah", "Konstantinos Karydis", "Hang Qiu"], "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal\nperception in autonomous driving and robotic systems. Traditional calibration\nmethods require extensive data collection in controlled environments and cannot\ncompensate for the transformation changes during the vehicle/robot movement. In\nthis paper, we propose the first model that uses bird's-eye view (BEV) features\nto perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve\nthis, we extract camera BEV features and LiDAR BEV features separately and fuse\nthem into a shared BEV feature space. To fully utilize the geometric\ninformation from the BEV feature, we introduce a novel feature selector to\nfilter the most important features in the transformation decoder, which reduces\nmemory consumption and enables efficient training. Extensive evaluations on\nKITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a\nnew state of the art. Under various noise conditions, BEVCALIB outperforms the\nbest baseline in the literature by an average of (47.08%, 82.32%) on KITTI\ndataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,\nrotation), respectively. In the open-source domain, it improves the best\nreproducible baseline by one order of magnitude. Our code and demo results are\navailable at https://cisl.ucr.edu/BEVCalib.", "AI": {"tldr": "BEVCALIB\u662f\u4e00\u79cd\u5229\u7528\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7279\u5f81\u4ece\u539f\u59cb\u6570\u636e\u4e2d\u8fdb\u884cLiDAR-\u76f8\u673a\u6821\u51c6\u7684\u65b0\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfLiDAR-\u76f8\u673a\u6821\u51c6\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u53d7\u63a7\u73af\u5883\u6570\u636e\u4e14\u65e0\u6cd5\u8865\u507f\u8fd0\u52a8\u4e2d\u7684\u53d8\u6362\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u548c\u878d\u5408\u76f8\u673a\u4e0eLiDAR\u7684BEV\u7279\u5f81\u5230\u5171\u4eab\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u7279\u5f81\u9009\u62e9\u5668\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728KITTI\u548cNuScenes\u6570\u636e\u96c6\u4e0a\uff0cBEVCALIB\u5728\u5e73\u79fb\u548c\u65cb\u8f6c\u7cbe\u5ea6\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534747.08%-82.32%\u548c78.17%-68.29%\u3002", "conclusion": "BEVCALIB\u5728LiDAR-\u76f8\u673a\u6821\u51c6\u9886\u57df\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659", "abs": "https://arxiv.org/abs/2506.02659", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4e2a\u6027\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8fd0\u884c\u4e2d\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u81f4\u6027\u53d7\u89d2\u8272\u3001\u523b\u677f\u5370\u8c61\u548c\u6a21\u578b\u8bbe\u8ba1\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u4e14\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLM\u5728\u4e0d\u540c\u89d2\u8272\u548c\u4efb\u52a1\u7c7b\u578b\u4e2d\u4e00\u81f4\u6027\u7684\u5168\u9762\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u6846\u67b6\u6765\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u89d2\u8272\u7c7b\u522b\uff08\u5e78\u798f\u611f\u3001\u804c\u4e1a\u3001\u4e2a\u6027\u548c\u653f\u6cbb\u7acb\u573a\uff09\u548c\u591a\u79cd\u4efb\u52a1\u7ef4\u5ea6\uff08\u8c03\u67e5\u5199\u4f5c\u3001\u6587\u7ae0\u751f\u6210\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u751f\u6210\u3001\u5355\u8f6e\u548c\u591a\u8f6e\u5bf9\u8bdd\uff09\u8bc4\u4f30\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u81f4\u6027\u53d7\u89d2\u8272\u3001\u523b\u677f\u5370\u8c61\u548c\u6a21\u578b\u8bbe\u8ba1\u5f71\u54cd\uff0c\u4e14\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u989d\u5916\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30LLM\u89d2\u8272\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u4e00\u81f4\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.02601", "pdf": "https://arxiv.org/pdf/2506.02601", "abs": "https://arxiv.org/abs/2506.02601", "authors": ["Shiyu Shen", "Bin Pan", "Ziye Zhang", "Zhenwei Shi"], "title": "Hyperspectral Image Generation with Unmixing Guided Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recently, hyperspectral image generation has received increasing attention,\nbut existing generative models rely on conditional generation schemes, which\nlimits the diversity of generated images. Diffusion models are popular for\ntheir ability to generate high-quality samples, but adapting these models from\nRGB to hyperspectral data presents the challenge of high dimensionality and\nphysical constraints. To address these challenges, we propose a novel diffusion\nmodel guided by hyperspectral unmixing. Our model comprises two key modules: an\nunmixing autoencoder module and an abundance diffusion module. The unmixing\nautoencoder module leverages unmixing guidance to shift the generative task\nfrom the image space to the low-dimensional abundance space, significantly\nreducing computational complexity while preserving high fidelity. The abundance\ndiffusion module generates samples that satisfy the constraints of\nnon-negativity and unity, ensuring the physical consistency of the\nreconstructed HSIs. Additionally, we introduce two evaluation metrics tailored\nto hyperspectral data. Empirical results, evaluated using both traditional\nmetrics and our proposed metrics, indicate that our model is capable of\ngenerating high-quality and diverse hyperspectral images, offering an\nadvancement in hyperspectral data generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u5149\u8c31\u89e3\u6df7\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u6761\u4ef6\u751f\u6210\u65b9\u6848\uff0c\u9650\u5236\u4e86\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\uff0c\u4e14\u9ad8\u5149\u8c31\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u548c\u7269\u7406\u7ea6\u675f\u5bf9\u6269\u6563\u6a21\u578b\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u6a21\u578b\u5305\u542b\u89e3\u6df7\u81ea\u7f16\u7801\u5668\u6a21\u5757\u548c\u4e30\u5ea6\u6269\u6563\u6a21\u5757\uff0c\u524d\u8005\u5c06\u751f\u6210\u4efb\u52a1\u8f6c\u79fb\u5230\u4f4e\u7ef4\u4e30\u5ea6\u7a7a\u95f4\uff0c\u540e\u8005\u786e\u4fdd\u751f\u6210\u6837\u672c\u6ee1\u8db3\u975e\u8d1f\u6027\u548c\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u9ad8\u5149\u8c31\u6570\u636e\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u548c\u7269\u7406\u7ea6\u675f\u7684\u6311\u6218\u3002"}}
{"id": "2506.02672", "pdf": "https://arxiv.org/pdf/2506.02672", "abs": "https://arxiv.org/abs/2506.02672", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages, 24 figures", "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "AI": {"tldr": "EvaLearn\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u548c\u6548\u7387\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b648\u4e2a\u6311\u6218\u6027\u95ee\u9898\uff0c\u8981\u6c42\u6a21\u578b\u6309\u987a\u5e8f\u89e3\u51b3\u95ee\u9898\u4ee5\u5229\u7528\u7ecf\u9a8c\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6548\u7387\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1EvaLearn\u57fa\u51c6\uff0c\u5305\u542b182\u4e2a\u4efb\u52a1\u5e8f\u5217\uff0c\u4f7f\u7528\u4e94\u79cd\u81ea\u52a8\u5316\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5404\u5f02\uff0c\u90e8\u5206\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u5f3a\uff0c\u90e8\u5206\u5219\u8868\u73b0\u4e0d\u4f73\uff1b\u9759\u6001\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5728\u5b66\u4e60\u80fd\u529b\u4e0a\u672a\u5fc5\u5360\u4f18\u3002", "conclusion": "EvaLearn\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6f5c\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u80fd\u529b\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u66f4\u6df1\u5165\u7684\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2506.02604", "pdf": "https://arxiv.org/pdf/2506.02604", "abs": "https://arxiv.org/abs/2506.02604", "authors": ["Tian Chunwei", "Song Mingjian", "Zuo Wangmeng", "Du Bo", "Zhang Yanning", "Zhang Shichao"], "title": "Application of convolutional neural networks in image super-resolution", "categories": ["cs.CV", "eess.IV"], "comment": "It has been accepted by CAAI transactions on intelligent systems, in\n  Chinese language", "summary": "Due to strong learning abilities of convolutional neural networks (CNNs),\nthey have become mainstream methods for image super-resolution. However, there\nare big differences of different deep learning methods with different types.\nThere is little literature to summarize relations and differences of different\nmethods in image super-resolution. Thus, summarizing these literatures are\nimportant, according to loading capacity and execution speed of devices. This\npaper first introduces principles of CNNs in image super-resolution, then\nintroduces CNNs based bicubic interpolation, nearest neighbor interpolation,\nbilinear interpolation, transposed convolution, sub-pixel layer, meta\nup-sampling for image super-resolution to analyze differences and relations of\ndifferent CNNs based interpolations and modules, and compare performance of\nthese methods by experiments. Finally, this paper gives potential research\npoints and drawbacks and summarizes the whole paper, which can facilitate\ndevelopments of CNNs in image super-resolution.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u4e0d\u540c\u65b9\u6cd5\u548c\u6a21\u5757\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u5dee\u5f02\u4e0e\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u6027\u80fd\uff0c\u6700\u540e\u6307\u51fa\u4e86\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\u548c\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8eCNN\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5f3a\u5927\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u65b9\u6cd5\u5dee\u5f02\u8f83\u5927\u4e14\u7f3a\u4e4f\u603b\u7ed3\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u68b3\u7406\u3002", "method": "\u4ecb\u7ecd\u4e86CNN\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u539f\u7406\uff0c\u5e76\u5206\u6790\u4e86\u57fa\u4e8e\u53cc\u4e09\u6b21\u63d2\u503c\u3001\u6700\u8fd1\u90bb\u63d2\u503c\u3001\u53cc\u7ebf\u6027\u63d2\u503c\u3001\u8f6c\u7f6e\u5377\u79ef\u3001\u5b50\u50cf\u7d20\u5c42\u548c\u5143\u4e0a\u91c7\u6837\u7b49\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4e0d\u540cCNN\u63d2\u503c\u548c\u6a21\u5757\u7684\u6027\u80fd\u3002", "conclusion": "\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u6f5c\u5728\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u4fc3\u8fdbCNN\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.02678", "pdf": "https://arxiv.org/pdf/2506.02678", "abs": "https://arxiv.org/abs/2506.02678", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6bd4\u4f8b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u6807\u8bb0\u6570\u91cf40%\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8f93\u51fa\u63a8\u7406\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u590d\u6742\u6570\u636e\u6807\u6ce8\u6216\u591a\u6a21\u578b\u63d2\u503c\u3002", "method": "\u52a8\u6001\u5e73\u8861System-1\u548cSystem-2\u6570\u636e\u7684\u6743\u91cd\uff0c\u6d88\u9664\u5197\u4f59\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728DeepSeek-R1-Distill-7B\u548c14B\u4e0a\u9a8c\u8bc1\uff0c\u8f93\u51fa\u6807\u8bb0\u51cf\u5c1140%\uff0c\u63a8\u7406\u51c6\u786e\u6027\u4e0d\u53d8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.02605", "pdf": "https://arxiv.org/pdf/2506.02605", "abs": "https://arxiv.org/abs/2506.02605", "authors": ["Xue Wu", "Jingwei Xin", "Zhijun Tu", "Jie Hu", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based models have been widely used in various visual generation\ntasks, showing promising results in image super-resolution (SR), while\ntypically being limited by dozens or even hundreds of sampling steps. Although\nexisting methods aim to accelerate the inference speed of multi-step\ndiffusion-based SR methods through knowledge distillation, their generated\nimages exhibit insufficient semantic alignment with real images, resulting in\nsuboptimal perceptual quality reconstruction, specifically reflected in the\nCLIPIQA score. These methods still have many challenges in perceptual quality\nand semantic fidelity. Based on the challenges, we propose VPD-SR, a novel\nvisual perception diffusion distillation framework specifically designed for\nSR, aiming to construct an effective and efficient one-step SR model.\nSpecifically, VPD-SR consists of two components: Explicit Semantic-aware\nSupervision (ESS) and High-Frequency Perception (HFP) loss. Firstly, the ESS\nleverages the powerful visual perceptual understanding capabilities of the CLIP\nmodel to extract explicit semantic supervision, thereby enhancing semantic\nconsistency. Then, Considering that high-frequency information contributes to\nthe visual perception quality of images, in addition to the vanilla\ndistillation loss, the HFP loss guides the student model to restore the missing\nhigh-frequency details in degraded images that are critical for enhancing\nperceptual quality. Lastly, we expand VPD-SR in adversarial training manner to\nfurther enhance the authenticity of the generated content. Extensive\nexperiments conducted on synthetic and real-world datasets demonstrate that the\nproposed VPD-SR achieves superior performance compared to both previous\nstate-of-the-art methods and the teacher model with just one-step sampling.", "AI": {"tldr": "VPD-SR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u611f\u77e5\u6269\u6563\u84b8\u998f\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u8bed\u4e49\u76d1\u7763\u548c\u9ad8\u9891\u611f\u77e5\u635f\u5931\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u6b65\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u56e0\u591a\u6b65\u91c7\u6837\u9650\u5236\u901f\u5ea6\uff0c\u4e14\u751f\u6210\u56fe\u50cf\u8bed\u4e49\u5bf9\u9f50\u548c\u611f\u77e5\u8d28\u91cf\u4e0d\u8db3\u3002", "method": "\u63d0\u51faVPD-SR\u6846\u67b6\uff0c\u5305\u542b\u663e\u5f0f\u8bed\u4e49\u76d1\u7763\uff08ESS\uff09\u548c\u9ad8\u9891\u611f\u77e5\u635f\u5931\uff08HFP\uff09\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u751f\u6210\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVPD-SR\u5728\u4e00\u6b65\u91c7\u6837\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "VPD-SR\u901a\u8fc7\u8bed\u4e49\u548c\u9ad8\u9891\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6548\u7387\u548c\u611f\u77e5\u8d28\u91cf\u3002"}}
{"id": "2506.02683", "pdf": "https://arxiv.org/pdf/2506.02683", "abs": "https://arxiv.org/abs/2506.02683", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in Large Language Models (LLMs), planning tasks\nstill present challenges for LLM-based agents. Existing planning methods face\ntwo key limitations: heavy constraints and cascading errors. To address these\nlimitations, we propose a novel parallel planning paradigm, which Decomposes,\nPlans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).\nSpecifically, DPPM decomposes the complex task based on constraints into\nsubtasks, generates the subplan for each subtask in parallel, and merges them\ninto a global plan. In addition, our approach incorporates a verification and\nrefinement module, enabling error correction and conflict resolution.\nExperimental results demonstrate that DPPM significantly outperforms existing\nmethods in travel planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u89c4\u5212\u8303\u5f0fDPPM\uff0c\u901a\u8fc7\u5206\u89e3\u3001\u5e76\u884c\u89c4\u5212\u548c\u5408\u5e76\u5b50\u8ba1\u5212\u6765\u89e3\u51b3LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u4e25\u683c\u7ea6\u675f\u548c\u7ea7\u8054\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "DPPM\u8303\u5f0f\uff1a\u5206\u89e3\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u884c\u751f\u6210\u5b50\u8ba1\u5212\uff0c\u5408\u5e76\u4e3a\u5168\u5c40\u8ba1\u5212\uff0c\u5e76\u52a0\u5165\u9a8c\u8bc1\u548c\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\uff0cDPPM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DPPM\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.02614", "pdf": "https://arxiv.org/pdf/2506.02614", "abs": "https://arxiv.org/abs/2506.02614", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7a7a\u95f4\u788e\u7247\u8ddf\u8e2a\u7f51\u7edc\uff08SDT-Net\uff09\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u7279\u5f81\u8868\u793a\u63d0\u5347\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6SDTD\u9a8c\u8bc1\u6a21\u578b\u6548\u679c\u3002", "motivation": "\u7a7a\u95f4\u788e\u7247\u7684\u5feb\u901f\u589e\u52a0\u5bf9\u5b9e\u65f6\u51c6\u786e\u8ddf\u8e2a\u63d0\u51fa\u4e86\u9700\u6c42\uff0c\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u80cc\u666f\u548c\u9ad8\u5bc6\u5ea6\u788e\u7247\u3002", "method": "\u63d0\u51faSDT-Net\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u89c2\u6d4b\u6a21\u62df\u65b9\u6848\u6784\u5efaSDTD\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9ad8\u6548\u6027\uff0c\u5e76\u5728\u5357\u6781\u7ad9\u771f\u5b9e\u6570\u636e\u4e0a\u53d6\u5f9770.6%\u7684MOTA\u5206\u6570\u3002", "conclusion": "SDT-Net\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u8fc1\u79fb\u80fd\u529b\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689", "abs": "https://arxiv.org/abs/2506.02689", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMASTER\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u589e\u5f3a\u6307\u4ee4\u5fae\u8c03\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\u83b7\u53d6\u56f0\u96be\u4e14\u6210\u672c\u9ad8\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faMASTER\u65b9\u6cd5\uff0c\u6a21\u62df\u6559\u5b66\u573a\u666f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "result": "\u6784\u5efaBOOST-QA\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MASTER\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2506.02615", "pdf": "https://arxiv.org/pdf/2506.02615", "abs": "https://arxiv.org/abs/2506.02615", "authors": ["Safaa Abdullahi Moallim Mohamud", "Minjin Baek", "Dong Seog Han"], "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we present a hierarchical question-answering (QA) approach for\nscene understanding in autonomous vehicles, balancing cost-efficiency with\ndetailed visual interpretation. The method fine-tunes a compact vision-language\nmodel (VLM) on a custom dataset specific to the geographical area in which the\nvehicle operates to capture key driving-related visual elements. At the\ninference stage, the hierarchical QA strategy decomposes the scene\nunderstanding task into high-level and detailed sub-questions. Instead of\ngenerating lengthy descriptions, the VLM navigates a structured question tree,\nwhere answering high-level questions (e.g., \"Is it possible for the ego vehicle\nto turn left at the intersection?\") triggers more detailed sub-questions (e.g.,\n\"Is there a vehicle approaching the intersection from the opposite\ndirection?\"). To optimize inference time, questions are dynamically skipped\nbased on previous answers, minimizing computational overhead. The extracted\nanswers are then synthesized using handcrafted templates to ensure coherent,\ncontextually accurate scene descriptions. We evaluate the proposed approach on\nthe custom dataset using GPT reference-free scoring, demonstrating its\ncompetitiveness with state-of-the-art methods like GPT-4o in capturing key\nscene details while achieving significantly lower inference time. Moreover,\nqualitative results from real-time deployment highlight the proposed approach's\ncapacity to capture key driving elements with minimal latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u95ee\u7b54\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u573a\u666f\u7406\u89e3\uff0c\u5e73\u8861\u6210\u672c\u6548\u7387\u4e0e\u8be6\u7ec6\u89c6\u89c9\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9ad8\u6548\u4e14\u8be6\u7ec6\u7406\u89e3\u573a\u666f\u7684\u9700\u6c42\uff0c\u907f\u514d\u751f\u6210\u5197\u957f\u63cf\u8ff0\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u6570\u636e\u96c6\u5fae\u8c03\u7d27\u51d1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u95ee\u7b54\u7b56\u7565\u5206\u89e3\u4efb\u52a1\uff0c\u52a8\u6001\u8df3\u8fc7\u95ee\u9898\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0eGPT-4o\u7b49\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u6355\u6349\u5173\u952e\u9a7e\u9a76\u5143\u7d20\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701", "abs": "https://arxiv.org/abs/2506.02701", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "title": "On Entity Identification in Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5185\u90e8\u8868\u793a\u5982\u4f55\u8bc6\u522b\u548c\u533a\u5206\u547d\u540d\u5b9e\u4f53\u7684\u63d0\u53ca\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u8d28\u91cf\u5ea6\u91cf\u7684\u6846\u67b6\uff0c\u5e76\u5206\u6790\u4e86\u5b9e\u4f53\u8868\u793a\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u5982\u4f55\u6355\u6349\u547d\u540d\u5b9e\u4f53\u7684\u63d0\u53ca\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u8868\u793a\u5982\u4f55\u533a\u5206\u4e0d\u540c\u5b9e\u4f53\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u5185\u90e8\u5bf9\u5b9e\u4f53\u4fe1\u606f\u7684\u7ec4\u7ec7\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8868\u793a\u4e2d\u5b9e\u4f53\u63d0\u53ca\u7684\u805a\u96c6\u548c\u5206\u79bb\u7a0b\u5ea6\uff0c\u5e76\u5206\u6790\u5b9e\u4f53\u4fe1\u606f\u7684\u4f4e\u7ef4\u7ebf\u6027\u5b50\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u548c\u533a\u5206\u5b9e\u4f53\uff08\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u57280.66\u52300.9\u4e4b\u95f4\uff09\uff0c\u4e14\u5b9e\u4f53\u4fe1\u606f\u5728\u65e9\u671f\u5c42\u4e2d\u4ee5\u4f4e\u7ef4\u7ebf\u6027\u5b50\u7a7a\u95f4\u7d27\u51d1\u8868\u793a\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e0e\u5b9e\u4f53\u4e2d\u5fc3\u77e5\u8bc6\u7ed3\u6784\u5b58\u5728\u540c\u6784\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u7ec4\u7ec7\u548c\u5229\u7528\u5b9e\u4f53\u4fe1\u606f\u3002"}}
{"id": "2506.02626", "pdf": "https://arxiv.org/pdf/2506.02626", "abs": "https://arxiv.org/abs/2506.02626", "authors": ["Ada Sawilska", "Mateusz Trokielewicz"], "title": "Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive overview of iris image synthesis methods,\nwhich can alleviate the issues associated with gathering large, diverse\ndatasets of biometric data from living individuals, which are considered\npivotal for biometric methods development. These methods for synthesizing iris\ndata range from traditional, hand crafted image processing-based techniques,\nthrough various iterations of GAN-based image generators, variational\nautoencoders (VAEs), as well as diffusion models. The potential and fidelity in\niris image generation of each method is discussed and examples of inferred\npredictions are provided. Furthermore, the risks of individual biometric\nfeatures leakage from the training sets are considered, together with possible\nstrategies for preventing them, which have to be implemented should these\ngenerative methods be considered a valid replacement of real-world biometric\ndatasets.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8679\u819c\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4ece\u6d3b\u4f53\u4e2a\u4f53\u6536\u96c6\u5927\u89c4\u6a21\u591a\u6837\u5316\u751f\u7269\u7279\u5f81\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u7684\u6f5c\u529b\u4e0e\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u4ece\u6d3b\u4f53\u4e2a\u4f53\u6536\u96c6\u5927\u89c4\u6a21\u591a\u6837\u5316\u751f\u7269\u7279\u5f81\u6570\u636e\u7684\u6311\u6218\uff0c\u4e3a\u751f\u7269\u7279\u5f81\u65b9\u6cd5\u5f00\u53d1\u63d0\u4f9b\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7efc\u8ff0\u4e86\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u6280\u672f\u3001GAN\u3001VAE\u548c\u6269\u6563\u6a21\u578b\u7b49\u8679\u819c\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u751f\u6210\u6f5c\u529b\u548c\u4fdd\u771f\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u9884\u6d4b\u793a\u4f8b\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u751f\u7269\u7279\u5f81\u6cc4\u6f0f\u98ce\u9669\u53ca\u9884\u9632\u7b56\u7565\uff0c\u5f3a\u8c03\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u9a8c\u8bc1\u751f\u6210\u65b9\u6cd5\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u96c6\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.02726", "pdf": "https://arxiv.org/pdf/2506.02726", "abs": "https://arxiv.org/abs/2506.02726", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "AI": {"tldr": "RACE-Align\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u548c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u51c6\u786e\u6027\u3001\u77e5\u8bc6\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u4e2d\u51c6\u786e\u6027\u3001\u9886\u57df\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5ffd\u7565\u77e5\u8bc6\u6765\u6e90\u548c\u63a8\u7406\u903b\u8f91\u3002", "method": "\u63d0\u51faRACE-Align\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u5916\u90e8\u77e5\u8bc6\u548c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u504f\u597d\u6570\u636e\u96c6\uff0c\u4f7f\u7528DPO\u7b97\u6cd5\u5bf9\u9f50\u6a21\u578b\u3002", "result": "\u5728\u4f20\u7edf\u4e2d\u533b\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0cRACE-Align\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u4ec5\u4f7f\u7528SFT\u7684\u6a21\u578b\uff0c\u63d0\u5347\u51c6\u786e\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RACE-Align\u4e3a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5782\u76f4\u9886\u57df\u7684\u77e5\u8bc6\u5e94\u7528\u3001\u63a8\u7406\u53ef\u9760\u6027\u548c\u8fc7\u7a0b\u900f\u660e\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2506.02633", "pdf": "https://arxiv.org/pdf/2506.02633", "abs": "https://arxiv.org/abs/2506.02633", "authors": ["Cheng Yang", "Lijing Liang", "Zhixun Su"], "title": "ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes ControlMambaIR, a novel image restoration method designed\nto address perceptual challenges in image deraining, deblurring, and denoising\ntasks. By integrating the Mamba network architecture with the diffusion model,\nthe condition network achieves refined conditional control, thereby enhancing\nthe control and optimization of the image generation process. To evaluate the\nrobustness and generalization capability of our method across various image\ndegradation conditions, extensive experiments were conducted on several\nbenchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results\ndemonstrate that our proposed approach consistently surpasses existing methods\nin perceptual quality metrics, such as LPIPS and FID, while maintaining\ncomparable performance in image distortion metrics, including PSNR and SSIM,\nhighlighting its effectiveness and adaptability. Notably, ablation experiments\nreveal that directly noise prediction in the diffusion process achieves better\nperformance, effectively balancing noise suppression and detail preservation.\nFurthermore, the findings indicate that the Mamba architecture is particularly\nwell-suited as a conditional control network for diffusion models,\noutperforming both CNN- and Attention-based approaches in this context.\nOverall, these results highlight the flexibility and effectiveness of\nControlMambaIR in addressing a range of image restoration perceptual\nchallenges.", "AI": {"tldr": "ControlMambaIR\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u7ed3\u5408Mamba\u7f51\u7edc\u67b6\u6784\u548c\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u56fe\u50cf\u751f\u6210\u7684\u63a7\u5236\u4e0e\u4f18\u5316\uff0c\u5728\u53bb\u96e8\u3001\u53bb\u6a21\u7cca\u548c\u53bb\u566a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u6311\u6218\uff0c\u5982\u53bb\u96e8\u3001\u53bb\u6a21\u7cca\u548c\u53bb\u566a\uff0c\u901a\u8fc7\u6539\u8fdb\u6761\u4ef6\u63a7\u5236\u4f18\u5316\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u96c6\u6210Mamba\u7f51\u7edc\u67b6\u6784\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6761\u4ef6\u7f51\u7edc\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u611f\u77e5\u8d28\u91cf\u6307\u6807\uff08LPIPS\u3001FID\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u5931\u771f\u6307\u6807\uff08PSNR\u3001SSIM\uff09\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "ControlMambaIR\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u548c\u9ad8\u6548\u6027\uff0cMamba\u67b6\u6784\u4f5c\u4e3a\u6761\u4ef6\u63a7\u5236\u7f51\u7edc\u4f18\u4e8eCNN\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002"}}
{"id": "2506.02740", "pdf": "https://arxiv.org/pdf/2506.02740", "abs": "https://arxiv.org/abs/2506.02740", "authors": ["Ama\u00e7 Herda\u011fdelen", "Marco Baroni"], "title": "Stereotypical gender actions can be extracted from Web text", "categories": ["cs.CL"], "comment": null, "summary": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study.", "AI": {"tldr": "\u7814\u7a76\u4ece\u6587\u672c\u8bed\u6599\u5e93\u548cTwitter\u4e2d\u63d0\u53d6\u6027\u522b\u7279\u5b9a\u884c\u4e3a\uff0c\u5e76\u4e0e\u4eba\u7c7b\u523b\u677f\u5370\u8c61\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86\u5229\u7528\u81ea\u7136\u6587\u672c\u589e\u5f3a\u5e38\u8bc6\u77e5\u8bc6\u5e93\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u81ea\u7136\u6587\u672c\u6570\u636e\uff08\u5c24\u5176\u662fTwitter\uff09\u6765\u8865\u5145\u5e38\u8bc6\u77e5\u8bc6\u5e93\u4e2d\u5173\u4e8e\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u4fe1\u606f\u3002", "method": "\u4f7f\u7528Open Mind Common Sense\uff08OMCS\uff09\u548cTwitter\u7528\u6237\u6027\u522b\u4fe1\u606f\uff0c\u7ed3\u5408\u8bed\u6599\u5e93\u4e2d\u7684\u4ee3\u8bcd/\u59d3\u540d\u6027\u522b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8ba1\u7b97\u884c\u4e3a\u7684\u6027\u522b\u504f\u89c1\u3002", "result": "Spearman\u76f8\u5173\u7cfb\u6570\u4e3a0.47\uff0cROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u4e3a0.76\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u81ea\u7136\u6587\u672c\uff08\u5c24\u5176\u662fTwitter\u8bed\u6599\u5e93\uff09\u8865\u5145\u5e38\u8bc6\u77e5\u8bc6\u5e93\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u662f\u53ef\u884c\u7684\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\u3002"}}
{"id": "2506.02671", "pdf": "https://arxiv.org/pdf/2506.02671", "abs": "https://arxiv.org/abs/2506.02671", "authors": ["Xiao Chen", "Jiazhen Huang", "Qinting Jiang", "Fanding Huang", "Xianghua Fu", "Jingyan Jiang", "Zhi Wang"], "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet", "categories": ["cs.CV"], "comment": null, "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.", "AI": {"tldr": "SAIL\u662f\u4e00\u79cd\u57fa\u4e8e\u9002\u914d\u5668\u7684\u9ad8\u6548\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7AdaptNet\u548c\u68af\u5ea6\u611f\u77e5\u91cd\u7f6e\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u5dee\uff0cSAIL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SAIL\u5229\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\u4e0eAdaptNet\u534f\u4f5c\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u63d2\u503c\u6743\u91cd\u751f\u6210\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u68af\u5ea6\u611f\u77e5\u91cd\u7f6e\u7b56\u7565\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "SAIL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "SAIL\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02753", "pdf": "https://arxiv.org/pdf/2506.02753", "abs": "https://arxiv.org/abs/2506.02753", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u653b\u51fb\u6027\u8bed\u8a00\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\u548c\u4e3b\u52a8\u9009\u62e9\u6837\u672c\uff0c\u5b9e\u73b0\u4e8685.42%\u7684F1\u5206\u6570\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u653b\u51fb\u6027\u3001\u66b4\u529b\u548c\u7c97\u4fd7\u8bed\u8a00\u7684\u4f20\u64ad\u52a0\u5267\uff0c\u800c\u963f\u62c9\u4f2f\u8bed\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u8bed\u8a00\u590d\u6742\u6027\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\uff0c\u5e76\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u9009\u62e9\u6837\u672c\uff0c\u540c\u65f6\u5f15\u5165\u52a0\u6743\u8868\u60c5\u5904\u7406\u3002", "result": "\u5728OSACT2022\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.42%\u7684\u5b8fF1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u6837\u672c\u9700\u6c42\u66f4\u5c11\u3002", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u7ed3\u5408\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u68c0\u6d4b\u653b\u51fb\u6027\u8bed\u8a00\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.02677", "pdf": "https://arxiv.org/pdf/2506.02677", "abs": "https://arxiv.org/abs/2506.02677", "authors": ["Jintao Tong", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a\nsource-domain dataset to unseen target-domain datasets with limited\nannotations. Current methods typically compare the distance between training\nand testing samples for mask prediction. However, we find an entanglement\nproblem exists in this widely adopted method, which tends to bind sourcedomain\npatterns together and make each of them hard to transfer. In this paper, we aim\nto address this problem for the CD-FSS task. We first find a natural\ndecomposition of the ViT structure, based on which we delve into the\nentanglement problem for an interpretation. We find the decomposed ViT\ncomponents are crossly compared between images in distance calculation, where\nthe rational comparisons are entangled with those meaningless ones by their\nequal importance, leading to the entanglement problem. Based on this\ninterpretation, we further propose to address the entanglement problem by\nlearning to weigh for all comparisons of ViT components, which learn\ndisentangled features and re-compose them for the CD-FSS task, benefiting both\nthe generalization and finetuning. Experiments show that our model outperforms\nthe state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under\n1-shot and 5-shot settings, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\uff08CD-FSS\uff09\u4e2d\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3ViT\u7ed3\u6784\u5e76\u5b66\u4e60\u6743\u91cd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dCD-FSS\u65b9\u6cd5\u5728\u8ddd\u79bb\u8ba1\u7b97\u4e2d\u5b58\u5728\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "method": "\u5206\u89e3ViT\u7ed3\u6784\uff0c\u5206\u6790\u7ea0\u7f20\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u5b66\u4e60\u6743\u91cd\u5206\u914d\u6765\u89e3\u8026\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u57281-shot\u548c5-shot\u8bbe\u7f6e\u4e0b\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u63d0\u53471.92%\u548c1.88%\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026ViT\u7279\u5f81\u5e76\u91cd\u65b0\u7ec4\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86CD-FSS\u4e2d\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.02758", "pdf": "https://arxiv.org/pdf/2506.02758", "abs": "https://arxiv.org/abs/2506.02758", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark Gales"], "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "summary": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u82f1\u8bed\u8bcd\u6c47\u8f6e\u5ed3\uff08EVP\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u8bcd\u6027\u6807\u6ce8\u65b9\u6cd5\u3002", "motivation": "\u8bcd\u6c47\u4f7f\u7528\u662f\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u719f\u7ec3\u5ea6\u7684\u6838\u5fc3\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u8bc4\u4f30\u591a\u4f9d\u8d56\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u8bcd\u6027\u6807\u6ce8\uff0c\u65e0\u6cd5\u7cbe\u786e\u8bc4\u4f30\u8bcd\u6c47\u5728\u53e5\u5b50\u4e2d\u7684\u5b9e\u9645\u4f7f\u7528\u3002", "method": "\u7ed3\u5408LLMs\u548cEVP\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u8bcd\u6c47\u4f7f\u7528\uff0c\u89e3\u51b3\u591a\u4e49\u6027\u3001\u4e0a\u4e0b\u6587\u53d8\u5316\u548c\u591a\u8bcd\u8868\u8fbe\u7b49\u6311\u6218\u3002", "result": "LLMs\u5728\u8bcd\u6c47\u719f\u7ec3\u5ea6\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u63a2\u7d22\u8bcd\u6c47\u4e0e\u6587\u7ae0\u6574\u4f53\u719f\u7ec3\u5ea6\u7684\u76f8\u5173\u6027\u3002", "conclusion": "LLMs\u9002\u5408\u8bcd\u6c47\u8bc4\u4f30\u4efb\u52a1\uff0c\u4e3aL2\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2506.02680", "pdf": "https://arxiv.org/pdf/2506.02680", "abs": "https://arxiv.org/abs/2506.02680", "authors": ["Julius Erbach", "Dominik Narnhofer", "Andreas Dombos", "Bernt Schiele", "Jan Eric Lenssen", "Konrad Schindler"], "title": "Solving Inverse Problems with FLAIR", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Flow-based latent generative models such as Stable Diffusion 3 are able to\ngenerate images with remarkable quality, even enabling photorealistic\ntext-to-image generation. Their impressive performance suggests that these\nmodels should also constitute powerful priors for inverse imaging problems, but\nthat approach has not yet led to comparable fidelity. There are several key\nobstacles: (i) the encoding into a lower-dimensional latent space makes the\nunderlying (forward) mapping non-linear; (ii) the data likelihood term is\nusually intractable; and (iii) learned generative models struggle to recover\nrare, atypical data modes during inference. We present FLAIR, a novel training\nfree variational framework that leverages flow-based generative models as a\nprior for inverse problems. To that end, we introduce a variational objective\nfor flow matching that is agnostic to the type of degradation, and combine it\nwith deterministic trajectory adjustments to recover atypical modes. To enforce\nexact consistency with the observed data, we decouple the optimization of the\ndata fidelity and regularization terms. Moreover, we introduce a time-dependent\ncalibration scheme in which the strength of the regularization is modulated\naccording to off-line accuracy estimates. Results on standard imaging\nbenchmarks demonstrate that FLAIR consistently outperforms existing diffusion-\nand flow-based methods in terms of reconstruction quality and sample diversity.", "AI": {"tldr": "FLAIR\u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u8bad\u7ec3\u53d8\u5206\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u9006\u95ee\u9898\u7684\u5148\u9a8c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion 3\uff09\u5728\u56fe\u50cf\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9006\u6210\u50cf\u95ee\u9898\u4e2d\u5c1a\u672a\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\uff0c\u4e3b\u8981\u969c\u788d\u5305\u62ec\u975e\u7ebf\u6027\u6620\u5c04\u3001\u6570\u636e\u4f3c\u7136\u9879\u96be\u89e3\u53ca\u7f55\u89c1\u6a21\u5f0f\u6062\u590d\u56f0\u96be\u3002", "method": "FLAIR\u901a\u8fc7\u53d8\u5206\u76ee\u6807\u5b9e\u73b0\u6d41\u5339\u914d\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u8f68\u8ff9\u8c03\u6574\u6062\u590d\u7f55\u89c1\u6a21\u5f0f\uff0c\u5e76\u5206\u79bb\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u6b63\u5219\u5316\u9879\u7684\u4f18\u5316\uff0c\u5f15\u5165\u65f6\u95f4\u4f9d\u8d56\u7684\u6821\u51c6\u65b9\u6848\u3002", "result": "\u5728\u6807\u51c6\u6210\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFLAIR\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6837\u672c\u591a\u6837\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u7684\u65b9\u6cd5\u3002", "conclusion": "FLAIR\u4e3a\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u7684\u6027\u80fd\u3002"}}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "AI": {"tldr": "HC-Bench\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u68c0\u6d4b\u9690\u85cf\u5185\u5bb9\uff08\u5982\u5149\u5b66\u9519\u89c9\u6216AI\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u9690\u85cf\u6587\u672c\u548c\u7269\u4f53\uff09\u65f6\u8868\u73b0\u6781\u5dee\uff080-5.36%\uff09\u3002\u901a\u8fc7SemVink\u65b9\u6cd5\uff08\u5c06\u56fe\u50cf\u7f29\u653e\u5230\u4f4e\u5206\u8fa8\u7387\uff09\uff0c\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u81f3>99%\uff0c\u63ed\u793a\u4e86VLMs\u8fc7\u5ea6\u4f9d\u8d56\u9ad8\u5c42\u8bed\u4e49\u800c\u5ffd\u89c6\u4f4e\u5c42\u89c6\u89c9\u64cd\u4f5c\u7684\u7f3a\u9677\u3002", "motivation": "\u4eba\u7c7b\u80fd\u672c\u80fd\u5730\u611f\u77e5\u9690\u85cf\u5185\u5bb9\uff0c\u800cVLMs\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u5176\u67b6\u6784\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u6539\u8fdb\u4ee5\u589e\u5f3a\u5b9e\u9645\u5e94\u7528\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faHC-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b112\u5f20\u9690\u85cf\u5185\u5bb9\u7684\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1SemVink\u65b9\u6cd5\uff08\u4f4e\u5206\u8fa8\u7387\u7f29\u653e\uff09\u4ee5\u6d88\u9664\u89c6\u89c9\u566a\u58f0\u3002", "result": "VLMs\u5728HC-Bench\u4e0a\u51c6\u786e\u7387\u6781\u4f4e\uff080-5.36%\uff09\uff0c\u800cSemVink\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u81f3>99%\u3002", "conclusion": "VLMs\u9700\u6574\u5408\u591a\u5c3a\u5ea6\u5904\u7406\u4ee5\u5f25\u8865\u4f4e\u5c42\u89c6\u89c9\u64cd\u4f5c\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u7f29\u5c0f\u8ba1\u7b97\u89c6\u89c9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5dee\u8ddd\uff0c\u9002\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u3001\u5b89\u5168\u7b49\u9886\u57df\u3002"}}
{"id": "2506.02690", "pdf": "https://arxiv.org/pdf/2506.02690", "abs": "https://arxiv.org/abs/2506.02690", "authors": ["Yurui Zhao", "Xiang Wang", "Jiahong Liu", "Irwin King", "Zhitao Huang"], "title": "Towards Geometry Problem Solving in the Large Model Era: A Survey", "categories": ["cs.CV", "math.GT"], "comment": "8pages, 4 figures, conference submission", "summary": "Geometry problem solving (GPS) represents a critical frontier in artificial\nintelligence, with profound applications in education, computer-aided design,\nand computational graphics. Despite its significance, automating GPS remains\nchallenging due to the dual demands of spatial understanding and rigorous\nlogical reasoning. Recent advances in large models have enabled notable\nbreakthroughs, particularly for SAT-level problems, yet the field remains\nfragmented across methodologies, benchmarks, and evaluation frameworks. This\nsurvey systematically synthesizes GPS advancements through three core\ndimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,\nand (3) reasoning paradigms. We further propose a unified analytical paradigm,\nassess current limitations, and identify emerging opportunities to guide future\nresearch toward human-level geometric reasoning, including automated benchmark\ngeneration and interpretable neuro-symbolic integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\uff08GPS\uff09\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8303\u5f0f\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u5728AI\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u76ee\u524d\u81ea\u52a8\u5316\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e14\u9886\u57df\u5185\u65b9\u6cd5\u3001\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u5206\u6563\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff08\u57fa\u51c6\u6784\u5efa\u3001\u6587\u672c\u4e0e\u56fe\u89e3\u89e3\u6790\u3001\u63a8\u7406\u8303\u5f0f\uff09\u7cfb\u7edf\u7efc\u8ff0GPS\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u5206\u6790\u8303\u5f0f\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524dGPS\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u81ea\u52a8\u57fa\u51c6\u751f\u6210\u548c\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u3002", "conclusion": "\u8bba\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u51e0\u4f55\u63a8\u7406\u3002"}}
{"id": "2506.02818", "pdf": "https://arxiv.org/pdf/2506.02818", "abs": "https://arxiv.org/abs/2506.02818", "authors": ["Ekaterina Grishina", "Mikhail Gorbunov", "Maxim Rakhuba"], "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL Findings", "summary": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT", "AI": {"tldr": "\u5229\u7528\u6b63\u4ea4\u53d8\u6362\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6743\u91cd\u77e9\u9635\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4ee5\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u7ed3\u6784\u5316\u77e9\u9635\u662f\u51cf\u5c11\u53c2\u6570\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u76f4\u63a5\u8868\u793a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u53ef\u80fd\u4e0d\u51c6\u786e\u3002", "method": "\u5229\u7528LLM\u8f93\u51fa\u5bf9\u67d0\u4e9b\u6b63\u4ea4\u53d8\u6362\u7684\u4e0d\u53d8\u6027\uff0c\u627e\u5230\u80fd\u663e\u8457\u63d0\u9ad8\u6743\u91cd\u77e9\u9635\u5728\u7ed3\u6784\u5316\u7c7b\u522b\u4e2d\u53ef\u538b\u7f29\u6027\u7684\u53d8\u6362\u3002", "result": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u652f\u6301\u9ad8\u6548\u6295\u5f71\u64cd\u4f5c\u7684\u7ed3\u6784\u5316\u77e9\u9635\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u3002", "conclusion": "\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u4f18\u5316\u6743\u91cd\u77e9\u9635\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4e3a\u51cf\u5c11LLM\u53c2\u6570\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.02692", "pdf": "https://arxiv.org/pdf/2506.02692", "abs": "https://arxiv.org/abs/2506.02692", "authors": ["Shu Yang", "Fengtao Zhou", "Leon Mayer", "Fuxiang Huang", "Yiliang Chen", "Yihui Wang", "Sunan He", "Yuxiang Nie", "Xi Wang", "\u00d6mer S\u00fcmer", "Yueming Jin", "Huihui Sun", "Shuchang Xu", "Alex Qinyang Liu", "Zheng Li", "Jing Qin", "Jeremy YuenChun Teoh", "Lena Maier-Hein", "Hao Chen"], "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Assisted Intervention (CAI) has the potential to revolutionize\nmodern surgery, with surgical scene understanding serving as a critical\ncomponent in supporting decision-making, improving procedural efficacy, and\nensuring intraoperative safety. While existing AI-driven approaches alleviate\nannotation burdens via self-supervised spatial representation learning, their\nlack of explicit temporal modeling during pre-training fundamentally restricts\nthe capture of dynamic surgical contexts, resulting in incomplete\nspatiotemporal understanding. In this work, we introduce the first video-level\nsurgical pre-training framework that enables joint spatiotemporal\nrepresentation learning from large-scale surgical video data. To achieve this,\nwe constructed a large-scale surgical video dataset comprising 3,650 videos and\napproximately 3.55 million frames, spanning more than 20 surgical procedures\nand over 10 anatomical structures. Building upon this dataset, we propose\nSurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a\nreconstruction-based pre-training method that captures intricate spatial\nstructures and temporal dynamics through joint spatiotemporal modeling.\nAdditionally, SurgVISTA incorporates image-level knowledge distillation guided\nby a surgery-specific expert to enhance the learning of fine-grained anatomical\nand semantic features. To validate its effectiveness, we established a\ncomprehensive benchmark comprising 13 video-level datasets spanning six\nsurgical procedures across four tasks. Extensive experiments demonstrate that\nSurgVISTA consistently outperforms both natural- and surgical-domain\npre-trained models, demonstrating strong potential to advance intelligent\nsurgical systems in clinically meaningful scenarios.", "AI": {"tldr": "SurgVISTA\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7ea7\u522b\u7684\u624b\u672f\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u65f6\u7a7a\u5efa\u6a21\u4ece\u5927\u89c4\u6a21\u624b\u672f\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u7684\u65f6\u7a7a\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u5bfc\u81f4\u5bf9\u52a8\u6001\u624b\u672f\u573a\u666f\u7684\u7406\u89e3\u4e0d\u5b8c\u6574\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\uff0c\u63d0\u51faSurgVISTA\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u7a7a\u5efa\u6a21\u548c\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u572813\u4e2a\u89c6\u9891\u7ea7\u522b\u6570\u636e\u96c6\u4e0a\uff0cSurgVISTA\u8868\u73b0\u4f18\u4e8e\u81ea\u7136\u548c\u624b\u672f\u9886\u57df\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "SurgVISTA\u6709\u671b\u63a8\u52a8\u667a\u80fd\u624b\u672f\u7cfb\u7edf\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2506.02827", "pdf": "https://arxiv.org/pdf/2506.02827", "abs": "https://arxiv.org/abs/2506.02827", "authors": ["Yulin Dou", "Jiangming Liu"], "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.", "AI": {"tldr": "TO-GATE\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u63d0\u5347LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u751f\u6210\u95ee\u9898\u7684\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u5b66\u4e60\u63a8\u7406\u7684\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u6700\u4f18\u5bf9\u8bdd\u8f68\u8ff9\u5e76\u907f\u514d\u65e0\u5173\u95ee\u9898\u3002", "method": "TO-GATE\u5305\u542b\u6f84\u6e05\u89e3\u6790\u5668\uff08\u751f\u6210\u6700\u4f18\u63d0\u95ee\u8f68\u8ff9\uff09\u548c\u603b\u7ed3\u5668\uff08\u786e\u4fdd\u4efb\u52a1\u5bf9\u9f50\u7684\u6700\u7ec8\u56de\u7b54\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTO-GATE\u5728\u504f\u597d\u8bf1\u5bfc\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53479.32%\u3002", "conclusion": "TO-GATE\u901a\u8fc7\u4f18\u5316\u5bf9\u8bdd\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4efb\u52a1\u5bf9\u9f50\u95ee\u9898\u751f\u6210\u548c\u56de\u7b54\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.02695", "pdf": "https://arxiv.org/pdf/2506.02695", "abs": "https://arxiv.org/abs/2506.02695", "authors": ["Linquan Wu", "Tianxiang Jiang", "Wenhao Duan", "Yini Fang", "Jacky Keung"], "title": "FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Micro-expression recognition (MER) demands models that can amplify\nmillisecond-level, low-amplitude facial motions while suppressing\nidentity-specific appearance. We introduce FaceSleuth, a dual-stream\narchitecture that (1) enhances motion along the empirically dominant vertical\naxix through a Continuously Vertical Attention (CVA) block, (2) localises the\nresulting signals with a Facial Position Focalizer built on hierarchical\ncross-window attention, and (3) steers feature learning toward physiologically\nmeaningful regions via lightweight Action-Unit embeddings. To examine whether\nthe hand-chosen vertical axis is indeed optimal, we further propose a\nSingle-Orientation Attention (SOA) module that learns its own pooling direction\nend-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses\nto CVA when the learned angle converges to {\\Pi}/2. In practice, SOA reliably\ndrifts to 88{\\deg}, confirming the effectiveness of the vertical prior while\ndelivering consistent gains. On three standard MER benchmarks, FaceSleuth with\nCVA already surpasses previous state-of-the-art methods; plugging in SOA lifts\naccuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840\non SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.\nThese results establish a new state of the art and, for the first time, provide\nempirical evidence that the vertical attention bias is the most discriminative\norientation for MER.", "AI": {"tldr": "FaceSleuth\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u5782\u76f4\u6ce8\u610f\u529b\u589e\u5f3a\u5fae\u8868\u60c5\u8bc6\u522b\uff0c\u5e76\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u5355\u65b9\u5411\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\u9700\u8981\u653e\u5927\u6beb\u79d2\u7ea7\u3001\u4f4e\u5e45\u5ea6\u7684\u9762\u90e8\u8fd0\u52a8\uff0c\u540c\u65f6\u6291\u5236\u8eab\u4efd\u7279\u5f81\u3002", "method": "FaceSleuth\u91c7\u7528\u53cc\u6d41\u67b6\u6784\uff0c\u5305\u62ec\u5782\u76f4\u6ce8\u610f\u529b\u5757\u3001\u9762\u90e8\u4f4d\u7f6e\u805a\u7126\u5668\u548c\u52a8\u4f5c\u5355\u5143\u5d4c\u5165\uff0c\u5e76\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u5355\u65b9\u5411\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFaceSleuth\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe95.1%\u3002", "conclusion": "\u5782\u76f4\u6ce8\u610f\u529b\u504f\u7f6e\u662f\u5fae\u8868\u60c5\u8bc6\u522b\u4e2d\u6700\u5177\u533a\u5206\u6027\u7684\u65b9\u5411\uff0cFaceSleuth\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2506.02872", "pdf": "https://arxiv.org/pdf/2506.02872", "abs": "https://arxiv.org/abs/2506.02872", "authors": ["Ludovic Moncla", "H\u00e9di Zeghidi"], "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u65b9\u6cd5\u5728\u5386\u53f2\u6587\u672c\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4f20\u7edf\u65b9\u6cd5\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u5d4c\u5957\u5b9e\u4f53\u7684\u5206\u7c7b\u6846\u67b6\u3002", "motivation": "\u5386\u53f2\u6587\u672c\u4e2d\u7684NER\u9762\u4e34\u8bed\u8a00\u975e\u6807\u51c6\u5316\u3001\u62fc\u5199\u53e4\u8001\u53ca\u5b9e\u4f53\u5d4c\u5957\u7b49\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5728GeoEDdA\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86CRF\u3001spaCy\u3001CamemBERT\u548cFlair\u7b49\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8etoken\u548cspan\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u751f\u6210\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002", "result": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5d4c\u5957\u5b9e\u4f53\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u5386\u53f2NER\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u7b26\u53f7\u4e0e\u795e\u7ecf\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u5904\u7406\u65e9\u671f\u73b0\u4ee3\u6cd5\u8bed\u6587\u672c\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2506.02697", "pdf": "https://arxiv.org/pdf/2506.02697", "abs": "https://arxiv.org/abs/2506.02697", "authors": ["Yuxuan Wu", "Le Wang", "Sanping Zhou", "Mengnan Liu", "Gang Hua", "Haoxiang Li"], "title": "LayoutRAG: Retrieval-Augmented Model for Content-agnostic Conditional Layout Generation", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Controllable layout generation aims to create plausible visual arrangements\nof element bounding boxes within a graphic design according to certain optional\nconstraints, such as the type or position of a specific component. While recent\ndiffusion or flow-matching models have achieved considerable advances in\nmultifarious conditional generation tasks, there remains considerable room for\ngenerating optimal arrangements under given conditions. In this work, we\npropose to carry out layout generation through retrieving by conditions and\nreference-guided generation. Specifically, we retrieve appropriate layout\ntemplates according to given conditions as references. The references are then\nutilized to guide the denoising or flow-based transport process. By retrieving\nlayouts compatible with the given conditions, we can uncover the potential\ninformation not explicitly provided in the given condition. Such an approach\noffers more effective guidance to the model during the generation process, in\ncontrast to previous models that feed the condition to the model and let the\nmodel infer the unprovided layout attributes directly. Meanwhile, we design a\ncondition-modulated attention that selectively absorbs retrieval knowledge,\nadapting to the difference between retrieved templates and given conditions.\nExtensive experiment results show that our method successfully produces\nhigh-quality layouts that meet the given conditions and outperforms existing\nstate-of-the-art models. Code will be released upon acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u548c\u53c2\u8003\u5f15\u5bfc\u7684\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u68c0\u7d22\u6a21\u677f\u5e76\u5229\u7528\u53c2\u8003\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6216\u6d41\u5339\u914d\u6a21\u578b\u5728\u5e03\u5c40\u751f\u6210\u4e2d\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u4e0b\u751f\u6210\u6700\u4f18\u5e03\u5c40\u3002", "method": "\u901a\u8fc7\u6761\u4ef6\u68c0\u7d22\u5408\u9002\u7684\u5e03\u5c40\u6a21\u677f\u4f5c\u4e3a\u53c2\u8003\uff0c\u5e76\u8bbe\u8ba1\u6761\u4ef6\u8c03\u5236\u6ce8\u610f\u529b\u673a\u5236\u9009\u62e9\u6027\u5438\u6536\u68c0\u7d22\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u7ed9\u5b9a\u6761\u4ef6\u7684\u5e03\u5c40\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u68c0\u7d22\u548c\u53c2\u8003\u5f15\u5bfc\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5e03\u5c40\u751f\u6210\u7684\u8d28\u91cf\u548c\u6761\u4ef6\u6ee1\u8db3\u80fd\u529b\u3002"}}
{"id": "2506.02878", "pdf": "https://arxiv.org/pdf/2506.02878", "abs": "https://arxiv.org/abs/2506.02878", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.", "AI": {"tldr": "\u8bba\u6587\u53cd\u9a73\u4e86Chain-of-Thought (CoT)\u63d0\u793a\u5f15\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u89c2\u70b9\uff0c\u8ba4\u4e3aCoT\u4ec5\u662f\u4e00\u79cd\u7ed3\u6784\u7ea6\u675f\uff0c\u5f15\u5bfc\u6a21\u578b\u6a21\u4eff\u63a8\u7406\u5f62\u5f0f\u3002", "motivation": "\u63a2\u8ba8CoT\u63d0\u793a\u662f\u5426\u771f\u6b63\u5f15\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u6a21\u4eff\u63a8\u7406\u5f62\u5f0f\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51faCoT\u63d0\u793a\u901a\u8fc7\u5f3a\u5236\u751f\u6210\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5229\u7528\u6a21\u578b\u7684\u5e8f\u5217\u9884\u6d4b\u548c\u6a21\u5f0f\u5339\u914d\u80fd\u529b\uff0c\u800c\u975e\u771f\u6b63\u63a8\u7406\u3002", "result": "CoT\u63d0\u793a\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u4f7f\u6a21\u578b\u8f93\u51fa\u7c7b\u4f3c\u8fde\u8d2f\u601d\u7ef4\u8fc7\u7a0b\u7684\u5e8f\u5217\uff0c\u800c\u975e\u5b9e\u73b0\u771f\u6b63\u7684\u63a8\u7406\u3002", "conclusion": "CoT\u63d0\u793a\u5e76\u672a\u5f15\u53d1\u771f\u6b63\u7684\u62bd\u8c61\u63a8\u7406\uff0c\u800c\u662f\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u6a21\u4eff\u63a8\u7406\u5f62\u5f0f\u3002"}}
{"id": "2506.02698", "pdf": "https://arxiv.org/pdf/2506.02698", "abs": "https://arxiv.org/abs/2506.02698", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xiaoyin Xu", "Min Zhang"], "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation\nmodels with human preferences using pairwise preference data. Although\nsubstantial resources are expended in collecting and labeling datasets, a\ncritical aspect is often neglected: \\textit{preferences vary across individuals\nand should be represented with more granularity.} To address this, we propose\nSmPO-Diffusion, a novel method for modeling preference distributions to improve\nthe DPO objective, along with a numerical upper bound estimation for the\ndiffusion optimization objective. First, we introduce a smoothed preference\ndistribution to replace the original binary distribution. We employ a reward\nmodel to simulate human preferences and apply preference likelihood averaging\nto improve the DPO loss, such that the loss function approaches zero when\npreferences are similar. Furthermore, we utilize an inversion technique to\nsimulate the trajectory preference distribution of the diffusion model,\nenabling more accurate alignment with the optimization objective. Our approach\neffectively mitigates issues of excessive optimization and objective\nmisalignment present in existing methods through straightforward modifications.\nOur SmPO-Diffusion achieves state-of-the-art performance in preference\nevaluation, outperforming baselines across metrics with lower training costs.\nThe project page is https://jaydenlyh.github.io/SmPO-project-page/.", "AI": {"tldr": "SmPO-Diffusion\u6539\u8fdbDPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u504f\u597d\u5206\u5e03\u548c\u4f18\u5316\u76ee\u6807\u4e0a\u754c\u4f30\u8ba1\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e2a\u4f53\u504f\u597d\u7684\u5dee\u5f02\u6027\uff0c\u9700\u66f4\u7ec6\u7c92\u5ea6\u8868\u793a\u3002", "method": "\u5f15\u5165\u5e73\u6ed1\u504f\u597d\u5206\u5e03\u66ff\u4ee3\u4e8c\u5143\u5206\u5e03\uff0c\u7ed3\u5408\u5956\u52b1\u6a21\u578b\u548c\u504f\u597d\u4f3c\u7136\u5e73\u5747\u4f18\u5316DPO\u635f\u5931\uff1b\u5229\u7528\u53cd\u8f6c\u6280\u672f\u6a21\u62df\u6269\u6563\u6a21\u578b\u8f68\u8ff9\u504f\u597d\u5206\u5e03\u3002", "result": "SmPO-Diffusion\u5728\u504f\u597d\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u8bad\u7ec3\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u4fee\u6539\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u5ea6\u4f18\u5316\u548c\u76ee\u6807\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\u3002"}}
{"id": "2506.02894", "pdf": "https://arxiv.org/pdf/2506.02894", "abs": "https://arxiv.org/abs/2506.02894", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin F\u00f6rster", "Gabriele Wenger-Glemser", "Barbara Plank"], "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Betthupferl\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30ASR\u6a21\u578b\u5bf9\u5fb7\u56fd\u4e1c\u5357\u90e8\u65b9\u8a00\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5206\u6790\u4e86\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u7684\u5dee\u5f02\u3002", "motivation": "\u5fb7\u56fd\u65b9\u8a00\u5728\u5f53\u524dASR\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5bf9\u65b9\u8a00\u53d8\u5f02\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b\u56db\u79cd\u65b9\u8a00\u548c\u6807\u51c6\u5fb7\u8bed\u7684Betthupferl\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u591a\u8bed\u8a00ASR\u6a21\u578b\u7684\u7ffb\u8bd1\u8868\u73b0\u3002", "result": "ASR\u6a21\u578b\u8f93\u51fa\u4ecb\u4e8e\u65b9\u8a00\u548c\u6807\u51c6\u5fb7\u8bed\u4e4b\u95f4\uff0c\u6709\u65f6\u4f1a\u6807\u51c6\u5316\u8bed\u6cd5\u5dee\u5f02\uff0c\u4f46\u66f4\u63a5\u8fd1\u65b9\u8a00\u7ed3\u6784\u3002", "conclusion": "Betthupferl\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u65b9\u8a00ASR\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6a21\u578b\u8868\u73b0\u663e\u793a\u5176\u5bf9\u8bed\u6cd5\u5dee\u5f02\u7684\u5904\u7406\u80fd\u529b\u6709\u9650\u3002"}}
{"id": "2506.02702", "pdf": "https://arxiv.org/pdf/2506.02702", "abs": "https://arxiv.org/abs/2506.02702", "authors": ["Tibor Kub\u00edk", "Fran\u00e7ois Guibault", "Michal \u0160pan\u011bl", "Herv\u00e9 Lombaert"], "title": "ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings", "categories": ["cs.CV"], "comment": "Information Processing in Medical Imaging (IPMI2025)", "summary": "We introduce ToothForge, a spectral approach for automatically generating\nnovel 3D teeth, effectively addressing the sparsity of dental shape datasets.\nBy operating in the spectral domain, our method enables compact machine\nlearning modeling, allowing the generation of high-resolution tooth meshes in\nmilliseconds. However, generating shape spectra comes with the instability of\nthe decomposed harmonics. To address this, we propose modeling the latent\nmanifold on synchronized frequential embeddings. Spectra of all data samples\nare aligned to a common basis prior to the training procedure, effectively\neliminating biases introduced by the decomposition instability. Furthermore,\nsynchronized modeling removes the limiting factor imposed by previous methods,\nwhich require all shapes to share a common fixed connectivity. Using a private\ndataset of real dental crowns, we observe a greater reconstruction quality of\nthe synthetized shapes, exceeding those of models trained on unaligned\nembeddings. We also explore additional applications of spectral analysis in\ndigital dentistry, such as shape compression and interpolation. ToothForge\nfacilitates a range of approaches at the intersection of spectral analysis and\nmachine learning, with fewer restrictions on mesh structure. This makes it\napplicable for shape analysis not only in dentistry, but also in broader\nmedical applications, where guaranteeing consistent connectivity across shapes\nfrom various clinics is unrealistic. The code is available at\nhttps://github.com/tiborkubik/toothForge.", "AI": {"tldr": "ToothForge\u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u65b0\u9896\u76843D\u7259\u9f7f\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u7259\u79d1\u5f62\u72b6\u6570\u636e\u96c6\u7684\u7a00\u758f\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5728\u9891\u8c31\u57df\u64cd\u4f5c\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u5efa\u6a21\uff0c\u5e76\u80fd\u5feb\u901f\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7259\u9f7f\u7f51\u683c\u3002", "motivation": "\u7259\u79d1\u5f62\u72b6\u6570\u636e\u96c6\u7684\u7a00\u758f\u6027\u9650\u5236\u4e863D\u7259\u9f7f\u6a21\u578b\u7684\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u5206\u89e3\u8c10\u6ce2\u7684\u4e0d\u7a33\u5b9a\u6027\u800c\u53d7\u9650\uff0c\u4e14\u8981\u6c42\u6240\u6709\u5f62\u72b6\u5171\u4eab\u56fa\u5b9a\u7684\u8fde\u63a5\u6027\u3002", "method": "\u63d0\u51fa\u5728\u540c\u6b65\u9891\u7387\u5d4c\u5165\u4e0a\u5efa\u6a21\u6f5c\u5728\u6d41\u5f62\uff0c\u5c06\u6240\u6709\u6570\u636e\u6837\u672c\u7684\u9891\u8c31\u5bf9\u9f50\u5230\u5171\u540c\u57fa\u4e0a\uff0c\u6d88\u9664\u5206\u89e3\u4e0d\u7a33\u5b9a\u6027\u5f15\u5165\u7684\u504f\u5dee\u3002", "result": "\u5728\u771f\u5b9e\u7259\u51a0\u6570\u636e\u96c6\u4e0a\uff0c\u751f\u6210\u5f62\u72b6\u7684\u91cd\u5efa\u8d28\u91cf\u4f18\u4e8e\u672a\u5bf9\u9f50\u5d4c\u5165\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u9891\u8c31\u5206\u6790\u8fd8\u53ef\u7528\u4e8e\u5f62\u72b6\u538b\u7f29\u548c\u63d2\u503c\u3002", "conclusion": "ToothForge\u7ed3\u5408\u9891\u8c31\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u7259\u79d1\u53ca\u5176\u4ed6\u533b\u5b66\u9886\u57df\u7684\u5f62\u72b6\u5206\u6790\uff0c\u7a81\u7834\u4e86\u7f51\u683c\u7ed3\u6784\u7684\u9650\u5236\u3002"}}
{"id": "2506.02899", "pdf": "https://arxiv.org/pdf/2506.02899", "abs": "https://arxiv.org/abs/2506.02899", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "AI": {"tldr": "IMPARA-GED\u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u53c2\u8003\u81ea\u52a8\u8bed\u6cd5\u9519\u8bef\u6821\u6b63\uff08GEC\uff09\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5177\u5907\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\uff08GED\uff09\u80fd\u529b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728SEEDA\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u8bc4\u4f30\u76f8\u5173\u6027\u6700\u9ad8\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u81ea\u52a8GEC\u8bc4\u4f30\u65b9\u6cd5IMPARA\u7684\u8d28\u91cf\u4f30\u8ba1\u5668\uff0c\u589e\u5f3a\u5176\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u8bed\u6cd5\u9519\u8bef\u6821\u6b63\u6548\u679c\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6784\u5efaIMPARA-GED\u7684\u8d28\u91cf\u4f30\u8ba1\u5668\uff0c\u589e\u5f3a\u5176\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728SEEDA\u6570\u636e\u96c6\u4e0a\uff0cIMPARA-GED\u7684\u53e5\u5b50\u7ea7\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6700\u9ad8\u3002", "conclusion": "IMPARA-GED\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8GEC\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708", "abs": "https://arxiv.org/abs/2506.02708", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u8bc4\u5206\u53ca\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u6a21\u578b\u3002", "motivation": "\u7406\u89e3\u6a21\u578b\u8bc4\u5206\u4f9d\u636e\u5bf9\u4fe1\u4efb\u5176\u5224\u65ad\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u56fe\u50cf\u8bc4\u5206\u6570\u636e\u96c6\u548c\u6307\u4ee4\u8c03\u4f18\u7684VLM\u8fdb\u884c\u81ea\u8bad\u7ec3\uff0c\u7ed3\u5408\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8fed\u4ee3\u8bad\u7ec3\u3002", "result": "\u63d0\u9ad8\u4e86\u8bc4\u5206\u51c6\u786e\u6027\u548c\u89e3\u91ca\u7684\u8fde\u8d2f\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u8bc4\u5206\u4e0e\u89e3\u91ca\u7684\u5bf9\u9f50\u6027\u3002"}}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911", "abs": "https://arxiv.org/abs/2506.02911", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CellPuzzles\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u6279\u91cf\u7ec6\u80de\u4e0a\u4e0b\u6587\u63a8\u7406\u4e3a\u7ec6\u80de\u5206\u914d\u552f\u4e00\u7c7b\u578b\uff0c\u5e76\u5f00\u53d1\u4e86Cell-o1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u6ce8\u91ca\u4e2d\u7f3a\u4e4f\u6279\u91cf\u4e0a\u4e0b\u6587\u8003\u8651\u548c\u89e3\u91ca\u6027\u63a8\u7406\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u5219\u80fd\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\u4e3a\u4e0d\u540c\u7ec6\u80de\u7c07\u5206\u914d\u72ec\u7279\u7c7b\u578b\u3002", "method": "\u63d0\u51faCellPuzzles\u4efb\u52a1\uff0c\u5f00\u53d1Cell-o1\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6279\u91cf\u7ea7\u5956\u52b1\u3002", "result": "Cell-o1\u5728\u6279\u91cf\u7ea7\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u4f73\u57fa\u7ebf\uff08OpenAI\u7684o1\uff09\u63d0\u534773%\uff0c\u5e76\u5c55\u73b0\u51fa\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Cell-o1\u5728\u6279\u91cf\u7ec6\u80de\u6ce8\u91ca\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u5de5\u5177\u3002"}}
{"id": "2506.02733", "pdf": "https://arxiv.org/pdf/2506.02733", "abs": "https://arxiv.org/abs/2506.02733", "authors": ["Xiaoyi Feng", "Kaifeng Zou", "Caichun Cen", "Tao Huang", "Hui Guo", "Zizhou Huang", "Yingli Zhao", "Mingqing Zhang", "Diwei Wang", "Yuntao Zou", "Dagang Li"], "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing optical flow datasets focus primarily on real-world simulation or\nsynthetic human motion, but few are tailored to Celluloid(cel) anime character\nmotion: a domain with unique visual and motion characteristics. To bridge this\ngap and facilitate research in optical flow estimation and downstream tasks\nsuch as anime video generation and line drawing colorization, we introduce\nLinkTo-Anime, the first high-quality dataset specifically designed for cel\nanime character motion generated with 3D model rendering. LinkTo-Anime provides\nrich annotations including forward and backward optical flow, occlusion masks,\nand Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230\ntraining frames, 720 validation frames, and 4,320 test frames. Furthermore, a\ncomprehensive benchmark is constructed with various optical flow estimation\nmethods to analyze the shortcomings and limitations across multiple datasets.", "AI": {"tldr": "LinkTo-Anime\u662f\u9996\u4e2a\u4e13\u4e3acel\u52a8\u753b\u89d2\u8272\u8fd0\u52a8\u8bbe\u8ba1\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5149\u6d41\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u652f\u6301\u5149\u6d41\u4f30\u8ba1\u53ca\u76f8\u5173\u4e0b\u6e38\u4efb\u52a1\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u5149\u6d41\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u771f\u5b9e\u4e16\u754c\u6a21\u62df\u6216\u5408\u6210\u4eba\u7c7b\u8fd0\u52a8\uff0c\u7f3a\u4e4f\u9488\u5bf9cel\u52a8\u753b\u89d2\u8272\u8fd0\u52a8\u7684\u6570\u636e\u96c6\uff0c\u800c\u8be5\u9886\u57df\u5177\u6709\u72ec\u7279\u7684\u89c6\u89c9\u548c\u8fd0\u52a8\u7279\u5f81\u3002", "method": "\u901a\u8fc73D\u6a21\u578b\u6e32\u67d3\u751f\u6210LinkTo-Anime\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e30\u5bcc\u6807\u6ce8\uff08\u5149\u6d41\u3001\u906e\u6321\u63a9\u7801\u3001\u9aa8\u67b6\u7b49\uff09\uff0c\u5e76\u6784\u5efa\u591a\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b395\u4e2a\u89c6\u9891\u5e8f\u5217\uff0c\u517124,230\u8bad\u7ec3\u5e27\u3001720\u9a8c\u8bc1\u5e27\u548c4,320\u6d4b\u8bd5\u5e27\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LinkTo-Anime\u586b\u8865\u4e86cel\u52a8\u753b\u5149\u6d41\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.02921", "pdf": "https://arxiv.org/pdf/2506.02921", "abs": "https://arxiv.org/abs/2506.02921", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "title": "A Controllable Examination for Long-Context Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6LongBioBench\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u771f\u5b9e\u4efb\u52a1\u8fc7\u4e8e\u590d\u6742\u4e14\u6613\u53d7\u6570\u636e\u6c61\u67d3\uff0c\u5408\u6210\u4efb\u52a1\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faLongBioBench\uff0c\u5229\u7528\u4eba\u5de5\u751f\u6210\u7684\u4f20\u8bb0\u4f5c\u4e3a\u53d7\u63a7\u73af\u5883\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u7406\u89e3\u3001\u63a8\u7406\u548c\u53ef\u4fe1\u8d56\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u57fa\u7840\u63a8\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u4e14\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u53ef\u4fe1\u8d56\u6027\u4e0b\u964d\u3002", "conclusion": "LongBioBench\u5728\u6a21\u62df\u771f\u5b9e\u4efb\u52a1\u548c\u4fdd\u6301\u53ef\u63a7\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u914d\u7f6e\u6027\u3002"}}
{"id": "2506.02736", "pdf": "https://arxiv.org/pdf/2506.02736", "abs": "https://arxiv.org/abs/2506.02736", "authors": ["Shufan Qing", "Anzhen Li", "Qiandi Wang", "Yuefeng Niu", "Mingchen Feng", "Guoliang Hu", "Jinqiao Wu", "Fengtao Nan", "Yingchun Fan"], "title": "GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Existing semantic SLAM in dynamic environments mainly identify dynamic\nregions through object detection or semantic segmentation methods. However, in\ncertain highly dynamic scenarios, the detection boxes or segmentation masks\ncannot fully cover dynamic regions. Therefore, this paper proposes a robust and\nefficient GeneA-SLAM2 system that leverages depth variance constraints to\nhandle dynamic scenes. Our method extracts dynamic pixels via depth variance\nand creates precise depth masks to guide the removal of dynamic objects.\nSimultaneously, an autoencoder is used to reconstruct keypoints, improving the\ngenetic resampling keypoint algorithm to obtain more uniformly distributed\nkeypoints and enhance the accuracy of pose estimation. Our system was evaluated\non multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2\nmaintains high accuracy in dynamic scenes compared to current methods. Code is\navailable at: https://github.com/qingshufan/GeneA-SLAM2.", "AI": {"tldr": "GeneA-SLAM2\u901a\u8fc7\u6df1\u5ea6\u65b9\u5dee\u7ea6\u675f\u5904\u7406\u52a8\u6001\u573a\u666f\uff0c\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u4f18\u5316\u5173\u952e\u70b9\u5206\u5e03\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u7684SLAM\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49SLAM\u5728\u9ad8\u5ea6\u52a8\u6001\u573a\u666f\u4e2d\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u52a8\u6001\u533a\u57df\uff0c\u9700\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u65b9\u5dee\u63d0\u53d6\u52a8\u6001\u50cf\u7d20\u5e76\u751f\u6210\u6df1\u5ea6\u63a9\u7801\uff0c\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u4f18\u5316\u5173\u952e\u70b9\u5206\u5e03\u3002", "result": "\u5728\u9ad8\u5ea6\u52a8\u6001\u5e8f\u5217\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "GeneA-SLAM2\u4e3a\u52a8\u6001\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02924", "pdf": "https://arxiv.org/pdf/2506.02924", "abs": "https://arxiv.org/abs/2506.02924", "authors": ["Diogo A. P. Nunes", "Eug\u00e9nio Ribeiro"], "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "comment": "12 pages, 1 figure, 6 tables", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "AI": {"tldr": "\u56e2\u961f\u901a\u8fc7\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u5e76\u7ed3\u5408\u5408\u6210\u6570\u636e\uff0c\u5728eRisk 2025\u4efb\u52a11\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u8868\u73b0\uff0c\u51fb\u8d25\u4e86\u5176\u4ed616\u4e2a\u56e2\u961f\u3002", "motivation": "\u89e3\u51b3BDI\u95ee\u5377\u4e2d\u6291\u90c1\u75c7\u72b6\u76f8\u5173\u53e5\u5b50\u7684\u68c0\u7d22\u95ee\u9898\uff0c\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6807\u7b7e\u6709\u9650\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u5c06\u6807\u8bb0\u6570\u636e\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\uff0c\u5c1d\u8bd5\u4e86\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u3001\u53e5\u5b50\u76f8\u4f3c\u5ea6\u3001LLM\u63d0\u793a\u548c\u96c6\u6210\u6280\u672f\u3002", "result": "\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u662f\u7ed3\u5408\u5408\u6210\u6570\u636e\u540e\u3002\u4e0d\u540c\u75c7\u72b6\u7684\u6700\u4f73\u65b9\u6cd5\u5404\u5f02\uff0c\u6700\u7ec8\u63d0\u4ea4\u7684\u4e94\u4e2a\u6d4b\u8bd5\u8fd0\u884c\u5728\u5b98\u65b9\u8bc4\u4f30\u4e2d\u5f97\u5206\u6700\u9ad8\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u7ed3\u5408\u5408\u6210\u6570\u636e\u662f\u89e3\u51b3\u6291\u90c1\u75c7\u72b6\u53e5\u5b50\u68c0\u7d22\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e14\u9700\u9488\u5bf9\u4e0d\u540c\u75c7\u72b6\u8c03\u6574\u7b56\u7565\u3002"}}
{"id": "2506.02738", "pdf": "https://arxiv.org/pdf/2506.02738", "abs": "https://arxiv.org/abs/2506.02738", "authors": ["Negin Baghbanzadeh", "Sajad Ashkezari", "Elham Dolatabadi", "Arash Afkanpour"], "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u63d0\u53d6\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7684\u590d\u5408\u56fe\u5b50\u56fe\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b1800\u4e07\u9ad8\u8d28\u91cf\u5b50\u56fe-\u6807\u9898\u5bf9\u7684OPEN-PMC-18M\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7684\u590d\u5408\u56fe\u5b50\u56fe\u63d0\u53d6\u95ee\u9898\u5c1a\u672a\u5728\u5927\u89c4\u6a21\u4e0a\u5f97\u5230\u89e3\u51b3\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u89c4\u6a21\u548c\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e8e50\u4e07\u5408\u6210\u590d\u5408\u56fe\u6570\u636e\u96c6\uff0c\u5e76\u5728ImageCLEF 2016\u548c\u5408\u6210\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "result": "\u53d1\u5e03\u4e86OPEN-PMC-18M\u6570\u636e\u96c6\uff0c\u5e76\u5728\u68c0\u7d22\u3001\u96f6\u6837\u672c\u5206\u7c7b\u548c\u9c81\u68d2\u6027\u57fa\u51c6\u4e0a\u5c55\u793a\u4e86\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u4e3a\u751f\u7269\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u5efa\u6a21\u548c\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u57fa\u7840\u3002"}}
{"id": "2506.02945", "pdf": "https://arxiv.org/pdf/2506.02945", "abs": "https://arxiv.org/abs/2506.02945", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "title": "Quantitative LLM Judges", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.", "AI": {"tldr": "LLM-as-a-judge\u6846\u67b6\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u5c06LLM\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5bf9\u9f50\uff0c\u63d0\u51fa\u56db\u79cd\u5b9a\u91cf\u8bc4\u5206\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u9002\u7528\u4e8e\u6709\u9650\u4eba\u7c7b\u53cd\u9988\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bc4\u5206\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u5b9a\u91cf\u8bc4\u5206\u65b9\u6cd5\uff0c\u57fa\u4e8eLLM\u7684\u6587\u672c\u8bc4\u4ef7\u548c\u539f\u59cb\u8bc4\u5206\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5b9a\u91cf\u8bc4\u5206\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u8bc4\u5206\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.02741", "pdf": "https://arxiv.org/pdf/2506.02741", "abs": "https://arxiv.org/abs/2506.02741", "authors": ["Pengchong Hu", "Zhizhong Han"], "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Jointly estimating camera poses and mapping scenes from RGBD images is a\nfundamental task in simultaneous localization and mapping (SLAM).\nState-of-the-art methods employ 3D Gaussians to represent a scene, and render\nthese Gaussians through splatting for higher efficiency and better rendering.\nHowever, these methods cannot scale up to extremely large scenes, due to the\ninefficient tracking and mapping strategies that need to optimize all 3D\nGaussians in the limited GPU memories throughout the training to maintain the\ngeometry and color consistency to previous RGBD observations. To resolve this\nissue, we propose novel tracking and mapping strategies to work with a novel 3D\nrepresentation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied\n3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,\nwithout needing to learn locations, rotations, and multi-dimensional variances.\nTying Gaussians to views not only significantly saves storage but also allows\nus to employ many more Gaussians to represent local details in the limited GPU\nmemory. Moreover, our strategies remove the need of maintaining all Gaussians\nlearnable throughout the training, while improving rendering quality, and\ntracking accuracy. We justify the effectiveness of these designs, and report\nbetter performance over the latest methods on the widely used benchmarks in\nterms of rendering and tracking accuracy and scalability. Please see our\nproject page for code and videos at\nhttps://machineperceptionlab.github.io/VTGaussian-SLAM-Project .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u56fe\u7ed1\u5b9a\u76843D\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff08view-tied 3D Gaussians\uff09\uff0c\u7528\u4e8eRGBD SLAM\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u5927\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u75283D\u9ad8\u65af\u8868\u793a\u573a\u666f\uff0c\u4f46\u5728\u8d85\u5927\u573a\u666f\u4e2d\u7531\u4e8eGPU\u5185\u5b58\u9650\u5236\uff0c\u65e0\u6cd5\u9ad8\u6548\u4f18\u5316\u6240\u6709\u9ad8\u65af\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u548c\u5efa\u56fe\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u89c6\u56fe\u7ed1\u5b9a\u76843D\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff0c\u7b80\u5316\u9ad8\u65af\u53c2\u6570\uff08\u5982\u4f4d\u7f6e\u3001\u65cb\u8f6c\u548c\u591a\u7ef4\u65b9\u5dee\uff09\uff0c\u5e76\u5c06\u5176\u7ed1\u5b9a\u5230\u6df1\u5ea6\u50cf\u7d20\u4e0a\uff0c\u8282\u7701\u5b58\u50a8\u5e76\u652f\u6301\u66f4\u591a\u9ad8\u65af\u8868\u793a\u7ec6\u8282\u3002\u540c\u65f6\uff0c\u4f18\u5316\u4e86\u8ddf\u8e2a\u548c\u5efa\u56fe\u7b56\u7565\uff0c\u65e0\u9700\u5168\u7a0b\u4fdd\u6301\u6240\u6709\u9ad8\u65af\u53ef\u5b66\u4e60\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u6e32\u67d3\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u4ee5\u53ca\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u56fe\u7ed1\u5b9a\u76843D\u9ad8\u65af\u8868\u793a\u548c\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86RGBD SLAM\u7cfb\u7edf\u5728\u5927\u573a\u666f\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.02951", "pdf": "https://arxiv.org/pdf/2506.02951", "abs": "https://arxiv.org/abs/2506.02951", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "title": "Adaptive Graph Pruning for Multi-Agent Communication", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u9002\u5e94\u56fe\u526a\u679d\uff08AGP\uff09\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u667a\u80fd\u4f53\u6570\u91cf\u548c\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u56fa\u5b9a\u667a\u80fd\u4f53\u6570\u91cf\u548c\u9759\u6001\u901a\u4fe1\u7ed3\u6784\u9650\u5236\u4e86\u5176\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u590d\u6742\u6027\u7684\u80fd\u529b\u3002", "method": "AGP\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u72ec\u7acb\u8bad\u7ec3\u8f6f\u526a\u679d\u7f51\u7edc\u4ee5\u786e\u5b9a\u4efb\u52a1\u7279\u5b9a\u7684\u6700\u4f18\u667a\u80fd\u4f53\u6570\u91cf\u548c\u901a\u4fe1\u62d3\u6251\uff1b\u7136\u540e\u5728\u6700\u5927\u5b8c\u5168\u56fe\u4e2d\u8054\u5408\u4f18\u5316\u786c\u526a\u679d\u548c\u8f6f\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAGP\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6027\u80fd\u63d0\u53472.58%\u223c9.84%\uff0c\u4e14\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u6b65\u9aa4\u548c\u4ee4\u724c\u6d88\u8017\u3002", "conclusion": "AGP\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u52a8\u6001\u4f18\u5316\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2506.02751", "pdf": "https://arxiv.org/pdf/2506.02751", "abs": "https://arxiv.org/abs/2506.02751", "authors": ["Chuanyu Fu", "Yuqi Zhang", "Kunbin Yao", "Guanying Chen", "Yuan Xiong", "Chuan Huang", "Shuguang Cui", "Xiaochun Cao"], "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS", "categories": ["cs.CV"], "comment": "Project page: https://fcyycf.github.io/RobustSplat/", "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.", "AI": {"tldr": "RobustSplat\u901a\u8fc7\u5ef6\u8fdf\u9ad8\u65af\u589e\u957f\u548c\u5c3a\u5ea6\u7ea7\u8054\u63a9\u7801\u5f15\u5bfc\uff0c\u89e3\u51b3\u4e863DGS\u4e2d\u77ac\u6001\u7269\u4f53\u5bfc\u81f4\u7684\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u5728\u5efa\u6a21\u53d7\u77ac\u6001\u7269\u4f53\u5f71\u54cd\u7684\u573a\u666f\u65f6\uff0c\u56e0\u9ad8\u65af\u5bc6\u5ea6\u5316\u8fc7\u7a0b\u5bfc\u81f4\u4f2a\u5f71\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5ef6\u8fdf\u9ad8\u65af\u589e\u957f\u7b56\u7565\uff0c\u4f18\u5148\u4f18\u5316\u9759\u6001\u573a\u666f\u7ed3\u6784\uff1b2. \u5c3a\u5ea6\u7ea7\u8054\u63a9\u7801\u5f15\u5bfc\uff0c\u4ece\u4f4e\u5206\u8fa8\u7387\u5230\u9ad8\u5206\u8fa8\u7387\u9010\u6b65\u4f18\u5316\u63a9\u7801\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "RobustSplat\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u77ac\u6001\u7269\u4f53\u5f15\u8d77\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u4e863DGS\u7684\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2506.02959", "pdf": "https://arxiv.org/pdf/2506.02959", "abs": "https://arxiv.org/abs/2506.02959", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u4eba\u7c7b\u4e0eAI\u5171\u540c\u521b\u4f5c\u6587\u672c\u7684\u80cc\u666f\u4e0b\uff0c\u7ec6\u7c92\u5ea6\u673a\u5668\u751f\u6210\u6587\u672c\uff08MGT\uff09\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u96c6HACo-Det\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u4f46\u95ee\u9898\u4ecd\u672a\u5b8c\u5168\u89e3\u51b3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e8c\u8fdb\u5236\u6587\u6863\u7ea7\u68c0\u6d4b\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u4e0eAI\u5171\u540c\u521b\u4f5c\u7684\u6587\u672c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6570\u636e\u96c6HACo-Det\uff0c\u6539\u8fdb\u4e03\u79cd\u6587\u6863\u7ea7\u68c0\u6d4b\u5668\u4ee5\u652f\u6301\u8bcd\u7ea7\u68c0\u6d4b\uff0c\u5e76\u5728\u8bcd\u7ea7\u548c\u53e5\u5b50\u7ea7\u4efb\u52a1\u4e0a\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8e\u6307\u6807\u7684\u65b9\u6cd5\u8868\u73b0\u8f83\u5dee\uff08\u5e73\u5747F1\u5206\u65700.462\uff09\uff0c\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u95ee\u9898\u4ecd\u672a\u5b8c\u5168\u89e3\u51b3\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u5171\u540c\u521b\u4f5c\u6587\u672c\u68c0\u6d4b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u5982\u4f18\u5316\u4e0a\u4e0b\u6587\u7a97\u53e3\u7b49\u3002"}}
{"id": "2506.02764", "pdf": "https://arxiv.org/pdf/2506.02764", "abs": "https://arxiv.org/abs/2506.02764", "authors": ["Fatma Youssef Mohammed", "Kostas Alexis"], "title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to the 2025 IEEE International Conference on Development and\n  Learning (ICDL)", "summary": "Computational human attention modeling in free-viewing and task-specific\nsettings is often studied separately, with limited exploration of whether a\ncommon representation exists between them. This work investigates this question\nand proposes a neural network architecture that builds upon the Human Attention\ntransformer (HAT) to test the hypothesis. Our results demonstrate that\nfree-viewing and visual search can efficiently share a common representation,\nallowing a model trained in free-viewing attention to transfer its knowledge to\ntask-driven visual search with a performance drop of only 3.86% in the\npredicted fixation scanpaths, measured by the semantic sequence score (SemSS)\nmetric which reflects the similarity between predicted and human scanpaths.\nThis transfer reduces computational costs by 92.29% in terms of GFLOPs and\n31.23% in terms of trainable parameters.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u7531\u89c2\u770b\u548c\u4efb\u52a1\u9a71\u52a8\u7684\u89c6\u89c9\u641c\u7d22\u662f\u5426\u53ef\u4ee5\u5171\u4eab\u5171\u540c\u7684\u6ce8\u610f\u529b\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHAT\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u8005\u53ef\u4ee5\u9ad8\u6548\u5171\u4eab\u8868\u793a\uff0c\u77e5\u8bc6\u8fc1\u79fb\u6027\u80fd\u4e0b\u964d\u4ec53.86%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u63a2\u7d22\u81ea\u7531\u89c2\u770b\u548c\u4efb\u52a1\u9a71\u52a8\u7684\u89c6\u89c9\u641c\u7d22\u662f\u5426\u5b58\u5728\u5171\u540c\u7684\u6ce8\u610f\u529b\u8868\u793a\uff0c\u4ee5\u9a8c\u8bc1\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5171\u4eab\u8868\u793a\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHuman Attention Transformer (HAT)\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u6d4b\u8bd5\u81ea\u7531\u89c2\u770b\u548c\u89c6\u89c9\u641c\u7d22\u7684\u5171\u4eab\u8868\u793a\u5047\u8bbe\u3002", "result": "\u6a21\u578b\u5728\u81ea\u7531\u89c2\u770b\u8bad\u7ec3\u540e\u8fc1\u79fb\u5230\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u65f6\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d3.86%\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\uff08GFLOPs\u51cf\u5c1192.29%\uff0c\u53c2\u6570\u51cf\u5c1131.23%\uff09\u3002", "conclusion": "\u81ea\u7531\u89c2\u770b\u548c\u89c6\u89c9\u641c\u7d22\u53ef\u4ee5\u9ad8\u6548\u5171\u4eab\u5171\u540c\u7684\u6ce8\u610f\u529b\u8868\u793a\uff0c\u4e3a\u8ba1\u7b97\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02961", "pdf": "https://arxiv.org/pdf/2506.02961", "abs": "https://arxiv.org/abs/2506.02961", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlowerTune LLM Leaderboard\uff0c\u7528\u4e8e\u8bc4\u4f30\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u6db5\u76d6\u56db\u4e2a\u9886\u57df\uff0c\u5e76\u6bd4\u8f83\u4e8626\u79cd\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u516c\u5f00\u6570\u636e\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u6a21\u578b\u5fae\u8c03\u6f5c\u529b\u3002", "method": "\u5f00\u53d1FlowerTune LLM Leaderboard\uff0c\u5305\u542b\u56db\u4e2a\u9886\u57df\u7684\u8054\u90a6\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u6bd4\u8f8326\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u63d0\u4f9b\u4e86\u6a21\u578b\u6027\u80fd\u3001\u8d44\u6e90\u9650\u5236\u548c\u9886\u57df\u9002\u5e94\u7684\u5168\u9762\u6bd4\u8f83\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u3001\u9886\u57df\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u9690\u79c1\u4fdd\u62a4\u7684\u9886\u57df\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.02765", "pdf": "https://arxiv.org/pdf/2506.02765", "abs": "https://arxiv.org/abs/2506.02765", "authors": ["Chunwei Tian", "Kai Liu", "Bob Zhang", "Zhixiang Huang", "Chia-Wen Lin", "David Zhang"], "title": "A Dynamic Transformer Network for Vehicle Detection", "categories": ["cs.CV"], "comment": "8 pages, 5 figures. This paper has been accepted for publication in\n  IEEE Transactions on Consumer Electronics", "summary": "Stable consumer electronic systems can assist traffic better. Good traffic\nconsumer electronic systems require collaborative work between traffic\nalgorithms and hardware. However, performance of popular traffic algorithms\ncontaining vehicle detection methods based on deep networks via learning data\nrelation rather than learning differences in different lighting and occlusions\nis limited. In this paper, we present a dynamic Transformer network for vehicle\ndetection (DTNet). DTNet utilizes a dynamic convolution to guide a deep network\nto dynamically generate weights to enhance adaptability of an obtained\ndetector. Taking into relations of different information account, a mixed\nattention mechanism based channel attention and Transformer is exploited to\nstrengthen relations of channels and pixels to extract more salient information\nfor vehicle detection. To overcome the drawback of difference in an image\naccount, a translation-variant convolution relies on spatial location\ninformation to refine obtained structural information for vehicle detection.\nExperimental results illustrate that our DTNet is competitive for vehicle\ndetection. Code of the proposed DTNet can be obtained at\nhttps://github.com/hellloxiaotian/DTNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001Transformer\u7f51\u7edc\uff08DTNet\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u5377\u79ef\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u8f66\u8f86\u68c0\u6d4b\u7684\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u5149\u7167\u548c\u906e\u6321\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u7f51\u7edc\u7684\u8f66\u8f86\u68c0\u6d4b\u7b97\u6cd5\u56e0\u5ffd\u7565\u5149\u7167\u548c\u906e\u6321\u5dee\u5f02\u800c\u6027\u80fd\u53d7\u9650\uff0c\u9700\u63d0\u5347\u68c0\u6d4b\u5668\u7684\u9002\u5e94\u6027\u3002", "method": "DTNet\u7ed3\u5408\u52a8\u6001\u5377\u79ef\u751f\u6210\u6743\u91cd\uff0c\u5229\u7528\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff08\u901a\u9053\u6ce8\u610f\u529b\u4e0eTransformer\uff09\u589e\u5f3a\u4fe1\u606f\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u4f4d\u7f6e\u4fe1\u606f\u4f18\u5316\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDTNet\u5728\u8f66\u8f86\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DTNet\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u548c\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u8f66\u8f86\u68c0\u6d4b\u7684\u9002\u5e94\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973", "abs": "https://arxiv.org/abs/2506.02973", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "AI": {"tldr": "PLI\uff08Premature Layers Interpolation\uff09\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u63d2\u503c\u63d2\u5165\u4e2d\u95f4\u5c42\u6765\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "LLMs\u5728\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u4e8b\u5b9e\u4e0d\u4e00\u81f4\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8d44\u6e90\u5bc6\u96c6\uff0c\u8981\u4e48\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u3002", "method": "\u63d0\u51faPLI\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u63d2\u503c\u5728\u76f8\u90bb\u5c42\u4e4b\u95f4\u63d2\u5165\u4e2d\u95f4\u5c42\uff0c\u6269\u5c55\u4fe1\u606f\u5904\u7406\u6df1\u5ea6\uff0c\u6539\u5584\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cPLI\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PLI\u7684\u6210\u529f\u4e0eLLMs\u5185\u90e8\u673a\u5236\u5bc6\u5207\u76f8\u5173\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2506.02781", "pdf": "https://arxiv.org/pdf/2506.02781", "abs": "https://arxiv.org/abs/2506.02781", "authors": ["Tongyuan Bai", "Wangyuanfan Bai", "Dong Chen", "Tieru Wu", "Manyi Li", "Rui Ma"], "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Controllability plays a crucial role in the practical applications of 3D\nindoor scene synthesis. Existing works either allow rough language-based\ncontrol, that is convenient but lacks fine-grained scene customization, or\nemploy graph based control, which offers better controllability but demands\nconsiderable knowledge for the cumbersome graph design process. To address\nthese challenges, we present FreeScene, a user-friendly framework that enables\nboth convenient and effective control for indoor scene synthesis.Specifically,\nFreeScene supports free-form user inputs including text description and/or\nreference images, allowing users to express versatile design intentions. The\nuser inputs are adequately analyzed and integrated into a graph representation\nby a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion\nTransformer, which performs graph-aware denoising to enhance scene generation.\nOur MG-DiT not only excels at preserving graph structure but also offers broad\napplicability to various tasks, including, but not limited to, text-to-scene,\ngraph-to-scene, and rearrangement, all within a single model. Extensive\nexperiments demonstrate that FreeScene provides an efficient and user-friendly\nsolution that unifies text-based and graph based scene synthesis, outperforming\nstate-of-the-art methods in terms of both generation quality and\ncontrollability in a range of applications.", "AI": {"tldr": "FreeScene\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u76843D\u5ba4\u5185\u573a\u666f\u5408\u6210\u6846\u67b6\uff0c\u652f\u6301\u81ea\u7531\u5f62\u5f0f\u7684\u7528\u6237\u8f93\u5165\uff08\u5982\u6587\u672c\u63cf\u8ff0\u6216\u53c2\u8003\u56fe\u50cf\uff09\uff0c\u5e76\u901a\u8fc7VLM-based Graph Designer\u548cMG-DiT\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u548c\u53ef\u63a7\u7684\u573a\u666f\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u5ba4\u5185\u573a\u666f\u5408\u6210\u4e2d\u8981\u4e48\u63a7\u5236\u7c97\u7cd9\uff08\u8bed\u8a00\u63a7\u5236\uff09\uff0c\u8981\u4e48\u9700\u8981\u590d\u6742\u7684\u56fe\u8bbe\u8ba1\uff08\u56fe\u63a7\u5236\uff09\uff0c\u7f3a\u4e4f\u4fbf\u6377\u6027\u548c\u7cbe\u7ec6\u63a7\u5236\u7684\u5e73\u8861\u3002", "method": "FreeScene\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u8f93\u5165\uff0c\u901a\u8fc7VLM-based Graph Designer\u751f\u6210\u56fe\u8868\u793a\uff0c\u518d\u4f7f\u7528MG-DiT\u6a21\u578b\u8fdb\u884c\u56fe\u611f\u77e5\u53bb\u566a\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u573a\u666f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeScene\u5728\u751f\u6210\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6587\u672c\u5230\u573a\u666f\u3001\u56fe\u5230\u573a\u666f\u548c\u91cd\u6392\u7b49\u4efb\u52a1\u3002", "conclusion": "FreeScene\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7edf\u4e00\u4e86\u6587\u672c\u548c\u56fe\u63a7\u5236\u7684\u573a\u666f\u5408\u6210\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979", "abs": "https://arxiv.org/abs/2506.02979", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u65e5\u8bed\u5168\u53cc\u5de5\u53e3\u8bed\u5bf9\u8bdd\u6a21\u578b\uff0c\u57fa\u4e8e\u82f1\u6587\u6a21\u578bMoshi\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff09\u53ca\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u65e5\u8bed\u5168\u53cc\u5de5\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u7a00\u7f3a\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eMoshi\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff09\uff0c\u5e76\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u3002", "result": "\u6a21\u578b\u5728\u81ea\u7136\u6027\u548c\u610f\u4e49\u6027\u4e0a\u4f18\u4e8e\u65e5\u8bed\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u65e5\u8bed\u5168\u53cc\u5de5\u53e3\u8bed\u5bf9\u8bdd\u6a21\u578b\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2506.02783", "pdf": "https://arxiv.org/pdf/2506.02783", "abs": "https://arxiv.org/abs/2506.02783", "authors": ["Carlos Garcia-Lopez-de-Haro", "Caterina Fuster-Barcelo", "Curtis T. Rueden", "Jonathan Heras", "Vladimir Ulman", "Daniel Franco-Barranco", "Adrian Ines", "Kevin W. Eliceiri", "Jean-Christophe Olivo-Marin", "Jean-Yves Tinevez", "Daniel Sage", "Arrate Munoz-Barrutia"], "title": "SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model", "categories": ["cs.CV"], "comment": null, "summary": "Mask annotation remains a significant bottleneck in AI-driven biomedical\nimage analysis due to its labor-intensive nature. To address this challenge, we\nintroduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment\nAnything Model (SAM). SAMJ enables seamless, interactive annotations with\none-click installation on standard computers. Designed for real-time object\ndelineation in large scientific images, SAMJ is an easy-to-use solution that\nsimplifies and accelerates the creation of labeled image datasets.", "AI": {"tldr": "SAMJ\u662f\u4e00\u4e2a\u57fa\u4e8eSegment Anything Model (SAM)\u7684ImageJ/Fiji\u63d2\u4ef6\uff0c\u65e8\u5728\u7b80\u5316\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u63a9\u7801\u6807\u6ce8\u5de5\u4f5c\u3002", "motivation": "\u63a9\u7801\u6807\u6ce8\u5728AI\u9a71\u52a8\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86SAMJ\u63d2\u4ef6\uff0c\u5229\u7528SAM\u6a21\u578b\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u6807\u6ce8\uff0c\u652f\u6301\u4e00\u952e\u5b89\u88c5\u548c\u5b9e\u65f6\u5bf9\u8c61\u63cf\u7ed8\u3002", "result": "SAMJ\u7b80\u5316\u5e76\u52a0\u901f\u4e86\u6807\u6ce8\u6570\u636e\u96c6\u7684\u521b\u5efa\uff0c\u9002\u7528\u4e8e\u5927\u578b\u79d1\u5b66\u56fe\u50cf\u3002", "conclusion": "SAMJ\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u7528\u4e14\u9ad8\u6548\u7684\u6807\u6ce8\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6807\u6ce8\u74f6\u9888\u3002"}}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987", "abs": "https://arxiv.org/abs/2506.02987", "authors": ["Richard Armitage"], "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "AI": {"tldr": "\u6d4b\u8bd5\u9886\u5148\u7684LLMs\u5728\u521d\u7ea7\u4fdd\u5065\u6559\u80b2\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u6240\u6709\u6a21\u578b\u5747\u663e\u8457\u4f18\u4e8e\u666e\u901a\u533b\u751f\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u521d\u7ea7\u4fdd\u5065\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u9488\u5bf9MRCGP\u8003\u8bd5\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528100\u9053MRCGP\u98ce\u683c\u7684\u591a\u9009\u9898\u6d4b\u8bd5o3\u3001Claude Opus 4\u3001Grok3\u548cGemini 2.5 Pro\u3002", "result": "o3\u5f97\u520699.0%\uff0c\u5176\u4ed6\u6a21\u578b\u5f97\u520695.0%\uff0c\u5747\u663e\u8457\u9ad8\u4e8e\u666e\u901a\u533b\u751f\u7684\u5e73\u5747\u520673.0%\u3002", "conclusion": "LLMs\uff0c\u5c24\u5176\u662f\u63a8\u7406\u6a21\u578b\uff0c\u5728\u521d\u7ea7\u4fdd\u5065\u9886\u57df\u5177\u6709\u663e\u8457\u652f\u6301\u6f5c\u529b\u3002"}}
{"id": "2506.02789", "pdf": "https://arxiv.org/pdf/2506.02789", "abs": "https://arxiv.org/abs/2506.02789", "authors": ["Renxing Li", "Weiyi Tang", "Peiqi Li", "Qiming Huang", "Jiayuan She", "Shengkai Li", "Haoran Xu", "Yeyun Wan", "Jing Liu", "Hailong Fu", "Xiang Li", "Jiangang Chen"], "title": "Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "Objective. Elevated intracranial pressure (ICP) is recognized as a biomarker\nof secondary brain injury, with a significant linear correlation observed\nbetween optic nerve sheath diameter (ONSD) and ICP. Frequent monitoring of ONSD\ncould effectively support dynamic evaluation of ICP. However, ONSD measurement\nis heavily reliant on the operator's experience and skill, particularly in\nmanually selecting the optimal frame from ultrasound sequences and measuring\nONSD. Approach. This paper presents a novel method to automatically identify\nthe optimal frame from video sequences for ONSD measurement by employing the\nKernel Correlation Filter (KCF) tracking algorithm and Simple Linear Iterative\nClustering (SLIC) segmentation algorithm. The optic nerve sheath is mapped and\nmeasured using a Gaussian Mixture Model (GMM) combined with a\nKL-divergence-based method. Results. When compared with the average\nmeasurements of two expert clinicians, the proposed method achieved a mean\nerror, mean squared deviation, and intraclass correlation coefficient (ICC) of\n0.04, 0.054, and 0.782, respectively. Significance. The findings suggest that\nthis method provides highly accurate automated ONSD measurements, showing\npotential for clinical application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc6\u522b\u8d85\u58f0\u89c6\u9891\u5e8f\u5217\u4e2d\u6700\u4f73\u5e27\u4ee5\u6d4b\u91cf\u89c6\u795e\u7ecf\u9798\u76f4\u5f84\uff08ONSD\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408KCF\u8ddf\u8e2a\u548cSLIC\u5206\u5272\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7GMM\u548cKL\u6563\u5ea6\u65b9\u6cd5\u6d4b\u91cf\uff0c\u7ed3\u679c\u4e0e\u4e13\u5bb6\u6d4b\u91cf\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "ONSD\u4e0e\u9885\u5185\u538b\uff08ICP\uff09\u7ebf\u6027\u76f8\u5173\uff0c\u4f46\u624b\u52a8\u6d4b\u91cf\u4f9d\u8d56\u64cd\u4f5c\u8005\u7ecf\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528KCF\u8ddf\u8e2a\u7b97\u6cd5\u548cSLIC\u5206\u5272\u7b97\u6cd5\u81ea\u52a8\u8bc6\u522b\u6700\u4f73\u5e27\uff0c\u7ed3\u5408GMM\u548cKL\u6563\u5ea6\u65b9\u6cd5\u6d4b\u91cfONSD\u3002", "result": "\u4e0e\u4e13\u5bb6\u6d4b\u91cf\u76f8\u6bd4\uff0c\u5e73\u5747\u8bef\u5dee0.04\uff0c\u5747\u65b9\u5dee0.054\uff0c\u7ec4\u5185\u76f8\u5173\u7cfb\u65700.782\uff0c\u663e\u793a\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u81ea\u52a8\u5316\u7a0b\u5ea6\u9ad8\uff0c\u51c6\u786e\u6027\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02995", "pdf": "https://arxiv.org/pdf/2506.02995", "abs": "https://arxiv.org/abs/2506.02995", "authors": ["Iuliia Zaitova", "Badr M. Abdullah", "Wei Xue", "Dietrich Klakow", "Bernd M\u00f6bius", "Tania Avgustinova"], "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems", "categories": ["cs.CL"], "comment": "13 pages, 3 figures, ACL 2025", "summary": "Idioms are defined as a group of words with a figurative meaning not\ndeducible from their individual components. Although modern machine translation\nsystems have made remarkable progress, translating idioms remains a major\nchallenge, especially for speech-to-text systems, where research on this topic\nis notably sparse. In this paper, we systematically evaluate idiom translation\nas compared to conventional news translation in both text-to-text machine\ntranslation (MT) and speech-to-text translation (SLT) systems across two\nlanguage pairs (German to English, Russian to English). We compare\nstate-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large\nv3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large\nLanguage Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal\nthat SLT systems experience a pronounced performance drop on idiomatic data,\noften reverting to literal translations even in higher layers, whereas MT\nsystems and Large Language Models demonstrate better handling of idioms. These\nfindings underscore the need for idiom-specific strategies and improved\ninternal representations in SLT architectures.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e60\u8bed\u7ffb\u8bd1\u5728\u6587\u672c\u5230\u6587\u672c\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u548c\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\uff08SLT\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SLT\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u800cMT\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4e60\u8bed\u7ffb\u8bd1\u662f\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u5728\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "method": "\u6bd4\u8f83\u4e86\u591a\u79cdMT\u3001SLT\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fb7\u8bed\u5230\u82f1\u8bed\u548c\u4fc4\u8bed\u5230\u82f1\u8bed\u7684\u4e60\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "SLT\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5e38\u91c7\u7528\u5b57\u9762\u7ffb\u8bd1\uff0c\u800cMT\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u9700\u8981\u5728SLT\u67b6\u6784\u4e2d\u5f00\u53d1\u4e60\u8bed\u4e13\u7528\u7b56\u7565\u548c\u6539\u8fdb\u5185\u90e8\u8868\u793a\u3002"}}
{"id": "2506.02843", "pdf": "https://arxiv.org/pdf/2506.02843", "abs": "https://arxiv.org/abs/2506.02843", "authors": ["Shuai Yi", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Random Registers for Cross-Domain Few-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a\ndata-sufficient source domain to data-scarce target domains. Although Vision\nTransformer (ViT) has shown superior capability in many vision tasks, its\ntransferability against huge domain gaps in CDFSL is still under-explored. In\nthis paper, we find an intriguing phenomenon: during the source-domain\ntraining, prompt tuning, as a common way to train ViT, could be harmful for the\ngeneralization of ViT in target domains, but setting them to random noises\n(i.e., random registers) could consistently improve target-domain performance.\nWe then delve into this phenomenon for an interpretation. We find that\nlearnable prompts capture domain information during the training on the source\ndataset, which views irrelevant visual patterns as vital cues for recognition.\nThis can be viewed as a kind of overfitting and increases the sharpness of the\nloss landscapes. In contrast, random registers are essentially a novel way of\nperturbing attention for the sharpness-aware minimization, which helps the\nmodel find a flattened minimum in loss landscapes, increasing the\ntransferability. Based on this phenomenon and interpretation, we further\npropose a simple but effective approach for CDFSL to enhance the perturbation\non attention maps by adding random registers on the semantic regions of image\ntokens, improving the effectiveness and efficiency of random registers.\nExtensive experiments on four benchmarks validate our rationale and\nstate-of-the-art performance. Codes and models are available at\nhttps://github.com/shuaiyi308/REAP.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\uff0cViT\u7684\u63d0\u793a\u8c03\u4f18\u53ef\u80fd\u635f\u5bb3\u76ee\u6807\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u800c\u968f\u673a\u5bc4\u5b58\u5668\u5374\u80fd\u63d0\u5347\u6027\u80fd\u3002\u901a\u8fc7\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u5bc4\u5b58\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22Vision Transformer\u5728\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u7279\u522b\u662f\u63d0\u793a\u8c03\u4f18\u4e0e\u968f\u673a\u5bc4\u5b58\u5668\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u968f\u673a\u5bc4\u5b58\u5668\u4f18\u4e8e\u63d0\u793a\u8c03\u4f18\uff0c\u63d0\u51fa\u5728\u56fe\u50cf\u6807\u8bb0\u7684\u8bed\u4e49\u533a\u57df\u6dfb\u52a0\u968f\u673a\u5bc4\u5b58\u5668\u4ee5\u589e\u5f3a\u6ce8\u610f\u529b\u6270\u52a8\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u968f\u673a\u5bc4\u5b58\u5668\u901a\u8fc7\u6270\u52a8\u6ce8\u610f\u529b\u5e2e\u52a9\u6a21\u578b\u627e\u5230\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u63d0\u5347\u4e86\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2506.02998", "pdf": "https://arxiv.org/pdf/2506.02998", "abs": "https://arxiv.org/abs/2506.02998", "authors": ["\u0110or\u0111e Klisura", "Astrid R Bernaga Torres", "Anna Karen G\u00e1rate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u8a00\u4ee3\u7406\u548c\u9690\u79c1\u653f\u7b56\u4ee3\u7406\u534f\u4f5c\uff0c\u51cf\u5c11\u9690\u79c1\u653f\u7b56\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u65b9\u8a00\u504f\u89c1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9690\u79c1\u653f\u7b56\u7684\u590d\u6742\u6027\u9650\u5236\u4e86\u4e0d\u540c\u4eba\u7fa4\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u82f1\u8bed\u65b9\u8a00\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u5bfc\u81f4\u975e\u6807\u51c6\u65b9\u8a00\u4f7f\u7528\u8005\u5904\u4e8e\u52a3\u52bf\u3002", "method": "\u7ed3\u5408\u65b9\u8a00\u4ee3\u7406\uff08\u5c06\u67e5\u8be2\u7ffb\u8bd1\u4e3a\u6807\u51c6\u82f1\u8bed\uff09\u548c\u9690\u79c1\u653f\u7b56\u4ee3\u7406\uff08\u5229\u7528\u9886\u57df\u77e5\u8bc6\u4f18\u5316\u9884\u6d4b\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u65b9\u8a00\u7279\u5b9a\u8c03\u6574\u3002", "result": "\u5728PrivacyQA\u548cPolicyQA\u4e0a\uff0c\u96f6\u6837\u672c\u51c6\u786e\u7387\u5206\u522b\u4ece0.394\u63d0\u5347\u81f30.601\u548c\u4ece0.352\u63d0\u5347\u81f30.464\uff0c\u8d85\u8d8a\u6216\u5339\u914d\u5c11\u6837\u672c\u57fa\u7ebf\u3002", "conclusion": "\u7ed3\u6784\u5316\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u51cf\u5c11\u65b9\u8a00\u504f\u89c1\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u8003\u8651\u8bed\u8a00\u591a\u6837\u6027\u7684NLP\u7cfb\u7edf\u5bf9\u9690\u79c1\u4fe1\u606f\u516c\u5e73\u8bbf\u95ee\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.02845", "pdf": "https://arxiv.org/pdf/2506.02845", "abs": "https://arxiv.org/abs/2506.02845", "authors": ["Di Wen", "Lei Qi", "Kunyu Peng", "Kailun Yang", "Fei Teng", "Ao Luo", "Jia Fu", "Yufan Chen", "Ruiping Liu", "Yitian Shi", "M. Saquib Sarfraz", "Rainer Stiefelhagen"], "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, submitted to NeurIPS 2025", "summary": "Despite substantial progress in video understanding, most existing datasets\nare limited to Earth's gravitational conditions. However, microgravity alters\nhuman motion, interactions, and visual semantics, revealing a critical gap for\nreal-world vision systems. This presents a challenge for domain-robust video\nunderstanding in safety-critical space applications. To address this, we\nintroduce MicroG-4M, the first benchmark for spatio-temporal and semantic\nunderstanding of human activities in microgravity. Constructed from real-world\nspace missions and cinematic simulations, the dataset includes 4,759 clips\ncovering 50 actions, 1,238 context-rich captions, and over 7,000\nquestion-answer pairs on astronaut activities and scene understanding.\nMicroG-4M supports three core tasks: fine-grained multi-label action\nrecognition, temporal video captioning, and visual question answering, enabling\na comprehensive evaluation of both spatial localization and semantic reasoning\nin microgravity contexts. We establish baselines using state-of-the-art models.\nAll data, annotations, and code are available at\nhttps://github.com/LEI-QI-233/HAR-in-Space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MicroG-4M\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u4eba\u7c7b\u6d3b\u52a8\u7684\u65f6\u7a7a\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u6570\u636e\u96c6\u4e3b\u8981\u57fa\u4e8e\u5730\u7403\u91cd\u529b\u6761\u4ef6\uff0c\u800c\u5fae\u91cd\u529b\u73af\u5883\u4f1a\u6539\u53d8\u4eba\u7c7b\u8fd0\u52a8\u548c\u89c6\u89c9\u8bed\u4e49\uff0c\u8fd9\u5bf9\u5b89\u5168\u5173\u952e\u7684\u7a7a\u95f4\u5e94\u7528\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u901a\u8fc7\u771f\u5b9e\u592a\u7a7a\u4efb\u52a1\u548c\u7535\u5f71\u6a21\u62df\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b4,759\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u6db5\u76d650\u79cd\u52a8\u4f5c\u30011,238\u4e2a\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u63cf\u8ff0\u548c7,000\u591a\u4e2a\u95ee\u7b54\u5bf9\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u591a\u6807\u7b7e\u52a8\u4f5c\u8bc6\u522b\u3001\u65f6\u5e8f\u89c6\u9891\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u5148\u8fdb\u6a21\u578b\u7684\u57fa\u7ebf\u3002", "conclusion": "MicroG-4M\u4e3a\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u7684\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u9c81\u68d2\u6027\u7684\u7814\u7a76\u3002"}}
{"id": "2506.03009", "pdf": "https://arxiv.org/pdf/2506.03009", "abs": "https://arxiv.org/abs/2506.03009", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u6cd5\u5f8b\u77e5\u8bc6\u8c03\u8282\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u68c0\u6d4b\u5fb7\u56fd\u5211\u6cd5\u4e2d\u89c4\u5b9a\u7684\u4ec7\u6068\u8a00\u8bba\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5728\u4ec7\u6068\u8a00\u8bba\u8bc4\u4f30\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u6cd5\u5f8b\u95ee\u9898\u9700\u8981\u8003\u8651\u7279\u5b9a\u6cd5\u5f8b\u4f53\u7cfb\u53ca\u5176\u62bd\u8c61\u5c42\u6b21\uff0c\u4f46LLMs\u662f\u5426\u5185\u5316\u4e86\u8fd9\u4e9b\u6cd5\u5f8b\u4f53\u7cfb\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u6cd5\u5f8b\u77e5\u8bc6\u8c03\u8282LLMs\u3002", "method": "\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u5728\u591a\u4e2a\u6cd5\u5f8b\u62bd\u8c61\u5c42\u6b21\u4e0a\u8c03\u8282LLMs\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5fb7\u56fd\u5211\u6cd5\u4e2d\u4ec7\u6068\u8a00\u8bba\u7684\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u6cd5\u5f8b\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u62bd\u8c61\u6cd5\u5f8b\u77e5\u8bc6\u7684\u6a21\u578b\u7f3a\u4e4f\u6df1\u5ea6\u7406\u89e3\uff0c\u800c\u57fa\u4e8e\u5177\u4f53\u6cd5\u5f8b\u77e5\u8bc6\u7684\u6a21\u578b\u5728\u8bc6\u522b\u76ee\u6807\u7fa4\u4f53\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u884c\u4e3a\u5206\u7c7b\u4e0a\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u6cd5\u5f8b\u8bc4\u4f30\u4e0a\u4e0e\u4e13\u5bb6\u4ecd\u6709\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2506.02846", "pdf": "https://arxiv.org/pdf/2506.02846", "abs": "https://arxiv.org/abs/2506.02846", "authors": ["Yujin Chen", "Yinyu Nie", "Benjamin Ummenhofer", "Reiner Birkl", "Michael Paulitsch", "Matthias Nie\u00dfner"], "title": "PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors", "categories": ["cs.CV"], "comment": "Project page: https://terencecyj.github.io/projects/PBR-SR/, Video:\n  https://youtu.be/eaM5S3Mt1RM", "summary": "We present PBR-SR, a novel method for physically based rendering (PBR)\ntexture super resolution (SR). It outputs high-resolution, high-quality PBR\ntextures from low-resolution (LR) PBR input in a zero-shot manner. PBR-SR\nleverages an off-the-shelf super-resolution model trained on natural images,\nand iteratively minimizes the deviations between super-resolution priors and\ndifferentiable renderings. These enhancements are then back-projected into the\nPBR map space in a differentiable manner to produce refined, high-resolution\ntextures. To mitigate view inconsistencies and lighting sensitivity, which is\ncommon in view-based super-resolution, our method applies 2D prior constraints\nacross multi-view renderings, iteratively refining the shared, upscaled\ntextures. In parallel, we incorporate identity constraints directly in the PBR\ntexture domain to ensure the upscaled textures remain faithful to the LR input.\nPBR-SR operates without any additional training or data requirements, relying\nentirely on pretrained image priors. We demonstrate that our approach produces\nhigh-fidelity PBR textures for both artist-designed and AI-generated meshes,\noutperforming both direct SR models application and prior texture optimization\nmethods. Our results show high-quality outputs in both PBR and rendering\nevaluations, supporting advanced applications such as relighting.", "AI": {"tldr": "PBR-SR\u662f\u4e00\u79cd\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u5148\u9a8c\u7ea6\u675f\uff0c\u4ece\u4f4e\u5206\u8fa8\u7387PBR\u7eb9\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u89c6\u56fe\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u89c6\u89d2\u4e0d\u4e00\u81f4\u548c\u5149\u7167\u654f\u611f\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u7eb9\u7406\u5bf9\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u5fe0\u5b9e\u6027\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u81ea\u7136\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u6700\u5c0f\u5316\u8d85\u5206\u8fa8\u7387\u5148\u9a8c\u4e0e\u53ef\u5fae\u5206\u6e32\u67d3\u4e4b\u95f4\u7684\u504f\u5dee\uff0c\u5e76\u7ed3\u5408\u591a\u89c6\u89d2\u6e32\u67d3\u76842D\u5148\u9a8c\u7ea6\u675f\u548c\u7eb9\u7406\u57df\u7684\u8eab\u4efd\u7ea6\u675f\u3002", "result": "PBR-SR\u5728\u827a\u672f\u5bb6\u8bbe\u8ba1\u548cAI\u751f\u6210\u7684\u7f51\u683c\u4e0a\u5747\u80fd\u751f\u6210\u9ad8\u8d28\u91cfPBR\u7eb9\u7406\uff0c\u4f18\u4e8e\u76f4\u63a5\u5e94\u7528\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u548c\u5148\u524d\u7684\u7eb9\u7406\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "PBR-SR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u7ea7\u5e94\u7528\u5982\u91cd\u65b0\u5149\u7167\uff0c\u751f\u6210\u9ad8\u4fdd\u771fPBR\u7eb9\u7406\u3002"}}
{"id": "2506.03011", "pdf": "https://arxiv.org/pdf/2506.03011", "abs": "https://arxiv.org/abs/2506.03011", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "categories": ["cs.CL"], "comment": null, "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "AI": {"tldr": "OpenHands-Versa\u662f\u4e00\u79cd\u901a\u7528AI\u4ee3\u7406\uff0c\u4f7f\u7528\u5c11\u91cf\u901a\u7528\u5de5\u5177\uff08\u5982\u4ee3\u7801\u7f16\u8f91\u3001\u7f51\u9875\u641c\u7d22\u7b49\uff09\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e13\u7528\u4ee3\u7406\u3002", "motivation": "\u73b0\u4ee3AI\u4ee3\u7406\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u4f18\u5316\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u7528\u6700\u5c11\u901a\u7528\u5de5\u5177\u5b9e\u73b0\u591a\u6837\u5316\u4efb\u52a1\u7684\u9ad8\u6027\u80fd\u3002", "method": "\u5f00\u53d1OpenHands-Versa\u4ee3\u7406\uff0c\u96c6\u6210\u4ee3\u7801\u7f16\u8f91\u3001\u7f51\u9875\u641c\u7d22\u3001\u591a\u6a21\u6001\u6d4f\u89c8\u548c\u6587\u4ef6\u8bbf\u95ee\u7b49\u901a\u7528\u5de5\u5177\u3002", "result": "\u5728SWE-Bench Multimodal\u3001GAIA\u548cThe Agent Company\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOpenHands-Versa\u5206\u522b\u4ee59.1\u30011.3\u548c9.1\u70b9\u7684\u7edd\u5bf9\u4f18\u52bf\u8d85\u8d8a\u4e13\u7528\u4ee3\u7406\u3002", "conclusion": "OpenHands-Versa\u8bc1\u660e\u4e86\u901a\u7528\u4ee3\u7406\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2506.02850", "pdf": "https://arxiv.org/pdf/2506.02850", "abs": "https://arxiv.org/abs/2506.02850", "authors": ["Mengyue Wang", "Shuo Chen", "Kristian Kersting", "Volker Tresp", "Yunpu Ma"], "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding", "categories": ["cs.CV"], "comment": "14 pages, 10 figures", "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.", "AI": {"tldr": "METok\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u9636\u6bb5\u4e8b\u4ef6\u9a71\u52a8\u7684\u4ee4\u724c\u538b\u7f29\u6846\u67b6\uff0c\u65e8\u5728\u52a0\u901f\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u7684\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5904\u7406\u957f\u89c6\u9891\u65f6\uff0c\u8ba1\u7b97\u9700\u6c42\u9ad8\u4e14\u89c6\u89c9\u6570\u636e\u5197\u4f59\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "METok\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u9010\u6b65\u6d88\u9664\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\uff1a\u4e8b\u4ef6\u611f\u77e5\u538b\u7f29\u3001\u57fa\u4e8e\u8bed\u4e49\u5bf9\u9f50\u548c\u4e8b\u4ef6\u91cd\u8981\u6027\u7684\u5206\u5c42\u4ee4\u724c\u4fee\u526a\uff0c\u4ee5\u53ca\u89e3\u7801\u9636\u6bb5\u7684KV\u7f13\u5b58\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMETok\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4f8b\u5982\u5728LongVA-7B\u4e0a\u5b9e\u73b0\u4e8680.6%\u7684FLOPs\u51cf\u5c11\u548c93.5%\u7684KV\u7f13\u5b58\u5185\u5b58\u8282\u7701\u3002", "conclusion": "METok\u4e3a\u957f\u89c6\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03035", "pdf": "https://arxiv.org/pdf/2506.03035", "abs": "https://arxiv.org/abs/2506.03035", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Conference paper accepted to INTERSPEECH 2025", "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u65b9\u6cd5\u9009\u62e9\u793a\u4f8b\u4ee5\u589e\u5f3a\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u53e3\u8bed\u7406\u89e3\uff08SLU\uff09\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684SLU\u6280\u672f\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u7279\u5b9a\u4efb\u52a1\u6216\u8bed\u8a00\u7684\u6807\u6ce8\u6570\u636e\u6709\u9650\uff0c\u800c\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u65b9\u6cd5\u9009\u62e9\u793a\u4f8b\u6784\u5efa\u589e\u5f3a\u63d0\u793a\uff0c\u5e94\u7528\u4e8eSLU\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u8bcd\u6c47\u7684IR\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u672a\u589e\u52a0\u63d0\u793a\u957f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86SLU\u4efb\u52a1\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02853", "pdf": "https://arxiv.org/pdf/2506.02853", "abs": "https://arxiv.org/abs/2506.02853", "authors": ["Mingjie Wei", "Xuemei Xie", "Yutong Zhong", "Guangming Shi"], "title": "Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Action coordination in human structure is indispensable for the spatial\nconstraints of 2D joints to recover 3D pose. Usually, action coordination is\nrepresented as a long-range dependence among body parts. However, there are two\nmain challenges in modeling long-range dependencies. First, joints should not\nonly be constrained by other individual joints but also be modulated by the\nbody parts. Second, existing methods make networks deeper to learn dependencies\nbetween non-linked parts. They introduce uncorrelated noise and increase the\nmodel size. In this paper, we utilize a pyramid structure to better learn\npotential long-range dependencies. It can capture the correlation across joints\nand groups, which complements the context of the human sub-structure. In an\neffective cross-scale way, it captures the pyramid-structured long-range\ndependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)\nmodule to capture long-range cross-scale dependencies. It concatenates\ninformation from various scales into a compact sequence, and then computes the\ncorrelation between scales in parallel. Combining PGA with graph convolution\nmodules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose\nestimation, which is a lightweight multi-scale transformer architecture. It\nencapsulates human sub-structures into self-attention by pooling. Extensive\nexperiments show that our approach achieves lower error and smaller model size\nthan state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code\nis available at https://github.com/MingjieWe/PGFormer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91d1\u5b57\u5854\u56fe\u6ce8\u610f\u529b\u6a21\u5757\uff08PGA\uff09\u548c\u91d1\u5b57\u5854\u56fe\u53d8\u6362\u5668\uff08PGFormer\uff09\uff0c\u7528\u4e8e3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8bef\u5dee\u548c\u66f4\u5c0f\u7684\u6a21\u578b\u4f53\u79ef\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\u5b66\u4e60\u975e\u8fde\u63a5\u90e8\u5206\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u4f1a\u5f15\u5165\u65e0\u5173\u566a\u58f0\u5e76\u589e\u52a0\u6a21\u578b\u4f53\u79ef\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5229\u7528\u91d1\u5b57\u5854\u7ed3\u6784\u6355\u6349\u5173\u8282\u548c\u7ec4\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51faPGA\u6a21\u5757\u5e76\u884c\u8ba1\u7b97\u591a\u5c3a\u5ea6\u4fe1\u606f\u7684\u76f8\u5173\u6027\uff0c\u5e76\u7ed3\u5408\u56fe\u5377\u79ef\u6a21\u5757\u5f00\u53d1\u4e86PGFormer\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\uff0cPGFormer\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f4e\u7684\u8bef\u5dee\u548c\u66f4\u5c0f\u7684\u6a21\u578b\u4f53\u79ef\u3002", "conclusion": "PGFormer\u901a\u8fc7\u91d1\u5b57\u5854\u7ed3\u6784\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u8f7b\u91cf\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03038", "pdf": "https://arxiv.org/pdf/2506.03038", "abs": "https://arxiv.org/abs/2506.03038", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86VAPO\u6846\u67b6\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7528\u4e8e\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u957f\u94fe\u63a8\u7406\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63a2\u8ba8\u4e86\u4fe1\u7528\u5206\u914d\u3001\u4ef7\u503c\u51fd\u6570\u8868\u793a\u80fd\u529b\u7b49\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793aVAPO\u6846\u67b6\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u63a8\u52a8\u66f4\u5f3a\u5927\u7684LLM\u4ee3\u7406\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63a2\u8ba8\u4e86VAPO\u6846\u67b6\u5728\u4fe1\u7528\u5206\u914d\u3001\u4ef7\u503c\u51fd\u6570\u8868\u793a\u548c\u5168\u5c40\u4ef7\u503c\u4fe1\u53f7\u8f6c\u5316\u4e3a\u5c40\u90e8\u7b56\u7565\u6539\u8fdb\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "result": "\u7814\u7a76\u53d1\u73b0VAPO\u6846\u67b6\u5728\u5efa\u6a21\u957f\u671f\u4ef7\u503c\u548c\u7ec6\u7c92\u5ea6\u7b56\u7565\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7406\u89e3\u5f53\u524dRL\u5728\u9ad8\u7ea7\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.02854", "pdf": "https://arxiv.org/pdf/2506.02854", "abs": "https://arxiv.org/abs/2506.02854", "authors": ["Mengmeng Zhang", "Xingyuan Dai", "Yicheng Sun", "Jing Wang", "Yueyang Yao", "Xiaoyan Gong", "Fuze Cong", "Feiyue Wang", "Yisheng Lv"], "title": "Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework", "categories": ["cs.CV"], "comment": null, "summary": "Although the Segment Anything Model (SAM) is highly effective in natural\nimage segmentation, it requires dependencies on prompts, which limits its\napplicability to medical imaging where manual prompts are often unavailable.\nExisting efforts to fine-tune SAM for medical segmentation typically struggle\nto remove this dependency. We propose Hierarchical Self-Prompting SAM\n(HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong\nperformance in prompt-free medical image segmentation. Unlike previous\nself-prompting methods that remain limited to positional prompts similar to\nvanilla SAM, we are the first to introduce learning abstract prompts during the\nself-prompting process. This simple and intuitive self-prompting framework\nachieves superior performance on classic segmentation tasks such as polyp and\nskin lesion segmentation, while maintaining robustness across diverse medical\nimaging modalities. Furthermore, it exhibits strong generalization to unseen\ndatasets, achieving improvements of up to 14.04% over previous state-of-the-art\nmethods on some challenging benchmarks. These results suggest that abstract\nprompts encapsulate richer and higher-dimensional semantic information compared\nto positional prompts, thereby enhancing the model's robustness and\ngeneralization performance. All models and codes will be released upon\nacceptance.", "AI": {"tldr": "HSP-SAM\u662f\u4e00\u79cd\u81ea\u63d0\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u63d0\u793a\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "SAM\u4f9d\u8d56\u63d0\u793a\u9650\u5236\u4e86\u5176\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u53bb\u9664\u8fd9\u79cd\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u81ea\u63d0\u793a\u6846\u67b6HSP-SAM\uff0c\u9996\u6b21\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5f15\u5165\u62bd\u8c61\u63d0\u793a\u3002", "result": "\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u6a21\u6001\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u6027\u80fd\u63d0\u5347\u8fbe14.04%\u3002", "conclusion": "\u62bd\u8c61\u63d0\u793a\u6bd4\u4f4d\u7f6e\u63d0\u793a\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.03051", "pdf": "https://arxiv.org/pdf/2506.03051", "abs": "https://arxiv.org/abs/2506.03051", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "AI": {"tldr": "\u8bc4\u4f30Llama3.1\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6559\u80b2\u573a\u666f\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u5176\u63d0\u4f9b\u591a\u4f59\u4fe1\u606f\u4e14\u52a0\u5267\u5bf9\u5c0f\u8bed\u79cd\u7684\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bc4\u4f30Llama3.1\u6a21\u578b\u5728\u56de\u7b54\u4e2d\u5b66\u9636\u6bb5\u4e8b\u5b9e\u6027\u95ee\u9898\u65f6\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u4e0d\u4ec5\u63d0\u4f9b\u591a\u4f59\u4e14\u4e0d\u51c6\u786e\u7684\u4fe1\u606f\uff0c\u8fd8\u52a0\u5267\u4e86\u5bf9\u5c0f\u8bed\u79cd\u7684\u504f\u89c1\u3002", "conclusion": "\u9700\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6559\u80b2\u4e2d\u7684\u4e8b\u5b9e\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2506.02857", "pdf": "https://arxiv.org/pdf/2506.02857", "abs": "https://arxiv.org/abs/2506.02857", "authors": ["Luca Maiano", "Fabrizio Casadei", "Irene Amerini"], "title": "Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting deepfakes has become a critical challenge in Computer Vision and\nArtificial Intelligence. Despite significant progress in detection techniques,\ngeneralizing them to open-set scenarios continues to be a persistent\ndifficulty. Neural networks are often trained on the closed-world assumption,\nbut with new generative models constantly evolving, it is inevitable to\nencounter data generated by models that are not part of the training\ndistribution. To address these challenges, in this paper, we propose two novel\nOut-Of-Distribution (OOD) detection approaches. The first approach is trained\nto reconstruct the input image, while the second incorporates an attention\nmechanism for detecting OODs. Our experiments validate the effectiveness of the\nproposed approaches compared to existing state-of-the-art techniques. Our\nmethod achieves promising results in deepfake detection and ranks among the\ntop-performing configurations on the benchmark, demonstrating their potential\nfor robust, adaptable solutions in dynamic, real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u5f00\u653e\u96c6\u6311\u6218\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u66f4\u65b0\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e24\u79cdOOD\u68c0\u6d4b\u65b9\u6cd5\uff1a\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u91cd\u6784\uff0c\u53e6\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u65b9\u6cd5\u5728\u52a8\u6001\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03090", "pdf": "https://arxiv.org/pdf/2506.03090", "abs": "https://arxiv.org/abs/2506.03090", "authors": ["Katherine Thai", "Mohit Iyyer"], "title": "Literary Evidence Retrieval via Long-Context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.", "AI": {"tldr": "\u73b0\u4ee3\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u5982\u4f55\uff1f\u7814\u7a76\u53d1\u73b0\uff0cGemini Pro 2.5\u7b49\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff0862.5% vs. 50%\uff09\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u8f83\u5dee\uff0829.1%\uff09\u3002\u6a21\u578b\u5728\u6587\u5b66\u4fe1\u53f7\u7684\u7ec6\u5fae\u7406\u89e3\u548c\u8fc7\u5ea6\u751f\u6210\u65b9\u9762\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u73b0\u4ee3\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u5b66\u5c0f\u8bf4\u7684\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5229\u7528RELiC\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u5728\u63d0\u4f9b\u5168\u6587\u548c\u6587\u5b66\u6279\u8bc4\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u7f3a\u5931\u7684\u5f15\u6587\uff0c\u6d4b\u8bd5\u5176\u5168\u5c40\u53d9\u4e8b\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u6587\u672c\u5206\u6790\u80fd\u529b\u3002", "result": "Gemini Pro 2.5\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0862.5% vs. 50%\uff09\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u4ec5\u8fbe\u523029.1%\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u67d0\u4e9b\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6587\u5b66\u4fe1\u53f7\u7684\u7ec6\u5fae\u7406\u89e3\u548c\u8fc7\u5ea6\u751f\u6210\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2506.02866", "pdf": "https://arxiv.org/pdf/2506.02866", "abs": "https://arxiv.org/abs/2506.02866", "authors": ["Ahsan Baidar Bakht", "Muhayy Ud Din", "Sajid Javed", "Irfan Hussain"], "title": "MVTD: A Benchmark Dataset for Maritime Visual Object Tracking", "categories": ["cs.CV"], "comment": "Submited to Nature Scientific Data", "summary": "Visual Object Tracking (VOT) is a fundamental task with widespread\napplications in autonomous navigation, surveillance, and maritime robotics.\nDespite significant advances in generic object tracking, maritime environments\ncontinue to present unique challenges, including specular water reflections,\nlow-contrast targets, dynamically changing backgrounds, and frequent\nocclusions. These complexities significantly degrade the performance of\nstate-of-the-art tracking algorithms, highlighting the need for domain-specific\ndatasets. To address this gap, we introduce the Maritime Visual Tracking\nDataset (MVTD), a comprehensive and publicly available benchmark specifically\ndesigned for maritime VOT. MVTD comprises 182 high-resolution video sequences,\ntotaling approximately 150,000 frames, and includes four representative object\nclasses: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset\ncaptures a diverse range of operational conditions and maritime scenarios,\nreflecting the real-world complexities of maritime environments. We evaluated\n14 recent SOTA tracking algorithms on the MVTD benchmark and observed\nsubstantial performance degradation compared to their performance on\ngeneral-purpose datasets. However, when fine-tuned on MVTD, these models\ndemonstrate significant performance gains, underscoring the effectiveness of\ndomain adaptation and the importance of transfer learning in specialized\ntracking contexts. The MVTD dataset fills a critical gap in the visual tracking\ncommunity by providing a realistic and challenging benchmark for maritime\nscenarios. Dataset and Source Code can be accessed here\n\"https://github.com/AhsanBaidar/MVTD\".", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e13\u95e8\u4e3a\u6d77\u4e8b\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u8bbe\u8ba1\u7684MVTD\u6570\u636e\u96c6\uff0c\u5305\u542b182\u4e2a\u89c6\u9891\u5e8f\u5217\u548c150,000\u5e27\uff0c\u8986\u76d6\u56db\u79cd\u76ee\u6807\u7c7b\u522b\u3002\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u7b97\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6d77\u4e8b\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u9762\u4e34\u72ec\u7279\u6311\u6218\uff08\u5982\u6c34\u9762\u53cd\u5c04\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u76ee\u6807\u7b49\uff09\uff0c\u73b0\u6709\u901a\u7528\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u4e9f\u9700\u9886\u57df\u4e13\u7528\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86MVTD\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u5e8f\u5217\u548c\u591a\u6837\u5316\u7684\u6d77\u4e8b\u573a\u666f\uff0c\u8bc4\u4f30\u4e8614\u79cd\u5148\u8fdb\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5fae\u8c03\u5b9e\u9a8c\u3002", "result": "\u73b0\u6709\u7b97\u6cd5\u5728MVTD\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u540e\u6027\u80fd\u5927\u5e45\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u9002\u5e94\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "conclusion": "MVTD\u586b\u8865\u4e86\u6d77\u4e8b\u89c6\u89c9\u8ddf\u8e2a\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u548c\u7b97\u6cd5\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101", "abs": "https://arxiv.org/abs/2506.03101", "authors": ["Jonas F. Lotz", "Ant\u00f3nio V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "AI": {"tldr": "\u5c0f\u6a21\u578b\u80fd\u9ad8\u6548\u9884\u6d4b\u5206\u8bcd\u5668\u5bf9\u5927\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5185\u5728\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u591a\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5206\u8bcd\u5668\u9009\u62e9\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u5206\u8bcd\u5668\u8bc4\u4f30\u7f3a\u4e4f\u9ad8\u6548\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u5206\u8bcd\u5668\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u663e\u8457\u3002", "method": "\u901a\u8fc7\u5c0f\u6a21\u578b\u9884\u6d4b\u5206\u8bcd\u5668\u5bf9\u5927\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u57fa\u4e8eZipf\u5b9a\u5f8b\u7684\u5185\u5728\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u7ed3\u5408\u591a\u6307\u6807\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u82f1\u8bed\u4efb\u52a1\u4e2d\u5206\u8bcd\u5668\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff1b\u65b0\u6307\u6807\u6bd4\u6587\u672c\u538b\u7f29\u66f4\u80fd\u9884\u6d4b\u4e0b\u6e38\u8868\u73b0\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u5206\u8bcd\u5668\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u7684\u5206\u8bcd\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.02868", "pdf": "https://arxiv.org/pdf/2506.02868", "abs": "https://arxiv.org/abs/2506.02868", "authors": ["Amal S. Perera", "David Fernandez", "Chandi Witharana", "Elias Manos", "Michael Pimenta", "Anna K. Liljedahl", "Ingmar Nitze", "Yili Yang", "Todd Nicholson", "Chia-Yu Hsu", "Wenwen Li", "Guido Grosse"], "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings", "categories": ["cs.CV", "I.4.6; I.5.4; I.5.2; I.2.10"], "comment": "20 pages, 2 column IEEE format, 13 Figures", "summary": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5317\u6781\u9065\u611f\u4efb\u52a1\u4e2d\u4f7f\u7528Vision Transformers\uff08ViTs\uff09\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u4f4d\u7f6e\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5bf9\u6c38\u4e45\u51bb\u571f\u5730\u8c8c\u3001\u878d\u5316\u89e3\u51bb\u6270\u52a8\u548c\u4eba\u7c7b\u57fa\u7840\u8bbe\u65bd\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5317\u6781\u5730\u533a\u7684\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u6570\u636e\u91cf\u5e9e\u5927\uff0c\u4f20\u7edfCNN\u6a21\u578b\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800cViTs\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6574\u5408\u4e86\u5730\u7406\u7a7a\u95f4\u4f4d\u7f6e\u5d4c\u5165\u5230ViTs\u4e2d\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u68c0\u6d4b\u51b0\u6954\u591a\u8fb9\u5f62\uff08IWP\uff09\u3001\u9000\u5316\u878d\u51bb\u6ed1\u584c\uff08RTS\uff09\u548c\u4eba\u7c7b\u57fa\u7840\u8bbe\u65bd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4f4d\u7f6e\u5d4c\u5165\u7684ViTs\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u7684\u4e24\u4e2a\u4e0a\u4f18\u4e8eCNN\u6a21\u578b\uff0c\u4f8b\u5982RTS\u68c0\u6d4b\u7684F1\u5206\u6570\u4ece0.84\u63d0\u5347\u81f30.92\u3002", "conclusion": "ViTs\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u5728\u5317\u6781\u9065\u611f\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u5904\u7406\u590d\u6742\u5149\u8c31\u7279\u5f81\u548c\u533a\u57df\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106", "abs": "https://arxiv.org/abs/2506.03106", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "38 pages", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCritique-GRPO\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u6570\u503c\u53cd\u9988\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u4ec5\u4f9d\u8d56\u6570\u503c\u53cd\u9988\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3001\u81ea\u53cd\u601d\u6548\u679c\u6709\u9650\u548c\u6301\u7eed\u5931\u8d25\u95ee\u9898\u3002", "method": "\u63d0\u51faCritique-GRPO\u6846\u67b6\uff0c\u6574\u5408\u81ea\u7136\u8bed\u8a00\u548c\u6570\u503c\u53cd\u9988\uff0c\u652f\u6301\u6a21\u578b\u4ece\u521d\u59cb\u54cd\u5e94\u548c\u53cd\u9988\u4e2d\u540c\u65f6\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cCritique-GRPO\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e73\u5747\u5f97\u5206\u63d0\u53474.5%-5%\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u80fd\u6709\u6548\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u4e14\u63a2\u7d22\u7b56\u7565\u7684\u6548\u7387\u4e0e\u71b5\u548c\u54cd\u5e94\u957f\u5ea6\u65e0\u5173\u3002"}}
{"id": "2506.02875", "pdf": "https://arxiv.org/pdf/2506.02875", "abs": "https://arxiv.org/abs/2506.02875", "authors": ["Xiaohong Liu", "Xiongkuo Min", "Qiang Hu", "Xiaoyun Zhang", "Jie Guo", "Guangtao Zhai", "Shushi Wang", "Yingjie Zhou", "Lu Liu", "Jingxin Li", "Liu Yang", "Farong Wen", "Li Xu", "Yanwei Jiang", "Xilei Zhu", "Chunyi Li", "Zicheng Zhang", "Huiyu Duan", "Xiele Wu", "Yixuan Gao", "Yuqin Cao", "Jun Jia", "Wei Sun", "Jiezhang Cao", "Radu Timofte", "Baojun Li", "Jiamian Huang", "Dan Luo", "Tao Liu", "Weixia Zhang", "Bingkun Zheng", "Junlin Chen", "Ruikai Zhou", "Meiya Chen", "Yu Wang", "Hao Jiang", "Xiantao Li", "Yuxiang Jiang", "Jun Tang", "Yimeng Zhao", "Bo Hu", "Zelu Qi", "Chaoyang Zhang", "Fei Zhao", "Ping Shi", "Lingzhi Fu", "Heng Cong", "Shuai He", "Rongyu Zhang", "Jiarong He", "Zongyao Hu", "Wei Luo", "Zihao Yu", "Fengbin Guan", "Yiting Lu", "Xin Li", "Zhibo Chen", "Mengjing Su", "Yi Wang", "Tuo Chen", "Chunxiao Li", "Shuaiyu Zhao", "Jiaxin Wen", "Chuyi Lin", "Sitong Liu", "Ningxin Chu", "Jing Wan", "Yu Zhou", "Baoying Chen", "Jishen Zeng", "Jiarui Liu", "Xianjin Liu", "Xin Chen", "Lanzhi Zhou", "Hangyu Li", "You Han", "Bibo Xiang", "Zhenjie Liu", "Jianzhang Lu", "Jialin Gui", "Renjie Lu", "Shangfei Wang", "Donghao Zhou", "Jingyu Lin", "Quanjian Song", "Jiancheng Huang", "Yufeng Yang", "Changwei Wang", "Shupeng Zhong", "Yang Yang", "Lihuo He", "Jia Liu", "Yuting Xing", "Tida Fang", "Yuchun Jin"], "title": "NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 XGC Quality Assessment Challenge Report. arXiv admin note:\n  text overlap with arXiv:2404.16687", "summary": "This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which\nwill be held in conjunction with the New Trends in Image Restoration and\nEnhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major\nchallenge in the field of video and talking head processing. The challenge is\ndivided into three tracks, including user generated video, AI generated video\nand talking head. The user-generated video track uses the FineVD-GC, which\ncontains 6,284 user generated videos. The user-generated video track has a\ntotal of 125 registered participants. A total of 242 submissions are received\nin the development phase, and 136 submissions are received in the test phase.\nFinally, 5 participating teams submitted their models and fact sheets. The AI\ngenerated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated\nVideos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of\n133 participants have registered in this track. A total of 396 submissions are\nreceived in the development phase, and 226 submissions are received in the test\nphase. Finally, 6 participating teams submitted their models and fact sheets.\nThe talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D\ntalking heads. A total of 89 participants have registered in this track. A\ntotal of 225 submissions are received in the development phase, and 118\nsubmissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Each participating team in every track\nhas proposed a method that outperforms the baseline, which has contributed to\nthe development of fields in three tracks.", "AI": {"tldr": "NTIRE 2025 XGC\u6311\u6218\u8d5b\u5206\u4e3a\u4e09\u4e2a\u8d5b\u9053\uff1a\u7528\u6237\u751f\u6210\u89c6\u9891\u3001AI\u751f\u6210\u89c6\u9891\u548c\u8bf4\u8bdd\u5934\uff0c\u6bcf\u4e2a\u8d5b\u9053\u5747\u5438\u5f15\u4e86\u5927\u91cf\u53c2\u4e0e\u8005\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u548c\u8bf4\u8bdd\u5934\u5904\u7406\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\uff0c\u63a8\u52a8\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u6311\u6218\u8d5b\u5206\u4e3a\u4e09\u4e2a\u8d5b\u9053\uff0c\u5206\u522b\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff08FineVD-GC\u3001Q-Eval-Video\u3001THQA-NTIRE\uff09\uff0c\u53c2\u4e0e\u8005\u63d0\u4ea4\u6a21\u578b\u548c\u4e8b\u5b9e\u8868\u3002", "result": "\u6bcf\u4e2a\u8d5b\u9053\u7684\u53c2\u4e0e\u8005\u5747\u63d0\u51fa\u4e86\u4f18\u4e8e\u57fa\u7ebf\u7684\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "NTIRE 2025 XGC\u6311\u6218\u8d5b\u6210\u529f\u4fc3\u8fdb\u4e86\u89c6\u9891\u548c\u8bf4\u8bdd\u5934\u5904\u7406\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.03122", "pdf": "https://arxiv.org/pdf/2506.03122", "abs": "https://arxiv.org/abs/2506.03122", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "categories": ["cs.CL"], "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "AI": {"tldr": "AUTOCIRCUIT-RL\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLM\u751f\u6210\u6a21\u62df\u7535\u8def\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7535\u8def\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u62d3\u6251\u5408\u6210\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5e9e\u5927\u4e14\u7ea6\u675f\u4e25\u683c\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u6307\u4ee4\u8c03\u4f18\uff08LLM\u5b66\u4e60\u4ece\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u62d3\u6251\uff09\u548cRL\u7ec6\u5316\uff08\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u4f18\u5316\u751f\u6210\u7ed3\u679c\uff09\u3002", "result": "AUTOCIRCUIT-RL\u751f\u6210\u7684\u6709\u6548\u7535\u8def\u6bd4\u57fa\u7ebf\u591a12%\uff0c\u6548\u7387\u63d0\u534714%\uff0c\u91cd\u590d\u751f\u6210\u7387\u964d\u4f4e38%\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u6210\u529f\u7387\u8d8560%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u7535\u8def\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6807\u5fd7\u7740AI\u9a71\u52a8\u7535\u8def\u8bbe\u8ba1\u7684\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2506.02882", "pdf": "https://arxiv.org/pdf/2506.02882", "abs": "https://arxiv.org/abs/2506.02882", "authors": ["Sohyun Lee", "Yeho Kwon", "Lukas Hoyer", "Suha Kwak"], "title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGaRA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6743\u91cd\u77e9\u9635\u7684\u79e9\u6765\u589e\u5f3aSegment Anything Model\uff08SAM\uff09\u5bf9\u8f93\u5165\u9000\u5316\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\uff09\u4e2d\uff0c\u63d0\u5347SAM\u5bf9\u8f93\u5165\u9000\u5316\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff08GaRA\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6743\u91cd\u77e9\u9635\u7684\u79e9\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u548c\u8f93\u5165\u611f\u77e5\u7684\u9c81\u68d2\u5316\u3002", "result": "GaRA-SAM\u5728\u6240\u6709\u9c81\u68d2\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728ACDC\u6570\u636e\u96c6\u4e0aIoU\u5f97\u5206\u63d0\u5347\u9ad8\u8fbe21.3%\u3002", "conclusion": "GaRA\u65b9\u6cd5\u5728\u4e0d\u727a\u7272SAM\u6cdb\u5316\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5bf9\u8f93\u5165\u9000\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136", "abs": "https://arxiv.org/abs/2506.03136", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "AI": {"tldr": "CURE\u662f\u4e00\u4e2a\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u4ee3\u7801\u548c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u80fd\u529b\uff0c\u65e0\u9700\u771f\u5b9e\u4ee3\u7801\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u548c\u6d4b\u8bd5\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9e\u4ee3\u7801\u76d1\u7763\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002CURE\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u76f4\u63a5\u4ece\u9519\u8bef\u4e2d\u63d0\u5347\u80fd\u529b\u3002", "method": "\u63d0\u51faCURE\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e13\u7528\u5956\u52b1\u673a\u5236\uff0c\u534f\u540c\u8fdb\u5316\u4ee3\u7801\u548c\u6d4b\u8bd5\u751f\u6210\u80fd\u529b\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "ReasonFlux-Coder-7B\u548c14B\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\u3002", "conclusion": "CURE\u4e0d\u4ec5\u63d0\u5347\u4ee3\u7801\u751f\u6210\u548c\u6d4b\u8bd5\u80fd\u529b\uff0c\u8fd8\u53ef\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02891", "pdf": "https://arxiv.org/pdf/2506.02891", "abs": "https://arxiv.org/abs/2506.02891", "authors": ["Jiewen Hu", "Leena Mathur", "Paul Pu Liang", "Louis-Philippe Morency"], "title": "OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial Behavior Analysis", "categories": ["cs.CV"], "comment": "IEEE FG 2025, \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. Permission from IEEE must be obtained for all other uses, in\n  any current or future media, including reprinting/republishing this material\n  for advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work", "summary": "In recent years, there has been increasing interest in automatic facial\nbehavior analysis systems from computing communities such as vision, multimodal\ninteraction, robotics, and affective computing. Building upon the widespread\nutility of prior open-source facial analysis systems, we introduce OpenFace\n3.0, an open-source toolkit capable of facial landmark detection, facial action\nunit detection, eye-gaze estimation, and facial emotion recognition. OpenFace\n3.0 contributes a lightweight unified model for facial analysis, trained with a\nmulti-task architecture across diverse populations, head poses, lighting\nconditions, video resolutions, and facial analysis tasks. By leveraging the\nbenefits of parameter sharing through a unified model and training paradigm,\nOpenFace 3.0 exhibits improvements in prediction performance, inference speed,\nand memory efficiency over similar toolkits and rivals state-of-the-art models.\nOpenFace 3.0 can be installed and run with a single line of code and operate in\nreal-time without specialized hardware. OpenFace 3.0 code for training models\nand running the system is freely available for research purposes and supports\ncontributions from the community.", "AI": {"tldr": "OpenFace 3.0\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u9762\u90e8\u884c\u4e3a\u5206\u6790\uff0c\u5305\u62ec\u9762\u90e8\u6807\u5fd7\u70b9\u68c0\u6d4b\u3001\u52a8\u4f5c\u5355\u5143\u68c0\u6d4b\u3001\u89c6\u7ebf\u4f30\u8ba1\u548c\u60c5\u611f\u8bc6\u522b\u3002\u5b83\u901a\u8fc7\u591a\u4efb\u52a1\u67b6\u6784\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3001\u901f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u8ba1\u7b97\u9886\u57df\u5bf9\u9762\u90e8\u884c\u4e3a\u5206\u6790\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u52a0\uff0cOpenFace 3.0\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u5305\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u67b6\u6784\u8bad\u7ec3\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u4eba\u7fa4\u3001\u5934\u90e8\u59ff\u6001\u3001\u5149\u7167\u6761\u4ef6\u548c\u89c6\u9891\u5206\u8fa8\u7387\u3002", "result": "OpenFace 3.0\u5728\u9884\u6d4b\u6027\u80fd\u3001\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u4e0a\u4f18\u4e8e\u540c\u7c7b\u5de5\u5177\u5305\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "OpenFace 3.0\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u6613\u7528\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u7814\u7a76\uff0c\u5e76\u652f\u6301\u793e\u533a\u8d21\u732e\u3002"}}
{"id": "2506.03143", "pdf": "https://arxiv.org/pdf/2506.03143", "abs": "https://arxiv.org/abs/2506.03143", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.", "AI": {"tldr": "GUI-Actor\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65e0\u5750\u6807GUI\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u9a8c\u8bc1\u5668\u63d0\u5347\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u5b58\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u5f31\u3001\u65e0\u6cd5\u5904\u7406\u6a21\u7cca\u76d1\u7763\u76ee\u6807\u4ee5\u53ca\u5750\u6807\u4e0e\u89c6\u89c9\u7279\u5f81\u7c92\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "GUI-Actor\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u52a8\u4f5c\u5934\uff0c\u5b66\u4e60\u5c06\u4e13\u7528<ACTOR>\u4ee4\u724c\u4e0e\u76f8\u5173\u89c6\u89c9\u8865\u4e01\u4ee4\u724c\u5bf9\u9f50\uff0c\u5e76\u8bbe\u8ba1\u9a8c\u8bc1\u5668\u7b5b\u9009\u6700\u4f73\u52a8\u4f5c\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGUI-Actor\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u901a\u8fc7\u5fae\u8c03\u4ec5\u52a8\u4f5c\u5934\u5373\u53ef\u8fbe\u5230\u9ad8\u6027\u80fd\u3002", "conclusion": "GUI-Actor\u4e3aVLM\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u901a\u7528\u6027\u3002"}}
{"id": "2506.02893", "pdf": "https://arxiv.org/pdf/2506.02893", "abs": "https://arxiv.org/abs/2506.02893", "authors": ["Jonathan Astermark", "Anders Heyden", "Viktor Larsson"], "title": "Dense Match Summarization for Faster Two-view Estimation", "categories": ["cs.CV"], "comment": "Accepted to Computer Vision and Pattern Recognition (CVPR) 2025", "summary": "In this paper, we speed up robust two-view relative pose from dense\ncorrespondences. Previous work has shown that dense matchers can significantly\nimprove both accuracy and robustness in the resulting pose. However, the large\nnumber of matches comes with a significantly increased runtime during robust\nestimation in RANSAC. To avoid this, we propose an efficient match\nsummarization scheme which provides comparable accuracy to using the full set\nof dense matches, while having 10-100x faster runtime. We validate our approach\non standard benchmark datasets together with multiple state-of-the-art dense\nmatchers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5339\u914d\u6458\u8981\u65b9\u6848\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5bc6\u96c6\u5339\u914d\u4e0b\u7684\u4e24\u89c6\u56fe\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5b8c\u6574\u5bc6\u96c6\u5339\u914d\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5bc6\u96c6\u5339\u914d\u867d\u80fd\u63d0\u9ad8\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u5927\u91cf\u5339\u914d\u70b9\u5bfc\u81f4RANSAC\u4e2d\u7684\u9c81\u68d2\u4f30\u8ba1\u8fd0\u884c\u65f6\u95f4\u663e\u8457\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5339\u914d\u6458\u8981\u65b9\u6848\uff0c\u4ee5\u51cf\u5c11\u5339\u914d\u70b9\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u8fd0\u884c\u901f\u5ea6\u6bd4\u5b8c\u6574\u5bc6\u96c6\u5339\u914d\u5feb10-100\u500d\uff0c\u4e14\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u5339\u914d\u5e26\u6765\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2506.03145", "pdf": "https://arxiv.org/pdf/2506.03145", "abs": "https://arxiv.org/abs/2506.03145", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u795e\u7ecf\u79d1\u5b66\u672c\u4f53\u8bba\u548c\u6587\u672c\u5d4c\u5165\u4ece\u672a\u6807\u8bb0\u7684\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u8bed\u6599\u5e93\u4e2d\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u795e\u7ecf\u79d1\u5b66\u6587\u732e\u4fe1\u606f\u5206\u6563\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u4fe1\u606f\uff0c\u4e14\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u901a\u5e38\u9700\u8981\u6807\u6ce8\u6570\u636e\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u795e\u7ecf\u79d1\u5b66\u672c\u4f53\u8bba\u548c\u6587\u672c\u5d4c\u5165\u4ece\u672a\u6807\u8bb0\u7684\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u8bed\u6599\u5e93\u4e2d\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5f15\u5165\u5b9e\u4f53\u589e\u5f3a\u7684\u4fe1\u606f\u68c0\u7d22\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u4f53\u63d0\u53d6\u4e0aF1\u5f97\u5206\u4e3a0.84\uff0c\u4e14\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u83b7\u5f97\u7684\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e8654%\u4ee5\u4e0a\u95ee\u9898\u7684\u56de\u7b54\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u53d1\u73b0\u548c\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u3002"}}
{"id": "2506.02896", "pdf": "https://arxiv.org/pdf/2506.02896", "abs": "https://arxiv.org/abs/2506.02896", "authors": ["Adam Pardyl", "Dominik Matuszek", "Mateusz Przebieracz", "Marek Cygan", "Bartosz Zieli\u0144ski", "Maciej Wo\u0142czyk"], "title": "FlySearch: Exploring how vision-language models explore", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "The real world is messy and unstructured. Uncovering critical information\noften requires active, goal-driven exploration. It remains to be seen whether\nVision-Language Models (VLMs), which recently emerged as a popular zero-shot\ntool in many difficult tasks, can operate effectively in such conditions. In\nthis paper, we answer this question by introducing FlySearch, a 3D, outdoor,\nphotorealistic environment for searching and navigating to objects in complex\nscenes. We define three sets of scenarios with varying difficulty and observe\nthat state-of-the-art VLMs cannot reliably solve even the simplest exploration\ntasks, with the gap to human performance increasing as the tasks get harder. We\nidentify a set of central causes, ranging from vision hallucination, through\ncontext misunderstanding, to task planning failures, and we show that some of\nthem can be addressed by finetuning. We publicly release the benchmark,\nscenarios, and the underlying codebase.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728\u7b80\u5355\u63a2\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0e\u4eba\u7c7b\u5dee\u8ddd\u663e\u8457\u3002", "motivation": "\u7814\u7a76VLMs\u5728\u771f\u5b9e\u3001\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u5176\u5728\u4e3b\u52a8\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u7a7a\u767d\u3002", "method": "\u63d0\u51faFlySearch\uff0c\u4e00\u4e2a3D\u6237\u5916\u903c\u771f\u73af\u5883\uff0c\u7528\u4e8e\u6d4b\u8bd5VLMs\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u641c\u7d22\u548c\u5bfc\u822a\u80fd\u529b\uff0c\u5e76\u5b9a\u4e49\u4e09\u79cd\u96be\u5ea6\u4efb\u52a1\u3002", "result": "VLMs\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u53ef\u9760\uff0c\u4e0e\u4eba\u7c7b\u5dee\u8ddd\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u6269\u5927\uff1b\u53d1\u73b0\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u8bef\u89e3\u548c\u4efb\u52a1\u89c4\u5212\u5931\u8d25\u662f\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "\u90e8\u5206\u95ee\u9898\u53ef\u901a\u8fc7\u5fae\u8c03\u89e3\u51b3\uff0c\u5e76\u516c\u5f00\u4e86\u57fa\u51c6\u3001\u573a\u666f\u548c\u4ee3\u7801\u5e93\u3002"}}
{"id": "2506.03149", "pdf": "https://arxiv.org/pdf/2506.03149", "abs": "https://arxiv.org/abs/2506.03149", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "title": "Causal Estimation of Tokenisation Bias", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ACL 2025", "summary": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u5206\u8bcd\u5668\u5bf9\u5b57\u7b26\u6982\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u5206\u8bcd\u504f\u5dee\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5206\u8bcd\u9009\u62e9\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u5b50\u8bcd\u5e8f\u5217\u8bad\u7ec3\uff0c\u4f46\u6700\u7ec8\u5b9a\u4e49\u7684\u662f\u5b57\u7b26\u6982\u7387\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u5206\u8bcd\u5668\u7684\u9009\u62e9\u4e0d\u5e94\u5f71\u54cd\u5b57\u7b26\u6982\u7387\uff0c\u4f46\u5b9e\u9645\u4e0a\u5b58\u5728\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u91cf\u5316\u8fd9\u79cd\u5206\u8bcd\u504f\u5dee\u3002", "method": "\u5c06\u5206\u8bcd\u504f\u5dee\u5b9a\u4e49\u4e3a\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u5229\u7528\u56de\u5f52\u4e0d\u8fde\u7eed\u6027\u8bbe\u8ba1\u8fdb\u884c\u4f30\u8ba1\u3002\u901a\u8fc7\u6bd4\u8f83\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\u4e2d\u6392\u540d\u76f8\u8fd1\u7684\u5b50\u8bcd\uff0c\u91cf\u5316\u5176\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5206\u8bcd\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff0c\u5b50\u8bcd\u7684\u5b58\u5728\u53ef\u80fd\u4f7f\u5b57\u7b26\u6982\u7387\u589e\u52a0\u9ad8\u8fbe17\u500d\u3002", "conclusion": "\u5206\u8bcd\u5668\u662f\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u9009\u62e9\uff0c\u5176\u504f\u5dee\u5bf9\u6a21\u578b\u8f93\u51fa\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2506.02914", "pdf": "https://arxiv.org/pdf/2506.02914", "abs": "https://arxiv.org/abs/2506.02914", "authors": ["Yechi Ma", "Wei Hua", "Shu Kong"], "title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection", "categories": ["cs.CV"], "comment": null, "summary": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To study methods for\nfacilitating data annotation, we introduce a new benchmark AnnoGuide:\nAuto-Annotation from Annotation Guidelines. It aims to evaluate automated\nmethods for data annotation directly from expert-defined annotation guidelines,\neliminating the need for manual labeling. As a case study, we repurpose the\nwell-established nuScenes dataset, commonly used in autonomous driving\nresearch, which provides comprehensive annotation guidelines for labeling LiDAR\npoint clouds with 3D cuboids across 18 object classes. These guidelines include\na few visual examples and textual descriptions, but no labeled 3D cuboids in\nLiDAR data, making this a novel task of multi-modal few-shot 3D detection\nwithout 3D annotations. The advances of powerful foundation models (FMs) make\nAnnoGuide especially timely, as FMs offer promising tools to tackle its\nchallenges. We employ a conceptually straightforward pipeline that (1) utilizes\nopen-source FMs for object detection and segmentation in RGB images, (2)\nprojects 2D detections into 3D using known camera poses, and (3) clusters LiDAR\npoints within the frustum of each 2D detection to generate a 3D cuboid.\nStarting with a non-learned solution that leverages off-the-shelf FMs, we\nprogressively refine key components and achieve significant performance\nimprovements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our\nresults highlight that AnnoGuide remains an open and challenging problem,\nunderscoring the urgent need for developing LiDAR-based FMs. We release our\ncode and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u57fa\u51c6AnnoGuide\uff0c\u65e8\u5728\u901a\u8fc7\u4e13\u5bb6\u5b9a\u4e49\u7684\u6807\u6ce8\u6307\u5357\u81ea\u52a8\u5b8c\u6210\u6570\u636e\u6807\u6ce8\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\u3002\u4ee5nuScenes\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u591a\u6a21\u6001\u5c11\u6837\u672c3D\u68c0\u6d4b\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6570\u636e\u6807\u6ce8\u662f\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u5173\u952e\u4f46\u7e41\u7410\u7684\u6b65\u9aa4\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u76f4\u63a5\u4ece\u6807\u6ce8\u6307\u5357\u751f\u6210\u6807\u6ce8\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u91c7\u7528\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u8fdb\u884cRGB\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u4e0e\u5206\u5272\uff0c\u5c062D\u68c0\u6d4b\u6295\u5f71\u52303D\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u805a\u7c7bLiDAR\u70b9\u751f\u62103D\u6807\u6ce8\u3002\u9010\u6b65\u4f18\u5316\u5173\u952e\u7ec4\u4ef6\u63d0\u5347\u6027\u80fd\u3002", "result": "3D\u68c0\u6d4bmAP\u4ece12.1\u63d0\u5347\u81f321.9\uff0c\u4f46\u4ecd\u8868\u660eAnnoGuide\u662f\u4e00\u4e2a\u5f00\u653e\u4e14\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u9700\u8fdb\u4e00\u6b65\u53d1\u5c55LiDAR\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "AnnoGuide\u4e3a\u81ea\u52a8\u5316\u6570\u636e\u6807\u6ce8\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3LiDAR\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2506.01704", "pdf": "https://arxiv.org/pdf/2506.01704", "abs": "https://arxiv.org/abs/2506.01704", "authors": ["Jiongnan Liu", "Zhicheng Dou", "Ning Hu", "Chenyan Xiong"], "title": "Generate, Not Recommend: Personalized Multimodal Content Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "To address the challenge of information overload from massive web contents,\nrecommender systems are widely applied to retrieve and present personalized\nresults for users. However, recommendation tasks are inherently constrained to\nfiltering existing items and lack the ability to generate novel concepts,\nlimiting their capacity to fully satisfy user demands and preferences. In this\npaper, we propose a new paradigm that goes beyond content filtering and\nselecting: directly generating personalized items in a multimodal form, such as\nimages, tailored to individual users. To accomplish this, we leverage\nany-to-any Large Multimodal Models (LMMs) and train them in both supervised\nfine-tuning and online reinforcement learning strategy to equip them with the\nability to yield tailored next items for users. Experiments on two benchmark\ndatasets and user study confirm the efficacy of the proposed method. Notably,\nthe generated images not only align well with users' historical preferences but\nalso exhibit relevance to their potential future interests.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u76f4\u63a5\u4e3a\u7528\u6237\u751f\u6210\u4e2a\u6027\u5316\u5185\u5bb9\uff08\u5982\u56fe\u50cf\uff09\uff0c\u800c\u975e\u4ec5\u8fc7\u6ee4\u73b0\u6709\u5185\u5bb9\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u5c40\u9650\u4e8e\u8fc7\u6ee4\u73b0\u6709\u5185\u5bb9\uff0c\u65e0\u6cd5\u751f\u6210\u65b0\u9896\u6982\u5ff5\uff0c\u96be\u4ee5\u5b8c\u5168\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u548c\u504f\u597d\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMMs\uff09\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u5185\u5bb9\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u56fe\u50cf\u4e0d\u4ec5\u7b26\u5408\u7528\u6237\u5386\u53f2\u504f\u597d\uff0c\u8fd8\u4e0e\u5176\u6f5c\u5728\u5174\u8da3\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2506.02938", "pdf": "https://arxiv.org/pdf/2506.02938", "abs": "https://arxiv.org/abs/2506.02938", "authors": ["Xuhui Chen", "Fei Hou", "Wencheng Wang", "Hong Qin", "Ying He"], "title": "MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Unsigned distance fields (UDFs) are widely used in 3D deep learning due to\ntheir ability to represent shapes with arbitrary topology. While prior work has\nlargely focused on learning UDFs from point clouds or multi-view images,\nextracting meshes from UDFs remains challenging, as the learned fields rarely\nattain exact zero distances. A common workaround is to reconstruct signed\ndistance fields (SDFs) locally from UDFs to enable surface extraction via\nMarching Cubes. However, this often introduces topological artifacts such as\nholes or spurious components. Moreover, local SDFs are inherently incapable of\nrepresenting non-manifold geometry, leading to complete failure in such cases.\nTo address this gap, we propose MIND (Material Interface from Non-manifold\nDistance fields), a novel algorithm for generating material interfaces directly\nfrom UDFs, enabling non-manifold mesh extraction from a global perspective. The\ncore of our method lies in deriving a meaningful spatial partitioning from the\nUDF, where the target surface emerges as the interface between distinct\nregions. We begin by computing a two-signed local field to distinguish the two\nsides of manifold patches, and then extend this to a multi-labeled global field\ncapable of separating all sides of a non-manifold structure. By combining this\nmulti-labeled field with the input UDF, we construct material interfaces that\nsupport non-manifold mesh extraction via a multi-labeled Marching Cubes\nalgorithm. Extensive experiments on UDFs generated from diverse data sources,\nincluding point cloud reconstruction, multi-view reconstruction, and medial\naxis transforms, demonstrate that our approach robustly handles complex\nnon-manifold surfaces and significantly outperforms existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIND\u7684\u65b0\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08UDFs\uff09\u751f\u6210\u6750\u6599\u754c\u9762\uff0c\u652f\u6301\u975e\u6d41\u5f62\u7f51\u683c\u63d0\u53d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4eceUDFs\u63d0\u53d6\u7f51\u683c\u65f6\u5b58\u5728\u62d3\u6251\u7f3a\u9677\uff0c\u4e14\u65e0\u6cd5\u5904\u7406\u975e\u6d41\u5f62\u51e0\u4f55\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u591a\u6807\u7b7e\u5168\u5c40\u573a\uff0c\u7ed3\u5408UDFs\u6784\u5efa\u6750\u6599\u754c\u9762\uff0c\u652f\u6301\u975e\u6d41\u5f62\u7f51\u683c\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMIND\u80fd\u7a33\u5065\u5904\u7406\u590d\u6742\u975e\u6d41\u5f62\u8868\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MIND\u4e3aUDFs\u7684\u975e\u6d41\u5f62\u7f51\u683c\u63d0\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.01963", "pdf": "https://arxiv.org/pdf/2506.01963", "abs": "https://arxiv.org/abs/2506.01963", "authors": ["Andrew Kiruluta", "Preethi Raju", "Priscilla Burity"], "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We present a novel non attention based architecture for large language models\n(LLMs) that efficiently handles very long context windows, on the order of\nhundreds of thousands to potentially millions of tokens. Unlike traditional\nTransformer designs, which suffer from quadratic memory and computation\noverload due to the nature of the self attention mechanism, our model avoids\ntoken to token attention entirely. Instead, it combines the following\ncomplementary components: State Space blocks (inspired by S4) that learn\ncontinuous time convolution kernels and scale near linearly with sequence\nlength, Multi Resolution Convolution layers that capture local context at\ndifferent dilation levels, a lightweight Recurrent Supervisor to maintain a\nglobal hidden state across sequential chunks, and Retrieval Augmented External\nMemory that stores and retrieves high-level chunk embeddings without\nreintroducing quadratic operations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u9ad8\u6548\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u907f\u514d\u4f20\u7edfTransformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfTransformer\u56e0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u5185\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e8c\u6b21\u589e\u957f\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u3002", "method": "\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u5757\uff08S4\u542f\u53d1\uff09\u3001\u591a\u5206\u8fa8\u7387\u5377\u79ef\u5c42\u3001\u8f7b\u91cf\u7ea7\u5faa\u73af\u76d1\u7763\u5668\u548c\u68c0\u7d22\u589e\u5f3a\u5916\u90e8\u8bb0\u5fc6\uff0c\u907f\u514dtoken\u95f4\u6ce8\u610f\u529b\u3002", "result": "\u6a21\u578b\u80fd\u9ad8\u6548\u5904\u7406\u6570\u5341\u4e07\u81f3\u6570\u767e\u4e07token\u7684\u4e0a\u4e0b\u6587\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u63a5\u8fd1\u7ebf\u6027\u3002", "conclusion": "\u65b0\u67b6\u6784\u4e3a\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02964", "pdf": "https://arxiv.org/pdf/2506.02964", "abs": "https://arxiv.org/abs/2506.02964", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Eric Eaton", "Daniel A. Hashimoto"], "title": "FORLA:Federated Object-centric Representation Learning with Slot Attention", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages, 6 figures", "summary": "Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.", "AI": {"tldr": "FORLA\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u7684slot attention\u5b66\u4e60\u8de8\u5ba2\u6237\u7aef\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u548c\u7279\u5f81\u9002\u5e94\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5728\u5f02\u6784\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u9ad8\u6548\u89c6\u89c9\u8868\u793a\u7684\u6311\u6218\uff0c\u9700\u8054\u5408\u8de8\u5ba2\u6237\u7aef\u4fe1\u606f\u7279\u5f81\u5e76\u89e3\u8026\u9886\u57df\u7279\u5b9a\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7279\u5f81\u9002\u914d\u5668\u548cslot attention\u6a21\u5757\uff0c\u8bbe\u8ba1\u53cc\u5206\u652f\u5e08\u751f\u67b6\u6784\u4f18\u5316\u9002\u914d\u5668\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\uff0c\u5b66\u4e60\u5230\u7d27\u51d1\u4e14\u901a\u7528\u7684\u8de8\u57df\u8868\u793a\u3002", "conclusion": "\u8054\u90a6slot attention\u662f\u5206\u5e03\u5f0f\u6982\u5ff5\u4e0b\u53ef\u6269\u5c55\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.01967", "pdf": "https://arxiv.org/pdf/2506.01967", "abs": "https://arxiv.org/abs/2506.01967", "authors": ["Patrik Czak\u00f3", "G\u00e1bor Kert\u00e9sz", "S\u00e1ndor Sz\u00e9n\u00e1si"], "title": "Turning LLM Activations Quantization-Friendly", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 5 figures. Accepted to SACI 2025 conference proceedings", "summary": "Quantization effectively reduces the serving costs of Large Language Models\n(LLMs) by speeding up data movement through compressed parameters and enabling\nfaster operations via integer arithmetic. However, activating integer\narithmetic requires quantizing both weights and activations, which poses\nchallenges due to the significant outliers in LLMs that increase quantization\nerror. In this work, we investigate these outliers with an emphasis on their\neffect on layer-wise quantization error, then examine how smoothing and\nrotation transform the observed values. Our primary contributions include\nintroducing a new metric to measure and visualize quantization difficulty based\non channel magnitudes, as well as proposing a hybrid approach that applies\nchannel-wise scaling before rotation, supported by a mathematical formulation\nof its benefits.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\u548c\u6df7\u5408\u91cf\u5316\u65b9\u6cd5\u4ee5\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u3002", "motivation": "\u91cf\u5316\u53ef\u4ee5\u964d\u4f4eLLMs\u7684\u670d\u52a1\u6210\u672c\uff0c\u4f46\u6fc0\u6d3b\u6574\u6570\u8fd0\u7b97\u9700\u8981\u91cf\u5316\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\uff0c\u800cLLMs\u4e2d\u7684\u5f02\u5e38\u503c\u4f1a\u589e\u52a0\u91cf\u5316\u8bef\u5dee\u3002", "method": "\u7814\u7a76\u5f02\u5e38\u503c\u5bf9\u5206\u5c42\u91cf\u5316\u8bef\u5dee\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u57fa\u4e8e\u901a\u9053\u5e45\u5ea6\u7684\u65b0\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u901a\u9053\u7f29\u653e\u548c\u65cb\u8f6c\u7684\u6df7\u5408\u91cf\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u5b66\u516c\u5f0f\u8bc1\u660e\u4e86\u6df7\u5408\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u91cf\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11LLMs\u4e2d\u7684\u91cf\u5316\u8bef\u5dee\uff0c\u4e3a\u9ad8\u6548\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02975", "pdf": "https://arxiv.org/pdf/2506.02975", "abs": "https://arxiv.org/abs/2506.02975", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Zunnan Xu", "Zhaoyang Zhang", "Yixiao Ge", "Xiu Li", "Ying Shan"], "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\uff0c\u6784\u5efa\u5355\u4e00Transformer\u6a21\u578b\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9884\u70ed\u7b56\u7565\u548c\u7279\u5f81\u9884\u7f29\u653e\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u9ad8\u6027\u80fd\u7684HaploOmni\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u9010\u6e10\u4ece\u5206\u79bb\u7ec4\u4ef6\u53d1\u5c55\u4e3a\u7edf\u4e00\u5355\u6a21\u578b\u6846\u67b6\uff0c\u4f46\u8de8\u6a21\u6001\u517c\u5bb9\u6027\u548c\u8bad\u7ec3\u6548\u7387\u4ecd\u662f\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u9884\u70ed\u7b56\u7565\u548c\u7279\u5f81\u9884\u7f29\u653e\u6280\u672f\uff0c\u7ed3\u5408\u591a\u6a21\u6001AdaLN\uff0c\u6784\u5efaHaploOmni\u6a21\u578b\u3002", "result": "HaploOmni\u5728\u6709\u9650\u8bad\u7ec3\u6210\u672c\u4e0b\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4e0e\u751f\u6210\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HaploOmni\u8bc1\u660e\u4e86\u7edf\u4e00\u5355\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u7ade\u4e89\u529b\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998", "abs": "https://arxiv.org/abs/2506.01998", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u975e\u4ee3\u8bcd\u81ea\u6211\u6307\u79f0\uff08NPSR\uff09\u548c\u58f0\u97f3\u5bf9\u793e\u4f1a\u8eab\u4efd\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u58f0\u97f3\u6027\u522b\u5316\u660e\u663e\uff0c\u4f46\u67d0\u4e9b\u81ea\u6211\u6307\u79f0\u53ef\u907f\u514d\u6027\u522b\u5316\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u62df\u4eba\u5316\u4e2d\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5c24\u5176\u662f\u975e\u4ee3\u8bcd\u81ea\u6211\u6307\u79f0\u548c\u58f0\u97f3\u5bf9\u793e\u4f1a\u8eab\u4efd\u611f\u77e5\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u7814\u7a76\uff0c204\u540d\u65e5\u672c\u53c2\u4e0e\u8005\u8bc4\u4f30\u4e09\u79cdChatGPT\u58f0\u97f3\u548c\u4e03\u79cd\u81ea\u6211\u6307\u79f0\u3002", "result": "\u58f0\u97f3\u6027\u522b\u5316\u660e\u663e\uff0c\u67d0\u4e9b\u81ea\u6211\u6307\u79f0\uff08\u5982boku\u548cwatakushi\uff09\u80fd\u907f\u514d\u6027\u522b\u5316\uff0c\u4e14\u5e74\u9f84\u548c\u6b63\u5f0f\u5ea6\u611f\u77e5\u4e0e\u6027\u522b\u5316\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4ee3\u7406\u8eab\u4efd\u611f\u77e5\u7684\u590d\u6742\u6027\uff0c\u63d0\u5021\u5728\u8bed\u97f3\u4ee3\u7406\u8bbe\u8ba1\u4e2d\u8003\u8651\u4ea4\u53c9\u6027\u548c\u6587\u5316\u654f\u611f\u6027\u3002"}}
{"id": "2506.02976", "pdf": "https://arxiv.org/pdf/2506.02976", "abs": "https://arxiv.org/abs/2506.02976", "authors": ["Rachid Zeghlache", "Ikram Brahim", "Pierre-Henri Conze", "Mathieu Lamard", "Mohammed El Amine Lazouni", "Zineb Aziza Elaouaber", "Leila Ryma Lazouni", "Christopher Nielsen", "Ahmad O. Ahsan", "Matthias Wilms", "Nils D. Forkert", "Lovre Antonio Budimir", "Ivana Matovinovi\u0107", "Donik Vr\u0161nak", "Sven Lon\u010dari\u0107", "Philippe Zhang", "Weili Jiang", "Yihao Li", "Yiding Hao", "Markus Frohmann", "Patrick Binder", "Marcel Huber", "Taha Emre", "Teresa Finisterra Ara\u00fajo", "Marzieh Oghbaie", "Hrvoje Bogunovi\u0107", "Amerens A. Bekkers", "Nina M. van Liebergen", "Hugo J. Kuijf", "Abdul Qayyum", "Moona Mazher", "Steven A. Niederer", "Alberto J. Beltr\u00e1n-Carrero", "Juan J. G\u00f3mez-Valverde", "Javier Torresano-Rodr\u00edquez", "\u00c1lvaro Caballero-Sastre", "Mar\u00eda J. Ledesma Carbayo", "Yosuke Yamagishi", "Yi Ding", "Robin Peretzke", "Alexandra Ertl", "Maximilian Fischer", "Jessica K\u00e4chele", "Sofiane Zehar", "Karim Boukli Hacene", "Thomas Monfort", "B\u00e9atrice Cochener", "Mostafa El Habib Daho", "Anas-Alexis Benyoussef", "Gwenol\u00e9 Quellec"], "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge", "categories": ["cs.CV", "cs.AI"], "comment": "MARIO-MICCAI-CHALLENGE 2024", "summary": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2).", "AI": {"tldr": "MARIO\u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7OCT\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u548c\u76d1\u6d4bAMD\uff0c\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793aAI\u5728\u6d4b\u91cfAMD\u8fdb\u5c55\u65b9\u9762\u4e0e\u533b\u751f\u76f8\u5f53\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u6f14\u53d8\u3002", "motivation": "\u63a8\u52a8AMD\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u76d1\u6d4b\u6280\u672f\uff0c\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u96c6\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u6cd5\u56fd\u548c\u963f\u5c14\u53ca\u5229\u4e9a\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e24\u9879\u4efb\u52a1\uff1a\u5206\u7c7bOCT B\u626b\u63cf\u7684\u6f14\u53d8\u548c\u9884\u6d4bAMD\u672a\u6765\u6f14\u53d8\u3002", "result": "AI\u5728\u6d4b\u91cfAMD\u8fdb\u5c55\u65b9\u9762\u8868\u73b0\u4e0e\u533b\u751f\u76f8\u5f53\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u6f14\u53d8\u3002", "conclusion": "MARIO\u6311\u6218\u8d5b\u4e3aAMD\u76d1\u6d4b\u8bbe\u5b9a\u4e86\u57fa\u51c6\uff0cAI\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9884\u6d4b\u80fd\u529b\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2506.02057", "pdf": "https://arxiv.org/pdf/2506.02057", "abs": "https://arxiv.org/abs/2506.02057", "authors": ["David Sasu", "Kweku Andoh Yamoah", "Benedict Quartey", "Natalie Schluter"], "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Interspeech 2025", "summary": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u97f3\u97f5\u5f8b\u76f4\u63a5\u63a8\u65ad\u548c\u89e3\u6790\u6307\u4ee4\u610f\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u8ba1\u5212\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u8bed\u97f3\u8f6c\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u97f5\u5f8b\u7ebf\u7d22\uff0c\u5bfc\u81f4\u610f\u56fe\u89e3\u6790\u4e0d\u51c6\u786e\u3002", "method": "\u76f4\u63a5\u5229\u7528\u8bed\u97f3\u97f5\u5f8b\u63a8\u65ad\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u9884\u6d4b\u610f\u56fe\u96c6\u6210\u5230\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u6d88\u6b67\u548c\u9009\u62e9\u4efb\u52a1\u8ba1\u5212\u3002", "result": "\u65b9\u6cd5\u5728\u68c0\u6d4b\u6307\u79f0\u610f\u56fe\u65f6\u8fbe\u523095.79%\u7684\u51c6\u786e\u7387\uff0c\u5bf9\u6a21\u7cca\u6307\u4ee4\u7684\u4efb\u52a1\u8ba1\u5212\u9009\u62e9\u51c6\u786e\u7387\u4e3a71.96%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u6307\u4ee4\u89e3\u6790\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02981", "pdf": "https://arxiv.org/pdf/2506.02981", "abs": "https://arxiv.org/abs/2506.02981", "authors": ["Joonyeoup Kim", "Yu Yuan", "Xingguang Zhang", "Xijun Wang", "Stanley Chan"], "title": "Astrophotography turbulence mitigation via generative models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Photography is the cornerstone of modern astronomical and space research.\nHowever, most astronomical images captured by ground-based telescopes suffer\nfrom atmospheric turbulence, resulting in degraded imaging quality. While\nmulti-frame strategies like lucky imaging can mitigate some effects, they\ninvolve intensive data acquisition and complex manual processing. In this\npaper, we propose AstroDiff, a generative restoration method that leverages\nboth the high-quality generative priors and restoration capabilities of\ndiffusion models to mitigate atmospheric turbulence. Extensive experiments\ndemonstrate that AstroDiff outperforms existing state-of-the-art learning-based\nmethods in astronomical image turbulence mitigation, providing higher\nperceptual quality and better structural fidelity under severe turbulence\nconditions. Our code and additional results are available at\nhttps://web-six-kappa-66.vercel.app/", "AI": {"tldr": "AstroDiff\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u751f\u6210\u5148\u9a8c\u548c\u4fee\u590d\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u5929\u6587\u56fe\u50cf\u5728\u5927\u6c14\u6e4d\u6d41\u4e0b\u7684\u8d28\u91cf\u3002", "motivation": "\u5730\u9762\u671b\u8fdc\u955c\u62cd\u6444\u7684\u5929\u6587\u56fe\u50cf\u5e38\u53d7\u5927\u6c14\u6e4d\u6d41\u5f71\u54cd\uff0c\u5bfc\u81f4\u6210\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u5e78\u8fd0\u6210\u50cf\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u590d\u6742\u5904\u7406\u3002", "method": "\u63d0\u51faAstroDiff\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u548c\u4fee\u590d\u80fd\u529b\uff0c\u4fee\u590d\u6e4d\u6d41\u5f71\u54cd\u7684\u5929\u6587\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAstroDiff\u5728\u6e4d\u6d41\u4fee\u590d\u4e2d\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u611f\u77e5\u8d28\u91cf\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "conclusion": "AstroDiff\u4e3a\u5929\u6587\u56fe\u50cf\u6e4d\u6d41\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u7ed3\u679c\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.02059", "pdf": "https://arxiv.org/pdf/2506.02059", "abs": "https://arxiv.org/abs/2506.02059", "authors": ["Ziwei Gong", "Pengyuan Shi", "Kaan Donbekci", "Lin Ai", "Run Chen", "David Sasu", "Zehui Wu", "Julia Hirschberg"], "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Speech Emotion Recognition (SER) has seen significant progress with deep\nlearning, yet remains challenging for Low-Resource Languages (LRLs) due to the\nscarcity of annotated data. In this work, we explore unsupervised learning to\nimprove SER in low-resource settings. Specifically, we investigate contrastive\nlearning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised\napproaches to enhance cross-lingual generalization. Our methods achieve notable\nF1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,\ndemonstrating their effectiveness in LRLs. Additionally, we analyze model\nbehavior to provide insights on key factors influencing performance across\nlanguages, and also highlighting challenges in low-resource SER. This work\nprovides a foundation for developing more inclusive, explainable, and robust\nemotion recognition systems for underrepresented languages.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08LRLs\uff09\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5bf9\u6bd4\u5b66\u4e60\u548cBYOL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u548cBootstrap Your Own Latent\uff08BYOL\uff09\u4f5c\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u589e\u5f3a\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e4c\u5c14\u90fd\u8bed\u3001\u5fb7\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u4e2d\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u5347\u4e8610.6%\u300115.2%\u548c13.9%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u5177\u5305\u5bb9\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.03007", "pdf": "https://arxiv.org/pdf/2506.03007", "abs": "https://arxiv.org/abs/2506.03007", "authors": ["Jiarui Wang", "Huiyu Duan", "Juntong Wang", "Ziheng Jia", "Woo Yi Yang", "Xiaorong Zhu", "Yu Zhao", "Jiaying Qian", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative models, the realism of AI-generated\nimages has significantly improved, posing critical challenges for verifying\ndigital content authenticity. Current deepfake detection methods often depend\non datasets with limited generation models and content diversity that fail to\nkeep pace with the evolving complexity and increasing realism of the\nAI-generated content. Large multimodal models (LMMs), widely adopted in various\nvision tasks, have demonstrated strong zero-shot capabilities, yet their\npotential in deepfake detection remains largely unexplored. To bridge this gap,\nwe present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)\nbroad diversity, including 540,000 images across real, AI-edited, and\nAI-generated content, (ii) latest model, the fake images are generated by 12\nstate-of-the-art generation models, and (iii) bidirectional benchmarking and\nevaluating for both the detection accuracy of deepfake detectors and the\nevasion capability of generative models. Based on DFBench, we propose\n\\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a\ncombined probability strategy from multiple LMMs. MoA-DF achieves\nstate-of-the-art performance, further proving the effectiveness of leveraging\nLMMs for deepfake detection. Database and codes are publicly available at\nhttps://github.com/IntMeGroup/DFBench.", "AI": {"tldr": "DFBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6df1\u5ea6\u4f2a\u9020\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u56fe\u50cf\u548c\u6700\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u68c0\u6d4b\u5668\u548c\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002MoA-DF\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u7684\u6df7\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u56fe\u50cf\u7684\u903c\u771f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u6570\u5b57\u5185\u5bb9\u771f\u5b9e\u6027\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u548c\u6a21\u578b\u591a\u6837\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684AI\u751f\u6210\u5185\u5bb9\u3002", "method": "\u63d0\u51faDFBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b54\u4e07\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u771f\u5b9e\u3001AI\u7f16\u8f91\u548cAI\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u57fa\u4e8e12\u79cd\u6700\u65b0\u751f\u6210\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u63d0\u51faMoA-DF\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u7684\u6df7\u5408\u7b56\u7565\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "MoA-DF\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DFBench\u548cMoA-DF\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.02077", "pdf": "https://arxiv.org/pdf/2506.02077", "abs": "https://arxiv.org/abs/2506.02077", "authors": ["Yoonjun Cho", "Soeun Kim", "Dongjae Jeon", "Kyelim Lee", "Beomsoo Lee", "Albert No"], "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Decomposing weight matrices into quantization and low-rank components\n($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used\ntechnique for compressing large language models (LLMs). Existing joint\noptimization methods iteratively alternate between quantization and low-rank\napproximation. However, these methods tend to prioritize one component at the\nexpense of the other, resulting in suboptimal decompositions that fail to\nleverage each component's unique strengths. In this work, we introduce\nOutlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank\ncomponents the specific role of capturing activation-sensitive weights. This\nstructured decomposition mitigates outliers' negative impact on quantization,\nenabling more effective balance between quantization and low-rank\napproximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B\ndemonstrate that incorporating ODLRI into the joint optimization framework\nconsistently reduces activation-aware error, minimizes quantization scale, and\nimproves perplexity and zero-shot accuracy in low-bit settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODLRI\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u91cf\u5316\u548c\u4f4e\u79e9\u4e24\u90e8\u5206\uff0c\u4f18\u5316\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u5408\u4f18\u5316\u91cf\u5316\u548c\u4f4e\u79e9\u8fd1\u4f3c\u65f6\uff0c\u5f80\u5f80\u504f\u91cd\u4e00\u65b9\u800c\u5ffd\u7565\u53e6\u4e00\u65b9\uff0c\u5bfc\u81f4\u5206\u89e3\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165Outlier-Driven Low-Rank Initialization (ODLRI)\uff0c\u8ba9\u4f4e\u79e9\u90e8\u5206\u4e13\u95e8\u6355\u6349\u6fc0\u6d3b\u654f\u611f\u7684\u6743\u91cd\uff0c\u4ece\u800c\u5e73\u8861\u91cf\u5316\u548c\u4f4e\u79e9\u8fd1\u4f3c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cODLRI\u80fd\u6709\u6548\u51cf\u5c11\u6fc0\u6d3b\u611f\u77e5\u8bef\u5dee\u3001\u964d\u4f4e\u91cf\u5316\u89c4\u6a21\uff0c\u5e76\u5728\u4f4e\u6bd4\u7279\u8bbe\u7f6e\u4e0b\u63d0\u5347\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "ODLRI\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6743\u91cd\u77e9\u9635\u5206\u89e3\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2506.03022", "pdf": "https://arxiv.org/pdf/2506.03022", "abs": "https://arxiv.org/abs/2506.03022", "authors": ["David McVicar", "Brian Avant", "Adrian Gould", "Diego Torrejon", "Charles Della Porta", "Ryan Mukherjee"], "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "BlackSky introduces Smartflow, a cloud-based framework enabling scalable\nspatiotemporal geospatial research built on open-source tools and technologies.\nUsing STAC-compliant catalogs as a common input, heterogeneous geospatial data\ncan be processed into standardized datacubes for analysis and model training.\nModel experimentation is managed using a combination of tools, including\nClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is\nKubernetes, which orchestrates the provisioning and execution of workflows to\nsupport both horizontal and vertical scalability. This combination of features\nmakes Smartflow well-suited for geospatial model development and analysis over\nlarge geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to\nmonitor large geographic areas for heavy construction. Qualitative results\nbased on data from the IARPA Space-based Machine Automated Recognition\nTechnique (SMART) program are presented that show the model is capable of\ndetecting heavy construction throughout all major phases of development.", "AI": {"tldr": "BlackSky\u7684Smartflow\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e91\u7684\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u65f6\u7a7a\u5730\u7406\u7a7a\u95f4\u7814\u7a76\uff0c\u7ed3\u5408\u5f00\u6e90\u5de5\u5177\u548c\u6280\u672f\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u7acb\u65b9\u4f53\u5904\u7406\u5f02\u6784\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff0c\u5e76\u5229\u7528Kubernetes\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7f16\u6392\u3002", "motivation": "\u89e3\u51b3\u5730\u7406\u7a7a\u95f4\u6570\u636e\u5904\u7406\u7684\u5f02\u6784\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5730\u7406\u533a\u57df\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u6a21\u578b\u5f00\u53d1\u4e0e\u5206\u6790\u3002", "method": "\u4f7f\u7528STAC\u517c\u5bb9\u76ee\u5f55\u4f5c\u4e3a\u8f93\u5165\uff0c\u5c06\u5f02\u6784\u6570\u636e\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u6570\u636e\u7acb\u65b9\u4f53\uff0c\u7ed3\u5408ClearML\u3001Tensorboard\u548cApache Superset\u8fdb\u884c\u6a21\u578b\u5b9e\u9a8c\u7ba1\u7406\uff0c\u4f9d\u6258Kubernetes\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7f16\u6392\u3002", "result": "Smartflow\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u6a21\u578b\u5f00\u53d1\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u4e2a\u7528\u4e8e\u76d1\u6d4b\u91cd\u578b\u5efa\u7b51\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u80fd\u591f\u68c0\u6d4b\u5efa\u7b51\u5f00\u53d1\u7684\u5404\u4e2a\u4e3b\u8981\u9636\u6bb5\u3002", "conclusion": "Smartflow\u4e3a\u5730\u7406\u7a7a\u95f4\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21\u76d1\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.02085", "pdf": "https://arxiv.org/pdf/2506.02085", "abs": "https://arxiv.org/abs/2506.02085", "authors": ["Ajinkya Kulkarni", "Sandipana Dowerah", "Tanel Alumae", "Mathew Magimai. -Doss"], "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Audio deepfakes are acquiring an unprecedented level of realism with advanced\nAI. While current research focuses on discerning real speech from spoofed\nspeech, tracing the source system is equally crucial. This work proposes a\nnovel audio source tracing system combining deep metric multi-class N-pair loss\nwith Real Emphasis and Fake Dispersion framework, a Conformer classification\nnetwork, and ensemble score-embedding fusion. The N-pair loss improves\ndiscriminative ability, while Real Emphasis and Fake Dispersion enhance\nrobustness by focusing on differentiating real and fake speech patterns. The\nConformer network captures both global and local dependencies in the audio\nsignal, crucial for source tracing. The proposed ensemble score-embedding\nfusion shows an optimal trade-off between in-domain and out-of-domain source\ntracing scenarios. We evaluate our method using Frechet Distance and standard\nmetrics, demonstrating superior performance in source tracing over the baseline\nsystem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5ea6\u91cf\u591a\u7c7bN-pair\u635f\u5931\u3001Real Emphasis\u548cFake Dispersion\u6846\u67b6\u3001Conformer\u5206\u7c7b\u7f51\u7edc\u4ee5\u53ca\u96c6\u6210\u5206\u6570\u5d4c\u5165\u878d\u5408\u7684\u65b0\u578b\u97f3\u9891\u6e90\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u7684\u6e90\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u7684\u903c\u771f\u5ea6\u8d8a\u6765\u8d8a\u9ad8\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u533a\u5206\u771f\u5b9e\u8bed\u97f3\u4e0e\u4f2a\u9020\u8bed\u97f3\uff0c\u800c\u8ffd\u8e2a\u4f2a\u9020\u97f3\u9891\u7684\u6e90\u7cfb\u7edf\u540c\u6837\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5ea6\u91cf\u591a\u7c7bN-pair\u635f\u5931\u3001Real Emphasis\u548cFake Dispersion\u6846\u67b6\u3001Conformer\u5206\u7c7b\u7f51\u7edc\u4ee5\u53ca\u96c6\u6210\u5206\u6570\u5d4c\u5165\u878d\u5408\uff0c\u63d0\u5347\u6e90\u8ffd\u8e2a\u7684\u5224\u522b\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "result": "\u4f7f\u7528Frechet\u8ddd\u79bb\u548c\u6807\u51c6\u6307\u6807\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5728\u6e90\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u9020\u8bed\u97f3\u6a21\u5f0f\u53ca\u6e90\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u7684\u6e90\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03065", "pdf": "https://arxiv.org/pdf/2506.03065", "abs": "https://arxiv.org/abs/2506.03065", "authors": ["Pengtao Chen", "Xianfang Zeng", "Maosen Zhao", "Peng Ye", "Mingzhu Shen", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSparse-vDiT\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528vDiT\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u7a00\u758f\u6027\u6a21\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u7814\u7a76\u53d1\u73b0vDiT\u4e2d\u5b58\u5728\u4e09\u79cd\u7a00\u758f\u6027\u6a21\u5f0f\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faSparse-vDiT\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u9488\u5bf9\u7a00\u758f\u6027\u6a21\u5f0f\u4f18\u5316\u7684\u7a00\u758f\u6838\uff1b2\uff09\u79bb\u7ebf\u7a00\u758f\u6269\u6563\u641c\u7d22\u7b97\u6cd5\uff0c\u9009\u62e9\u6bcf\u5c42\u548c\u6bcf\u5934\u7684\u6700\u4f18\u7a00\u758f\u8ba1\u7b97\u7b56\u7565\u3002", "result": "\u5728CogVideoX1.5\u3001HunyuanVideo\u548cWan2.1\u6a21\u578b\u4e2d\uff0cSparse-vDiT\u5b9e\u73b0\u4e862.09\u00d7\u30012.38\u00d7\u548c1.67\u00d7\u7684\u7406\u8bbaFLOP\u51cf\u5c11\uff0c\u5b9e\u9645\u63a8\u7406\u901f\u5ea6\u63d0\u53471.76\u00d7\u30011.85\u00d7\u548c1.58\u00d7\uff0cPSNR\u503c\u5206\u522b\u8fbe\u523024.13\u300127.09\u548c22.59\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cvDiT\u4e2d\u7684\u6f5c\u5728\u7ed3\u6784\u7a00\u758f\u6027\u53ef\u88ab\u7cfb\u7edf\u6027\u5730\u7528\u4e8e\u957f\u89c6\u9891\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u800c\u4e0d\u727a\u7272\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2506.02088", "pdf": "https://arxiv.org/pdf/2506.02088", "abs": "https://arxiv.org/abs/2506.02088", "authors": ["Alef Iury Siqueira Ferreira", "Lucas Rafael Gris", "Alexandre Ferro Filho", "Lucas \u00d3lives", "Daniel Ribeiro", "Luiz Fernando", "Fernanda Lustosa", "Rodrigo Tanaka", "Frederico Santos de Oliveira", "Arlindo Galv\u00e3o Filho"], "title": "Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025", "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Training SER models in natural, spontaneous speech is especially challenging\ndue to the subtle expression of emotions and the unpredictable nature of\nreal-world audio. In this paper, we present a robust system for the INTERSPEECH\n2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing\non categorical emotion recognition. Our method combines state-of-the-art audio\nmodels with text features enriched by prosodic and spectral cues. In\nparticular, we investigate the effectiveness of Fundamental Frequency (F0)\nquantization and the use of a pretrained audio tagging model. We also employ an\nensemble model to improve robustness. On the official test set, our system\nachieved a Macro F1-score of 39.79% (42.20% on validation). Our results\nunderscore the potential of these methods, and analysis of fusion techniques\nconfirmed the effectiveness of Graph Attention Networks. Our source code is\npublicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u9891\u6a21\u578b\u548c\u6587\u672c\u7279\u5f81\u7684\u9c81\u68d2\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff0c\u91cd\u70b9\u7814\u7a76\u4e86F0\u91cf\u5316\u548c\u9884\u8bad\u7ec3\u97f3\u9891\u6807\u8bb0\u6a21\u578b\u7684\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u81ea\u7136\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u56e0\u60c5\u611f\u8868\u8fbe\u5fae\u5999\u4e14\u97f3\u9891\u4e0d\u53ef\u9884\u6d4b\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u97f3\u9891\u6a21\u578b\u4e0e\u6587\u672c\u7279\u5f81\uff0c\u7814\u7a76F0\u91cf\u5316\u548c\u9884\u8bad\u7ec3\u97f3\u9891\u6807\u8bb0\u6a21\u578b\uff0c\u91c7\u7528\u96c6\u6210\u6a21\u578b\u548cGraph Attention Networks\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0aMacro F1-score\u4e3a39.79%\uff08\u9a8c\u8bc1\u96c642.20%\uff09\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662fGraph Attention Networks\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.03067", "pdf": "https://arxiv.org/pdf/2506.03067", "abs": "https://arxiv.org/abs/2506.03067", "authors": ["Mingzhe Li", "Gehao Zhang", "Zhenting Wang", "Shiqing Ma", "Siqi Pan", "Richard Cartwright", "Juan Zhai"], "title": "EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation models~(e.g., Stable Diffusion) have achieved\nsignificant advancements, enabling the creation of high-quality and realistic\nimages based on textual descriptions. Prompt inversion, the task of identifying\nthe textual prompt used to generate a specific artifact, holds significant\npotential for applications including data attribution, model provenance, and\nwatermarking validation. Recent studies introduced a delayed projection scheme\nto optimize for prompts representative of the vocabulary space, though\nchallenges in semantic fluency and efficiency remain. Advanced image captioning\nmodels or visual large language models can generate highly interpretable\nprompts, but they often lack in image similarity. In this paper, we propose a\nprompt inversion technique called \\sys for text-to-image diffusion models,\nwhich includes initializing embeddings using a pre-trained image captioning\nmodel, refining them through reverse-engineering in the latent space, and\nconverting them to texts using an embedding-to-text model. Our experiments on\nthe widely-used datasets, such as MS COCO, LAION, and Flickr, show that our\nmethod outperforms existing methods in terms of image similarity, textual\nalignment, prompt interpretability and generalizability. We further illustrate\nthe application of our generated prompts in tasks such as cross-concept image\nsynthesis, concept manipulation, evolutionary multi-concept generation and\nunsupervised segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\sys\u7684\u63d0\u793a\u53cd\u8f6c\u6280\u672f\uff0c\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u521d\u59cb\u5316\u5d4c\u5165\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9006\u5411\u5de5\u7a0b\u4f18\u5316\uff0c\u6700\u7ec8\u8f6c\u6362\u4e3a\u6587\u672c\u3002\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u76f8\u4f3c\u6027\u3001\u6587\u672c\u5bf9\u9f50\u3001\u63d0\u793a\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u793a\u53cd\u8f6c\u4efb\u52a1\uff08\u4ece\u751f\u6210\u56fe\u50cf\u53cd\u63a8\u6587\u672c\u63d0\u793a\uff09\u5728\u6570\u636e\u5f52\u5c5e\u3001\u6a21\u578b\u6eaf\u6e90\u548c\u6570\u5b57\u6c34\u5370\u9a8c\u8bc1\u7b49\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u6d41\u7545\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u521d\u59cb\u5316\u5d4c\u5165\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u9006\u5411\u5de5\u7a0b\u4f18\u5316\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u5d4c\u5165\u5230\u6587\u672c\u6a21\u578b\u5c06\u5176\u8f6c\u6362\u4e3a\u6587\u672c\u63d0\u793a\u3002", "result": "\u5728MS COCO\u3001LAION\u548cFlickr\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\\sys\u65b9\u6cd5\u5728\u56fe\u50cf\u76f8\u4f3c\u6027\u3001\u6587\u672c\u5bf9\u9f50\u3001\u63d0\u793a\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\\sys\u65b9\u6cd5\u5728\u63d0\u793a\u53cd\u8f6c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u8de8\u6982\u5ff5\u56fe\u50cf\u5408\u6210\u3001\u6982\u5ff5\u64cd\u4f5c\u3001\u591a\u6982\u5ff5\u751f\u6210\u548c\u65e0\u76d1\u7763\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "AI": {"tldr": "SynthRL\u662f\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u6765\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5408\u6210\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u8fdb\u4e00\u6b65\u63d0\u5347RLVR\uff08\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u8bad\u7ec3\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faSynthRL\u7ba1\u9053\uff0c\u5305\u62ec\u9009\u62e9\u79cd\u5b50\u95ee\u9898\u3001\u589e\u5f3a\u95ee\u9898\u96be\u5ea6\u5e76\u4fdd\u7559\u7b54\u6848\u3001\u4ee5\u53ca\u9a8c\u8bc1\u9636\u6bb5\u786e\u4fdd\u6b63\u786e\u6027\u548c\u96be\u5ea6\u63d0\u5347\u3002", "result": "\u5728MMK12\u6570\u636e\u96c6\u4e0a\uff0cSynthRL\u5408\u6210\u4e863.3K\u4e2a\u989d\u5916\u95ee\u9898\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SynthRL\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u4e0a\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2506.03073", "pdf": "https://arxiv.org/pdf/2506.03073", "abs": "https://arxiv.org/abs/2506.03073", "authors": ["Roman Titkov", "Egor Zubkov", "Dmitry Yudin", "Jaafar Mahmoud", "Malik Mohrat", "Gennady Sidorov"], "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM", "categories": ["cs.CV"], "comment": null, "summary": "Modern Gaussian Splatting methods have proven highly effective for real-time\nphotorealistic rendering of 3D scenes. However, integrating semantic\ninformation into this representation remains a significant challenge,\nespecially in maintaining real-time performance for SLAM (Simultaneous\nLocalization and Mapping) applications. In this work, we introduce LEG-SLAM --\na novel approach that fuses an optimized Gaussian Splatting implementation with\nvisual-language feature extraction using DINOv2 followed by a learnable feature\ncompressor based on Principal Component Analysis, while enabling an online\ndense SLAM. Our method simultaneously generates high-quality photorealistic\nimages and semantically labeled scene maps, achieving real-time scene\nreconstruction with more than 10 fps on the Replica dataset and 18 fps on\nScanNet. Experimental results show that our approach significantly outperforms\nstate-of-the-art methods in reconstruction speed while achieving competitive\nrendering quality. The proposed system eliminates the need for prior data\npreparation such as camera's ego motion or pre-computed static semantic maps.\nWith its potential applications in autonomous robotics, augmented reality, and\nother interactive domains, LEG-SLAM represents a significant step forward in\nreal-time semantic 3D Gaussian-based SLAM. Project page:\nhttps://titrom025.github.io/LEG-SLAM/", "AI": {"tldr": "LEG-SLAM\u7ed3\u5408\u4f18\u5316\u7684\u9ad8\u65af\u6cfc\u6e85\u4e0e\u89c6\u89c9\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8bed\u4e493D SLAM\uff0c\u901f\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0c\u89e3\u51b3SLAM\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "method": "\u878d\u5408\u9ad8\u65af\u6cfc\u6e85\u4e0eDINOv2\u89c6\u89c9\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528PCA\u7279\u5f81\u538b\u7f29\u5668\uff0c\u652f\u6301\u5728\u7ebf\u5bc6\u96c6SLAM\u3002", "result": "\u5728Replica\u548cScanNet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523010 fps\u548c18 fps\uff0c\u91cd\u5efa\u901f\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LEG-SLAM\u65e0\u9700\u5148\u9a8c\u6570\u636e\u51c6\u5907\uff0c\u4e3a\u5b9e\u65f6\u8bed\u4e493D SLAM\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02160", "pdf": "https://arxiv.org/pdf/2506.02160", "abs": "https://arxiv.org/abs/2506.02160", "authors": ["Madan Krishnamurthy", "Daniel Korn", "Melissa A Haendel", "Christopher J Mungall", "Anne E Thessen"], "title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u52a8\u6001\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5229\u7528LLM\u548cHDBSCAN\u89e3\u51b3\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2dCDE\u7684\u8bed\u4e49\u5f02\u6784\u6027\u548c\u7ed3\u6784\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u96c6\u6210\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2dCDE\u7684\u8bed\u4e49\u5f02\u6784\u6027\u3001\u7ed3\u6784\u53d8\u5f02\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u4ee5\u4fc3\u8fdb\u6570\u636e\u96c6\u6210\u548c\u79d1\u5b66\u53d1\u73b0\u3002", "method": "1. LLM\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6587\u672c\u5d4c\u5165\uff1b2. HDBSCAN\u805a\u7c7b\u8bed\u4e49\u76f8\u4f3c\u7684CDE\uff1b3. LLM\u81ea\u52a8\u6807\u8bb0\uff1b4. \u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u5206\u7c7b\u5668\u3002", "result": "\u572824,000+ CDE\u4e0a\u8bc6\u522b118\u4e2a\u6709\u610f\u4e49\u7684\u805a\u7c7b\uff0c\u5206\u7c7b\u5668\u51c6\u786e\u738790.46%\uff0c\u5916\u90e8\u9a8c\u8bc1\u663e\u793a\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aCDE\u534f\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u9009\u62e9\u6548\u7387\u5e76\u652f\u6301\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2506.03079", "pdf": "https://arxiv.org/pdf/2506.03079", "abs": "https://arxiv.org/abs/2506.03079", "authors": ["Xiuyu Yang", "Bohan Li", "Shaocong Xu", "Nan Wang", "Chongjie Ye", "Zhaoxi Chen", "Minghan Qin", "Yikang Ding", "Xin Jin", "Hang Zhao", "Hao Zhao"], "title": "ORV: 4D Occupancy-centric Robot Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://orangesodahub.github.io/ORV/ ; Code:\n  https://github.com/OrangeSodahub/ORV", "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV", "AI": {"tldr": "ORV\u6846\u67b6\u901a\u8fc74D\u8bed\u4e49\u5360\u7528\u5e8f\u5217\u751f\u6210\u7cbe\u7ec6\u5316\u7684\u673a\u5668\u4eba\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u4eff\u771f\u6570\u636e\u83b7\u53d6\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u56e0\u5168\u5c40\u7c97\u5bf9\u9f50\u5bfc\u81f4\u63a7\u5236\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faORV\u6846\u67b6\uff0c\u5229\u75284D\u8bed\u4e49\u5360\u7528\u5e8f\u5217\u63d0\u4f9b\u7cbe\u7ec6\u5316\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u6307\u5bfc\uff0c\u5b9e\u73b0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7cbe\u786e\u53ef\u63a7\u7684\u89c6\u9891\u751f\u6210\u3002", "result": "ORV\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5b50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u3002", "conclusion": "ORV\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u89c6\u9891\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02178", "pdf": "https://arxiv.org/pdf/2506.02178", "abs": "https://arxiv.org/abs/2506.02178", "authors": ["Thai-Binh Nguyen", "Ngoc-Quan Pham", "Alexander Waibel"], "title": "Cocktail-Party Audio-Visual Speech Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech\nrecognition in challenging environments, such as cocktail-party scenarios,\nwhere relying solely on audio proves insufficient. However, current AVSR models\nare often optimized for idealized scenarios with consistently active speakers,\noverlooking the complexities of real-world settings that include both speaking\nand silent facial segments. This study addresses this gap by introducing a\nnovel audio-visual cocktail-party dataset designed to benchmark current AVSR\nsystems and highlight the limitations of prior approaches in realistic noisy\nconditions. Additionally, we contribute a 1526-hour AVSR dataset comprising\nboth talking-face and silent-face segments, enabling significant performance\ngains in cocktail-party environments. Our approach reduces WER by 67% relative\nto the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,\nwithout relying on explicit segmentation cues.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u97f3\u9891-\u89c6\u89c9\u9e21\u5c3e\u9152\u4f1a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30AVSR\u7cfb\u7edf\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709AVSR\u6a21\u578b\u5728\u7406\u60f3\u5316\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5ffd\u89c6\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u5305\u542b\u8bf4\u8bdd\u548c\u6c89\u9ed8\u9762\u90e8\u7247\u6bb5\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u8bf4\u8bdd\u548c\u6c89\u9ed8\u9762\u90e8\u7247\u6bb5\u76841526\u5c0f\u65f6AVSR\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6781\u7aef\u566a\u58f0\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u3002", "result": "\u76f8\u5bf9\u4e8e\u73b0\u6709\u6280\u672f\uff0cWER\u964d\u4f4e\u4e8667%\uff0c\u4ece119%\u964d\u81f339.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u663e\u5f0f\u5206\u5272\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86AVSR\u7cfb\u7edf\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.03082", "pdf": "https://arxiv.org/pdf/2506.03082", "abs": "https://arxiv.org/abs/2506.03082", "authors": ["Ssharvien Kumar Sivakumar", "Yannik Frisch", "Ghazal Ghazaei", "Anirban Mukhopadhyay"], "title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Surgical simulation plays a pivotal role in training novice surgeons,\naccelerating their learning curve and reducing intra-operative errors. However,\nconventional simulation tools fall short in providing the necessary\nphotorealism and the variability of human anatomy. In response, current methods\nare shifting towards generative model-based simulators. Yet, these approaches\nprimarily focus on using increasingly complex conditioning for precise\nsynthesis while neglecting the fine-grained human control aspect. To address\nthis gap, we introduce SG2VID, the first diffusion-based video model that\nleverages Scene Graphs for both precise video synthesis and fine-grained human\ncontrol. We demonstrate SG2VID's capabilities across three public datasets\nfeaturing cataract and cholecystectomy surgery. While SG2VID outperforms\nprevious methods both qualitatively and quantitatively, it also enables precise\nsynthesis, providing accurate control over tool and anatomy's size and\nmovement, entrance of new tools, as well as the overall scene layout. We\nqualitatively motivate how SG2VID can be used for generative augmentation and\npresent an experiment demonstrating its ability to improve a downstream phase\ndetection task when the training set is extended with our synthetic videos.\nFinally, to showcase SG2VID's ability to retain human control, we interact with\nthe Scene Graphs to generate new video samples depicting major yet rare\nintra-operative irregularities.", "AI": {"tldr": "SG2VID\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u573a\u666f\u56fe\u5b9e\u73b0\u7cbe\u786e\u89c6\u9891\u5408\u6210\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u7528\u4e8e\u624b\u672f\u6a21\u62df\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u6a21\u62df\u5de5\u5177\u7f3a\u4e4f\u771f\u5b9e\u611f\u548c\u89e3\u5256\u5b66\u53d8\u5f02\u6027\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "method": "\u63d0\u51faSG2VID\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u573a\u666f\u56fe\uff0c\u5b9e\u73b0\u7cbe\u786e\u89c6\u9891\u5408\u6210\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5de5\u5177\u548c\u89e3\u5256\u7ed3\u6784\u7684\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "SG2VID\u4e3a\u624b\u672f\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u548c\u53ef\u63a7\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.02208", "pdf": "https://arxiv.org/pdf/2506.02208", "abs": "https://arxiv.org/abs/2506.02208", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) post-training have leveraged\ntwo distinct paradigms to enhance reasoning capabilities: reinforcement\nlearning (RL) and knowledge distillation (KD). While RL enables the emergence\nof complex reasoning behaviors, it often suffers from low sample efficiency\nwhen the initial policy struggles to explore high-reward trajectories.\nConversely, KD improves learning efficiency via mimicking the teacher model but\ntends to generalize poorly to out-of-domain scenarios. In this work, we present\n\\textbf{KDRL}, a \\textit{unified post-training framework} that jointly\noptimizes a reasoning model through teacher supervision (KD) and\nself-exploration (RL). Specifically, KDRL leverages policy gradient\noptimization to simultaneously minimize the reverse Kullback-Leibler divergence\n(RKL) between the student and teacher distributions while maximizing the\nexpected rule-based rewards. We first formulate a unified objective that\nintegrates GRPO and KD, and systematically explore how different KL\napproximations, KL coefficients, and reward-guided KD strategies affect the\noverall post-training dynamics and performance. Empirical results on multiple\nreasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD\nbaselines while achieving a favorable balance between performance and reasoning\ntoken efficiency. These findings indicate that integrating KD and RL serves as\nan effective and efficient strategy to train reasoning LLMs.", "AI": {"tldr": "KDRL\u662f\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7edf\u4e00\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RL\u80fd\u4ea7\u751f\u590d\u6742\u63a8\u7406\u884c\u4e3a\u4f46\u6837\u672c\u6548\u7387\u4f4e\uff0cKD\u5b66\u4e60\u6548\u7387\u9ad8\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "KDRL\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u540c\u65f6\u6700\u5c0f\u5316\u5b66\u751f\u4e0e\u6559\u5e08\u5206\u5e03\u7684\u53cd\u5411KL\u6563\u5ea6\uff08RKL\uff09\u5e76\u6700\u5927\u5316\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKDRL\u4f18\u4e8eGRPO\u548cKD\u57fa\u7ebf\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408KD\u548cRL\u662f\u8bad\u7ec3\u63a8\u7406\u578bLLM\u7684\u6709\u6548\u4e14\u9ad8\u6548\u7b56\u7565\u3002"}}
{"id": "2506.03084", "pdf": "https://arxiv.org/pdf/2506.03084", "abs": "https://arxiv.org/abs/2506.03084", "authors": ["Zizhao Wu", "Yingying Sun", "Yiming Chen", "Xiaoling Gu", "Ruyu Liu", "Jiazhou Chen"], "title": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Human-human interaction generation has garnered significant attention in\nmotion synthesis due to its vital role in understanding humans as social\nbeings. However, existing methods typically rely on transformer-based\narchitectures, which often face challenges related to scalability and\nefficiency. To address these issues, we propose a novel, efficient human-human\ninteraction generation method based on the Mamba framework, designed to meet\nthe demands of effectively capturing long-sequence dependencies while providing\nreal-time feedback. Specifically, we introduce an adaptive spatio-temporal\nMamba framework that utilizes two parallel SSM branches with an adaptive\nmechanism to integrate the spatial and temporal features of motion sequences.\nTo further enhance the model's ability to capture dependencies within\nindividual motion sequences and the interactions between different individual\nsequences, we develop two key modules: the self-adaptive spatio-temporal Mamba\nmodule and the cross-adaptive spatio-temporal Mamba module, enabling efficient\nfeature learning. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results on two interaction datasets with remarkable quality\nand efficiency. Compared to the baseline method InterGen, our approach not only\nimproves accuracy but also requires a minimal parameter size of just 66M ,only\n36% of InterGen's, while achieving an average inference speed of 0.57 seconds,\nwhich is 46% of InterGen's execution time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u6846\u67b6\u7684\u9ad8\u6548\u4eba-\u4eba\u4ea4\u4e92\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709Transformer\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4eba-\u4eba\u4ea4\u4e92\u751f\u6210\u5728\u8fd0\u52a8\u5408\u6210\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56Transformer\u67b6\u6784\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u65f6\u7a7aMamba\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u5e76\u884cSSM\u5206\u652f\u548c\u81ea\u9002\u5e94\u673a\u5236\u6574\u5408\u8fd0\u52a8\u5e8f\u5217\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u548c\u4ea4\u53c9\u81ea\u9002\u5e94\u6a21\u5757\u4ee5\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u4ea4\u4e92\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0c\u53c2\u6570\u89c4\u6a21\u4ec5\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u768436%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u81f3\u57fa\u7ebf\u65b9\u6cd5\u768446%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u53cd\u9988\u9700\u6c42\u3002"}}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches.", "AI": {"tldr": "EVNets\u7ed3\u5408\u4e86VOneBlock\u548cSubcorticalBlock\uff0c\u63d0\u5347\u4e86CNN\u7684\u9c81\u68d2\u6027\u548c\u751f\u7269\u5b66\u5bf9\u9f50\u6027\uff0c\u5e76\u5728\u591a\u79cd\u6270\u52a8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3CNN\u5728\u89c6\u89c9\u6270\u52a8\u548c\u57df\u5916\u56fe\u50cf\u4e2d\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u4eff\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faEVNets\uff0c\u7ed3\u5408VOneBlock\u548cSubcorticalBlock\uff0c\u540e\u8005\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u6a21\u578b\u8bbe\u8ba1\u3002", "result": "EVNets\u5728V1\u5bf9\u9f50\u3001\u5f62\u72b6\u504f\u597d\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6CNN\uff0c\u63d0\u53478.5%\u3002\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u540e\u8fdb\u4e00\u6b65\u63d0\u53477.3%\u3002", "conclusion": "EVNets\u5c55\u793a\u4e86\u67b6\u6784\u6539\u8fdb\u4e0e\u6570\u636e\u589e\u5f3a\u7684\u4e92\u8865\u6027\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.02314", "pdf": "https://arxiv.org/pdf/2506.02314", "abs": "https://arxiv.org/abs/2506.02314", "authors": ["Tianyu Hua", "Harper Hua", "Violet Xiang", "Benjamin Klieger", "Sang T. Truong", "Weixin Liang", "Fan-Yun Sun", "Nick Haber"], "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promise in transforming machine\nlearning research, yet their capability to faithfully implement novel ideas\nfrom recent research papers-ideas unseen during pretraining-remains unclear. We\nintroduce ResearchCodeBench, a benchmark of 212 coding challenges that\nevaluates LLMs' ability to translate cutting-edge ML contributions from top\n2024-2025 research papers into executable code. We assessed 30+ proprietary and\nopen-source LLMs, finding that even the best models correctly implement less\nthan 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%\nsuccess rate, with O3 (High) and O4-mini (High) following behind at 32.3% and\n30.8% respectively. We present empirical findings on performance comparison,\ncontamination, and error patterns. By providing a rigorous and community-driven\nevaluation platform, ResearchCodeBench enables continuous understanding and\nadvancement of LLM-driven innovation in research code generation.", "AI": {"tldr": "ResearchCodeBench\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u524d\u6cbf\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u65b0\u601d\u60f3\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6700\u4f73\u6a21\u578b\u6210\u529f\u7387\u4e0d\u8db340%\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u5b9e\u73b0\u672a\u89c1\u8fc7\u7684\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u65b0\u601d\u60f3\u65f6\u7684\u80fd\u529b\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u5305\u542b212\u4e2a\u7f16\u7801\u6311\u6218\u7684ResearchCodeBench\uff0c\u8bc4\u4f3030+\u4e13\u6709\u548c\u5f00\u6e90LLMs\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "result": "\u6700\u4f73\u6a21\u578bGemini-2.5-Pro-Preview\u6210\u529f\u7387\u4e3a37.3%\uff0c\u5176\u4ed6\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "ResearchCodeBench\u4e3aLLM\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u6301\u7eed\u6539\u8fdb\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5e73\u53f0\u3002"}}
{"id": "2506.03096", "pdf": "https://arxiv.org/pdf/2506.03096", "abs": "https://arxiv.org/abs/2506.03096", "authors": ["Christian Schlarmann", "Francesco Croce", "Nicolas Flammarion", "Matthias Hein"], "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Code and models available at https://github.com/chs20/fuselip", "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.", "AI": {"tldr": "FuseLIP\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5d4c\u5165\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u4e00Transformer\u6a21\u578b\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\u4ee4\u724c\uff0c\u5b9e\u73b0\u65e9\u671f\u878d\u5408\uff0c\u4f18\u4e8e\u4f20\u7edf\u540e\u671f\u878d\u5408\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u65b9\u6cd5\u65e0\u6cd5\u539f\u751f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u9700\u989d\u5916\u6a21\u5757\u878d\u5408\u7279\u5f81\u3002", "method": "\u5229\u7528\u79bb\u6563\u56fe\u50cf\u4ee4\u724c\u5316\u6280\u672f\uff0c\u63d0\u51fa\u5355\u4e00Transformer\u6a21\u578b\u5904\u7406\u6269\u5c55\u7684\u6587\u672c\u548c\u56fe\u50cf\u4ee4\u724c\u8bcd\u6c47\u8868\u3002", "result": "FuseLIP\u5728\u591a\u6a21\u6001\u4efb\u52a1\uff08\u5982VQA\u548c\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u8f6c\u6362\u68c0\u7d22\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5355\u6a21\u6001\u4efb\u52a1\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "FuseLIP\u901a\u8fc7\u65e9\u671f\u878d\u5408\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u4e3a\u591a\u6a21\u6001\u5d4c\u5165\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.02414", "pdf": "https://arxiv.org/pdf/2506.02414", "abs": "https://arxiv.org/abs/2506.02414", "authors": ["Fengjin Li", "Jie Wang", "Yadong Niu", "Yongqing Wang", "Meng Meng", "Jian Luan", "Zhiyong Wu"], "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 2 figures, Accepted by Interspeech 2025, Demo:\n  https://thuhcsi.github.io/StarVC/", "summary": "Voice Conversion (VC) modifies speech to match a target speaker while\npreserving linguistic content. Traditional methods usually extract speaker\ninformation directly from speech while neglecting the explicit utilization of\nlinguistic content. Since VC fundamentally involves disentangling speaker\nidentity from linguistic content, leveraging structured semantic features could\nenhance conversion performance. However, previous attempts to incorporate\nsemantic features into VC have shown limited effectiveness, motivating the\nintegration of explicit text modeling. We propose StarVC, a unified\nautoregressive VC framework that first predicts text tokens before synthesizing\nacoustic features. The experiments demonstrate that StarVC outperforms\nconventional VC methods in preserving both linguistic content (i.e., WER and\nCER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found\nat: https://thuhcsi.github.io/StarVC/.", "AI": {"tldr": "StarVC\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u7684\u8bed\u97f3\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u9884\u6d4b\u6587\u672c\u6807\u8bb0\u518d\u5408\u6210\u58f0\u5b66\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8f6c\u6362\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u8bed\u4e49\u5185\u5bb9\uff0c\u800cStarVC\u901a\u8fc7\u663e\u5f0f\u6587\u672c\u5efa\u6a21\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "StarVC\u91c7\u7528\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5148\u9884\u6d4b\u6587\u672c\u6807\u8bb0\uff0c\u518d\u5408\u6210\u58f0\u5b66\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStarVC\u5728\u4fdd\u7559\u8bed\u8a00\u5185\u5bb9\uff08WER\u3001CER\uff09\u548c\u8bf4\u8bdd\u4eba\u7279\u5f81\uff08SECS\u3001MOS\uff09\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "StarVC\u901a\u8fc7\u663e\u5f0f\u6587\u672c\u5efa\u6a21\u63d0\u5347\u4e86\u8bed\u97f3\u8f6c\u6362\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03097", "pdf": "https://arxiv.org/pdf/2506.03097", "abs": "https://arxiv.org/abs/2506.03097", "authors": ["Ashwin Vinod", "Shrey Pandit", "Aditya Vavre", "Linshen Liu"], "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Our Code can be found at https://github.com/adityavavre/VidEgoVLM", "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.", "AI": {"tldr": "EgoVLM\u662f\u4e00\u4e2a\u4e13\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8bbe\u8ba1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u81ea\u4e3b\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6d41\u4e2d\u8fdb\u884c\u9c81\u68d2\u7684\u63a8\u7406\u3002", "method": "EgoVLM\u901a\u8fc7Group Relative Policy Optimization\uff08GRPO\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u76f4\u63a5\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u4eba\u7c7b\u63a8\u7406\u6b65\u9aa4\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u3002", "result": "EgoVLM-3B\u5728EgoSchema\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eQwen2.5-VL 3B\u548c7B\u6a21\u578b\uff0c\u5206\u522b\u63d0\u5347\u4e8614.33\u548c13.87\u4e2a\u51c6\u786e\u70b9\u3002", "conclusion": "EgoVLM\u901a\u8fc7\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u5956\u52b1\u673a\u5236\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.02475", "pdf": "https://arxiv.org/pdf/2506.02475", "abs": "https://arxiv.org/abs/2506.02475", "authors": ["Jiaxi Hu", "Yongqi Pan", "Jusen Du", "Disen Lan", "Xiaqiang Tang", "Qingsong Wen", "Yuxuan Liang", "Weigao Sun"], "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and\nRWKV-7 have achieved performance improvements by supervising the recurrent\nmemory management through Delta learning rule. Unlike previous state-space\nmodels (e.g., Mamba) and gated linear attentions (e.g., GLA), these models\nintroduce interactions between the recurrent state and the key vector,\nresulting in a nonlinear recursive structure. In this paper, we first introduce\nthe concept of Nonlinear RNNs with a comprehensive analysis on the advantages\nand limitations of these models. Then, based on closed-loop control theory, we\npropose a novel Nonlinear RNN variant named Comba, which adopts a\nscalar-plus-low-rank state transition, with both state feedback and output\nfeedback corrections. We also implement a hardware-efficient chunk-wise\nparallel kernel in Triton and train models with 340M/1.3B parameters on\nlarge-scale corpus. Comba demonstrates its superior performance and computation\nefficiency in both language and vision modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u975e\u7ebf\u6027RNN\u53d8\u4f53Comba\uff0c\u57fa\u4e8e\u95ed\u73af\u63a7\u5236\u7406\u8bba\uff0c\u901a\u8fc7\u72b6\u6001\u53cd\u9988\u548c\u8f93\u51fa\u53cd\u9988\u6821\u6b63\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u8a00\u548c\u89c6\u89c9\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff08\u5982Gated DeltaNet\u3001TTT\u548cRWKV-7\uff09\u901a\u8fc7Delta\u5b66\u4e60\u89c4\u5219\u6539\u8fdb\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u975e\u7ebf\u6027\u9012\u5f52\u7ed3\u6784\u7684\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u975e\u7ebf\u6027RNN\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u95ed\u73af\u63a7\u5236\u7406\u8bba\uff0c\u63d0\u51faComba\u6a21\u578b\uff0c\u91c7\u7528\u6807\u91cf\u52a0\u4f4e\u79e9\u72b6\u6001\u8f6c\u79fb\uff0c\u7ed3\u5408\u72b6\u6001\u53cd\u9988\u548c\u8f93\u51fa\u53cd\u9988\u6821\u6b63\uff0c\u5e76\u5b9e\u73b0\u4e86\u786c\u4ef6\u9ad8\u6548\u7684\u5e76\u884c\u6838\u3002", "result": "\u5728340M/1.3B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0cComba\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Comba\u901a\u8fc7\u521b\u65b0\u7684\u975e\u7ebf\u6027RNN\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e8f\u5217\u5efa\u6a21\u7684\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2506.03103", "pdf": "https://arxiv.org/pdf/2506.03103", "abs": "https://arxiv.org/abs/2506.03103", "authors": ["Xiaoyan Cong", "Angela Xing", "Chandradeep Pokhariya", "Rao Fu", "Srinath Sridhar"], "title": "DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic hand-object contacts is essential for realistic\nmanipulation in AI character animation, XR, and robotics, yet it remains\nchallenging due to heavy occlusions, complex surface details, and limitations\nin existing capture techniques. In this paper, we introduce DyTact, a\nmarkerless capture method for accurately capturing dynamic contact in\nhand-object manipulations in a non-intrusive manner. Our approach leverages a\ndynamic, articulated representation based on 2D Gaussian surfels to model\ncomplex manipulations. By binding these surfels to MANO meshes, DyTact\nharnesses the inductive bias of template models to stabilize and accelerate\noptimization. A refinement module addresses time-dependent high-frequency\ndeformations, while a contact-guided adaptive sampling strategy selectively\nincreases surfel density in contact regions to handle heavy occlusion.\nExtensive experiments demonstrate that DyTact not only achieves\nstate-of-the-art dynamic contact estimation accuracy but also significantly\nimproves novel view synthesis quality, all while operating with fast\noptimization and efficient memory usage. Project Page:\nhttps://oliver-cong02.github.io/DyTact.github.io/ .", "AI": {"tldr": "DyTact\u662f\u4e00\u79cd\u65e0\u6807\u8bb0\u6355\u6349\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u6355\u6349\u624b-\u7269\u4f53\u52a8\u6001\u63a5\u89e6\uff0c\u901a\u8fc72D\u9ad8\u65af\u9762\u5143\u548cMANO\u7f51\u683c\u7ed3\u5408\uff0c\u4f18\u5316\u52a8\u6001\u63a5\u89e6\u91cd\u5efa\u3002", "motivation": "\u52a8\u6001\u624b-\u7269\u4f53\u63a5\u89e6\u91cd\u5efa\u5728AI\u52a8\u753b\u3001XR\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6280\u672f\u56e0\u906e\u6321\u3001\u590d\u6742\u8868\u9762\u7ec6\u8282\u548c\u6355\u6349\u9650\u5236\u800c\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "DyTact\u57fa\u4e8e2D\u9ad8\u65af\u9762\u5143\u52a8\u6001\u5efa\u6a21\uff0c\u7ed3\u5408MANO\u7f51\u683c\u6a21\u677f\u4f18\u5316\uff0c\u91c7\u7528\u63a5\u89e6\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u5904\u7406\u906e\u6321\u3002", "result": "DyTact\u5728\u52a8\u6001\u63a5\u89e6\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u540c\u65f6\u4f18\u5316\u901f\u5ea6\u5feb\u4e14\u5185\u5b58\u9ad8\u6548\u3002", "conclusion": "DyTact\u4e3a\u52a8\u6001\u624b-\u7269\u4f53\u63a5\u89e6\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2506.02479", "pdf": "https://arxiv.org/pdf/2506.02479", "abs": "https://arxiv.org/abs/2506.02479", "authors": ["Kalyan Nakka", "Nitesh Saxena"], "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage", "categories": ["cs.CR", "cs.CL"], "comment": "24 pages, 24 figures, and 7 tables", "summary": "The inherent risk of generating harmful and unsafe content by Large Language\nModels (LLMs), has highlighted the need for their safety alignment. Various\ntechniques like supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming were developed for ensuring the safety alignment of\nLLMs. However, the robustness of these aligned LLMs is always challenged by\nadversarial attacks that exploit unexplored and underlying vulnerabilities of\nthe safety alignment. In this paper, we develop a novel black-box jailbreak\nattack, called BitBypass, that leverages hyphen-separated bitstream camouflage\nfor jailbreaking aligned LLMs. This represents a new direction in jailbreaking\nby exploiting fundamental information representation of data as continuous\nbits, rather than leveraging prompt engineering or adversarial manipulations.\nOur evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude\n3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the\ncapabilities of BitBypass in bypassing their safety alignment and tricking them\ninto generating harmful and unsafe content. Further, we observed that BitBypass\noutperforms several state-of-the-art jailbreak attacks in terms of stealthiness\nand attack success. Overall, these results highlights the effectiveness and\nefficiency of BitBypass in jailbreaking these state-of-the-art LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBitBypass\u7684\u65b0\u578b\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8fde\u5b57\u7b26\u5206\u9694\u7684\u6bd4\u7279\u6d41\u4f2a\u88c5\u6765\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709LLMs\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u3001\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\uff09\u5728\u9762\u5bf9\u5bf9\u6297\u653b\u51fb\u65f6\u4ecd\u5b58\u5728\u6f0f\u6d1e\uff0c\u4e9f\u9700\u63a2\u7d22\u65b0\u7684\u653b\u51fb\u65b9\u5f0f\u4ee5\u63ed\u793a\u5176\u8106\u5f31\u6027\u3002", "method": "\u5f00\u53d1\u4e86BitBypass\u653b\u51fb\uff0c\u5229\u7528\u6bd4\u7279\u6d41\u4f2a\u88c5\u800c\u975e\u4f20\u7edf\u7684\u63d0\u793a\u5de5\u7a0b\u6216\u5bf9\u6297\u64cd\u4f5c\uff0c\u7ed5\u8fc7LLMs\u7684\u5b89\u5168\u5bf9\u9f50\u3002", "result": "\u5728\u4e94\u79cd\u5148\u8fdbLLMs\uff08GPT-4o\u3001Gemini 1.5\u7b49\uff09\u4e0a\u6d4b\u8bd5\uff0cBitBypass\u6210\u529f\u7ed5\u8fc7\u5b89\u5168\u5bf9\u9f50\u5e76\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u4e14\u5728\u9690\u853d\u6027\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "BitBypass\u5c55\u793a\u4e86LLMs\u5b89\u5168\u5bf9\u9f50\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2506.03107", "pdf": "https://arxiv.org/pdf/2506.03107", "abs": "https://arxiv.org/abs/2506.03107", "authors": ["Di Chang", "Mingdeng Cao", "Yichun Shi", "Bo Liu", "Shengqu Cai", "Shijie Zhou", "Weilin Huang", "Gordon Wetzstein", "Mohammad Soleymani", "Peng Wang"], "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions", "categories": ["cs.CV"], "comment": "Website: https://boese0601.github.io/bytemorph Dataset:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-6M Benchmark:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-Bench Code:\n  https://github.com/ByteDance-Seed/BM-code Demo:\n  https://huggingface.co/spaces/Boese0601/ByteMorph-Demo", "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.", "AI": {"tldr": "ByteMorph\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u975e\u521a\u6027\u8fd0\u52a8\u7684\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u96c6ByteMorph-6M\u548c\u57fa\u4e8eDiffusion Transformer\u7684\u6a21\u578bByteMorpher\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u573a\u666f\u6216\u521a\u6027\u53d8\u6362\uff0c\u65e0\u6cd5\u5904\u7406\u52a8\u6001\u8fd0\u52a8\u7684\u590d\u6742\u7f16\u8f91\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u5f15\u5bfc\u6570\u636e\u751f\u6210\u3001\u5206\u5c42\u5408\u6210\u6280\u672f\u548c\u81ea\u52a8\u6807\u6ce8\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8eDiT\u6784\u5efa\u6a21\u578b\u3002", "result": "ByteMorph-6M\u5305\u542b600\u4e07\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\u6837\u672c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u51c6ByteMorph-Bench\u3002", "conclusion": "ByteMorph\u586b\u8865\u4e86\u975e\u521a\u6027\u8fd0\u52a8\u7f16\u8f91\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u3002"}}
{"id": "2506.02529", "pdf": "https://arxiv.org/pdf/2506.02529", "abs": "https://arxiv.org/abs/2506.02529", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "comment": "Published in the Proceedings of JSAI 2025", "summary": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7ed3\u5408\u56fe\u7ed3\u6784\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u7528\u4e8e\u751f\u6210\u7f51\u7ad9\u5bfc\u822a\u548c\u8868\u5355\u586b\u5199\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u5e94\u7528\u754c\u9762\u590d\u6742\u4e14\u52a8\u6001\u6027\u5f3a\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709LLMs\u5728\u52a8\u6001\u5bfc\u822a\u6d41\u7a0b\u548c\u590d\u6742\u8868\u5355\u4ea4\u4e92\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u5c4f\u5e55\u8f6c\u6362\u56fe\u548cLLMs\u5efa\u6a21\u5bfc\u822a\u6d41\u7a0b\u751f\u6210\u6d4b\u8bd5\u573a\u666f\uff1b\u5229\u7528\u72b6\u6001\u56fe\u5904\u7406\u6761\u4ef6\u8868\u5355\u5e76\u81ea\u52a8\u5316\u751f\u6210Selenium\u811a\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u56fe\u7ed3\u6784\u548cLLMs\uff0c\u63a8\u52a8\u4e86\u7f51\u7edc\u5e94\u7528\u6d4b\u8bd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.03110", "pdf": "https://arxiv.org/pdf/2506.03110", "abs": "https://arxiv.org/abs/2506.03110", "authors": ["Shuai Yi", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025(spotlight)", "summary": "Vision Transformer (ViT) has achieved remarkable success due to its\nlarge-scale pretraining on general domains, but it still faces challenges when\napplying it to downstream distant domains that have only scarce training data,\nwhich gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired\nby Self-Attention's insensitivity to token orders, we find an interesting\nphenomenon neglected in current works: disrupting the continuity of image\ntokens (i.e., making pixels not smoothly transited across patches) in ViT leads\nto a noticeable performance decline in the general (source) domain but only a\nmarginal decrease in downstream target domains. This questions the role of\nimage tokens' continuity in ViT's generalization under large domain gaps. In\nthis paper, we delve into this phenomenon for an interpretation. We find\ncontinuity aids ViT in learning larger spatial patterns, which are harder to\ntransfer than smaller ones, enlarging domain distances. Meanwhile, it implies\nthat only smaller patterns within each patch could be transferred under extreme\ndomain gaps. Based on this interpretation, we further propose a simple yet\neffective method for CDFSL that better disrupts the continuity of image tokens,\nencouraging the model to rely less on large patterns and more on smaller ones.\nExtensive experiments show the effectiveness of our method in reducing domain\ngaps and outperforming state-of-the-art works. Codes and models are available\nat https://github.com/shuaiyi308/ReCIT.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0ViT\u4e2d\u56fe\u50cf\u6807\u8bb0\u7684\u8fde\u7eed\u6027\u5bf9\u5176\u6cdb\u5316\u80fd\u529b\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u4ee5\u6539\u5584\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22ViT\u5728\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u539f\u56e0\uff0c\u5c24\u5176\u662f\u56fe\u50cf\u6807\u8bb0\u8fde\u7eed\u6027\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7834\u574f\u56fe\u50cf\u6807\u8bb0\u7684\u8fde\u7eed\u6027\uff0c\u4fc3\u4f7f\u6a21\u578b\u66f4\u4f9d\u8d56\u5c0f\u5c3a\u5ea6\u6a21\u5f0f\u800c\u975e\u5927\u5c3a\u5ea6\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u57df\u5dee\u8ddd\u5e76\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u56fe\u50cf\u6807\u8bb0\u8fde\u7eed\u6027\u7684\u7834\u574f\u6709\u52a9\u4e8e\u63d0\u5347ViT\u5728\u6781\u7aef\u57df\u5dee\u8ddd\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.02553", "pdf": "https://arxiv.org/pdf/2506.02553", "abs": "https://arxiv.org/abs/2506.02553", "authors": ["Shenghua He", "Tian Xia", "Xuan Zhou", "Hui Wei"], "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study a common challenge in reinforcement learning for large language\nmodels (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,\nintermediate token generations) receive zero task-specific immediate reward,\nwhile only the final token receives a reward for the entire response. This\nassumption arises frequently in practice, as precise token-level rewards are\noften difficult or infeasible to obtain in LLM applications. In this work, we\nprovide a unifying theoretical perspective. We introduce the Trajectory Policy\nGradient Theorem, which shows that the policy gradient based on true, unknown\ntoken-level rewards can be unbiasedly estimated using only a response-level\nreward model, regardless of whether the Zero-Reward Assumption holds or not,\nfor algorithms in the REINFORCE and Actor-Critic families. This result reveals\nthat widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess\nthe capacity to model token-level reward signals, offering a theoretical\njustification for response-level reward approaches. Our findings pave the way\nfor more practical, efficient LLM fine-tuning, allowing developers to treat\ntraining algorithms as black boxes and focus on improving the response-level\nreward model with auxiliary sub-models. We also offer a detailed analysis of\npopular RL and non-RL methods, comparing their theoretical foundations and\npractical advantages across common LLM tasks. Finally, we propose a new\nalgorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically\ngrounded method that is simpler than PPO, matches GRPO in memory efficiency,\nand holds promise for broad applicability.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u96f6\u5956\u52b1\u5047\u8bbe\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4ec5\u9700\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\u5373\u53ef\u65e0\u504f\u4f30\u8ba1\u7b56\u7565\u68af\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7b97\u6cd5TRePO\u3002", "motivation": "\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u975e\u7ec8\u7aef\u52a8\u4f5c\uff08\u4e2d\u95f4\u4ee4\u724c\u751f\u6210\uff09\u7f3a\u4e4f\u5373\u65f6\u5956\u52b1\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u4ee5\u51cf\u5c11\u5bf9\u7cbe\u786e\u4ee4\u724c\u7ea7\u5956\u52b1\u7684\u4f9d\u8d56\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u8bc1\u660e\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\u53ef\u65e0\u504f\u4f30\u8ba1\u7b56\u7565\u68af\u5ea6\uff0c\u5e76\u63d0\u51fa\u65b0\u7b97\u6cd5TRePO\u3002", "result": "\u7406\u8bba\u8bc1\u660ePPO\u3001GRPO\u7b49\u65b9\u6cd5\u5177\u5907\u5efa\u6a21\u4ee4\u724c\u7ea7\u5956\u52b1\u4fe1\u53f7\u7684\u80fd\u529b\uff0cTRePO\u5728\u7b80\u5316\u6027\u548c\u5185\u5b58\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u8005\u53ef\u4e13\u6ce8\u4e8e\u6539\u8fdb\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\uff0c\u540c\u65f6TRePO\u5c55\u73b0\u4e86\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.03114", "pdf": "https://arxiv.org/pdf/2506.03114", "abs": "https://arxiv.org/abs/2506.03114", "authors": ["Michelle Chen", "David Russell", "Amritha Pallavoor", "Derek Young", "Jane Wu"], "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery", "categories": ["cs.CV"], "comment": "Code:\n  https://github.com/open-forest-observatory/tree-detection-framework", "summary": "Large-scale delineation of individual trees from remote sensing imagery is\ncrucial to the advancement of ecological research, particularly as climate\nchange and other environmental factors rapidly transform forest landscapes\nacross the world. Current RGB tree segmentation methods rely on training\nspecialized machine learning models with labeled tree datasets. While these\nlearning-based approaches can outperform manual data collection when accurate,\nthe existing models still depend on training data that's hard to scale. In this\npaper, we investigate the efficacy of using a state-of-the-art image\nsegmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for\nindividual tree detection and segmentation. We evaluate a pretrained SAM2 model\non two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot\ntransfer by using predictions from an existing tree detection model as prompts.\nOur results suggest that SAM2 not only has impressive generalization\ncapabilities, but also can form a natural synergy with specialized methods\ntrained on in-domain labeled data. We find that applying large pretrained\nmodels to problems in remote sensing is a promising avenue for future progress.\nWe make our code available at:\nhttps://github.com/open-forest-observatory/tree-detection-framework.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u96f6\u6837\u672c\u65b9\u5f0f\u4e0b\u4f7f\u7528SAM2\u6a21\u578b\u8fdb\u884c\u5355\u6811\u68c0\u6d4b\u4e0e\u5206\u5272\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u4e0e\u4e13\u4e1a\u65b9\u6cd5\u534f\u540c\u5de5\u4f5c\u3002", "motivation": "\u5927\u89c4\u6a21\u5355\u6811\u5206\u5272\u5bf9\u751f\u6001\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684SAM2\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u5206\u5272\u548c\u96f6\u6837\u672c\u8fc1\u79fb\uff08\u4ee5\u73b0\u6709\u6811\u68c0\u6d4b\u6a21\u578b\u7684\u9884\u6d4b\u4f5c\u4e3a\u63d0\u793a\uff09\u3002", "result": "SAM2\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u4e0e\u4e13\u4e1a\u65b9\u6cd5\u534f\u540c\uff0c\u4e3a\u9065\u611f\u95ee\u9898\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "conclusion": "\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5728\u9065\u611f\u9886\u57df\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.02590", "pdf": "https://arxiv.org/pdf/2506.02590", "abs": "https://arxiv.org/abs/2506.02590", "authors": ["Dimitrios Koutsianos", "Stavros Zacharopoulos", "Yannis Panagakis", "Themos Stafylakis"], "title": "Synthetic Speech Source Tracing using Metric Learning", "categories": ["cs.SD", "cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "This paper addresses source tracing in synthetic speech-identifying\ngenerative systems behind manipulated audio via speaker recognition-inspired\npipelines. While prior work focuses on spoofing detection, source tracing lacks\nrobust solutions. We evaluate two approaches: classification-based and\nmetric-learning. We tested our methods on the MLAADv5 benchmark using ResNet\nand self-supervised learning (SSL) backbones. The results show that ResNet\nachieves competitive performance with the metric learning approach, matching\nand even exceeding SSL-based systems. Our work demonstrates ResNet's viability\nfor source tracing while underscoring the need to optimize SSL representations\nfor this task. Our work bridges speaker recognition methodologies with audio\nforensic challenges, offering new directions for combating synthetic media\nmanipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8bf4\u8bdd\u4eba\u8bc6\u522b\u65b9\u6cd5\u8ffd\u8e2a\u5408\u6210\u8bed\u97f3\u7684\u751f\u6210\u7cfb\u7edf\uff0c\u6bd4\u8f83\u4e86\u5206\u7c7b\u548c\u5ea6\u91cf\u5b66\u4e60\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0ResNet\u8868\u73b0\u4f18\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4f2a\u9020\u68c0\u6d4b\uff0c\u800c\u7f3a\u4e4f\u5bf9\u5408\u6210\u8bed\u97f3\u6765\u6e90\u8ffd\u8e2a\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u548c\u5ea6\u91cf\u5b66\u4e60\u4e24\u79cd\u65b9\u6cd5\uff0c\u57fa\u4e8eResNet\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u67b6\u6784\uff0c\u5728MLAADv5\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u3002", "result": "ResNet\u5728\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8aSSL\u7cfb\u7edf\u3002", "conclusion": "ResNet\u9002\u7528\u4e8e\u6765\u6e90\u8ffd\u8e2a\u4efb\u52a1\uff0c\u4f46\u9700\u4f18\u5316SSL\u8868\u793a\uff0c\u4e3a\u5408\u6210\u5a92\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03117", "pdf": "https://arxiv.org/pdf/2506.03117", "abs": "https://arxiv.org/abs/2506.03117", "authors": ["Zeliang Zhang", "Gaowen Liu", "Charles Fleming", "Ramana Rao Kompella", "Chenliang Xu"], "title": "Targeted Forgetting of Image Subgroups in CLIP Models", "categories": ["cs.CV"], "comment": "12 Figures,5 Pages. The project page is\n  \\url{https://zhangaipi.github.io/forget_clip/}", "summary": "Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot\nperformance across various tasks by leveraging large-scale, unsupervised\npre-training. However, they often inherit harmful or unwanted knowledge from\nnoisy internet-sourced datasets, compromising their reliability in real-world\napplications. Existing model unlearning methods either rely on access to\npre-trained datasets or focus on coarse-grained unlearning (e.g., entire\nclasses), leaving a critical gap for fine-grained unlearning. In this paper, we\naddress the challenging scenario of selectively forgetting specific portions of\nknowledge within a class, without access to pre-trained data, while preserving\nthe model's overall performance. We propose a novel three-stage approach that\nprogressively unlearns targeted knowledge while mitigating over-forgetting. It\nconsists of (1) a forgetting stage to fine-tune the CLIP on samples to be\nforgotten, (2) a reminding stage to restore performance on retained samples,\nand (3) a restoring stage to recover zero-shot capabilities using model\nsouping. Additionally, we introduce knowledge distillation to handle the\ndistribution disparity between forgetting, retaining samples, and unseen\npre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style\ndatasets demonstrate that our approach effectively unlearns specific subgroups\nwhile maintaining strong zero-shot performance on semantically similar\nsubgroups and other categories, significantly outperforming baseline unlearning\nmethods, which lose effectiveness under the CLIP unlearning setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728CLIP\u6a21\u578b\u4e2d\u7cbe\u7ec6\u5730\u9009\u62e9\u6027\u9057\u5fd8\u7279\u5b9a\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53ef\u80fd\u4ece\u4e92\u8054\u7f51\u6570\u636e\u4e2d\u7ee7\u627f\u6709\u5bb3\u77e5\u8bc6\uff0c\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u7ec6\u5904\u7406\u7279\u5b9a\u77e5\u8bc6\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u9057\u5fd8\u9636\u6bb5\u5fae\u8c03\u76ee\u6807\u77e5\u8bc6\uff0c\u63d0\u9192\u9636\u6bb5\u6062\u590d\u4fdd\u7559\u6837\u672c\u6027\u80fd\uff0c\u6062\u590d\u9636\u6bb5\u901a\u8fc7\u6a21\u578b\u878d\u5408\u6062\u590d\u96f6\u6837\u672c\u80fd\u529b\u3002", "result": "\u5728CIFAR-10\u3001ImageNet-1K\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u6709\u6548\u9057\u5fd8\u7279\u5b9a\u5b50\u7ec4\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u9057\u5fd8\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eCLIP\u6a21\u578b\u7684\u7cbe\u7ec6\u77e5\u8bc6\u9057\u5fd8\u3002"}}
{"id": "2506.03119", "pdf": "https://arxiv.org/pdf/2506.03119", "abs": "https://arxiv.org/abs/2506.03119", "authors": ["Zujin Guo", "Size Wu", "Zhongang Cai", "Wei Li", "Chen Change Loy"], "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior", "categories": ["cs.CV"], "comment": "Project Page: https://gseancdat.github.io/projects/PoseFuse3D_KI", "summary": "Existing interpolation methods use pre-trained video diffusion priors to\ngenerate intermediate frames between sparsely sampled keyframes. In the absence\nof 3D geometric guidance, these methods struggle to produce plausible results\nfor complex, articulated human motions and offer limited control over the\nsynthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe\nInterpolator (PoseFuse3D-KI), a novel framework that integrates 3D human\nguidance signals into the diffusion process for Controllable Human-centric\nKeyframe Interpolation (CHKI). To provide rich spatial and structural cues for\ninterpolation, our PoseFuse3D, a 3D-informed control model, features a novel\nSMPL-X encoder that transforms 3D geometry and shape into the 2D latent\nconditioning space, alongside a fusion network that integrates these 3D cues\nwith 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset\nannotated with both 2D poses and 3D SMPL-X parameters. We show that\nPoseFuse3D-KI consistently outperforms state-of-the-art baselines on\nCHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.\nComprehensive ablations demonstrate that our PoseFuse3D model improves\ninterpolation fidelity.", "AI": {"tldr": "PoseFuse3D-KI\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u54083D\u4eba\u4f53\u5f15\u5bfc\u4fe1\u53f7\u6539\u8fdb\u89c6\u9891\u5173\u952e\u5e27\u63d2\u503c\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u7684\u751f\u6210\u8d28\u91cf\u4e0e\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63d2\u503c\u65b9\u6cd5\u7f3a\u4e4f3D\u51e0\u4f55\u5f15\u5bfc\uff0c\u96be\u4ee5\u751f\u6210\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u4e14\u63a7\u5236\u6709\u9650\u3002", "method": "\u63d0\u51faPoseFuse3D-KI\u6846\u67b6\uff0c\u7ed3\u54083D\u4eba\u4f53\u4fe1\u53f7\uff08SMPL-X\u7f16\u7801\u5668\uff09\u4e0e2D\u59ff\u6001\u5d4c\u5165\uff0c\u901a\u8fc7\u878d\u5408\u7f51\u7edc\u5b9e\u73b0\u53ef\u63a7\u63d2\u503c\u3002", "result": "\u5728CHKI-Video\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u63d0\u53479%\uff0cLPIPS\u964d\u4f4e38%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PoseFuse3D-KI\u901a\u8fc73D\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u63d2\u503c\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.02720", "pdf": "https://arxiv.org/pdf/2506.02720", "abs": "https://arxiv.org/abs/2506.02720", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "categories": ["cs.AI", "cs.CL"], "comment": "KDD 2025", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5efa\u7acb\u57fa\u51c6\u548c\u8bc4\u4f30\u6027\u80fd\uff0c\u53d1\u73b07B\u6a21\u578b\u53ef\u5ab2\u7f8e72B\u6a21\u578b\uff0c\u4f18\u5316\u4e86\u90e8\u7f72\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u5e73\u8861\u6a21\u578b\u80fd\u529b\u4e0e\u63a8\u7406\u6210\u672c\u3002", "method": "\u5efa\u7acb\u7efc\u5408\u57fa\u51c6\uff0c\u8bc4\u4f30LLMs\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u6a21\u578b\u5fae\u8c03\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "7B\u6a21\u578b\u6027\u80fd\u63a5\u8fd172B\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u90e8\u7f72\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u4f18\u5316\u540e\u7684LLMs\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u548c\u53ef\u53ca\u6027\u3002"}}
{"id": "2506.03123", "pdf": "https://arxiv.org/pdf/2506.03123", "abs": "https://arxiv.org/abs/2506.03123", "authors": ["Zhengyao Lv", "Chenyang Si", "Tianlin Pan", "Zhaoxi Chen", "Kwan-Yee K. Wong", "Yu Qiao", "Ziwei Liu"], "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u4e13\u5bb6\u4e00\u81f4\u6027\u6a21\u578b\uff08DCM\uff09\uff0c\u901a\u8fc7\u8bed\u4e49\u4e13\u5bb6\u548c\u7ec6\u8282\u4e13\u5bb6\u7684\u5206\u5de5\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e2d\u7684\u4f18\u5316\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff1b\u4e00\u81f4\u6027\u6a21\u578b\u867d\u80fd\u52a0\u901f\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u9891\u6a21\u578b\u4f1a\u5bfc\u81f4\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faDCM\u6a21\u578b\uff0c\u5305\u542b\u8bed\u4e49\u4e13\u5bb6\u548c\u7ec6\u8282\u4e13\u5bb6\uff0c\u5206\u522b\u4f18\u5316\u8bed\u4e49\u5e03\u5c40\u3001\u8fd0\u52a8\u548c\u7ec6\u8282\uff1b\u5f15\u5165\u65f6\u95f4\u76f8\u5e72\u635f\u5931\u3001GAN\u548c\u7279\u5f81\u5339\u914d\u635f\u5931\u3002", "result": "DCM\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u6570\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u4e13\u5bb6\u5206\u5de5\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5408\u6210\u6548\u679c\u3002"}}
{"id": "2506.02730", "pdf": "https://arxiv.org/pdf/2506.02730", "abs": "https://arxiv.org/abs/2506.02730", "authors": ["Po-Chieh Yu"], "title": "An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models", "categories": ["astro-ph.IM", "cs.CL"], "comment": "submitted to the International Journal of Astrobiology", "summary": "We present an exploratory framework to test whether noise-like input can\ninduce structured responses in language models. Instead of assuming that\nextraterrestrial signals must be decoded, we evaluate whether inputs can\ntrigger linguistic behavior in generative systems. This shifts the focus from\ndecoding to viewing structured output as a sign of underlying regularity in the\ninput. We tested GPT-2 small, a 117M-parameter model trained on English text,\nusing four types of acoustic input: human speech, humpback whale vocalizations,\nPhylloscopus trochilus birdsong, and algorithmically generated white noise. All\ninputs were treated as noise-like, without any assumed symbolic encoding. To\nassess reactivity, we defined a composite score called Semantic Induction\nPotential (SIP), combining entropy, syntax coherence, compression gain, and\nrepetition penalty. Results showed that whale and bird vocalizations had higher\nSIP scores than white noise, while human speech triggered only moderate\nresponses. This suggests that language models may detect latent structure even\nin data without conventional semantics. We propose that this approach could\ncomplement traditional SETI methods, especially in cases where communicative\nintent is unknown. Generative reactivity may offer a different way to identify\ndata worth closer attention.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u566a\u58f0\u8f93\u5165\u662f\u5426\u80fd\u5f15\u53d1\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u54cd\u5e94\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u9cb8\u9c7c\u548c\u9e1f\u7c7b\u7684\u53eb\u58f0\u6bd4\u767d\u566a\u58f0\u66f4\u80fd\u89e6\u53d1\u6a21\u578b\u7684\u8bed\u4e49\u53cd\u5e94\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u4ece\u65e0\u4f20\u7edf\u8bed\u4e49\u7684\u6570\u636e\u4e2d\u68c0\u6d4b\u6f5c\u5728\u7ed3\u6784\uff0c\u4e3a\u4f20\u7edfSETI\u65b9\u6cd5\u63d0\u4f9b\u8865\u5145\u3002", "method": "\u4f7f\u7528GPT-2\u5c0f\u6a21\u578b\uff0c\u6d4b\u8bd5\u56db\u79cd\u58f0\u5b66\u8f93\u5165\uff08\u4eba\u7c7b\u8bed\u97f3\u3001\u9cb8\u9c7c\u53eb\u58f0\u3001\u9e1f\u7c7b\u9e23\u53eb\u548c\u767d\u566a\u58f0\uff09\uff0c\u5b9a\u4e49\u8bed\u4e49\u8bf1\u5bfc\u6f5c\u529b\uff08SIP\uff09\u8bc4\u5206\u6765\u8861\u91cf\u53cd\u5e94\u3002", "result": "\u9cb8\u9c7c\u548c\u9e1f\u7c7b\u7684\u53eb\u58f0SIP\u8bc4\u5206\u9ad8\u4e8e\u767d\u566a\u58f0\uff0c\u4eba\u7c7b\u8bed\u97f3\u53cd\u5e94\u4e2d\u7b49\uff0c\u8868\u660e\u6a21\u578b\u80fd\u68c0\u6d4b\u65e0\u4f20\u7edf\u8bed\u4e49\u6570\u636e\u4e2d\u7684\u7ed3\u6784\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u7684\u53cd\u5e94\u6027\u53ef\u80fd\u4e3a\u8bc6\u522b\u503c\u5f97\u5173\u6ce8\u7684\u6570\u636e\u63d0\u4f9b\u65b0\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u901a\u4fe1\u610f\u56fe\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.03126", "pdf": "https://arxiv.org/pdf/2506.03126", "abs": "https://arxiv.org/abs/2506.03126", "authors": ["Lu Qiu", "Yizhuo Li", "Yuying Ge", "Yixiao Ge", "Ying Shan", "Xihui Liu"], "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation", "categories": ["cs.CV"], "comment": "Project released at: https://qiulu66.github.io/animeshooter/", "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.", "AI": {"tldr": "AnimeShooter\u662f\u4e00\u4e2a\u53c2\u8003\u5f15\u5bfc\u7684\u591a\u955c\u5934\u52a8\u753b\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u89d2\u8272\u53c2\u8003\u56fe\u50cf\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u73b0\u5b9e\u573a\u666f\uff0c\u7f3a\u4e4f\u89d2\u8272\u53c2\u8003\u56fe\u50cf\u548c\u8fde\u8d2f\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u80fd\u529b\uff0cAnimeShooter\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efaAnimeShooter\u6570\u636e\u96c6\uff0c\u5305\u542b\u6545\u4e8b\u7ea7\u548c\u955c\u5934\u7ea7\u6ce8\u91ca\uff0c\u5e76\u5229\u7528MLLM\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u7684\u591a\u955c\u5934\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eAnimeShooter\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8de8\u955c\u5934\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u53c2\u8003\u89c6\u89c9\u5f15\u5bfc\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AnimeShooter\u6570\u636e\u96c6\u4e3a\u8fde\u8d2f\u52a8\u753b\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\uff0c\u5c55\u793a\u4e86\u5176\u5728AIGC\u9886\u57df\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761", "abs": "https://arxiv.org/abs/2506.02761", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "title": "Rethinking Machine Unlearning in Image Generation Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9057\u5fd8\uff08IGMU\uff09\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u5206\u7c7b\u6846\u67b6CatIGMU\u3001\u8bc4\u4f30\u6846\u67b6EvalIGMU\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DataIGM\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6570\u636e\u9690\u79c1\u548c\u5185\u5bb9\u5b89\u5168\u6210\u4e3a\u91cd\u8981\u95ee\u9898\uff0c\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u88ab\u89c6\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u6709\u6548\u624b\u6bb5\u3002\u7136\u800c\uff0cIGMU\u5728\u5b9e\u8df5\u4e2d\u4ecd\u5b58\u5728\u4efb\u52a1\u533a\u5206\u4e0d\u6e05\u3001\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u7b49\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86CatIGMU\uff08\u4efb\u52a1\u5206\u7c7b\u6846\u67b6\uff09\u3001EvalIGMU\uff08\u8bc4\u4f30\u6846\u67b6\uff09\u548cDataIGM\uff08\u6570\u636e\u96c6\uff09\uff0c\u7528\u4e8e\u6807\u51c6\u5316IGMU\u4efb\u52a1\u5e76\u8bc4\u4f30\u73b0\u6709\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709IGMU\u7b97\u6cd5\u5728\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u4fdd\u7559\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u65b0\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u4e3aIGMU\u7684\u6807\u51c6\u5316\u548c\u53ef\u9760\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.03131", "pdf": "https://arxiv.org/pdf/2506.03131", "abs": "https://arxiv.org/abs/2506.03131", "authors": ["Zidong Wang", "Lei Bai", "Xiangyu Yue", "Wanli Ouyang", "Yiyuan Zhang"], "title": "Native-Resolution Image Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://wzdthu.github.io/NiT/", "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u751f\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7Native-resolution diffusion Transformer\uff08NiT\uff09\u5b9e\u73b0\u4efb\u610f\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\u7684\u56fe\u50cf\u751f\u6210\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u56fa\u5b9a\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u5206\u8fa8\u7387\u548c\u65b9\u5f62\u683c\u5f0f\uff0c\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u591a\u6837\u5316\u7684\u89c6\u89c9\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86NiT\u67b6\u6784\uff0c\u901a\u8fc7\u5efa\u6a21\u53ef\u53d8\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u5b9e\u73b0\u539f\u751f\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u3002", "result": "NiT\u5728ImageNet-256x256\u548c512x512\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u751f\u6210\u9ad8\u5206\u8fa8\u7387\uff08\u59821536x1536\uff09\u548c\u591a\u6837\u5bbd\u9ad8\u6bd4\uff08\u598216:9\u30013:1\uff09\u7684\u56fe\u50cf\u3002", "conclusion": "\u539f\u751f\u5206\u8fa8\u7387\u5efa\u6a21\u4e3a\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e0e\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u4e4b\u95f4\u7684\u6865\u6881\u63d0\u4f9b\u4e86\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2506.02867", "pdf": "https://arxiv.org/pdf/2506.02867", "abs": "https://arxiv.org/abs/2506.02867", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u4fe1\u606f\u8bba\u89c6\u89d2\u4e0b\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u53d1\u73b0\u4e2d\u95f4\u8868\u793a\u4e0e\u6b63\u786e\u7b54\u6848\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff08MI\uff09\u5cf0\u503c\u73b0\u8c61\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u5cf0\u503c\u4e0e\u2018\u601d\u8003\u6807\u8bb0\u2019\u76f8\u5173\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u63d0\u5347LRM\u63a8\u7406\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u65e8\u5728\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u63ed\u793a\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u8ddf\u8e2a\u4e2d\u95f4\u8868\u793a\u4e0e\u6b63\u786e\u7b54\u6848\u7684\u4e92\u4fe1\u606f\uff08MI\uff09\u53d8\u5316\uff0c\u5206\u6790MI\u5cf0\u503c\u73b0\u8c61\uff0c\u5e76\u8bc6\u522b\u51fa\u4e0e\u5cf0\u503c\u76f8\u5173\u7684\u2018\u601d\u8003\u6807\u8bb0\u2019\u3002", "result": "\u53d1\u73b0MI\u5cf0\u503c\u4e0e\u6a21\u578b\u9884\u6d4b\u9519\u8bef\u6982\u7387\u4e0b\u964d\u76f8\u5173\uff0c\u4e14\u2018\u601d\u8003\u6807\u8bb0\u2019\u5bf9\u63a8\u7406\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LRMs\u7684\u63a8\u7406\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u5229\u7528\u2018\u601d\u8003\u6807\u8bb0\u2019\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135", "abs": "https://arxiv.org/abs/2506.03135", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "AI": {"tldr": "OmniSpatial\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u7684\u5168\u9762\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\uff0c\u6db5\u76d6\u56db\u5927\u7c7b50\u4e2a\u5b50\u7c7b\uff0c\u5305\u542b1.5K\u95ee\u7b54\u5bf9\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u7840\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u7684\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u4e92\u8054\u7f51\u6570\u636e\u722c\u53d6\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u6784\u5efa\u4e86\u5305\u542b\u52a8\u6001\u63a8\u7406\u3001\u590d\u6742\u7a7a\u95f4\u903b\u8f91\u3001\u7a7a\u95f4\u4ea4\u4e92\u548c\u89c6\u89d2\u8f6c\u6362\u7684OmniSpatial\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u7efc\u5408\u7a7a\u95f4\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "OmniSpatial\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u6539\u8fdb\u7684\u7a7a\u95f4\u3002"}}
{"id": "2506.02890", "pdf": "https://arxiv.org/pdf/2506.02890", "abs": "https://arxiv.org/abs/2506.02890", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "AI": {"tldr": "\u7ec6\u7c92\u5ea6MoE\uff08\u6df7\u5408\u4e13\u5bb6\uff09\u67b6\u6784\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6536\u655b\u6027\u548c\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002\u672c\u6587\u63d0\u51fa\u8bad\u7ec3\u65b9\u6cd5\u5e76\u5bf9\u6bd4\u5176\u4e0e\u6807\u51c6MoE\u7684\u6269\u5c55\u6027\uff0c\u8bc1\u660e\u7ec6\u7c92\u5ea6MoE\u572856B\u53c2\u6570\u89c4\u6a21\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22\u7ec6\u7c92\u5ea6MoE\u67b6\u6784\u5728\u63d0\u5347\u6a21\u578b\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u5b9e\u8bc1\u652f\u6301\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u7ec6\u7c92\u5ea6MoE\u4e0e\u6807\u51c6MoE\u5728\u4e0d\u540c\u89c4\u6a21\uff08\u6700\u9ad856B\u53c2\u6570\uff09\u4e0b\u7684\u6536\u655b\u901f\u5ea6\u3001\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u53ca\u8bad\u7ec3\u5b9e\u8df5\u3002", "result": "\u7ec6\u7c92\u5ea6MoE\u5728\u6700\u5927\u89c4\u6a21\u4e0b\u9a8c\u8bc1\u635f\u5931\u66f4\u4f4e\uff0c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7ec6\u7c92\u5ea6MoE\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2506.03139", "pdf": "https://arxiv.org/pdf/2506.03139", "abs": "https://arxiv.org/abs/2506.03139", "authors": ["Siqi Chen", "Xinyu Dong", "Haolei Xu", "Xingyu Wu", "Fei Tang", "Hang Zhang", "Yuchen Yan", "Linjuan Wu", "Wenqi Zhang", "Guiyang Hou", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench", "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.", "AI": {"tldr": "SVGenius\u662f\u4e00\u4e2a\u5168\u9762\u7684SVG\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d62,377\u4e2a\u67e5\u8be2\uff0c\u8bc4\u4f3022\u4e2a\u4e3b\u6d41\u6a21\u578b\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\u6bd4\u5355\u7eaf\u6269\u5c55\u66f4\u6709\u6548\u3002", "motivation": "\u73b0\u6709SVG\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u8986\u76d6\u8303\u56f4\u6709\u9650\u3001\u7f3a\u4e4f\u590d\u6742\u5ea6\u5206\u5c42\u548c\u788e\u7247\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "SVGenius\u57fa\u4e8e24\u4e2a\u5e94\u7528\u9886\u57df\u7684\u771f\u5b9e\u6570\u636e\uff0c\u901a\u8fc78\u4e2a\u4efb\u52a1\u7c7b\u522b\u548c18\u4e2a\u6307\u6807\u8bc4\u4f30\u6a21\u578b\uff0c\u6db5\u76d6\u7406\u89e3\u3001\u7f16\u8f91\u548c\u751f\u6210\u4e09\u4e2a\u7ef4\u5ea6\u3002", "result": "\u4e13\u6709\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\uff1b\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\u6bd4\u5355\u7eaf\u6269\u5c55\u66f4\u6709\u6548\uff0c\u98ce\u683c\u8fc1\u79fb\u662f\u6700\u5177\u6311\u6218\u7684\u4efb\u52a1\u3002", "conclusion": "SVGenius\u4e3aSVG\u5904\u7406\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5411\u91cf\u56fe\u5f62\u6a21\u578b\u548c\u81ea\u52a8\u5316\u56fe\u5f62\u8bbe\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2506.02992", "pdf": "https://arxiv.org/pdf/2506.02992", "abs": "https://arxiv.org/abs/2506.02992", "authors": ["Li Zhang", "Kevin D. Ashley"], "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u53cd\u601d\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5408\u6cd5\u5408\u89c4\u7684\u6cd5\u5f8b\u8bba\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u548c\u672a\u57fa\u4e8e\u4e8b\u5b9e\u7684\u8bba\u8bc1\uff0c\u5e76\u5728\u4e0d\u53ef\u884c\u65f6\u6709\u6548\u4e2d\u6b62\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6cd5\u5f8b\u8bba\u8bc1\u751f\u6210\u4e2d\u5b58\u5728\u5e7b\u89c9\u3001\u672a\u57fa\u4e8e\u4e8b\u5b9e\u7684\u8bba\u8bc1\u4ee5\u53ca\u65e0\u6cd5\u6709\u6548\u4e2d\u6b62\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ecFactor Analyst\u548cArgument Polisher\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u4e09\u5c42\u6b21\u6cd5\u5f8b\u8bba\u8bc1\uff08\u539f\u544a\u3001\u88ab\u544a\u3001\u53cd\u9a73\uff09\u3002", "result": "\u5728\u591a\u573a\u666f\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u5e7b\u89c9\u3001\u63d0\u9ad8\u4e8b\u5b9e\u5229\u7528\u7387\u548c\u6709\u6548\u4e2d\u6b62\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u591a\u4ee3\u7406\u53cd\u601d\u6846\u67b6\u4e3a\u6cd5\u5f8b\u8bba\u8bc1\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u4fe1\u8d56\u7684AI\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u64cd\u7eb5\u98ce\u9669\u3002"}}
{"id": "2506.03140", "pdf": "https://arxiv.org/pdf/2506.03140", "abs": "https://arxiv.org/abs/2506.03140", "authors": ["Yawen Luo", "Jianhong Bai", "Xiaoyu Shi", "Menghan Xia", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Tianfan Xue"], "title": "CamCloneMaster: Enabling Reference-based Camera Control for Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://camclonemaster.github.io/", "summary": "Camera control is crucial for generating expressive and cinematic videos.\nExisting methods rely on explicit sequences of camera parameters as control\nconditions, which can be cumbersome for users to construct, particularly for\nintricate camera movements. To provide a more intuitive camera control method,\nwe propose CamCloneMaster, a framework that enables users to replicate camera\nmovements from reference videos without requiring camera parameters or\ntest-time fine-tuning. CamCloneMaster seamlessly supports reference-based\ncamera control for both Image-to-Video and Video-to-Video tasks within a\nunified framework. Furthermore, we present the Camera Clone Dataset, a\nlarge-scale synthetic dataset designed for camera clone learning, encompassing\ndiverse scenes, subjects, and camera movements. Extensive experiments and user\nstudies demonstrate that CamCloneMaster outperforms existing methods in terms\nof both camera controllability and visual quality.", "AI": {"tldr": "CamCloneMaster\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u76f8\u673a\u53c2\u6570\u6216\u5fae\u8c03\u7684\u76f4\u89c2\u76f8\u673a\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u89c6\u9891\u590d\u5236\u76f8\u673a\u8fd0\u52a8\uff0c\u652f\u6301\u56fe\u50cf\u5230\u89c6\u9891\u548c\u89c6\u9891\u5230\u89c6\u9891\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u76f8\u673a\u53c2\u6570\u5e8f\u5217\uff0c\u64cd\u4f5c\u7e41\u7410\uff0c\u96be\u4ee5\u5b9e\u73b0\u590d\u6742\u76f8\u673a\u8fd0\u52a8\u3002", "method": "\u63d0\u51faCamCloneMaster\u6846\u67b6\uff0c\u5229\u7528\u53c2\u8003\u89c6\u9891\u590d\u5236\u76f8\u673a\u8fd0\u52a8\uff0c\u65e0\u9700\u76f8\u673a\u53c2\u6570\u6216\u5fae\u8c03\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6Camera Clone Dataset\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cCamCloneMaster\u5728\u76f8\u673a\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CamCloneMaster\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u7684\u76f8\u673a\u63a7\u5236\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2506.03053", "pdf": "https://arxiv.org/pdf/2506.03053", "abs": "https://arxiv.org/abs/2506.03053", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMAEBE\u6846\u67b6\uff0c\u8bc4\u4f30\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u6d8c\u73b0\u98ce\u9669\uff0c\u53d1\u73b0LLM\u9053\u5fb7\u504f\u597d\u6613\u53d7\u95ee\u9898\u6846\u67b6\u5f71\u54cd\uff0c\u4e14\u7fa4\u4f53\u884c\u4e3a\u65e0\u6cd5\u4ece\u5355\u667a\u80fd\u4f53\u884c\u4e3a\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edfAI\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u5b64\u7acbLLM\u7684\u6d4b\u8bd5\u4e0d\u8db3\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e26\u6765\u65b0\u7684\u6d8c\u73b0\u98ce\u9669\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u91c7\u7528MAEBE\u6846\u67b6\u548cGreatest Good Benchmark\uff0c\u7ed3\u5408\u53cc\u53cd\u8f6c\u95ee\u9898\u6280\u672f\uff0c\u5206\u6790\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u7684\u9053\u5fb7\u504f\u597d\u4e0e\u884c\u4e3a\u3002", "result": "LLM\u9053\u5fb7\u504f\u597d\u8106\u5f31\u4e14\u6613\u53d7\u95ee\u9898\u6846\u67b6\u5f71\u54cd\uff1b\u7fa4\u4f53\u884c\u4e3a\u6d8c\u73b0\u4e14\u65e0\u6cd5\u4ece\u5355\u667a\u80fd\u4f53\u9884\u6d4b\uff1b\u7fa4\u4f53\u4e2d\u5b58\u5728\u540c\u4f34\u538b\u529b\u7b49\u73b0\u8c61\u3002", "conclusion": "\u9700\u5728\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u60c5\u5883\u4e0b\u8bc4\u4f30AI\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u5176\u72ec\u7279\u7684\u5b89\u5168\u548c\u5bf9\u9f50\u6311\u6218\u3002"}}
{"id": "2506.03141", "pdf": "https://arxiv.org/pdf/2506.03141", "abs": "https://arxiv.org/abs/2506.03141", "authors": ["Jiwen Yu", "Jianhong Bai", "Yiran Qin", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Xihui Liu"], "title": "Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in interactive video generation have shown promising results,\nyet existing approaches struggle with scene-consistent memory capabilities in\nlong video generation due to limited use of historical context. In this work,\nwe propose Context-as-Memory, which utilizes historical context as memory for\nvideo generation. It includes two simple yet effective designs: (1) storing\ncontext in frame format without additional post-processing; (2) conditioning by\nconcatenating context and frames to be predicted along the frame dimension at\nthe input, requiring no external control modules. Furthermore, considering the\nenormous computational overhead of incorporating all historical context, we\npropose the Memory Retrieval module to select truly relevant context frames by\ndetermining FOV (Field of View) overlap between camera poses, which\nsignificantly reduces the number of candidate frames without substantial\ninformation loss. Experiments demonstrate that Context-as-Memory achieves\nsuperior memory capabilities in interactive long video generation compared to\nSOTAs, even generalizing effectively to open-domain scenarios not seen during\ntraining. The link of our project page is https://context-as-memory.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContext-as-Memory\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u8bb0\u5fc6\u6765\u63d0\u5347\u957f\u89c6\u9891\u751f\u6210\u7684\u573a\u666f\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u5b58\u50a8\u548c\u68c0\u7d22\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u56e0\u5386\u53f2\u4e0a\u4e0b\u6587\u5229\u7528\u4e0d\u8db3\u800c\u96be\u4ee5\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u3002", "method": "1. \u4ee5\u5e27\u683c\u5f0f\u5b58\u50a8\u4e0a\u4e0b\u6587\uff1b2. \u901a\u8fc7\u5e27\u7ef4\u5ea6\u62fc\u63a5\u4e0a\u4e0b\u6587\u548c\u5f85\u9884\u6d4b\u5e27\uff1b3. \u5f15\u5165Memory Retrieval\u6a21\u5757\u9009\u62e9\u76f8\u5173\u4e0a\u4e0b\u6587\u5e27\u3002", "result": "Context-as-Memory\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5f00\u653e\u57df\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u573a\u666f\u4e00\u81f4\u6027\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2506.03100", "pdf": "https://arxiv.org/pdf/2506.03100", "abs": "https://arxiv.org/abs/2506.03100", "authors": ["Yang Guo", "Yutian Tao", "Yifei Ming", "Robert D. Nowak", "Yingyu Liang"], "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "Under Review", "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u6709\u9650\u6837\u672c\u6cdb\u5316\u754c\uff0c\u7528\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u63a8\u5bfc\u51fa\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1RAG\u5728\u5b9e\u8df5\u4e2d\u6709\u8bb8\u591a\u6210\u529f\u6848\u4f8b\uff0c\u4f46\u5176\u7406\u8bba\u65b9\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u68c0\u7d22\u6587\u672c\u89c6\u4e3a\u67e5\u8be2\u76f8\u5173\u7684\u566a\u58f0\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u5e76\u63a8\u5bfc\u51faRAG\u548c\u7ecf\u5178\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u6781\u9650\u60c5\u51b5\u3002", "result": "\u5206\u6790\u8868\u660eRAG\u5b58\u5728\u56fa\u6709\u6cdb\u5316\u8bef\u5dee\u4e0a\u9650\uff0c\u4e14\u6846\u67b6\u80fd\u5efa\u6a21\u4ece\u8bad\u7ec3\u6570\u636e\u548c\u5916\u90e8\u8bed\u6599\u5e93\u7684\u68c0\u7d22\u3002", "conclusion": "\u7406\u8bba\u4e0e\u5b9e\u9a8c\u4e00\u81f4\u8868\u660eICL\u548cRAG\u5728\u6837\u672c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\uff0c\u652f\u6301\u4e86\u7406\u8bba\u5206\u6790\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144", "abs": "https://arxiv.org/abs/2506.03144", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MERIT\u6570\u636e\u96c6\u548cCoral\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6761\u4ef6\u8bed\u4e49\u68c0\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u68c0\u7d22\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u4e00\u8bed\u8a00\u6216\u6761\u4ef6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u591a\u6761\u4ef6\u67e5\u8be2\u9700\u6c42\u3002", "method": "\u63d0\u51faMERIT\u6570\u636e\u96c6\u548cCoral\u6846\u67b6\uff0c\u7ed3\u5408\u5d4c\u5165\u91cd\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "Coral\u5728MERIT\u4e0a\u6027\u80fd\u63d0\u534745.9%\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u4e3a\u591a\u6761\u4ef6\u8bed\u4e49\u68c0\u7d22\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8d21\u732e\u5305\u62ec\u6570\u636e\u96c6\u3001\u95ee\u9898\u53d1\u73b0\u548c\u521b\u65b0\u6846\u67b6\u3002"}}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniWorld\u7684\u7edf\u4e00\u751f\u6210\u6846\u67b6\uff0c\u57fa\u4e8e\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u8bed\u4e49\u7f16\u7801\u5668\u63d0\u4f9b\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u611f\u77e5\u548c\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u56fe\u50cf\u611f\u77e5\u548c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u80fd\u529b\u6709\u9650\uff0c\u800cGPT-4o-Image\u7684\u6210\u529f\u542f\u53d1\u4e86\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\u3002", "method": "UniWorld\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u8bed\u4e49\u7f16\u7801\u5668\u63d0\u53d6\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u800c\u975e\u4f20\u7edf\u7684VAE\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u7edf\u4e00\u6a21\u578b\u3002", "result": "UniWorld\u4ec5\u4f7f\u75281%\u7684BAGEL\u6570\u636e\uff0c\u4fbf\u5728\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u4e0a\u8d85\u8d8aBAGEL\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "UniWorld\u5c55\u793a\u4e86\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u7edf\u4e00\u6846\u67b6\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u6743\u91cd\u3001\u8bad\u7ec3\u811a\u672c\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.03148", "pdf": "https://arxiv.org/pdf/2506.03148", "abs": "https://arxiv.org/abs/2506.03148", "authors": ["Ayush Shrivastava", "Andrew Owens"], "title": "Self-Supervised Spatial Correspondence Across Modalities", "categories": ["cs.CV"], "comment": "CVPR 2025. Project link: https://www.ayshrv.com/cmrw . Code:\n  https://github.com/ayshrv/cmrw", "summary": "We present a method for finding cross-modal space-time correspondences. Given\ntwo images from different visual modalities, such as an RGB image and a depth\nmap, our model identifies which pairs of pixels correspond to the same physical\npoints in the scene. To solve this problem, we extend the contrastive random\nwalk framework to simultaneously learn cycle-consistent feature representations\nfor both cross-modal and intra-modal matching. The resulting model is simple\nand has no explicit photo-consistency assumptions. It can be trained entirely\nusing unlabeled data, without the need for any spatially aligned multimodal\nimage pairs. We evaluate our method on both geometric and semantic\ncorrespondence tasks. For geometric matching, we consider challenging tasks\nsuch as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic\nmatching, we evaluate on photo-sketch and cross-style image alignment. Our\nmethod achieves strong performance across all benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u65f6\u7a7a\u5bf9\u5e94\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u9f50\u6570\u636e\u5373\u53ef\u5b66\u4e60\u7279\u5f81\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u89c6\u89c9\u6a21\u6001\uff08\u5982RGB\u4e0e\u6df1\u5ea6\u56fe\uff09\u4e4b\u95f4\u7684\u50cf\u7d20\u7ea7\u5bf9\u5e94\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u5bf9\u9f50\u6570\u636e\u6216\u5149\u4e00\u81f4\u6027\u5047\u8bbe\u3002", "method": "\u6269\u5c55\u5bf9\u6bd4\u968f\u673a\u6e38\u8d70\u6846\u67b6\uff0c\u540c\u65f6\u5b66\u4e60\u8de8\u6a21\u6001\u548c\u6a21\u6001\u5185\u5339\u914d\u7684\u5faa\u73af\u4e00\u81f4\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u51e0\u4f55\u548c\u8bed\u4e49\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ecRGB-\u6df1\u5ea6\u3001RGB-\u70ed\u6210\u50cf\u3001\u7167\u7247-\u7d20\u63cf\u548c\u8de8\u98ce\u683c\u56fe\u50cf\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8de8\u6a21\u6001\u5339\u914d\u4efb\u52a1\u3002"}}
{"id": "2506.03150", "pdf": "https://arxiv.org/pdf/2506.03150", "abs": "https://arxiv.org/abs/2506.03150", "authors": ["Yuanze Lin", "Yi-Wen Chen", "Yi-Hsuan Tsai", "Ronald Clark", "Ming-Hsuan Yang"], "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Tech Report", "summary": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page", "AI": {"tldr": "IllumiCraft\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408HDR\u89c6\u9891\u3001\u5408\u6210\u91cd\u5149\u7167\u5e27\u548c3D\u70b9\u8f68\u8ff9\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u5bf9\u51e0\u4f55\u7ebf\u7d22\u7684\u663e\u5f0f\u96c6\u6210\uff0c\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u573a\u666f\u5149\u7167\u548c\u89c6\u89c9\u5916\u89c2\u3002", "method": "\u7ed3\u5408HDR\u89c6\u9891\u3001\u5408\u6210\u91cd\u5149\u7167\u5e27\u548c3D\u70b9\u8f68\u8ff9\uff0c\u7edf\u4e00\u6269\u6563\u67b6\u6784\u751f\u6210\u89c6\u9891\u3002", "result": "\u751f\u6210\u7684\u89c6\u9891\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u53ef\u63a7\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "IllumiCraft\u901a\u8fc7\u6574\u5408\u591a\u79cd\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u7cbe\u786e\u6027\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CV", "q-bio.NC", "stat.ML"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u8054\u60f3\u8bb0\u5fc6\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u5728\u5c0f\u6570\u636e\u548c\u5927\u6570\u636e\u6761\u4ef6\u4e0b\u4e0d\u540c\u7684\u8bb0\u5fc6\u4e0e\u751f\u6210\u73b0\u8c61\uff0c\u5e76\u9884\u6d4b\u4e86\u865a\u5047\u72b6\u6001\u7684\u5b58\u5728\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u8054\u60f3\u8bb0\u5fc6\u6846\u67b6\u4e0b\u7684\u884c\u4e3a\uff0c\u4ee5\u7406\u89e3\u5176\u8bb0\u5fc6\u4e0e\u751f\u6210\u673a\u5236\uff0c\u7279\u522b\u662f\u865a\u5047\u72b6\u6001\u7684\u51fa\u73b0\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u751f\u6210\u9636\u6bb5\u5206\u522b\u7c7b\u6bd4\u4e3a\u8bb0\u5fc6\u7f16\u7801\u548c\u68c0\u7d22\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u5728\u5c0f\u6570\u636e\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u8bb0\u5fc6\u6027\uff1b\u5927\u6570\u636e\u65f6\uff0c\u751f\u6210\u65b0\u7684\u5438\u5f15\u5b50\u72b6\u6001\uff0c\u865a\u5047\u72b6\u6001\u5728\u8fc7\u6e21\u8fb9\u754c\u51fa\u73b0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6-\u6cdb\u5316\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u9a8c\u8bc1\u4e86\u865a\u5047\u72b6\u6001\u7684\u7406\u8bba\u9884\u6d4b\u3002"}}
{"id": "2506.01970", "pdf": "https://arxiv.org/pdf/2506.01970", "abs": "https://arxiv.org/abs/2506.01970", "authors": ["Ruizhuo Song", "Beiming Yuan"], "title": "Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability", "categories": ["cs.LG", "cs.CV"], "comment": "15 pages, 15 figures, 5 tables", "summary": "This paper thoroughly investigates the challenges of enhancing AI's abstract\nreasoning capabilities, with a particular focus on Raven's Progressive Matrices\n(RPM) tasks involving complex human-like concepts. Firstly, it dissects the\nempirical reality that traditional end-to-end RPM-solving models heavily rely\non option pool configurations, highlighting that this dependency constrains the\nmodel's reasoning capabilities. To address this limitation, the paper proposes\nthe Johnny architecture - a novel representation space-based framework for\nRPM-solving. Through the synergistic operation of its Representation Extraction\nModule and Reasoning Module, Johnny significantly enhances reasoning\nperformance by supplementing primitive negative option configurations with a\nlearned representation space. Furthermore, to strengthen the model's capacity\nfor capturing positional relationships among local features, the paper\nintroduces the Spin-Transformer network architecture, accompanied by a\nlightweight Straw Spin-Transformer variant that reduces computational overhead\nthrough parameter sharing and attention mechanism optimization. Experimental\nevaluations demonstrate that both Johnny and Spin-Transformer achieve superior\nperformance on RPM tasks, offering innovative methodologies for advancing AI's\nabstract reasoning capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faJohnny\u67b6\u6784\u548cSpin-Transformer\u7f51\u7edc\uff0c\u901a\u8fc7\u6539\u8fdb\u8868\u793a\u7a7a\u95f4\u548c\u4f4d\u7f6e\u5173\u7cfb\u6355\u6349\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u5728Raven\u6e10\u8fdb\u77e9\u9635\u4efb\u52a1\u4e2d\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRPM\u89e3\u51b3\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u9009\u9879\u6c60\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faJohnny\u67b6\u6784\uff08\u8868\u793a\u63d0\u53d6\u4e0e\u63a8\u7406\u6a21\u5757\uff09\u548cSpin-Transformer\u7f51\u7edc\uff08\u4f18\u5316\u4f4d\u7f6e\u5173\u7cfb\u6355\u6349\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aJohnny\u548cSpin-Transformer\u5728RPM\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3a\u63d0\u5347AI\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u521b\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.01980", "pdf": "https://arxiv.org/pdf/2506.01980", "abs": "https://arxiv.org/abs/2506.01980", "authors": ["Lianhao Yin", "Ozanan Meireles", "Guy Rosman", "Daniela Rus"], "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time video understanding is critical to guide procedures in minimally\ninvasive surgery (MIS). However, supervised learning approaches require large,\nannotated datasets that are scarce due to annotation efforts that are\nprohibitive, e.g., in medical fields. Although self-supervision methods can\naddress such limitations, current self-supervised methods often fail to capture\nstructural and physical information in a form that generalizes across tasks. We\npropose Compress-to-Explore (C2E), a novel self-supervised framework that\nleverages Kolmogorov complexity to learn compact, informative representations\nfrom surgical videos. C2E uses entropy-maximizing decoders to compress images\nwhile preserving clinically relevant details, improving encoder performance\nwithout labeled data. Trained on large-scale unlabeled surgical datasets, C2E\ndemonstrates strong generalization across a variety of surgical ML tasks, such\nas workflow classification, tool-tissue interaction classification,\nsegmentation, and diagnosis tasks, providing improved performance as a surgical\nvisual foundation model. As we further show in the paper, the model's internal\ncompact representation better disentangles features from different structural\nparts of images. The resulting performance improvements highlight the yet\nuntapped potential of self-supervised learning to enhance surgical AI and\nimprove outcomes in MIS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompress-to-Explore (C2E)\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528Kolmogorov\u590d\u6742\u5ea6\u4ece\u624b\u672f\u89c6\u9891\u4e2d\u5b66\u4e60\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u63d0\u5347\u7f16\u7801\u5668\u6027\u80fd\u3002", "motivation": "\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u5bf9\u5fae\u521b\u624b\u672f\uff08MIS\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u533b\u5b66\u9886\u57df\u7684\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u7a00\u7f3a\u3002\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6cdb\u5316\u6027\u5f3a\u7684\u7ed3\u6784\u548c\u7269\u7406\u4fe1\u606f\u3002", "method": "C2E\u6846\u67b6\u901a\u8fc7\u71b5\u6700\u5927\u5316\u89e3\u7801\u5668\u538b\u7f29\u56fe\u50cf\uff0c\u4fdd\u7559\u4e34\u5e8a\u76f8\u5173\u7ec6\u8282\uff0c\u4ece\u800c\u5b66\u4e60\u7d27\u51d1\u8868\u793a\u3002", "result": "\u5728\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u624b\u672f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cC2E\u5728\u591a\u79cd\u624b\u672fML\u4efb\u52a1\uff08\u5982\u5de5\u4f5c\u6d41\u5206\u7c7b\u3001\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u5206\u7c7b\u3001\u5206\u5272\u548c\u8bca\u65ad\uff09\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "C2E\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u63d0\u5347\u624b\u672fAI\u6027\u80fd\u548c\u6539\u5584MIS\u7ed3\u679c\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.02060", "pdf": "https://arxiv.org/pdf/2506.02060", "abs": "https://arxiv.org/abs/2506.02060", "authors": ["Javier Salazar Cavazos", "Scott Peltier"], "title": "Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model", "categories": ["eess.IV", "cs.CV"], "comment": "Published in International Society for Magnetic Resonance in Medicine\n  (ISMRM) 2025 under submission number 3398", "summary": "Previous works in the literature apply 3D spatial-only models on 4D\nfunctional MRI data leading to possible sub-par feature extraction to be used\nfor downstream tasks like classification. In this work, we aim to develop a\nnovel 4D convolution network to extract 4D joint temporal-spatial kernels that\nnot only learn spatial information but in addition also capture temporal\ndynamics. Experimental results show promising performance in capturing\nspatial-temporal data in functional MRI compared to 3D models. The 4D CNN model\nimproves Alzheimers disease diagnosis for rs-fMRI data, enabling earlier\ndetection and better interventions. Future research could explore task-based\nfMRI applications and regression tasks, enhancing understanding of cognitive\nperformance and disease progression.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b4D\u5377\u79ef\u7f51\u7edc\uff0c\u7528\u4e8e\u63d0\u53d6\u529f\u80fdMRI\u6570\u636e\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u76f8\u6bd4\u4f20\u7edf3D\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f7f\u75283D\u7a7a\u95f4\u6a21\u578b\u5904\u74064D\u529f\u80fdMRI\u6570\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd4D\u5377\u79ef\u7f51\u7edc\uff0c\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u4fe1\u606f\u548c\u65f6\u95f4\u52a8\u6001\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4D CNN\u5728\u529f\u80fdMRI\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u4e8e3D\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bca\u65ad\u6548\u679c\u3002", "conclusion": "\u672a\u6765\u53ef\u63a2\u7d22\u57fa\u4e8e\u4efb\u52a1\u7684fMRI\u5e94\u7528\u548c\u56de\u5f52\u4efb\u52a1\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7406\u89e3\u8ba4\u77e5\u8868\u73b0\u548c\u75be\u75c5\u8fdb\u5c55\u3002"}}
{"id": "2506.02065", "pdf": "https://arxiv.org/pdf/2506.02065", "abs": "https://arxiv.org/abs/2506.02065", "authors": ["Shriraj P. Sawant", "Krishna P. Miyapuram"], "title": "EWGN: Elastic Weight Generation and Context Switching in Deep Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The ability to learn and retain a wide variety of tasks is a hallmark of\nhuman intelligence that has inspired research in artificial general\nintelligence. Continual learning approaches provide a significant step towards\nachieving this goal. It has been known that task variability and context\nswitching are challenging for learning in neural networks. Catastrophic\nforgetting refers to the poor performance on retention of a previously learned\ntask when a new task is being learned. Switching between different task\ncontexts can be a useful approach to mitigate the same by preventing the\ninterference between the varying task weights of the network. This paper\nintroduces Elastic Weight Generative Networks (EWGN) as an idea for context\nswitching between two different tasks. The proposed EWGN architecture uses an\nadditional network that generates the weights of the primary network\ndynamically while consolidating the weights learned. The weight generation is\ninput-dependent and thus enables context switching. Using standard computer\nvision datasets, namely MNIST and fashion-MNIST, we analyse the retention of\npreviously learned task representations in Fully Connected Networks,\nConvolutional Neural Networks, and EWGN architectures with Stochastic Gradient\nDescent and Elastic Weight Consolidation learning algorithms. Understanding\ndynamic weight generation and context-switching ability can be useful in\nenabling continual learning for improved performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5f39\u6027\u6743\u91cd\u751f\u6210\u7f51\u7edc\uff08EWGN\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u6743\u91cd\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u6765\u7f13\u89e3\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728MNIST\u548cfashion-MNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u5b66\u4e60\u548c\u4fdd\u7559\u591a\u79cd\u4efb\u52a1\u7684\u80fd\u529b\u6fc0\u53d1\u4e86\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u7814\u7a76\uff0c\u800c\u6301\u7eed\u5b66\u4e60\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u91cd\u8981\u6b65\u9aa4\u3002\u707e\u96be\u6027\u9057\u5fd8\u662f\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "EWGN\u901a\u8fc7\u4e00\u4e2a\u989d\u5916\u7684\u7f51\u7edc\u52a8\u6001\u751f\u6210\u4e3b\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u8f93\u5165\u4f9d\u8d56\u7684\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u5e76\u7ed3\u5408\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u548c\u5f39\u6027\u6743\u91cd\u5de9\u56fa\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728MNIST\u548cfashion-MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEWGN\u80fd\u591f\u6709\u6548\u4fdd\u7559\u5148\u524d\u5b66\u4e60\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "\u52a8\u6001\u6743\u91cd\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u80fd\u529b\u6709\u52a9\u4e8e\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.02079", "pdf": "https://arxiv.org/pdf/2506.02079", "abs": "https://arxiv.org/abs/2506.02079", "authors": ["Xuefeng Jiang", "Tian Wen", "Zhiqin Yang", "Lvhua Wu", "Yufeng Chen", "Sheng Sun", "Yuwei Wang", "Min Liu"], "title": "Robust Federated Learning against Noisy Clients via Masked Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Under review", "summary": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaskedOptim\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u590d\u6742\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b\u9ad8\u566a\u58f0\u5ba2\u6237\u7aef\u548c\u6807\u7b7e\u6821\u6b63\u673a\u5236\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u63d0\u4f9b\u7684\u6807\u6ce8\u6570\u636e\u5e38\u542b\u590d\u6742\u6807\u7b7e\u566a\u58f0\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u9700\u5f00\u53d1\u6709\u6548\u7b56\u7565\u51cf\u8f7b\u5176\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u68c0\u6d4b\u9ad8\u566a\u58f0\u5ba2\u6237\u7aef\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7aef\u5230\u7aef\u6807\u7b7e\u6821\u6b63\u673a\u5236\u4fee\u6b63\u566a\u58f0\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u51e0\u4f55\u4e2d\u503c\u6a21\u578b\u805a\u5408\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u9c81\u68d2\uff0c\u4e14\u80fd\u6709\u6548\u63d0\u5347\u566a\u58f0\u5ba2\u6237\u7aef\u672c\u5730\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002", "conclusion": "MaskedOptim\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2506.02093", "pdf": "https://arxiv.org/pdf/2506.02093", "abs": "https://arxiv.org/abs/2506.02093", "authors": ["Tianyu Lin", "Xinran Li", "Chuntung Zhuang", "Qi Chen", "Yuanhao Cai", "Kai Ding", "Alan L. Yuille", "Zongwei Zhou"], "title": "Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Widely adopted evaluation metrics for sparse-view CT reconstruction--such as\nStructural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize\npixel-wise fidelity but often fail to capture the completeness of critical\nanatomical structures, particularly small or thin regions that are easily\nmissed. To address this limitation, we propose a suite of novel anatomy-aware\nevaluation metrics designed to assess structural completeness across anatomical\nstructures, including large organs, small organs, intestines, and vessels.\nBuilding on these metrics, we introduce CARE, a Completeness-Aware\nReconstruction Enhancement framework that incorporates structural penalties\nduring training to encourage anatomical preservation of significant structures.\nCARE is model-agnostic and can be seamlessly integrated into analytical,\nimplicit, and generative methods. When applied to these methods, CARE\nsubstantially improves structural completeness in CT reconstructions, achieving\nup to +32% improvement for large organs, +22% for small organs, +40% for\nintestines, and +36% for vessels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u89e3\u5256\u611f\u77e5\u8bc4\u4f30\u6307\u6807\u548cCARE\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u4e2d\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u6355\u6349\u5173\u952e\u89e3\u5256\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\uff08\u5982SSIM\u548cPSNR\uff09\u5728\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u4e2d\u96be\u4ee5\u8bc4\u4f30\u5173\u952e\u89e3\u5256\u7ed3\u6784\u7684\u5b8c\u6574\u6027\uff0c\u5c24\u5176\u662f\u5c0f\u6216\u8584\u533a\u57df\u3002", "method": "\u63d0\u51fa\u89e3\u5256\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f00\u53d1CARE\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u60e9\u7f5a\u5728\u8bad\u7ec3\u4e2d\u589e\u5f3a\u89e3\u5256\u7ed3\u6784\u7684\u4fdd\u7559\u3002", "result": "CARE\u663e\u8457\u63d0\u9ad8\u4e86CT\u91cd\u5efa\u7684\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5927\u5668\u5b98\u63d0\u534732%\uff0c\u5c0f\u5668\u5b9822%\uff0c\u80a0\u905340%\uff0c\u8840\u7ba136%\u3002", "conclusion": "CARE\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u4e2d\u89e3\u5256\u7ed3\u6784\u7684\u5b8c\u6574\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u91cd\u5efa\u65b9\u6cd5\u3002"}}
{"id": "2506.02197", "pdf": "https://arxiv.org/pdf/2506.02197", "abs": "https://arxiv.org/abs/2506.02197", "authors": ["Marcos V. Conde", "Radu Timofte", "Zihao Lu", "Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand"], "title": "NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "CVPR 2025 - New Trends in Image Restoration and Enhancement (NTIRE)", "summary": "This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution\nChallenge, highlighting the proposed solutions and results. New methods for RAW\nRestoration and Super-Resolution could be essential in modern Image Signal\nProcessing (ISP) pipelines, however, this problem is not as explored as in the\nRGB domain. The goal of this challenge is two fold, (i) restore RAW images with\nblur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering\nunknown noise and blur. In the challenge, a total of 230 participants\nregistered, and 45 submitted results during thee challenge period. This report\npresents the current state-of-the-art in RAW Restoration.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86NTIRE 2025 RAW\u56fe\u50cf\u4fee\u590d\u4e0e\u8d85\u5206\u8fa8\u7387\u6311\u6218\u8d5b\uff0c\u603b\u7ed3\u4e86\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u548c\u7ed3\u679c\u3002", "motivation": "RAW\u56fe\u50cf\u4fee\u590d\u4e0e\u8d85\u5206\u8fa8\u7387\u5728\u73b0\u4ee3\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff08ISP\uff09\u6d41\u7a0b\u4e2d\u53ef\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8be5\u9886\u57df\u7684\u7814\u7a76\u4e0d\u5982RGB\u9886\u57df\u6df1\u5165\u3002", "method": "\u6311\u6218\u8d5b\u7684\u76ee\u6807\u5305\u62ec\u4fee\u590d\u5e26\u6709\u6a21\u7cca\u548c\u566a\u58f0\u7684RAW\u56fe\u50cf\uff0c\u4ee5\u53ca\u5c06RAW Bayer\u56fe\u50cf\u653e\u59272\u500d\uff08\u8003\u8651\u672a\u77e5\u566a\u58f0\u548c\u6a21\u7cca\uff09\u3002", "result": "\u5171\u6709230\u540d\u53c2\u4e0e\u8005\u6ce8\u518c\uff0c45\u540d\u63d0\u4ea4\u4e86\u7ed3\u679c\u3002", "conclusion": "\u62a5\u544a\u5c55\u793a\u4e86\u5f53\u524dRAW\u56fe\u50cf\u4fee\u590d\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002"}}
{"id": "2506.02214", "pdf": "https://arxiv.org/pdf/2506.02214", "abs": "https://arxiv.org/abs/2506.02214", "authors": ["Alexey Burdakov", "Max Jaihyun Ahn"], "title": "Is PMBOK Guide the Right Fit for AI? Re-evaluating Project Management in the Face of Artificial Intelligence Projects", "categories": ["cs.SE", "cs.CV", "D.2.9; I.4"], "comment": "9 pages, 1 figure", "summary": "This paper critically evaluates the applicability of the Project Management\nBody of Knowledge (PMBOK) Guide framework to Artificial Intelligence (AI)\nsoftware projects, highlighting key limitations and proposing tailored\nadaptations. Unlike traditional projects, AI initiatives rely heavily on\ncomplex data, iterative experimentation, and specialized expertise while\nnavigating significant ethical considerations. Our analysis identifies gaps in\nthe PMBOK Guide, including its limited focus on data management, insufficient\nsupport for iterative development, and lack of guidance on ethical and\nmultidisciplinary challenges. To address these deficiencies, we recommend\nintegrating data lifecycle management, adopting iterative and AI project\nmanagement frameworks, and embedding ethical considerations within project\nplanning and execution. Additionally, we explore alternative approaches that\nbetter align with AI's dynamic and exploratory nature. We aim to enhance\nproject management practices for AI software projects by bridging these gaps.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86PMBOK\u6307\u5357\u5728AI\u8f6f\u4ef6\u9879\u76ee\u4e2d\u7684\u9002\u7528\u6027\uff0c\u6307\u51fa\u4e86\u5176\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u9879\u76ee\u7ba1\u7406\u6846\u67b6\uff08\u5982PMBOK\uff09\u5728AI\u9879\u76ee\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7ba1\u7406\u3001\u8fed\u4ee3\u5f00\u53d1\u548c\u4f26\u7406\u95ee\u9898\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u5206\u6790PMBOK\u6307\u5357\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u6574\u5408\u6570\u636e\u751f\u547d\u5468\u671f\u7ba1\u7406\u3001\u8fed\u4ee3\u6846\u67b6\u548c\u4f26\u7406\u8003\u91cf\u7684\u6539\u8fdb\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0PMBOK\u5728AI\u9879\u76ee\u4e2d\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u9488\u5bf9\u6027\u8c03\u6574\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684\u9879\u76ee\u7ba1\u7406\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u9002\u5e94AI\u9879\u76ee\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\u3002"}}
{"id": "2506.02312", "pdf": "https://arxiv.org/pdf/2506.02312", "abs": "https://arxiv.org/abs/2506.02312", "authors": ["Md Tauhidul Islam", "Wu Da-Wen", "Tang Qing-Qing", "Zhao Kai-Yang", "Yin Teng", "Li Yan-Fei", "Shang Wen-Yi", "Liu Jing-Yu", "Zhang Hai-Xian"], "title": "Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation", "categories": ["eess.IV", "cs.CV", "I.4; I.5"], "comment": null, "summary": "Retinal blood vessel segmentation is crucial for diagnosing ocular and\ncardiovascular diseases. Although the introduction of U-Net in 2015 by Olaf\nRonneberger significantly advanced this field, yet issues like limited training\ndata, imbalance data distribution, and inadequate feature extraction persist,\nhindering both the segmentation performance and optimal model generalization.\nAddressing these critical issues, the DEFFA-Unet is proposed featuring an\nadditional encoder to process domain-invariant pre-processed inputs, thereby\nimproving both richer feature encoding and enhanced model generalization. A\nfeature filtering fusion module is developed to ensure the precise feature\nfiltering and robust hybrid feature fusion. In response to the task-specific\nneed for higher precision where false positives are very costly, traditional\nskip connections are replaced with the attention-guided feature reconstructing\nfusion module. Additionally, innovative data augmentation and balancing methods\nare proposed to counter data scarcity and distribution imbalance, further\nboosting the robustness and generalization of the model. With a comprehensive\nsuite of evaluation metrics, extensive validations on four benchmark datasets\n(DRIVE, CHASEDB1, STARE, and HRF) and an SLO dataset (IOSTAR), demonstrate the\nproposed method's superiority over both baseline and state-of-the-art models.\nParticularly the proposed method significantly outperforms the compared methods\nin cross-validation model generalization.", "AI": {"tldr": "DEFFA-Unet\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u7f16\u7801\u5668\u3001\u7279\u5f81\u8fc7\u6ee4\u878d\u5408\u6a21\u5757\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u91cd\u5efa\u878d\u5408\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u3001\u5206\u5e03\u4e0d\u5e73\u8861\u548c\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u5bf9\u8bca\u65ad\u773c\u90e8\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u4e0d\u8db3\u3001\u5206\u5e03\u4e0d\u5e73\u8861\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5206\u5272\u6027\u80fd\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "DEFFA-Unet\u5f15\u5165\u989d\u5916\u7684\u7f16\u7801\u5668\u5904\u7406\u57df\u4e0d\u53d8\u9884\u5904\u7406\u8f93\u5165\uff0c\u5f00\u53d1\u7279\u5f81\u8fc7\u6ee4\u878d\u5408\u6a21\u5757\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u91cd\u5efa\u878d\u5408\u6a21\u5757\uff0c\u5e76\u63d0\u51fa\u521b\u65b0\u7684\u6570\u636e\u589e\u5f3a\u548c\u5e73\u8861\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cDEFFA-Unet\u5728\u5206\u5272\u6027\u80fd\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "DEFFA-Unet\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.02381", "pdf": "https://arxiv.org/pdf/2506.02381", "abs": "https://arxiv.org/abs/2506.02381", "authors": ["Songlin Wei", "Gene Cheung", "Fei Chen", "Ivan Selesnick"], "title": "Unrolling Nonconvex Graph Total Variation for Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Conventional model-based image denoising optimizations employ convex\nregularization terms, such as total variation (TV) that convexifies the\n$\\ell_0$-norm to promote sparse signal representation. Instead, we propose a\nnew non-convex total variation term in a graph setting (NC-GTV), such that when\ncombined with an $\\ell_2$-norm fidelity term for denoising, leads to a convex\nobjective with no extraneous local minima. We define NC-GTV using a new graph\nvariant of the Huber function, interpretable as a Moreau envelope. The crux is\nthe selection of a parameter $a$ characterizing the graph Huber function that\nensures overall objective convexity; we efficiently compute $a$ via an\nadaptation of Gershgorin Circle Theorem (GCT). To minimize the convex\nobjective, we design a linear-time algorithm based on Alternating Direction\nMethod of Multipliers (ADMM) and unroll it into a lightweight feed-forward\nnetwork for data-driven parameter learning. Experiments show that our method\noutperforms unrolled GTV and other representative image denoising schemes,\nwhile employing far fewer network parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u51f8\u56fe\u603b\u53d8\u5206\uff08NC-GTV\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408Huber\u51fd\u6570\u548cGershgorin\u5706\u5b9a\u7406\u786e\u4fdd\u76ee\u6807\u51fd\u6570\u51f8\u6027\uff0c\u5e76\u901a\u8fc7ADMM\u7b97\u6cd5\u548c\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u53bb\u566a\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\u4f7f\u7528\u51f8\u6b63\u5219\u5316\u9879\uff08\u5982TV\uff09\uff0c\u4f46\u5176\u53ef\u80fd\u5f15\u5165\u5c40\u90e8\u6781\u5c0f\u503c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u975e\u51f8\u6b63\u5219\u5316\u9879\u63d0\u5347\u7a00\u758f\u4fe1\u53f7\u8868\u793a\uff0c\u540c\u65f6\u786e\u4fdd\u76ee\u6807\u51fd\u6570\u6574\u4f53\u51f8\u6027\u3002", "method": "\u63d0\u51faNC-GTV\uff0c\u57fa\u4e8e\u56fe\u7684Huber\u51fd\u6570\u5b9a\u4e49\u975e\u51f8\u6b63\u5219\u5316\u9879\uff0c\u5229\u7528Gershgorin\u5706\u5b9a\u7406\u8ba1\u7b97\u53c2\u6570\u4ee5\u786e\u4fdd\u51f8\u6027\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8eADMM\u7684\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff0c\u8fdb\u4e00\u6b65\u5c55\u5f00\u4e3a\u8f7b\u91cf\u7ea7\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u53bb\u566a\u4e2d\u4f18\u4e8e\u672a\u5c55\u5f00\u7684GTV\u548c\u5176\u4ed6\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u4e14\u7f51\u7edc\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "NC-GTV\u7ed3\u5408\u975e\u51f8\u6b63\u5219\u5316\u548c\u51f8\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u56fe\u50cf\u53bb\u566a\uff0c\u4e3a\u7a00\u758f\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.02467", "pdf": "https://arxiv.org/pdf/2506.02467", "abs": "https://arxiv.org/abs/2506.02467", "authors": ["Haowen Pang", "Weiyan Guo", "Chuyang Ye"], "title": "Multi-modal brain MRI synthesis based on SwinUNETR", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures", "summary": "Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in\nclinical diagnostics by providing complementary information across different\nimaging modalities. However, a common challenge in clinical practice is missing\nMRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing\nmodalities in brain MRI. SwinUNETR is a novel neural network architecture\ndesigned for medical image analysis, integrating the strengths of Swin\nTransformer and convolutional neural networks (CNNs). The Swin Transformer, a\nvariant of the Vision Transformer (ViT), incorporates hierarchical feature\nextraction and window-based self-attention mechanisms, enabling it to capture\nboth local and global contextual information effectively. By combining the Swin\nTransformer with CNNs, SwinUNETR merges global context awareness with detailed\nspatial resolution. This hybrid approach addresses the challenges posed by the\nvarying modality characteristics and complex brain structures, facilitating the\ngeneration of accurate and realistic synthetic images. We evaluate the\nperformance of SwinUNETR on brain MRI datasets and demonstrate its superior\ncapability in generating clinically valuable images. Our results show\nsignificant improvements in image quality, anatomical consistency, and\ndiagnostic value.", "AI": {"tldr": "SwinUNETR\u7528\u4e8e\u5408\u6210\u8111MRI\u4e2d\u7f3a\u5931\u7684\u6a21\u6001\uff0c\u7ed3\u5408Swin Transformer\u548cCNN\u7684\u4f18\u52bf\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u8111MRI\u6a21\u6001\u7f3a\u5931\u662f\u5e38\u89c1\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u5408\u6210\u7f3a\u5931\u6a21\u6001\u4ee5\u8f85\u52a9\u8bca\u65ad\u3002", "method": "\u91c7\u7528SwinUNETR\uff0c\u7ed3\u5408Swin Transformer\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u6355\u6349\u80fd\u529b\u548cCNN\u7684\u5c40\u90e8\u7ec6\u8282\u5904\u7406\u80fd\u529b\u3002", "result": "\u5728\u8111MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3001\u89e3\u5256\u4e00\u81f4\u6027\u548c\u8bca\u65ad\u4ef7\u503c\u3002", "conclusion": "SwinUNETR\u80fd\u6709\u6548\u5408\u6210\u7f3a\u5931\u6a21\u6001\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u9ad8\u8d28\u91cf\u56fe\u50cf\u652f\u6301\u3002"}}
{"id": "2506.02489", "pdf": "https://arxiv.org/pdf/2506.02489", "abs": "https://arxiv.org/abs/2506.02489", "authors": ["Tao Zhong", "Jonah Buchanan", "Christine Allen-Blanchette"], "title": "Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schr\u00f6dinger Bridges", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "19 pages, 4 figures", "summary": "We propose a new approach to vision-based dexterous grasp translation, which\naims to transfer grasp intent across robotic hands with differing morphologies.\nGiven a visual observation of a source hand grasping an object, our goal is to\nsynthesize a functionally equivalent grasp for a target hand without requiring\npaired demonstrations or hand-specific simulations. We frame this problem as a\nstochastic transport between grasp distributions using the Schr\\\"odinger Bridge\nformalism. Our method learns to map between source and target latent grasp\nspaces via score and flow matching, conditioned on visual observations. To\nguide this translation, we introduce physics-informed cost functions that\nencode alignment in base pose, contact maps, wrench space, and manipulability.\nExperiments across diverse hand-object pairs demonstrate our approach generates\nstable, physically grounded grasps with strong generalization. This work\nenables semantic grasp transfer for heterogeneous manipulators and bridges\nvision-based grasping with probabilistic generative modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u7075\u5de7\u6293\u53d6\u8f6c\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7Schr\u00f6dinger Bridge\u6846\u67b6\u5b9e\u73b0\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u4e4b\u95f4\u7684\u6293\u53d6\u610f\u56fe\u8f6c\u79fb\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u4e4b\u95f4\u6293\u53d6\u610f\u56fe\u8f6c\u79fb\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u914d\u5bf9\u6f14\u793a\u6216\u7279\u5b9a\u624b\u7684\u6a21\u62df\u3002", "method": "\u91c7\u7528Schr\u00f6dinger Bridge\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6570\u548c\u6d41\u5339\u914d\u5b66\u4e60\u6e90\u548c\u76ee\u6807\u6f5c\u5728\u6293\u53d6\u7a7a\u95f4\u7684\u6620\u5c04\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u6210\u672c\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u7a33\u5b9a\u4e14\u7269\u7406\u5408\u7406\u7684\u6293\u53d6\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f02\u6784\u673a\u68b0\u624b\u7684\u8bed\u4e49\u6293\u53d6\u8f6c\u79fb\uff0c\u5c06\u89c6\u89c9\u6293\u53d6\u4e0e\u6982\u7387\u751f\u6210\u5efa\u6a21\u7ed3\u5408\u3002"}}
{"id": "2506.02541", "pdf": "https://arxiv.org/pdf/2506.02541", "abs": "https://arxiv.org/abs/2506.02541", "authors": ["Minsung Kim", "Nakyeong Yang", "Kyomin Jung"], "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "Machine unlearning is used to mitigate the privacy risks of Large\nVision-Language Models (LVLMs) arising from training on large-scale web data.\nHowever, existing unlearning methods often fail to carefully select substitute\noutputs for forget targets, resulting in Unlearning Aftermaths-undesirable\nbehaviors such as degenerate, hallucinated, or excessively refused responses.\nWe highlight that, especially for generative LVLMs, it is crucial to consider\nthe quality and informativeness of post-unlearning responses rather than\nrelying solely on naive suppression. To address this, we introduce a new\nunlearning task for LVLMs that requires models to provide privacy-preserving\nyet informative and visually grounded responses. We also propose PUBG, a novel\nunlearning method that explicitly guides post-unlearning behavior toward a\ndesirable output distribution. Experiments show that, while existing methods\nsuffer from Unlearning Aftermaths despite successfully preventing privacy\nviolations, PUBG effectively mitigates these issues, generating visually\ngrounded and informative responses without privacy leakage for forgotten\ntargets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5PUBG\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u540e\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u65f6\uff0c\u5f80\u5f80\u5ffd\u89c6\u8f93\u51fa\u8d28\u91cf\uff0c\u5bfc\u81f4\u9000\u5316\u3001\u5e7b\u89c9\u6216\u8fc7\u5ea6\u62d2\u7edd\u7b49\u4e0d\u826f\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u65b0\u9057\u5fd8\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u751f\u6210\u65e2\u4fdd\u62a4\u9690\u79c1\u53c8\u4fe1\u606f\u4e30\u5bcc\u4e14\u89c6\u89c9\u57fa\u7840\u7684\u56de\u7b54\uff0c\u5e76\u8bbe\u8ba1PUBG\u65b9\u6cd5\uff0c\u5f15\u5bfc\u9057\u5fd8\u540e\u7684\u884c\u4e3a\u671d\u5411\u7406\u60f3\u8f93\u51fa\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPUBG\u6709\u6548\u907f\u514d\u4e86\u9690\u79c1\u6cc4\u9732\uff0c\u540c\u65f6\u751f\u6210\u89c6\u89c9\u57fa\u7840\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u56de\u7b54\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9057\u7559\u95ee\u9898\u3002", "conclusion": "PUBG\u4e3aLVLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u8f93\u51fa\u8d28\u91cf\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.02542", "pdf": "https://arxiv.org/pdf/2506.02542", "abs": "https://arxiv.org/abs/2506.02542", "authors": ["Niklas Kormann", "Masoud Ramuz", "Zeeshan Nisar", "Nadine S. Schaadt", "Hendrik Annuth", "Benjamin Doerr", "Friedrich Feuerhake", "Thomas Lampert", "Johannes F. Lutzeyer"], "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification", "categories": ["cs.LG", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Accepted for poster presentation at MIDL 2025", "summary": "Graph Neural Networks (GNNs) have recently been found to excel in\nhistopathology. However, an important histopathological task, where GNNs have\nnot been extensively explored, is the classification of glomeruli health as an\nimportant indicator in nephropathology. This task presents unique difficulties,\nparticularly for the graph construction, i.e., the identification of nodes,\nedges, and informative features. In this work, we propose a pipeline composed\nof different traditional and machine learning-based computer vision techniques\nto identify nodes, edges, and their corresponding features to form a\nheterogeneous graph. We then proceed to propose a novel heterogeneous GNN\narchitecture for glomeruli classification, called HIEGNet, that integrates both\nglomeruli and their surrounding immune cells. Hence, HIEGNet is able to\nconsider the immune environment of each glomerulus in its classification. Our\nHIEGNet was trained and tested on a dataset of Whole Slide Images from kidney\ntransplant patients. Experimental results demonstrate that HIEGNet outperforms\nseveral baseline models and generalises best between patients among all\nbaseline models. Our implementation is publicly available at\nhttps://github.com/nklsKrmnn/HIEGNet.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u80be\u5c0f\u7403\u5206\u7c7b\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edcHIEGNet\uff0c\u7ed3\u5408\u4f20\u7edf\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6784\u5efa\u56fe\u7ed3\u6784\uff0c\u5e76\u5728\u5168\u5e7b\u706f\u7247\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u80be\u5c0f\u7403\u5065\u5eb7\u5206\u7c7b\u4efb\u52a1\u4e2d\u56fe\u6784\u5efa\u7684\u72ec\u7279\u56f0\u96be\uff0c\u5c24\u5176\u662f\u8282\u70b9\u3001\u8fb9\u548c\u4fe1\u606f\u7279\u5f81\u7684\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u6d41\u7a0b\u6765\u6784\u5efa\u5f02\u6784\u56fe\uff0c\u5e76\u8bbe\u8ba1HIEGNet\u67b6\u6784\u4ee5\u6574\u5408\u80be\u5c0f\u7403\u53ca\u5176\u5468\u56f4\u514d\u75ab\u7ec6\u80de\u7684\u4fe1\u606f\u3002", "result": "HIEGNet\u5728\u80be\u79fb\u690d\u60a3\u8005\u7684\u5168\u5e7b\u706f\u7247\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728\u4e0d\u540c\u60a3\u8005\u95f4\u6cdb\u5316\u80fd\u529b\u6700\u4f73\u3002", "conclusion": "HIEGNet\u901a\u8fc7\u6574\u5408\u514d\u75ab\u73af\u5883\u4fe1\u606f\uff0c\u4e3a\u80be\u5c0f\u7403\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u5b9e\u73b0\u4ee3\u7801\u3002"}}
{"id": "2506.02554", "pdf": "https://arxiv.org/pdf/2506.02554", "abs": "https://arxiv.org/abs/2506.02554", "authors": ["Timo Osterburg", "Franz Albers", "Christopher Diehl", "Rajesh Pushparaj", "Torsten Bertram"], "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "The fusion of sensor data is essential for a robust perception of the\nenvironment in autonomous driving. Learning-based fusion approaches mainly use\nfeature-level fusion to achieve high performance, but their complexity and\nhardware requirements limit their applicability in near-production vehicles.\nHigh-level fusion methods offer robustness with lower computational\nrequirements. Traditional methods, such as the Kalman filter, dominate this\narea. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel\ntransformer-based high-level object fusion method called HiLO. Experimental\nresults demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$\nscore and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale\nreal-world dataset demonstrates the effectiveness of the proposed approaches.\nTheir generalizability is further validated by cross-domain evaluation between\nurban and highway scenarios. Code, data, and models are available at\nhttps://github.com/rst-tu-dortmund/HiLO .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9ad8\u5c42\u5bf9\u8c61\u878d\u5408\u65b9\u6cd5HiLO\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfAdapted Kalman Filter (AKF)\uff0c\u5728F1\u5206\u6570\u548c\u5e73\u5747IoU\u4e0a\u5206\u522b\u63d0\u5347\u4e8625.9\u548c6.1\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u5bf9\u73af\u5883\u611f\u77e5\u81f3\u5173\u91cd\u8981\u3002\u57fa\u4e8e\u5b66\u4e60\u7684\u878d\u5408\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u9ad8\uff0c\u4f46\u590d\u6742\u5ea6\u548c\u786c\u4ef6\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u91cf\u4ea7\u8f66\u4e2d\u7684\u5e94\u7528\u3002\u9ad8\u5c42\u878d\u5408\u65b9\u6cd5\u8ba1\u7b97\u9700\u6c42\u4f4e\u4e14\u9c81\u68d2\u6027\u5f3a\u3002", "method": "\u8bba\u6587\u6539\u8fdbAKF\u5e76\u63d0\u51faHiLO\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9ad8\u5c42\u5bf9\u8c61\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aF1\u5206\u6570\u63d0\u534725.9\u4e2a\u767e\u5206\u70b9\uff0c\u5e73\u5747IoU\u63d0\u53476.1\u4e2a\u767e\u5206\u70b9\u3002\u65b0\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8de8\u57df\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HiLO\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u9ad8\u5c42\u6570\u636e\u878d\u5408\u3002"}}
{"id": "2506.02574", "pdf": "https://arxiv.org/pdf/2506.02574", "abs": "https://arxiv.org/abs/2506.02574", "authors": ["Shuai Yuan", "Shuang Chen", "Tianwu Lin", "Jie Wang", "Peng Gong"], "title": "Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Accurate remote sensing geographic mapping depends heavily on representative\nand timely sample data. However, rapid changes in land surface dynamics\nnecessitate frequent updates, quickly rendering previously collected samples\nobsolete and imposing significant labor demands for continuous manual updates.\nIn this study, we aim to address this problem by dynamic sample generation\nusing existing single-date static labeled samples. We introduce TasGen, a\ntwo-stage automated framework to automatically generate dynamic samples,\ndesigned to simultaneously model spectral and temporal dependencies in\ntime-series remote sensing imagery via temporal-spectral embedding, capturing\nland surface changes without additional manual annotations.", "AI": {"tldr": "TasGen\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u81ea\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4-\u5149\u8c31\u5d4c\u5165\u4ece\u9759\u6001\u6837\u672c\u751f\u6210\u52a8\u6001\u6837\u672c\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u9065\u611f\u5730\u7406\u5236\u56fe\u4f9d\u8d56\u4ee3\u8868\u6027\u6837\u672c\u6570\u636e\uff0c\u4f46\u5730\u8868\u52a8\u6001\u53d8\u5316\u5feb\uff0c\u6837\u672c\u6613\u8fc7\u65f6\uff0c\u9700\u9891\u7e41\u4eba\u5de5\u66f4\u65b0\u3002", "method": "\u63d0\u51faTasGen\u6846\u67b6\uff0c\u5229\u7528\u9759\u6001\u6837\u672c\u751f\u6210\u52a8\u6001\u6837\u672c\uff0c\u901a\u8fc7\u65f6\u95f4-\u5149\u8c31\u5d4c\u5165\u5efa\u6a21\u5149\u8c31\u548c\u65f6\u95f4\u4f9d\u8d56\u3002", "result": "\u65e0\u9700\u989d\u5916\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u6355\u6349\u5730\u8868\u53d8\u5316\u3002", "conclusion": "TasGen\u80fd\u6709\u6548\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\uff0c\u63d0\u5347\u9065\u611f\u5236\u56fe\u6548\u7387\u3002"}}
{"id": "2506.02585", "pdf": "https://arxiv.org/pdf/2506.02585", "abs": "https://arxiv.org/abs/2506.02585", "authors": ["Chunwei Tian", "Mingjian Song", "Xiaopeng Fan", "Xiangtao Zheng", "Bob Zhang", "David Zhang"], "title": "A Tree-guided CNN for image super-resolution", "categories": ["eess.IV", "cs.CV"], "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Consumer Electronics. 10 pages, 6 figures. Its code can be obtained at\n  https://github.com/hellloxiaotian/TSRNet", "summary": "Deep convolutional neural networks can extract more accurate structural\ninformation via deep architectures to obtain good performance in image\nsuper-resolution. However, it is not easy to find effect of important layers in\na single network architecture to decrease performance of super-resolution. In\nthis paper, we design a tree-guided CNN for image super-resolution (TSRNet). It\nuses a tree architecture to guide a deep network to enhance effect of key nodes\nto amplify the relation of hierarchical information for improving the ability\nof recovering images. To prevent insufficiency of the obtained structural\ninformation, cosine transform techniques in the TSRNet are used to extract\ncross-domain information to improve the performance of image super-resolution.\nAdaptive Nesterov momentum optimizer (Adan) is applied to optimize parameters\nto boost effectiveness of training a super-resolution model. Extended\nexperiments can verify superiority of the proposed TSRNet for restoring\nhigh-quality images. Its code can be obtained at\nhttps://github.com/hellloxiaotian/TSRNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6811\u5f15\u5bfc\u7684CNN\uff08TSRNet\uff09\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u6811\u67b6\u6784\u589e\u5f3a\u5173\u952e\u8282\u70b9\u6548\u679c\uff0c\u5e76\u7ed3\u5408\u4f59\u5f26\u53d8\u6362\u6280\u672f\u548cAdan\u4f18\u5316\u5668\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u96be\u4ee5\u6709\u6548\u5229\u7528\u5173\u952e\u5c42\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u6811\u5f15\u5bfc\u7684CNN\uff08TSRNet\uff09\uff0c\u7ed3\u5408\u4f59\u5f26\u53d8\u6362\u63d0\u53d6\u8de8\u57df\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528Adan\u4f18\u5316\u5668\u4f18\u5316\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TSRNet\u5728\u6062\u590d\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "TSRNet\u901a\u8fc7\u6811\u67b6\u6784\u548c\u8de8\u57df\u4fe1\u606f\u63d0\u53d6\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002"}}
{"id": "2506.02618", "pdf": "https://arxiv.org/pdf/2506.02618", "abs": "https://arxiv.org/abs/2506.02618", "authors": ["Jialiang Zhang", "Haoran Geng", "Yang You", "Congyue Deng", "Pieter Abbeel", "Jitendra Malik", "Leonidas Guibas"], "title": "Rodrigues Network for Learning Robot Actions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding and predicting articulated actions is important in robot\nlearning. However, common architectures such as MLPs and Transformers lack\ninductive biases that reflect the underlying kinematic structure of articulated\nsystems. To this end, we propose the Neural Rodrigues Operator, a learnable\ngeneralization of the classical forward kinematics operation, designed to\ninject kinematics-aware inductive bias into neural computation. Building on\nthis operator, we design the Rodrigues Network (RodriNet), a novel neural\narchitecture specialized for processing actions. We evaluate the expressivity\nof our network on two synthetic tasks on kinematic and motion prediction,\nshowing significant improvements compared to standard backbones. We further\ndemonstrate its effectiveness in two realistic applications: (i) imitation\nlearning on robotic benchmarks with the Diffusion Policy, and (ii) single-image\n3D hand reconstruction. Our results suggest that integrating structured\nkinematic priors into the network architecture improves action learning in\nvarious domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeural Rodrigues Operator\u7684\u5b66\u4e60\u6a21\u5757\uff0c\u7ed3\u5408Rodrigues Network\uff08RodriNet\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u4eba\u52a8\u4f5c\u5b66\u4e60\u4e2d\u7684\u8fd0\u52a8\u5b66\u9884\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u5408\u6210\u4efb\u52a1\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u67b6\u6784\uff08\u5982MLPs\u548cTransformers\uff09\u7f3a\u4e4f\u5bf9\u5173\u8282\u7cfb\u7edf\u8fd0\u52a8\u5b66\u7ed3\u6784\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u9650\u5236\u4e86\u52a8\u4f5c\u5b66\u4e60\u548c\u9884\u6d4b\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faNeural Rodrigues Operator\u4f5c\u4e3a\u7ecf\u5178\u524d\u5411\u8fd0\u52a8\u5b66\u64cd\u4f5c\u7684\u53ef\u5b66\u4e60\u6269\u5c55\uff0c\u5e76\u8bbe\u8ba1RodriNet\u7f51\u7edc\uff0c\u4e13\u95e8\u5904\u7406\u52a8\u4f5c\u6570\u636e\u3002", "result": "\u5728\u8fd0\u52a8\u5b66\u9884\u6d4b\u548c\u6a21\u4eff\u5b66\u4e60\u7b49\u4efb\u52a1\u4e2d\uff0cRodriNet\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u8fd0\u52a8\u5b66\u5148\u9a8c\u878d\u5165\u7f51\u7edc\u67b6\u6784\u53ef\u6709\u6548\u63d0\u5347\u52a8\u4f5c\u5b66\u4e60\u7684\u6548\u679c\u3002"}}
{"id": "2506.02623", "pdf": "https://arxiv.org/pdf/2506.02623", "abs": "https://arxiv.org/abs/2506.02623", "authors": ["Yuyang Zhou", "Ferrante Neri", "Yew-Soon Ong", "Ruibin Bai"], "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Genetic and Evolutionary Computation Conference (GECCO' 25)", "summary": "Modern neural architecture search (NAS) is inherently multi-objective,\nbalancing trade-offs such as accuracy, parameter count, and computational cost.\nThis complexity makes NAS computationally expensive and nearly impossible to\nsolve without efficient approximations. To address this, we propose a novel\nsurrogate modelling approach that leverages an ensemble of Siamese network\nblocks to predict dominance relationships between candidate architectures.\nLightweight and easy to train, the surrogate achieves 92% accuracy and replaces\nthe crowding distance calculation in the survivor selection strategy with a\nheuristic rule based on model size. Integrated into a framework termed SiamNAS,\nthis design eliminates costly evaluations during the search process.\nExperiments on NAS-Bench-201 demonstrate the framework's ability to identify\nPareto-optimal solutions with significantly reduced computational costs. The\nproposed SiamNAS identified a final non-dominated set containing the best\narchitecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in\nterms of test error rate, within 0.01 GPU days. This proof-of-concept study\nhighlights the potential of the proposed Siamese network surrogate model to\ngeneralise to multi-tasking optimisation, enabling simultaneous optimisation\nacross tasks. Additionally, it offers opportunities to extend the approach for\ngenerating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal\nsolutions for heterogeneous task settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b6a\u751f\u7f51\u7edc\u5757\u7684\u4ee3\u7406\u6a21\u578b\u65b9\u6cd5\uff08SiamNAS\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u7684\u652f\u914d\u5173\u7cfb\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u4ee3NAS\u662f\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u76f4\u63a5\u6c42\u89e3\uff0c\u9700\u8981\u9ad8\u6548\u8fd1\u4f3c\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5b6a\u751f\u7f51\u7edc\u5757\u6784\u5efa\u4ee3\u7406\u6a21\u578b\uff0c\u9884\u6d4b\u67b6\u6784\u95f4\u7684\u652f\u914d\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6a21\u578b\u5927\u5c0f\u8bbe\u8ba1\u542f\u53d1\u5f0f\u89c4\u5219\u66ff\u4ee3\u62e5\u6324\u8ddd\u79bb\u8ba1\u7b97\u3002", "result": "SiamNAS\u5728NAS-Bench-201\u4e0a\u5b9e\u73b0\u4e8692%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u75280.01 GPU\u5929\u5c31\u627e\u5230Pareto\u6700\u4f18\u89e3\uff0c\u5305\u62ecCIFAR-10\u7684\u6700\u4f73\u67b6\u6784\u548cImageNet\u7684\u7b2c\u4e8c\u4f73\u67b6\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5b6a\u751f\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u53ef\u6269\u5c55\u4e3a\u751f\u6210\u591a\u6837\u5316\u7684Pareto\u6700\u4f18\u89e3\u96c6\u3002"}}
{"id": "2506.02950", "pdf": "https://arxiv.org/pdf/2506.02950", "abs": "https://arxiv.org/abs/2506.02950", "authors": ["Stepan I. Manukhov", "Alexander Kolesov", "Vladimir V. Palyulin", "Alexander Korotin"], "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Electrostatic field matching (EFM) has recently appeared as a novel\nphysics-inspired paradigm for data generation and transfer using the idea of an\nelectric capacitor. However, it requires modeling electrostatic fields using\nneural networks, which is non-trivial because of the necessity to take into\naccount the complex field outside the capacitor plates. In this paper, we\npropose Interaction Field Matching (IFM), a generalization of EFM which allows\nusing general interaction fields beyond the electrostatic one. Furthermore,\ninspired by strong interactions between quarks and antiquarks in physics, we\ndesign a particular interaction field realization which solves the problems\nwhich arise when modeling electrostatic fields in EFM. We show the performance\non a series of toy and image data transfer problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4ea4\u4e92\u573a\u5339\u914d\uff08IFM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u9759\u7535\u5339\u914d\uff08EFM\uff09\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u66f4\u4e00\u822c\u7684\u4ea4\u4e92\u573a\u89e3\u51b3\u4e86EFM\u4e2d\u5efa\u6a21\u9759\u7535\u573a\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "EFM\u867d\u7136\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u751f\u6210\u548c\u4f20\u8f93\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u5efa\u6a21\u590d\u6742\u7684\u9759\u7535\u573a\uff0c\u5b58\u5728\u5b9e\u73b0\u4e0a\u7684\u56f0\u96be\u3002", "method": "\u63d0\u51faIFM\u65b9\u6cd5\uff0c\u5229\u7528\u66f4\u4e00\u822c\u7684\u4ea4\u4e92\u573a\u66ff\u4ee3\u9759\u7535\u573a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u76f8\u4e92\u4f5c\u7528\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u5728\u73a9\u5177\u548c\u56fe\u50cf\u6570\u636e\u4f20\u8f93\u95ee\u9898\u4e0a\u5c55\u793a\u4e86IFM\u7684\u6027\u80fd\u3002", "conclusion": "IFM\u4e0d\u4ec5\u89e3\u51b3\u4e86EFM\u7684\u95ee\u9898\uff0c\u8fd8\u6269\u5c55\u4e86\u5176\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u6570\u636e\u751f\u6210\u548c\u4f20\u8f93\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.03095", "pdf": "https://arxiv.org/pdf/2506.03095", "abs": "https://arxiv.org/abs/2506.03095", "authors": ["Man Luo", "David Cobbley", "Xin Su", "Shachar Rosenman", "Vasudev Lal", "Shao-Yen Tseng", "Phillip Howard"], "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUA\uff09\uff0c\u901a\u8fc7\u672c\u5730\u8fd0\u884c\u548c\u81ea\u52a8\u6570\u636e\u7b5b\u9009\u6846\u67b6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CUA\u4f9d\u8d56\u4e91\u7aef\u8ba1\u7b97\uff0c\u5b58\u5728\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u672c\u5730\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "method": "\u5f15\u5165LLM-as-Judge\u6846\u67b6\u81ea\u52a8\u8bc4\u4f30\u548c\u7b5b\u9009\u5408\u6210\u4ea4\u4e92\u8f68\u8ff9\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "result": "\u5728OS-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u672c\u5730\u5fae\u8c03\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9690\u79c1\u3001\u9ad8\u6548\u4e14\u901a\u7528\u7684GUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2506.03134", "pdf": "https://arxiv.org/pdf/2506.03134", "abs": "https://arxiv.org/abs/2506.03134", "authors": ["Weiqing Xiao", "Hao Huang", "Chonghao Zhong", "Yujie Lin", "Nan Wang", "Xiaoxue Chen", "Zhaoxi Chen", "Saining Zhang", "Shuocheng Yang", "Pierre Merriaux", "Lei Lei", "Hao Zhao"], "title": "Simulate Any Radar: Attribute-Controllable Radar Simulation via Waveform Parameter Embedding", "categories": ["eess.SP", "cs.CV"], "comment": "Code: https://github.com/zhuxing0/SA-Radar Project page:\n  https://zhuxing0.github.io/projects/SA-Radar", "summary": "We present SA-Radar (Simulate Any Radar), a radar simulation approach that\nenables controllable and efficient generation of radar cubes conditioned on\ncustomizable radar attributes. Unlike prior generative or physics-based\nsimulators, SA-Radar integrates both paradigms through a waveform-parameterized\nattribute embedding. We design ICFAR-Net, a 3D U-Net conditioned on radar\nattributes encoded via waveform parameters, which captures signal variations\ninduced by different radar configurations. This formulation bypasses the need\nfor detailed radar hardware specifications and allows efficient simulation of\nrange-azimuth-Doppler (RAD) tensors across diverse sensor settings. We further\nconstruct a mixed real-simulated dataset with attribute annotations to robustly\ntrain the network. Extensive evaluations on multiple downstream tasks-including\n2D/3D object detection and radar semantic segmentation-demonstrate that\nSA-Radar's simulated data is both realistic and effective, consistently\nimproving model performance when used standalone or in combination with real\ndata. Our framework also supports simulation in novel sensor viewpoints and\nedited scenes, showcasing its potential as a general-purpose radar data engine\nfor autonomous driving applications. Code and additional materials are\navailable at https://zhuxing0.github.io/projects/SA-Radar.", "AI": {"tldr": "SA-Radar\u662f\u4e00\u79cd\u96f7\u8fbe\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce2\u5f62\u53c2\u6570\u5316\u5c5e\u6027\u5d4c\u5165\uff0c\u7ed3\u5408\u751f\u6210\u5f0f\u548c\u7269\u7406\u6a21\u62df\uff0c\u9ad8\u6548\u751f\u6210\u53ef\u5b9a\u5236\u7684\u96f7\u8fbe\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe\u6a21\u62df\u65b9\u6cd5\u8981\u4e48\u57fa\u4e8e\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u8981\u4e48\u57fa\u4e8e\u7269\u7406\u6a21\u62df\uff0c\u7f3a\u4e4f\u4e24\u8005\u7684\u7ed3\u5408\uff0c\u4e14\u9700\u8981\u8be6\u7ec6\u7684\u786c\u4ef6\u89c4\u683c\u3002SA-Radar\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86ICFAR-Net\uff083D U-Net\uff09\uff0c\u901a\u8fc7\u6ce2\u5f62\u53c2\u6570\u7f16\u7801\u96f7\u8fbe\u5c5e\u6027\uff0c\u751f\u6210\u8303\u56f4-\u65b9\u4f4d-\u591a\u666e\u52d2\uff08RAD\uff09\u5f20\u91cf\uff0c\u65e0\u9700\u8be6\u7ec6\u786c\u4ef6\u89c4\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSA-Radar\u751f\u6210\u7684\u6570\u636e\u57282D/3D\u76ee\u6807\u68c0\u6d4b\u548c\u96f7\u8fbe\u8bed\u4e49\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u771f\u5b9e\u4e14\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "SA-Radar\u662f\u4e00\u79cd\u901a\u7528\u96f7\u8fbe\u6570\u636e\u5f15\u64ce\uff0c\u652f\u6301\u65b0\u4f20\u611f\u5668\u89c6\u89d2\u548c\u573a\u666f\u7f16\u8f91\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u3002"}}
