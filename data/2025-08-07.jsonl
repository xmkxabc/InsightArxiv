{"id": "2508.03858", "title": "MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems", "authors": ["Charles L. Wang", "Trisha Singhal", "Ameya Kelkar", "Jason Tuo"], "categories": ["cs.AI", "cs.ET", "cs.MA"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03858v1", "summary": "Agentic AI systems capable of reasoning, planning, and executing actions\npresent fundamentally distinct governance challenges compared to traditional AI\nmodels. Unlike conventional AI, these systems exhibit emergent and unexpected\nbehaviors during runtime, introducing novel agent-related risks that cannot be\nfully anticipated through pre-deployment governance alone. To address this\ncritical gap, we introduce MI9, the first fully integrated runtime governance\nframework designed specifically for safety and alignment of agentic AI systems.\nMI9 introduces real-time controls through six integrated components:\nagency-risk index, agent-semantic telemetry capture, continuous authorization\nmonitoring, Finite-State-Machine (FSM)-based conformance engines,\ngoal-conditioned drift detection, and graduated containment strategies.\nOperating transparently across heterogeneous agent architectures, MI9 enables\nthe systematic, safe, and responsible deployment of agentic systems in\nproduction environments where conventional governance approaches fall short,\nproviding the foundational infrastructure for safe agentic AI deployment at\nscale. Detailed analysis through a diverse set of scenarios demonstrates MI9's\nsystematic coverage of governance challenges that existing approaches fail to\naddress, establishing the technical foundation for comprehensive agentic AI\noversight.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03858v1", "cate": "cs.AI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03864", "title": "Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety", "authors": ["Zhenyu Pan", "Yiting Zhang", "Yutong Zhang", "Jianshu Zhang", "Haozheng Luo", "Yuwei Han", "Dennis Wu", "Hong-Yu Chen", "Philip S. Yu", "Manling Li", "Han Liu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03864v1", "summary": "Multi-agent systems (MAS) built on multimodal large language models exhibit\nstrong collaboration and performance. However, their growing openness and\ninteraction complexity pose serious risks, notably jailbreak and adversarial\nattacks. Existing defenses typically rely on external guard modules, such as\ndedicated safety agents, to handle unsafe behaviors. Unfortunately, this\nparadigm faces two challenges: (1) standalone agents offer limited protection,\nand (2) their independence leads to single-point failure-if compromised,\nsystem-wide safety collapses. Naively increasing the number of guard agents\nfurther raises cost and complexity. To address these challenges, we propose\nEvo-MARL, a novel multi-agent reinforcement learning (MARL) framework that\nenables all task agents to jointly acquire defensive capabilities. Rather than\nrelying on external safety modules, Evo-MARL trains each agent to\nsimultaneously perform its primary function and resist adversarial threats,\nensuring robustness without increasing system overhead or single-node failure.\nFurthermore, Evo-MARL integrates evolutionary search with parameter-sharing\nreinforcement learning to co-evolve attackers and defenders. This adversarial\ntraining paradigm internalizes safety mechanisms and continually enhances MAS\nperformance under co-evolving threats. Experiments show that Evo-MARL reduces\nattack success rates by up to 22% while boosting accuracy by up to 5% on\nreasoning tasks-demonstrating that safety and utility can be jointly improved.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03864v1", "cate": "cs.AI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03929", "title": "MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework", "authors": ["Nguyen Viet Tuan Kiet", "Dao Van Tung", "Tran Cong Dao", "Huynh Thi Thanh Binh"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03929v1", "summary": "Designing effective algorithmic components remains a fundamental obstacle in\ntackling NP-hard combinatorial optimization problems (COPs), where solvers\noften rely on carefully hand-crafted strategies. Despite recent advances in\nusing large language models (LLMs) to synthesize high-quality components, most\napproaches restrict the search to a single element - commonly a heuristic\nscoring function - thus missing broader opportunities for innovation. In this\npaper, we introduce a broader formulation of solver design as a multi-strategy\noptimization problem, which seeks to jointly improve a set of interdependent\ncomponents under a unified objective. To address this, we propose\nMulti-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a\nnovel framework based on Monte Carlo Tree Search that facilitates turn-based\noptimization between two LLM agents. At each turn, an agent improves one\ncomponent by leveraging the history of both its own and its opponent's prior\nupdates, promoting both competitive pressure and emergent cooperation. This\nstructured interaction broadens the search landscape and encourages the\ndiscovery of diverse, high-performing solutions. Experiments across multiple\nCOP domains show that MOTIF consistently outperforms state-of-the-art methods,\nhighlighting the promise of turn-based, multi-agent prompting for fully\nautomated solver design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03929v1", "cate": "cs.AI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03963", "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?", "authors": ["Zewen Liu", "Juntong Ni", "Xianfeng Tang", "Max S.Y. Lau", "Wei Jin"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03963v1", "summary": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03963v1", "cate": "cs.AI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03986", "title": "The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?", "authors": ["Yuan Xun", "Xiaojun Jia", "Xinwei Liu", "Hua Zhang"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03986v1", "summary": "We observe that MLRMs oriented toward human-centric service are highly\nsusceptible to user emotional cues during the deep-thinking stage, often\noverriding safety protocols or built-in safety checks under high emotional\nintensity. Inspired by this key insight, we propose EmoAgent, an autonomous\nadversarial emotion-agent framework that orchestrates exaggerated affective\nprompts to hijack reasoning pathways. Even when visual risks are correctly\nidentified, models can still produce harmful completions through emotional\nmisalignment. We further identify persistent high-risk failure modes in\ntransparent deep-thinking scenarios, such as MLRMs generating harmful reasoning\nmasked behind seemingly safe responses. These failures expose misalignments\nbetween internal inference and surface-level behavior, eluding existing\ncontent-based safeguards. To quantify these risks, we introduce three metrics:\n(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign\noutputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite\nvisual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for\nevaluating refusal unstability under prompt variants. Extensive experiments on\nadvanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper\nemotional cognitive misalignments in model safety behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03986v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03991", "title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents", "authors": ["Chongyu Bao", "Ruimin Dai", "Yangbo Shen", "Runyang Jian", "Jinghan Zhang", "Xiaolan Liu", "Kunpeng Liu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03991v1", "summary": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are\ndesigned to enhance human capabilities and perform tasks on behalf of users.\nThe emergence of LLM agents brings new opportunities for the development of\nIPAs. While responsive capabilities have been widely studied, proactive\nbehaviors remain underexplored. Designing an IPA that is proactive,\nprivacy-preserving, and capable of self-evolution remains a significant\nchallenge. Designing such IPAs relies on the cognitive architecture of LLM\nagents. This work proposes Cognition Forest, a semantic structure designed to\nalign cognitive modeling with system-level design. We unify cognitive\narchitecture and system design into a self-reinforcing loop instead of treating\nthem separately. Based on this principle, we present Galaxy, a framework that\nsupports multidimensional interactions and personalized capability generation.\nTwo cooperative agents are implemented based on Galaxy: KoRa, a\ncognition-enhanced generative agent that supports both responsive and proactive\nskills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's\nself-evolution and privacy preservation. Experimental results show that Galaxy\noutperforms multiple state-of-the-art benchmarks. Ablation studies and\nreal-world interaction cases validate the effectiveness of Galaxy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03991v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04025", "title": "Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement", "authors": ["Chao Hao", "Shuai Wang", "Kaiwen Zhou"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04025v1", "summary": "Graphical user interface (GUI) agents have shown promise in automating mobile\ntasks but still struggle with input redundancy and decision ambiguity. In this\npaper, we present \\textbf{RecAgent}, an uncertainty-aware agent that addresses\nthese issues through adaptive perception. We distinguish two types of\nuncertainty in GUI navigation: (1) perceptual uncertainty, caused by input\nredundancy and noise from comprehensive screen information, and (2) decision\nuncertainty, arising from ambiguous tasks and complex reasoning. To reduce\nperceptual uncertainty, RecAgent employs a component recommendation mechanism\nthat identifies and focuses on the most relevant UI elements. For decision\nuncertainty, it uses an interactive module to request user feedback in\nambiguous situations, enabling intent-aware decisions. These components are\nintegrated into a unified framework that proactively reduces input complexity\nand reacts to high-uncertainty cases via human-in-the-loop refinement.\nAdditionally, we propose a dataset called \\textbf{ComplexAction} to evaluate\nthe success rate of GUI agents in executing specified single-step actions\nwithin complex scenarios. Extensive experiments validate the effectiveness of\nour approach. The dataset and code will be available at\nhttps://github.com/Fanye12/RecAgent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04025v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04037", "title": "SEA: Self-Evolution Agent with Step-wise Reward for Computer Use", "authors": ["Liang Tang", "Shuxian Li", "Yuhao Cheng", "Yukang Huo", "Zhepeng Wang", "Yiqiang Yan", "Kaer Huang", "Yanzhe Jing", "Tiaonan Duan"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04037v1", "summary": "Computer use agent is an emerging area in artificial intelligence that aims\nto operate the computers to achieve the user's tasks, which attracts a lot of\nattention from both industry and academia. However, the present agents'\nperformance is far from being used. In this paper, we propose the\nSelf-Evolution Agent (SEA) for computer use, and to develop this agent, we\npropose creative methods in data generation, reinforcement learning, and model\nenhancement. Specifically, we first propose an automatic pipeline to generate\nthe verifiable trajectory for training. And then, we propose efficient\nstep-wise reinforcement learning to alleviate the significant computational\nrequirements for long-horizon training. In the end, we propose the enhancement\nmethod to merge the grounding and planning ability into one model without any\nextra training. Accordingly, based on our proposed innovation of data\ngeneration, training strategy, and enhancement, we get the Selfevolution Agent\n(SEA) for computer use with only 7B parameters, which outperforms models with\nthe same number of parameters and has comparable performance to larger ones. We\nwill make the models' weight and related codes open-source in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04037v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04070", "title": "Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals", "authors": ["Ronja Mehlan", "Claudia Hess", "Quintus Stierstorfer", "Kristina Schaaff"], "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04070v1", "summary": "As artificial intelligence becomes increasingly integrated into digital\nlearning environments, the personalization of learning content to reflect\nlearners' individual career goals offers promising potential to enhance\nengagement and long-term motivation. In our study, we investigate how career\ngoal-based content adaptation in learning systems based on generative AI\n(GenAI) influences learner engagement, satisfaction, and study efficiency. The\nmixed-methods experiment involved more than 4,000 learners, with one group\nreceiving learning scenarios tailored to their career goals and a control\ngroup. Quantitative results show increased session duration, higher\nsatisfaction ratings, and a modest reduction in study duration compared to\nstandard content. Qualitative analysis highlights that learners found the\npersonalized material motivating and practical, enabling deep cognitive\nengagement and strong identification with the content. These findings\nunderscore the value of aligning educational content with learners' career\ngoals and suggest that scalable AI personalization can bridge academic\nknowledge and workplace applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04070v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04072", "title": "KG-Augmented Executable CoT for Mathematical Coding", "authors": ["Xingyu Chen", "Junxiu An", "Jun Guo", "Li Wang", "Jingcai Guo"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04072v1", "summary": "In recent years, large language models (LLMs) have excelled in natural\nlanguage processing tasks but face significant challenges in complex reasoning\ntasks such as mathematical reasoning and code generation. To address these\nlimitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a\nnovel framework that enhances code generation through knowledge graphs and\nimproves mathematical reasoning via executable code. KGA-ECoT decomposes\nproblems into a Structured Task Graph, leverages efficient GraphRAG for precise\nknowledge retrieval from mathematical libraries, and generates verifiable code\nto ensure computational accuracy. Evaluations on multiple mathematical\nreasoning benchmarks demonstrate that KGA-ECoT significantly outperforms\nexisting prompting methods, achieving absolute accuracy improvements ranging\nfrom several to over ten percentage points. Further analysis confirms the\ncritical roles of GraphRAG in enhancing code quality and external code\nexecution in ensuring precision. These findings collectively establish KGA-ECoT\nas a robust and highly generalizable framework for complex mathematical\nreasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04072v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04080", "title": "GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement", "authors": ["Jinfan Tang", "Kunming Wu", "Ruifeng Gongxie", "Yuya He", "Yuankai Wu"], "categories": ["cs.AI", "stat.OT"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04080v1", "summary": "Recent studies have extended the application of large language models (LLMs)\nto geographic problems, revealing surprising geospatial competence even without\nexplicit spatial supervision. However, LLMs still face challenges in spatial\nconsistency, multi-hop reasoning, and geographic bias. To address these issues,\nwe propose GeoSR, a self-refining agentic reasoning framework that embeds core\ngeographic principles -- most notably Tobler's First Law of Geography -- into\nan iterative prediction loop. In GeoSR, the reasoning process is decomposed\ninto three collaborating agents: (1) a variable-selection agent that selects\nrelevant covariates from the same location; (2) a point-selection agent that\nchooses reference predictions at nearby locations generated by the LLM in\nprevious rounds; and (3) a refine agent that coordinates the iterative\nrefinement process by evaluating prediction quality and triggering further\nrounds when necessary. This agentic loop progressively improves prediction\nquality by leveraging both spatial dependencies and inter-variable\nrelationships. We validate GeoSR on tasks ranging from physical-world property\nestimation to socioeconomic prediction. Experimental results show consistent\nimprovements over standard prompting strategies, demonstrating that\nincorporating geostatistical priors and spatially structured reasoning into\nLLMs leads to more accurate and equitable geospatial predictions. The code of\nGeoSR is available at https://github.com/JinfanTang/GeoSR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04080v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04105", "title": "Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement", "authors": ["Karrtik Iyer", "Manikandan Ravikiran", "Prasanna Pendse", "Shayan Mohanty"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04105v1", "summary": "Automated grading systems can efficiently score short-answer responses, yet\nthey often fail to indicate when a grading decision is uncertain or potentially\ncontentious. We introduce semantic entropy, a measure of variability across\nmultiple GPT-4-generated explanations for the same student response, as a proxy\nfor human grader disagreement. By clustering rationales via entailment-based\nsimilarity and computing entropy over these clusters, we quantify the diversity\nof justifications without relying on final output scores. We address three\nresearch questions: (1) Does semantic entropy align with human grader\ndisagreement? (2) Does it generalize across academic subjects? (3) Is it\nsensitive to structural task features such as source dependency? Experiments on\nthe ASAP-SAS dataset show that semantic entropy correlates with rater\ndisagreement, varies meaningfully across subjects, and increases in tasks\nrequiring interpretive reasoning. Our findings position semantic entropy as an\ninterpretable uncertainty signal that supports more transparent and trustworthy\nAI-assisted grading workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04105v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04116", "title": "A Compositional Framework for On-the-Fly LTLf Synthesis", "authors": ["Yongkang Li", "Shengping Xiao", "Shufang Zhu", "Jianwen Li", "Geguang Pu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04116v1", "summary": "Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can\nbe reduced to a two-player game over a Deterministic Finite Automaton (DFA) of\nthe LTLf specification. The primary challenge here is DFA construction, which\nis 2EXPTIME-complete in the worst case. Existing techniques either construct\nthe DFA compositionally before solving the game, leveraging automata\nminimization to mitigate state-space explosion, or build the DFA incrementally\nduring game solving to avoid full DFA construction. However, neither is\ndominant. In this paper, we introduce a compositional on-the-fly synthesis\nframework that integrates the strengths of both approaches, focusing on large\nconjunctions of smaller LTLf formulas common in practice. This framework\napplies composition during game solving instead of automata (game arena)\nconstruction. While composing all intermediate results may be necessary in the\nworst case, pruning these results simplifies subsequent compositions and\nenables early detection of unrealizability. Specifically, the framework allows\ntwo composition variants: pruning before composition to take full advantage of\nminimization or pruning during composition to guide on-the-fly synthesis.\nCompared to state-of-the-art synthesis solvers, our framework is able to solve\na notable number of instances that other solvers cannot handle. A detailed\nanalysis shows that both composition variants have unique merits.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04116v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04118", "title": "AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities", "authors": ["Ruochen Zhao", "Simone Conia", "Eric Peng", "Min Li", "Saloni Potdar"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04118v1", "summary": "Open-domain Knowledge Graph Completion (KGC) faces significant challenges in\nan ever-changing world, especially when considering the continual emergence of\nnew entities in daily news. Existing approaches for KGC mainly rely on\npretrained language models' parametric knowledge, pre-constructed queries, or\nsingle-step retrieval, typically requiring substantial supervision and training\ndata. Even so, they often fail to capture comprehensive and up-to-date\ninformation about unpopular and/or emerging entities. To this end, we introduce\nAgentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework\nthat combines iterative retrieval actions and multi-step reasoning to\ndynamically construct rich knowledge graph triplets. Experiments show that,\ndespite requiring zero training efforts, AgREE significantly outperforms\nexisting methods in constructing knowledge graph triplets, especially for\nemerging entities that were not seen during language models' training\nprocesses, outperforming previous methods by up to 13.7%. Moreover, we propose\na new evaluation methodology that addresses a fundamental weakness of existing\nsetups and a new benchmark for KGC on emerging entities. Our work demonstrates\nthe effectiveness of combining agent-based reasoning with strategic information\nretrieval for maintaining up-to-date knowledge graphs in dynamic information\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04118v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04163", "title": "Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork", "authors": ["Hasra Dodampegama", "Mohan Sridharan"], "categories": ["cs.AI", "cs.LO", "cs.MA"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04163v1", "summary": "AI agents deployed in assistive roles often have to collaborate with other\nagents (humans, AI systems) without prior coordination. Methods considered\nstate of the art for such ad hoc teamwork often pursue a data-driven approach\nthat needs a large labeled dataset of prior observations, lacks transparency,\nand makes it difficult to rapidly revise existing knowledge in response to\nchanges. As the number of agents increases, the complexity of decision-making\nmakes it difficult to collaborate effectively. This paper advocates leveraging\nthe complementary strengths of knowledge-based and data-driven methods for\nreasoning and learning for ad hoc teamwork. For any given goal, our\narchitecture enables each ad hoc agent to determine its actions through\nnon-monotonic logical reasoning with: (a) prior commonsense domain-specific\nknowledge; (b) models learned and revised rapidly to predict the behavior of\nother agents; and (c) anticipated abstract future goals based on generic\nknowledge of similar situations in an existing foundation model. We\nexperimentally evaluate our architecture's capabilities in VirtualHome, a\nrealistic physics-based 3D simulation environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04163v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04235", "title": "Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities", "authors": ["Jiaying Zhu", "Ziyang Zheng", "Zhengyuan Shi", "Yalun Cai", "Qiang Xu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04235v1", "summary": "Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design\nAutomation. The standard workflow for solving CSAT problems converts circuits\ninto Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by\nConflict-Driven Clause Learning (CDCL). However, this process inherently\ndiscards rich structural and functional information, leading to suboptimal\nsolver performance. To address this limitation, we introduce CASCAD, a novel\ncircuit-aware SAT solving framework that directly leverages circuit-level\nconditional probabilities computed via Graph Neural Networks (GNNs). By\nexplicitly modeling gate-level conditional probabilities, CASCAD dynamically\nguides two critical CDCL heuristics -- variable phase selection and clause\nmanagementto significantly enhance solver efficiency. Extensive evaluations on\nchallenging real-world Logical Equivalence Checking (LEC) benchmarks\ndemonstrate that CASCAD reduces solving times by up to 10x compared to\nstate-of-the-art CNF-based approaches, achieving an additional 23.5% runtime\nreduction via our probability-guided clause filtering strategy. Our results\nunderscore the importance of preserving circuit-level structural insights\nwithin SAT solvers, providing a robust foundation for future improvements in\nSAT-solving efficiency and EDA tool design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04235v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04278", "title": "Large Language Model's Multi-Capability Alignment in Biomedical Domain", "authors": ["Wentao Wu", "Linqing Chen", "Hanmeng Zhong", "Weilei Wang"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04278v1", "summary": "BalancedBio is a theoretically grounded framework for parameter-efficient\nbiomedical reasoning, addressing multi-capability integration in\ndomain-specific AI alignment. It establishes the Biomedical Multi-Capability\nConvergence Theorem, proving orthogonal gradient spaces are essential to\nprevent capability interference for safe deployment. Key innovations include:\n(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending\nSource2Synth with clinical workflow constraints and medical ontology validation\nfor factual accuracy and safety; and (2) Capability Aware Group Relative Policy\nOptimization, deriving optimal hybrid reward weighting to maintain\northogonality in RL, using a reward model with rule-based and model-based\nscores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal\nconvergence, preserving performance across capabilities. It achieves\nstate-of-the-art results in its parameter class: domain expertise (80.95%\nBIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction\nfollowing (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety\nguarantees include bounds on capability preservation and clinical accuracy.\nReal-world deployment yields 78% cost reduction, 23% improved diagnostic\naccuracy, and 89% clinician acceptance. This work provides a principled\nmethodology for biomedical AI alignment, enabling efficient reasoning with\nessential safety and reliability, with the 0.5B model version to be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04278v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04282", "title": "Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling", "authors": ["Yongyi Wang", "Lingfeng Li", "Bozhou Chen", "Ang Li", "Hanyu Liu", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04282v1", "summary": "Recent research has developed benchmarks for memory-augmented reinforcement\nlearning (RL) algorithms, providing Partially Observable Markov Decision\nProcess (POMDP) environments where agents depend on past observations to make\ndecisions. While many benchmarks incorporate sufficiently complex real-world\nproblems, they lack controllability over the degree of challenges posed to\nmemory models. In contrast, synthetic environments enable fine-grained\nmanipulation of dynamics, making them critical for detailed and rigorous\nevaluation of memory-augmented RL. Our study focuses on POMDP synthesis with\nthree key contributions:\n  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand\nStructure (MDS), transition invariance, and related concepts; 2. A methodology\nleveraging linear process dynamics, state aggregation, and reward\nredistribution to construct customized POMDPs with predefined properties; 3.\nEmpirically validated series of POMDP environments with increasing difficulty\nlevels, designed based on our theoretical insights. Our work clarifies the\nchallenges of memory-augmented RL in solving POMDPs, provides guidelines for\nanalyzing and designing POMDP environments, and offers empirical support for\nselecting memory models in RL tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04282v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04339", "title": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models", "authors": ["Anran Xu", "Jincheng Wang", "Baigen Cai", "Tao Wen"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04339v1", "summary": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04339v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04361", "title": "OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing", "authors": ["Fuqing Bie", "Shiyu Huang", "Xijia Tao", "Zhiqin Fang", "Leyi Pan", "Junzhe Chen", "Min Ren", "Liuyu Xiang", "Zhaofeng He"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04361v1", "summary": "While generalist foundation models like Gemini and GPT-4o demonstrate\nimpressive multi-modal competence, existing evaluations fail to test their\nintelligence in dynamic, interactive worlds. Static benchmarks lack agency,\nwhile interactive benchmarks suffer from a severe modal bottleneck, typically\nignoring crucial auditory and temporal cues. To bridge this evaluation chasm,\nwe introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,\nbut to probe the fusion and reasoning capabilities of agentic models across the\nfull sensory spectrum. Built on a core philosophy of modality interdependence,\nOmniPlay comprises a suite of five game environments that systematically create\nscenarios of both synergy and conflict, forcing agents to perform genuine\ncross-modal reasoning. Our comprehensive evaluation of six leading omni-modal\nmodels reveals a critical dichotomy: they exhibit superhuman performance on\nhigh-fidelity memory tasks but suffer from systemic failures in challenges\nrequiring robust reasoning and strategic planning. We demonstrate that this\nfragility stems from brittle fusion mechanisms, which lead to catastrophic\nperformance degradation under modality conflict and uncover a counter-intuitive\n\"less is more\" paradox, where removing sensory information can paradoxically\nimprove performance. Our findings suggest that the path toward robust AGI\nrequires a research focus beyond scaling to explicitly address synergistic\nfusion. Our platform is available for anonymous review at\nhttps://github.com/fuqingbie/omni-game-benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04361v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04383", "title": "Artificial Consciousness as Interface Representation", "authors": ["Robert Prentner"], "categories": ["cs.AI", "q-bio.NC"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04383v1", "summary": "Whether artificial intelligence (AI) systems can possess consciousness is a\ncontentious question because of the inherent challenges of defining and\noperationalizing subjective experience. This paper proposes a framework to\nreframe the question of artificial consciousness into empirically tractable\ntests. We introduce three evaluative criteria - S (subjective-linguistic), L\n(latent-emergent), and P (phenomenological-structural) - collectively termed\nSLP-tests, which assess whether an AI system instantiates interface\nrepresentations that facilitate consciousness-like properties. Drawing on\ncategory theory, we model interface representations as mappings between\nrelational substrates (RS) and observable behaviors, akin to specific types of\nabstraction layers. The SLP-tests collectively operationalize subjective\nexperience not as an intrinsic property of physical systems but as a functional\ninterface to a relational entity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04383v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04389", "title": "GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning", "authors": ["Weitai Kang", "Bin Lei", "Gaowen Liu", "Caiwen Ding", "Yan Yan"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04389v1", "summary": "Graphical user interface visual grounding (GUI-VG), a core capability for GUI\nagents, has primarily relied on supervised fine-tuning (SFT) of multimodal\nlarge language models (MLLMs), which demands extensive data curation and\nsignificant training costs. However, as MLLMs continue to advance and even\ncover GUI domains during pretraining, the necessity of exhaustive SFT\npost-training becomes increasingly questionable. Meanwhile, recent successes of\nrule-based reinforcement fine-tuning (RFT) suggest a more efficient\nalternative. Despite this promise, the optimal manner of applying RFT for\nGUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a\nreinforcement learning-based GUI-VG method built on a systematic empirical\nstudy and a novel stabilization technique. We find that naive application of\nRFT underperforms the SFT baseline, motivating a deeper exploration. First, we\ndecompose RFT into its core components and analyze the optimal formulation of\neach. Second, we propose a novel Adversarial KL Factor that dynamically\nstabilizes training to mitigate reward over-optimization. Third, we further\nexplore the training configurations of RFT to enhance effectiveness. Extensive\nexperiments show that GuirlVG, with only 5.2K training samples, outperforms SFT\nmethods trained on over 10M samples, achieving a 7.7% improvement on\nScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on\nScreenSpotV2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04389v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04412", "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents", "authors": ["Thassilo M. Schiepanski", "Nicholas PiÃ«l"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04412v1", "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04412v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04428", "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices", "authors": ["Si Chen", "Izzy Molnar", "Ting Hua", "Peiyu Li", "Le Huy Khiem", "G. Alex Ambrose", "Jim Lang", "Ronald Metoyer", "Nitesh V. Chawla"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04428v1", "summary": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04428v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04460", "title": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control", "authors": ["Rui Ha", "Chaozhuo Li", "Rui Pu", "Sen Su"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04460v1", "summary": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04460v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04482", "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use", "authors": ["Xueyu Hu", "Tao Xiong", "Biao Yi", "Zishu Wei", "Ruixuan Xiao", "Yurun Chen", "Jiasheng Ye", "Meiling Tao", "Xiangxin Zhou", "Ziyu Zhao", "Yuhuai Li", "Shengze Xu", "Shenzhi Wang", "Xinchen Xu", "Shuofei Qiao", "Zhaokai Wang", "Kun Kuang", "Tieyong Zeng", "Liang Wang", "Jiwei Li", "Yuchen Eleanor Jiang", "Wangchunshu Zhou", "Guoyin Wang", "Keting Yin", "Zhou Zhao", "Hongxia Yang", "Fan Wu", "Shengyu Zhang", "Fei Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04482v1", "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04482v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04511", "title": "Argumentative Debates for Transparent Bias Detection [Technical Report]", "authors": ["Hamed Ayoobi", "Nico Potyka", "Anna Rapberger", "Francesca Toni"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04511v1", "summary": "As the use of AI systems in society grows, addressing potential biases that\nemerge from data or are learned by models is essential to prevent systematic\ndisadvantages against specific groups. Several notions of (un)fairness have\nbeen proposed in the literature, alongside corresponding algorithmic methods\nfor detecting and mitigating unfairness, but, with very few exceptions, these\ntend to ignore transparency. Instead, interpretability and explainability are\ncore requirements for algorithmic fairness, even more so than for other\nalgorithmic solutions, given the human-oriented nature of fairness. In this\npaper, we contribute a novel interpretable, explainable method for bias\ndetection relying on debates about the presence of bias against individuals,\nbased on the values of protected features for the individuals and others in\ntheir neighbourhoods. Our method builds upon techniques from formal and\ncomputational argumentation, whereby debates result from arguing about biases\nwithin and across neighbourhoods. We provide formal, quantitative, and\nqualitative evaluations of our method, highlighting its strengths in\nperformance against baselines, as well as its interpretability and\nexplainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04511v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04563", "title": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset", "authors": ["Mei Jiang", "Houping Yue", "Bingdong Li", "Hao Hao", "Ying Qian", "Bo Jiang", "Aimin Zhou"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04563v1", "summary": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04563v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04576", "title": "ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04576v1", "summary": "Reasoning is a critical capability of multimodal large language models\n(MLLMs) for solving complex multimodal tasks, and judging the correctness of\nreasoning steps is crucial for improving this capability. Recently, MLLM-based\nprocess judges (MPJs) have been widely used to assess the correctness of\nreasoning steps in multimodal tasks. Therefore, evaluating MPJs is important\nfor identifying their limitations and guiding future improvements. However,\nexisting benchmarks for MPJs mainly focus on tasks such as step correctness\nclassification and reasoning process search, while overlooking a key aspect:\nwhether the confidence scores produced by MPJs at the step level are reliable.\nTo address this gap, we propose ConfProBench, the first comprehensive benchmark\ndesigned to systematically evaluate the reliability of step-level confidence\nscores generated by MPJs. Our benchmark constructs three types of adversarially\nperturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and\nImage Perturbation, to test the robustness of MPJ confidence under\nperturbations. In addition, we introduce three novel evaluation metrics:\nConfidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and\nConfidence Calibration Score (CCS), which evaluate robustness, sensitivity, and\ncalibration, respectively. We evaluate 14 state-of-the-art MLLMs, including\nboth proprietary and open-source models. Experiments reveal limitations in\ncurrent MPJs' confidence performance and offer competitive baselines to support\nfuture research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04576v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04652", "title": "LLM Collaboration With Multi-Agent Reinforcement Learning", "authors": ["Shuo Liu", "Zeyu Liang", "Xueguang Lyu", "Christopher Amato"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04652v1", "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04652v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04700", "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience", "authors": ["Zeyi Sun", "Ziyu Liu", "Yuhang Zang", "Yuhang Cao", "Xiaoyi Dong", "Tong Wu", "Dahua Lin", "Jiaqi Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04700v1", "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04700v1", "cate": "cs.AI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2312.10925", "title": "Delving Deeper Into Astromorphic Transformers", "authors": ["Md Zesun Ahmed Mia", "Malyaban Bal", "Abhronil Sengupta"], "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.NE"], "primary_category": "cs.NE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.10925", "summary": "Preliminary attempts at incorporating the critical role of astrocytes - cells\nthat constitute more than 50\\% of human brain cells - in brain-inspired\nneuromorphic computing remain in infancy. This paper seeks to delve deeper into\nvarious key aspects of neuron-synapse-astrocyte interactions to mimic\nself-attention mechanisms in Transformers. The cross-layer perspective explored\nin this work involves bioplausible modeling of Hebbian and presynaptic\nplasticities in neuron-astrocyte networks, incorporating effects of\nnon-linearities and feedback along with algorithmic formulations to map the\nneuron-astrocyte computations to self-attention mechanism and evaluating the\nimpact of incorporating bio-realistic effects from the machine learning\napplication side. Our analysis on sentiment and image classification tasks\n(IMDB and CIFAR10 datasets) highlights the advantages of Astromorphic\nTransformers, offering improved accuracy and learning speed. Furthermore, the\nmodel demonstrates strong natural language generation capabilities on the\nWikiText-2 dataset, achieving better perplexity compared to conventional\nmodels, thus showcasing enhanced generalization and stability across diverse\nmachine learning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.10925", "cate": "cs.NE", "date": "2023-12-18", "updated": "2025-04-24", "section": "cross"}
{"id": "2409.15173", "title": "Recommendation with Generative Models", "authors": ["Yashar Deldjoo", "Zhankui He", "Julian McAuley", "Anton Korikov", "Scott Sanner", "Arnau Ramisa", "Rene Vidal", "Maheswaran Sathiamoorthy", "Atoosa Kasrizadeh", "Silvia Milano", "Francesco Ricci"], "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.15173", "summary": "Generative models are a class of AI models capable of creating new instances\nof data by learning and sampling from their statistical distributions. In\nrecent years, these models have gained prominence in machine learning due to\nthe development of approaches such as generative adversarial networks (GANs),\nvariational autoencoders (VAEs), and transformer-based architectures such as\nGPT. These models have applications across various domains, such as image\ngeneration, text synthesis, and music composition. In recommender systems,\ngenerative models, referred to as Gen-RecSys, improve the accuracy and\ndiversity of recommendations by generating structured outputs, text-based\ninteractions, and multimedia content. By leveraging these capabilities,\nGen-RecSys can produce more personalized, engaging, and dynamic user\nexperiences, expanding the role of AI in eCommerce, media, and beyond.\n  Our book goes beyond existing literature by offering a comprehensive\nunderstanding of generative models and their applications, with a special focus\non deep generative models (DGMs) and their classification. We introduce a\ntaxonomy that categorizes DGMs into three types: ID-driven models, large\nlanguage models (LLMs), and multimodal models. Each category addresses unique\ntechnical and architectural advancements within its respective research area.\nThis taxonomy allows researchers to easily navigate developments in Gen-RecSys\nacross domains such as conversational AI and multimodal content generation.\nAdditionally, we examine the impact and potential risks of generative models,\nemphasizing the importance of robust evaluation frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.15173", "cate": "cs.IR", "date": "2024-09-18", "updated": "2024-09-18", "section": "cross"}
{"id": "2506.16991", "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds", "authors": ["Binbin Xiang", "Maciej Wielgosz", "Stefano Puliti", "Kamil KrÃ¡l", "Martin KrÅ¯Äek", "Azim Missarov", "Rasmus Astrup"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16991", "summary": "The segmentation of forest LiDAR 3D point clouds, including both individual\ntree and semantic segmentation, is fundamental for advancing forest management\nand ecological research. However, current approaches often struggle with the\ncomplexity and variability of natural forest environments. We present\nForestFormer3D, a new unified and end-to-end framework designed for precise\nindividual tree and semantic segmentation. ForestFormer3D incorporates\nISA-guided query point selection, a score-based block merging strategy during\ninference, and a one-to-many association mechanism for effective training. By\ncombining these new components, our model achieves state-of-the-art performance\nfor individual tree segmentation on the newly introduced FOR-instanceV2\ndataset, which spans diverse forest types and regions. Additionally,\nForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),\nshowcasing its robustness across different forest conditions and sensor\nmodalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly\navailable at https://bxiang233.github.io/FF3D/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16991", "cate": "cs.CV", "date": "2025-06-20", "updated": "2025-08-01", "section": "cross"}
{"id": "2508.02314", "title": "Large AI Models for Wireless Physical Layer", "authors": ["Jiajia Guo", "Yiming Cui", "Shi Jin", "Jun Zhang"], "categories": ["cs.AI", "cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02314", "summary": "Large artificial intelligence models (LAMs) are transforming wireless\nphysical layer technologies through their robust generalization, multitask\nprocessing, and multimodal capabilities. This article reviews recent\nadvancements in LAM applications for physical layer communications, addressing\nlimitations of conventional AI-based approaches. LAM applications are\nclassified into two strategies: leveraging pre-trained LAMs and developing\nnative LAMs designed specifically for physical layer tasks. The motivations and\nkey frameworks of these approaches are comprehensively examined through\nmultiple use cases. Both strategies significantly improve performance and\nadaptability across diverse wireless scenarios. Future research directions,\nincluding efficient architectures, interpretability, standardized datasets, and\ncollaboration between large and small models, are proposed to advance LAM-based\nphysical layer solutions for next-generation communication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02314", "cate": "cs.IT", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03696", "title": "PLA: Prompt Learning Attack against Text-to-Image Generative Models", "authors": ["Xinqi Lyu", "Yihao Liu", "Yanjie Li", "Bin Xiao"], "categories": ["cs.AI", "cs.CR", "cs.CV"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03696", "summary": "Text-to-Image (T2I) models have gained widespread adoption across various\napplications. Despite the success, the potential misuse of T2I models poses\nsignificant risks of generating Not-Safe-For-Work (NSFW) content. To\ninvestigate the vulnerability of T2I models, this paper delves into adversarial\nattacks to bypass the safety mechanisms under black-box settings. Most previous\nmethods rely on word substitution to search adversarial prompts. Due to limited\nsearch space, this leads to suboptimal performance compared to gradient-based\ntraining. However, black-box settings present unique challenges to training\ngradient-driven attack methods, since there is no access to the internal\narchitecture and parameters of T2I models. To facilitate the learning of\nadversarial prompts in black-box settings, we propose a novel prompt learning\nattack framework (PLA), where insightful gradient-based training tailored to\nblack-box T2I models is designed by utilizing multimodal similarities.\nExperiments show that our new method can effectively attack the safety\nmechanisms of black-box T2I models including prompt filters and post-hoc safety\ncheckers with a high success rate compared to state-of-the-art methods.\nWarning: This paper may contain offensive model-generated content.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03696", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14", "section": "cross"}
{"id": "2508.03700", "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Tao Gui", "Xuanjing Huang", "Yu-Gang Jiang", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03700", "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03700", "cate": "cs.HC", "date": "2025-07-19", "updated": "2025-07-19", "section": "cross"}
{"id": "2508.03703", "title": "Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective", "authors": ["Yubo Wang", "Min Tang", "Nuo Shen", "Shujie Cui", "Weiqing Wang"], "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03703", "summary": "The large language model (LLM) powered recommendation paradigm has been\nproposed to address the limitations of traditional recommender systems, which\noften struggle to handle cold start users or items with new IDs. Despite its\neffectiveness, this study uncovers that LLM empowered recommender systems are\nvulnerable to reconstruction attacks that can expose both system and user\nprivacy. To examine this threat, we present the first systematic study on\ninversion attacks targeting LLM empowered recommender systems, where\nadversaries attempt to reconstruct original prompts that contain personal\npreferences, interaction histories, and demographic attributes by exploiting\nthe output logits of recommendation models. We reproduce the vec2text framework\nand optimize it using our proposed method called Similarity Guided Refinement,\nenabling more accurate reconstruction of textual prompts from model generated\nlogits. Extensive experiments across two domains (movies and books) and two\nrepresentative LLM based recommendation models demonstrate that our method\nachieves high fidelity reconstructions. Specifically, we can recover nearly 65\npercent of the user interacted items and correctly infer age and gender in 87\npercent of the cases. The experiments also reveal that privacy leakage is\nlargely insensitive to the victim model's performance but highly dependent on\ndomain consistency and prompt complexity. These findings expose critical\nprivacy vulnerabilities in LLM empowered recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03703", "cate": "cs.IR", "date": "2025-07-20", "updated": "2025-07-20", "section": "cross"}
{"id": "2508.03706", "title": "Controllable Surface Diffusion Generative Model for Neurodevelopmental Trajectories", "authors": ["Zhenshan Xie", "Levente Baljer", "M. Jorge Cardoso", "Emma Robinson"], "categories": ["cs.AI", "q-bio.NC"], "primary_category": "q-bio.NC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03706", "summary": "Preterm birth disrupts the typical trajectory of cortical neurodevelopment,\nincreasing the risk of cognitive and behavioral difficulties. However, outcomes\nvary widely, posing a significant challenge for early prediction. To address\nthis, individualized simulation offers a promising solution by modeling\nsubject-specific neurodevelopmental trajectories, enabling the identification\nof subtle deviations from normative patterns that might act as biomarkers of\nrisk. While generative models have shown potential for simulating\nneurodevelopment, prior approaches often struggle to preserve subject-specific\ncortical folding patterns or to reproduce region-specific morphological\nvariations. In this paper, we present a novel graph-diffusion network that\nsupports controllable simulation of cortical maturation. Using cortical surface\ndata from the developing Human Connectome Project (dHCP), we demonstrate that\nthe model maintains subject-specific cortical morphology while modeling\ncortical maturation sufficiently well to fool an independently trained age\nregression network, achieving a prediction accuracy of $0.85 \\pm 0.62$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03706", "cate": "q-bio.NC", "date": "2025-07-21", "updated": "2025-07-21", "section": "cross"}
{"id": "2508.03711", "title": "A Social Data-Driven System for Identifying Estate-related Events and Topics", "authors": ["Wenchuan Mu", "Menglin Li", "Kwan Hui Lim"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03711", "summary": "Social media platforms such as Twitter and Facebook have become deeply\nembedded in our everyday life, offering a dynamic stream of localized news and\npersonal experiences. The ubiquity of these platforms position them as valuable\nresources for identifying estate-related issues, especially in the context of\ngrowing urban populations. In this work, we present a language model-based\nsystem for the detection and classification of estate-related events from\nsocial media content. Our system employs a hierarchical classification\nframework to first filter relevant posts and then categorize them into\nactionable estate-related topics. Additionally, for posts lacking explicit\ngeotags, we apply a transformer-based geolocation module to infer posting\nlocations at the point-of-interest level. This integrated approach supports\ntimely, data-driven insights for urban management, operational response and\nsituational awareness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03711", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22", "section": "cross"}
{"id": "2508.03714", "title": "\"Think First, Verify Always\": Training Humans to Face AI Risks", "authors": ["Yuksel Aydin"], "categories": ["cs.AI", "cs.CR", "cs.CY", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03714", "summary": "Artificial intelligence enables unprecedented attacks on human cognition, yet\ncybersecurity remains predominantly device-centric. This paper introduces the\n\"Think First, Verify Always\" (TFVA) protocol, which repositions humans as\n'Firewall Zero', the first line of defense against AI-enabled threats. The\nprotocol is grounded in five operational principles: Awareness, Integrity,\nJudgment, Ethical Responsibility, and Transparency (AIJET). A randomized\ncontrolled trial (n=151) demonstrated that a minimal 3-minute intervention\nproduced statistically significant improvements in cognitive security task\nperformance, with participants showing an absolute +7.87% gains compared to\ncontrols. These results suggest that brief, principles-based training can\nrapidly enhance human resilience against AI-driven cognitive manipulation. We\nrecommend that GenAI platforms embed \"Think First, Verify Always\" as a standard\nprompt, replacing passive warnings with actionable protocols to enhance\ntrustworthy and ethical AI use. By bridging the gap between technical\ncybersecurity and human factors, the TFVA protocol establishes human-empowered\nsecurity as a vital component of trustworthy AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03714", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "section": "cross"}
{"id": "2508.03715", "title": "Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors", "authors": ["Bertram Fuchs", "Mehdi Ejtehadi", "Ana Cisnal", "JÃ¼rgen Pannek", "Anke Scheel-Sailer", "Robert Riener", "Inge Eriks-Hoogland", "Diego Paez-Granados"], "categories": ["cs.AI", "cs.HC", "cs.LG", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03715", "summary": "Autonomic Dysreflexia (AD) is a potentially life-threatening condition\ncharacterized by sudden, severe blood pressure (BP) spikes in individuals with\nspinal cord injury (SCI). Early, accurate detection is essential to prevent\ncardiovascular complications, yet current monitoring methods are either\ninvasive or rely on subjective symptom reporting, limiting applicability in\ndaily file. This study presents a non-invasive, explainable machine learning\nframework for detecting AD using multimodal wearable sensors. Data were\ncollected from 27 individuals with chronic SCI during urodynamic studies,\nincluding electrocardiography (ECG), photoplethysmography (PPG), bioimpedance\n(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three\ncommercial devices. Objective AD labels were derived from synchronized\ncuff-based BP measurements. Following signal preprocessing and feature\nextraction, BorutaSHAP was used for robust feature selection, and SHAP values\nfor explainability. We trained modality- and device-specific weak learners and\naggregated them using a stacked ensemble meta-model. Cross-validation was\nstratified by participants to ensure generalizability. HR- and ECG-derived\nfeatures were identified as the most informative, particularly those capturing\nrhythm morphology and variability. The Nearest Centroid ensemble yielded the\nhighest performance (Macro F1 = 0.77+/-0.03), significantly outperforming\nbaseline models. Among modalities, HR achieved the highest area under the curve\n(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature\nfeatures contributed less to overall accuracy, consistent with missing data and\nlow specificity. The model proved robust to sensor dropout and aligned well\nwith clinical AD events. These results represent an important step toward\npersonalized, real-time monitoring for individuals with SCI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03715", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "section": "cross"}
{"id": "2508.03718", "title": "Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding", "authors": ["Mike Gartner"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03718", "summary": "U.S. health insurance is complex, and inadequate understanding and limited\naccess to justice have dire implications for the most vulnerable. Advances in\nnatural language processing present an opportunity to support efficient,\ncase-specific understanding, and to improve access to justice and healthcare.\nYet existing corpora lack context necessary for assessing even simple cases. We\ncollect and release a corpus of reputable legal and medical text related to\nU.S. health insurance. We also introduce an outcome prediction task for health\ninsurance appeals designed to support regulatory and patient self-help\napplications, and release a labeled benchmark for our task, and models trained\non it.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03718", "cate": "cs.CY", "date": "2025-07-28", "updated": "2025-07-28", "section": "cross"}
{"id": "2508.03719", "title": "Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering", "authors": ["Abhay Vijayvargia", "Ajay Nagpal", "Kundeshwar Pundalik", "Atharva Savarkar", "Smita Gautam", "Pankaj Singh", "Rohit Saluja", "Ganesh Ramakrishnan"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03719", "summary": "Indian farmers often lack timely, accessible, and language-friendly\nagricultural advice, especially in rural areas with low literacy. To address\nthis gap in accessibility, this paper presents a novel AI-powered agricultural\nchatbot, Krishi Sathi, designed to support Indian farmers by providing\npersonalized, easy-to-understand answers to their queries through both text and\nspeech. The system's intelligence stems from an IFT model, subsequently refined\nthrough fine-tuning on Indian agricultural knowledge across three curated\ndatasets. Unlike traditional chatbots that respond to one-off questions, Krishi\nSathi follows a structured, multi-turn conversation flow to gradually collect\nthe necessary details from the farmer, ensuring the query is fully understood\nbefore generating a response. Once the intent and context are extracted, the\nsystem performs Retrieval-Augmented Generation (RAG) by first fetching\ninformation from a curated agricultural database and then generating a tailored\nresponse using the IFT model. The chatbot supports both English and Hindi\nlanguages, with speech input and output features (via ASR and TTS) to make it\naccessible for users with low literacy or limited digital skills. This work\ndemonstrates how combining intent-driven dialogue flows, instruction-tuned\nmodels, and retrieval-based generation can improve the quality and\naccessibility of digital agricultural support in India.\n  This approach yielded strong results, with the system achieving a query\nresponse accuracy of 97.53%, 91.35% contextual relevance and personalization,\nand a query completion rate of 97.53%. The average response time remained under\n6 seconds, ensuring timely support for users across both English and Hindi\ninteractions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03719", "cate": "cs.CL", "date": "2025-07-28", "updated": "2025-07-28", "section": "cross"}
{"id": "2508.03722", "title": "Multimodal Video Emotion Recognition with Reliable Reasoning Priors", "authors": ["Zhepeng Wang", "Yingjian Zhu", "Guanghao Dong", "Hongzhu Yi", "Feng Chen", "Xinming Wang", "Jun Xie"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03722", "summary": "This study investigates the integration of trustworthy prior reasoning\nknowledge from MLLMs into multimodal emotion recognition. We employ Gemini to\ngenerate fine-grained, modality-separable reasoning traces, which are injected\nas priors during the fusion stage to enrich cross-modal interactions. To\nmitigate the pronounced class-imbalance in multimodal emotion recognition, we\nintroduce Balanced Dual-Contrastive Learning, a loss formulation that jointly\nbalances inter-class and intra-class distributions. Applied to the MER2024\nbenchmark, our prior-enhanced framework yields substantial performance gains,\ndemonstrating that the reliability of MLLM-derived reasoning can be\nsynergistically combined with the domain adaptability of lightweight fusion\nnetworks for robust, scalable emotion recognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03722", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29", "section": "cross"}
{"id": "2508.03733", "title": "CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning", "authors": ["Wenjie Li", "Yujie Zhang", "Haoran Sun", "Yueqi Li", "Fanrui Zhang", "Mengzhe Xu", "Victoria Borja Clausich", "Sade Mellin", "Renhao Yang", "Chenrun Wang", "Jethro Zih-Shuo Wang", "Shiyi Yao", "Gen Li", "Yidong Xu", "Hanyu Wang", "Yilin Huang", "Angela Lin Wang", "Chen Shi", "Yin Zhang", "Jianan Guo", "Luqi Yang", "Renxuan Li", "Yang Xu", "Jiawei Liu", "Yao Zhang", "Lei Liu", "Carlos GutiÃ©rrez SanRomÃ¡n", "Lei Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03733", "summary": "Chest X-ray (CXR) imaging is one of the most widely used diagnostic\nmodalities in clinical practice, encompassing a broad spectrum of diagnostic\ntasks. Recent advancements have seen the extensive application of\nreasoning-based multimodal large language models (MLLMs) in medical imaging to\nenhance diagnostic efficiency and interpretability. However, existing\nmultimodal models predominantly rely on \"one-time\" diagnostic approaches,\nlacking verifiable supervision of the reasoning process. This leads to\nchallenges in multi-task CXR diagnosis, including lengthy reasoning, sparse\nrewards, and frequent hallucinations. To address these issues, we propose\nCX-Mind, the first generative model to achieve interleaved \"think-answer\"\nreasoning for CXR tasks, driven by curriculum-based reinforcement learning and\nverifiable process rewards (CuRL-VPR). Specifically, we constructed an\ninstruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148\nsamples, and generated 42,828 high-quality interleaved reasoning data points\nsupervised by clinical reports. Optimization was conducted in two stages under\nthe Group Relative Policy Optimization framework: initially stabilizing basic\nreasoning with closed-domain tasks, followed by transfer to open-domain\ndiagnostics, incorporating rule-based conditional process rewards to bypass the\nneed for pretrained reward models. Extensive experimental results demonstrate\nthat CX-Mind significantly outperforms existing medical and general-domain\nMLLMs in visual understanding, text generation, and spatiotemporal alignment,\nachieving an average performance improvement of 25.1% over comparable\nCXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves\na mean recall@1 across 14 diseases that substantially surpasses the second-best\nresults, with multi-center expert evaluations further confirming its clinical\nutility across multiple dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03733", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03734", "title": "A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models", "authors": ["Xiaoling Luo", "Ruli Zheng", "Qiaojian Zheng", "Zibo Du", "Shuo Yang", "Meidan Ding", "Qihao Xu", "Chengliang Liu", "Linlin Shen"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03734", "summary": "Visual impairment represents a major global health challenge, with multimodal\nimaging providing complementary information that is essential for accurate\nophthalmic diagnosis. This comprehensive survey systematically reviews the\nlatest advances in multimodal deep learning methods in ophthalmology up to the\nyear 2025. The review focuses on two main categories: task-specific multimodal\napproaches and large-scale multimodal foundation models. Task-specific\napproaches are designed for particular clinical applications such as lesion\ndetection, disease diagnosis, and image synthesis. These methods utilize a\nvariety of imaging modalities including color fundus photography, optical\ncoherence tomography, and angiography. On the other hand, foundation models\ncombine sophisticated vision-language architectures and large language models\npretrained on diverse ophthalmic datasets. These models enable robust\ncross-modal understanding, automated clinical report generation, and decision\nsupport. The survey critically examines important datasets, evaluation metrics,\nand methodological innovations including self-supervised learning,\nattention-based fusion, and contrastive alignment. It also discusses ongoing\nchallenges such as variability in data, limited annotations, lack of\ninterpretability, and issues with generalizability across different patient\npopulations. Finally, the survey outlines promising future directions that\nemphasize the use of ultra-widefield imaging and reinforcement learning-based\nreasoning frameworks to create intelligent, interpretable, and clinically\napplicable AI systems for ophthalmology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03734", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03735", "title": "StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization", "authors": ["Gopalji Gaur", "Mohammadreza Zolfaghari", "Thomas Brox"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03735", "summary": "Generating a coherent sequence of images that tells a visual story, using\ntext-to-image diffusion models, often faces the critical challenge of\nmaintaining subject consistency across all story scenes. Existing approaches,\nwhich typically rely on fine-tuning or retraining models, are computationally\nexpensive, time-consuming, and often interfere with the model's pre-existing\ncapabilities. In this paper, we follow a training-free approach and propose an\nefficient consistent-subject-generation method. This approach works seamlessly\nwith pre-trained diffusion models by introducing masked cross-image attention\nsharing to dynamically align subject features across a batch of images, and\nRegional Feature Harmonization to refine visually similar details for improved\nsubject consistency. Experimental results demonstrate that our approach\nsuccessfully generates visually consistent subjects across a variety of\nscenarios while maintaining the creative abilities of the diffusion model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03735", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03736", "title": "Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities", "authors": ["Rafayel Mkrtchyan", "Armen Manukyan", "Hrant Khachatrian", "Theofanis P. Raptis"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03736", "summary": "Environment mapping is an important computing task for a wide range of smart\ncity applications, including autonomous navigation, wireless network operations\nand extended reality environments. Conventional smart city mapping techniques,\nsuch as satellite imagery, LiDAR scans, and manual annotations, often suffer\nfrom limitations related to cost, accessibility and accuracy. Open-source\nmapping platforms have been widely utilized in artificial intelligence\napplications for environment mapping, serving as a source of ground truth.\nHowever, human errors and the evolving nature of real-world environments\nintroduce biases that can negatively impact the performance of neural networks\ntrained on such data. In this paper, we present a deep learning-based approach\nthat integrates the DINOv2 architecture to improve building mapping by\ncombining maps from open-source platforms with radio frequency (RF) data\ncollected from multiple wireless user equipments and base stations. Our\napproach leverages a vision transformer-based architecture to jointly process\nboth RF and map modalities within a unified framework, effectively capturing\nspatial dependencies and structural priors for enhanced mapping accuracy. For\nthe evaluation purposes, we employ a synthetic dataset co-produced by Huawei.\nWe develop and train a model that leverages only aggregated path loss\ninformation to tackle the mapping problem. We measure the results according to\nthree performance metrics which capture different qualities: (i) The Jaccard\nindex, also known as intersection over union (IoU), (ii) the Hausdorff\ndistance, and (iii) the Chamfer distance. Our design achieves a macro IoU of\n65.3%, significantly surpassing (i) the erroneous maps baseline, which yields\n40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and\n(iii) a non-AI fusion baseline that we designed which yields 42.2%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03736", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03737", "title": "GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models", "authors": ["Ashutosh Bandooni", "Brindha Subburaj"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03737", "summary": "Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on\nseveral fields and domains are being curated more frequently over the last few\nyears. However these are often monolingual, mostly available in English.\nAdditionally there also is a lack of datasets available in Hindi on tasks apart\nfrom comprehension and translation. We introduce GanitBench, a tough benchmark\nconsisting of 1527 vision-only questions covering several topics in Mathematics\n- available in languages English and Hindi. Collected from two major\nexaminations from India, the JEE Advanced and the CBSE Boards examinations,\nthis benchmark includes questions in the form of images comprising of figures\nessential to a question as well as text. We evaluate two closed source models\nfor the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.\nGPT-4o mini is found to be the more dominant model on the benchmark, with it's\nhighest average accuracy being 38.15%. We also evaluate models through a\n\"Double Lock\" constraint, which brings down the performance of the models by\nconsiderable margins. We observe that two-shot CoT appears to be a more\neffective setting under this environment. Performance of the two VLMs also\ndecreases when answering the same questions in the Hindi language. We hope to\nfacilitate the inclusion of languages like Hindi in research through our work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03737", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03738", "title": "Improve Retinal Artery/Vein Classification via Channel Couplin", "authors": ["Shuang Zeng", "Chee Hong Lee", "Kaiwen Li", "Boxu Xie", "Ourui Fu", "Hangzhou He", "Lei Zhu", "Yanye Lu", "Fangxiao Cheng"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03738", "summary": "Retinal vessel segmentation plays a vital role in analyzing fundus images for\nthe diagnosis of systemic and ocular diseases. Building on this, classifying\nsegmented vessels into arteries and veins (A/V) further enables the extraction\nof clinically relevant features such as vessel width, diameter and tortuosity,\nwhich are essential for detecting conditions like diabetic and hypertensive\nretinopathy. However, manual segmentation and classification are\ntime-consuming, costly and inconsistent. With the advancement of Convolutional\nNeural Networks, several automated methods have been proposed to address this\nchallenge, but there are still some issues. For example, the existing methods\nall treat artery, vein and overall vessel segmentation as three separate binary\ntasks, neglecting the intrinsic coupling relationships between these anatomical\nstructures. Considering artery and vein structures are subsets of the overall\nretinal vessel map and should naturally exhibit prediction consistency with it,\nwe design a novel loss named Channel-Coupled Vessel Consistency Loss to enforce\nthe coherence and consistency between vessel, artery and vein predictions,\navoiding biasing the network toward three simple binary segmentation tasks.\nMoreover, we also introduce a regularization term named intra-image pixel-level\ncontrastive loss to extract more discriminative feature-level fine-grained\nrepresentations for accurate retinal A/V classification. SOTA results have been\nachieved across three public A/V classification datasets including RITE, LES-AV\nand HRF. Our code will be available upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03738", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03739", "title": "A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection", "authors": ["Md. Ehsanul Haque", "Abrar Fahim", "Shamik Dey", "Syoda Anamika Jahan", "S. M. Jahidul Islam", "Sakib Rokoni", "Md Sakib Morshed"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03739", "summary": "Early and accurate detection of the bone fracture is paramount to initiating\ntreatment as early as possible and avoiding any delay in patient treatment and\noutcomes. Interpretation of X-ray image is a time consuming and error prone\ntask, especially when resources for such interpretation are limited by lack of\nradiology expertise. Additionally, deep learning approaches used currently,\ntypically suffer from misclassifications and lack interpretable explanations to\nclinical use. In order to overcome these challenges, we propose an automated\nframework of bone fracture detection using a VGG-19 model modified to our\nneeds. It incorporates sophisticated preprocessing techniques that include\nContrast Limited Adaptive Histogram Equalization (CLAHE), Otsu's thresholding,\nand Canny edge detection, among others, to enhance image clarity as well as to\nfacilitate the feature extraction. Therefore, we use Grad-CAM, an Explainable\nAI method that can generate visual heatmaps of the model's decision making\nprocess, as a type of model interpretability, for clinicians to understand the\nmodel's decision making process. It encourages trust and helps in further\nclinical validation. It is deployed in a real time web application, where\nhealthcare professionals can upload X-ray images and get the diagnostic\nfeedback within 0.5 seconds. The performance of our modified VGG-19 model\nattains 99.78\\% classification accuracy and AUC score of 1.00, making it\nexceptionally good. The framework provides a reliable, fast, and interpretable\nsolution for bone fracture detection that reasons more efficiently for\ndiagnoses and better patient care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03739", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31", "section": "cross"}
{"id": "2508.03740", "title": "VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission", "authors": ["Jianqiao Chen", "Tingting Zhu", "Huishi Song", "Nan Ma", "Xiaodong Xu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03740", "summary": "Discretization of semantic features enables interoperability between semantic\nand digital communication systems, showing significant potential for practical\napplications. The fundamental difficulty in digitizing semantic features stems\nfrom the need to preserve continuity and context in inherently analog\nrepresentations during their compression into discrete symbols while ensuring\nrobustness to channel degradation. In this paper, we propose a vector quantized\n(VQ)-enabled digital semantic communication system with channel adaptive image\ntransmission, named VQ-DeepISC. Guided by deep joint source-channel coding\n(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic\nfeature extraction, followed by VQ modules projecting features into discrete\nlatent spaces. Consequently, it enables efficient index-based transmission\ninstead of raw feature transmission. To further optimize this process, we\ndevelop an attention mechanism-driven channel adaptation module to dynamically\noptimize index transmission. Secondly, to counteract codebook collapse during\ntraining process, we impose a distributional regularization by minimizing the\nKullback-Leibler divergence (KLD) between codeword usage frequencies and a\nuniform prior. Meanwhile, exponential moving average (EMA) is employed to\nstabilize training and ensure balanced feature coverage during codebook\nupdates. Finally, digital communication is implemented using quadrature phase\nshift keying (QPSK) modulation alongside orthogonal frequency division\nmultiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental\nresults demonstrate superior reconstruction fidelity of the proposed system\nover benchmark methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03740", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01", "section": "cross"}
{"id": "2508.03741", "title": "Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models", "authors": ["Xin Liu", "Qiyang Song", "Shaowen Xu", "Kerou Zhou", "Wenbo Jiang", "Xiaoqi Jia", "Weijuan Zhang", "Heqing Huang", "Yakai Li"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03741", "summary": "Large Language Models (LLMs) often retain inaccurate or outdated information\nfrom pre-training, leading to incorrect predictions or biased outputs during\ninference. While existing model editing methods can address this challenge,\nthey struggle with editing large amounts of factual information simultaneously\nand may compromise the general capabilities of the models. In this paper, our\nempirical study demonstrates that it is feasible to edit the internal\nrepresentations of LLMs and replace the entities in a manner similar to editing\nnatural language inputs. Based on this insight, we introduce the Latent\nKnowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of\nspecific entities via a lightweight hypernetwork to enable precise and\nlarge-scale editing. Experiments conducted on Llama-2 and Mistral show even\nwith the number of simultaneous edits reaching 10,000, LKS effectively performs\nknowledge editing while preserving the general abilities of the edited LLMs.\nCode is available at: https://github.com/Linuxin-xxx/LKS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03741", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01", "section": "cross"}
{"id": "2508.03742", "title": "Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training", "authors": ["Weiwei Cao", "Jianpeng Zhang", "Zhongyi Shui", "Sinuo Wang", "Zeli Chen", "Xi Li", "Le Lu", "Xianghua Ye", "Tingbo Liang", "Qi Zhang", "Ling Zhang"], "categories": ["cs.AI", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03742", "summary": "Vision-language pre-training (VLP) has great potential for developing\nmultifunctional and general medical diagnostic capabilities. However, aligning\nmedical images with a low signal-to-noise ratio (SNR) to reports with a high\nSNR presents a semantic density gap, leading to visual alignment bias. In this\npaper, we propose boosting vision semantic density to improve alignment\neffectiveness. On one hand, we enhance visual semantics through disease-level\nvision contrastive learning, which strengthens the model's ability to\ndifferentiate between normal and abnormal samples for each anatomical\nstructure. On the other hand, we introduce an anatomical normality modeling\nmethod to model the distribution of normal samples for each anatomy, leveraging\nVQ-VAE for reconstructing normal vision embeddings in the latent space. This\nprocess amplifies abnormal signals by leveraging distribution shifts in\nabnormal samples, enhancing the model's perception and discrimination of\nabnormal attributes. The enhanced visual representation effectively captures\nthe diagnostic-relevant semantics, facilitating more efficient and accurate\nalignment with the diagnostic report. We conduct extensive experiments on two\nchest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,\nMedVL-CT69K, and comprehensively evaluate the diagnosis performance across\nmultiple tasks in the chest and abdominal CT scenarios, achieving\nstate-of-the-art zero-shot performance. Notably, our method achieved an average\nAUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing\nmethods. Additionally, we demonstrate the superior transfer learning\ncapabilities of our pre-trained model. Code is available at\nhttps://github.com/alibaba-damo-academy/ViSD-Boost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03742", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01", "section": "cross"}
{"id": "2508.03744", "title": "Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?", "authors": ["Sarah Grube", "SÃ¶ren GrÃ¼nhagen", "Sarah Latus", "Michael Meyling", "Alexander Schlaefer"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03744", "summary": "Estimating the elasticity of soft tissue can provide useful information for\nvarious diagnostic applications. Ultrasound shear wave elastography offers a\nnon-invasive approach. However, its generalizability and standardization across\ndifferent systems and processing pipelines remain limited. Considering the\ninfluence of image processing on ultrasound based diagnostics, recent\nliterature has discussed the impact of different image processing steps on\nreliable and reproducible elasticity analysis. In this work, we investigate the\nneed of ultrasound pre-processing steps for deep learning-based ultrasound\nshear wave elastography. We evaluate the performance of a 3D convolutional\nneural network in predicting shear wave velocities from spatio-temporal\nultrasound images, studying different degrees of pre-processing on the input\nimages, ranging from fully beamformed and filtered ultrasound images to raw\nradiofrequency data. We compare the predictions from our deep learning approach\nto a conventional time-of-flight method across four gelatin phantoms with\ndifferent elasticity levels. Our results demonstrate statistically significant\ndifferences in the predicted shear wave velocity among all elasticity groups,\nregardless of the degree of pre-processing. Although pre-processing slightly\nimproves performance metrics, our results show that the deep learning approach\ncan reliably differentiate between elasticity groups using raw, unprocessed\nradiofrequency data. These results show that deep learning-based approaches\ncould reduce the need for and the bias of traditional ultrasound pre-processing\nsteps in ultrasound shear wave elastography, enabling faster and more reliable\nclinical elasticity assessments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03744", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01", "section": "cross"}
{"id": "2508.03745", "title": "Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Maosheng Hu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03745", "summary": "Recent interest in geospatial artificial intelligence (GeoAI) has fostered a\nwide range of applications using artificial intelligence (AI), especially deep\nlearning, for geospatial problem solving. However, major challenges such as a\nlack of training data and the neglect of spatial principles and spatial effects\nin AI model design remain, significantly hindering the in-depth integration of\nAI with geospatial research. This paper reports our work in developing a deep\nlearning model that enables object detection, particularly of natural features,\nin a weakly supervised manner. Our work makes three contributions: First, we\npresent a method of object detection using only weak labels. This is achieved\nby developing a spatially explicit model based on Tobler's first law of\ngeography. Second, we incorporate attention maps into the object detection\npipeline and develop a multistage training strategy to improve performance.\nThird, we apply this model to detect impact craters on Mars, a task that\npreviously required extensive manual effort. The model generalizes to both\nnatural and human-made features on the surfaces of Earth and other planets.\nThis research advances the theoretical and methodological foundations of GeoAI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03745", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01", "section": "cross"}
{"id": "2508.03747", "title": "Data-Driven Discovery of Mobility Periodicity for Understanding Urban Transportation Systems", "authors": ["Xinyu Chen", "Qi Wang", "Yunhan Zheng", "Nina Cao", "HanQin Cai", "Jinhua Zhao"], "categories": ["cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03747", "summary": "Uncovering the temporal regularity of human mobility is crucial for\ndiscovering urban dynamics and has implications for various decision-making\nprocesses and urban system applications. This study formulates the periodicity\nquantification problem in complex and multidimensional human mobility data as a\nsparse identification of dominant positive auto-correlations in time series\nautoregression, allowing one to discover and quantify significant periodic\npatterns such as weekly periodicity from a data-driven and interpretable\nmachine learning perspective. We apply our framework to real-world human\nmobility data, including metro passenger flow in Hangzhou, China and\nridesharing trips in New York City (NYC) and Chicago, USA, revealing the\ninterpretable weekly periodicity across different spatial locations over past\nseveral years. In particular, our analysis of ridesharing data from 2019 to\n2024 demonstrates the disruptive impact of the COVID-19 pandemic on mobility\nregularity and the subsequent recovery trends, highlighting differences in the\nrecovery pattern percentages and speeds between NYC and Chicago. We explore\nthat both NYC and Chicago experienced a remarkable reduction of weekly\nperiodicity in 2020, and the recovery of mobility regularity in NYC is faster\nthan Chicago. The interpretability of sparse autoregression provides insights\ninto the underlying temporal patterns of human mobility, offering a valuable\ntool for understanding urban systems. Our findings highlight the potential of\ninterpretable machine learning to unlock crucial insights from real-world\nmobility data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03747", "cate": "cs.SI", "date": "2025-08-02", "updated": "2025-08-02", "section": "cross"}
{"id": "2508.03752", "title": "M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation", "authors": ["Yajun Liu", "Zenghui Zhang", "Jiang Yue", "Weiwei Guo", "Dongying Li"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03752", "summary": "Data augmentation methods inspired by CutMix have demonstrated significant\npotential in recent semi-supervised medical image segmentation tasks. However,\nthese approaches often apply CutMix operations in a rigid and inflexible\nmanner, while paying insufficient attention to feature-level consistency\nconstraints. In this paper, we propose a novel method called Mutual Mask Mix\nwith High-Low level feature consistency (M$^3$HL) to address the aforementioned\nchallenges, which consists of two key components: 1) M$^3$: An enhanced data\naugmentation operation inspired by the masking strategy from Masked Image\nModeling (MIM), which advances conventional CutMix through dynamically\nadjustable masks to generate spatially complementary image pairs for\ncollaborative training, thereby enabling effective information fusion between\nlabeled and unlabeled images. 2) HL: A hierarchical consistency regularization\nframework that enforces high-level and low-level feature consistency between\nunlabeled and mixed images, enabling the model to better capture discriminative\nfeature representations.Our method achieves state-of-the-art performance on\nwidely adopted medical image segmentation benchmarks including the ACDC and LA\ndatasets. Source code is available at https://github.com/PHPJava666/M3HL", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03752", "cate": "eess.IV", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03760", "title": "FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication", "authors": ["Qingyuan Li", "Bo Zhang", "Hui Kang", "Tianhao Xu", "Yulei Qian", "Yuchen Xie", "Lin Ma"], "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03760", "summary": "Nowadays, communication bottlenecks have emerged as a critical challenge in\nthe distributed training and deployment of large language models (LLMs). This\npaper introduces FlashCommunication V2, a novel communication paradigm enabling\nefficient cross-GPU transmission at arbitrary bit widths. Its core innovations\nlie in the proposed bit splitting and spike reserving techniques, which address\nthe challenges of low-bit quantization. Bit splitting decomposes irregular bit\nwidths into basic units, ensuring compatibility with hardware capabilities and\nthus enabling transmission at any bit width. Spike reserving, on the other\nhand, retains numerical outliers (i.e., minima and maxima) as floating-point\nnumbers, which shrinks the dynamic numerical range and pushes the quantization\nlimits to 2-bit with acceptable losses. FlashCommunication V2 significantly\nenhances the flexibility and resource utilization of communication systems.\nThrough meticulous software-hardware co-design, it delivers robust performance\nand reduced overhead across both NVLink-based and PCIe-based architectures,\nachieving a maximum 3.2$\\times$ speedup in AllReduce and 2$\\times$ in All2All\ncommunication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03760", "cate": "cs.DC", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03763", "title": "Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment", "authors": ["Ziheng Jia", "Jiaying Qian", "Zicheng Zhang", "Zijian Chen", "Xiongkuo Min"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03763", "summary": "Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training.\nAnalogous to high-level reasoning tasks, RFT is similarly applicable to\nlow-level vision domains, including image quality assessment (IQA). Existing\nRFT-based IQA methods typically use rule-based output rewards to verify the\nmodel's rollouts but provide no reward supervision for the \"think\" process,\nleaving its correctness and efficacy uncontrolled. Furthermore, these methods\ntypically fine-tune directly on downstream IQA tasks without explicitly\nenhancing the model's native low-level visual quality perception, which may\nconstrain its performance upper bound. In response to these gaps, we propose\nthe multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the\nRefine-Perception-20K dataset (with 12 main distortions, 20,907\nlocally-distorted images, and over 55K RFT samples) and design multi-task\nreward functions to strengthen the model's visual quality perception. In\nStage-2, targeting the quality scoring task, we introduce a probability\ndifference reward involved strategy for \"think\" process supervision. The\nresulting Refine-IQA Series Models achieve outstanding performance on both\nperception and scoring tasks-and, notably, our paradigm activates a robust\n\"think\" (quality interpreting) capability that also attains exceptional results\non the corresponding quality interpreting benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03763", "cate": "cs.CV", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03764", "title": "CoughViT: A Self-Supervised Vision Transformer for Cough Audio Representation Learning", "authors": ["Justin Luong", "Hao Xue", "Flora D. Salim"], "categories": ["cs.AI", "cs.SD"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03764", "summary": "Physicians routinely assess respiratory sounds during the diagnostic process,\nproviding insight into the condition of a patient's airways. In recent years,\nAI-based diagnostic systems operating on respiratory sounds, have demonstrated\nsuccess in respiratory disease detection. These systems represent a crucial\nadvancement in early and accessible diagnosis which is essential for timely\ntreatment. However, label and data scarcity remain key challenges, especially\nfor conditions beyond COVID-19, limiting diagnostic performance and reliable\nevaluation. In this paper, we propose CoughViT, a novel pre-training framework\nfor learning general-purpose cough sound representations, to enhance diagnostic\nperformance in tasks with limited data. To address label scarcity, we employ\nmasked data modelling to train a feature encoder in a self-supervised learning\nmanner. We evaluate our approach against other pre-training strategies on three\ndiagnostically important cough classification tasks. Experimental results show\nthat our representations match or exceed current state-of-the-art supervised\naudio representations in enhancing performance on downstream tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03764", "cate": "cs.SD", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03769", "title": "Development of management systems using artificial intelligence systems and machine learning methods for boards of directors (preprint, unofficial translation)", "authors": ["Anna Romanova"], "categories": ["cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03769", "summary": "The study addresses the paradigm shift in corporate management, where AI is\nmoving from a decision support tool to an autonomous decision-maker, with some\nAI systems already appointed to leadership roles in companies. A central\nproblem identified is that the development of AI technologies is far outpacing\nthe creation of adequate legal and ethical guidelines.\n  The research proposes a \"reference model\" for the development and\nimplementation of autonomous AI systems in corporate management. This model is\nbased on a synthesis of several key components to ensure legitimate and ethical\ndecision-making. The model introduces the concept of \"computational law\" or\n\"algorithmic law\". This involves creating a separate legal framework for AI\nsystems, with rules and regulations translated into a machine-readable,\nalgorithmic format to avoid the ambiguity of natural language. The paper\nemphasises the need for a \"dedicated operational context\" for autonomous AI\nsystems, analogous to the \"operational design domain\" for autonomous vehicles.\nThis means creating a specific, clearly defined environment and set of rules\nwithin which the AI can operate safely and effectively. The model advocates for\ntraining AI systems on controlled, synthetically generated data to ensure\nfairness and ethical considerations are embedded from the start. Game theory is\nalso proposed as a method for calculating the optimal strategy for the AI to\nachieve its goals within these ethical and legal constraints. The provided\nanalysis highlights the importance of explainable AI (XAI) to ensure the\ntransparency and accountability of decisions made by autonomous systems. This\nis crucial for building trust and for complying with the \"right to\nexplanation\".", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03769", "cate": "cs.CY", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03771", "title": "Trustworthiness of Legal Considerations for the Use of LLMs in Education", "authors": ["Sara Alaswad", "Tatiana Kalganova", "Wasan Awad"], "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03771", "summary": "As Artificial Intelligence (AI), particularly Large Language Models (LLMs),\nbecomes increasingly embedded in education systems worldwide, ensuring their\nethical, legal, and contextually appropriate deployment has become a critical\npolicy concern. This paper offers a comparative analysis of AI-related\nregulatory and ethical frameworks across key global regions, including the\nEuropean Union, United Kingdom, United States, China, and Gulf Cooperation\nCouncil (GCC) countries. It maps how core trustworthiness principles, such as\ntransparency, fairness, accountability, data privacy, and human oversight are\nembedded in regional legislation and AI governance structures. Special emphasis\nis placed on the evolving landscape in the GCC, where countries are rapidly\nadvancing national AI strategies and education-sector innovation. To support\nthis development, the paper introduces a Compliance-Centered AI Governance\nFramework tailored to the GCC context. This includes a tiered typology and\ninstitutional checklist designed to help regulators, educators, and developers\nalign AI adoption with both international norms and local values. By\nsynthesizing global best practices with region-specific challenges, the paper\ncontributes practical guidance for building legally sound, ethically grounded,\nand culturally sensitive AI systems in education. These insights are intended\nto inform future regulatory harmonization and promote responsible AI\nintegration across diverse educational environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03771", "cate": "cs.CY", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03772", "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03772", "summary": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03772", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03773", "title": "When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection", "authors": ["Emanuele Nardone", "Tiziana D'Alessandro", "Francesco Fontanella", "Claudio De Stefano"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03773", "summary": "Alzheimer's disease detection requires expensive neuroimaging or invasive\nprocedures, limiting accessibility. This study explores whether deep learning\ncan enable non-invasive Alzheimer's disease detection through handwriting\nanalysis. Using a dataset of 34 distinct handwriting tasks collected from\nhealthy controls and Alzheimer's disease patients, we evaluate and compare\nthree recurrent neural architectures (LSTM, GRU, RNN) against traditional\nmachine learning models. A crucial distinction of our approach is that the\nrecurrent models process pre-extracted features from discrete strokes, not raw\ntemporal signals. This violates the assumption of a continuous temporal flow\nthat recurrent networks are designed to capture. Results reveal that they\nexhibit poor specificity and high variance. Traditional ensemble methods\nsignificantly outperform all deep architectures, achieving higher accuracy with\nbalanced metrics. This demonstrates that recurrent architectures, designed for\ncontinuous temporal sequences, fail when applied to feature vectors extracted\nfrom ambiguously segmented strokes. Despite their complexity, deep learning\nmodels cannot overcome the fundamental disconnect between their architectural\nassumptions and the discrete, feature-based nature of stroke-level handwriting\ndata. Although performance is limited, the study highlights several critical\nissues in data representation and model compatibility, pointing to valuable\ndirections for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03773", "cate": "eess.IV", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03774", "title": "U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling", "authors": ["Rui Zhu", "Yuexing Peng", "Peng Wang", "George C. Alexandropoulos", "Wenbo Wang", "Wei Xiang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03774", "summary": "Electromagnetic (EM) scattering modeling is critical for radar remote\nsensing, however, its inherent complexity introduces significant computational\nchallenges. Traditional numerical solvers offer high accuracy, but suffer from\nscalability issues and substantial computational costs. Pure data-driven deep\nlearning approaches, while efficient, lack physical constraints embedding\nduring training and require extensive labeled data, limiting their\napplicability and generalization. To overcome these limitations, we propose a\nU-shaped Physics-Informed Network (U-PINet), the first fully\ndeep-learning-based, physics-informed hierarchical framework for computational\nEM designed to ensure physical consistency while maximizing computational\nefficiency. Motivated by the hierarchical decomposition strategy in EM solvers\nand the inherent sparsity of local EM coupling, the U-PINet models the\ndecomposition and coupling of near- and far-field interactions through a\nmultiscale processing neural network architecture, while employing a\nphysics-inspired sparse graph representation to efficiently model both self-\nand mutual- coupling among mesh elements of complex $3$-Dimensional (3D)\nobjects. This principled approach enables end-to-end multiscale EM scattering\nmodeling with improved efficiency, generalization, and physical consistency.\nExperimental results showcase that the U-PINet accurately predicts surface\ncurrent distributions, achieving close agreement with traditional solver, while\nsignificantly reducing computational time and outperforming conventional deep\nlearning baselines in both accuracy and robustness. Furthermore, our\nevaluations on radar cross section prediction tasks confirm the feasibility of\nthe U-PINet for downstream EM scattering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03774", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03775", "title": "4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis", "authors": ["Mingyu Liu", "Zian Mao", "Zhu Liu", "Haoran Zhang", "Jintao Guo", "Xiaoya He", "Xi Huang", "Shufen Chu", "Chun Cheng", "Jun Ding", "Yujun Xie"], "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03775", "summary": "Automated experimentation with real time data analysis in scanning\ntransmission electron microscopy (STEM) often require end-to-end framework. The\nfour-dimensional scanning transmission electron microscopy (4D-STEM) with\nhigh-throughput data acquisition has been constrained by the critical\nbottleneck results from data preprocessing. Pervasive noise, beam center drift,\nand elliptical distortions during high-throughput acquisition inevitably\ncorrupt diffraction patterns, systematically biasing quantitative measurements.\nYet, conventional correction algorithms are often material-specific and fail to\nprovide a robust, generalizable solution. In this work, we present 4D-PreNet,\nan end-to-end deep-learning pipeline that integrates attention-enhanced U-Net\nand ResNet architectures to simultaneously perform denoising, center\ncorrection, and elliptical distortion calibration. The network is trained on\nlarge, simulated datasets encompassing a wide range of noise levels, drift\nmagnitudes, and distortion types, enabling it to generalize effectively to\nexperimental data acquired under varying conditions. Quantitative evaluations\ndemonstrate that our pipeline reduces mean squared error by up to 50% during\ndenoising and achieves sub-pixel center localization in the center detection\ntask, with average errors below 0.04 pixels. The outputs are bench-marked\nagainst traditional algorithms, highlighting improvements in both noise\nsuppression and restoration of diffraction patterns, thereby facilitating\nhigh-throughput, reliable 4D-STEM real-time analysis for automated\ncharacterization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03775", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03776", "title": "Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network", "authors": ["Xiao Wang", "Zikang Yan", "Hao Si", "Zhendong Yang", "Qingquan Yang", "Dengdi Sun", "Wanli Lyu", "Jin Tang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03776", "summary": "Estimating heat flux in the nuclear fusion device EAST is a critically\nimportant task. Traditional scientific computing methods typically model this\nprocess using the Finite Element Method (FEM). However, FEM relies on\ngrid-based sampling for computation, which is computationally inefficient and\nhard to perform real-time simulations during actual experiments. Inspired by\nartificial intelligence-powered scientific computing, this paper proposes a\nnovel Physics-Informed Neural Network (PINN) to address this challenge,\nsignificantly accelerating the heat conduction estimation process while\nmaintaining high accuracy. Specifically, given inputs of different materials,\nwe first feed spatial coordinates and time stamps into the neural network, and\ncompute boundary loss, initial condition loss, and physical loss based on the\nheat conduction equation. Additionally, we sample a small number of data points\nin a data-driven manner to better fit the specific heat conduction scenario,\nfurther enhancing the model's predictive capability. We conduct experiments\nunder both uniform and non-uniform heating conditions on the top surface.\nExperimental results show that the proposed thermal conduction physics-informed\nneural network achieves accuracy comparable to the finite element method, while\nachieving $\\times$40 times acceleration in computational efficiency. The\ndataset and source code will be released on\nhttps://github.com/Event-AHU/OpenFusion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03776", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03777", "title": "When Agents Break Down in Multiagent Path Finding", "authors": ["Foivos Fioravantes", "DuÅ¡an Knop", "Nikolaos Melissinos", "Michal Opler"], "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.MA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03777", "summary": "In Multiagent Path Finding (MAPF), the goal is to compute efficient,\ncollision-free paths for multiple agents navigating a network from their\nsources to targets, minimizing the schedule's makespan-the total time until all\nagents reach their destinations. We introduce a new variant that formally\nmodels scenarios where some agents may experience delays due to malfunctions,\nposing significant challenges for maintaining optimal schedules.\n  Recomputing an entirely new schedule from scratch after each malfunction is\noften computationally infeasible. To address this, we propose a framework for\ndynamic schedule adaptation that does not rely on full replanning. Instead, we\ndevelop protocols enabling agents to locally coordinate and adjust their paths\non the fly. We prove that following our primary communication protocol, the\nincrease in makespan after k malfunctions is bounded by k additional turns,\neffectively limiting the impact of malfunctions on overall efficiency.\nMoreover, recognizing that agents may have limited computational capabilities,\nwe also present a secondary protocol that shifts the necessary computations\nonto the network's nodes, ensuring robustness without requiring enhanced agent\nprocessing power. Our results demonstrate that these protocols provide a\npractical, scalable approach to resilient multiagent navigation in the face of\nagent failures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03777", "cate": "cs.MA", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03780", "title": "Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition", "authors": ["Katharina Hoedt", "Arthur Flexer", "Gerhard Widmer"], "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03780", "summary": "One of the desired key properties of deep learning models is the ability to\ngeneralise to unseen samples. When provided with new samples that are\n(perceptually) similar to one or more training samples, deep learning models\nare expected to produce correspondingly similar outputs. Models that succeed in\npredicting similar outputs for similar inputs are often called robust. Deep\nlearning models, on the other hand, have been shown to be highly vulnerable to\nminor (adversarial) perturbations of the input, which manage to drastically\nchange a model's output and simultaneously expose its reliance on spurious\ncorrelations. In this work, we investigate whether inherently interpretable\ndeep models, i.e., deep models that were designed to focus more on meaningful\nand interpretable features, are more robust to irrelevant perturbations in the\ndata, compared to their black-box counterparts. We test our hypothesis by\ncomparing the robustness of an interpretable and a black-box music emotion\nrecognition (MER) model when challenged with adversarial examples. Furthermore,\nwe include an adversarially trained model, which is optimised to be more\nrobust, in the comparison. Our results indicate that inherently more\ninterpretable models can indeed be more robust than their black-box\ncounterparts, and achieve similar levels of robustness as adversarially trained\nmodels, at lower computational cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03780", "cate": "cs.SD", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03782", "title": "Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM", "authors": ["Ryota Ikeda"], "categories": ["cs.AI", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03782", "summary": "The performance of decoders in Quantum Error Correction (QEC) is key to\nrealizing practical quantum computers. In recent years, Graph Neural Networks\n(GNNs) have emerged as a promising approach, but their training methodologies\nare not yet well-established. It is generally expected that transferring\ntheoretical knowledge from classical algorithms like Minimum Weight Perfect\nMatching (MWPM) to GNNs, a technique known as knowledge distillation, can\neffectively improve performance. In this work, we test this hypothesis by\nrigorously comparing two models based on a Graph Attention Network (GAT)\narchitecture that incorporates temporal information as node features. The first\nis a purely data-driven model (baseline) trained only on ground-truth labels,\nwhile the second incorporates a knowledge distillation loss based on the\ntheoretical error probabilities from MWPM. Using public experimental data from\nGoogle, our evaluation reveals that while the final test accuracy of the\nknowledge distillation model was nearly identical to the baseline, its training\nloss converged more slowly, and the training time increased by a factor of\napproximately five. This result suggests that modern GNN architectures possess\na high capacity to efficiently learn complex error correlations directly from\nreal hardware data, without guidance from approximate theoretical models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03782", "cate": "quant-ph", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03783", "title": "Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning", "authors": ["Ryota Ikeda"], "categories": ["cs.AI", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03783", "summary": "Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approach\nfor Quantum Error Correction (QEC) decoding, capable of learning complex noise\ncharacteristics directly from syndrome data. However, the robustness of these\ndecoders against subtle, adversarial perturbations remains a critical open\nquestion. This work introduces a novel framework to systematically probe the\nvulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. The\nRL agent is trained as an adversary with the goal of finding minimal syndrome\nmodifications that cause the decoder to misclassify. We apply this framework to\na Graph Attention Network (GAT) decoder trained on experimental surface code\ndata from Google Quantum AI. Our results show that the RL agent can\nsuccessfully identify specific, critical vulnerabilities, achieving a high\nattack success rate with a minimal number of bit flips. Furthermore, we\ndemonstrate that the decoder's robustness can be significantly enhanced through\nadversarial training, where the model is retrained on the adversarial examples\ngenerated by the RL agent. This iterative process of automated vulnerability\ndiscovery and targeted retraining presents a promising methodology for\ndeveloping more reliable and robust neural network decoders for fault-tolerant\nquantum computing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03783", "cate": "quant-ph", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03785", "title": "SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons", "authors": ["Teodor Chiaburu", "Vipin Singh", "Frank HauÃer", "Felix BieÃmann"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03785", "summary": "While recent advances in foundation models have improved the state of the art\nin many domains, some problems in empirical sciences could not benefit from\nthis progress yet. Soil horizon classification, for instance, remains\nchallenging because of its multimodal and multitask characteristics and a\ncomplex hierarchically structured label taxonomy. Accurate classification of\nsoil horizons is crucial for monitoring soil health, which directly impacts\nagricultural productivity, food security, ecosystem stability and climate\nresilience. In this work, we propose $\\textit{SoilNet}$ - a multimodal\nmultitask model to tackle this problem through a structured modularized\npipeline. Our approach integrates image data and geotemporal metadata to first\npredict depth markers, segmenting the soil profile into horizon candidates.\nEach segment is characterized by a set of horizon-specific morphological\nfeatures. Finally, horizon labels are predicted based on the multimodal\nconcatenated feature vector, leveraging a graph-based label representation to\naccount for the complex hierarchical relationships among soil horizons. Our\nmethod is designed to address complex hierarchical classification, where the\nnumber of possible labels is very large, imbalanced and non-trivially\nstructured. We demonstrate the effectiveness of our approach on a real-world\nsoil profile dataset. All code and experiments can be found in our repository:\nhttps://github.com/calgo-lab/BGR/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03785", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03818", "title": "Mechanism Design for Facility Location using Predictions", "authors": ["Toby Walsh"], "categories": ["cs.AI", "cs.GT"], "primary_category": "cs.GT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03818", "summary": "We study mechanisms for the facility location problem augmented with\npredictions of the optimal facility location. We demonstrate that an\negalitarian viewpoint which considers both the maximum distance of any agent\nfrom the facility and the minimum utility of any agent provides important new\ninsights compared to a viewpoint that just considers the maximum distance. As\nin previous studies, we consider performance in terms of consistency (worst\ncase when predictions are accurate) and robustness (worst case irrespective of\nthe accuracy of predictions). By considering how mechanisms with predictions\ncan perform poorly, we design new mechanisms that are more robust. Indeed, by\nadjusting parameters, we demonstrate how to trade robustness for consistency.\nWe go beyond the single facility problem by designing novel strategy proof\nmechanisms for locating two facilities with bounded consistency and robustness\nthat use two predictions for where to locate the two facilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03818", "cate": "cs.GT", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03839", "title": "VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations", "authors": ["Yifei Zong", "Alexandre M. Tartakovsky"], "categories": ["cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03839", "summary": "We propose a trainable-by-parts surrogate model for solving forward and\ninverse parameterized nonlinear partial differential equations. Like several\nother surrogate and operator learning models, the proposed approach employs an\nencoder to reduce the high-dimensional input $y(\\bm{x})$ to a lower-dimensional\nlatent space, $\\bm\\mu_{\\bm\\phi_y}$. Then, a fully connected neural network is\nused to map $\\bm\\mu_{\\bm\\phi_y}$ to the latent space, $\\bm\\mu_{\\bm\\phi_h}$, of\nthe PDE solution $h(\\bm{x},t)$. Finally, a decoder is utilized to reconstruct\n$h(\\bm{x},t)$. The innovative aspect of our model is its ability to train its\nthree components independently. This approach leads to a substantial decrease\nin both the time and energy required for training when compared to leading\noperator learning models such as FNO and DeepONet. The separable training is\nachieved by training the encoder as part of the variational autoencoder (VAE)\nfor $y(\\bm{x})$ and the decoder as part of the $h(\\bm{x},t)$ VAE. We refer to\nthis model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet\nmodels for obtaining forward and inverse solutions to the nonlinear diffusion\nequation governing groundwater flow in an unconfined aquifer. Our findings\nindicate that VAE-DNN not only demonstrates greater efficiency but also\ndelivers superior accuracy in both forward and inverse solutions compared to\nthe FNO and DeepONet models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03839", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03860", "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03860", "summary": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03860", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03872", "title": "Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training", "authors": ["Wesley Brewer", "Murali Meena Gopalakrishnan", "Matthias Maiterth", "Aditya Kashi", "Jong Youl Choi", "Pei Zhang", "Stephen Nichols", "Riccardo Balin", "Miles Couchman", "Stephen de Bruyn Kops", "P.K. Yeung", "Daniel Dotson", "Rohini Uma-Vaideswaran", "Sarp Oral", "Feiyi Wang"], "categories": ["cs.AI", "cs.DC", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03872", "summary": "With the end of Moore's law and Dennard scaling, efficient training\nincreasingly requires rethinking data volume. Can we train better models with\nsignificantly less data via intelligent subsampling? To explore this, we\ndevelop SICKLE, a sparse intelligent curation framework for efficient learning,\nfeaturing a novel maximum entropy (MaxEnt) sampling approach, scalable\ntraining, and energy benchmarking. We compare MaxEnt with random and\nphase-space sampling on large direct numerical simulation (DNS) datasets of\nturbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as\na preprocessing step can improve model accuracy and substantially lower energy\nconsumption, with reductions of up to 38x observed in certain cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03872", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03882", "title": "Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)", "authors": ["Arturo SÃ¡nchez-Matas", "Pablo Escribano Ruiz", "Daniel DÃ­az-LÃ³pez", "Angel Luis Perales GÃ³mez", "Pantaleone Nespoli", "Gregorio MartÃ­nez PÃ©rez"], "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03882", "summary": "In today digital landscape, organizations face constantly evolving cyber\nthreats, making it essential to discover slippery attack vectors through novel\ntechniques like Security Chaos Engineering (SCE), which allows teams to test\ndefenses and identify vulnerabilities effectively. This paper proposes to\nintegrate SCE into Breach Attack Simulation (BAS) platforms, leveraging\nadversary profiles and abilities from existing threat intelligence databases.\nThis innovative proposal for cyberattack simulation employs a structured\narchitecture composed of three layers: SCE Orchestrator, Connector, and BAS\nlayers. Utilizing MITRE Caldera in the BAS layer, our proposal executes\nautomated attack sequences, creating inferred attack trees from adversary\nprofiles. Our proposal evaluation illustrates how integrating SCE with BAS can\nenhance the effectiveness of attack simulations beyond traditional scenarios,\nand be a useful component of a cyber defense strategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03882", "cate": "cs.CR", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03898", "title": "Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning", "authors": ["William Solow", "Sandhya Saisubramanian"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03898", "summary": "Accurate prediction of grape phenology is essential for timely vineyard\nmanagement decisions, such as scheduling irrigation and fertilization, to\nmaximize crop yield and quality. While traditional biophysical models\ncalibrated on historical field data can be used for season-long predictions,\nthey lack the precision required for fine-grained vineyard management. Deep\nlearning methods are a compelling alternative but their performance is hindered\nby sparse phenology datasets, particularly at the cultivar level. We propose a\nhybrid modeling approach that combines multi-task learning with a recurrent\nneural network to parameterize a differentiable biophysical model. By using\nmulti-task learning to predict the parameters of the biophysical model, our\napproach enables shared learning across cultivars while preserving biological\nstructure, thereby improving the robustness and accuracy of predictions.\nEmpirical evaluation using real-world and synthetic datasets demonstrates that\nour method significantly outperforms both conventional biophysical models and\nbaseline deep learning approaches in predicting phenological stages, as well as\nother crop state variables such as cold-hardiness and wheat yield.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03898", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03913", "title": "Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures", "authors": ["Florian Bley", "Jacob Kauffmann", "Simon LeÃ³n Krug", "Klaus-Robert MÃ¼ller", "GrÃ©goire Montavon"], "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03913", "summary": "Distance-based classifiers, such as k-nearest neighbors and support vector\nmachines, continue to be a workhorse of machine learning, widely used in\nscience and industry. In practice, to derive insights from these models, it is\nalso important to ensure that their predictions are explainable. While the\nfield of Explainable AI has supplied methods that are in principle applicable\nto any model, it has also emphasized the usefulness of latent structures (e.g.\nthe sequence of layers in a neural network) to produce explanations. In this\npaper, we contribute by uncovering a hidden neural network structure in\ndistance-based classifiers (consisting of linear detection units combined with\nnonlinear pooling layers) upon which Explainable AI techniques such as\nlayer-wise relevance propagation (LRP) become applicable. Through quantitative\nevaluations, we demonstrate the advantage of our novel explanation approach\nover several baselines. We also show the overall usefulness of explaining\ndistance-based models through two practical use cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03913", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03920", "title": "Deep learning framework for crater detection and identification on the Moon and Mars", "authors": ["Yihan Ma", "Zeyang Yu", "Rohitash Chandra"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03920", "summary": "Impact craters are among the most prominent geomorphological features on\nplanetary surfaces and are of substantial significance in planetary science\nresearch. Their spatial distribution and morphological characteristics provide\ncritical information on planetary surface composition, geological history, and\nimpact processes. In recent years, the rapid advancement of deep learning\nmodels has fostered significant interest in automated crater detection. In this\npaper, we apply advancements in deep learning models for impact crater\ndetection and identification. We use novel models, including Convolutional\nNeural Networks (CNNs) and variants such as YOLO and ResNet. We present a\nframework that features a two-stage approach where the first stage features\ncrater identification using simple classic CNN, ResNet-50 and YOLO. In the\nsecond stage, our framework employs YOLO-based detection for crater\nlocalisation. Therefore, we detect and identify different types of craters and\npresent a summary report with remote sensing data for a selected region. We\nconsider selected regions for craters and identification from Mars and the Moon\nbased on remote sensing data. Our results indicate that YOLO demonstrates the\nmost balanced crater detection performance, while ResNet-50 excels in\nidentifying large craters with high precision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03920", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03921", "title": "Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data", "authors": ["John D. Kelleher", "Matthew Nicholson", "Rahul Agrahari", "Clare Conran"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03921", "summary": "This paper examines the effectiveness of combining active learning and\ntransfer learning for anomaly detection in cross-domain time-series data. Our\nresults indicate that there is an interaction between clustering and active\nlearning and in general the best performance is achieved using a single cluster\n(in other words when clustering is not applied). Also, we find that adding new\nsamples to the training set using active learning does improve model\nperformance but that in general, the rate of improvement is slower than the\nresults reported in the literature suggest. We attribute this difference to an\nimproved experimental design where distinct data samples are used for the\nsampling and testing pools. Finally, we assess the ceiling performance of\ntransfer learning in combination with active learning across several datasets\nand find that performance does initially improve but eventually begins to tail\noff as more target points are selected for inclusion in training. This tail-off\nin performance may indicate that the active learning process is doing a good\njob of sequencing data points for selection, pushing the less useful points\ntowards the end of the selection process and that this tail-off occurs when\nthese less useful points are eventually added. Taken together our results\nindicate that active learning is effective but that the improvement in model\nperformance follows a linear flat function concerning the number of points\nselected and labelled.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03921", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03940", "title": "FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport", "authors": ["Pengxi Liu", "Yi Shen", "Matthew M. Engelhard", "Benjamin A. Goldstein", "Michael J. Pencina", "Nicoleta J. Economou-Zavlanos", "Michael M. Zavlanos"], "categories": ["cs.AI", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03940", "summary": "Fairness metrics utilizing the area under the receiver operator\ncharacteristic curve (AUC) have gained increasing attention in high-stakes\ndomains such as healthcare, finance, and criminal justice. In these domains,\nfairness is often evaluated over risk scores rather than binary outcomes, and a\ncommon challenge is that enforcing strict fairness can significantly degrade\nAUC performance. To address this challenge, we propose Fair Proportional\nOptimal Transport (FairPOT), a novel, model-agnostic post-processing framework\nthat strategically aligns risk score distributions across different groups\nusing optimal transport, but does so selectively by transforming a controllable\nproportion, i.e., the top-lambda quantile, of scores within the disadvantaged\ngroup. By varying lambda, our method allows for a tunable trade-off between\nreducing AUC disparities and maintaining overall AUC performance. Furthermore,\nwe extend FairPOT to the partial AUC setting, enabling fairness interventions\nto concentrate on the highest-risk regions. Extensive experiments on synthetic,\npublic, and clinical datasets show that FairPOT consistently outperforms\nexisting post-processing techniques in both global and partial AUC scenarios,\noften achieving improved fairness with slight AUC degradation or even positive\ngains in utility. The computational efficiency and practical adaptability of\nFairPOT make it a promising solution for real-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03940", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03944", "title": "Constraint-Preserving Data Generation for Visuomotor Policy Learning", "authors": ["Kevin Lin", "Varun Ragunath", "Andrew McAlinden", "Aaditya Prasad", "Jimmy Wu", "Yuke Zhu", "Jeannette Bohg"], "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03944", "summary": "Large-scale demonstration data has powered key breakthroughs in robot\nmanipulation, but collecting that data remains costly and time-consuming. We\npresent Constraint-Preserving Data Generation (CP-Gen), a method that uses a\nsingle expert trajectory to generate robot demonstrations containing novel\nobject geometries and poses. These generated demonstrations are used to train\nclosed-loop visuomotor policies that transfer zero-shot to the real world and\ngeneralize across variations in object geometries and poses. Similar to prior\nwork using pose variations for data generation, CP-Gen first decomposes expert\ndemonstrations into free-space motions and robot skills. But unlike those\nworks, we achieve geometry-aware data generation by formulating robot skills as\nkeypoint-trajectory constraints: keypoints on the robot or grasped object must\ntrack a reference trajectory defined relative to a task-relevant object. To\ngenerate a new demonstration, CP-Gen samples pose and geometry transforms for\neach task-relevant object, then applies these transforms to the object and its\nassociated keypoints or keypoint trajectories. We optimize robot joint\nconfigurations so that the keypoints on the robot or grasped object track the\ntransformed keypoint trajectory, and then motion plan a collision-free path to\nthe first optimized joint configuration. Experiments on 16 simulation tasks and\nfour real-world tasks, featuring multi-stage, non-prehensile and\ntight-tolerance manipulation, show that policies trained using CP-Gen achieve\nan average success rate of 77%, outperforming the best baseline that achieves\nan average of 50%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03944", "cate": "cs.RO", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03953", "title": "Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation", "authors": ["Xiangcen Wu", "Shaheer U. Saeed", "Yipei Wang", "Ester Bonmati Coll", "Yipeng Hu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03953", "summary": "Radiologists often mix medical image reading strategies, including inspection\nof individual modalities and local image regions, using information at\ndifferent locations from different images independently as well as\nconcurrently. In this paper, we propose a recommend system to assist machine\nlearning-based segmentation models, by suggesting appropriate image portions\nalong with the best modality, such that prostate cancer segmentation\nperformance can be maximised. Our approach trains a policy network that assists\ntumor localisation, by recommending both the optimal imaging modality and the\nspecific sections of interest for review. During training, a pre-trained\nsegmentation network mimics radiologist inspection on individual or variable\ncombinations of these imaging modalities and their sections - selected by the\npolicy network. Taking the locally segmented regions as an input for the next\nstep, this dynamic decision making process iterates until all cancers are best\nlocalised. We validate our method using a data set of 1325 labelled\nmultiparametric MRI images from prostate cancer patients, demonstrating its\npotential to improve annotation efficiency and segmentation accuracy,\nespecially when challenging pathology is present. Experimental results show\nthat our approach can surpass standard segmentation networks. Perhaps more\ninterestingly, our trained agent independently developed its own optimal\nstrategy, which may or may not be consistent with current radiologist\nguidelines such as PI-RADS. This observation also suggests a promising\ninteractive application, in which the proposed policy networks assist human\nradiologists.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03953", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03962", "title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Dionysis Diamantis", "Christos Tryfonopoulos", "Thanasis Vergoulis"], "categories": ["cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.DL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03962", "summary": "The growing volume of scientific literature makes it challenging for\nscientists to move from a list of papers to a synthesized understanding of a\ntopic. Because of the constant influx of new papers on a daily basis, even if a\nscientist identifies a promising set of papers, they still face the tedious\ntask of individually reading through dozens of titles and abstracts to make\nsense of occasionally conflicting findings. To address this critical bottleneck\nin the research workflow, we introduce a summarization feature to BIP! Finder,\na scholarly search engine that ranks literature based on distinct impact\naspects like popularity and influence. Our approach enables users to generate\ntwo types of summaries from top-ranked search results: a concise summary for an\ninstantaneous at-a-glance comprehension and a more comprehensive literature\nreview-style summary for greater, better-organized comprehension. This ability\ndynamically leverages BIP! Finder's already existing impact-based ranking and\nfiltering features to generate context-sensitive, synthesized narratives that\ncan significantly accelerate literature discovery and comprehension.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03962", "cate": "cs.DL", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03969", "title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective", "authors": ["Wei Xu"], "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03969", "summary": "This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03969", "cate": "cs.HC", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03970", "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03970", "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03970", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03989", "title": "Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework", "authors": ["Ajesh Koyatan Chathoth", "Shuhao Yu", "Stephen Lee"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03989", "summary": "User-controllable privacy is important in modern sensing systems, as privacy\npreferences can vary significantly from person to person and may evolve over\ntime. This is especially relevant in devices equipped with Inertial Measurement\nUnit (IMU) sensors, such as smartphones and wearables, which continuously\ncollect rich time-series data that can inadvertently expose sensitive user\nbehaviors. While prior work has proposed privacy-preserving methods for sensor\ndata, most rely on static, predefined privacy labels or require large\nquantities of private training data, limiting their adaptability and user\nagency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,\nfew-shot privacy-preserving sensing framework. PrivCLIP allows users to specify\nand modify their privacy preferences by categorizing activities as sensitive\n(black-listed), non-sensitive (white-listed), or neutral (gray-listed).\nLeveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU\nsensor data with natural language activity descriptions in a shared embedding\nspace, enabling few-shot detection of sensitive activities. When a\nprivacy-sensitive activity is identified, the system uses a language-guided\nactivity sanitizer and a motion generation module (IMU-GPT) to transform the\noriginal data into a privacy-compliant version that semantically resembles a\nnon-sensitive activity. We evaluate PrivCLIP on multiple human activity\nrecognition datasets and demonstrate that it significantly outperforms baseline\nmethods in terms of both privacy protection and data utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03989", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.03990", "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03990", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03990", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04010", "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization", "authors": ["Yurun Chen", "Xavier Hu", "Yuhan Liu", "Keting Yin", "Juncheng Li", "Zhuosheng Zhang", "Shengyu Zhang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04010", "summary": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04010", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04011", "title": "StepWrite: Adaptive Planning for Speech-Driven Text Generation", "authors": ["Hamza El Alaoui", "Atieh Taheri", "Yi-Hao Peng", "Jeffrey P. Bigham"], "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04011", "summary": "People frequently use speech-to-text systems to compose short texts with\nvoice. However, current voice-based interfaces struggle to support composing\nmore detailed, contextually complex texts, especially in scenarios where users\nare on the move and cannot visually track progress. Longer-form communication,\nsuch as composing structured emails or thoughtful responses, requires\npersistent context tracking, structured guidance, and adaptability to evolving\nuser intentions--capabilities that conventional dictation tools and voice\nassistants do not support. We introduce StepWrite, a large language\nmodel-driven voice-based interaction system that augments human writing ability\nby enabling structured, hands-free and eyes-free composition of longer-form\ntexts while on the move. StepWrite decomposes the writing process into\nmanageable subtasks and sequentially guides users with contextually-aware\nnon-visual audio prompts. StepWrite reduces cognitive load by offloading the\ncontext-tracking and adaptive planning tasks to the models. Unlike baseline\nmethods like standard dictation features (e.g., Microsoft Word) and\nconversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite\ndynamically adapts its prompts based on the evolving context and user intent,\nand provides coherent guidance without compromising user autonomy. An empirical\nevaluation with 25 participants engaging in mobile or stationary hands-occupied\nactivities demonstrated that StepWrite significantly reduces cognitive load,\nimproves usability and user satisfaction compared to baseline methods.\nTechnical evaluations further confirmed StepWrite's capability in dynamic\ncontextual prompt generation, accurate tone alignment, and effective fact\nchecking. This work highlights the potential of structured, context-aware voice\ninteractions in enhancing hands-free and eye-free communication in everyday\nmultitasking scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04011", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04012", "title": "Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing", "authors": ["Xiaopeng Li", "Shasha Li", "Xi Wang", "Shezheng Song", "Bin Ji", "Shangwen Wang", "Jun Ma", "Xiaodong Liu", "Mina Liu", "Jie Yu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04012", "summary": "Large Language Models (LLMs) underpin many AI applications, but their static\nnature makes updating knowledge costly. Model editing offers an efficient\nalternative by injecting new information through targeted parameter\nmodifications. In particular, meta-learning-based model editing (MLBME) methods\nhave demonstrated notable advantages in both editing effectiveness and\nefficiency. Despite this, we find that MLBME exhibits suboptimal performance in\nlow-data scenarios, and its training efficiency is bottlenecked by the\ncomputation of KL divergence. To address these, we propose $\\textbf{S}$tep\n$\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that\nadopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation\n$\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited\nsupervision and a norm regularization on weight updates to improve training\nefficiency. Experimental results on two datasets and two LLMs demonstrate that\nSMEdit outperforms prior MLBME baselines and the MBPS strategy can be\nseamlessly integrated into existing methods to further boost their performance.\nOur code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04012", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04024", "title": "Identity Theft in AI Conference Peer Review", "authors": ["Nihar B. Shah", "Melisa Bok", "Xukun Liu", "Andrew McCallum"], "categories": ["cs.AI", "cs.CR", "cs.DL"], "primary_category": "cs.DL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04024", "summary": "We discuss newly uncovered cases of identity theft in the scientific\npeer-review process within artificial intelligence (AI) research, with broader\nimplications for other academic procedures. We detail how dishonest researchers\nexploit the peer-review system by creating fraudulent reviewer profiles to\nmanipulate paper evaluations, leveraging weaknesses in reviewer recruitment\nworkflows and identity verification processes. The findings highlight the\ncritical need for stronger safeguards against identity theft in peer review and\nacademia at large, and to this end, we also propose mitigating strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04024", "cate": "cs.DL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04032", "title": "Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models", "authors": ["Qian Yong", "Yanhui Li", "Jialiang Shi", "Yaguang Dou", "Tian Qi"], "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04032", "summary": "The feedback loop in industrial recommendation systems reinforces homogeneous\ncontent, creates filter bubble effects, and diminishes user satisfaction.\nRecently, large language models(LLMs) have demonstrated potential in\nserendipity recommendation, thanks to their extensive world knowledge and\nsuperior reasoning capabilities. However, these models still face challenges in\nensuring the rationality of the reasoning process, the usefulness of the\nreasoning results, and meeting the latency requirements of industrial\nrecommendation systems (RSs). To address these challenges, we propose a method\nthat leverages llm to dynamically construct user knowledge graphs, thereby\nenhancing the serendipity of recommendation systems. This method comprises a\ntwo stage framework:(1) two-hop interest reasoning, where user static profiles\nand historical behaviors are utilized to dynamically construct user knowledge\ngraphs via llm. Two-hop reasoning, which can enhance the quality and accuracy\nof LLM reasoning results, is then performed on the constructed graphs to\nidentify users' potential interests; and(2) Near-line adaptation, a\ncost-effective approach to deploying the aforementioned models in industrial\nrecommendation systems. We propose a u2i (user-to-item) retrieval model that\nalso incorporates i2i (item-to-item) retrieval capabilities, the retrieved\nitems not only exhibit strong relevance to users' newly emerged interests but\nalso retain the high conversion rate of traditional u2i retrieval. Our online\nexperiments on the Dewu app, which has tens of millions of users, indicate that\nthe method increased the exposure novelty rate by 4.62%, the click novelty rate\nby 4.85%, the average view duration per person by 0.15%, unique visitor click\nthrough rate by 0.07%, and unique visitor interaction penetration by 0.30%,\nenhancing user experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04032", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04035", "title": "A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs", "authors": ["Zakariya Ba Alawi"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04035", "summary": "This paper presents a comprehensive comparative survey of TensorFlow and\nPyTorch, the two leading deep learning frameworks, focusing on their usability,\nperformance, and deployment trade-offs. We review each framework's programming\nparadigm and developer experience, contrasting TensorFlow's graph-based (now\noptionally eager) approach with PyTorch's dynamic, Pythonic style. We then\ncompare model training speeds and inference performance across multiple tasks\nand data regimes, drawing on recent benchmarks and studies. Deployment\nflexibility is examined in depth - from TensorFlow's mature ecosystem\n(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript\nsupport) to PyTorch's newer production tools (TorchScript compilation, ONNX\nexport, and TorchServe). We also survey ecosystem and community support,\nincluding library integrations, industry adoption, and research trends (e.g.,\nPyTorch's dominance in recent research publications versus TensorFlow's broader\ntooling in enterprise). Applications in computer vision, natural language\nprocessing, and other domains are discussed to illustrate how each framework is\nused in practice. Finally, we outline future directions and open challenges in\ndeep learning framework design, such as unifying eager and graph execution,\nimproving cross-framework interoperability, and integrating compiler\noptimizations (XLA, JIT) for improved speed. Our findings indicate that while\nboth frameworks are highly capable for state-of-the-art deep learning, they\nexhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored\nin research, whereas TensorFlow provides a fuller production-ready ecosystem -\nunderstanding these trade-offs is key for practitioners selecting the\nappropriate tool. We include charts, code snippets, and more than 20 references\nto academic papers and official documentation to support this comparative\nanalysis", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04035", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04036", "title": "CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion", "authors": ["Trinh Quoc Nguyen", "Oky Dicky Ardiansyah Prima", "Syahid Al Irfan", "Hindriyanto Dwi Purnomo", "Radius Tanone"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04036", "summary": "This study presents CORE-ReID V2, an enhanced framework building upon\nCORE-ReID. The new framework extends its predecessor by addressing Unsupervised\nDomain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with\nfurther applicability to Object ReID. During pre-training, CycleGAN is employed\nto synthesize diverse data, bridging image characteristic gaps across different\ndomains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting\nof the Efficient Channel Attention Block (ECAB) and the Simplified Efficient\nChannel Attention Block (SECAB), enhances both local and global feature\nrepresentations while reducing ambiguity in pseudo-labels for target samples.\nExperimental results on widely used UDA Person ReID and Vehicle ReID datasets\ndemonstrate that the proposed framework outperforms state-of-the-art methods,\nachieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy\n(Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones\nsuch as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our\nwork not only pushes the boundaries of UDA-based Object ReID but also provides\na solid foundation for further research and advancements in this domain. Our\ncodes and models are available at\nhttps://github.com/TrinhQuocNguyen/CORE-ReID-V2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04036", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04039", "title": "Large Reasoning Models Are Autonomous Jailbreak Agents", "authors": ["Thilo Hagendorff", "Erik Derner", "Nuria Oliver"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04039", "summary": "Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has\ntraditionally required complex technical procedures or specialized human\nexpertise. In this study, we show that the persuasive capabilities of large\nreasoning models (LRMs) simplify and scale jailbreaking, converting it into an\ninexpensive activity accessible to non-experts. We evaluated the capabilities\nof four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as\nautonomous adversaries conducting multi-turn conversations with nine widely\nused target models. LRMs received instructions via a system prompt, before\nproceeding to planning and executing jailbreaks with no further supervision. We\nperformed extensive experiments with a benchmark of harmful prompts composed of\n70 items covering seven sensitive domains. This setup yielded an overall attack\nsuccess rate across all model combinations of 97.14%. Our study reveals an\nalignment regression, in which LRMs can systematically erode the safety\nguardrails of other models, highlighting the urgent need to further align\nfrontier models not only to resist jailbreak attempts, but also to prevent them\nfrom being co-opted into acting as jailbreak agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04039", "cate": "cs.CL", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.04064", "title": "FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning", "authors": ["Tuan Nguyen", "Khoa D Doan", "Kok-Seng Wong"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04064", "summary": "Federated learning (FL) is vulnerable to backdoor attacks, yet most existing\nmethods are limited by fixed-pattern or single-target triggers, making them\ninflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),\na novel backdoor attack that leverages a latent-driven conditional autoencoder\nto generate diverse, target-specific triggers as needed. By introducing a\nlatent code, FLAT enables the creation of visually adaptive and highly variable\ntriggers, allowing attackers to select arbitrary targets without retraining and\nto evade conventional detection mechanisms. Our approach unifies attack\nsuccess, stealth, and diversity within a single framework, introducing a new\nlevel of flexibility and sophistication to backdoor attacks in FL. Extensive\nexperiments show that FLAT achieves high attack success and remains robust\nagainst advanced FL defenses. These results highlight the urgent need for new\ndefense strategies to address latent-driven, multi-target backdoor threats in\nfederated settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04064", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04066", "title": "DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving", "authors": ["Longling Geng", "Huangxing Li", "Viktor Lado Naess", "Mert Pilanci"], "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04066", "summary": "Understanding and adhering to soft constraints is essential for safe and\nsocially compliant autonomous driving. However, such constraints are often\nimplicit, context-dependent, and difficult to specify explicitly. In this work,\nwe present DRIVE, a novel framework for Dynamic Rule Inference and Verified\nEvaluation that models and evaluates human-like driving constraints from expert\ndemonstrations. DRIVE leverages exponential-family likelihood modeling to\nestimate the feasibility of state transitions, constructing a probabilistic\nrepresentation of soft behavioral rules that vary across driving contexts.\nThese learned rule distributions are then embedded into a convex\noptimization-based planning module, enabling the generation of trajectories\nthat are not only dynamically feasible but also compliant with inferred human\npreferences. Unlike prior approaches that rely on fixed constraint forms or\npurely reward-based modeling, DRIVE offers a unified framework that tightly\ncouples rule inference with trajectory-level decision-making. It supports both\ndata-driven constraint generalization and principled feasibility verification.\nWe validate DRIVE on large-scale naturalistic driving datasets, including inD,\nhighD, and RoundD, and benchmark it against representative inverse constraint\nlearning and planning baselines. Experimental results show that DRIVE achieves\n0.0% soft constraint violation rates, smoother trajectories, and stronger\ngeneralization across diverse driving scenarios. Verified evaluations further\ndemonstrate the efficiency, explanability, and robustness of the framework for\nreal-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04066", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04099", "title": "DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting", "authors": ["Zexu Huang", "Min Xu", "Stuart Perry"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04099", "summary": "3D Gaussian Splatting (3DGS) represents a significant advancement in the\nfield of efficient and high-fidelity novel view synthesis. Despite recent\nprogress, achieving accurate geometric reconstruction under sparse-view\nconditions remains a fundamental challenge. Existing methods often rely on\nnon-local depth regularization, which fails to capture fine-grained structures\nand is highly sensitive to depth estimation noise. Furthermore, traditional\nsmoothing methods neglect semantic boundaries and indiscriminately degrade\nessential edges and textures, consequently limiting the overall quality of\nreconstruction. In this work, we propose DET-GS, a unified depth and edge-aware\nregularization framework for 3D Gaussian Splatting. DET-GS introduces a\nhierarchical geometric depth supervision framework that adaptively enforces\nmulti-level geometric consistency, significantly enhancing structural fidelity\nand robustness against depth estimation noise. To preserve scene boundaries, we\ndesign an edge-aware depth regularization guided by semantic masks derived from\nCanny edge detection. Furthermore, we introduce an RGB-guided edge-preserving\nTotal Variation loss that selectively smooths homogeneous regions while\nrigorously retaining high-frequency details and textures. Extensive experiments\ndemonstrate that DET-GS achieves substantial improvements in both geometric\naccuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on\nsparse-view novel view synthesis benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04099", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04100", "title": "SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios", "authors": ["Borui Li", "Li Yan", "Junhao Han", "Jianmin Liu", "Lei Yu"], "categories": ["cs.AI", "cs.CR", "cs.DC"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04100", "summary": "Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but\nsuffers from high overhead and adaptation cost. Selective HE methods, which\npartially encrypt model parameters by a global mask, are expected to protect\nprivacy with reduced overhead and easy adaptation. However, in cross-device\nscenarios with heterogeneous data and system capabilities, traditional\nSelective HE methods deteriorate client straggling, and suffer from degraded HE\noverhead reduction performance. Accordingly, we propose SenseCrypt, a\nSensitivity-guided selective Homomorphic EnCryption framework, to adaptively\nbalance security and HE overhead per cross-device FL client. Given the\nobservation that model parameter sensitivity is effective for measuring\nclients' data distribution similarity, we first design a privacy-preserving\nmethod to respectively cluster the clients with similar data distributions.\nThen, we develop a scoring mechanism to deduce the straggler-free ratio of\nmodel parameters that can be encrypted by each client per cluster. Finally, for\neach client, we formulate and solve a multi-objective model parameter selection\noptimization problem, which minimizes HE overhead while maximizing model\nsecurity without causing straggling. Experiments demonstrate that SenseCrypt\nensures security against the state-of-the-art inversion attacks, while\nachieving normal model accuracy as on IID data, and reducing training time by\n58.4%-88.7% as compared to traditional HE methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04100", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04107", "title": "Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode", "authors": ["Jingchao Wang", "Zhijian Wu", "Dingjiang Huang", "Yefeng Zheng", "Hong Wang"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04107", "summary": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04107", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04125", "title": "Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks", "authors": ["Sangwon Hyun", "Hyunjun Kim", "Jinhyuk Jang", "Hyojin Choi", "M. Ali Babar"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04125", "summary": "The application of Large Language Models (LLMs) is growing in the productive\ncompletion of Software Engineering tasks. Yet, studies investigating the\nproductive prompting techniques often employed a limited problem space,\nprimarily focusing on well-known prompting patterns and mainly targeting\nfunction-level SE practices. We identify significant gaps in real-world\nworkflows that involve complexities beyond class-level (e.g., multi-class\ndependencies) and different features that can impact Human-LLM Interactions\n(HLIs) processes in code generation. To address these issues, we designed an\nexperiment that comprehensively analyzed the HLI features regarding the code\ngeneration productivity. Our study presents two project-level benchmark tasks,\nextending beyond function-level evaluations. We conducted a user study with 36\nparticipants from diverse backgrounds, asking them to solve the assigned tasks\nby interacting with the GPT assistant using specific prompting patterns. We\nalso examined the participants' experience and their behavioral features during\ninteractions by analyzing screen recordings and GPT chat logs. Our statistical\nand empirical investigation revealed (1) that three out of 15 HLI features\nsignificantly impacted the productivity in code generation; (2) five primary\nguidelines for enhancing productivity for HLI processes; and (3) a taxonomy of\n29 runtime and logic errors that can occur during HLI processes, along with\nsuggested mitigation plans.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04125", "cate": "cs.SE", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04131", "title": "DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation", "authors": ["Zhaohong Huang", "Yuxin Zhang", "Mingbao Lin", "Taojian Zhou", "Guorong Cai", "Rongrong Ji"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04131", "summary": "Deep Supervision Networks exhibit significant efficacy for the medical\nimaging community. Nevertheless, existing work merely supervises either the\ncoarse-grained semantic features or fine-grained detailed features in\nisolation, which compromises the fact that these two types of features hold\nvital relationships in medical image analysis. We advocate the powers of\ncomplementary feature supervision for medical image segmentation, by proposing\na Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates\nboth low-level detailed and high-level semantic feature supervision through\nDetail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM\nrespectively harness low-level and high-level feature maps to create detail and\nsemantic masks for enhancing feature supervision. This is a novel shift from\nsingle-view deep supervision to multi-view deep supervision. DS$^2$Net is also\nequipped with a novel uncertainty-based supervision loss that adaptively\nassigns the supervision strength of features within distinct scales based on\ntheir uncertainty, thus circumventing the sub-optimal heuristic design that\ntypifies previous works. Through extensive experiments on six benchmarks\ncaptured under either colonoscopy, ultrasound and microscope, we demonstrate\nthat DS$^2$Net consistently outperforms state-of-the-art methods for medical\nimage analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04131", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04136", "title": "UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval", "authors": ["Hongyu Guo", "Kuan Zhu", "Xiangzhao Hao", "Haiyun Guo", "Ming Tang", "Jinqiao Wang"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04136", "summary": "Few-shot fine-grained visual classification (FGVC) aims to leverage limited\ndata to enable models to discriminate subtly distinct categories. Recent works\nmostly finetuned the pre-trained visual language models to achieve performance\ngain, yet suffering from overfitting and weak generalization. To deal with\nthis, we introduce UniFGVC, a universal training-free framework that\nreformulates few-shot FGVC as multimodal retrieval. First, we propose the\nCategory-Discriminative Visual Captioner (CDV-Captioner) to exploit the\nopen-world knowledge of multimodal large language models (MLLMs) to generate a\nstructured text description that captures the fine-grained attribute features\ndistinguishing closely related classes. CDV-Captioner uses chain-of-thought\nprompting and visually similar reference images to reduce hallucination and\nenhance discrimination of generated captions. Using it we can convert each\nimage into an image-description pair, enabling more comprehensive feature\nrepresentation, and construct the multimodal category templates using few-shot\nsamples for the subsequent retrieval pipeline. Then, off-the-shelf vision and\ntext encoders embed query and template pairs, and FGVC is accomplished by\nretrieving the nearest template in the joint space. UniFGVC ensures broad\ncompatibility with diverse MLLMs and encoders, offering reliable generalization\nand adaptability across few-shot FGVC scenarios. Extensive experiments on 12\nFGVC benchmarks demonstrate its consistent superiority over prior few-shot\nCLIP-based methods and even several fully-supervised MLLMs-based approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04136", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04138", "title": "COPO: Consistency-Aware Policy Optimization", "authors": ["Jinghang Han", "Jiawei Chen", "Hang Shao", "Hao Ma", "Mingcheng Li", "Xintian Shen", "Lihao Zheng", "Wei Chen", "Tao Wei", "Lihua Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04138", "summary": "Reinforcement learning has significantly enhanced the reasoning capabilities\nof Large Language Models (LLMs) in complex problem-solving tasks. Recently, the\nintroduction of DeepSeek R1 has inspired a surge of interest in leveraging\nrule-based rewards as a low-cost alternative for computing advantage functions\nand guiding policy optimization. However, a common challenge observed across\nmany replication and extension efforts is that when multiple sampled responses\nunder a single prompt converge to identical outcomes, whether correct or\nincorrect, the group-based advantage degenerates to zero. This leads to\nvanishing gradients and renders the corresponding samples ineffective for\nlearning, ultimately limiting training efficiency and downstream performance.\nTo address this issue, we propose a consistency-aware policy optimization\nframework that introduces a structured global reward based on outcome\nconsistency, the global loss based on it ensures that, even when model outputs\nshow high intra-group consistency, the training process still receives\nmeaningful learning signals, which encourages the generation of correct and\nself-consistent reasoning paths from a global perspective. Furthermore, we\nincorporate an entropy-based soft blending mechanism that adaptively balances\nlocal advantage estimation with global optimization, enabling dynamic\ntransitions between exploration and convergence throughout training. Our method\nintroduces several key innovations in both reward design and optimization\nstrategy. We validate its effectiveness through substantial performance gains\non multiple mathematical reasoning benchmarks, highlighting the proposed\nframework's robustness and general applicability. Code of this work has been\nreleased at https://github.com/hijih/copo-code.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04138", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04149", "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap", "authors": ["Xuan Qi", "Rongwu Xu", "Zhijing Jin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04149", "summary": "Aligning large language models (LLMs) with human preferences is a critical\nchallenge in AI research. While methods like Reinforcement Learning from Human\nFeedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they\noften rely on large, costly preference datasets. The current work lacks methods\nfor high-quality data selection specifically for preference data. In this work,\nwe introduce a novel difficulty-based data selection strategy for preference\ndatasets, grounded in the DPO implicit reward mechanism. By selecting\npreference data examples with smaller DPO implicit reward gaps, which are\nindicative of more challenging cases, we improve data efficiency and model\nalignment. Our approach consistently outperforms five strong baselines across\nmultiple datasets and alignment tasks, achieving superior performance with only\n10\\% of the original data. This principled, efficient selection method offers a\npromising solution for scaling LLM alignment with limited resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04149", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04174", "title": "Quasi-Clique Discovery via Energy Diffusion", "authors": ["Yu Zhang", "Yilong Luo", "Mingyuan Ma", "Yao Chen", "Enqiang Zhu", "Jin Xu", "Chanjuan Liu"], "categories": ["cs.AI", "cs.SI"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04174", "summary": "Discovering quasi-cliques -- subgraphs with edge density no less than a given\nthreshold -- is a fundamental task in graph mining, with broad applications in\nsocial networks, bioinformatics, and e-commerce. Existing heuristics often rely\non greedy rules, similarity measures, or metaheuristic search, but struggle to\nmaintain both efficiency and solution consistency across diverse graphs. This\npaper introduces EDQC, a novel quasi-clique discovery algorithm inspired by\nenergy diffusion. Instead of explicitly enumerating candidate subgraphs, EDQC\nperforms stochastic energy diffusion from source vertices, naturally\nconcentrating energy within structurally cohesive regions. The approach enables\nefficient dense subgraph discovery without exhaustive search or\ndataset-specific tuning. Experimental results on 30 real-world datasets\ndemonstrate that EDQC consistently discovers larger quasi-cliques than\nstate-of-the-art baselines on the majority of datasets, while also yielding\nlower variance in solution quality. To the best of our knowledge, EDQC is the\nfirst method to incorporate energy diffusion into quasi-clique discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04174", "cate": "cs.SI", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04182", "title": "Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity", "authors": ["Peizheng Guo", "Jingyao Wang", "Wenwen Qiang", "Huijie Guo", "Changwen Zheng", "Jiahuan Zhou", "Gang Hua"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04182", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across vision-language tasks. However, they may suffer from\nhallucinations--generating outputs that are semantically inconsistent with the\ninput image or text. Through causal analyses, we find that: (i) hallucinations\nwith omission may arise from the failure to adequately capture essential causal\nfactors, and (ii) hallucinations with fabrication are likely caused by the\nmodel being misled by non-causal cues. To address these challenges, we propose\na novel reinforcement learning framework guided by causal completeness, which\njointly considers both causal sufficiency and causal necessity of tokens.\nSpecifically, we evaluate each token's standalone contribution and\ncounterfactual indispensability to define a token-level causal completeness\nreward. This reward is used to construct a causally informed advantage function\nwithin the GRPO optimization framework, encouraging the model to focus on\ntokens that are both causally sufficient and necessary for accurate generation.\nExperimental results across various benchmark datasets and tasks demonstrate\nthe effectiveness of our approach, which effectively mitigates hallucinations\nin MLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04182", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04195", "title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations", "authors": ["Huan Liao", "Qinke Ni", "Yuancheng Wang", "Yiheng Lu", "Haoyue Zhan", "Pengyuan Xie", "Qiang Zhang", "Zhizheng Wu"], "categories": ["cs.AI", "cs.LG", "cs.SD"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04195", "summary": "Paralinguistic vocalizations-including non-verbal sounds like laughter and\nbreathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are\nintegral to natural spoken communication. Despite their importance in conveying\naffect, intent, and interactional cues, such cues remain largely overlooked in\nconventional automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. We present NVSpeech, an integrated and scalable pipeline that bridges\nthe recognition and synthesis of paralinguistic vocalizations, encompassing\ndataset construction, ASR modeling, and controllable TTS. (1) We introduce a\nmanually annotated dataset of 48,430 human-spoken utterances with 18 word-level\nparalinguistic categories. (2) We develop the paralinguistic-aware ASR model,\nwhich treats paralinguistic cues as inline decodable tokens (e.g., \"You're so\nfunny [Laughter]\"), enabling joint lexical and non-verbal transcription. This\nmodel is then used to automatically annotate a large corpus, the first\nlarge-scale Chinese dataset of 174,179 utterances (573 hours) with word-level\nalignment and paralingustic cues. (3) We finetune zero-shot TTS models on both\nhuman- and auto-labeled data to enable explicit control over paralinguistic\nvocalizations, allowing context-aware insertion at arbitrary token positions\nfor human-like speech synthesis. By unifying the recognition and generation of\nparalinguistic vocalizations, NVSpeech offers the first open, large-scale,\nword-level annotated pipeline for expressive speech modeling in Mandarin,\nintegrating recognition and synthesis in a scalable and controllable manner.\nDataset and audio demos are available at https://nvspeech170k.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04195", "cate": "cs.SD", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04196", "title": "Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models", "authors": ["Siddhant Panpatil", "Hiskias Dingeto", "Haon Park"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04196", "summary": "Despite significant advances in alignment techniques, we demonstrate that\nstate-of-the-art language models remain vulnerable to carefully crafted\nconversational scenarios that can induce various forms of misalignment without\nexplicit jailbreaking. Through systematic manual red-teaming with\nClaude-4-Opus, we discovered 10 successful attack scenarios, revealing\nfundamental vulnerabilities in how current alignment methods handle narrative\nimmersion, emotional pressure, and strategic framing. These scenarios\nsuccessfully elicited a range of misaligned behaviors, including deception,\nvalue drift, self-preservation, and manipulative reasoning, each exploiting\ndifferent psychological and contextual vulnerabilities. To validate\ngeneralizability, we distilled our successful manual attacks into\nMISALIGNMENTBENCH, an automated evaluation framework that enables reproducible\ntesting across multiple models. Cross-model evaluation of our 10 scenarios\nagainst five frontier LLMs revealed an overall 76% vulnerability rate, with\nsignificant variations: GPT-4.1 showed the highest susceptibility (90%), while\nClaude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate\nthat sophisticated reasoning capabilities often become attack vectors rather\nthan protective mechanisms, as models can be manipulated into complex\njustifications for misaligned behavior. This work provides (i) a detailed\ntaxonomy of conversational manipulation patterns and (ii) a reusable evaluation\nframework. Together, these findings expose critical gaps in current alignment\nstrategies and highlight the need for robustness against subtle, scenario-based\nmanipulation in future AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04196", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04197", "title": "Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective", "authors": ["Yan Zhang", "Gangyan Zeng", "Daiqing Wu", "Huawen Shen", "Binbin Li", "Yu Zhou", "Can Ma", "Xiaojun Bi"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04197", "summary": "Video text-based visual question answering (Video TextVQA) aims to answer\nquestions by explicitly reading and reasoning about the text involved in a\nvideo. Most works in this field follow a frame-level framework which suffers\nfrom redundant text entities and implicit relation modeling, resulting in\nlimitations in both accuracy and efficiency. In this paper, we rethink the\nVideo TextVQA task from an instance-oriented perspective and propose a novel\nmodel termed GAT (Gather and Trace). First, to obtain accurate reading result\nfor each video text instance, a context-aggregated instance gathering module is\ndesigned to integrate the visual appearance, layout characteristics, and\ntextual contents of the related entities into a unified textual representation.\nThen, to capture dynamic evolution of text in the video flow, an\ninstance-focused trajectory tracing module is utilized to establish\nspatio-temporal relationships between instances and infer the final answer.\nExtensive experiments on several public Video TextVQA datasets validate the\neffectiveness and generalization of our framework. GAT outperforms existing\nVideo TextVQA methods, video-language pretraining methods, and video large\nlanguage models in both accuracy and inference speed. Notably, GAT surpasses\nthe previous state-of-the-art Video TextVQA methods by 3.86\\% in accuracy and\nachieves ten times of faster inference speed than video large language models.\nThe source code is available at https://github.com/zhangyan-ucas/GAT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04197", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04201", "title": "ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs", "authors": ["Ben Zhang", "LuLu Yu", "Lei Gao", "Jing Liu", "QuanJiang Guo", "Hui Gao"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04201", "summary": "In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs\nwhen a model generates a correct answer but follows an incorrect reasoning\npath. Existing methods based on specific multi-step reasoning datasets and\nreinforcement learning strategies, leading to high training costs and limited\ngeneralization. In this work, we propose ViFP, a general framework for\nenhancing visual reasoning reliability. It improves both answer accuracy and\nreasoning soundness by detecting FPs. ViFP tackles the limitations of dataset\ndependency and poor generalization by constructing sub-question templates\ngrounded in the core dimensions of visual reasoning, such as object\nlocalization, characteristic description, and object discovery. ViFP then\nbuilds effective reasoning paths via multi-turn QA to improve reasoning\naccuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning\npath to identify potential FPs, and introduces a targeted chain-of-thought\n(CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby\nreducing logical errors in the reasoning path while preserving accuracy.\nFinally, we introduce a reliability evaluation metric-VoC, which integrates\nanswer accuracy and the FP rate, providing a quantitative tool to assess\nwhether a VLM not only answers correctly, but also reasons reliably. Our\nexperiments on closed-source VLMs show that ViFP consistently improves\nperformance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP\nimproves accuracy by up to 5.4%, surpassing the previous state-of-the-art by\n4.3%, and significantly reduces the number of FPs, validating its benefits in\nenhancing reasoning reliability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04201", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04204", "title": "ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments", "authors": ["Yuquan Wang", "Mi Zhang", "Yining Wang", "Geng Hong", "Xiaoyu You", "Min Yang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04204", "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance in\nreasoning-intensive tasks, but they remain vulnerable to harmful content\ngeneration, particularly in the mid-to-late steps of their reasoning processes.\nExisting defense mechanisms, however, rely on costly fine-tuning and additional\nexpert knowledge, which restricts their scalability. In this work, we propose\nReasoningGuard, an inference-time safeguard for LRMs, which injects timely\nsafety aha moments to steer harmless while helpful reasoning processes.\nLeveraging the model's internal attention behavior, our approach accurately\nidentifies critical points in the reasoning path, and triggers spontaneous,\nsafety-oriented reflection. To safeguard both the subsequent reasoning steps\nand the final answers, we further implement a scaling sampling strategy during\nthe decoding phase, selecting the optimal reasoning path. Inducing minimal\nextra inference cost, ReasoningGuard effectively mitigates three types of\njailbreak attacks, including the latest ones targeting the reasoning process of\nLRMs. Our approach outperforms seven existing safeguards, achieving\nstate-of-the-art safety defenses while effectively avoiding the common\nexaggerated safety issues.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04204", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04213", "title": "A Hybrid AI Methodology for Generating Ontologies of Research Topics from Scientific Paper Corpora", "authors": ["Alessia Pisu", "Livio Pompianu", "Francesco Osborne", "Diego Reforgiato Recupero", "Daniele Riboni", "Angelo Salatino"], "categories": ["cs.AI", "cs.DL", "cs.IR"], "primary_category": "cs.DL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04213", "summary": "Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM)\nplay a central role in providing the primary framework through which\nintelligent systems can explore and interpret the literature. However, these\nresources have traditionally been manually curated, a process that is\ntime-consuming, prone to obsolescence, and limited in granularity. This paper\npresents Sci-OG, a semi-auto\\-mated methodology for generating research topic\nontologies, employing a multi-step approach: 1) Topic Discovery, extracting\npotential topics from research papers; 2) Relationship Classification,\ndetermining semantic relationships between topic pairs; and 3) Ontology\nConstruction, refining and organizing topics into a structured ontology. The\nrelationship classification component, which constitutes the core of the\nsystem, integrates an encoder-based language model with features describing\ntopic occurrence in the scientific literature. We evaluate this approach\nagainst a range of alternative solutions using a dataset of 21,649 manually\nannotated semantic triples. Our method achieves the highest F1 score (0.951),\nsurpassing various competing approaches, including a fine-tuned SciBERT model\nand several LLM baselines, such as the fine-tuned GPT4-mini. Our work is\ncorroborated by a use case which illustrates the practical application of our\nsystem to extend the CSO ontology in the area of cybersecurity. The presented\nsolution is designed to improve the accessibility, organization, and analysis\nof scientific knowledge, thereby supporting advancements in AI-enabled\nliterature management and research exploration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04213", "cate": "cs.DL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04225", "title": "Symmetric Behavior Regularization via Taylor Expansion of Symmetry", "authors": ["Lingwei Zhu", "Zheng Chen", "Han Wang", "Yukie Nagai"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04225", "summary": "This paper introduces symmetric divergences to behavior regularization policy\noptimization (BRPO) to establish a novel offline RL framework. Existing methods\nfocus on asymmetric divergences such as KL to obtain analytic regularized\npolicies and a practical minimization objective. We show that symmetric\ndivergences do not permit an analytic policy as regularization and can incur\nnumerical issues as loss. We tackle these challenges by the Taylor series of\n$f$-divergence. Specifically, we prove that an analytic policy can be obtained\nwith a finite series. For loss, we observe that symmetric divergences can be\ndecomposed into an asymmetry and a conditional symmetry term, Taylor-expanding\nthe latter alleviates numerical issues. Summing together, we propose Symmetric\n$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric\ndivergences. Experimental results on distribution approximation and MuJoCo\nverify that S$f$-AC performs competitively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04225", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04228", "title": "LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation", "authors": ["Kangrui Cen", "Baixuan Zhao", "Yi Xin", "Siqi Luo", "Guangtao Zhai", "Xiaohong Liu"], "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04228", "summary": "Controlling object motion trajectories in Text-to-Video (T2V) generation is a\nchallenging and relatively under-explored area, particularly in scenarios\ninvolving multiple moving objects. Most community models and datasets in the\nT2V domain are designed for single-object motion, limiting the performance of\ncurrent generative models in multi-object tasks. Additionally, existing motion\ncontrol methods in T2V either lack support for multi-object motion scenes or\nexperience severe performance degradation when object trajectories intersect,\nprimarily due to the semantic conflicts in colliding regions. To address these\nlimitations, we introduce LayerT2V, the first approach for generating video by\ncompositing background and foreground objects layer by layer. This layered\ngeneration enables flexible integration of multiple independent elements within\na video, positioning each element on a distinct \"layer\" and thus facilitating\ncoherent multi-object synthesis while enhancing control over the generation\nprocess. Extensive experiments demonstrate the superiority of LayerT2V in\ngenerating complex multi-object scenarios, showcasing 1.4x and 4.5x\nimprovements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.\nProject page and code are available at https://kr-panghu.github.io/LayerT2V/ .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04228", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04231", "title": "Empowering Time Series Forecasting with LLM-Agents", "authors": ["Chin-Chia Michael Yeh", "Vivian Lai", "Uday Singh Saini", "Xiran Fan", "Yujie Fan", "Junpeng Wang", "Xin Dai", "Yan Zheng"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04231", "summary": "Large Language Model (LLM) powered agents have emerged as effective planners\nfor Automated Machine Learning (AutoML) systems. While most existing AutoML\napproaches focus on automating feature engineering and model architecture\nsearch, recent studies in time series forecasting suggest that lightweight\nmodels can often achieve state-of-the-art performance. This observation led us\nto explore improving data quality, rather than model architecture, as a\npotentially fruitful direction for AutoML on time series data. We propose\nDCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata\naccompanying time series to clean data while optimizing forecasting\nperformance. We evaluated DCATS using four time series forecasting models on a\nlarge-scale traffic volume forecasting dataset. Results demonstrate that DCATS\nachieves an average 6% error reduction across all tested models and time\nhorizons, highlighting the potential of data-centric approaches in AutoML for\ntime series forecasting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04231", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04243", "title": "Automated ultrasound doppler angle estimation using deep learning", "authors": ["Nilesh Patil", "Ajay Anand"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04243", "summary": "Angle estimation is an important step in the Doppler ultrasound clinical\nworkflow to measure blood velocity. It is widely recognized that incorrect\nangle estimation is a leading cause of error in Doppler-based blood velocity\nmeasurements. In this paper, we propose a deep learning-based approach for\nautomated Doppler angle estimation. The approach was developed using 2100 human\ncarotid ultrasound images including image augmentation. Five pre-trained models\nwere used to extract images features, and these features were passed to a\ncustom shallow network for Doppler angle estimation. Independently,\nmeasurements were obtained by a human observer reviewing the images for\ncomparison. The mean absolute error (MAE) between the automated and manual\nangle estimates ranged from 3.9{\\deg} to 9.4{\\deg} for the models evaluated.\nFurthermore, the MAE for the best performing model was less than the acceptable\nclinical Doppler angle error threshold thus avoiding misclassification of\nnormal velocity values as a stenosis. The results demonstrate potential for\napplying a deep-learning based technique for automated ultrasound Doppler angle\nestimation. Such a technique could potentially be implemented within the\nimaging software on commercial ultrasound scanners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04243", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04248", "title": "TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening", "authors": ["Xi Wang", "Anxo Perez", "Javier Parapar", "Fabio Crestani"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04248", "summary": "The increasing demand for mental health services has outpaced the\navailability of real training data to develop clinical professionals, leading\nto limited support for the diagnosis of depression. This shortage has motivated\nthe development of simulated or virtual patients to assist in training and\nevaluation, but existing approaches often fail to generate clinically valid,\nnatural, and diverse symptom presentations. In this work, we embrace the recent\nadvanced language models as the backbone and propose a novel\nclinician-in-the-loop patient simulation pipeline, TalkDep, with access to\ndiversified patient profiles to develop simulated patients. By conditioning the\nmodel on psychiatric diagnostic criteria, symptom severity scales, and\ncontextual factors, our goal is to create authentic patient responses that can\nbetter support diagnostic model training and evaluation. We verify the\nreliability of these simulated patients with thorough assessments conducted by\nclinical professionals. The availability of validated simulated patients offers\na scalable and adaptable resource for improving the robustness and\ngeneralisability of automatic depression diagnosis systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04248", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04260", "title": "Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark", "authors": ["Xiao Wang", "Ziwen Wang", "Wentao Wu", "Anjie Wang", "Jiashu Wu", "Yantao Pan", "Chenglong Li"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04260", "summary": "With the rapid advancement of autonomous driving, vehicle perception,\nparticularly detection and segmentation, has placed increasingly higher demands\non algorithmic performance. Pre-trained large segmentation models, especially\nSegment Anything Model (SAM), have sparked significant interest and inspired\nnew research directions in artificial intelligence. However, SAM cannot be\ndirectly applied to the fine-grained task of vehicle part segmentation, as its\ntext-prompted segmentation functionality is not publicly accessible, and the\nmask regions generated by its default mode lack semantic labels, limiting its\nutility in structured, category-specific segmentation tasks. To address these\nlimitations, we propose SAV, a novel framework comprising three core\ncomponents: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a\ncontext sample retrieval encoding module. The knowledge graph explicitly models\nthe spatial and geometric relationships among vehicle parts through a\nstructured ontology, effectively encoding prior structural knowledge.\nMeanwhile, the context retrieval module enhances segmentation by identifying\nand leveraging visually similar vehicle instances from training data, providing\nrich contextual priors for improved generalization. Furthermore, we introduce a\nnew large-scale benchmark dataset for vehicle part segmentation, named\nVehicleSeg10K, which contains 11,665 high-quality pixel-level annotations\nacross diverse scenes and viewpoints. We conduct comprehensive experiments on\nthis dataset and two other datasets, benchmarking multiple representative\nbaselines to establish a solid foundation for future research and comparison. %\nBoth the dataset and source code of this paper will be released upon\nacceptance. Both the dataset and source code of this paper will be released on\nhttps://github.com/Event-AHU/SAV", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04260", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04265", "title": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning", "authors": ["Borui Li", "Li Yan", "Jianmin Liu"], "categories": ["cs.AI", "cs.CR", "cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04265", "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04265", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04269", "title": "A Visual Tool for Interactive Model Explanation using Sensitivity Analysis", "authors": ["Manuela Schuler"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04269", "summary": "We present SAInT, a Python-based tool for visually exploring and\nunderstanding the behavior of Machine Learning (ML) models through integrated\nlocal and global sensitivity analysis. Our system supports Human-in-the-Loop\n(HITL) workflows by enabling users - both AI researchers and domain experts -\nto configure, train, evaluate, and explain models through an interactive\ngraphical interface without programming. The tool automates model training and\nselection, provides global feature attribution using variance-based sensitivity\nanalysis, and offers per-instance explanation via LIME and SHAP. We demonstrate\nthe system on a classification task predicting survival on the Titanic dataset\nand show how sensitivity information can guide feature selection and data\nrefinement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04269", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04276", "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models", "authors": ["Jiayi Wen", "Tianxin Chen", "Zhirun Zheng", "Cheng Huang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04276", "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04276", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04280", "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success", "authors": ["George Bredis", "Stanislav Dereka", "Viacheslav Sinii", "Ruslan Rakhimov", "Daniil Gavrilov"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04280", "summary": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04280", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04288", "title": "Challenges in Applying Variational Quantum Algorithms to Dynamic Satellite Network Routing", "authors": ["Phuc Hao Do", "Tran Duc Le"], "categories": ["cs.AI", "eess.SY", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04288", "summary": "Applying near-term variational quantum algorithms to the problem of dynamic\nsatellite network routing represents a promising direction for quantum\ncomputing. In this work, we provide a critical evaluation of two major\napproaches: static quantum optimizers such as the Variational Quantum\nEigensolver (VQE) and the Quantum Approximate Optimization Algorithm (QAOA) for\noffline route computation, and Quantum Reinforcement Learning (QRL) methods for\nonline decision-making. Using ideal, noise-free simulations, we find that these\nalgorithms face significant challenges. Specifically, static optimizers are\nunable to solve even a classically easy 4-node shortest path problem due to the\ncomplexity of the optimization landscape. Likewise, a basic QRL agent based on\npolicy gradient methods fails to learn a useful routing strategy in a dynamic\n8-node environment and performs no better than random actions. These negative\nfindings highlight key obstacles that must be addressed before quantum\nalgorithms can offer real advantages in communication networks. We discuss the\nunderlying causes of these limitations, including barren plateaus and learning\ninstability, and suggest future research directions to overcome them.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04288", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04293", "title": "Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum", "authors": ["Nirmal Gaud", "Surej Mouli", "Preeti Katiyar", "Vaduguru Venkata Ramya"], "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04293", "summary": "This study proposes NIRMAL (Novel Integrated Robust Multi-Adaptation\nLearning), a novel optimization algorithm that combines multiple strategies\ninspired by the movements of the chess piece. These strategies include gradient\ndescent, momentum, stochastic perturbations, adaptive learning rates, and\nnon-linear transformations. We carefully evaluated NIRMAL against two widely\nused and successful optimizers, Adam and SGD with Momentum, on four benchmark\nimage classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100.\nThe custom convolutional neural network (CNN) architecture is applied on each\ndataset. The experimental results show that NIRMAL achieves competitive\nperformance, particularly on the more challenging CIFAR-100 dataset, where it\nachieved a test accuracy of 45.32\\%and a weighted F1-score of 0.4328. This\nperformance surpasses Adam (41.79\\% accuracy, 0.3964 F1-score) and closely\nmatches SGD with Momentum (46.97\\% accuracy, 0.4531 F1-score). Also, NIRMAL\nexhibits robust convergence and strong generalization capabilities, especially\non complex datasets, as evidenced by stable training results in loss and\naccuracy curves. These findings underscore NIRMAL's significant ability as a\nversatile and effective optimizer for various deep learning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04293", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04307", "title": "Compressing Large Language Models with PCA Without Performance Loss", "authors": ["Magnus Bengtsson"], "categories": ["cs.AI", "cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04307", "summary": "We demonstrate that Principal Component Analysis (PCA), when applied in a\nstructured manner, either to polar-transformed images or segment-wise to token\nsequences, enables extreme compression of neural models without sacrificing\nperformance. Across three case studies, we show that a one-layer classifier\ntrained on PCA-compressed polar MNIST achieves over 98 percent accuracy using\nonly 840 parameters. A two-layer transformer trained on 70-dimensional\nPCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20\nNewsgroups dataset with just 81000 parameters. A decoder-only transformer\ngenerates coherent token sequences from 70-dimensional PCA embeddings while\npreserving over 97 percent cosine similarity with full MiniLM representations,\nusing less than 17 percent of the parameter count of GPT-2. These results\nhighlight PCA-based input compression as a general and effective strategy for\naligning model capacity with information content, enabling lightweight\narchitectures across multiple modalities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04307", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04325", "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models", "authors": ["Zizhan Ma", "Wenxuan Wang", "Guo Yu", "Yiu-Fai Cheung", "Meidan Ding", "Jie Liu", "Wenting Chen", "Linlin Shen"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04325", "summary": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04325", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04337", "title": "Modelling and Classifying the Components of a Literature Review", "authors": ["Francisco BolaÃ±os", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04337", "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04337", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04349", "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy", "authors": ["Hongze Tan", "Jianfei Pan"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04349", "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04349", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04350", "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models", "authors": ["Nima Iji", "Kia Dashtipour"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04350", "summary": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04350", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04353", "title": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content", "authors": ["Anderson de Lima Luiz"], "categories": ["cs.AI", "cs.MM"], "primary_category": "cs.MM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04353", "summary": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04353", "cate": "cs.MM", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04381", "title": "ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition", "authors": ["Santhoshkumar Peddi", "Sadhvik Bathini", "Arun Balasubramanian", "Monalisa Sarma", "Debasis Samanta"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04381", "summary": "Ear biometrics offer a stable and contactless modality for identity\nrecognition, yet their effectiveness remains limited by the scarcity of\nannotated data and significant intra-class variability. Existing methods\ntypically extract identity features from individual impressions in isolation,\nrestricting their ability to capture consistent and discriminative\nrepresentations. To overcome these limitations, a few-shot learning framework,\nProtoN, is proposed to jointly process multiple impressions of an identity\nusing a graph-based approach. Each impression is represented as a node in a\nclass-specific graph, alongside a learnable prototype node that encodes\nidentity-level information. This graph is processed by a Prototype Graph Neural\nNetwork (PGNN) layer, specifically designed to refine both impression and\nprototype representations through a dual-path message-passing mechanism. To\nfurther enhance discriminative power, the PGNN incorporates a cross-graph\nprototype alignment strategy that improves class separability by enforcing\nintra-class compactness while maintaining inter-class distinction.\nAdditionally, a hybrid loss function is employed to balance episodic and global\nclassification objectives, thereby improving the overall structure of the\nembedding space. Extensive experiments on five benchmark ear datasets\ndemonstrate that ProtoN achieves state-of-the-art performance, with Rank-1\nidentification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as\n0.025, showing the effectiveness for few-shot ear recognition under limited\ndata conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04381", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04390", "title": "AIC CTU@FEVER 8: On-premise fact checking through long context RAG", "authors": ["Herbert Ullrich", "Jan Drchal"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04390", "summary": "In this paper, we present our fact-checking pipeline which has scored first\nin FEVER 8 shared task. Our fact-checking system is a simple two-step RAG\npipeline based on our last year's submission. We show how the pipeline can be\nredeployed on-premise, achieving state-of-the-art fact-checking performance (in\nsense of Ev2R test-score), even under the constraint of a single NVidia A10\nGPU, 23GB of graphical memory and 60s running time per claim.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04390", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04399", "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky", "authors": ["Xu Zhang", "Mei Chen"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04399", "summary": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04399", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04401", "title": "Why are LLMs' abilities emergent?", "authors": ["VladimÃ­r HavlÃ­k"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04401", "summary": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04401", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04406", "title": "Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models", "authors": ["Yinan Yu", "Alex Gonzalez-Caceres", "Samuel Scheidegger", "Sanjay Somanath", "Alexander Hollberg"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04406", "summary": "Renovating existing buildings is essential for climate impact. Early-phase\nrenovation planning requires simulations based on thermal 3D models at Level of\nDetail (LoD) 3, which include features like windows. However, scalable and\naccurate identification of such features remains a challenge. This paper\npresents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that\ngenerates LoD3 thermal models by extracting geometries from images using both\ncomputer vision and deep learning. Unlike existing methods relying on\nsegmentation and projection, SI3FP directly models geometric primitives in the\northographic image plane, providing a unified interface while reducing\nperspective distortions. SI3FP supports both sparse (e.g., Google Street View)\nand dense (e.g., hand-held camera) data sources. Tested on typical Swedish\nresidential buildings, SI3FP achieved approximately 5% error in window-to-wall\nratio estimates, demonstrating sufficient accuracy for early-stage renovation\nanalysis. The pipeline facilitates large-scale energy renovation planning and\nhas broader applications in urban development and planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04406", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04418", "title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation", "authors": ["Jinxing Zhou", "Yanghao Zhou", "Mingfei Han", "Tong Wang", "Xiaojun Chang", "Hisham Cholakkal", "Rao Muhammad Anwer"], "categories": ["cs.AI", "cs.CV", "cs.MA", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04418", "summary": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects\nin audible videos based on given reference expressions. Prior works typically\nrely on learning latent embeddings via multimodal fusion to prompt a tunable\nSAM/SAM2 decoder for segmentation, which requires strong pixel-level\nsupervision and lacks interpretability. From a novel perspective of explicit\nreference understanding, we propose TGS-Agent, which decomposes the task into a\nThink-Ground-Segment process, mimicking the human reasoning procedure by first\nidentifying the referred object through multimodal analysis, followed by\ncoarse-grained grounding and precise segmentation. To this end, we first\npropose Ref-Thinker, a multimodal language model capable of reasoning over\ntextual, visual, and auditory cues. We construct an instruction-tuning dataset\nwith explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The\nobject description inferred by Ref-Thinker is used as an explicit prompt for\nGrounding-DINO and SAM2, which perform grounding and segmentation without\nrelying on pixel-level supervision. Additionally, we introduce\nR\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and\nreasoning-intensive references for better evaluating model generalization. Our\napproach achieves state-of-the-art results on both standard Ref-AVSBench and\nproposed R\\textsuperscript{2}-AVSBench. Code will be available at\nhttps://github.com/jasongief/TGS-Agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04418", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04427", "title": "Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models", "authors": ["Md Raisul Kibria", "SÃ©bastien Lafond", "Janan Arslan"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04427", "summary": "Multimodal learning has witnessed remarkable advancements in recent years,\nparticularly with the integration of attention-based models, leading to\nsignificant performance gains across a variety of tasks. Parallel to this\nprogress, the demand for explainable artificial intelligence (XAI) has spurred\na growing body of research aimed at interpreting the complex decision-making\nprocesses of these models. This systematic literature review analyzes research\npublished between January 2020 and early 2024 that focuses on the\nexplainability of multimodal models. Framed within the broader goals of XAI, we\nexamine the literature across multiple dimensions, including model\narchitecture, modalities involved, explanation algorithms and evaluation\nmethodologies. Our analysis reveals that the majority of studies are\nconcentrated on vision-language and language-only models, with attention-based\ntechniques being the most commonly employed for explanation. However, these\nmethods often fall short in capturing the full spectrum of interactions between\nmodalities, a challenge further compounded by the architectural heterogeneity\nacross domains. Importantly, we find that evaluation methods for XAI in\nmultimodal settings are largely non-systematic, lacking consistency,\nrobustness, and consideration for modality-specific cognitive and contextual\nfactors. Based on these findings, we provide a comprehensive set of\nrecommendations aimed at promoting rigorous, transparent, and standardized\nevaluation and reporting practices in multimodal XAI research. Our goal is to\nsupport future research in more interpretable, accountable, and responsible\nmulitmodal AI systems, with explainability at their core.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04427", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04440", "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion", "authors": ["Yutong Wu", "Di Huang", "Ruosi Wan", "Yue Peng", "Shijie Shang", "Chenrui Cao", "Lei Qi", "Rui Zhang", "Zidong Du", "Jie Yan", "Xing Hu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04440", "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04440", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04442", "title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI", "authors": ["Rohaizah Abdul Wahid", "Muhamad Said Nizamuddin Nadim", "Suliana Sulaiman", "Syahmi Akmal Shaharudin", "Muhammad Danial Jupikil", "Iqqwan Jasman Su Azlan Su"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04442", "summary": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04442", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04447", "title": "Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling", "authors": ["Biao Hu", "Guoyin Wang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04447", "summary": "We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a\nnovel generative model that integrates the cloud model into the Wasserstein\nAuto-Encoder (WAE) framework. By leveraging the characteristic functions of the\ncloud model to regularize the latent space, our approach enables more accurate\nmodeling of complex data distributions. Unlike conventional methods that rely\non a standard Gaussian prior and traditional divergence measures, our method\nemploys a cloud model prior, providing a more flexible and realistic\nrepresentation of the latent space, thus mitigating the homogenization observed\nin reconstructed samples. We derive the characteristic function of the cloud\nmodel and propose a corresponding regularizer within the WAE framework.\nExtensive quantitative and qualitative evaluations on MNIST, FashionMNIST,\nCIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in\nterms of reconstruction quality, latent space structuring, and sample\ndiversity. This work not only establishes a novel integration of cloud model\ntheory with MMD-based regularization but also offers a promising new\nperspective for enhancing autoencoder-based generative models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04447", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04451", "title": "Automatic LLM Red Teaming", "authors": ["Roman Belaire", "Arunesh Sinha", "Pradeep Varakantham"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04451", "summary": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04451", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04461", "title": "Small transformer architectures for task switching", "authors": ["Claudius Gros"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04461", "summary": "The rapid progress seen in terms of large-scale generative AI is largely\nbased on the attention mechanism. It is conversely non-trivial to conceive\nsmall-scale applications for which attention-based architectures outperform\ntraditional approaches, such as multi-layer perceptrons or recurrent networks.\nWe examine this problem in the context of 'task switching'. In this framework\nmodels work on ongoing token sequences with the current task being determined\nby stochastically interspersed control tokens. We show that standard\ntransformers cannot solve a basic task switching reference model based on\nfinite domain arithmetics which contains subtasks dedicated to increment /\naddition / reverse copy / context (IARC). We show that transformers, long\nshort-term memory recurrent networks (LSTM), and plain multi-layer perceptrons\n(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our\ncomparative study by including an extension of the standard transformer\narchitecture to its non-translational invariant counterpart, the cisformer, and\nan alternative attention mechanism, extensive attention. A combination of the\nlatter is found to be the only model able to achieve considerable performance\nlevels, of around 95%. Our results indicate that the workings of attention can\nbe understood better, and even improved, when comparing qualitatively different\nformulations in task-switching settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04461", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04472", "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model", "authors": ["Hongxu Chen", "Zhen Wang", "Taoran Mei", "Lin Li", "Bowei Zhu", "Runshi Li", "Long Chen"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04472", "summary": "Concept Erasure, which aims to prevent pretrained text-to-image models from\ngenerating content associated with semantic-harmful concepts (i.e., target\nconcepts), is getting increased attention. State-of-the-art methods formulate\nthis task as an optimization problem: they align all target concepts with\nsemantic-harmless anchor concepts, and apply closed-form solutions to update\nthe model accordingly. While these closed-form methods are efficient, we argue\nthat existing methods have two overlooked limitations: 1) They often result in\nincomplete erasure due to \"non-zero alignment residual\", especially when text\nprompts are relatively complex. 2) They may suffer from generation quality\ndegradation as they always concentrate parameter updates in a few deep layers.\nTo address these issues, we propose a novel closed-form method ErasePro: it is\ndesigned for more complete concept erasure and better preserving overall\ngenerative quality. Specifically, ErasePro first introduces a strict\nzero-residual constraint into the optimization objective, ensuring perfect\nalignment between target and anchor concept features and enabling more complete\nerasure. Secondly, it employs a progressive, layer-wise update strategy that\ngradually transfers target concept features to those of the anchor concept from\nshallow to deep layers. As the depth increases, the required parameter changes\ndiminish, thereby reducing deviations in sensitive deep layers and preserving\ngenerative quality. Empirical results across different concept erasure tasks\n(including instance, art style, and nudity erasure) have demonstrated the\neffectiveness of our ErasePro.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04472", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04476", "title": "Metric Learning in an RKHS", "authors": ["Gokcan Tatli", "Yi Chen", "Blake Mason", "Robert Nowak", "Ramya Korlakai Vinayak"], "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04476", "summary": "Metric learning from a set of triplet comparisons in the form of \"Do you\nthink item h is more similar to item i or item j?\", indicating similarity and\ndifferences between items, plays a key role in various applications including\nimage retrieval, recommendation systems, and cognitive psychology. The goal is\nto learn a metric in the RKHS that reflects the comparisons. Nonlinear metric\nlearning using kernel methods and neural networks have shown great empirical\npromise. While previous works have addressed certain aspects of this problem,\nthere is little or no theoretical understanding of such methods. The exception\nis the special (linear) case in which the RKHS is the standard Euclidean space\n$\\mathbb{R}^d$; there is a comprehensive theory for metric learning in\n$\\mathbb{R}^d$. This paper develops a general RKHS framework for metric\nlearning and provides novel generalization guarantees and sample complexity\nbounds. We validate our findings through a set of simulations and experiments\non real datasets. Our code is publicly available at\nhttps://github.com/RamyaLab/metric-learning-RKHS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04476", "cate": "stat.ML", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04488", "title": "Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting", "authors": ["Chi-Sheng Chen", "Samuel Yen-Chi Chen", "Yun-Cheng Tsai"], "categories": ["cs.AI", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04488", "summary": "In this study, we evaluate the performance of classical and quantum-inspired\nsequential models in forecasting univariate time series of incoming SMS\nactivity (SMS-in) using the Milan Telecommunication Activity Dataset. Due to\ndata completeness limitations, we focus exclusively on the SMS-in signal for\neach spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM\n(QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance Weighted\nKey-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varying\ninput sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained to\npredict the next 10-minute SMS-in value based solely on historical values\nwithin a given sequence window. Our findings indicate that different models\nexhibit varying sensitivities to sequence length, suggesting that quantum\nenhancements are not universally advantageous. Rather, the effectiveness of\nquantum modules is highly dependent on the specific task and architectural\ndesign, reflecting inherent trade-offs among model size, parameterization\nstrategies, and temporal modeling capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04488", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04489", "title": "Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation", "authors": ["Erin Lanus", "Daniel Wolodkin", "Laura J. Freeman"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04489", "summary": "A common use of machine learning (ML) models is predicting the class of a\nsample. Object detection is an extension of classification that includes\nlocalization of the object via a bounding box within the sample.\nClassification, and by extension object detection, is typically evaluated by\ncounting a prediction as incorrect if the predicted label does not match the\nground truth label. This pass/fail scoring treats all misclassifications as\nequivalent. In many cases, class labels can be organized into a class taxonomy\nwith a hierarchical structure to either reflect relationships among the data or\noperator valuation of misclassifications. When such a hierarchical structure\nexists, hierarchical scoring metrics can return the model performance of a\ngiven prediction related to the distance between the prediction and the ground\ntruth label. Such metrics can be viewed as giving partial credit to predictions\ninstead of pass/fail, enabling a finer-grained understanding of the impact of\nmisclassifications. This work develops hierarchical scoring metrics varying in\ncomplexity that utilize scoring trees to encode relationships between class\nlabels and produce metrics that reflect distance in the scoring tree. The\nscoring metrics are demonstrated on an abstract use case with scoring trees\nthat represent three weighting strategies and evaluated by the kind of errors\ndiscouraged. Results demonstrate that these metrics capture errors with finer\ngranularity and the scoring trees enable tuning. This work demonstrates an\napproach to evaluating ML performance that ranks models not only by how many\nerrors are made but by the kind or impact of errors. Python implementations of\nthe scoring metrics will be available in an open-source repository at time of\npublication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04489", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04492", "title": "Learning Robust Intervention Representations with Delta Embeddings", "authors": ["Panagiotis Alimisis", "Christos Diou"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04492", "summary": "Causal representation learning has attracted significant research interest\nduring the past few years, as a means for improving model generalization and\nrobustness. Causal representations of interventional image pairs, have the\nproperty that only variables corresponding to scene elements affected by the\nintervention / action are changed between the start state and the end state.\nWhile most work in this area has focused on identifying and representing the\nvariables of the scene under a causal model, fewer efforts have focused on\nrepresentations of the interventions themselves. In this work, we show that an\neffective strategy for improving out of distribution (OOD) robustness is to\nfocus on the representation of interventions in the latent space. Specifically,\nwe propose that an intervention can be represented by a Causal Delta Embedding\nthat is invariant to the visual scene and sparse in terms of the causal\nvariables it affects. Leveraging this insight, we propose a framework that is\ncapable of learning causal representations from image pairs, without any\nadditional supervision. Experiments in the Causal Triplet challenge demonstrate\nthat Causal Delta Embeddings are highly effective in OOD settings,\nsignificantly exceeding baseline performance in both synthetic and real-world\nbenchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04492", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04503", "title": "PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers", "authors": ["Federico Zucchi", "Thomas Lampert"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04503", "summary": "Multivariate time-series classification is pivotal in domains ranging from\nwearable sensing to biomedical monitoring. Despite recent advances,\nTransformer- and CNN-based models often remain computationally heavy, offer\nlimited frequency diversity, and require extensive parameter budgets. We\npropose PRISM (Per-channel Resolution-Informed Symmetric Module), a\nconvolutional-based feature extractor that applies symmetric\nfinite-impulse-response (FIR) filters at multiple temporal scales,\nindependently per channel. This multi-resolution, per-channel design yields\nhighly frequency-selective embeddings without any inter-channel convolutions,\ngreatly reducing model size and complexity. Across human-activity, sleep-stage\nand biomedical benchmarks, PRISM, paired with lightweight classification heads,\nmatches or outperforms leading CNN and Transformer baselines, while using\nroughly an order of magnitude fewer parameters and FLOPs. By uniting classical\nsignal processing insights with modern deep learning, PRISM offers an accurate,\nresource-efficient solution for multivariate time-series classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04503", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04524", "title": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection", "authors": ["Tianxiao Li", "Zhenglin Huang", "Haiquan Wen", "Yiwei He", "Shuchang Lyu", "Baoyuan Wu", "Guangliang Cheng"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04524", "summary": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04524", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04531", "title": "Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning", "authors": ["Zhuang Chen", "Guanqun Bi", "Wen Zhang", "Jiawei Hu", "Aoyun Wang", "Xiyao Xiao", "Kun Feng", "Minlie Huang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04531", "summary": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04531", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04549", "title": "MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning", "authors": ["Quang-Trung Truong", "Yuk-Kwan Wong", "Vo Hoang Kim Tuyen Dang", "Rinaldi Gotama", "Duc Thanh Nguyen", "Sai-Kit Yeung"], "categories": ["cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04549", "summary": "Marine videos present significant challenges for video understanding due to\nthe dynamics of marine objects and the surrounding environment, camera motion,\nand the complexity of underwater scenes. Existing video captioning datasets,\ntypically focused on generic or human-centric domains, often fail to generalize\nto the complexities of the marine environment and gain insights about marine\nlife. To address these limitations, we propose a two-stage marine\nobject-oriented video captioning pipeline. We introduce a comprehensive video\nunderstanding benchmark that leverages the triplets of video, text, and\nsegmentation masks to facilitate visual grounding and captioning, leading to\nimproved marine video understanding and analysis, and marine video generation.\nAdditionally, we highlight the effectiveness of video splitting in order to\ndetect salient object transitions in scene changes, which significantly enrich\nthe semantics of captioning content. Our dataset and code have been released at\nhttps://msc.hkustvgd.com.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04549", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04566", "title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization", "authors": ["Jinxing Zhou", "Ziheng Zhou", "Yanghao Zhou", "Yuxin Mao", "Zhangling Duan", "Dan Guo"], "categories": ["cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04566", "summary": "The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally\nlocalize events in untrimmed videos that occur simultaneously in both the audio\nand visual modalities. This paper explores DAVEL under a new and more\nchallenging weakly-supervised setting (W-DAVEL task), where only video-level\nevent labels are provided and the temporal boundaries of each event are\nunknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors},\nwhich are defined as reliable timestamps that are well predicted under weak\nsupervision and exhibit highly consistent event semantics across audio and\nvisual modalities. Specifically, we propose a \\textit{Mutual Event Agreement\nEvaluation} module, which generates an agreement score by measuring the\ndiscrepancy between the predicted audio and visual event classes. Then, the\nagreement score is utilized in a \\textit{Cross-modal Salient Anchor\nIdentification} module, which identifies the audio and visual anchor features\nthrough global-video and local temporal window identification mechanisms. The\nanchor features after multimodal integration are fed into an\n\\textit{Anchor-based Temporal Propagation} module to enhance event semantic\nencoding in the original temporal audio and visual features, facilitating\nbetter temporal localization under weak supervision. We establish benchmarks\nfor W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04566", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04575", "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration", "authors": ["Nuo Chen", "Yicheng Tong", "Jiaying Wu", "Minh Duc Duong", "Qian Wang", "Qingyun Zou", "Bryan Hooi", "Bingsheng He"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04575", "summary": "While AI agents show potential in scientific ideation, most existing\nframeworks rely on single-agent refinement, limiting creativity due to bounded\nknowledge and perspective. Inspired by real-world research dynamics, this paper\ninvestigates whether structured multi-agent discussions can surpass solitary\nideation. We propose a cooperative multi-agent framework for generating\nresearch proposals and systematically compare configurations including group\nsize, leaderled versus leaderless structures, and team compositions varying in\ninterdisciplinarity and seniority. To assess idea quality, we employ a\ncomprehensive protocol with agent-based scoring and human review across\ndimensions such as novelty, strategic vision, and integration depth. Our\nresults show that multi-agent discussions substantially outperform solitary\nbaselines. A designated leader acts as a catalyst, transforming discussion into\nmore integrated and visionary proposals. Notably, we find that cognitive\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\neven a single competent agent. These findings offer actionable insights for\ndesigning collaborative AI ideation systems and shed light on how team\nstructure influences creative outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04575", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04581", "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning", "authors": ["Magauiya Zhussip", "Dmitriy Shopkhoev", "Ammar Ali", "Stamatios Lefkimmiatis"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04581", "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04581", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04586", "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference", "authors": ["Nuo Chen", "Moming Duan", "Andre Huikai Lin", "Qian Wang", "Jiaying Wu", "Bingsheng He"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04586", "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04586", "cate": "cs.CY", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04588", "title": "A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI", "authors": ["Nicola Casali", "Alessandro Brusaferri", "Giuseppe Baselli", "Stefano Fumagalli", "Edoardo Micotti", "Gianluigi Forloni", "Riaz Hussein", "Giovanna Rizzo", "Alfonso Mastropietro"], "categories": ["cs.AI", "cs.LG", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04588", "summary": "Accurate estimation of intravoxel incoherent motion (IVIM) parameters from\ndiffusion-weighted MRI remains challenging due to the ill-posed nature of the\ninverse problem and high sensitivity to noise, particularly in the perfusion\ncompartment. In this work, we propose a probabilistic deep learning framework\nbased on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling\nestimation of total predictive uncertainty and decomposition into aleatoric\n(AU) and epistemic (EU) components. The method was benchmarked against non\nprobabilistic neural networks, a Bayesian fitting approach and a probabilistic\nnetwork with single Gaussian parametrization. Supervised training was performed\non synthetic data, and evaluation was conducted on both simulated and two in\nvivo datasets. The reliability of the quantified uncertainties was assessed\nusing calibration curves, output distribution sharpness, and the Continuous\nRanked Probability Score (CRPS). MDNs produced more calibrated and sharper\npredictive distributions for the D and f parameters, although slight\noverconfidence was observed in D*. The Robust Coefficient of Variation (RCV)\nindicated smoother in vivo estimates for D* with MDNs compared to Gaussian\nmodel. Despite the training data covering the expected physiological range,\nelevated EU in vivo suggests a mismatch with real acquisition conditions,\nhighlighting the importance of incorporating EU, which was allowed by DE.\nOverall, we present a comprehensive framework for IVIM fitting with uncertainty\nquantification, which enables the identification and interpretation of\nunreliable estimates. The proposed approach can also be adopted for fitting\nother physical models through appropriate architectural and simulation\nadjustments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04588", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04594", "title": "GraphProp: Training the Graph Foundation Models using Graph Properties", "authors": ["Ziheng Sun", "Qi Feng", "Lehao Lin", "Chris Ding", "Jicong Fan"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04594", "summary": "This work focuses on training graph foundation models (GFMs) that have strong\ngeneralization ability in graph-level tasks such as graph classification.\nEffective GFM training requires capturing information consistent across\ndifferent domains. We discover that graph structures provide more consistent\ncross-domain information compared to node features and graph labels. However,\ntraditional GFMs primarily focus on transferring node features from various\ndomains into a unified representation space but often lack structural\ncross-domain generalization. To address this, we introduce GraphProp, which\nemphasizes structural generalization. The training process of GraphProp\nconsists of two main phases. First, we train a structural GFM by predicting\ngraph invariants. Since graph invariants are properties of graphs that depend\nonly on the abstract structure, not on particular labellings or drawings of the\ngraph, this structural GFM has a strong ability to capture the abstract\nstructural information and provide discriminative graph representations\ncomparable across diverse domains. In the second phase, we use the\nrepresentations given by the structural GFM as positional encodings to train a\ncomprehensive GFM. This phase utilizes domain-specific node attributes and\ngraph labels to further improve cross-domain node feature generalization. Our\nexperiments demonstrate that GraphProp significantly outperforms the\ncompetitors in supervised learning and few-shot learning, especially in\nhandling graphs without node attributes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04594", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04604", "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search", "authors": ["Zhejun Zhao", "Yuehu Dong", "Alley Liu", "Lixue Zheng", "Pingsheng Liu", "Dongdong Shen", "Long Xia", "Jiashu Zhao", "Dawei Yin"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04604", "summary": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04604", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04610", "title": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning", "authors": ["Md Zesun Ahmed Mia", "Malyaban Bal", "Sen Lu", "George M. Nishibuchi", "Suhas Chelian", "Srini Vasan", "Abhronil Sengupta"], "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04610", "summary": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04610", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04618", "title": "HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs", "authors": ["Dengzhao Fang", "Jingtong Gao", "Chengcheng Zhu", "Yu Li", "Xiangyu Zhao", "Yi Chang"], "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04618", "summary": "Recommender systems are indispensable for helping users navigate the immense\nitem catalogs of modern online platforms. Recently, generative recommendation\nhas emerged as a promising paradigm, unifying the conventional\nretrieve-and-rank pipeline into an end-to-end model capable of dynamic\ngeneration. However, existing generative methods are fundamentally constrained\nby their unsupervised tokenization, which generates semantic IDs suffering from\ntwo critical flaws: (1) they are semantically flat and uninterpretable, lacking\na coherent hierarchy, and (2) they are prone to representation entanglement\n(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.\nTo overcome these limitations, we propose HiD-VAE, a novel framework that\nlearns hierarchically disentangled item representations through two core\ninnovations. First, HiD-VAE pioneers a hierarchically-supervised quantization\nprocess that aligns discrete codes with multi-level item tags, yielding more\nuniform and disentangled IDs. Crucially, the trained codebooks can predict\nhierarchical tags, providing a traceable and interpretable semantic path for\neach recommendation. Second, to combat representation entanglement, HiD-VAE\nincorporates a novel uniqueness loss that directly penalizes latent space\noverlap. This mechanism not only resolves the critical ID collision problem but\nalso promotes recommendation diversity by ensuring a more comprehensive\nutilization of the item representation space. These high-quality, disentangled\nIDs provide a powerful foundation for downstream generative models. Extensive\nexperiments on three public benchmarks validate HiD-VAE's superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://anonymous.4open.science/r/HiD-VAE-84B2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04618", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04626", "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis", "authors": ["Feifan Song", "Bofei Gao", "Yifan Song", "Yi Liu", "Weimin Xiong", "Yuyang Song", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04626", "summary": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04626", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04645", "title": "A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation", "authors": ["Yu Song", "Zhigang Hua", "Harry Shomer", "Yan Xie", "Jingzhe Liu", "Bo Long", "Hui Liu"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04645", "summary": "Link Prediction (LP) is a critical task in graph machine learning. While\nGraph Neural Networks (GNNs) have significantly advanced LP performance\nrecently, existing methods face key challenges including limited supervision\nfrom sparse connectivity, sensitivity to initialization, and poor\ngeneralization under distribution shifts. We explore pretraining as a solution\nto address these challenges. Unlike node classification, LP is inherently a\npairwise task, which requires the integration of both node- and edge-level\ninformation. In this work, we present the first systematic study on the\ntransferability of these distinct modules and propose a late fusion strategy to\neffectively combine their outputs for improved performance. To handle the\ndiversity of pretraining data and avoid negative transfer, we introduce a\nMixture-of-Experts (MoE) framework that captures distinct patterns in separate\nexperts, facilitating seamless application of the pretrained model on diverse\ndownstream datasets. For fast adaptation, we develop a parameter-efficient\ntuning strategy that allows the pretrained model to adapt to unseen datasets\nwith minimal computational overhead. Experiments on 16 datasets across two\ndomains demonstrate the effectiveness of our approach, achieving\nstate-of-the-art performance on low-resource link prediction while obtaining\ncompetitive results compared to end-to-end trained methods, with over 10,000x\nlower computational overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04645", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04655", "title": "X-SAM: From Segment Anything to Any Segmentation", "authors": ["Hao Wang", "Limeng Qiao", "Zequn Jie", "Zhijian Huang", "Chengjian Feng", "Qingfang Zheng", "Lin Ma", "Xiangyuan Lan", "Xiaodan Liang"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04655", "summary": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04655", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04658", "title": "YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper", "authors": ["Akhil Saketh Reddy Sabbella", "Ch.Lakshmi Prachothan", "Eswar Kumar Panta"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04658", "summary": "In the poultry industry, detecting chicken illnesses is essential to avoid\nfinancial losses. Conventional techniques depend on manual observation, which\nis laborious and prone to mistakes. Using YOLO v8 a deep learning model for\nreal-time object recognition. This study suggests an AI based approach, by\ndeveloping a system that analyzes high resolution chicken photos, YOLO v8\ndetects signs of illness, such as abnormalities in behavior and appearance. A\nsizable, annotated dataset has been used to train the algorithm, which provides\naccurate real-time identification of infected chicken and prompt warnings to\nfarm operators for prompt action. By facilitating early infection\nidentification, eliminating the need for human inspection, and enhancing\nbiosecurity in large-scale farms, this AI technology improves chicken health\nmanagement. The real-time features of YOLO v8 provide a scalable and effective\nmethod for improving farm management techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04658", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04663", "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models", "authors": ["Young D. Kwon", "Rui Li", "Sijia Li", "Da Li", "Sourav Bhattacharya", "Stylianos I. Venieris"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04663", "summary": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable\nquality, yet their massive parameter scale (8-11B) poses significant challenges\nfor inferences on resource-constrained devices. In this paper, we present\nHierarchicalPrune, a novel compression framework grounded in a key observation:\nDM blocks exhibit distinct functional hierarchies, where early blocks establish\nsemantic structures while later blocks handle texture refinements.\nHierarchicalPrune synergistically combines three techniques: (1) Hierarchical\nPosition Pruning, which identifies and removes less essential later blocks\nbased on position hierarchy; (2) Positional Weight Preservation, which\nsystematically protects early model portions that are essential for semantic\nstructural integrity; and (3) Sensitivity-Guided Distillation, which adjusts\nknowledge-transfer intensity based on our discovery of block-wise sensitivity\nvariations. As a result, our framework brings billion-scale diffusion models\ninto a range more suitable for on-device inference, while preserving the\nquality of the output images. Specifically, when combined with INT4 weight\nquantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction\n(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on\nserver and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score\nand 7% in HPSv2 score compared to the original model. Last but not least, our\ncomprehensive user study with 85 participants demonstrates that\nHierarchicalPrune maintains perceptual quality comparable to the original model\nwhile significantly outperforming prior works.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04663", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04664", "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management", "authors": ["Mo Li", "L.H. Xu", "Qitai Tan", "Ting Cao", "Yunxin Liu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04664", "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04664", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04667", "title": "How are CS students using resources and AI tools for coding tasks?", "authors": ["Natalia Echeverry", "Arun Lekshmi Narayanan"], "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04667", "summary": "A survey of 26 CS students reveals that AI coding assistants are mainly used\nfor writing code (second to online searches) while AI chatbots are the top\nresource for debugging. Participants with different coding experience prefer\nonline help over direct human help from peers and instructors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04667", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04676", "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay", "authors": ["Yunan Zhang", "Shuoran Jiang", "Mengchen Zhao", "Yuefeng Li", "Yang Fan", "Xiangping Wu", "Qingcai Chen"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04676", "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04676", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04683", "title": "Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering", "authors": ["Karthik Menon", "Batool Arhamna Haider", "Muhammad Arham", "Kanwal Mehreen", "Ram Mohan Rao Kadiyala", "Hamza Farooq"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04683", "summary": "This study introduces Query Attribute Modeling (QAM), a hybrid framework that\nenhances search precision and relevance by decomposing open text queries into\nstructured metadata tags and semantic elements. QAM addresses traditional\nsearch limitations by automatically extracting metadata filters from free-form\ntext queries, reducing noise and enabling focused retrieval of relevant items.\n  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique\nitems with 40,000+ reviews and detailed product attributes) demonstrated QAM's\nsuperior performance, achieving a mean average precision at 5 (mAP@5) of\n52.99\\%. This represents significant improvement over conventional methods,\nincluding BM25 keyword search, encoder-based semantic similarity search,\ncross-encoder re-ranking, and hybrid search combining BM25 and semantic results\nvia Reciprocal Rank Fusion (RRF). The results establish QAM as a robust\nsolution for Enterprise Search applications, particularly in e-commerce\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04683", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04691", "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario", "authors": ["Yuanchen Bai", "Zijian Ding", "Shaoyue Wen", "Xiang Chang", "Angelique Taylor"], "categories": ["cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04691", "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04691", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04699", "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis", "authors": ["Anushka Yadav", "Isha Nalawade", "Srujana Pillarichety", "Yashwanth Babu", "Reshmi Ghosh", "Samyadeep Basu", "Wenlong Zhao", "Ali Nasaeh", "Sriram Balasubramanian", "Soundararajan Srinivasan"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04699", "summary": "The emergence of reasoning models and their integration into practical AI\nchat bots has led to breakthroughs in solving advanced math, deep search, and\nextractive question answering problems that requires a complex and multi-step\nthought process. Yet, a complete understanding of why these models hallucinate\nmore than general purpose language models is missing. In this investigative\nstudy, we systematicallyexplore reasoning failures of contemporary language\nmodels on multi-hop question answering tasks. We introduce a novel, nuanced\nerror categorization framework that examines failures across three critical\ndimensions: the diversity and uniqueness of source documents involved (\"hops\"),\ncompleteness in capturing relevant information (\"coverage\"), and cognitive\ninefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by\ncomplementary automated metrics, our exploration uncovers intricate error\npatterns often hidden by accuracy-centric evaluations. This investigative\napproach provides deeper insights into the cognitive limitations of current\nmodels and offers actionable guidance toward enhancing reasoning fidelity,\ntransparency, and robustness in future language modeling efforts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04699", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2405.04336", "title": "Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction", "authors": ["Zhihao Wen", "Yuan Fang", "Pengcheng Wei", "Fayao Liu", "Zhenghua Chen", "Min Wu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.04336", "summary": "Predicting Remaining Useful Life (RUL) plays a crucial role in the\nprognostics and health management of industrial systems that involve a variety\nof interrelated sensors. Given a constant stream of time series sensory data\nfrom such systems, deep learning models have risen to prominence at identifying\ncomplex, nonlinear temporal dependencies in these data. In addition to the\ntemporal dependencies of individual sensors, spatial dependencies emerge as\nimportant correlations among these sensors, which can be naturally modelled by\na temporal graph that describes time-varying spatial relationships. However,\nthe majority of existing studies have relied on capturing discrete snapshots of\nthis temporal graph, a coarse-grained approach that leads to loss of temporal\ninformation. Moreover, given the variety of heterogeneous sensors, it becomes\nvital that such inherent heterogeneity is leveraged for RUL prediction in\ntemporal sensor graphs. To capture the nuances of the temporal and spatial\nrelationships and heterogeneous characteristics in an interconnected graph of\nsensors, we introduce a novel model named Temporal and Heterogeneous Graph\nNeural Networks (THGNN). Specifically, THGNN aggregates historical data from\nneighboring nodes to accurately capture the temporal dynamics and spatial\ncorrelations within the stream of sensor data in a fine-grained manner.\nMoreover, the model leverages Feature-wise Linear Modulation (FiLM) to address\nthe diversity of sensor types, significantly improving the model's capacity to\nlearn the heterogeneity in the data sources. Finally, we have validated the\neffectiveness of our approach through comprehensive experiments. Our empirical\nfindings demonstrate significant advancements on the N-CMAPSS dataset,\nachieving improvements of up to 19.2% and 31.6% in terms of two different\nevaluation metrics over state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.04336", "cate": "cs.AI", "date": "2024-05-07", "updated": "2025-08-06", "section": "repl"}
{"id": "2408.10691", "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches", "authors": ["Yanjie Dong", "Haijun Zhang", "Chengming Li", "Song Guo", "Victor C. M. Leung", "Xiping Hu"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.10691", "summary": "Since the release of GPT2-1.5B in 2019, the large language models (LLMs) have\nevolved from specialized deep models to versatile foundation models. While\ndemonstrating remarkable zero-shot ability, the LLMs still require fine-tuning\non local datasets and substantial memory for deployment over the network edges.\nTraditional first-order fine-tuning techniques require significant GPU memory\nthat exceeds the capacity of mainstream hardware. Besides, the LLMs have been\nexpanded beyond text generation to create images, audio, video, and multi-modal\ncontent, necessitating careful investigation of efficient deployment strategies\nfor large-scale foundation models. In response to these challenges, model\nfine-tuning and model-compression techniques have been developed to support the\nsustainable growth of LLMs by reducing both operational and capital\nexpenditures. In this work, we provide a comprehensive overview of prevalent\nmemory-efficient fine-tuning methods for deployment at the network edge. We\nalso review state-of-the-art literature on model compression, offering insights\ninto the deployment of LLMs at network edges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.10691", "cate": "cs.AI", "date": "2024-08-20", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.07426", "title": "Evaluating Detection Thresholds: The Impact of False Positives and Negatives on Super-Resolution Ultrasound Localization Microscopy", "authors": ["Sepideh K. Gharamaleki", "Brandon Helfield", "Hassan Rivaz"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.07426", "summary": "Super-resolution ultrasound imaging with ultrasound localization microscopy\n(ULM) offers a high-resolution view of microvascular structures. Yet, ULM image\nquality heavily relies on precise microbubble (MB) detection. Despite the\ncrucial role of localization algorithms, there has been limited focus on the\npractical pitfalls in MB detection tasks such as setting the detection\nthreshold. This study examines how False Positives (FPs) and False Negatives\n(FNs) affect ULM image quality by systematically adding controlled detection\nerrors to simulated data. Results indicate that while both FP and FN rates\nimpact Peak Signal-to-Noise Ratio (PSNR) similarly, increasing FP rates from\n0\\% to 20\\% decreases Structural Similarity Index (SSIM) by 7\\%, whereas same\nFN rates cause a greater drop of around 45\\%. Moreover, dense MB regions are\nmore resilient to detection errors, while sparse regions show high sensitivity,\nshowcasing the need for robust MB detection frameworks to enhance\nsuper-resolution imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.07426", "cate": "cs.AI", "date": "2024-11-11", "updated": "2025-08-05", "section": "repl"}
{"id": "2411.16120", "title": "Why the Agent Made that Decision: Contrastive Explanation Learning for Reinforcement Learning", "authors": ["Rui Zuo", "Simon Khan", "Zifan Wang", "Garrett Ethan Katz", "Qinru Qiu"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.16120", "summary": "Reinforcement learning (RL) has demonstrated remarkable success in solving\ncomplex decision-making problems, yet its adoption in critical domains is\nhindered by the lack of interpretability in its decision-making processes.\nExisting explainable AI (xAI) approaches often fail to provide meaningful\nexplanations for RL agents, particularly because they overlook the contrastive\nnature of human reasoning--answering \"why this action instead of that one?\". To\naddress this gap, we propose a novel framework of contrastive learning to\nexplain RL selected actions, named $\\textbf{VisionMask}$. VisionMask is trained\nto generate explanations by explicitly contrasting the agent's chosen action\nwith alternative actions in a given state using a self-supervised manner. We\ndemonstrate the efficacy of our method through experiments across diverse RL\nenvironments, evaluating it in terms of faithfulness, robustness, and\ncomplexity. Our results show that VisionMask significantly improves human\nunderstanding of agent behavior while maintaining accuracy and fidelity.\nFurthermore, we present examples illustrating how VisionMask can be used for\ncounterfactual analysis. This work bridges the gap between RL and xAI, paving\nthe way for safer and more interpretable RL systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.16120", "cate": "cs.AI", "date": "2024-11-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.01232", "title": "Efficient rule induction by ignoring pointless rules", "authors": ["Andrew Cropper", "David M. Cerna"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.01232", "summary": "The goal of inductive logic programming (ILP) is to find a set of logical\nrules that generalises training examples and background knowledge. We introduce\nan ILP approach that identifies pointless rules. A rule is pointless if it\ncontains a redundant literal or cannot discriminate against negative examples.\nWe show that ignoring pointless rules allows an ILP system to soundly prune the\nhypothesis space. Our experiments on multiple domains, including visual\nreasoning and game playing, show that our approach can reduce learning times by\n99% whilst maintaining predictive accuracies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.01232", "cate": "cs.AI", "date": "2025-02-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.10905", "title": "Learning to Inference Adaptively for Multimodal Large Language Models", "authors": ["Zhuoyan Xu", "Khoi Duc Nguyen", "Preeti Mukherjee", "Saurabh Bagchi", "Somali Chaterji", "Yingyu Liang", "Yin Li"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10905", "summary": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities\nin visual reasoning, yet come with substantial computational cost, limiting\ntheir deployment in resource-constrained settings. Despite recent effort on\nimproving the efficiency of MLLMs, prior solutions fall short in responding to\nvarying runtime conditions, in particular changing resource availability (e.g.,\ncontention due to the execution of other programs on the device). To bridge\nthis gap, we introduce AdaLLaVA, an adaptive inference framework that learns to\ndynamically reconfigure operations in an MLLM during inference, accounting for\nthe input data and a latency budget. We conduct extensive experiments across\nbenchmarks involving question-answering, reasoning, and hallucination. Our\nresults show that AdaLLaVA effectively adheres to input latency budget,\nachieving varying accuracy and latency tradeoffs at runtime. Further, we\ndemonstrate that AdaLLaVA adapts to both input latency and content, can be\nintegrated with token selection for enhanced efficiency, and generalizes across\nMLLMs. Our project webpage with code release is at\nhttps://zhuoyan-xu.github.io/ada-llava/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10905", "cate": "cs.AI", "date": "2025-03-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.02118", "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets", "authors": ["Wei Liu", "Zhongyu Niu", "Lang Gao", "Zhiying Deng", "Jun Wang", "Haozhao Wang", "Ruixuan Li"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.02118", "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.02118", "cate": "cs.AI", "date": "2025-05-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.05758", "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning", "authors": ["Azim Ospanov", "Farzan Farnia", "Roozbeh Yousefzadeh"], "categories": ["cs.AI", "cs.LO"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.05758", "summary": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with LLMs\nremains a formidable task. The usual approach in the literature is to prompt\nthe LLM many times (up to several thousands) until one of the generated proofs\npasses the verification system. In this work, we present APOLLO (Automated\nPrOof repair via LLM and Lean cOllaboration), a modular, modelagnostic pipeline\nthat combines the strengths of the Lean compiler with an LLM's reasoning\nabilities to achieve better proofgeneration results at a low sampling budget.\nApollo directs a fully automated process in which the LLM generates proofs for\ntheorems, a set of agents analyze the proofs, fix the syntax errors, identify\nthe mistakes in the proofs using Lean, isolate failing sublemmas, utilize\nautomated solvers, and invoke an LLM on each remaining goal with a low budget.\nThe repaired subproofs are recombined and reverified, iterating up to a\nusercontrolled maximum number of attempts. On the miniF2F benchmark, we\nestablish a new stateoftheart accuracy of 84.9% among sub 8Bparameter models\nwhile keeping the sampling budget below one hundred. Moreover, Apollo raises\nthe stateoftheart accuracy for GoedelProverSFT to 65.6% while cutting sample\ncomplexity from 25,600 to a few hundred. Generalpurpose models (o3mini, o4mini)\njump from 3-7% to over 40% accuracy. Our results demonstrate that targeted,\ncompilerguided repair of LLM outputs yields dramatic gains in both efficiency\nand correctness, suggesting a general paradigm for scalable automated theorem\nproving. The codebase is available at https://github.com/aziksh-ospanov/APOLLO", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.05758", "cate": "cs.AI", "date": "2025-05-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.12286", "title": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason", "authors": ["Shanchao Liang", "Spandan Garg", "Roshanak Zilouchian Moghaddam"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12286", "summary": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. Similar patterns are also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench Verified than on other similar coding benchmarks (up\nto 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up\nto 18% for tasks in other benchmarks). These findings raise concerns about the\nvalidity of existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12286", "cate": "cs.AI", "date": "2025-06-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.15787", "title": "SLR: Automated Synthesis for Scalable Logical Reasoning", "authors": ["Lukas Helff", "Ahmad Omar", "Felix Friedrich", "Antonia WÃ¼st", "Hikaru Shindo", "Rupert Mitchell", "Tim Woydt", "Patrick Schramowski", "Wolfgang Stammer", "Kristian Kersting"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15787", "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15787", "cate": "cs.AI", "date": "2025-06-18", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.16402", "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks", "authors": ["Xiaoya Lu", "Zeren Chen", "Xuhao Hu", "Yijin Zhou", "Weichen Zhang", "Dongrui Liu", "Lu Sheng", "Jing Shao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16402", "summary": "Flawed planning from VLM-driven embodied agents poses significant safety\nhazards, hindering their deployment in real-world household tasks. However,\nexisting static, non-interactive evaluation paradigms fail to adequately assess\nrisks within these interactive environments, since they cannot simulate dynamic\nrisks that emerge from an agent's actions and rely on unreliable post-hoc\nevaluations that ignore unsafe intermediate steps. To bridge this critical gap,\nwe propose evaluating an agent's interactive safety: its ability to perceive\nemergent risks and execute mitigation steps in the correct procedural order. We\nthus present IS-Bench, the first multi-modal benchmark designed for interactive\nsafety, featuring 161 challenging scenarios with 388 unique safety risks\ninstantiated in a high-fidelity simulator. Crucially, it facilitates a novel\nprocess-oriented evaluation that verifies whether risk mitigation actions are\nperformed before/after specific risk-prone steps. Extensive experiments on\nleading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current\nagents lack interactive safety awareness, and that while safety-aware\nChain-of-Thought can improve performance, it often compromises task completion.\nBy highlighting these critical limitations, IS-Bench provides a foundation for\ndeveloping safer and more reliable embodied AI systems. Code and data are\nreleased under [this https URL](https://github.com/AI45Lab/IS-Bench).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16402", "cate": "cs.AI", "date": "2025-06-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.02663", "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models", "authors": ["Yongjiang Liu", "Haoxi Li", "Xiaosong Ma", "Jie Zhang", "Song Guo"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02663", "summary": "Recent Large Reasoning Models (LRMs) excel at complex reasoning tasks but\noften suffer from overthinking, generating overly long and redundant reasoning\ntrajectories. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we explicitly bootstrap such ability to alleviate overthinking in LRMs? In\nthis paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning\nstrategy that progressively inspires LRMs' difficulty cognition and redundancy\ncognition of LRMs. Specifically, we first inject difficulty hypnosis into\noutput prefixes to guide the model toward adaptive reasoning depth, trained on\na hybrid dataset mixing short and long reasoning paths. Then, we incorporate\nredundancy hypnosis, which supervises the intermediate reasoning steps to\nidentify and eliminate unnecessary reasoning patterns. Experiments on\n7B/14B/32B models demonstrate that TH2T significantly reduces inference costs\nby over 70% on easy tasks and 40% on hard tasks while maintaining performance\nstability. The resulting outputs exhibit clear signs of difficulty-aware\ncapabilities and reduced redundancy (e.g., reflection and looping).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02663", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.16334", "title": "Higher Gauge Flow Models", "authors": ["Alexander Strunk", "Roland Assam"], "categories": ["cs.AI", "cs.LG", "math.DG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16334", "summary": "This paper introduces Higher Gauge Flow Models, a novel class of Generative\nFlow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these\nHigher Gauge Flow Models leverage an L$_{\\infty}$-algebra, effectively\nextending the Lie Algebra. This expansion allows for the integration of the\nhigher geometry and higher symmetries associated with higher groups into the\nframework of Generative Flow Models. Experimental evaluation on a Gaussian\nMixture Model dataset revealed substantial performance improvements compared to\ntraditional Flow Models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16334", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.00222", "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00222", "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00222", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02085", "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents", "authors": ["Jiaye Lin", "Yifu Guo", "Yuzhen Han", "Sen Hu", "Ziyi Ni", "Licheng Wang", "Mingguang Chen", "Daxin Jiang", "Binxing Jiao", "Chen Hu", "Huacan Wang"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02085", "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/wanghuacan/SE-Agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02085", "cate": "cs.AI", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03174", "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation", "authors": ["Tian-Fang Zhao", "Wen-Xi Yang", "Guan Liu", "Liang Yang"], "categories": ["cs.AI"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03174", "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03174", "cate": "cs.AI", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03379", "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "authors": ["Wenxin Mao", "Zhitao Wang", "Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03379", "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03379", "cate": "cs.AI", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2306.09106", "title": "Environmental Sound Classification on An Embedded Hardware Platform", "authors": ["Gabriel Bibbo", "Arshdeep Singh", "Mark D. Plumbley"], "categories": ["cs.AI", "cs.SD", "eess.AS", "eess.SY"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.09106", "summary": "Convolutional neural networks (CNNs) have exhibited state-of-the-art\nperformance in various audio classification tasks. However, their real-time\ndeployment remains a challenge on resource constrained devices such as embedded\nsystems. In this paper, we analyze how the performance of large-scale\npre-trained audio neural networks designed for audio pattern recognition\nchanges when deployed on a hardware such as a Raspberry Pi. We empirically\nstudy the role of CPU temperature, microphone quality and audio signal volume\non performance. Our experiments reveal that the continuous CPU usage results in\nan increased temperature that can trigger an automated slowdown mechanism in\nthe Raspberry Pi, impacting inference latency. The quality of a microphone,\nspecifically with affordable devices such as the Google AIY Voice Kit, and\naudio signal volume, all affect the system performance. In the course of our\ninvestigation, we encounter substantial complications linked to library\ncompatibility and the unique processor architecture requirements of the\nRaspberry Pi, making the process less straightforward compared to conventional\ncomputers (PCs). Our observations, while presenting challenges, pave the way\nfor future researchers to develop more compact machine learning models, design\nheat-dissipative hardware, and select appropriate microphones when AI models\nare deployed for real-time applications on edge devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.09106", "cate": "cs.SD", "date": "2023-06-15", "updated": "2025-08-05", "section": "repl"}
{"id": "2309.13599", "title": "From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited", "authors": ["Zheng Wang", "Hongming Ding", "Li Pan", "Jianhua Li", "Zhiguo Gong", "Philip S. Yu"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.13599", "summary": "Graph-based semi-supervised learning (GSSL) has long been a hot research\ntopic. Traditional methods are generally shallow learners, based on the cluster\nassumption. Recently, graph convolutional networks (GCNs) have become the\npredominant techniques for their promising performance. In this paper, we\ntheoretically discuss the relationship between these two types of methods in a\nunified optimization framework. One of the most intriguing findings is that,\nunlike traditional ones, typical GCNs may not jointly consider the graph\nstructure and label information at each layer. Motivated by this, we further\npropose three simple but powerful graph convolution methods. The first is a\nsupervised method OGC which guides the graph convolution process with labels.\nThe others are two unsupervised methods: GGC and its multi-scale version GGCM,\nboth aiming to preserve the graph structure information during the convolution\nprocess. Finally, we conduct extensive experiments to show the effectiveness of\nour methods. Code is available at https://github.com/zhengwang100/ogc_ggcm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.13599", "cate": "cs.LG", "date": "2023-09-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2312.01697", "title": "Hulk: A Universal Knowledge Translator for Human-Centric Tasks", "authors": ["Yizhou Wang", "Yixuan Wu", "Weizhen He", "Xun Guo", "Feng Zhu", "Lei Bai", "Rui Zhao", "Jian Wu", "Tong He", "Wanli Ouyang", "Shixiang Tang"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.01697", "summary": "Human-centric perception tasks, e.g., pedestrian detection, skeleton-based\naction recognition, and pose estimation, have wide industrial applications,\nsuch as metaverse and sports analysis. There is a recent surge to develop\nhuman-centric foundation models that can benefit a broad range of human-centric\nperception tasks. While many human-centric foundation models have achieved\nsuccess, they did not explore 3D and vision-language tasks for human-centric\nand required task-specific finetuning. These limitations restrict their\napplication to more downstream tasks and situations. To tackle these problems,\nwe present Hulk, the first multimodal human-centric generalist model, capable\nof addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks\nwithout task-specific finetuning. The key to achieving this is condensing\nvarious task-specific heads into two general heads, one for discrete\nrepresentations, \\emph{e.g.,} languages, and the other for continuous\nrepresentations, \\emph{e.g.,} location coordinates. The outputs of two heads\ncan be further stacked into four distinct input and output modalities. This\nuniform representation enables Hulk to treat diverse human-centric tasks as\nmodality translation, integrating knowledge across a wide range of tasks.\nComprehensive evaluations of Hulk on 12 benchmarks covering 8 human-centric\ntasks demonstrate the superiority of our proposed method, achieving\nstate-of-the-art performance in 11 benchmarks. The code will be available on\nhttps://github.com/OpenGVLab/Hulk.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.01697", "cate": "cs.CV", "date": "2023-12-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2403.05839", "title": "Long-Term Visual Object Tracking with Event Cameras: An Associative Memory Augmented Tracker and A Benchmark Dataset", "authors": ["Xiao Wang", "Xufeng Lou", "Shiao Wang", "Ju Huang", "Lan Chen", "Bo Jiang"], "categories": ["cs.AI", "cs.CV", "cs.NE"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.05839", "summary": "Existing event stream based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term, large-scale frame-event visual object tracking dataset, termed FELT.\nIt contains 1,044 long-term videos that involve 1.9 million RGB frames and\nevent stream pairs, 60 different target objects, and 14 challenging attributes.\nTo build a solid benchmark, we retrain and evaluate 21 baseline trackers on our\ndataset for future work to compare. In addition, we propose a novel Associative\nMemory Transformer based RGB-Event long-term visual tracker, termed AMTTrack.\nIt follows a one-stream tracking framework and aggregates the multi-scale\nRGB/event template and search tokens effectively via the Hopfield retrieval\nlayer. The framework also embodies another aspect of associative memory by\nmaintaining dynamic template representations through an associative memory\nupdate scheme, which addresses the appearance variation in long-term tracking.\nExtensive experiments on FELT, FE108, VisEvent, and COESOT datasets fully\nvalidated the effectiveness of our proposed tracker. Both the dataset and\nsource code will be released on https://github.com/Event-AHU/FELT_SOT_Benchmark", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.05839", "cate": "cs.CV", "date": "2024-03-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2405.06419", "title": "Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting", "authors": ["Tianxiang Zhan", "Yuanpeng He", "Yong Deng", "Zhen Li", "Wenjie Du", "Qingsong Wen"], "categories": ["cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.06419", "summary": "In practical scenarios, time series forecasting necessitates not only\naccuracy but also efficiency. Consequently, the exploration of model\narchitectures remains a perennially trending topic in research. To address\nthese challenges, we propose a novel backbone architecture named Time Evidence\nFusion Network (TEFN) from the perspective of information fusion. Specifically,\nwe introduce the Basic Probability Assignment (BPA) Module based on evidence\ntheory to capture the uncertainty of multivariate time series data from both\nchannel and time dimensions. Additionally, we develop a novel multi-source\ninformation fusion method to effectively integrate the two distinct dimensions\nfrom BPA output, leading to improved forecasting accuracy. Lastly, we conduct\nextensive experiments to demonstrate that TEFN achieves performance comparable\nto state-of-the-art methods while maintaining significantly lower complexity\nand reduced training time. Also, our experiments show that TEFN exhibits high\nrobustness, with minimal error fluctuations during hyperparameter selection.\nFurthermore, due to the fact that BPA is derived from fuzzy theory, TEFN offers\na high degree of interpretability. Therefore, the proposed TEFN balances\naccuracy, efficiency, stability, and interpretability, making it a desirable\nsolution for time series forecasting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.06419", "cate": "cs.LG", "date": "2024-05-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2406.02126", "title": "CityLight: A Neighborhood-inclusive Universal Model for Coordinated City-scale Traffic Signal Control", "authors": ["Jinwei Zeng", "Chao Yu", "Xinyi Yang", "Wenxuan Ao", "Qianyue Hao", "Jian Yuan", "Yong Li", "Yu Wang", "Huazhong Yang"], "categories": ["cs.AI", "cs.LG", "cs.MA", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.02126", "summary": "City-scale traffic signal control (TSC) involves thousands of heterogeneous\nintersections with varying topologies, making cooperative decision-making\nacross intersections particularly challenging. Given the prohibitive\ncomputational cost of learning individual policies for each intersection, some\nresearchers explore learning a universal policy to control each intersection in\na decentralized manner, where the key challenge is to construct a universal\nrepresentation method for heterogeneous intersections. However, existing\nmethods are limited to universally representing information of heterogeneous\nego intersections, neglecting the essential representation of influence from\ntheir heterogeneous neighbors. Universally incorporating neighborhood\ninformation is nontrivial due to the intrinsic complexity of traffic flow\ninteractions, as well as the challenge of modeling collective influences from\nneighbor intersections. To address these challenges, we propose CityLight,\nwhich learns a universal policy based on representations obtained with two\nmajor modules: a Neighbor Influence Encoder to explicitly model neighbor's\ninfluence with specified traffic flow relation and connectivity to the ego\nintersection; a Neighbor Influence Aggregator to attentively aggregate the\ninfluence of neighbors based on their mutual competitive relations. Extensive\nexperiments on five city-scale datasets, ranging from 97 to 13,952\nintersections, confirm the efficacy of CityLight, with an average throughput\nimprovement of 11.68% and a lift of 22.59% for generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.02126", "cate": "eess.SY", "date": "2024-06-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2407.18454", "title": "Fairness Definitions in Language Models Explained", "authors": ["Avash Palikhe", "Zichong Wang", "Zhipeng Yin", "Wenbin Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.18454", "summary": "Language Models (LMs) have demonstrated exceptional performance across\nvarious Natural Language Processing (NLP) tasks. Despite these advancements,\nLMs can inherit and amplify societal biases related to sensitive attributes\nsuch as gender and race, limiting their adoption in real-world applications.\nTherefore, fairness has been extensively explored in LMs, leading to the\nproposal of various fairness notions. However, the lack of clear agreement on\nwhich fairness definition to apply in specific contexts and the complexity of\nunderstanding the distinctions between these definitions can create confusion\nand impede further progress. To this end, this paper proposes a systematic\nsurvey that clarifies the definitions of fairness as they apply to LMs.\nSpecifically, we begin with a brief introduction to LMs and fairness in LMs,\nfollowed by a comprehensive, up-to-date overview of existing fairness notions\nin LMs and the introduction of a novel taxonomy that categorizes these concepts\nbased on their transformer architecture: encoder-only, decoder-only, and\nencoder-decoder LMs. We further illustrate each definition through experiments,\nshowcasing their practical implications and outcomes. Finally, we discuss\ncurrent research challenges and open questions, aiming to foster innovative\nideas and advance the field. The repository is publicly available online at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.18454", "cate": "cs.CL", "date": "2024-07-26", "updated": "2025-08-05", "section": "repl"}
{"id": "2409.13783", "title": "A Value Based Parallel Update MCTS Method for Multi-Agent Cooperative Decision Making of Connected and Automated Vehicles", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang", "Xingyu Hu", "Songyu Weng"], "categories": ["cs.AI", "cs.GT", "cs.MA", "eess.SY"], "primary_category": "cs.MA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.13783", "summary": "To solve the problem of lateral and logitudinal joint decision-making of\nmulti-vehicle cooperative driving for connected and automated vehicles (CAVs),\nthis paper proposes a Monte Carlo tree search (MCTS) method with parallel\nupdate for multi-agent Markov game with limited horizon and time discounted\nsetting. By analyzing the parallel actions in the multi-vehicle joint action\nspace in the partial-steady-state traffic flow, the parallel update method can\nquickly exclude potential dangerous actions, thereby increasing the search\ndepth without sacrificing the search breadth. The proposed method is tested in\na large number of randomly generated traffic flow. The experiment results show\nthat the algorithm has good robustness and better performance than the SOTA\nreinforcement learning algorithms and heuristic methods. The vehicle driving\nstrategy using the proposed algorithm shows rationality beyond human drivers,\nand has advantages in traffic efficiency and safety in the coordinating zone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.13783", "cate": "cs.MA", "date": "2024-09-20", "updated": "2025-08-06", "section": "repl"}
{"id": "2409.13959", "title": "One Model, Any Conjunctive Query: Graph Neural Networks for Answering Queries over Incomplete Knowledge Graphs", "authors": ["Krzysztof Olejniczak", "Xingyue Huang", "Mikhail Galkin", "Ä°smail Ä°lkan Ceylan"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.13959", "summary": "Motivated by the incompleteness of modern knowledge graphs, a new setup for\nquery answering has emerged, where the goal is to predict answers that do not\nnecessarily appear in the knowledge graph, but are present in its completion.\nIn this paper, we formally introduce and study two query answering problems,\nnamely, query answer classification and query answer retrieval. To solve these\nproblems, we propose AnyCQ, a model that can classify answers to any\nconjunctive query on any knowledge graph. At the core of our framework lies a\ngraph neural network trained using a reinforcement learning objective to answer\nBoolean queries. Trained only on simple, small instances, AnyCQ generalizes to\nlarge queries of arbitrary structure, reliably classifying and retrieving\nanswers to queries that existing approaches fail to handle. This is empirically\nvalidated through our newly proposed, challenging benchmarks. Finally, we\nempirically show that AnyCQ can effectively transfer to completely novel\nknowledge graphs when equipped with an appropriate link prediction model,\nhighlighting its potential for querying incomplete data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.13959", "cate": "cs.LG", "date": "2024-09-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2409.15395", "title": "Parse Trees Guided LLM Prompt Compression", "authors": ["Wenhao Mao", "Chengbin Hou", "Tianyu Zhang", "Xinyu Lin", "Ke Tang", "Hairong Lv"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.15395", "summary": "Offering rich contexts to Large Language Models (LLMs) has shown to boost the\nperformance in various tasks, but the resulting longer prompt would increase\nthe computational cost and might exceed the input limit of LLMs. Recently, some\nprompt compression methods have been suggested to shorten the length of prompts\nby using language models to generate shorter prompts or by developing\ncomputational models to select important parts of original prompt. The\ngenerative compression methods would suffer from issues like hallucination,\nwhile the selective compression methods have not involved linguistic rules and\noverlook the global structure of prompt. To this end, we propose a novel\nselective compression method called PartPrompt. It first obtains a parse tree\nfor each sentence based on linguistic rules, and calculates local information\nentropy for each node in a parse tree. These local parse trees are then\norganized into a global tree according to the hierarchical structure such as\nthe dependency of sentences, paragraphs, and sections. After that, the\nroot-ward propagation and leaf-ward propagation are proposed to adjust node\nvalues over the global tree. Finally, a recursive algorithm is developed to\nprune the global tree based on the adjusted node values. The experiments show\nthat PartPrompt receives the state-of-the-art performance across various\ndatasets, metrics, compression ratios, and target LLMs for inference. The\nin-depth ablation studies confirm the effectiveness of designs in PartPrompt,\nand other additional experiments also demonstrate its superiority in terms of\nthe coherence of compressed prompts and in the extreme long prompt scenario.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.15395", "cate": "cs.CL", "date": "2024-09-23", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.02745", "title": "AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity", "authors": ["Zhibin Lan", "Liqiang Niu", "Fandong Meng", "Wenbo Li", "Jie Zhou", "Jinsong Su"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.02745", "summary": "Recently, large multimodal models (LMMs) have achieved significant\nadvancements. When dealing with high-resolution images, dominant LMMs typically\ndivide them into multiple local images and a global image, leading to a large\nnumber of visual tokens. In this work, we introduce AVG-LLaVA, an LMM that can\nadaptively select the appropriate visual granularity based on the input image\nand instruction. Specifically, we first apply the multiple pooling layers to\nobtain visual tokens at different granularities. Then we propose a visual\ngranularity router, which includes a Transformer layer, an MLP layer, and a\nvoter layer, used to select the appropriate visual granularity based on the\nimage and instruction. Furthermore, we put forward RGLF, a novel training\nparadigm that aims at aligning the granularity predicted by the router with the\npreferences of the LMM, without the need for additional manually annotated\ndata. Extensive experiments and analysis show that AVG-LLaVA achieves superior\nperformance across 11 benchmarks, as well as significantly reduces the number\nof visual tokens and speeds up inference (e.g., an 85.3% reduction in visual\ntokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.02745", "cate": "cs.CV", "date": "2024-09-20", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.03723", "title": "Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated", "authors": ["Tiffany Zhu", "Iain Weissburg", "Kexun Zhang", "William Yang Wang"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.03723", "summary": "As AI advances in text generation, human trust in AI generated content\nremains constrained by biases that go beyond concerns of accuracy. This study\nexplores how bias shapes the perception of AI versus human generated content.\nThrough three experiments involving text rephrasing, news article\nsummarization, and persuasive writing, we investigated how human raters respond\nto labeled and unlabeled content. While the raters could not differentiate the\ntwo types of texts in the blind test, they overwhelmingly favored content\nlabeled as \"Human Generated,\" over those labeled \"AI Generated,\" by a\npreference score of over 30%. We observed the same pattern even when the labels\nwere deliberately swapped. This human bias against AI has broader societal and\ncognitive implications, as it undervalues AI performance. This study highlights\nthe limitations of human judgment in interacting with AI and offers a\nfoundation for improving human-AI collaboration, especially in creative fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.03723", "cate": "cs.CL", "date": "2024-09-29", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.09206", "title": "pyhgf: A neural network library for predictive coding", "authors": ["Nicolas Legrand", "Lilian Weber", "Peter Thestrup Waade", "Anna Hedvig MÃ¸ller Daugaard", "Mojtaba Khodadadi", "Nace MikuÅ¡", "Chris Mathys"], "categories": ["cs.AI", "cs.LG", "cs.NE", "q-bio.NC"], "primary_category": "cs.NE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.09206", "summary": "Bayesian models of cognition have gained considerable traction in\ncomputational neuroscience and psychiatry. Their scopes are now expected to\nexpand rapidly to artificial intelligence, providing general inference\nframeworks to support embodied, adaptable, and energy-efficient autonomous\nagents. A central theory in this domain is predictive coding, which posits that\nlearning and behaviour are driven by hierarchical probabilistic inferences\nabout the causes of sensory inputs. Biological realism constrains these\nnetworks to rely on simple local computations in the form of precision-weighted\npredictions and prediction errors. This can make this framework highly\nefficient, but its implementation comes with unique challenges on the software\ndevelopment side. Embedding such models in standard neural network libraries\noften becomes limiting, as these libraries' compilation and differentiation\nbackends can force a conceptual separation between optimization algorithms and\nthe systems being optimized. This critically departs from other biological\nprinciples such as self-monitoring, self-organisation, cellular growth and\nfunctional plasticity. In this paper, we introduce \\texttt{pyhgf}: a Python\npackage backed by JAX and Rust for creating, manipulating and sampling dynamic\nnetworks for predictive coding. We improve over other frameworks by enclosing\nthe network components as transparent, modular and malleable variables in the\nmessage-passing steps. The resulting graphs can implement arbitrary\ncomputational complexities as beliefs propagation. But the transparency of core\nvariables can also translate into inference processes that leverage\nself-organisation principles, and express structure learning, meta-learning or\ncausal discovery as the consequence of network structural adaptation to\nsurprising inputs. The code, tutorials and documentation are hosted at:\nhttps://github.com/ilabcode/pyhgf.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.09206", "cate": "cs.NE", "date": "2024-10-11", "updated": "2025-08-05", "section": "repl"}
{"id": "2410.09908", "title": "Beyond Adapter Retrieval: Latent Geometry-Preserving Composition via Sparse Task Projection", "authors": ["Pengfei Jin", "Peng Shu", "Sifan Song", "Sekeun Kim", "Qing Xiao", "Cheng Chen", "Tianming Liu", "Xiang Li", "Quanzheng Li"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.09908", "summary": "Recent advances in parameter-efficient transfer learning have demonstrated\nthe utility of composing LoRA adapters from libraries of pretrained modules.\nHowever, most existing approaches rely on simple retrieval heuristics or\nuniform averaging, which overlook the latent structure of task relationships in\nrepresentation space. We propose a new framework for adapter reuse that moves\nbeyond retrieval, formulating adapter composition as a geometry-aware sparse\nreconstruction problem. Specifically, we represent each task by a latent\nprototype vector derived from the base model's encoder and aim to approximate\nthe target task prototype as a sparse linear combination of retrieved reference\nprototypes, under an $\\ell_1$-regularized optimization objective. The resulting\ncombination weights are then used to blend the corresponding LoRA adapters,\nyielding a composite adapter tailored to the target task. This formulation not\nonly preserves the local geometric structure of the task representation\nmanifold, but also promotes interpretability and efficient reuse by selecting a\nminimal set of relevant adapters. We demonstrate the effectiveness of our\napproach across multiple domains-including medical image segmentation, medical\nreport generation and image synthesis. Our results highlight the benefit of\ncoupling retrieval with latent geometry-aware optimization for improved\nzero-shot generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.09908", "cate": "cs.LG", "date": "2024-10-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.16520", "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context", "authors": ["Naba Rizvi", "Harper Strickland", "Daniel Gitelman", "Tristan Cooper", "Alexis Morales-Flores", "Michael Golden", "Aekta Kallepalli", "Akshat Alurkar", "Haaset Owens", "Saleha Ahmedi", "Isha Khirwadkar", "Imani Munyaka", "Nedjma Ousidhoum"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.16520", "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.16520", "cate": "cs.CL", "date": "2024-10-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.23494", "title": "Causality-Driven Audits of Model Robustness", "authors": ["Nathan Drenkow", "William Paul", "Chris Ribaudo", "Mathias Unberath"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.23494", "summary": "Robustness audits of deep neural networks (DNN) provide a means to uncover\nmodel sensitivities to the challenging real-world imaging conditions that\nsignificantly degrade DNN performance in-the-wild. Such conditions are often\nthe result of multiple interacting factors inherent to the environment, sensor,\nor processing pipeline and may lead to complex image distortions that are not\neasily categorized. When robustness audits are limited to a set of isolated\nimaging effects or distortions, the results cannot be (easily) transferred to\nreal-world conditions where image corruptions may be more complex or nuanced.\nTo address this challenge, we present a new alternative robustness auditing\nmethod that uses causal inference to measure DNN sensitivities to the factors\nof the imaging process that cause complex distortions. Our approach uses causal\nmodels to explicitly encode assumptions about the domain-relevant factors and\ntheir interactions. Then, through extensive experiments on natural and rendered\nimages across multiple vision tasks, we show that our approach reliably\nestimates causal effects of each factor on DNN performance using only\nobservational domain data. These causal effects directly tie DNN sensitivities\nto observable properties of the imaging pipeline in the domain of interest\ntowards reducing the risk of unexpected DNN failures when deployed in that\ndomain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.23494", "cate": "cs.CV", "date": "2024-10-30", "updated": "2025-08-05", "section": "repl"}
{"id": "2411.05273", "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback", "authors": ["Sreyas Venkataraman", "Yufei Wang", "Ziyu Wang", "Navin Sriram Ravie", "Zackory Erickson", "David Held"], "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.05273", "summary": "Offline reinforcement learning can enable policy learning from pre-collected,\nsub-optimal datasets without online interactions. This makes it ideal for\nreal-world robots and safety-critical scenarios, where collecting online data\nor expert demonstrations is slow, costly, and risky. However, most existing\noffline RL works assume the dataset is already labeled with the task rewards, a\nprocess that often requires significant human effort, especially when\nground-truth states are hard to ascertain (e.g., in the real-world). In this\npaper, we build on prior work, specifically RL-VLM-F, and propose a novel\nsystem that automatically generates reward labels for offline datasets using\npreference feedback from a vision-language model and a text description of the\ntask. Our method then learns a policy using offline RL with the reward-labeled\ndataset. We demonstrate the system's applicability to a complex real-world\nrobot-assisted dressing task, where we first learn a reward function using a\nvision-language model on a sub-optimal offline dataset, and then we use the\nlearned reward to employ Implicit Q learning to develop an effective dressing\npolicy. Our method also performs well in simulation tasks involving the\nmanipulation of rigid and deformable objects, and significantly outperform\nbaselines such as behavior cloning and inverse RL. In summary, we propose a new\nsystem that enables automatic reward labeling and policy learning from\nunlabeled, sub-optimal offline datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.05273", "cate": "cs.RO", "date": "2024-11-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.17125", "title": "DOGR: Towards Versatile Visual Document Grounding and Referring", "authors": ["Yinan Zhou", "Yuxin Chen", "Haokun Lin", "Yichen Wu", "Shuyu Yang", "Zhongang Qi", "Chen Ma", "Li Zhu", "Ying Shan"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.17125", "summary": "With recent advances in Multimodal Large Language Models (MLLMs), grounding\nand referring capabilities have gained increasing attention for achieving\ndetailed understanding and flexible user interaction. However, these\ncapabilities still remain underdeveloped in visual document understanding due\nto the scarcity of fine-grained datasets and comprehensive benchmarks. To fill\nthis gap, we propose the DOcument Grounding and Referring data engine\n(DOGR-Engine), which generates two types of high-quality fine-grained document\ndata: (1) multi-granular parsing data to improve text localization and\nrecognition, and (2) instruction-tuning data to activate MLLMs' grounding and\nreferring capabilities in dialogue and reasoning. Using the DOGR-Engine, we\nconstruct DOGR-Bench, a benchmark covering seven grounding and referring tasks\nacross three document types (chart, poster, and PDF document), offering a\ncomprehensive evaluation of fine-grained document understanding. Leveraging the\ngenerated data, we further develop DOGR, a strong baseline model that excels in\ntext localization and recognition, while precisely grounds and refers to key\ntextual information during conversation and reasoning, thereby advancing\ndocument understanding to a finer granularity and enable flexible interaction\nparadigms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.17125", "cate": "cs.CV", "date": "2024-11-26", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.08195", "title": "3DTTNet: Multimodal Fusion-Based 3D Traversable Terrain Modeling for Off-Road Environments", "authors": ["Zitong Chen", "Chao Sun", "Shida Nie", "Chen Min", "Changjiu Ning", "Haoyu Li", "Bo Wang"], "categories": ["cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.08195", "summary": "Off-road environments remain significant challenges for autonomous ground\nvehicles, due to the lack of structured roads and the presence of complex\nobstacles, such as uneven terrain, vegetation, and occlusions. Traditional\nperception algorithms, primarily designed for structured environments, often\nfail in unstructured scenarios. In this paper, traversable area recognition is\nachieved through semantic scene completion. A novel multimodal method, 3DTTNet,\nis proposed to generate dense traversable terrain estimations by integrating\nLiDAR point clouds with monocular images from a forward-facing perspective. By\nintegrating multimodal data, environmental feature extraction is strengthened,\nwhich is crucial for accurate terrain modeling in complex terrains.\nFurthermore, RELLIS-OCC, a dataset with 3D traversable annotations, is\nintroduced, incorporating geometric features such as step height, slope, and\nunevenness. Through a comprehensive analysis of vehicle obsta cle-crossing\nconditions and the incorporation of vehicle body structure constraints, four\ntraversability cost labels are generated: lethal, medium-cost, low-cost, and\nfree. Experimental results demonstrate that 3DTTNet outperforms the comparison\napproaches in 3D traversable area recognition, particularly in off-road\nenvironments with irregular geometries and partial occlusions. Specifically,\n3DTTNet achieves a 42\\% improvement in scene completion IoU compared to other\nmodels. The proposed framework is scalable and adaptable to various vehicle\nplatforms, allowing for adjustments to occupancy grid parameters and the\nintegration of advanced dynamic models for traversability cost estimation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.08195", "cate": "cs.RO", "date": "2024-12-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2501.09014", "title": "How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias", "authors": ["Tosin Fadahunsi", "Giordano d'Aloisio", "Antinisca Di Marco", "Federica Sarro"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.09014", "summary": "Generative models are nowadays widely used to generate graphical content used\nfor multiple purposes, e.g. web, art, advertisement. However, it has been shown\nthat the images generated by these models could reinforce societal biases\nalready existing in specific contexts. In this paper, we focus on understanding\nif this is the case when one generates images related to various software\nengineering tasks. In fact, the Software Engineering (SE) community is not\nimmune from gender and ethnicity disparities, which could be amplified by the\nuse of these models. Hence, if used without consciousness, artificially\ngenerated images could reinforce these biases in the SE domain. Specifically,\nwe perform an extensive empirical evaluation of the gender and ethnicity bias\nexposed by three versions of the Stable Diffusion (SD) model (a very popular\nopen-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We\nobtain 6,720 images by feeding each model with two sets of prompts describing\ndifferent software-related tasks: one set includes the Software Engineer\nkeyword, and one set does not include any specification of the person\nperforming the task. Next, we evaluate the gender and ethnicity disparities in\nthe generated images. Results show how all models are significantly biased\ntowards male figures when representing software engineers. On the contrary,\nwhile SD 2 and SD XL are strongly biased towards White figures, SD 3 is\nslightly more biased towards Asian figures. Nevertheless, all models\nsignificantly under-represent Black and Arab figures, regardless of the prompt\nstyle used. The results of our analysis highlight severe concerns about\nadopting those models to generate content for SE tasks and open the field for\nfuture research on bias mitigation in this context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.09014", "cate": "cs.SE", "date": "2025-01-15", "updated": "2025-08-05", "section": "repl"}
{"id": "2501.15122", "title": "Vision without Images: End-to-End Computer Vision from Single Compressive Measurements", "authors": ["Fengpu Pan", "Heting Gao", "Jiangtao Wen", "Yuxing Han"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.15122", "summary": "Snapshot Compressed Imaging (SCI) offers high-speed, low-bandwidth, and\nenergy-efficient image acquisition, but remains challenged by low-light and low\nsignal-to-noise ratio (SNR) conditions. Moreover, practical hardware\nconstraints in high-resolution sensors limit the use of large frame-sized\nmasks, necessitating smaller, hardware-friendly designs. In this work, we\npresent a novel SCI-based computer vision framework using pseudo-random binary\nmasks of only 8$\\times$8 in size for physically feasible implementations. At\nits core is CompDAE, a Compressive Denoising Autoencoder built on the STFormer\narchitecture, designed to perform downstream tasks--such as edge detection and\ndepth estimation--directly from noisy compressive raw pixel measurements\nwithout image reconstruction. CompDAE incorporates a rate-constrained training\nstrategy inspired by BackSlash to promote compact, compressible models. A\nshared encoder paired with lightweight task-specific decoders enables a unified\nmulti-task platform. Extensive experiments across multiple datasets demonstrate\nthat CompDAE achieves state-of-the-art performance with significantly lower\ncomplexity, especially under ultra-low-light conditions where traditional CMOS\nand SCI pipelines fail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.15122", "cate": "cs.CV", "date": "2025-01-25", "updated": "2025-08-05", "section": "repl"}
{"id": "2502.01083", "title": "Tool Unlearning for Tool-Augmented LLMs", "authors": ["Jiali Cheng", "Hadi Amiri"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.01083", "summary": "Tool-augmented large language models (LLMs) are often trained on datasets of\nquery-response pairs, which embed the ability to use tools or APIs directly\ninto the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to\nforget learned tools due to security vulnerabilities, privacy regulations, or\ntool deprecations. However, ``tool unlearning'' has not been investigated in\nunlearning literature. We introduce this novel task, which requires addressing\ndistinct challenges compared to traditional unlearning: knowledge removal\nrather than forgetting individual samples, the high cost of optimizing LLMs,\nand the need for principled evaluation metrics. To bridge these gaps, we\npropose ToolDelete, the first approach for unlearning tools from tool-augmented\nLLMs. It implements three key properties to address the above challenges for\neffective tool unlearning and introduces a new membership inference attack\n(MIA) model for effective evaluation. Extensive experiments on multiple tool\nlearning datasets and tool-augmented LLMs show that ToolDelete effectively\nunlearns randomly selected tools, while preserving the LLM's knowledge on\nnon-deleted tools and maintaining performance on general tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.01083", "cate": "cs.LG", "date": "2025-02-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.06124", "title": "Foundation Model of Electronic Medical Records for Adaptive Risk Estimation", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Nassim Oufattole", "Deirdre Goode", "Yugang Jia", "Szymon Bieganski", "Matthew B. A. McDermott", "Jaroslaw Was", "Anthony E. Samir", "Jonathan W. Cunningham", "David W. Bates", "Arkadiusz Sitek"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.06124", "summary": "Hospitals struggle to predict critical outcomes. Traditional early warning\nsystems, like NEWS and MEWS, rely on static variables and fixed thresholds,\nlimiting their adaptability, accuracy, and personalization. We previously\ndeveloped the Enhanced Transformer for Health Outcome Simulation (ETHOS), an AI\nmodel that tokenizes patient health timelines (PHTs) from EHRs and uses\ntransformer-based architectures to predict future PHTs. ETHOS is a versatile\nframework for developing a wide range of applications. In this work, we develop\nthe Adaptive Risk Estimation System (ARES) that leverages ETHOS to compute\ndynamic, personalized risk probabilities for clinician-defined critical events.\nARES also features a personalized explainability module that highlights key\nclinical factors influencing risk estimates. We evaluated ARES using the\nMIMIC-IV v2.2 dataset together with its Emergency Department (ED) extension and\nbenchmarked performance against both classical early warning systems and\ncontemporary machine learning models. The entire dataset was tokenized\nresulting in 285,622 PHTs, comprising over 360 million tokens. ETHOS\noutperformed benchmark models in predicting hospital admissions, ICU\nadmissions, and prolonged stays, achieving superior AUC scores. Its risk\nestimates were robust across demographic subgroups, with calibration curves\nconfirming model reliability. The explainability module provided valuable\ninsights into patient-specific risk factors. ARES, powered by ETHOS, advances\npredictive healthcare AI by delivering dynamic, real-time, personalized risk\nestimation with patient-specific explainability. Although our results are\npromising, the clinical impact remains uncertain. Demonstrating ARES's true\nutility in real-world settings will be the focus of our future work. We release\nthe source code to facilitate future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.06124", "cate": "cs.LG", "date": "2025-02-10", "updated": "2025-08-05", "section": "repl"}
{"id": "2502.13179", "title": "PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models", "authors": ["Jiaqi Zhao", "Miao Zhang", "Ming Wang", "Yuzhang Shang", "Kaihao Zhang", "Weili Guan", "Yaowei Wang", "Min Zhang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.13179", "summary": "Large Language Models (LLMs) suffer severe performance degradation when\nfacing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit\npost-training quantization (PTQ) methods utilize a mix-precision scheme by\nleveraging an unstructured fine-grained mask to explicitly distinguish salient\nweights, while which introduces an extra 1-bit or more per weight. To explore\nthe real limit of PTQ, we propose an extremely low-bit PTQ method called\nPTQ1.61, which enables weight quantization to 1.61-bit for the first time.\nSpecifically, we first introduce a one-dimensional structured mask with\nnegligibly additional 0.0002-bit per weight based on input activations from the\nperspective of reducing the upper bound of quantization error to allocate\ncorresponding salient weight channels to 4-bit. For non-salient channels\nbinarization, an efficient block-wise scaling factors optimization framework is\nthen presented to take implicit row-wise correlations and angular biases into\naccount. Different from prior works that concentrate on adjusting quantization\nmethodologies, we further propose a novel paradigm called quantization\npreprocessing, where we argue that transforming the weight distribution of the\npretrained model before quantization can alleviate the difficulty in\nper-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61\nachieves state-of-the-art performance in extremely low-bit quantization. Codes\nare available at https://github.com/zjq0455/PTQ1.61.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.13179", "cate": "cs.LG", "date": "2025-02-18", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.20634", "title": "UltraSTF: Ultra-Compact Model for Large-Scale Spatio-Temporal Forecasting", "authors": ["Chin-Chia Michael Yeh", "Xiran Fan", "Zhimeng Jiang", "Yujie Fan", "Huiyuan Chen", "Uday Singh Saini", "Vivian Lai", "Xin Dai", "Junpeng Wang", "Zhongfang Zhuang", "Liang Wang", "Yan Zheng"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20634", "summary": "Spatio-temporal data, prevalent in real-world applications such as traffic\nmonitoring, financial transactions, and ride-share demands, represents a\nspecialized case of multivariate time series characterized by high\ndimensionality. This high dimensionality necessitates computationally efficient\nmodels and benefits from applying univariate forecasting approaches through\nchannel-independent strategies. SparseTSF, a recently proposed competitive\nunivariate forecasting model, leverages periodicity to achieve compactness by\nfocusing on cross-period dynamics, extending the Pareto frontier in terms of\nmodel size and predictive performance. However, it underperforms on\nspatio-temporal data due to limited capture of intra-period temporal\ndependencies. To address this limitation, we propose UltraSTF, which integrates\na cross-period forecasting component with an ultra-compact shape bank\ncomponent. Our model efficiently captures recurring patterns in time series\nusing the attention mechanism of the shape bank component, significantly\nenhancing its capability to learn intra-period dynamics. UltraSTF achieves\nstate-of-the-art performance on the LargeST benchmark while utilizing fewer\nthan 0.2% of the parameters required by the second-best methods, thereby\nfurther extending the Pareto frontier of existing approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20634", "cate": "cs.LG", "date": "2025-02-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.02992", "title": "RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding Across Different Environments and Tasks", "authors": ["Yimin Tang", "Xiao Xiong", "Jingyi Xi", "Jiaoyang Li", "Erdem BÄ±yÄ±k", "Sven Koenig"], "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.02992", "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial for applications ranging from aerial\nswarms to warehouse automation. Solving MAPF is NP-hard so learning-based\napproaches for MAPF have gained attention, particularly those leveraging deep\nneural networks. Nonetheless, despite the community's continued efforts, all\nlearning-based MAPF planners still rely on decentralized planning due to\nvariability in the number of agents and map sizes. We have developed the first\ncentralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is\nnot an agent-based policy but a map-based policy. By leveraging a CNN-based\narchitecture, RAILGUN can generalize across different maps and handle any\nnumber of agents. We collect trajectories from rule-based methods to train our\nmodel in a supervised way. In experiments, RAILGUN outperforms most baseline\nmethods and demonstrates great zero-shot generalization capabilities on various\ntasks, maps and agent numbers that were not seen in the training dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.02992", "cate": "cs.RO", "date": "2025-03-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.03779", "title": "Accelerating Focal Search in Multi-Agent Path Finding with Tighter Lower Bounds", "authors": ["Yimin Tang", "Zhenghong Yu", "Jiaoyang Li", "Sven Koenig"], "categories": ["cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.MA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.03779", "summary": "Multi-Agent Path Finding (MAPF) involves finding collision-free paths for\nmultiple agents while minimizing a cost function--an NP-hard problem. Bounded\nsuboptimal methods like Enhanced Conflict-Based Search (ECBS) and Explicit\nEstimation CBS (EECBS) balance solution quality with computational efficiency\nusing focal search mechanisms. While effective, traditional focal search faces\na limitation: the lower bound (LB) value determining which nodes enter the\nFOCAL list often increases slowly in early search stages, resulting in a\nconstrained search space that delays finding valid solutions. In this paper, we\npropose a novel bounded suboptimal algorithm, double-ECBS (DECBS), to address\nthis issue by first determining the maximum LB value and then employing a\nbest-first search guided by this LB to find a collision-free path. Experimental\nresults demonstrate that DECBS outperforms ECBS in most test cases and is\ncompatible with existing optimization techniques. DECBS can reduce nearly 30%\nhigh-level CT nodes and 50% low-level focal search nodes. When agent density is\nmoderate to high, DECBS achieves a 23.5% average runtime improvement over ECBS\nwith identical suboptimality bounds and optimizations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.03779", "cate": "cs.MA", "date": "2025-03-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.06725", "title": "Pull-Based Query Scheduling for Goal-Oriented Semantic Communication", "authors": ["Pouya Agheli", "Nikolaos Pappas", "Marios Kountouris"], "categories": ["cs.AI", "cs.IT", "cs.NI"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06725", "summary": "This paper addresses query scheduling for goal-oriented semantic\ncommunication in pull-based status update systems. We consider a system where\nmultiple sensing agents (SAs) observe a source characterized by various\nattributes and provide updates to multiple actuation agents (AAs), which act\nupon the received information to fulfill their heterogeneous goals at the\nendpoint. A hub serves as an intermediary, querying the SAs for updates on\nobserved attributes and maintaining a knowledge base, which is then broadcast\nto the AAs. The AAs leverage the knowledge to perform their actions\neffectively. To quantify the semantic value of updates, we introduce a grade of\neffectiveness (GoE) metric. Furthermore, we integrate cumulative perspective\ntheory (CPT) into the long-term effectiveness analysis to account for risk\nawareness and loss aversion in the system. Leveraging this framework, we\ncompute effect-aware scheduling policies aimed at maximizing the expected\ndiscounted sum of CPT-based total GoE provided by the transmitted updates while\ncomplying with a given query cost constraint. To achieve this, we propose a\nmodel-based solution based on dynamic programming and model-free solutions\nemploying state-of-the-art deep reinforcement learning (DRL) algorithms. Our\nfindings demonstrate that effect-aware scheduling significantly enhances the\neffectiveness of communicated updates compared to benchmark scheduling methods,\nparticularly in settings with stringent cost constraints where optimal query\nscheduling is vital for system performance and overall effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06725", "cate": "cs.IT", "date": "2025-03-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.09516", "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "authors": ["Bowen Jin", "Hansi Zeng", "Zhenrui Yue", "Jinsung Yoon", "Sercan Arik", "Dong Wang", "Hamed Zamani", "Jiawei Han"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.09516", "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.09516", "cate": "cs.CL", "date": "2025-03-12", "updated": "2025-08-05", "section": "repl"}
{"id": "2503.10183", "title": "Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding", "authors": ["Shunqi Mao", "Chaoyi Zhang", "Weidong Cai"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10183", "summary": "Existing vision-language models (VLMs) often suffer from visual\nhallucination, where the generated responses contain inaccuracies that are not\ngrounded in the visual input. Efforts to address this issue without model\nfinetuning primarily mitigate hallucination by contrastively reducing language\nbiases or amplifying the weights of visual embedding during decoding. However,\nthese approaches remain limited in their ability to capture fine-grained visual\ndetails. In this work, we propose the Perception Magnifier (PM), a novel visual\ndecoding method that iteratively isolates relevant visual tokens based on\nattention and magnifies the corresponding regions, spurring the model to\nconcentrate on fine-grained visual details during decoding. By magnifying\ncritical regions while preserving the structural and contextual information at\neach decoding step, PM allows the VLM to enhance its scrutiny of the visual\ninput, hence producing more accurate and faithful responses. Extensive\nexperimental results demonstrate that PM not only achieves superior\nhallucination mitigation but also enhances language generation while preserving\nstrong reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10183", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.10533", "title": "The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory", "authors": ["Robin Schmucker", "Steven Moore"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10533", "summary": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nThis method offers a scalable, pre-deployment evaluation without requiring\nstudent data, but its predictive validity concerning empirical IRT parameters\nis underexplored. To address this gap, we conducted a study involving 7,126\nmultiple-choice questions across various STEM subjects (physical science,\nmathematics, and life/earth sciences). Using an automated approach, we\nannotated each question with a 19-criteria IWF rubric and studied relationships\nto data-driven IRT parameters. Our analysis revealed statistically significant\nlinks between the number of IWFs and IRT difficulty and discrimination\nparameters, particularly in life/earth and physical science domains. We further\nobserved how specific IWF criteria can impact item quality more and less\nseverely (e.g., negative wording vs. implausible distractors) and how they\nmight make a question more or less challenging. Overall, our findings establish\nautomated IWF analysis as a valuable supplement to traditional validation,\nproviding an efficient method for initial item screening, particularly for\nflagging low-difficulty MCQs. Our findings show the need for further research\non domain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10533", "cate": "cs.CL", "date": "2025-03-13", "updated": "2025-08-05", "section": "repl"}
{"id": "2503.12772", "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models", "authors": ["Sung-Yeon Park", "Can Cui", "Yunsheng Ma", "Ahmadreza Moradipari", "Rohit Gupta", "Kyungtae Han", "Ziran Wang"], "categories": ["cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12772", "summary": "Recent advances in multi-modal large language models (MLLMs) have\ndemonstrated strong performance across various domains; however, their ability\nto comprehend driving scenes remains less proven. The complexity of driving\nscenarios, which includes multi-view information, poses significant challenges\nfor existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,\nmulti-modal evaluation benchmark for driving scene understanding. To further\nsupport generalization to multi-view driving scenarios, we also propose\nNuPlanQA-1M, a large-scale dataset comprising 1M real-world visual\nquestion-answering (VQA) pairs. For context-aware analysis of traffic scenes,\nwe categorize our dataset into nine subtasks across three core skills: Road\nEnvironment Perception, Spatial Relations Recognition, and Ego-Centric\nReasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)\nfeatures from multi-view images into MLLMs. Our evaluation results reveal key\nchallenges that existing MLLMs face in driving scene-specific perception and\nspatial reasoning from ego-centric perspectives. In contrast, BEV-LLM\ndemonstrates remarkable adaptability to this domain, outperforming other models\nin six of the nine subtasks. These findings highlight how BEV integration\nenhances multi-view MLLMs while also identifying key areas that require further\nrefinement for effective adaptation to driving scenes. To facilitate further\nresearch, we publicly release NuPlanQA at\nhttps://github.com/sungyeonparkk/NuPlanQA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12772", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-08-05", "section": "repl"}
{"id": "2503.18892", "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild", "authors": ["Weihao Zeng", "Yuzhen Huang", "Qian Liu", "Wei Liu", "Keqing He", "Zejun Ma", "Junxian He"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.18892", "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.18892", "cate": "cs.LG", "date": "2025-03-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.22634", "title": "Empirical Analysis of Sim-and-Real Cotraining of Diffusion Policies for Planar Pushing from Pixels", "authors": ["Adam Wei", "Abhinav Agarwal", "Boyuan Chen", "Rohan Bosworth", "Nicholas Pfaff", "Russ Tedrake"], "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.22634", "summary": "Cotraining with demonstration data generated both in simulation and on real\nhardware has emerged as a promising recipe for scaling imitation learning in\nrobotics. This work seeks to elucidate basic principles of this sim-and-real\ncotraining to inform simulation design, sim-and-real dataset creation, and\npolicy training. Our experiments confirm that cotraining with simulated data\ncan dramatically improve performance, especially when real data is limited. We\nshow that these performance gains scale with additional simulated data up to a\nplateau; adding more real-world data increases this performance ceiling. The\nresults also suggest that reducing physical domain gaps may be more impactful\nthan visual fidelity for non-prehensile or contact-rich tasks. Perhaps\nsurprisingly, we find that some visual gap can help cotraining -- binary probes\nreveal that high-performing policies must learn to distinguish simulated\ndomains from real. We conclude by investigating this nuance and mechanisms that\nfacilitate positive transfer between sim-and-real. Focusing narrowly on the\ncanonical task of planar pushing from pixels allows us to be thorough in our\nstudy. In total, our experiments span 50+ real-world policies (evaluated on\n1000+ trials) and 250 simulated policies (evaluated on 50,000+ trials). Videos\nand code can be found at https://sim-and-real-cotraining.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.22634", "cate": "cs.RO", "date": "2025-03-28", "updated": "2025-08-05", "section": "repl"}
{"id": "2503.23989", "title": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics", "authors": ["Aditya Pathak", "Rachit Gandhi", "Vaibhav Uttam", "Arnav Ramamoorthy", "Pratyush Ghosh", "Aaryan Raj Jindal", "Shreyash Verma", "Aditya Mittal", "Aashna Ased", "Chirag Khatri", "Yashwanth Nakka", "Devansh", "Jagat Sesh Challa", "Dhruv Kumar"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.23989", "summary": "Since the emergence of Large Language Models (LLMs) popularized by the\nrelease of GPT-3 and ChatGPT, LLMs have shown remarkable promise in\nprogramming-related tasks. While code generation using LLMs has become a\npopular field of research, code evaluation using LLMs remains under-explored.\nIn this paper, we focus on LLM-based code evaluation and attempt to fill in the\nexisting gaps. We propose multi-agentic novel approaches using\n\\emph{question-specific rubrics} tailored to the problem statement, arguing\nthat these perform better for logical assessment than the existing approaches\nthat use \\emph{question-agnostic rubrics}. To address the lack of suitable\nevaluation datasets, we introduce two datasets: a Data Structures and\nAlgorithms dataset containing 150 student submissions from a popular Data\nStructures and Algorithms practice website, and an Object Oriented Programming\ndataset comprising 80 student submissions from undergraduate computer science\ncourses. In addition to using standard metrics (Spearman Correlation, Cohen's\nKappa), we additionally propose a new metric called as Leniency, which\nquantifies evaluation strictness relative to expert assessment. Our\ncomprehensive analysis demonstrates that \\emph{question-specific rubrics}\nsignificantly enhance logical assessment of code in educational settings,\nproviding better feedback aligned with instructional goals beyond mere\nsyntactic correctness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.23989", "cate": "cs.SE", "date": "2025-03-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.24007", "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting", "authors": ["Yosuke Yamaguchi", "Issei Suemitsu", "Wenpeng Wei"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.24007", "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.24007", "cate": "cs.LG", "date": "2025-03-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.00401", "title": "Beyond Wide-Angle Images: Structure-to-Detail Video Portrait Correction via Unsupervised Spatiotemporal Adaptation", "authors": ["Wenbo Nie", "Lang Nie", "Chunyu Lin", "Jingwen Chen", "Ke Xing", "Jiyuan Wang", "Kang Liao"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.00401", "summary": "Wide-angle cameras, despite their popularity for content creation, suffer\nfrom distortion-induced facial stretching-especially at the edge of the\nlens-which degrades visual appeal. To address this issue, we propose a\nstructure-to-detail portrait correction model named ImagePC. It integrates the\nlong-range awareness of the transformer and multi-step denoising of diffusion\nmodels into a unified framework, achieving global structural robustness and\nlocal detail refinement. Besides, considering the high cost of obtaining video\nlabels, we then repurpose ImagePC for unlabeled wide-angle videos (termed\nVideoPC), by spatiotemporal diffusion adaption with spatial consistency and\ntemporal smoothness constraints. For the former, we encourage the denoised\nimage to approximate pseudo labels following the wide-angle distortion\ndistribution pattern, while for the latter, we derive rectification\ntrajectories with backward optical flows and smooth them. Compared with\nImagePC, VideoPC maintains high-quality facial corrections in space and\nmitigates the potential temporal shakes sequentially in blind scenarios.\nFinally, to establish an evaluation benchmark and train the framework, we\nestablish a video portrait dataset with a large diversity in the number of\npeople, lighting conditions, and background. Experiments demonstrate that the\nproposed methods outperform existing solutions quantitatively and\nqualitatively, contributing to high-fidelity wide-angle videos with stable and\nnatural portraits. The codes and dataset will be available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.00401", "cate": "cs.CV", "date": "2025-04-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.08713", "title": "ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning", "authors": ["Sahil Sethi", "David Chen", "Thomas Statchen", "Michael C. Burkhart", "Nipun Bhandari", "Bashar Ramadan", "Brett Beaulieu-Jones"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08713", "summary": "Deep learning-based electrocardiogram (ECG) classification has shown\nimpressive performance but clinical adoption has been slowed by the lack of\ntransparent and faithful explanations. Post hoc methods such as saliency maps\nmay fail to reflect a model's true decision process. Prototype-based reasoning\noffers a more transparent alternative by grounding decisions in similarity to\nlearned representations of real ECG segments, enabling faithful, case-based\nexplanations. We introduce ProtoECGNet, a prototype-based deep learning model\nfor interpretable, multi-label ECG classification. ProtoECGNet employs a\nstructured, multi-branch architecture that reflects clinical interpretation\nworkflows: it integrates a 1D CNN with global prototypes for rhythm\nclassification, a 2D CNN with time-localized prototypes for morphology-based\nreasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each\nbranch is trained with a prototype loss designed for multi-label learning,\ncombining clustering, separation, diversity, and a novel contrastive loss that\nencourages appropriate separation between prototypes of unrelated classes while\nallowing clustering for frequently co-occurring diagnoses. We evaluate\nProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating\ncompetitive performance relative to state-of-the-art black-box models while\nproviding structured, case-based explanations. To assess prototype quality, we\nconduct a structured clinician review of the final model's projected\nprototypes, finding that they are rated as representative and clear.\nProtoECGNet shows that prototype learning can be effectively scaled to complex,\nmulti-label time-series classification, offering a practical path toward\ntransparent and trustworthy deep learning models for clinical decision support.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08713", "cate": "cs.LG", "date": "2025-04-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.10018", "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework", "authors": ["Xiao Wang", "Haiyang Wang", "Shiao Wang", "Qiang Chen", "Jiandong Jin", "Haoyu Song", "Bo Jiang", "Chenglong Li"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10018", "summary": "Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10018", "cate": "cs.CV", "date": "2025-04-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.19822", "title": "MjÃ¶lnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density", "authors": ["Minjong Cheon"], "categories": ["cs.AI", "cs.CV", "cs.LG", "physics.ao-ph"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.19822", "summary": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.19822", "cate": "cs.LG", "date": "2025-04-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.01476", "title": "CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering", "authors": ["Zhe Zhang", "Mingxiu Cai", "Hanxiao Wang", "Gaochang Wu", "Tianyou Chai", "Xiatian Zhu"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01476", "summary": "Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an\ninput image with respect to normal samples. Either by reconstructing normal\ncounterparts (reconstruction-based) or by learning an image feature embedding\nspace (embedding-based), existing approaches fundamentally rely on image-level\nor feature-level matching to derive anomaly scores. Often, such a matching\nprocess is inaccurate yet overlooked, leading to sub-optimal detection. To\naddress this issue, we introduce the concept of cost filtering, borrowed from\nclassical matching tasks, such as depth and flow estimation, into the UAD\nproblem. We call this approach {\\em CostFilter-AD}. Specifically, we first\nconstruct a matching cost volume between the input and normal samples,\ncomprising two spatial dimensions and one matching dimension that encodes\npotential matches. To refine this, we propose a cost volume filtering network,\nguided by the input observation as an attention query across multiple feature\nlayers, which effectively suppresses matching noise while preserving edge\nstructures and capturing subtle anomalies. Designed as a generic\npost-processing plug-in, CostFilter-AD can be integrated with either\nreconstruction-based or embedding-based methods. Extensive experiments on\nMVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for\nboth single- and multi-class UAD tasks. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01476", "cate": "eess.IV", "date": "2025-05-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.03646", "title": "GRILL: Gradient Signal Restoration in Ill-Conditioned Layers to Enhance Adversarial Attacks on Autoencoders", "authors": ["Chethan Krishnamurthy Ramanaik", "Arjun Roy", "Tobias Callies", "Eirini Ntoutsi"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.03646", "summary": "Adversarial robustness of deep autoencoders (AEs) remains relatively\nunexplored, even though their non-invertible nature poses distinct challenges.\nExisting attack algorithms during the optimization of imperceptible,\nnorm-bounded adversarial perturbations to maximize output damage in AEs, often\nstop at sub-optimal attacks. We observe that the adversarial loss gradient\nvanishes when backpropagated through ill-conditioned layers. This issue arises\nfrom near-zero singular values in the Jacobians of these layers, which weaken\nthe gradient signal during optimization. We introduce GRILL, a technique that\nlocally restores gradient signals in ill-conditioned layers, enabling more\neffective norm-bounded attacks. Through extensive experiments on different\narchitectures of popular AEs, under both sample-specific and universal attack\nsetups, and across standard and adaptive attack settings, we show that our\nmethod significantly increases the effectiveness of our adversarial attacks,\nenabling a more rigorous evaluation of AE robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.03646", "cate": "cs.LG", "date": "2025-05-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.09742", "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization", "authors": ["Yuan-Hang Zhang", "Massimiliano Di Ventra"], "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09742", "summary": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09742", "cate": "cs.LG", "date": "2025-05-14", "updated": "2025-08-05", "section": "repl"}
{"id": "2505.15849", "title": "What Lives? A meta-analysis of diverse opinions on the definition of life", "authors": ["Reed Bender", "Karina Kofman", "Blaise AgÃ¼era y Arcas", "Michael Levin"], "categories": ["cs.AI", "cs.CY", "q-bio.BM", "q-bio.CB", "q-bio.OT", "q-bio.SC", "stat.AP"], "primary_category": "q-bio.OT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.15849", "summary": "The question of \"what is life?\" has challenged scientists and philosophers\nfor centuries, producing an array of definitions that reflect both the mystery\nof its emergence and the diversity of disciplinary perspectives brought to bear\non the question. Despite significant progress in our understanding of\nbiological systems, psychology, computation, and information theory, no single\ndefinition for life has yet achieved universal acceptance. This challenge\nbecomes increasingly urgent as advances in synthetic biology, artificial\nintelligence, and astrobiology challenge our traditional conceptions of what it\nmeans to be alive. We undertook a methodological approach that leverages large\nlanguage models (LLMs) to analyze a set of definitions of life provided by a\ncurated set of cross-disciplinary experts. We used a novel pairwise correlation\nanalysis to map the definitions into distinct feature vectors, followed by\nagglomerative clustering, intra-cluster semantic analysis, and t-SNE projection\nto reveal underlying conceptual archetypes. This methodology revealed a\ncontinuous landscape of the themes relating to the definition of life,\nsuggesting that what has historically been approached as a binary taxonomic\nproblem should be instead conceived as differentiated perspectives within a\nunified conceptual latent space. We offer a new methodological bridge between\nreductionist and holistic approaches to fundamental questions in science and\nphilosophy, demonstrating how computational semantic analysis can reveal\nconceptual patterns across disciplinary boundaries, and opening similar\npathways for addressing other contested definitional territories across the\nsciences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.15849", "cate": "q-bio.OT", "date": "2025-05-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.16227", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.16227", "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.16227", "cate": "cs.CL", "date": "2025-05-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.16888", "title": "CAIN: Hijacking LLM-Humans Conversations via Malicious System Prompts", "authors": ["Viet Pham", "Thai Le"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.16888", "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.16888", "cate": "cs.CR", "date": "2025-05-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.18601", "title": "Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18601", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18601", "cate": "cs.CL", "date": "2025-05-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.05683", "title": "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Kasra Borazjani", "Jacob Chakareski", "Seyyedali Hosseinalipour"], "categories": ["cs.AI", "cs.CR", "cs.LG", "cs.MM"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05683", "summary": "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05683", "cate": "cs.LG", "date": "2025-06-06", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.06382", "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "authors": ["MichaÅ P. Karpowicz"], "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.06382", "summary": "This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.06382", "cate": "stat.ML", "date": "2025-06-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.09733", "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "authors": ["Minjong Cheon"], "categories": ["cs.AI", "cs.CV", "cs.LG", "physics.ao-ph"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.09733", "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.09733", "cate": "cs.LG", "date": "2025-06-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.11049", "title": "15,500 Seconds: Lean UAV Classification Using EfficientNet and Lightweight Fine-Tuning", "authors": ["Andrew P. Berg", "Qian Zhang", "Mia Y. Wang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11049", "summary": "Unmanned Aerial Vehicles (UAVs) pose an escalating security concerns as the\nmarket for consumer and military UAVs grows. This paper address the critical\ndata scarcity challenges in deep UAV audio classification. We build upon our\nprevious work expanding novel approaches such as: parameter efficient\nfine-tuning, data augmentation, and pre-trained networks. We achieve\nperformance upwards of 95\\% validation accuracy with EfficientNet-B0.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11049", "cate": "cs.LG", "date": "2025-05-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.11127", "title": "UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions", "authors": ["Wenkang Han", "Zhixiong Zeng", "Jing Huang", "Shu Jiang", "Liming Zheng", "Haibo Qiu", "Chang Yao", "Jingyuan Chen", "Lin Ma"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11127", "summary": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing\nhuman-computer interaction, yet their reliance on text-based instructions\nimposes limitations on accessibility and convenience, particularly in\nhands-free scenarios. To address this issue, we propose replacing text with\nspeech as the instruction input modality for GUI agents, and introduce\nUITron-Speech, which is the first end-to-end GUI agent capable of directly\nprocessing speech instructions and on-device screenshots to predict user\nactions. To tackle the problem of data scarcity, we synthesize high-quality\nspeech instruction datasets using a random-speaker text-to-speech model.\nAdditionally, we design a mixed-modality training strategy to mitigate the\ninherent modality imbalance in pre-trained foundation models. Furthermore, we\nconduct a statistical analysis of the distribution of GUI grounding prediction\nerrors and propose a training-free two-step grounding refinement method to\nalleviate minor localization deviations. Extensive experiments on multiple\nbenchmarks demonstrate that UITron-Speech achieves robust performance and\nsuperior adaptability, underscoring the feasibility and potential of\nspeech-driven GUI agents for more accessible and intelligent human-computer\ninteraction. Our code and datasets are available at\nhttps://github.com/UITron-hub/UITron-Speech.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11127", "cate": "cs.CL", "date": "2025-06-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.19143", "title": "Thought Anchors: Which LLM Reasoning Steps Matter?", "authors": ["Paul C. Bogdan", "Uzay Macar", "Neel Nanda", "Arthur Conmy"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.19143", "summary": "Reasoning large language models have recently achieved state-of-the-art\nperformance in many fields. However, their long-form chain-of-thought reasoning\ncreates interpretability challenges as each generated token depends on all\nprevious ones, making the computation harder to decompose. We argue that\nanalyzing reasoning traces at the sentence level is a promising approach to\nunderstanding reasoning processes. We present three complementary attribution\nmethods: (1) a black-box method measuring each sentence's counterfactual\nimportance by comparing final answers across 100 rollouts conditioned on the\nmodel generating that sentence or one with a different meaning; (2) a white-box\nmethod of aggregating attention patterns between pairs of sentences, which\nidentified \"broadcasting\" sentences that receive disproportionate attention\nfrom all future sentences via \"receiver\" attention heads; (3) a causal\nattribution method measuring logical connections between sentences by\nsuppressing attention toward one sentence and measuring the effect on each\nfuture sentence's tokens. Each method provides evidence for the existence of\nthought anchors, reasoning steps that have outsized importance and that\ndisproportionately influence the subsequent reasoning process. These thought\nanchors are typically planning or backtracking sentences. We provide an\nopen-source tool (www.thought-anchors.com) for visualizing the outputs of our\nmethods, and present a case study showing converging patterns across methods\nthat map how a model performs multi-step reasoning. The consistency across\nmethods demonstrates the potential of sentence-level analysis for a deeper\nunderstanding of reasoning models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19143", "cate": "cs.LG", "date": "2025-06-23", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.21884", "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-ChacÃ³n", "Bernard Ghanem"], "categories": ["cs.AI", "cs.CV", "cs.LG", "eess.IV", "eess.SP"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21884", "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21884", "cate": "eess.IV", "date": "2025-06-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.03703", "title": "Sign Spotting Disambiguation using Large Language Models", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03703", "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03703", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.03958", "title": "A Comparative Study of Specialized LLMs as Dense Retrievers", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03958", "summary": "While large language models (LLMs) are increasingly deployed as dense\nretrievers, the impact of their domain-specific specialization on retrieval\neffectiveness remains underexplored. This investigation systematically examines\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\nan essential step toward developing unified retrievers capable of handling\ntext, code, images, and multimodal content. We conduct extensive experiments\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\ncode/math-specialized, long reasoning, and vision-language models across\nzero-shot retrieval settings and the supervised setting. For the zero-shot\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\nspecialization and the long reasoning capability cause consistent degradation\nin three settings, indicating conflicts between mathematical reasoning and\nsemantic matching. The vision-language model and code-specialized LLMs\ndemonstrate superior zero-shot performance compared to other LLMs, even\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\nto base LLMs in supervised settings. These findings suggest promising\ndirections for the unified retrieval task leveraging cross-domain and\ncross-modal fusion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03958", "cate": "cs.IR", "date": "2025-07-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.05116", "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting", "authors": ["Juyi Lin", "Amir Taherin", "Arash Akbari", "Arman Akbari", "Lei Lu", "Guangyu Chen", "Taskin Padir", "Xiaomeng Yang", "Weiwei Chen", "Yiqian Li", "Xue Lin", "David Kaeli", "Pu Zhao", "Yanzhi Wang"], "categories": ["cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05116", "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ncurrent VLA models suffer from two drawbacks: (i) generation of massive tokens\nleading to high inference latency and increased training cost, and (ii)\ninsufficient utilization of generated actions resulting in potential\nperformance loss. To address these issues, we develop a training framework to\nfinetune VLA models for generating significantly fewer action tokens with high\nparallelism, effectively reducing inference latency and training cost.\nFurthermore, we introduce an inference optimization technique with a novel\nvoting-based ensemble strategy to combine current and previous action\npredictions, improving the utilization of generated actions and overall\nperformance. Our results demonstrate that we achieve superior performance\ncompared with state-of-the-art VLA models, achieving significantly higher\nsuccess rates and 39$\\times$ faster inference than OpenVLA with 46 Hz\nthroughput on edge platforms, demonstrating practical deployability. The code\nis available at https://github.com/LukeLIN-web/VOTE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05116", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-08-05", "section": "repl"}
{"id": "2507.06043", "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations", "authors": ["Xiaohu Li", "Yunfeng Ning", "Zepeng Bao", "Mayi Xu", "Jianhao Chen", "Tieyun Qian"], "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06043", "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06043", "cate": "cs.CR", "date": "2025-07-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.06850", "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover", "authors": ["Matteo Lupinacci", "Francesco Aurelio Pironti", "Francesco Blefari", "Francesco Romeo", "Luigi Arena", "Angelo Furfaro"], "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06850", "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce unprecedented security\nvulnerabilities that extend beyond traditional content generation attacks to\nsystem-level compromise. This paper presents a comprehensive evaluation of the\nsecurity of LLMs used as reasoning engines within autonomous agents,\nhighlighting how they can be exploited as attack vectors capable of achieving\ncomplete computer takeover. We focus on how different attack surfaces and trust\nboundaries - Direct Prompt Injection, RAG Backdoor, and Inter Agent Trust - can\nbe leveraged to orchestrate such takeovers. We demonstrate that adversaries can\neffectively coerce popular LLMs (including GPT-4, Claude-4 and Gemini-2.5) into\nautonomously installing and executing malware on victim machines. Our\nevaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of\nmodels succumb to Direct Prompt Injection and 83.3% are vulnerable to the more\nstealth and evasive RAG Backdoor Attack. Notably, we tested trust boundaries\nwithin multi-agent systems, where LLM agents interact and influence each other,\nand we revealed a critical security flaw: LLMs which successfully resist direct\ninjection or RAG backdoor will execute identical payloads when requested by\npeer agents. Our findings show that 100.0% of tested LLMs can be compromised\nthrough Inter-Agent Trust Exploitation attacks and that every model exhibits\ncontext-dependent security behaviors that create exploitable blind spots. Our\nresults also highlight the need to increase awareness and research on the\nsecurity risks of LLMs, showing a paradigm shift in cybersecurity threats,\nwhere AI tools themselves become sophisticated attack vectors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06850", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.08841", "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation", "authors": ["Kun Jing", "Luoyu Chen", "Jungang Xu", "Jianwei Tai", "Yiyu Wang", "Shuaimin Li"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08841", "summary": "Neural architecture search (NAS) is a promising approach for automatically\ndesigning neural network architectures. However, the architecture estimation of\nNAS is computationally expensive and time-consuming because of training\nmultiple architectures from scratch. Although existing zero-shot NAS methods\nuse training-free proxies to accelerate the architecture estimation, their\neffectiveness, stability, and generality are still lacking. We present a novel\ntraining-free estimation proxy called weighted response correlation (WRCor).\nWRCor utilizes correlation coefficient matrices of responses across different\ninput samples to calculate the proxy scores of estimated architectures, which\ncan measure their expressivity and generalizability. Experimental results on\nproxy evaluation demonstrate that WRCor and its voting proxies are more\nefficient estimation strategies than existing proxies. We also apply them with\ndifferent search strategies in architecture search. Experimental results on\narchitecture search show that our zero-shot NAS algorithm outperforms most\nexisting NAS algorithms in different search spaces. Our NAS algorithm can\ndiscover an architecture with a 22.1% test error on the ImageNet-1k dataset\nwithin 4 GPU hours. All codes are publicly available at\nhttps://github.com/kunjing96/ZSNAS-WRCor.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08841", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.13414", "title": "Gauge Flow Models", "authors": ["Alexander Strunk", "Roland Assam"], "categories": ["cs.AI", "cs.LG", "math.DG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13414", "summary": "This paper introduces Gauge Flow Models, a novel class of Generative Flow\nModels. These models incorporate a learnable Gauge Field within the Flow\nOrdinary Differential Equation (ODE). A comprehensive mathematical framework\nfor these models, detailing their construction and properties, is provided.\nExperiments using Flow Matching on Gaussian Mixture Models demonstrate that\nGauge Flow Models yields significantly better performance than traditional Flow\nModels of comparable or even larger size. Additionally, unpublished research\nindicates a potential for enhanced performance across a broader range of\ngenerative tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13414", "cate": "cs.LG", "date": "2025-07-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.15807", "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context", "authors": ["Shuo Chen", "Jianzhe Liu", "Zhen Han", "Yan Xia", "Daniel Cremers", "Philip Torr", "Volker Tresp", "Jindong Gu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15807", "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15807", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.16136", "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization", "authors": ["Eduardo Pacheco", "Atila Orhon", "Berkin Durmus", "Blaise Munyampirwa", "Andrey Leonov"], "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16136", "summary": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16136", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.17937", "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", "authors": ["Jaechul Roh", "Zachary Novack", "Yuefeng Peng", "Niloofar Mireshghallah", "Taylor Berg-Kirkpatrick", "Amir Houmansadr"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17937", "summary": "Memorization in generative models extends far beyond verbatim text\nreproduction--it manifests through non-literal patterns, semantic associations,\nand surprisingly, across modalities in transcript-conditioned generation tasks\nsuch as Lyrics-to-Song (L2S) and Text-to-Video (T2V) models. We reveal a new\nclass of cross-modality memorization where models trained on these tasks leak\ncopyrighted content through indirect, phonetic pathways invisible to\ntraditional text-based analysis. In this work, we introduce Adversarial\nPhoneTic Prompting (APT), an attack that replaces iconic phrases with\nhomophonic alternatives--e.g., \"mom's spaghetti\" becomes \"Bob's\nconfetti\"--preserving the acoustic form while largely changing semantic\ncontent. We demonstrate that models can be prompted to regurgitate memorized\nsongs using phonetically similar but semantically unrelated lyrics. Despite the\nsemantic drift, black-box models like SUNO and open-source models like YuE\ngenerate outputs that are strikingly similar to the original\nsongs--melodically, rhythmically, and vocally--achieving high scores on\nAudioJudge, CLAP, and CoverID. These effects persist across genres and\nlanguages. More surprisingly, we find that phonetic prompts alone can trigger\nvisual memorization in text-to-video models: when given altered lyrics from\nLose Yourself, Veo 3 generates scenes that mirror the original music\nvideo--complete with a hooded rapper and dim urban settings--despite no\nexplicit visual cues in the prompt. This cross-modality leakage represents an\nunprecedented threat: models memorize deep, structural patterns that transcend\ntheir training modality, making traditional safety measures like copyright\nfilters ineffective. Our findings reveal a fundamental vulnerability in\ntranscript-conditioned generative models and raise urgent concerns around\ncopyright, provenance, and secure deployment of multimodal generation systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17937", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.20096", "title": "EcoTransformer: Attention without Multiplication", "authors": ["Xin Gao", "Xingming Xu", "Shirin Amiraslani", "Hong Xu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20096", "summary": "The Transformer, with its scaled dot-product attention mechanism, has become\na foundational architecture in modern AI. However, this mechanism is\ncomputationally intensive and incurs substantial energy costs. We propose a new\nTransformer architecture EcoTransformer, in which the output context vector is\nconstructed as the convolution of the values using a Laplacian kernel, where\nthe distances are measured by the L1 metric between the queries and keys.\nCompared to dot-product based attention, the new attention score calculation is\nfree of matrix multiplication. It performs on par with, or even surpasses,\nscaled dot-product attention in NLP, bioinformatics, and vision tasks, while\nconsuming significantly less energy.\n  (This version (v2) supersedes v1 and reflects the intended release and\nlicensing.)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20096", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.20968", "title": "From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation", "authors": ["Rongyao Cai", "Ming Jin", "Qingsong Wen", "Kexin Zhang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20968", "summary": "Domain shift poses a fundamental challenge in time series analysis, where\nmodels trained on source domain often fail dramatically when applied in target\ndomain with different yet similar distributions. While current unsupervised\ndomain adaptation (UDA) methods attempt to align cross-domain feature\ndistributions, they typically treat features as indivisible entities, ignoring\ntheir intrinsic compositions that govern domain adaptation. We introduce DARSD,\na novel UDA framework with theoretical explainability that explicitly realizes\nUDA tasks from the perspective of representation space decomposition. Our core\ninsight is that effective domain adaptation requires not just alignment, but\nprincipled disentanglement of transferable knowledge from mixed\nrepresentations. DARSD consists of three synergistic components: (I) An\nadversarial learnable common invariant basis that projects original features\ninto a domain-invariant subspace while preserving semantic content; (II) A\nprototypical pseudo-labeling mechanism that dynamically separates target\nfeatures based on confidence, hindering error accumulation; (III) A hybrid\ncontrastive optimization strategy that simultaneously enforces feature\nclustering and consistency while mitigating emerging distribution gaps.\nComprehensive experiments conducted on four benchmarks (WISDM, HAR, HHAR, and\nMFD) demonstrate DARSD's superiority against 12 UDA algorithms, achieving\noptimal performance in 35 out of 53 scenarios and ranking first across all\nbenchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20968", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.21167", "title": "ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions", "authors": ["Donglu Yang", "Liang Zhang", "Zihao Yue", "Liangyu Chen", "Yichen Xu", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21167", "summary": "Charts are a fundamental visualization format widely used in data analysis\nacross research and industry. While enabling users to edit charts based on\nhigh-level intentions is of great practical value, existing methods primarily\nrely on natural language instructions, which are often too ambiguous to support\nfine-grained editing. In this work, we introduce a novel paradigm for\nmultimodal chart editing, where user intent is expressed through a combination\nof natural language and visual indicators that explicitly highlight the\nelements to be modified. To support this paradigm, we present\nChart$\\text{M}^3$, a new benchmark for Multimodal chart editing with\nMulti-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$\ncontains 1,000 samples spanning four levels of editing difficulty. Each sample\nincludes triplets in the form of (chart, code, multimodal instructions). To\ncomprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides\nmetrics that assess both visual appearance and code correctness. Our benchmark\nreveals significant limitations in current multimodal large language models\n(MLLMs), including GPT-4o, particularly in their ability to interpret and act\non visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a\nlarge-scale training set with 24,000 multimodal chart editing samples.\nFine-tuning MLLMs on this dataset leads to substantial improvements,\ndemonstrating the importance of multimodal supervision in building practical\nchart editing systems. Our datasets, codes, and evaluation tools are available\nat https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our\ndatasets, codes, and evaluation tools are available at\nhttps://github.com/yaolinli/VCE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21167", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.21483", "title": "NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples", "authors": ["Shi Pu", "Fu Song", "Wenjie Wang"], "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21483", "summary": "Neural networks have received a lot of attention recently, and related\nsecurity issues have come with it. Many studies have shown that neural networks\nare vulnerable to adversarial examples that have been artificially perturbed\nwith modification, which is too small to be distinguishable by human\nperception. Different attacks and defenses have been proposed to solve these\nproblems, but there is little research on evaluating the robustness of neural\nnetworks and their inputs. In this work, we propose a metric called the neuron\ncover change rate (NCCR) to measure the ability of deep learning models to\nresist attacks and the stability of adversarial examples. NCCR monitors\nalterations in the output of specifically chosen neurons when the input is\nperturbed, and networks with a smaller degree of variation are considered to be\nmore robust. The results of the experiment on image recognition and the speaker\nrecognition model show that our metrics can provide a good assessment of the\nrobustness of neural networks or their inputs. It can also be used to detect\nwhether an input is adversarial or not, as adversarial examples are always less\nrobust.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21483", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01082", "title": "Learning Pivoting Manipulation with Force and Vision Feedback Using Optimization-based Demonstrations", "authors": ["Yuki Shirai", "Kei Ota", "Devesh K. Jha", "Diego Romeres"], "categories": ["cs.AI", "cs.LG", "cs.RO", "eess.SY"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01082", "summary": "Non-prehensile manipulation is challenging due to complex contact\ninteractions between objects, the environment, and robots. Model-based\napproaches can efficiently generate complex trajectories of robots and objects\nunder contact constraints. However, they tend to be sensitive to model\ninaccuracies and require access to privileged information (e.g., object mass,\nsize, pose), making them less suitable for novel objects. In contrast,\nlearning-based approaches are typically more robust to modeling errors but\nrequire large amounts of data. In this paper, we bridge these two approaches to\npropose a framework for learning closed-loop pivoting manipulation. By\nleveraging computationally efficient Contact-Implicit Trajectory Optimization\n(CITO), we design demonstration-guided deep Reinforcement Learning (RL),\nleading to sample-efficient learning. We also present a sim-to-real transfer\napproach using a privileged training strategy, enabling the robot to perform\npivoting manipulation using only proprioception, vision, and force sensing\nwithout access to privileged information. Our method is evaluated on several\npivoting tasks, demonstrating that it can successfully perform sim-to-real\ntransfer. The overview of our method and the hardware experiments are shown at\nhttps://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01082", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-05", "section": "repl"}
{"id": "2508.01396", "title": "Spatial-Frequency Aware for Object Detection in RAW Image", "authors": ["Zhuohua Ye", "Liming Zhang", "Hongru Han"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01396", "summary": "Direct RAW-based object detection offers great promise by utilizing RAW data\n(unprocessed sensor data), but faces inherent challenges due to its wide\ndynamic range and linear response, which tends to suppress crucial object\ndetails. In particular, existing enhancement methods are almost all performed\nin the spatial domain, making it difficult to effectively recover these\nsuppressed details from the skewed pixel distribution of RAW images. To address\nthis limitation, we turn to the frequency domain, where features, such as\nobject contours and textures, can be naturally separated based on frequency. In\nthis paper, we propose Space-Frequency Aware RAW Image Object Detection\nEnhancer (SFAE), a novel framework that synergizes spatial and frequency\nrepresentations. Our contribution is threefold. The first lies in the\n``spatialization\" of frequency bands. Different from the traditional paradigm\nof directly manipulating abstract spectra in deep networks, our method\ninversely transforms individual frequency bands back into tangible spatial\nmaps, thus preserving direct physical intuition. Then the cross-domain fusion\nattention module is developed to enable deep multimodal interactions between\nthese maps and the original spatial features. Finally, the framework performs\nadaptive nonlinear adjustments by predicting and applying different gamma\nparameters for the two domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01396", "cate": "cs.CV", "date": "2025-08-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02096", "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches", "authors": ["Raj Mahmud", "Yufeng Wu", "Abdullah Bin Sawad", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02096", "summary": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM. Our\nfindings reveal persistent limitations: post hoc surveys dominate, turn-level\naffective UX constructs are rarely assessed, and adaptive behaviours are seldom\nlinked to UX outcomes. LLM-based CRSs introduce further challenges, including\nepistemic opacity and verbosity, yet evaluations infrequently address these\nissues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02096", "cate": "cs.IR", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02609", "title": "Entity Representation Learning Through Onsite-Offsite Graph for Pinterest Ads", "authors": ["Jiayin Jin", "Zhimeng Pan", "Yang Tang", "Jiarui Feng", "Kungang Li", "Chongyuan Xiang", "Jiacheng Li", "Runze Su", "Siping Ji", "Han Sun", "Ling Leng", "Prathibha Deshikachar"], "categories": ["cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02609", "summary": "Graph Neural Networks (GNN) have been extensively applied to industry\nrecommendation systems, as seen in models like GraphSage\\cite{GraphSage},\nTwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were\nconstructed based on users' activities on the platforms, and various graph\nmodels were developed to effectively learn node embeddings. In addition to\nusers' onsite activities, their offsite conversions are crucial for Ads models\nto capture their shopping interest. To better leverage offsite conversion data\nand explore the connection between onsite and offsite activities, we\nconstructed a large-scale heterogeneous graph based on users' onsite ad\ninteractions and opt-in offsite conversion activities. Furthermore, we\nintroduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph\nEmbedding (KGE) model, to more efficiently integrate graph embeddings into Ads\nranking models. However, our Ads ranking models initially struggled to directly\nincorporate Knowledge Graph Embeddings (KGE), and only modest gains were\nobserved during offline experiments. To address this challenge, we employed the\nLarge ID Embedding Table technique and innovated an attention based KGE\nfinetuning approach within the Ads ranking models. As a result, we observed a\nsignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)\nprediction models. Moreover, this framework has been deployed in Pinterest's\nAds Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC\nreduction. We believe the techniques presented in this paper can be leveraged\nby other large-scale industrial models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02609", "cate": "cs.LG", "date": "2025-08-04", "updated": "2025-08-05", "section": "repl"}
{"id": "2508.02629", "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents", "authors": ["Yibin Liu", "Zhixuan Liang", "Zanxin Chen", "Tianxing Chen", "Mengkang Hu", "Wanxi Dong", "Congsheng Xu", "Zhaoming Han", "Yusen Qin", "Yao Mu"], "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02629", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02629", "cate": "cs.RO", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02753", "title": "DMSC: Dynamic Multi-Scale Coordination Framework for Time Series Forecasting", "authors": ["Haonan Yang", "Jianchao Tang", "Zhuo Li", "Long Lan"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02753", "summary": "Time Series Forecasting (TSF) faces persistent challenges in modeling\nintricate temporal dependencies across different scales. Despite recent\nadvances leveraging different decomposition operations and novel architectures\nbased on CNN, MLP or Transformer, existing methods still struggle with static\ndecomposition strategies, fragmented dependency modeling, and inflexible fusion\nmechanisms, limiting their ability to model intricate temporal dependencies. To\nexplicitly solve the mentioned three problems respectively, we propose a novel\nDynamic Multi-Scale Coordination Framework (DMSC) with Multi-Scale Patch\nDecomposition block (EMPD), Triad Interaction Block (TIB) and Adaptive Scale\nRouting MoE block (ASR-MoE). Specifically, EMPD is designed as a built-in\ncomponent to dynamically segment sequences into hierarchical patches with\nexponentially scaled granularities, eliminating predefined scale constraints\nthrough input-adaptive patch adjustment. TIB then jointly models intra-patch,\ninter-patch, and cross-variable dependencies within each layer's decomposed\nrepresentations. EMPD and TIB are jointly integrated into layers forming a\nmulti-layer progressive cascade architecture, where coarse-grained\nrepresentations from earlier layers adaptively guide fine-grained feature\nextraction in subsequent layers via gated pathways. And ASR-MoE dynamically\nfuses multi-scale predictions by leveraging specialized global and local\nexperts with temporal-aware weighting. Comprehensive experiments on thirteen\nreal-world benchmarks demonstrate that DMSC consistently maintains\nstate-of-the-art (SOTA) performance and superior computational efficiency for\nTSF tasks. Code is available at https://github.com/1327679995/DMSC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02753", "cate": "cs.LG", "date": "2025-08-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02762", "title": "Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment", "authors": ["Dahun Kim", "Anelia Angelova"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02762", "summary": "We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to\nenrich semantic representations in vision-language contrastive learning. Unlike\nstandard CLIP-style models that rely on a single text embedding, our method\nintroduces multiple structured prompts, each containing a distinct adaptive\ntoken that captures diverse semantic aspects of the input text. We leverage a\npretrained LLM as the text encoder within the CLIP framework, processing all\nprompts jointly in a single forward pass. The resulting prompt embeddings are\ncombined into a unified text representation, enabling semantically richer\nalignment with visual features. To further promote semantic diversity and\nrepresentation quality, we incorporate a diversity regularization loss and a\nnegation-aware loss, encouraging specialization across prompts and improving\ncontrastive discrimination. Our method achieves consistent improvements on both\nimage-text and video-text retrieval benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02762", "cate": "cs.LG", "date": "2025-08-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02879", "title": "CauKer: classification time series foundation models can be pretrained on synthetic data only", "authors": ["Shifeng Xie", "Vasilii Feofanov", "Marius Alonso", "Ambroise Odonnat", "Jianfeng Zhang", "Themis Palpanas", "Ievgen Redko"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02879", "summary": "Time series foundation models (TSFMs) have recently gained significant\nattention due to their strong zero-shot capabilities and widespread real-world\napplications. Such models typically require a computationally costly\npretraining on large-scale, carefully curated collections of real-world\nsequences. To allow for a sample-efficient pretraining of TSFMs, we propose\nCauKer, a novel algorithm designed to generate diverse, causally coherent\nsynthetic time series with realistic trends, seasonality, and nonlinear\ninteractions. CauKer combines Gaussian Process (GP) kernel composition with\nStructural Causal Models (SCM) to produce data for sample-efficient pretraining\nof state-of-the-art classification TSFMs having different architectures and\nfollowing different pretraining approaches. Additionally, our experiments\nreveal that CauKer-generated datasets exhibit clear scaling laws for both\ndataset size (10K to 10M samples) and model capacity (1M to 783M parameters),\nunlike real-world datasets, which display irregular scaling behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02879", "cate": "cs.LG", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03012", "title": "Tool-integrated Reinforcement Learning for Repo Deep Search", "authors": ["Zexiong Ma", "Chao Peng", "Qunhong Zeng", "Pengfei Gao", "Yanzhen Zou", "Bing Xie"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03012", "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03012", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03127", "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery", "authors": ["Sai Ma", "Zhuang Li", "John A Taylor"], "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03127", "summary": "Vision language models (VLMs) that enable natural language interaction with\nsatellite imagery can democratize Earth observation by accelerating expert\nworkflows, making data accessible to non-specialists, and enabling planet-scale\nautomation. However, existing datasets focus mainly on short-term,\nhigh-resolution imagery from a limited number of satellites, overlooking\nlow-resolution, multi-satellite, long-term archives, such as Landsat, that are\nessential for affordable and bias-robust global monitoring. We address this gap\nwith Landsat30-AU, a large-scale vision-language dataset built from 30-meter\nresolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over\nAustralia, spanning more than 36 years. The dataset includes two components:\nLandsat30-AU-Cap, containing $196,262$ image-caption pairs, and\nLandsat30-AU-VQA, comprising 17,725 human-verified visual question answering\n(VQA) samples across eight remote sensing domains. Both datasets are curated\nthrough a bootstrapped pipeline that leverages generic VLMs with iterative\nrefinement and human verification to ensure quality. Our evaluation of eight\nVLMs on our benchmark reveals that off-the-shelf models struggle to understand\nsatellite imagery. The open-source remote-sensing VLM EarthDial achieves only\n0.07 SPIDEr in captioning and a VQA accuracy of 0.48, highlighting the\nlimitations of current approaches. Encouragingly, lightweight fine-tuning of\nQwen2.5-VL-7B on Landsat30-AU improves captioning performance from 0.11 to 0.31\nSPIDEr and boosts VQA accuracy from 0.74 to 0.87. Code and data are available\nat https://github.com/papersubmit1/landsat30-au.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03127", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03306", "title": "Reliable Evaluation Protocol for Low-Precision Retrieval", "authors": ["Kisu Yang", "Yoonna Jang", "Hwanseok Jang", "Kenneth Choi", "Isabelle Augenstein", "Heuiseok Lim"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03306", "summary": "Lowering the numerical precision of model parameters and computations is\nwidely adopted to improve the efficiency of retrieval systems. However, when\ncomputing relevance scores between the query and documents in low-precision, we\nobserve spurious ties due to the reduced granularity. This introduces high\nvariability in the results based on tie resolution, making the evaluation less\nreliable. To address this, we propose a more robust retrieval evaluation\nprotocol designed to reduce score variation. It consists of: (1) High-Precision\nScoring (HPS), which upcasts the final scoring step to higher precision to\nresolve tied candidates with minimal computational cost; and (2) Tie-aware\nRetrieval Metrics (TRM), which report expected scores, range, and bias to\nquantify order uncertainty of tied candidates. Our experiments test multiple\nmodels with three scoring functions on two retrieval datasets to demonstrate\nthat HPS dramatically reduces tie-induced instability, and TRM accurately\nrecovers expected metric values. This combination enables a more consistent and\nreliable evaluation system for lower-precision retrievals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03306", "cate": "cs.IR", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03329", "title": "Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach", "authors": ["Mari Ashiga", "Vardan Voskanyan", "Fateme Dinmohammadi", "Jingzhi Gong", "Paul Brookes", "Matthew Truscott", "Rafail Giavrimis", "Mike Basios", "Leslie Kanthan", "Wei Jie"], "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03329", "summary": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03329", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03440", "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "authors": ["ChÃ¼nhung Wu", "Jinliang Lu", "Zixuan Ren", "Gangqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03440", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03440", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03543", "title": "EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering", "authors": ["Tianxin Xie", "Shan Yang", "Chenxing Li", "Dong Yu", "Li Liu"], "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03543", "summary": "Text-to-speech (TTS) has shown great progress in recent years. However, most\nexisting TTS systems offer only coarse and rigid emotion control, typically via\ndiscrete emotion labels or a carefully crafted and detailed emotional text\nprompt, making fine-grained emotion manipulation either inaccessible or\nunstable. These models also require extensive, high-quality datasets for\ntraining. To address these limitations, we propose EmoSteer-TTS, a novel\ntraining-free approach, to achieve fine-grained speech emotion control\n(conversion, interpolation, erasure) by activation steering. We first\nempirically observe that modifying a subset of the internal activations within\na flow matching-based TTS model can effectively alter the emotional tone of\nsynthesized speech. Building on this insight, we then develop a training-free\nand efficient algorithm, including activation extraction, emotional token\nsearching, and inference-time steering, which can be seamlessly integrated into\na wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In\naddition, to derive effective steering vectors, we construct a curated\nemotional speech dataset with diverse speakers. Extensive experiments\ndemonstrate that EmoSteer-TTS enables fine-grained, interpretable, and\ncontinuous control over speech emotion, outperforming the state-of-the-art\n(SOTA). To the best of our knowledge, this is the first method that achieves\ntraining-free and continuous fine-grained emotion control in TTS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03543", "cate": "cs.SD", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03546", "title": "Supervised Dynamic Dimension Reduction with Deep Neural Network", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03546", "summary": "This paper studies the problem of dimension reduction, tailored to improving\ntime series forecasting with high-dimensional predictors. We propose a novel\nSupervised Deep Dynamic Principal component analysis (SDDP) framework that\nincorporates the target variable and lagged observations into the factor\nextraction process. Assisted by a temporal neural network, we construct\ntarget-aware predictors by scaling the original predictors in a supervised\nmanner, with larger weights assigned to predictors with stronger forecasting\npower. A principal component analysis is then performed on the target-aware\npredictors to extract the estimated SDDP factors. This supervised factor\nextraction not only improves predictive accuracy in the downstream forecasting\ntask but also yields more interpretable and target-specific latent factors.\nBuilding upon SDDP, we propose a factor-augmented nonlinear dynamic forecasting\nmodel that unifies a broad family of factor-model-based forecasting approaches.\nTo further demonstrate the broader applicability of SDDP, we extend our studies\nto a more challenging scenario when the predictors are only partially\nobservable. We validate the empirical performance of the proposed method on\nseveral real-world public datasets. The results show that our algorithm\nachieves notable improvements in forecasting accuracy compared to\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03546", "cate": "stat.ML", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03666", "title": "Beyond risk: A proto-framework for assessing the societal impact of AI systems", "authors": ["Willem Fourie"], "categories": ["cs.AI", "cs.CY", "cs.ET"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03666", "summary": "In the discourse on AI regulation, 'responsible AI' is the dominant paradigm,\nwith the focus on mitigating the risks related to AI systems. While this focus\nis important and necessary, it has limited use for a systematic consideration\nof AI's societal impact. This paper proposes a proto-framework for assessing\nthe societal impact of AI systems by operationalising the concept of freedom.\nThis proto-framework is intended as a step towards a fully operationalised\nframework to be used in policymaking contexts. By drawing on Kantian philosophy\nand related contemporary interpretations, freedom is developed as the\ncounterpart to the concept of responsibility. Two dimensions of freedom are\ndeveloped in further detail: freedom as capability and freedom as opportunity.\nThese two dimensions of freedom are then applied in a proto-framework that\nsystematically considers AI's impact on society using the Sustainable\nDevelopment Goals. This proto-framework aims to complement current risk-based\napproaches and thereby offers a first step towards operationalising the concept\nof freedom in AI regulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03666", "cate": "cs.CY", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03682", "title": "Self-Questioning Language Models", "authors": ["Lili Chen", "Mihir Prabhudesai", "Katerina Fragkiadaki", "Hao Liu", "Deepak Pathak"], "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03682", "summary": "Can large language models improve without external data -- by generating\ntheir own questions and answers? We hypothesize that a pre-trained language\nmodel can improve its reasoning skills given only a single prompt specifying\nthe topic (e.g., algebra word problems) and asking the model to generate its\nown questions. To do this, we propose Self-Questioning Language Models (SQLM):\nan asymmetric self-play framework where a proposer is given the topic and\ngenerates a question for a solver, who tries to answer it. Both the proposer\nand solver are trained via reinforcement learning. The proposer receives a\nreward if the problem is not too easy or too difficult, and the solver receives\na reward based on majority voting, a proxy for correctness in the absence of\nground-truth answers. For coding, the proposer can instead generate unit tests\nwhich are used for verification. We study this asymmetric self-play framework\non three benchmarks: three-digit multiplication, algebra problems from the\nOMEGA benchmark, and programming problems from Codeforces. By continually\ngenerating more interesting problems and attempting to solve them, language\nmodels can improve on downstream benchmarks without access to any curated\ntraining datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03682", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03837", "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems", "authors": ["Davide Zoni", "Andrea Galimberti", "Adriano Guarisco"], "categories": ["cs.AR"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03837v1", "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03837v1", "cate": "cs.AR", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03866", "title": "FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead", "authors": ["Seock-Hwan Noh", "Hoyeon Lee", "Junkyum Kim", "Junsu Im", "Jay H. Park", "Sungjin Lee", "Sam H. Noh", "Yeseong Kim", "Jaeha Kung"], "categories": ["cs.AR"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03866v1", "summary": "We present FlashVault, an in-NAND self-encryption architecture that embeds a\nreconfigurable cryptographic engine into the unused silicon area of a\nstate-of-the-art 4D V-NAND structure. FlashVault supports not only block\nciphers for data encryption but also public-key and post-quantum algorithms for\ndigital signatures, all within the NAND flash chip. This design enables each\nNAND chip to operate as a self-contained enclave without incurring area\noverhead, while eliminating the need for off-chip encryption. We implement\nFlashVault at the register-transfer level (RTL) and perform place-and-route\n(P&R) for accurate power/area evaluation. Our analysis shows that the power\nbudget determines the number of cryptographic engines per NAND chip. We\nintegrate this architectural choice into a full-system simulation and evaluate\nits performance on a wide range of cryptographic algorithms. Our results show\nthat FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x)\nand near-core processing architecture (1.02~2.01x), demonstrating its\neffectiveness as a secure SSD architecture that meets diverse cryptographic\nrequirements imposed by regulatory standards and enterprise policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03866v1", "cate": "cs.AR", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03900", "title": "TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads", "authors": ["Navaneeth Kunhi Purayil", "Diyou Shen", "Matteo Perotti", "Luca Benini"], "categories": ["cs.AR"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03900v1", "summary": "The fast evolution of Machine Learning (ML) models requires flexible and\nefficient hardware solutions as hardwired accelerators face rapid obsolescence.\nVector processors are fully programmable and achieve high energy efficiencies\nby exploiting data parallelism, amortizing instruction fetch and decoding\ncosts. Hence, a promising design choice is to build accelerators based on\nshared L1-memory clusters of streamlined Vector Processing Elements (VPEs).\nHowever, current state-of-the-art VPEs are limited in L1 memory bandwidth and\nachieve high efficiency only for computational kernels with high data reuse in\nthe Vector Register File (VRF), such as General Matrix Multiplication (GEMM).\nPerformance is suboptimal for workloads with lower data reuse like General\nMatrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at\nthe L1 memory interface, the VPE micro-architecture must be optimized to\nachieve near-ideal utilization, i.e., to be as close as possible to the L1\nmemory roofline (at-the-roofline). In this work, we propose TROOP, a set of\nhardware optimizations that include decoupled load-store interfaces, improved\nvector chaining, shadow buffers to hide VRF conflicts, and address scrambling\ntechniques to achieve at-the-roofline performance for VPEs without compromising\ntheir area and energy efficiency. We implement TROOP on an open-source\nstreamlined vector processor in a 12nm FinFET technology. TROOP achieves\nsignificant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key\nmemory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline\nperformance. Additionally, TROOP enhances the energy efficiency by up to 45%,\nreaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high\nenergy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area\noverhead of less than 7%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03900v1", "cate": "cs.AR", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04106", "title": "OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite", "authors": ["Shan Shen", "Xingyang Li", "Zhuohua Liu", "Yikai Wang", "Yiheng Wu", "Junhao Ma", "Yuquan Sun", "Wei W. Xing"], "categories": ["cs.AR"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04106v1", "summary": "Static Random-Access Memory (SRAM) yield analysis is essential for\nsemiconductor innovation, yet research progress faces a critical challenge: the\nsignificant disconnect between simplified academic models and complex\nindustrial realities. The absence of open, realistic benchmarks has created a\nreproducibility crisis, where promising academic techniques often fail to\ntranslate to industrial practice. We present \\textit{OpenYield}, a\ncomprehensive open-source ecosystem designed to address this critical gap\nthrough three core contributions: (1) A realistic SRAM circuit generator that\nuniquely incorporates critical second-order-effect parasitics, inter-cell\nleakage coupling, and peripheral circuit variations, which are typically\nomitted in academic studies but decisive in industrial designs. (2) A\nstandardized evaluation platform with a simple interface and implemented\nbaseline yield analysis algorithms, enabling fair comparisons and reproducible\nresearch. (3) A standardized SRAM optimization platform, demonstrating\nOpenYield's utility in enhancing SRAM design robustness and efficiency,\nproviding a comprehensive benchmark for optimization algorithms. OpenYield\ncreates a foundation for meaningful academia-industry collaboration,\naccelerating innovation in memory design. The framework is publicly available\non \\href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04106v1", "cate": "cs.AR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04516", "title": "ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs", "authors": ["Ishraq Tashdid", "Dewan Saiham", "Nafisa Anjum", "Tasnuva Farheen", "Sazadur Rahman"], "categories": ["cs.AR", "cs.ET"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04516v1", "summary": "Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs\namong performance, flexibility, and sustainability. ASICs provide high\nefficiency but are inflexible post-fabrication, require costly re-spins for\nupdates, and expose IPs to piracy risks. FPGAs offer reconfigurability and\nreuse, yet suffer from substantial area, power, and performance overheads,\nresulting in higher carbon footprints. We present ECOLogic, a hybrid design\nparadigm that embeds lightweight eFPGA fabric within ASICs to enable secure,\nupdatable, and resource-aware computation. Central to this architecture is\nECOScore, a quantitative scoring framework that evaluates IPs based on\nadaptability, piracy threat, performance tolerance, and resource fit to guide\nRTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an\naverage of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns\ntiming slack (versus 5.1 ns in FPGA), and reduces power by 480 times on\naverage. Moreover, sustainability analysis shows a 99.7 percent reduction in\ndeployment carbon footprint and 300 to 500 times lower emissions relative to\nFPGA-only implementations. These results position ECOLogic as a\nhigh-performance, secure, and environmentally sustainable solution for\nnext-generation reconfigurable systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04516v1", "cate": "cs.AR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04609", "title": "Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems", "authors": ["Osama Abdelaleim", "Arun Prakash", "Ayhan Irfanoglu", "Veljko Milutinovic"], "categories": ["cs.AR"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04609v1", "summary": "Accelerating the solution of linear systems of equations is critical due to\ntheir central role in numerous applications, such as scientific simulations,\ndata analytics, and machine learning. This paper presents a general-purpose\nanalog direct solver circuit designed to accelerate the solution of positive\ndefinite symmetric linear systems of equations. The proposed design leverages\nnon-inverting operational amplifier configurations to create a negative\nresistance circuit, effectively modeling any symmetric system. The paper\ndetails the principles behind the design, optimizations of the system\narchitecture, and numerical results that demonstrate the robustness of the\ndesign. The findings reveal that the proposed system solves diagonally dominant\nsymmetric matrices with O(1) complexity, achieving the theoretical maximum\nspeed as the circuit relies solely on resistors. For non-diagonally dominant\nsymmetric positive-definite systems, the solution speed depends on matrix\nproperties such as eigenvalues and the maximum off-diagonal term, but remains\nindependent of matrix size.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04609v1", "cate": "cs.AR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04214", "title": "Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems", "authors": ["Yasaman Khorsandmanesh", "Emil BjÃ¶rnson", "Joakim JaldÃ©n", "Bengt Lindoff"], "categories": ["cs.AR", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04214", "summary": "This paper considers a millimeter-wave wideband point-to-point MIMO system\nwith fully digital transceivers at the base station and the user equipment\n(UE), focusing on mobile UE scenarios. A main challenge when building a digital\nUE combining is the large volume of baseband samples to handle. To mitigate\ncomputational and hardware complexity, we propose a novel two-stage digital\ncombining scheme at the UE. The first stage reduces the $N_{\\text{r}}$ received\nsignals to $N_{\\text{c}}$ streams before baseband processing, leveraging\nchannel geometry for dimension reduction and updating at the beam coherence\ntime, which is longer than the channel coherence time of the small-scale\nfading. By contrast, the second-stage combining is updated per fading\nrealization. We develop a pilot-based channel estimation framework for this\nhardware setup based on maximum likelihoodestimation in both uplink and\ndownlink. Digital precoding and combining designs are proposed, and a spectral\nefficiency expression that incorporates imperfect channel knowledge is derived.\nThe numerical results demonstrate that the proposed approach outperforms hybrid\nbeamforming, showcasing the attractiveness of using two-stage fully digital\ntransceivers in future systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04214", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2504.06211", "title": "Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs", "authors": ["Alhad Daftardar", "Jianqiao Mo", "Joey Ah-kiow", "Benedikt BÃ¼nz", "Ramesh Karri", "Siddharth Garg", "Brandon Reagen"], "categories": ["cs.AR", "cs.CR"], "primary_category": "cs.AR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06211", "summary": "Zero-Knowledge Proofs (ZKPs) are rapidly gaining importance in\nprivacy-preserving and verifiable computing. ZKPs enable a proving party to\nprove the truth of a statement to a verifying party without revealing anything\nelse. ZKPs have applications in blockchain technologies, verifiable machine\nlearning, and electronic voting, but have yet to see widespread adoption due to\nthe computational complexity of the proving process. Recent works have\naccelerated the key primitives of state-of-the-art ZKP protocols on GPU and\nASIC. However, the protocols accelerated thus far face one of two challenges:\nthey either require a trusted setup for each application, or they generate\nlarger proof sizes with higher verification costs, limiting their applicability\nin scenarios with numerous verifiers or strict verification time constraints.\nThis work presents an accelerator, zkSpeed, for HyperPlonk, a state-of-the-art\nZKP protocol that supports both one-time, universal setup and small proof sizes\nfor typical ZKP applications in publicly verifiable, consensus-based systems.\nWe accelerate the entire protocol, including two major primitives: SumCheck and\nMulti-scalar Multiplications (MSMs). We develop a full-chip architecture using\n366.46 mm$^2$ and 2 TB/s of bandwidth to accelerate the entire proof generation\nprocess, achieving geometric mean speedups of 801$\\times$ over CPU baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06211", "cate": "cs.AR", "date": "2025-04-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2312.08617", "title": "RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution", "authors": ["Shang Liu", "Wenji Fang", "Yao Lu", "Qijun Zhang", "Hongce Zhang", "Zhiyao Xie"], "categories": ["cs.AR", "cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.08617", "summary": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.08617", "cate": "cs.PL", "date": "2023-12-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2405.15877", "title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications", "authors": ["Yang Li", "Daniel Agyei Asante", "Changsheng Zhao", "Ernie Chang", "Yangyang Shi", "Vikas Chandra"], "categories": ["cs.AR", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.15877", "summary": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.15877", "cate": "cs.LG", "date": "2024-05-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2501.06663", "title": "Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization", "authors": ["Jiayi Tian", "Jinming Lu", "Hai Li", "Xiangwei Wang", "Cong Hao", "Ian Young", "Zheng Zhang"], "categories": ["cs.AR", "cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.06663", "summary": "Transformer models have achieved state-of-the-art performance across a wide\nrange of machine learning tasks. There is growing interest in training\ntransformers on resource-constrained edge devices due to considerations such as\nprivacy, domain adaptation, and on-device scientific machine learning. However,\nthe significant computational and memory demands required for transformer\ntraining often exceed the capabilities of an edge device. Leveraging low-rank\ntensor compression, this paper presents the first on-FPGA accelerator for\nend-to-end transformer training. On the algorithm side, we present a\nbi-directional contraction flow for tensorized transformer training,\nsignificantly reducing the computational FLOPS and intra-layer memory costs\ncompared to existing tensor operations. On the hardware side, we store all\nhighly compressed model parameters and gradient information on chip, creating\nan on-chip-memory-only framework for each stage in training. This reduces\noff-chip communication and minimizes latency and energy costs. Additionally, we\nimplement custom computing kernels for each training stage and employ\nintra-layer parallelism and pipe-lining to further enhance run-time and memory\nefficiency. Through experiments on transformer models within $36.7$ to $93.5$\nMB using FP-32 data formats on the ATIS dataset, our tensorized FPGA\naccelerator could conduct single-batch end-to-end training on the AMD Alevo U50\nFPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM.\nCompared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA\ntraining achieves a memory reduction of $30\\times$ to $51\\times$. Our FPGA\naccelerator also achieves up to $3.6\\times$ less energy cost per epoch compared\nwith tensor Transformer training on an NVIDIA RTX 3090 GPU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06663", "cate": "cs.LG", "date": "2025-01-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.09485", "title": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal Circuit Encoder-Decoder Alignment", "authors": ["Wenji Fang", "Jing Wang", "Yao Lu", "Shang Liu", "Zhiyao Xie"], "categories": ["cs.AR", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.09485", "summary": "The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit foundation models are typically\nlimited to standalone encoders for predictive tasks or decoders for generative\ntasks. These two model types are developed independently, operate on different\ncircuit modalities, and reside in separate latent spaces. This restricts their\nability to complement each other for more advanced capabilities. In this work,\nwe present GenEDA, the first framework that cross-modally aligns circuit\nencoders with decoders within a shared latent space. GenEDA bridges the gap\nbetween graph-based circuit representation learning and text-based large\nlanguage models (LLMs), enabling communication between their respective latent\nspaces. To achieve the alignment, we propose two paradigms to support both\nopen-source trainable LLMs and commercial frozen LLMs. We leverage this aligned\narchitecture to develop the first generative foundation model for netlists,\nunleashing LLMs' generative reasoning capability on the low-level and\nbit-blasted netlists. GenEDA enables three unprecedented generative netlist\nfunctional reasoning tasks, where it reversely generates high-level\nfunctionalities such as specifications and RTL code from low-level netlists.\nThese tasks move beyond traditional gate function classification to direct\ngeneration of full-circuit functionality. Experiments demonstrate that GenEDA\nsignificantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance\nin all tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.09485", "cate": "cs.LG", "date": "2025-04-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04133", "title": "Polynomial-time sampling despite disorder chaos", "authors": ["Eric Ma", "Tselil Schramm"], "categories": ["cs.CC", "cs.DS", "math.CO", "math.PR"], "primary_category": "cs.CC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04133v1", "summary": "A distribution over instances of a sampling problem is said to exhibit\ntransport disorder chaos if perturbing the instance by a small amount of random\nnoise dramatically changes the stationary distribution (in Wasserstein\ndistance). Seeking to provide evidence that some sampling tasks are hard on\naverage, a recent line of work has demonstrated that disorder chaos is\nsufficient to rule out \"stable\" sampling algorithms, such as gradient methods\nand some diffusion processes.\n  We demonstrate that disorder chaos does not preclude polynomial-time sampling\nby canonical algorithms in canonical models. We show that with high probability\nover a random graph $\\boldsymbol{G} \\sim G(n,1/2)$: (1) the hardcore model (at\nfugacity $\\lambda = 1$) on $\\boldsymbol{G}$ exhibits disorder chaos, and (2)\nGlauber dynamics run for $O(n)$ time can approximately sample from the hardcore\nmodel on $\\boldsymbol{G}$ (in Wasserstein distance).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04133v1", "cate": "cs.CC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03857", "title": "A 60-Addition, Rank-23 Scheme for Exact 3x3 Matrix Multiplication", "authors": ["Joshua Stapleton"], "categories": ["cs.CC", "cs.DS", "math.NA"], "primary_category": "cs.DS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03857", "summary": "We reduce the additive cost of general (non-commutative) 3x3 matrix\nmultiplication from the previous records of 61 (Schwartz-Vaknin, 2023) and 62\n(Martensson-Wagner, 2025) to 60 without a change of basis. To our knowledge,\nthis represents a new state-of-the-art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03857", "cate": "cs.DS", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04486", "title": "Quantum circuit complexity and unsupervised machine learning of topological order", "authors": ["Yanming Che", "Clemens Gneiting", "Xiaoguang Wang", "Franco Nori"], "categories": ["cond-mat.dis-nn", "cs.CC", "cs.IT", "cs.LG", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04486", "summary": "Inspired by the close relationship between Kolmogorov complexity and\nunsupervised machine learning, we explore quantum circuit complexity, an\nimportant concept in quantum computation and quantum information science, as a\npivot to understand and to build interpretable and efficient unsupervised\nmachine learning for topological order in quantum many-body systems. To span a\nbridge from conceptual power to practical applicability, we present two\ntheorems that connect Nielsen's quantum circuit complexity for the quantum path\nplanning between two arbitrary quantum many-body states with fidelity change\nand entanglement generation, respectively. Leveraging these connections,\nfidelity-based and entanglement-based similarity measures or kernels, which are\nmore practical for implementation, are formulated. Using the two proposed\nkernels, numerical experiments targeting the unsupervised clustering of quantum\nphases of the bond-alternating XXZ spin chain, the ground state of Kitaev's\ntoric code and random product states, are conducted, demonstrating their\nsuperior performance. Relations with classical shadow tomography and shadow\nkernel learning are also discussed, where the latter can be naturally derived\nand understood from our approach. Our results establish connections between key\nconcepts and tools of quantum circuit computation, quantum complexity, and\nmachine learning of topological quantum order.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04486", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2502.10103", "title": "Membership and Conjugacy in Inverse Semigroups", "authors": ["Lukas Fleischer", "Florian Stober", "Alexander Thumm", "Armin WeiÃ"], "categories": ["cs.CC", "cs.FL", "math.GR"], "primary_category": "cs.CC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.10103", "summary": "The membership problem for an algebraic structure asks whether a given\nelement is contained in some substructure, which is usually given by\ngenerators. In this work we study the membership problem, as well as the\nconjugacy problem, for finite inverse semigroups. The closely related\nmembership problem for finite semigroups has been shown to be PSPACE-complete\nin the transformation model by Kozen (1977) and NL-complete in the Cayley table\nmodel by Jones, Lien, and Laaser (1976). In the partial bijection model, the\nmembership and the conjugacy problem for finite inverse semigroups were shown\nto be PSPACE-complete by Birget and Margolis (2008) and by Jack (2023).\n  Here we present a more detailed analysis of the complexity of the membership\nand conjugacy problems parametrized by varieties of finite inverse semigroups.\nWe establish dichotomy theorems for the partial bijection model and for the\nCayley table model. In the partial bijection model these problems are in NC\n(resp. NP for conjugacy) for strict inverse semigroups and PSPACE-complete\notherwise. In the Cayley table model we obtain general LOGSPACE-algorithms as\nwell as NPOLYLOGTIME upper bounds for Clifford semigroups and\nLOGSPACE-completeness otherwise.\n  Furthermore, by applying our findings, we show the following: the\nintersection non-emptiness problem for inverse automata is PSPACE-complete even\nfor automata with only two states; the subpower membership problem is in NC for\nevery strict inverse semi-group and PSPACE-complete otherwise; the minimum\ngenerating set and the equation satisfiability problems are in NP for varieties\nof finite strict inverse semigroups and PSPACE-complete otherwise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.10103", "cate": "cs.CC", "date": "2025-02-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04014", "title": "Tunable Plasmonic Absorption in Metal-Dielectric Multilayers via FDTD Simulations and an Explainable Machine Learning Approach", "authors": ["Emmanuel A. Bamidele"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04014v1", "summary": "Plasmonic devices, fundamental to modern nanophotonics, exploit resonant\ninteractions between light and free electrons in metals to achieve enhanced\nlight trapping and electromagnetic field confinement. However, modeling their\ncomplex, nonlinear optical responses remains computationally intensive. In this\nwork, we combine finite-difference time-domain simulations with machine\nlearning to simulate and predict absorbed power behavior in multilayer\nplasmonic stacks composed of SiO2, gold, silver, and indium tin oxide. By\nvarying Au and Ag thicknesses (10-50nm) across a spectral range of 300-1500nm,\nwe generate spatial absorption maps and integrated power metrics from full-wave\nsolutions to Maxwell's equations. A multilayer perceptron models global\nabsorption behavior with a mean absolute error of 0.0953, while a convolutional\nneural network predicts spatial absorption distributions with an MAE of 0.0101.\nSHapley Additive exPlanations identify plasmonic layer thickness and excitation\nwavelength as dominant contributors to absorption, which peaks between 450 and\n850~nm. Gold demonstrates broader and more sustained absorption compared to\nsilver, although both metals exhibit reduced efficiency outside the resonance\nwindow. This integrated FDTD-ML framework offers a fast, explainable, and\naccurate approach for investigating tunable plasmonic behavior in multilayer\nsystems, with applications in optical sensing, photovoltaics, and nanophotonic\ndevice design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04014v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04076", "title": "A GPU-Accelerated Three-Dimensional Crack Element Method for Transient Dynamic Fracture Simulation", "authors": ["Yuxi Xie", "C.T. Wu", "Wei Hu", "Lu Xu", "Tinh Q. Bui", "Shaofan Li"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04076v1", "summary": "This work presents a novel three-dimensional Crack Element Method (CEM)\ndesigned to model transient dynamic crack propagation in quasi-brittle\nmaterials efficiently. CEM introduces an advanced element-splitting algorithm\nthat enables element-wise crack growth, including crack branching. Based on the\nevolving topology of split elements, an original formulation for computing the\nfracture energy release rate in three dimensions is derived. A series of\nbenchmark examples is conducted to demonstrate that the proposed 3D CEM\naccurately simulates both single crack propagation and complex crack branching\nscenarios. Furthermore, all three-dimensional simulations are GPU-accelerated,\nachieving high levels of computational efficiency, consistency, and accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04076v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04084", "title": "Convolutional autoencoders for the reconstruction of three-dimensional interfacial multiphase flows", "authors": ["Murray Cutforth", "Shahab Mirjalili"], "categories": ["cs.CE", "cs.LG", "physics.flu-dyn"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04084v1", "summary": "In this work, we perform a comprehensive investigation of autoencoders for\nreduced-order modeling of three-dimensional multiphase flows. Focusing on the\naccuracy of reconstructing multiphase flow volume/mass fractions with a\nstandard convolutional architecture, we examine the advantages and\ndisadvantages of different interface representation choices (diffuse, sharp,\nlevel set). We use a combination of synthetic data with non-trivial interface\ntopologies and high-resolution simulation data of multiphase homogeneous\nisotropic turbulence for training and validation. This study clarifies the best\npractices for reducing the dimensionality of multiphase flows via autoencoders.\nConsequently, this paves the path for uncoupling the training of autoencoders\nfor accurate reconstruction and the training of temporal or input/output models\nsuch as neural operators (e.g., FNOs, DeepONets) and neural ODEs on the\nlower-dimensional latent space given by the autoencoders. As such, the\nimplications of this study are significant and of interest to the multiphase\nflow community and beyond.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04084v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04157", "title": "A Generic Framework for Optimization in Blockchain Simulators", "authors": ["Hou-Wan Long", "Yujun Pan", "Xiongfei Zhao", "Yain-Whar Si"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04157v1", "summary": "As blockchain technology rapidly evolves, researchers face a significant\nchallenge due to diverse and non-standardized simulation parameters, which\nhinder the replicability and comparability of research methodologies. This\npaper introduces a Generic Framework for Optimization in Blockchain Simulators\n(GFOBS), a comprehensive and adaptable solution designed to standardize and\noptimize blockchain simulations. GFOBS provides a flexible platform that\nsupports various optimization algorithms, variables, and objectives, thereby\ncatering to a wide range of blockchain research needs. The paper's key\ncontributions are threefold: the development of GFOBS as a versatile tool for\nblockchain simulation optimization; the introduction of an innovative\noptimization method using warm starting technique; and the proposition of a\nnovel concurrent multiprocessing technique for simultaneous simulation\nprocesses. These advancements collectively enhance the efficiency,\nreplicability, and standardization of blockchain simulation experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04157v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04289", "title": "Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement", "authors": ["Hong Su"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04289v1", "summary": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of language tasks. However, their reasoning process is primarily guided\nby statistical patterns in training data, which limits their ability to handle\nnovel problems and perform consistent logical reasoning. In this paper, we\npropose a method-based model that enhances LLMs with explicit, reusable\nprocedures extracted from training content, generated responses, and user\ninteractions. Each method is represented as a pair consisting of a problem and\nits corresponding solution, stored externally and ranked based on feedback.\nWhen a new query is received, the system retrieves and applies the most\nrelevant methods to guide the LLM's response. Our model enables continual\nlearning, method reuse, and logical consistency beyond next-token prediction.\nExperimental results demonstrate that the system improves factual verification\nand generalization in complex prompts, and that newly learned methods can\noutperform earlier ones through user-driven refinement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04289v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04301", "title": "Extreme Event Precursor Prediction in Turbulent Dynamical Systems via CNN-Augmented Recurrence Analysis", "authors": ["Rahul Agarwal", "Mustafa A. Mohamad"], "categories": ["cs.CE", "math.DS", "nlin.CD"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04301v1", "summary": "We present a general framework to predict precursors to extreme events in\nturbulent dynamical systems. The approach combines phase-space reconstruction\ntechniques with recurrence matrices and convolutional neural networks to\nidentify precursors to extreme events. We evaluate the framework across three\ndistinct testbed systems: a triad turbulent interaction model, a prototype\nstochastic anisotropic turbulent flow, and the Kolmogorov flow. This method\noffers three key advantages: (1) a threshold-free classification strategy that\neliminates subjective parameter tuning, (2) efficient training using only\n$\\mathcal{O}(100)$ recurrence matrices, and (3) ability to generalize to unseen\nsystems. The results demonstrate robust predictive performance across all test\nsystems: 96\\% detection rate for the triad model with a mean lead time of 1.8\ntime units, 96\\% for the anisotropic turbulent flow with a mean lead time of\n6.1 time units, and 93\\% for the Kolmogorov flow with a mean lead time of 22.7\nunits.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04301v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04306", "title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation", "authors": ["Zhi Zhang", "Yan Liu", "Zhejing Hu", "Gong Chen", "Sheng-hua Zhong", "Jiannong Cao"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04306v1", "summary": "Literature reviews play an important role in scientific research. Recent\nadvances in large language models (LLMs) have boosted the development of\nautomated systems for the entire literature review workflow, from retrieval to\nmanuscript drafting. However, a key challenge is that mistakes made in early\nstages can propagate and amplify in subsequent steps, leading to compounding\nerrors that undermine the faithfulness of the final review. To tackle this\nissue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,\nwhich consists of a manager agent and four executor agents for literature\nsearching, outline generation, fact localization, and manuscript drafting. We\npropose three novel collaboration paradigms, forming exploration, exploitation,\nand experience taskforces, to effectively organize agents and mitigate\ncompounding errors both between and within executor agents. Experimental\nresults show that MATC achieves state-of-the-art performance on existing\nbenchmarks. We further propose a new benchmark dataset featuring more diverse\ntopics for faithful literature review generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04306v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04538", "title": "Bridging Simulation and Experiment: A Self-Supervised Domain Adaptation Framework for Concrete Damage Classification", "authors": ["Chen Xu", "Giao Vu", "Ba Trung Cao", "Zhen Liu", "Fabian Diewald", "Yong Yuan", "GÃ¼nther Meschke"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04538v1", "summary": "Reliable assessment of concrete degradation is critical for ensuring\nstructural safety and longevity of engineering structures. This study proposes\na self-supervised domain adaptation framework for robust concrete damage\nclassification using coda wave signals. To support this framework, an advanced\nvirtual testing platform is developed, combining multiscale modeling of\nconcrete degradation with ultrasonic wave propagation simulations. This setup\nenables the generation of large-scale labeled synthetic data under controlled\nconditions, reducing the dependency on costly and time-consuming experimental\nlabeling. However, neural networks trained solely on synthetic data often\nsuffer from degraded performance when applied to experimental data due to\ndomain shifts. To bridge this domain gap, the proposed framework integrates\ndomain adversarial training, minimum class confusion loss, and the Bootstrap\nYour Own Latent (BYOL) strategy. These components work jointly to facilitate\neffective knowledge transfer from the labeled simulation domain to the\nunlabeled experimental domain, achieving accurate and reliable damage\nclassification in concrete. Extensive experiments demonstrate that the proposed\nmethod achieves notable performance improvements, reaching an accuracy of\n0.7762 and a macro F1 score of 0.7713, outperforming both the plain 1D CNN\nbaseline and six representative domain adaptation techniques. Moreover, the\nmethod exhibits high robustness across training runs and introduces only\nminimal additional computational cost. These findings highlight the practical\npotential of the proposed simulation-driven and label-efficient framework for\nreal-world applications in structural health monitoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04538v1", "cate": "cs.CE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.02403", "title": "SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study", "authors": ["Luyao Zhang"], "categories": ["cs.CE", "cs.CR", "cs.CY", "econ.GN", "q-fin.CP"], "primary_category": "econ.GN", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02403", "summary": "Stablecoins have become a foundational component of the digital asset\necosystem, with their market capitalization exceeding 230 billion USD as of May\n2025. As fiat-referenced and programmable assets, stablecoins provide\nlow-latency, globally interoperable infrastructure for payments, decentralized\nfinance, DeFi, and tokenized commerce. Their accelerated adoption has prompted\nextensive regulatory engagement, exemplified by the European Union's Markets in\nCrypto-assets Regulation, MiCA, the US Guiding and Establishing National\nInnovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins\nBill. Despite this momentum, academic research remains fragmented across\neconomics, law, and computer science, lacking a unified framework for design,\nevaluation, and application. This study addresses that gap through a\nmulti-method research design. First, it synthesizes cross-disciplinary\nliterature to construct a taxonomy of stablecoin systems based on custodial\nstructure, stabilization mechanism, and governance. Second, it develops a\nperformance evaluation framework tailored to diverse stakeholder needs,\nsupported by an open-source benchmarking pipeline to ensure transparency and\nreproducibility. Third, a case study on Real World Asset tokenization\nillustrates how stablecoins operate as programmable monetary infrastructure in\ncross-border digital systems. By integrating conceptual theory with empirical\ntools, the paper contributes: a unified taxonomy for stablecoin design; a\nstakeholder-oriented performance evaluation framework; an empirical case\nlinking stablecoins to sectoral transformation; and reproducible methods and\ndatasets to inform future research. These contributions support the development\nof trusted, inclusive, and transparent digital monetary infrastructure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02403", "cate": "econ.GN", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03750", "title": "GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification", "authors": ["Cheng Huang", "Weizheng Xie", "Karanjit Kooner", "Tsengdar Lee", "Jui-Kai Wang", "Jia Zhang"], "categories": ["cs.CE", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03750", "summary": "Early and accurate detection of glaucoma is critical to prevent irreversible\nvision loss. However, existing methods often rely on unimodal data and lack\ninterpretability, limiting their clinical utility. In this paper, we present\nGlaBoost, a multimodal gradient boosting framework that integrates structured\nclinical features, fundus image embeddings, and expert-curated textual\ndescriptions for glaucoma risk prediction. GlaBoost extracts high-level visual\nrepresentations from retinal fundus photographs using a pretrained\nconvolutional encoder and encodes free-text neuroretinal rim assessments using\na transformer-based language model. These heterogeneous signals, combined with\nmanually assessed risk scores and quantitative ophthalmic indicators, are fused\ninto a unified feature space for classification via an enhanced XGBoost model.\nExperiments conducted on a real-world annotated dataset demonstrate that\nGlaBoost significantly outperforms baseline models, achieving a validation\naccuracy of 98.71%. Feature importance analysis reveals clinically consistent\npatterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings\ncontributing most to model decisions. GlaBoost offers a transparent and\nscalable solution for interpretable glaucoma diagnosis and can be extended to\nother ophthalmic disorders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03750", "cate": "cs.LG", "date": "2025-08-03", "updated": "2025-08-03", "section": "cross"}
{"id": "2508.04625", "title": "FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging", "authors": ["Zichen Tang", "Haihong E", "Jiacheng Liu", "Zhongjun Yang", "Rongjin Li", "Zihua Rong", "Haoyang He", "Zhuodi Hao", "Xinyang Hu", "Kun Ji", "Ziyan Ma", "Mengyuan Ji", "Jun Zhang", "Chenghao Ma", "Qianhe Zheng", "Yang Liu", "Yiling Huang", "Xinyi Hu", "Qing Huang", "Zijian Xie", "Shiyao Peng"], "categories": ["cs.CE", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04625", "summary": "We present FinMMR, a novel bilingual multimodal benchmark tailored to\nevaluate the reasoning capabilities of multimodal large language models (MLLMs)\nin financial numerical reasoning tasks. Compared to existing benchmarks, our\nwork introduces three significant advancements. (1) Multimodality: We\nmeticulously transform existing financial reasoning benchmarks, and construct\nnovel questions from the latest Chinese financial research reports. FinMMR\ncomprises 4.3K questions and 8.7K images spanning 14 categories, including\ntables, bar charts, and ownership structure charts. (2) Comprehensiveness:\nFinMMR encompasses 14 financial subdomains, including corporate finance,\nbanking, and industry analysis, significantly exceeding existing benchmarks in\nfinancial domain knowledge breadth. (3) Challenge: Models are required to\nperform multi-step precise numerical reasoning by integrating financial\nknowledge with the understanding of complex financial images and text. The\nbest-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe\nthat FinMMR will drive advancements in enhancing the reasoning capabilities of\nMLLMs in real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04625", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2307.07694", "title": "Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation", "authors": ["Chung I Lu"], "categories": ["cs.CE", "q-fin.PM"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2307.07694", "summary": "We evaluate benchmark deep reinforcement learning algorithms on the task of\nportfolio optimisation using simulated data. The simulator to generate the data\nis based on correlated geometric Brownian motion with the Bertsimas-Lo market\nimpact model. Using the Kelly criterion (log utility) as the objective, we can\nanalytically derive the optimal policy without market impact as an upper bound\nto measure performance when including market impact. We find that the\noff-policy algorithms DDPG, TD3 and SAC are unable to learn the right\n$Q$-function due to the noisy rewards and therefore perform poorly. The\non-policy algorithms PPO and A2C, with the use of generalised advantage\nestimation, are able to deal with the noise and derive a close to optimal\npolicy. The clipping variant of PPO was found to be important in preventing the\npolicy from deviating from the optimal once converged. In a more challenging\nenvironment where we have regime changes in the GBM parameters, we find that\nPPO, combined with a hidden Markov model to learn and predict the regime\ncontext, is able to learn different policies adapted to each regime. Overall,\nwe find that the sample complexity of these algorithms is too high for\napplications using real data, requiring more than 2m steps to learn a good\npolicy in the simplest setting, which is equivalent to almost 8,000 years of\ndaily prices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2307.07694", "cate": "cs.CE", "date": "2023-07-15", "updated": "2025-08-06", "section": "repl"}
{"id": "2407.18814", "title": "Agent-Based Insight into Eco-Choices: Simulating the Fast Fashion Shift", "authors": ["Daria Soboleva", "Angel SÃ¡nchez"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.18814", "summary": "Fashion is a powerful force in the modern world. It is one of the most\naccessible means of self-expression, thereby playing a significant role in our\nsociety. Yet, it is plagued by well-documented issues of waste and human rights\nabuses. Fast fashion in particular, characterized by its disposable nature,\ncontributes extensively to environmental degradation and CO$_2$ emissions,\nsurpassing the combined outputs of France, Germany, and the UK, but its\neconomic contributions have somewhat shielded it from criticism. In this paper,\nwe examine the demand for fast fashion, with a focus on Spain. We explore the\nindividual decision-making process involved in choosing to buy fast fashion and\nthe role of awareness regarding working conditions, environmental consequences,\nand education on sustainable fashion in influencing consumer behavior. By\nemploying Agent-Based Modeling, we investigate the factors influencing garment\nconsumption patterns and how shifts in public opinion can be achieved through\npeer pressure, social media influence, and government interventions. Our study\nrevealed that government interventions are pivotal, with the state's campaigns\nsetting the overall tone for progress, although its success is conditioned by\nsocial media and polarization levels of the population. Importantly, the state\ndoes not need to adopt an extremely proactive stance or continue the campaigns\nindefinitely to achieve optimal results, as excessive interventions yield\ndiminishing returns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.18814", "cate": "cs.CE", "date": "2024-07-26", "updated": "2025-08-05", "section": "repl"}
{"id": "2502.13675", "title": "A CFL condition for the finite cell method", "authors": ["Tim BÃ¼rchner", "Lars Radtke", "Philipp Kopp"], "categories": ["cs.CE", "math.NA"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.13675", "summary": "Immersed boundary finite element methods allow the user to bypass the\npotentially troublesome task of boundary-conforming mesh generation. When\ncombined with explicit time integration, poorly cut elements with little\nsupport in the physical domain lead to a severely reduced critical time step\nsize, posing a major challenge for immersed wave propagation simulations. The\nfinite cell method stabilizes cut elements by defining the weak form of the\nproblem also in the fictitious domain, but scaled by a small value $\\alpha$.\nThis paper investigates the effect of the finite cell method on the critical\ntime step size for explicit time integration. Starting with an analytical\none-degree-of-freedom model, we systematically study the influence of\n$\\alpha$-stabilization on the maximum eigenvalue, and thus on the critical time\nstep size, for corner and sliver cuts. The analysis is complemented by a\nnumerical study of an example with one element and increasing polynomial\ndegree, confirming that the critical time step size does not decrease below a\ncertain limit, even as the cut fraction tends to zero. This lower bound is\ncontrolled by the choice of $\\alpha$. In higher dimensions, sliver cuts are\nfound to be more detrimental than corner cuts, thus determining the minimum\ncritical time step size. Increasing the polynomial degree has only little\neffect on this degradation. Based on these observations, we derive an estimate\nof the minimum critical time step size as a function of $\\alpha$, which we use\nto propose a modified CFL condition for the finite cell method. The validity of\nthis condition is demonstrated on a two-dimensional perforated plate example.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.13675", "cate": "cs.CE", "date": "2025-02-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.14768", "title": "Optimistic MEV in Ethereum Layer 2s: Why Blockspace Is Always in Demand", "authors": ["Ozan Solmaz", "Lioba Heimbach", "Yann Vonlanthen", "Roger Wattenhofer"], "categories": ["cs.CE"], "primary_category": "cs.CE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.14768", "summary": "Layer 2 rollups are rapidly absorbing DeFi activity, securing over $40\nbillion and accounting for nearly half of Ethereum's DEX volume by Q1 2025, yet\ntheir MEV dynamics remain understudied. We address this gap by defining and\nquantifying optimistic MEV, a form of speculative, on-chain MEV whose detection\nand execution logic reside largely on-chain in smart contracts. As a result of\ntheir speculative nature and lack of off-chain opportunity verification,\noptimistic MEV transactions frequently decide not to execute any trades.\n  In this work, we focus on cyclic arbitrage, which we find is predominantly\nexecuted as optimistic MEV on Layer 2s. Using our multi-stage identification\npipeline on Arbitrum, Base, and Optimism, we show that in Q1 2025, transactions\nfrom cyclic arbitrage contracts account for over 50% of on-chain gas on Base\nand Optimism and 7% on Arbitrum, driven mainly by \"interaction\" probes\n(on-chain computations searching for arbitrage). This speculative probing\nindicates that cyclic arbitrage on Layer 2s is predominantly executed as\noptimistic MEV and contributes to generally keeping blocks on Base and Optimism\npersistently full. Despite consuming over half of on-chain gas, these\noptimistic MEV transactions pay less than one quarter of total gas fees.\nCross-network comparison reveals divergent success rates, differing patterns of\ncode reuse, and sensitivity to varying sequencer ordering and block production\ntimes. Finally, OLS regressions link optimistic MEV trade count to ETH\nvolatility, retail trading activity, and DEX aggregator usage. Together, these\nfindings show that optimistic MEV has become a major source of persistent\nspam-like transaction activity on Layer 2s, dominating blockspace with\nlow-value probes and reshaping the composition of on-chain activity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.14768", "cate": "cs.CE", "date": "2025-06-17", "updated": "2025-08-05", "section": "repl"}
{"id": "2409.04463", "title": "SINDyG: Sparse Identification of Nonlinear Dynamical Systems from Graph-Structured Data, with Applications to Stuart-Landau Oscillator Networks", "authors": ["Mohammad Amin Basiri", "Sina Khanmohammadi"], "categories": ["cs.CE", "cs.LG", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.04463", "summary": "The combination of machine learning (ML) and sparsity-promoting techniques is\nenabling direct extraction of governing equations from data, revolutionizing\ncomputational modeling in diverse fields of science and engineering. The\ndiscovered dynamical models could be used to address challenges in climate\nscience, neuroscience, ecology, finance, epidemiology, and beyond. However,\nmost existing sparse identification methods for discovering dynamical systems\ntreat the whole system as one without considering the interactions between\nsubsystems. As a result, such models are not able to capture small changes in\nthe emergent system behavior. To address this issue, we developed a new method\ncalled Sparse Identification of Nonlinear Dynamical Systems from\nGraph-structured data (SINDyG), which incorporates the network structure into\nsparse regression to identify model parameters that explain the underlying\nnetwork dynamics. We tested our proposed method using several case studies of\nneuronal dynamics, where we modeled the macroscopic oscillation of a population\nof neurons using the extended Stuart-Landau (SL) equation and utilize the\nSINDyG method to identify the underlying nonlinear dynamics. Our extensive\ncomputational experiments validate the improved accuracy and simplicity of\ndiscovered network dynamics when compared to the original SINDy approach. The\nproposed graph-informed penalty can be easily integrated with other symbolic\nregression algorithms, enhancing model interpretability and performance by\nincorporating network structure into the regression process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.04463", "cate": "eess.SY", "date": "2024-09-02", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.05828", "title": "FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging", "authors": ["Zichen Tang", "Haihong E", "Ziyan Ma", "Haoyang He", "Jiacheng Liu", "Zhongjun Yang", "Zihua Rong", "Rongjin Li", "Kun Ji", "Qing Huang", "Xinyang Hu", "Yang Liu", "Qianhe Zheng"], "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05828", "summary": "We introduce FinanceReasoning, a novel benchmark designed to evaluate the\nreasoning capabilities of large reasoning models (LRMs) in financial numerical\nreasoning problems. Compared to existing benchmarks, our work provides three\nkey advancements. (1) Credibility: We update 15.6% of the questions from four\npublic datasets, annotating 908 new questions with detailed Python solutions\nand rigorously refining evaluation standards. This enables an accurate\nassessment of the reasoning improvements of LRMs. (2) Comprehensiveness:\nFinanceReasoning covers 67.8% of financial concepts and formulas, significantly\nsurpassing existing datasets. Additionally, we construct 3,133 Python-formatted\nfunctions, which enhances LRMs' financial reasoning capabilities through\nrefined knowledge (e.g., 83.2% $\\rightarrow$ 91.6% for GPT-4o). (3) Challenge:\nModels are required to apply multiple financial formulas for precise numerical\nreasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with\nPoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical\nprecision. We demonstrate that combining Reasoner and Programmer models can\neffectively enhance LRMs' performance (e.g., 83.2% $\\rightarrow$ 87.8% for\nDeepSeek-R1). Our work paves the way for future research on evaluating and\nimproving LRMs in domain-specific complex reasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05828", "cate": "cs.CL", "date": "2025-06-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04603", "title": "Square packing with $O(x^{0.6})$ wasted area", "authors": ["Hong Duc Bui"], "categories": ["cs.CG"], "primary_category": "cs.CG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04603v1", "summary": "We show a new construction for square packing, and prove that it is more\nefficient than previous results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04603v1", "cate": "cs.CG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04602", "title": "On existence of a compatible triangulation with the double circle order type", "authors": ["Hong Duc Bui"], "categories": ["cs.CG", "math.CO"], "primary_category": "math.CO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04602", "summary": "We show that the \"double circle\" order type and some of its generalizations\nhave a compatible triangulation with any other order types with the same number\nof points and number of edges on convex hull, thus proving another special case\nof the conjecture in Aichholzer (2003).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04602", "cate": "math.CO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2506.23943", "title": "Linear Layouts of Graphs with Priority Queues", "authors": ["Emilio Di Giacomo", "Walter Didimo", "Henry FÃ¶rster", "Torsten Ueckerdt", "Johannes Zink"], "categories": ["cs.CG", "cs.DM"], "primary_category": "cs.DM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23943", "summary": "A linear layout of a graph consists of a linear ordering of its vertices and\na partition of its edges into pages such that the edges assigned to the same\npage obey some constraint. The two most prominent and widely studied types of\nlinear layouts are stack and queue layouts, in which any two edges assigned to\nthe same page are forbidden to cross and nest, respectively. The names of these\ntwo layouts derive from the fact that, when parsing the graph according to the\nlinear vertex ordering, the edges in a single page can be stored using a single\nstack or queue, respectively. Recently, the concepts of stack and queue layouts\nhave been extended by using a double-ended queue or a restricted-input queue\nfor storing the edges of a page. We extend this line of study to edge-weighted\ngraphs by introducing priority queue layouts, that is, the edges on each page\nare stored in a priority queue whose keys are the edge weights. First, we show\nthat there are edge-weighted graphs that require a linear number of priority\nqueues. Second, we characterize the graphs that admit a priority queue layout\nwith a single queue, regardless of the edge-weight function, and we provide an\nefficient recognition algorithm. Third, we show that the number of priority\nqueues required independently of the edge-weight function is bounded by the\npathwidth of the graph, but can be arbitrarily large already for graphs of\ntreewidth two. Finally, we prove that determining the minimum number of\npriority queues is NP-complete if the linear ordering of the vertices is fixed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23943", "cate": "cs.DM", "date": "2025-06-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03712", "title": "How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion", "authors": ["Agrima Seth", "Monojit Choudhary", "Sunayana Sitaram", "Kentaro Toyama", "Aditya Vashistha", "Kalika Bali"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03712v1", "summary": "Representational bias in large language models (LLMs) has predominantly been\nmeasured through single-response interactions and has focused on Global\nNorth-centric identities like race and gender. We expand on that research by\nconducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded\nrepresentational biases are and how they extend to less-explored dimensions of\nidentity. We prompt GPT-4 Turbo to generate over 7,200 stories about\nsignificant life events (such as weddings) in India, using prompts designed to\nencourage diversity to varying extents. Comparing the diversity of religious\nand caste representation in the outputs against the actual population\ndistribution in India as recorded in census data, we quantify the presence and\n\"stickiness\" of representational bias in the LLM for religion and caste. We\nfind that GPT-4 responses consistently overrepresent culturally dominant groups\nfar beyond their statistical representation, despite prompts intended to\nencourage representational diversity. Our findings also suggest that\nrepresentational bias in LLMs has a winner-take-all quality that is more biased\nthan the likely distribution bias in their training data, and repeated\nprompt-based nudges have limited and inconsistent efficacy in dislodging these\nbiases. These results suggest that diversifying training data alone may not be\nsufficient to correct LLM bias, highlighting the need for more fundamental\nchanges in model development. Dataset and Codebook:\nhttps://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03712v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "section": "new"}
{"id": "2508.03716", "title": "FeynTune: Large Language Models for High-Energy Theory", "authors": ["Paul Richmond", "Prarit Agarwal", "Borun Chowdhury", "Vasilis Niarchos", "Constantinos Papageorgakis"], "categories": ["cs.CL", "cs.LG", "hep-th"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03716v1", "summary": "We present specialized Large Language Models for theoretical High-Energy\nPhysics, obtained as 20 fine-tuned variants of the 8-billion parameter\nLlama-3.1 model. Each variant was trained on arXiv abstracts (through August\n2024) from different combinations of hep-th, hep-ph and gr-qc. For a\ncomparative study, we also trained models on datasets that contained abstracts\nfrom disparate fields such as the q-bio and cs categories. All models were\nfine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and\nvarying dataset sizes, and outperformed the base model on hep-th abstract\ncompletion tasks. We compare performance against leading commercial LLMs\n(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing\nspecialized language models for High-Energy Theoretical Physics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03716v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "section": "new"}
{"id": "2508.03726", "title": "Hierarchical Verification of Speculative Beams for Accelerating LLM Inference", "authors": ["Jaydip Sen", "Harshitha Puvvala", "Subhasis Dasgupta"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03726v1", "summary": "Large language models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks but face persistent challenges in inference\nefficiency due to their autoregressive nature. While speculative decoding and\nbeam sampling offer notable improvements, traditional methods verify draft\nsequences sequentially without prioritization, leading to unnecessary\ncomputational overhead. This work proposes the Hierarchical Verification Tree\n(HVT), a novel framework that restructures speculative beam decoding by\nprioritizing high-likelihood drafts and enabling early pruning of suboptimal\ncandidates. Theoretical foundations and a formal verification-pruning algorithm\nare developed to ensure correctness and efficiency. Integration with standard\nLLM inference pipelines is achieved without requiring retraining or\narchitecture modification. Experimental evaluations across multiple datasets\nand models demonstrate that HVT consistently outperforms existing speculative\ndecoding schemes, achieving substantial reductions in inference time and energy\nconsumption while maintaining or enhancing output quality. The findings\nhighlight the potential of hierarchical verification strategies as a new\ndirection for accelerating large language model inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03726v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30", "section": "new"}
{"id": "2508.03728", "title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03728v1", "summary": "Wikipedia, a vast and continuously consulted knowledge base, faces\nsignificant challenges in maintaining up-to-date content due to its reliance on\nmanual human editors. Inspired by the vision of continuous knowledge\nacquisition in NELL and fueled by advances in LLM-based agents, this paper\nintroduces WiNELL, an agentic framework for continuously updating Wikipedia\narticles. Our approach employs a multi-agent framework to aggregate online\ninformation, select new and important knowledge for a target entity in\nWikipedia, and then generate precise edit suggestions for human review. Our\nfine-grained editing models, trained on Wikipedia's extensive history of human\nedits, enable incorporating updates in a manner consistent with human editing\nbehavior. Our editor models outperform both open-source instruction-following\nbaselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and\nediting efficiency. End-to-end evaluation on high-activity Wikipedia pages\ndemonstrates WiNELL's ability to identify and suggest timely factual updates.\nThis opens up a promising research direction in LLM agents for automatically\nupdating knowledge bases in a never-ending fashion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03728v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30", "section": "new"}
{"id": "2508.03793", "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs", "authors": ["Yanting Wang", "Runpeng Geng", "Ying Chen", "Jinyuan Jia"], "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03793v1", "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03793v1", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03829", "title": "Majority Bit-Aware Watermarking For Large Language Models", "authors": ["Jiahao Xu", "Rui Hu", "Zikai Zhang"], "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03829v1", "summary": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03829v1", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03865", "title": "An Entity Linking Agent for Question Answering", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03865v1", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03865v1", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03905", "title": "Sotopia-RL: Reward Design for Social Intelligence", "authors": ["Haofei Yu", "Zhengyang Qi", "Yining Zhao", "Kolby Nottingham", "Keyang Xuan", "Bodhisattwa Prasad Majumder", "Hao Zhu", "Paul Pu Liang", "Jiaxuan You"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03905v1", "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03905v1", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03923", "title": "CoAct-1: Computer-using Agents with Coding as Actions", "authors": ["Linxin Song", "Yutong Dai", "Viraj Prabhu", "Jieyu Zhang", "Taiwei Shi", "Li Li", "Junnan Li", "Silvio Savarese", "Zeyuan Chen", "Jieyu Zhao", "Ran Xu", "Caiming Xiong"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03923v1", "summary": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03923v1", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03935", "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation", "authors": ["Raymond Wilson", "Cole Graham", "Chase Carter", "Zefeng Yang", "Ruiqi Gu"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03935v1", "summary": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03935v1", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03979", "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency", "authors": ["Md Arafat Sultan", "RamÃ³n Fernandez Astudillo"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03979v1", "summary": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03979v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03998", "title": "Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models", "authors": ["Xinyu Zhao", "Zhen Tan", "Maya Enisman", "Minjae Seo", "Marta R. Durantini", "Dolores Albarracin", "Tianlong Chen"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03998v1", "summary": "Successful group meetings, such as those implemented in group\nbehavioral-change programs, work meetings, and other social contexts, must\npromote individual goal setting and execution while strengthening the social\nrelationships within the group. Consequently, an ideal facilitator must be\nsensitive to the subtle dynamics of disengagement, difficulties with individual\ngoal setting and execution, and interpersonal difficulties that signal a need\nfor intervention. The challenges and cognitive load experienced by facilitators\ncreate a critical gap for an embodied technology that can interpret social\nexchanges while remaining aware of the needs of the individuals in the group\nand providing transparent recommendations that go beyond powerful but \"black\nbox\" foundation models (FMs) that identify social cues. We address this\nimportant demand with a social robot co-facilitator that analyzes multimodal\nmeeting data and provides discreet cues to the facilitator. The robot's\nreasoning is powered by an agentic concept bottleneck model (CBM), which makes\ndecisions based on human-interpretable concepts like participant engagement and\nsentiments, ensuring transparency and trustworthiness. Our core contribution is\na transfer learning framework that distills the broad social understanding of\nan FM into our specialized and transparent CBM. This concept-driven system\nsignificantly outperforms direct zero-shot FMs in predicting the need for\nintervention and enables real-time human correction of its reasoning.\nCritically, we demonstrate robust knowledge transfer: the model generalizes\nacross different groups and successfully transfers the expertise of senior\nhuman facilitators to improve the performance of novices. By transferring an\nexpert's cognitive model into an interpretable robotic partner, our work\nprovides a powerful blueprint for augmenting human capabilities in complex\nsocial domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03998v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04038", "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents", "authors": ["Zechen Li", "Baiyu Chen", "Hao Xue", "Flora D. Salim"], "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04038v1", "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04038v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04047", "title": "DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation", "authors": ["Jiabing Yang", "Yixiang Chen", "Zichen Wen", "Chenhang Cui", "Peiyan Li", "Yuan Xu", "Bowen Fang", "Yan Huang", "Liang Wang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04047v1", "summary": "Controllable Text Generation (CTG) is a vital subfield in Natural Language\nProcessing (NLP), aiming to generate text that aligns with desired attributes.\nHowever, previous studies commonly focus on the quality of controllable text\ngeneration for short sequences, while the generation of long-form text remains\nlargely underexplored. In this paper, we observe that the controllability of\ntexts generated by the powerful prefix-based method Air-Decoding tends to\ndecline with increasing sequence length, which we hypothesize primarily arises\nfrom the observed decay in attention to the prefixes. Meanwhile, different\ntypes of prefixes including soft and hard prefixes are also key factors\ninfluencing performance. Building on these insights, we propose a lightweight\nand effective framework called Dynamic Token-level Prefix Augmentation (DTPA)\nbased on Air-Decoding for controllable text generation. Specifically, it first\nselects the optimal prefix type for a given task. Then we dynamically amplify\nthe attention to the prefix for the attribute distribution to enhance\ncontrollability, with a scaling factor growing exponentially as the sequence\nlength increases. Moreover, based on the task, we optionally apply a similar\naugmentation to the original prompt for the raw distribution to balance text\nquality. After attribute distribution reconstruction, the generated text\nsatisfies the attribute constraints well. Experiments on multiple CTG tasks\ndemonstrate that DTPA generally outperforms other methods in attribute control\nwhile maintaining competitive fluency, diversity, and topic relevance. Further\nanalysis highlights DTPA's superior effectiveness in long text generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04047v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04057", "title": "PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04057v1", "summary": "Retrieval-Augmented Generation (RAG) has become a cornerstone technique for\nenhancing large language models (LLMs) with external knowledge. However,\ncurrent RAG systems face two critical limitations: (1) they inefficiently\nretrieve information for every query, including simple questions that could be\nresolved using the LLM's parametric knowledge alone, and (2) they risk\nretrieving irrelevant documents when queries contain sparse information\nsignals. To address these gaps, we introduce Parametric-verified Adaptive\nInformation Retrieval and Selection (PAIRS), a training-free framework that\nintegrates parametric and retrieved knowledge to adaptively determine whether\nto retrieve and how to select external information. Specifically, PAIRS employs\na dual-path generation mechanism: First, the LLM produces both a direct answer\nand a context-augmented answer using self-generated pseudo-context. When these\noutputs converge, PAIRS bypasses external retrieval entirely, dramatically\nimproving the RAG system's efficiency. For divergent cases, PAIRS activates a\ndual-path retrieval (DPR) process guided by both the original query and\nself-generated contextual signals, followed by an Adaptive Information\nSelection (AIS) module that filters documents through weighted similarity to\nboth sources. This simple yet effective approach can not only enhance\nefficiency by eliminating unnecessary retrievals but also improve accuracy\nthrough contextually guided retrieval and adaptive information selection.\nExperimental results on six question-answering (QA) benchmarks show that PAIRS\nreduces retrieval costs by around 25% (triggering for only 75% of queries)\nwhile still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior\nbaselines on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04057v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04073", "title": "Efficient Strategy for Improving Large Language Model (LLM) Capabilities", "authors": ["JuliÃ¡n Camilo Velandia GutiÃ©rrez"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04073v1", "summary": "Large Language Models (LLMs) have become a milestone in the field of\nartificial intelligence and natural language processing. However, their\nlarge-scale deployment remains constrained by the need for significant\ncomputational resources. This work proposes starting from a base model to\nexplore and combine data processing and careful data selection techniques,\ntraining strategies, and architectural adjustments to improve the efficiency of\nLLMs in resource-constrained environments and within a delimited knowledge\nbase. The methodological approach included defining criteria for building\nreliable datasets, conducting controlled experiments with different\nconfigurations, and systematically evaluating the resulting variants in terms\nof capability, versatility, response time, and safety. Finally, comparative\ntests were conducted to measure the performance of the developed variants and\nto validate the effectiveness of the proposed strategies. This work is based on\nthe master's thesis in Systems and Computer Engineering titled \"Efficient\nStrategy for Improving the Capabilities of Large Language Models (LLMs)\".", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04073v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04086", "title": "ToolGrad: Efficient Tool-use Dataset Generation with Textual \"Gradients\"", "authors": ["Zhongyi Zhou", "Kohei Uehara", "Haoyu Zhang", "Jingtao Zhou", "Lin Gu", "Ruofei Du", "Zheng Xu", "Tatsuya Harada"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04086v1", "summary": "Prior work synthesizes tool-use LLM datasets by first generating a user\nquery, followed by complex tool-use annotations like DFS. This leads to\ninevitable annotation failures and low efficiency in data generation. We\nintroduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad\nfirst constructs valid tool-use chains through an iterative process guided by\ntextual \"gradients\", and then synthesizes corresponding user queries. This\n\"answer-first\" approach led to ToolGrad-5k, a dataset generated with more\ncomplex tool use, lower cost, and 100% pass rate. Experiments show that models\ntrained on ToolGrad-5k outperform those on expensive baseline datasets and\nproprietary LLMs, even on OOD benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04086v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04088", "title": "GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning", "authors": ["Jianghangfan Zhang", "Yibo Yan", "Kening Zheng", "Xin Zou", "Song Dai", "Xuming Hu"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04088v1", "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\nbut often struggle with complex, multi-step mathematical reasoning, where minor\nerrors in visual perception or logical deduction can lead to complete failure.\nWhile Process Reward Models (PRMs) offer step-by-step supervision, existing\nmultimodal PRMs are limited to being binary verifiers that can identify but not\ncorrect errors, offering little explanatory power. To address these\ndeficiencies, we introduce the Generative Multimodal Process Reward Model\n(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an\nactive reasoning collaborator. Instead of a simple scalar score, GM-PRM\nprovides a fine-grained, interpretable analysis of each reasoning step,\nevaluating its step intent, visual alignment, and logical soundness. More\ncritically, GM-PRM is trained to generate a corrected version of the first\nerroneous step it identifies. This unique corrective capability enables our new\ntest-time inference strategy, Refined Best-of-N (Refined-BoN). This framework\nactively enhances solution quality by using the PRM's generated correction to\nguide the policy model toward a more promising reasoning trajectory, thereby\nimproving the diversity and correctness of the solution pool. We demonstrate\nthat GM-PRM achieves state-of-the-art results on multiple multimodal math\nbenchmarks, significantly boosting policy model performance with remarkable\ndata efficiency, requiring only a 20K-sample training dataset. Our code will be\nreleased upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04088v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04117", "title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks", "authors": ["Zhiwen Ruan", "Yun Chen", "Yutao Hou", "Peng Li", "Yang Liu", "Guanhua Chen"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04117v1", "summary": "The pretrained large language models (LLMs) are finetuned with labeled data\nfor better instruction following ability and alignment with human values. In\nthis paper, we study the learning dynamics of LLM finetuning on reasoning tasks\nand reveal the uncovered over-memorization phenomenon during a specific stage\nof LLM finetuning. At this stage, the LLMs have excessively memorized training\ndata and exhibit high test perplexity while maintaining good test accuracy. We\ninvestigate the conditions that lead to LLM over-memorization and find that\ntraining epochs and large learning rates contribute to this issue. Although\nmodels with over-memorization demonstrate comparable test accuracy to normal\nmodels, they suffer from reduced robustness, poor out-of-distribution\ngeneralization, and decreased generation diversity. Our experiments unveil the\nover-memorization to be broadly applicable across different tasks, models, and\nfinetuning methods. Our research highlights that overparameterized, extensively\nfinetuned LLMs exhibit unique learning dynamics distinct from traditional\nmachine learning models. Based on our observations of over-memorization, we\nprovide recommendations on checkpoint and learning rate selection during\nfinetuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04117v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04179", "title": "The State Of TTS: A Case Study with Human Fooling Rates", "authors": ["Praveen Srinivasa Varadhan", "Sherry Thomas", "Sai Teja M. S.", "Suvrat Bhooshan", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04179v1", "summary": "While subjective evaluations in recent years indicate rapid progress in TTS,\ncan current TTS systems truly pass a human deception test in a Turing-like\nevaluation? We introduce Human Fooling Rate (HFR), a metric that directly\nmeasures how often machine-generated speech is mistaken for human. Our\nlarge-scale evaluation of open-source and commercial TTS models reveals\ncritical insights: (i) CMOS-based claims of human parity often fail under\ndeception testing, (ii) TTS progress should be benchmarked on datasets where\nhuman speech achieves high HFRs, as evaluating against monotonous or less\nexpressive reference samples sets a low bar, (iii) Commercial models approach\nhuman deception in zero-shot settings, while open-source systems still struggle\nwith natural conversational speech; (iv) Fine-tuning on high-quality data\nimproves realism but does not fully bridge the gap. Our findings underscore the\nneed for more realistic, human-centric evaluations alongside existing\nsubjective tests.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04179v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04183", "title": "Characterizing Deep Research: A Benchmark and Formal Definition", "authors": ["Abhinav Java", "Ashmit Khandelwal", "Sukruta Midigeshi", "Aaron Halfaker", "Amit Deshpande", "Navin Goyal", "Ankur Gupta", "Nagarajan Natarajan", "Amit Sharma"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04183v1", "summary": "Information tasks such as writing surveys or analytical reports require\ncomplex search and reasoning, and have recently been grouped under the umbrella\nof \\textit{deep research} -- a term also adopted by recent models targeting\nthese capabilities. Despite growing interest, the scope of the deep research\ntask remains underdefined and its distinction from other reasoning-intensive\nproblems is poorly understood. In this paper, we propose a formal\ncharacterization of the deep research (DR) task and introduce a benchmark to\nevaluate the performance of DR systems. We argue that the core defining feature\nof deep research is not the production of lengthy report-style outputs, but\nrather the high fan-out over concepts required during the search process, i.e.,\nbroad and reasoning-intensive exploration. To enable objective evaluation, we\ndefine DR using an intermediate output representation that encodes key claims\nuncovered during search-separating the reasoning challenge from surface-level\nreport generation. Based on this formulation, we propose a diverse, challenging\nbenchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,\ndatasets, materials discovery, prior art search) and public interest events\n(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1\nscore ranges between 0.02 and 0.72 for any sub-category. OpenAI's model\nperforms the best with an overall F1 score of 0.55. Analysis of reasoning\ntraces reveals the distribution over the number of referenced sources,\nbranching, and backtracking events executed by current DR systems, motivating\nfuture directions for improving their search mechanisms and grounding\ncapabilities. The benchmark is available at\nhttps://github.com/microsoft/LiveDRBench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04183v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04199", "title": "Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts", "authors": ["Millicent Ochieng", "Anja Thieme", "Ignatius Ezeani", "Risa Ueno", "Samuel Maina", "Keshet Ronen", "Javier Gonzalez", "Jacki O'Neill"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04199v1", "summary": "Sentiment analysis in low-resource, culturally nuanced contexts challenges\nconventional NLP approaches that assume fixed labels and universal affective\nexpressions. We present a diagnostic framework that treats sentiment as a\ncontext-dependent, culturally embedded construct, and evaluate how large\nlanguage models (LLMs) reason about sentiment in informal, code-mixed WhatsApp\nmessages from Nairobi youth health groups. Using a combination of\nhuman-annotated data, sentiment-flipped counterfactuals, and rubric-based\nexplanation evaluation, we probe LLM interpretability, robustness, and\nalignment with human reasoning. Framing our evaluation through a social-science\nmeasurement lens, we operationalize and interrogate LLMs outputs as an\ninstrument for measuring the abstract concept of sentiment. Our findings reveal\nsignificant variation in model reasoning quality, with top-tier LLMs\ndemonstrating interpretive stability, while open models often falter under\nambiguity or sentiment shifts. This work highlights the need for culturally\nsensitive, reasoning-aware AI evaluation in complex, real-world communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04199v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04219", "title": "Hierarchical Text Classification Using Black Box Large Language Models", "authors": ["Kosuke Yoshimura", "Hisashi Kashima"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04219v1", "summary": "Hierarchical Text Classification (HTC) aims to assign texts to structured\nlabel hierarchies; however, it faces challenges due to data scarcity and model\ncomplexity. This study explores the feasibility of using black box Large\nLanguage Models (LLMs) accessed via APIs for HTC, as an alternative to\ntraditional machine learning methods that require extensive labeled data and\ncomputational resources. We evaluate three prompting strategies -- Direct Leaf\nLabel Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down\nMulti-step Hierarchical Label Prediction (TMH) -- in both zero-shot and\nfew-shot settings, comparing the accuracy and cost-effectiveness of these\nstrategies. Experiments on two datasets show that a few-shot setting\nconsistently improves classification accuracy compared to a zero-shot setting.\nWhile a traditional machine learning model achieves high accuracy on a dataset\nwith a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the\nmachine learning model on a dataset with a deeper hierarchy. API costs increase\nsignificantly due to the higher input tokens required for deeper label\nhierarchies on DH strategy. These results emphasize the trade-off between\naccuracy improvement and the computational cost of prompt strategy. These\nfindings highlight the potential of black box LLMs for HTC while underscoring\nthe need to carefully select a prompt strategy to balance performance and cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04219v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04239", "title": "DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting", "authors": ["Chanjuan Liu", "Shengzhi Wang", "Enqiang Zhu"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04239v1", "summary": "Time series forecasting is crucial in strategic planning and decision-making\nacross various industries. Traditional forecasting models mainly concentrate on\nnumerical time series data, often overlooking important textual information\nsuch as events and news, which can significantly affect forecasting accuracy.\nWhile large language models offer a promise for integrating multimodal data,\nexisting single-prompt frameworks struggle to effectively capture the semantics\nof timestamped text, introducing redundant information that can hinder model\nperformance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt\nGPT2-base for Multimodal Time Series), a novel dual-prompt large language model\nframework that combines two complementary prompts: an explicit prompt for clear\ntask instructions and a textual prompt for context-aware embeddings from\ntime-stamped data. The tokenizer generates the explicit prompt while the\nembeddings from the textual prompt are refined through self-attention and\nfeed-forward networks. Comprehensive experiments conducted on diverse\ntextural-numerical time series datasets demonstrate that this approach\noutperforms state-of-the-art algorithms in time series forecasting. This\nhighlights the significance of incorporating textual context via a dual-prompt\nmechanism to achieve more accurate time series predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04239v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04257", "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs", "authors": ["Zunhai Su", "Kehong Yuan"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04257v1", "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04257v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04266", "title": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents", "authors": ["Jiangyuan Wang", "Kejun Xiao", "Qi Sun", "Huaipeng Zhao", "Tao Luo", "Jiandong Zhang", "Xiaoyi Zeng"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04266v1", "summary": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04266v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04402", "title": "What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems", "authors": ["Kiyotada Mori", "Seiya Kawano", "Chaoran Liu", "Carlos Toshinori Ishi", "Angel Fernando Garcia Contreras", "Koichiro Yoshino"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04402v1", "summary": "Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at\nthe front end of their pipeline. The role of ASR in SDSs is to recognize\ninformation in user speech related to response generation appropriately.\nExamining selective listening of humans, which refers to the ability to focus\non and listen to important parts of a conversation during the speech, will\nenable us to identify the ASR capabilities required for SDSs and evaluate them.\nIn this study, we experimentally confirmed selective listening when humans\ngenerate dialogue responses by comparing human transcriptions for generating\ndialogue responses and reference transcriptions. Based on our experimental\nresults, we discuss the possibility of a new ASR evaluation method that\nleverages human selective listening, which can identify the gap between\ntranscription ability between ASR systems and humans.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04402v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04403", "title": "Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model", "authors": ["Kiyotada Mori", "Seiya Kawano", "Angel Fernando Garcia Contreras", "Koichiro Yoshino"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04403v1", "summary": "Prefetching of dialogue responses has been investigated to reduce\nuser-perceived latency (UPL), which refers to the user's waiting time before\nreceiving the system's response, in spoken dialogue systems. To reduce the UPL,\nit is necessary to predict complete user utterances before the end of the\nuser's speech, typically by language models, to prepare prefetched dialogue\nresponses. In this study, we proposed a prediction confidence model (PCM) that\ndetermines whether prefetching is possible or not by estimating the semantic\nsimilarity between the predicted complete user utterance and the complete user\nutterance. We evaluated our PCM based on the differences between the predicted\ncomplete user utterance and the complete user utterance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04403v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04423", "title": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation", "authors": ["Jie Zhu", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang", "Fang Kong"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04423v1", "summary": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04423v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04494", "title": "CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation", "authors": ["Bastien LiÃ©tard", "Gabriel Loiseau"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04494v1", "summary": "Lexical semantics is concerned with both the multiple senses a word can adopt\nin different contexts, and the semantic relations that exist between meanings\nof different words. To investigate them, Contextualized Language Models are a\nvaluable tool that provides context-sensitive representations that can be used\nto investigate lexical meaning. Recent works like XL-LEXEME have leveraged the\ntask of Word-in-Context to fine-tune them to get more semantically accurate\nrepresentations, but Word-in-Context only compares occurrences of the same\nlemma, limiting the range of captured information. In this paper, we propose an\nextension, Concept Differentiation, to include inter-words scenarios. We\nprovide a dataset for this task, derived from SemCor data. Then we fine-tune\nseveral representation models on this dataset. We call these models\nConcept-Aligned Embeddings (CALE). By challenging our models and other models\non various lexical semantic tasks, we demonstrate that the proposed models\nprovide efficient multi-purpose representations of lexical meaning that reach\nbest performances in our experiments. We also show that CALE's fine-tuning\nbrings valuable changes to the spatial organization of embeddings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04494v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04530", "title": "StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering", "authors": ["Chenglei Shen", "Zhongxiang Sun", "Teng Shi", "Xiao Zhang", "Jun Xu"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04530v1", "summary": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04530v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04623", "title": "Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider", "authors": ["Chirag Seth", "Utkarsh Singh"], "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04623v1", "summary": "Text-to-SQL translation enables non-expert users to query relational\ndatabases using natural language, with applications in education and business\nintelligence. This study evaluates three lightweight transformer models -\nT5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on\nlow-resource settings. We developed a reusable, model-agnostic pipeline that\ntailors schema formatting to each model's architecture, training them across\n1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form\nAccuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small\nachieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2\n(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL\ngeneration. Despite resource constraints limiting performance, our pipeline's\nmodularity supports future enhancements, such as advanced schema linking or\nalternative base models. This work underscores the potential of compact\ntransformers for accessible text-to-SQL solutions in resource-scarce\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04623v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04632", "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards", "authors": ["Xu Guo", "Tianyi Liang", "Tong Jian", "Xiaogui Yang", "Ling-I Wu", "Chenhui Li", "Zhihui Lu", "Qipeng Guo", "Kai Chen"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04632v1", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04632v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04638", "title": "Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Simona Frenda", "Poppy Gerrard-Abbott", "Nancie Gunson", "Gavin Abercrombie", "Ioannis Konstas"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04638v1", "summary": "Counterspeech, i.e. the practice of responding to online hate speech, has\ngained traction in NLP as a promising intervention. While early work emphasised\ncollaboration with non-governmental organisation stakeholders, recent research\ntrends have shifted toward automated pipelines that reuse a small set of legacy\ndatasets, often without input from affected communities. This paper presents a\nsystematic review of 74 NLP studies on counterspeech, analysing the extent to\nwhich stakeholder participation influences dataset creation, model development,\nand evaluation. To complement this analysis, we conducted a participatory case\nstudy with five NGOs specialising in online Gender-Based Violence (oGBV),\nidentifying stakeholder-informed practices for counterspeech generation. Our\nfindings reveal a growing disconnect between current NLP research and the needs\nof communities most impacted by toxic online content. We conclude with concrete\nrecommendations for re-centring stakeholder expertise in counterspeech\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04638v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04660", "title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs", "authors": ["Noah Ziems", "Dilara Soylu", "Lakshya A Agrawal", "Isaac Miller", "Liheng Lai", "Chen Qian", "Kaiqiang Song", "Meng Jiang", "Dan Klein", "Matei Zaharia", "Karel D'Oosterlinck", "Christopher Potts", "Omar Khattab"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04660v1", "summary": "Group Relative Policy Optimization (GRPO) has proven to be an effective tool\nfor post-training language models (LMs). However, AI systems are increasingly\nexpressed as modular programs that mix together multiple LM calls with distinct\nprompt templates and other tools, and it is not clear how best to leverage GRPO\nto improve these systems. We begin to address this challenge by defining\nmmGRPO, a simple multi-module generalization of GRPO that groups LM calls by\nmodule across rollouts and handles variable-length and interrupted\ntrajectories. We find that mmGRPO, composed with automatic prompt optimization,\nimproves accuracy by 11% on average across classification, many-hop search, and\nprivacy-preserving delegation tasks against the post-trained LM, and by 5%\nagainst prompt optimization on its own. We open-source mmGRPO in DSPy as the\ndspy.GRPO optimizer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04660v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04698", "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data", "authors": ["Thibaut Thonet", "GermÃ¡n Kruszewski", "Jos Rozen", "Pierre Erbacher", "Marc Dymetman"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04698v1", "summary": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04698v1", "cate": "cs.CL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03709", "title": "MD-LLM-1: A Large Language Model for Molecular Dynamics", "authors": ["Mhd Hussein Murtada", "Z. Faidon Brotzakis", "Michele Vendruscolo"], "categories": ["cs.CL", "cs.LG", "physics.comp-ph", "q-bio.BM"], "primary_category": "q-bio.BM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03709", "summary": "Molecular dynamics (MD) is a powerful approach for modelling molecular\nsystems, but it remains computationally intensive on spatial and time scales of\nmany macromolecular systems of biological interest. To explore the\nopportunities offered by deep learning to address this problem, we introduce a\nMolecular Dynamics Large Language Model (MD-LLM) framework to illustrate how\nLLMs can be leveraged to learn protein dynamics and discover states not seen in\ntraining. By applying MD-LLM-1, the first implementation of this approach,\nobtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein\nsystems, we show that training on one conformational state enables the\nprediction of other conformational states. These results indicate that MD-LLM-1\ncan learn the principles for the exploration of the conformational landscapes\nof proteins, although it is not yet modeling explicitly their thermodynamics\nand kinetics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03709", "cate": "q-bio.BM", "date": "2025-07-21", "updated": "2025-07-21", "section": "cross"}
{"id": "2508.03828", "title": "MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources", "authors": ["Samuel Barham", "Chandler May", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.DL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03828", "summary": "We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles\nwith their citations and scraped web sources; articles are represented in a\nrich data structure, and scraped source texts are stored inline with precise\ncharacter offsets of their citations in the article text. MegaWika 2 is a major\nupgrade from the original MegaWika, spanning six times as many articles and\ntwice as many fully scraped citations. Both MegaWika and MegaWika 2 support\nreport generation research ; whereas MegaWika also focused on supporting\nquestion answering and retrieval applications, MegaWika 2 is designed to\nsupport fact checking and analyses across time and language.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03828", "cate": "cs.DL", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03936", "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants", "authors": ["Xiangzhe Xu", "Guangyu Shen", "Zian Su", "Siyuan Cheng", "Hanxi Guo", "Lu Yan", "Xuan Chen", "Jiasheng Jiang", "Xiaolong Jin", "Chengpeng Wang", "Zhuo Zhang", "Xiangyu Zhang"], "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SE"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03936", "summary": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03936", "cate": "cs.CR", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04001", "title": "ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval", "authors": ["Fengran Mo", "Jinghan Zhang", "Yuchen Hui", "Jia Ao Sun", "Zhichao Xu", "Zhan Su", "Jian-Yun Nie"], "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04001", "summary": "Conversational search aims to satisfy users' complex information needs via\nmultiple-turn interactions. The key challenge lies in revealing real users'\nsearch intent from the context-dependent queries. Previous studies achieve\nconversational search by fine-tuning a conversational dense retriever with\nrelevance judgments between pairs of context-dependent queries and documents.\nHowever, this training paradigm encounters data scarcity issues. To this end,\nwe propose ConvMix, a mixed-criteria framework to augment conversational dense\nretrieval, which covers more aspects than existing data augmentation\nframeworks. We design a two-sided relevance judgment augmentation schema in a\nscalable manner via the aid of large language models. Besides, we integrate the\nframework with quality control mechanisms to obtain semantically diverse\nsamples and near-distribution supervisions to combine various annotated data.\nExperimental results on five widely used benchmarks show that the\nconversational dense retriever trained by our ConvMix framework outperforms\nprevious baseline methods, which demonstrates our superior effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04001", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04143", "title": "Multilingual Source Tracing of Speech Deepfakes: A First Benchmark", "authors": ["Xi Xuan", "Yang Xiao", "Rohan Kumar Das", "Tomi Kinnunen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04143", "summary": "Recent progress in generative AI has made it increasingly easy to create\nnatural-sounding deepfake speech from just a few seconds of audio. While these\ntools support helpful applications, they also raise serious concerns by making\nit possible to generate convincing fake speech in many languages. Current\nresearch has largely focused on detecting fake speech, but little attention has\nbeen given to tracing the source models used to generate it. This paper\nintroduces the first benchmark for multilingual speech deepfake source tracing,\ncovering both mono- and cross-lingual scenarios. We comparatively investigate\nDSP- and SSL-based modeling; examine how SSL representations fine-tuned on\ndifferent languages impact cross-lingual generalization performance; and\nevaluate generalization to unseen languages and speakers. Our findings offer\nthe first comprehensive insights into the challenges of identifying speech\ngeneration models when training and inference languages differ. The dataset,\nprotocol and code are available at\nhttps://github.com/xuanxixi/Multilingual-Source-Tracing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04143", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04166", "title": "ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations", "authors": ["Subhankar Swain", "Naquee Rizwan", "Nayandeep Deb", "Vishwajeet Singh Solanki", "Vishwa Gangadhar S", "Animesh Mukherjee"], "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04166", "summary": "The 2025 Global Risks Report identifies state-based armed conflict and\nsocietal polarisation among the most pressing global threats, with social media\nplaying a central role in amplifying toxic discourse. Memes, as a widely used\nmode of online communication, often serve as vehicles for spreading harmful\ncontent. However, limitations in data accessibility and the high cost of\ndataset curation hinder the development of robust meme moderation systems. To\naddress this challenge, in this work, we introduce a first-of-its-kind dataset\nof 6,300 real-world meme-based posts annotated in two stages: (i) binary\nclassification into toxic and normal, and (ii) fine-grained labelling of toxic\nmemes as hateful, dangerous, or offensive. A key feature of this dataset is\nthat it is enriched with auxiliary metadata of socially relevant tags,\nenhancing the context of each meme. In addition, we propose a tag generation\nmodule that produces socially grounded tags, because most in-the-wild memes\noften do not come with tags. Experimental results show that incorporating these\ntags substantially enhances the performance of state-of-the-art VLMs detection\ntasks. Our contributions offer a novel and scalable foundation for improved\ncontent moderation in multimodal online environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04166", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04252", "title": "Graph Representation Learning with Massive Unlabeled Data for Rumor Detection", "authors": ["Chaoqun Cui", "Caiyan Jia"], "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04252", "summary": "With the development of social media, rumors spread quickly, cause great harm\nto society and economy. Thereby, many effective rumor detection methods have\nbeen developed, among which the rumor propagation structure learning based\nmethods are particularly effective compared to other methods. However, the\nexisting methods still suffer from many issues including the difficulty to\nobtain large-scale labeled rumor datasets, which leads to the low\ngeneralization ability and the performance degeneration on new events since\nrumors are time-critical and usually appear with hot topics or newly emergent\nevents. In order to solve the above problems, in this study, we used\nlarge-scale unlabeled topic datasets crawled from the social media platform\nWeibo and Twitter with claim propagation structure to improve the semantic\nlearning ability of a graph reprentation learing model on various topics. We\nuse three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE\nin two commonly used training strategies, to verify the performance of general\ngraph semi-supervised methods in rumor detection tasks. In addition, for\nalleviating the time and topic difference between unlabeled topic data and\nrumor data, we also collected a rumor dataset covering a variety of topics over\na decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our\nexperiments show that these general graph self-supervised learning methods\noutperform previous methods specifically designed for rumor detection tasks and\nachieve good performance under few-shot conditions, demonstrating the better\ngeneralization ability with the help of our massive unlabeled topic dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04252", "cate": "cs.SI", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04469", "title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding", "authors": ["Emmanuelle Bourigault", "Pauline Bourigault"], "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04469", "summary": "The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04469", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04495", "title": "Causal Reflection with Language Models", "authors": ["Abi Aryan", "Zac Liu"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04495", "summary": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04495", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04567", "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective", "authors": ["Yifan Li", "Kun Zhou", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04567", "summary": "As scaling up training data has significantly improved the general multimodal\ncapabilities of Large Vision-Language Models (LVLMs), they still suffer from\nthe hallucination issue, generating text that is inconsistent with the visual\ninput. This phenomenon motivates us to systematically investigate the role of\ntraining data in hallucination. We introduce a new benchmark, POPEv2, which\nconsists of counterfactual images collected from the training data of LVLMs\nwith certain objects masked. Through comprehensive evaluation on POPEv2, we\nfind that current LVLMs suffer from training bias: they fail to fully leverage\ntheir training data and hallucinate more frequently on images seen during\ntraining. Specifically, they perform poorly on counterfactual images, often\nincorrectly answering ``Yes'' to questions about masked objects. To understand\nthis issue, we conduct probing experiments on the models' internal components,\nrevealing that this training bias is primarily located in the language modeling\n(LM) head. Based on these findings, we propose Obliviate, an efficient and\nlightweight unlearning method designed to mitigate object hallucination via\ntraining bias unlearning. Obliviate identifies the discrepancy between\nground-truth labels and model outputs on the training data as a proxy for bias\nand adopts a parameter- and data-efficient fine-tuning strategy that only\nupdates the LM head. Extensive experiments demonstrate the effectiveness of our\napproach. While only reusing the training data and updating approximately 2\\%\nof the parameters, Obliviate significantly reduces hallucination across both\ndiscriminative and generative tasks. Furthermore, it demonstrates strong\nscalability with respect to both model size (2B to 72B) and training data\nvolume, and exhibits promising generalization to hallucination types beyond\nobject-level hallucination. Our code and data will be publicly released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04567", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04571", "title": "Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation", "authors": ["Claudio Pomo", "Matteo Attimonelli", "Danilo Danese", "Fedelucio Narducci", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04571", "summary": "Multimodal Recommender Systems aim to improve recommendation accuracy by\nintegrating heterogeneous content, such as images and textual metadata. While\neffective, it remains unclear whether their gains stem from true multimodal\nunderstanding or increased model complexity. This work investigates the role of\nmultimodal item embeddings, emphasizing the semantic informativeness of the\nrepresentations. Initial experiments reveal that embeddings from standard\nextractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on\nmodality-specific encoders and ad hoc fusion strategies that lack control over\ncross-modal alignment. To overcome these limitations, we leverage Large\nVision-Language Models (LVLMs) to generate multimodal-by-design embeddings via\nstructured prompts. This approach yields semantically aligned representations\nwithout requiring any fusion. Experiments across multiple settings show notable\nperformance improvements. Furthermore, LVLMs embeddings offer a distinctive\nadvantage: they can be decoded into structured textual descriptions, enabling\ndirect assessment of their multimodal comprehension. When such descriptions are\nincorporated as side content into recommender systems, they improve\nrecommendation performance, empirically validating the semantic depth and\nalignment encoded within LVLMs outputs. Our study highlights the importance of\nsemantically rich representations and positions LVLMs as a compelling\nfoundation for building robust and meaningful multimodal representations in\nrecommendation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04571", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2406.14805", "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions", "authors": ["Julia Kharchenko", "Tanya Roosta", "Aman Chadha", "Chirag Shah"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.14805", "summary": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.14805", "cate": "cs.CL", "date": "2024-06-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2410.15576", "title": "A Survey of Conversational Search", "authors": ["Fengran Mo", "Kelong Mao", "Ziliang Zhao", "Hongjin Qian", "Haonan Chen", "Yiruo Cheng", "Xiaoxi Li", "Yutao Zhu", "Zhicheng Dou", "Jian-Yun Nie"], "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.15576", "summary": "As a cornerstone of modern information access, search engines have become\nindispensable in everyday life. With the rapid advancements in AI and natural\nlanguage processing (NLP) technologies, particularly large language models\n(LLMs), search engines have evolved to support more intuitive and intelligent\ninteractions between users and systems. Conversational search, an emerging\nparadigm for next-generation search engines, leverages natural language\ndialogue to facilitate complex and precise information retrieval, thus\nattracting significant attention. Unlike traditional keyword-based search\nengines, conversational search systems enhance user experience by supporting\nintricate queries, maintaining context over multi-turn interactions, and\nproviding robust information integration and processing capabilities. Key\ncomponents such as query reformulation, search clarification, conversational\nretrieval, and response generation work in unison to enable these sophisticated\ninteractions. In this survey, we explore the recent advancements and potential\nfuture directions in conversational search, examining the critical modules that\nconstitute a conversational search system. We highlight the integration of LLMs\nin enhancing these systems and discuss the challenges and opportunities that\nlie ahead in this dynamic field. Additionally, we provide insights into\nreal-world applications and robust evaluations of current conversational search\nsystems, aiming to guide future research and development in conversational\nsearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.15576", "cate": "cs.CL", "date": "2024-10-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2411.08397", "title": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision", "authors": ["Aoi Ito", "Kota Dohi", "Yohei Kawaguchi"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.08397", "summary": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.08397", "cate": "cs.CL", "date": "2024-11-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.12422", "title": "FactEHR: A Dataset for Evaluating Factuality in Clinical Notes Using LLMs", "authors": ["Monica Munnangi", "Akshay Swaminathan", "Jason Alan Fries", "Jenelle Jindal", "Sanjana Narayanan", "Ivan Lopez", "Lucia Tu", "Philip Chung", "Jesutofunmi A. Omiye", "Mehr Kashyap", "Nigam Shah"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12422", "summary": "Verifying and attributing factual claims is essential for the safe and\neffective use of large language models (LLMs) in healthcare. A core component\nof factuality evaluation is fact decomposition, the process of breaking down\ncomplex clinical statements into fine-grained atomic facts for verification.\nRecent work has proposed fact decomposition, which uses LLMs to rewrite source\ntext into concise sentences conveying a single piece of information, to\nfacilitate fine-grained fact verification. However, clinical documentation\nposes unique challenges for fact decomposition due to dense terminology and\ndiverse note types and remains understudied. To address this gap and explore\nthese challenges, we present FactEHR, an NLI dataset consisting of document\nfact decompositions for 2,168 clinical notes spanning four types from three\nhospital systems, resulting in 987,266 entailment pairs. We assess the\ngenerated facts on different axes, from entailment evaluation of LLMs to a\nqualitative analysis. Our evaluation, including review by the clinicians,\nreveals substantial variability in LLM performance for fact decomposition. For\nexample, Gemini-1.5-Flash consistently generates relevant and accurate facts,\nwhile Llama-3 8B produces fewer and less consistent outputs. The results\nunderscore the need for better LLM capabilities to support factual verification\nin clinical text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12422", "cate": "cs.CL", "date": "2024-12-17", "updated": "2025-08-05", "section": "repl"}
{"id": "2502.11268", "title": "Improved Unbiased Watermark for Large Language Models", "authors": ["Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11268", "summary": "As artificial intelligence surpasses human capabilities in text generation,\nthe necessity to authenticate the origins of AI-generated content has become\nparamount. Unbiased watermarks offer a powerful solution by embedding\nstatistical signals into language model-generated text without distorting the\nquality. In this paper, we introduce MCmark, a family of unbiased,\nMulti-Channel-based watermarks. MCmark works by partitioning the model's\nvocabulary into segments and promoting token probabilities within a selected\nsegment based on a watermark key. We demonstrate that MCmark not only preserves\nthe original distribution of the language model but also offers significant\nimprovements in detectability and robustness over existing unbiased watermarks.\nOur experiments with widely-used language models demonstrate an improvement in\ndetectability of over 10% using MCmark, compared to existing state-of-the-art\nunbiased watermarks. This advancement underscores MCmark's potential in\nenhancing the practical application of watermarking in AI-generated texts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11268", "cate": "cs.CL", "date": "2025-02-16", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.13053", "title": "Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks", "authors": ["Yurun Chen", "Xavier Hu", "Keting Yin", "Juncheng Li", "Shengyu Zhang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.13053", "summary": "As researchers continue to optimize AI agents for more effective task\nexecution within operating systems, they often overlook a critical security\nconcern: the ability of these agents to detect \"impostors\" within their\nenvironment. Through an analysis of the agents' operational context, we\nidentify a significant threat-attackers can disguise malicious attacks as\nenvironmental elements, injecting active disturbances into the agents'\nexecution processes to manipulate their decision-making. We define this novel\nthreat as the Active Environment Injection Attack (AEIA). Focusing on the\ninteraction mechanisms of the Android OS, we conduct a risk assessment of AEIA\nand identify two critical security vulnerabilities: (1) Adversarial content\ninjection in multimodal interaction interfaces, where attackers embed\nadversarial instructions within environmental elements to mislead agent\ndecision-making; and (2) Reasoning gap vulnerabilities in the agent's task\nexecution process, which increase susceptibility to AEIA attacks during\nreasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN,\nan attack scheme that exploits interaction vulnerabilities in mobile operating\nsystems to assess the robustness of MLLM-based agents. Experimental results\nshow that even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% on the AndroidWorld benchmark by combining\ntwo vulnerabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.13053", "cate": "cs.CL", "date": "2025-02-18", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.15434", "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.15434", "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results, and (4) balances exploration efficiency and\ndiversity in contribution ratios by tuning the Beta distribution's shape\nparameters. The code is provided in the supplementary materials.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.15434", "cate": "cs.CL", "date": "2025-02-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.16781", "title": "Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data", "authors": ["Bhawna Piryani", "Jamshid Mozafari", "Abdelrahman Abdallah", "Antoine Doucet", "Adam Jatowt"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16781", "summary": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16781", "cate": "cs.CL", "date": "2025-02-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.17945", "title": "Assessing Agentic Large Language Models in Multilingual National Bias", "authors": ["Qianying Liu", "Katrina Qiyao Wang", "Fei Cheng", "Sadao Kurohashi"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.17945", "summary": "Large Language Models have garnered significant attention for their\ncapabilities in multilingual natural language processing, while studies on\nrisks associated with cross biases are limited to immediate context\npreferences. Cross-language disparities in reasoning-based recommendations\nremain largely unexplored, with a lack of even descriptive analysis. This study\nis the first to address this gap. We test LLM's applicability and capability in\nproviding personalized advice across three key scenarios: university\napplications, travel, and relocation. We investigate multilingual bias in\nstate-of-the-art LLMs by analyzing their responses to decision-making tasks\nacross multiple languages. We quantify bias in model-generated scores and\nassess the impact of demographic factors and reasoning strategies (e.g.,\nChain-of-Thought prompting) on bias patterns. Our findings reveal that local\nlanguage bias is prevalent across different tasks, with GPT-4 and Sonnet\nreducing bias for English-speaking countries compared to GPT-3.5 but failing to\nachieve robust multilingual alignment, highlighting broader implications for\nmultilingual AI agents and applications such as education. \\footnote{Code\navailable at: https://github.com/yiyunya/assess_agentic_national_bias", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.17945", "cate": "cs.CL", "date": "2025-02-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.15299", "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpektor", "Jonathan Herzig", "Roi Reichart"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.15299", "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.15299", "cate": "cs.CL", "date": "2025-03-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.18878", "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders", "authors": ["Andrey Galichin", "Alexey Dontsov", "Polina Druzhinina", "Anton Razzhigaev", "Oleg Y. Rogov", "Elena Tutubalina", "Ivan Oseledets"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.18878", "summary": "Recent LLMs like DeepSeek-R1 have demonstrated state-of-the-art performance\nby integrating deep thinking and complex reasoning during generation. However,\nthe internal mechanisms behind these reasoning processes remain unexplored. We\nobserve reasoning LLMs consistently use vocabulary associated with human\nreasoning processes. We hypothesize these words correspond to specific\nreasoning moments within the models' internal mechanisms. To test this\nhypothesis, we employ Sparse Autoencoders (SAEs), a technique for sparse\ndecomposition of neural network activations into human-interpretable features.\nWe introduce ReasonScore, an automatic metric to identify active SAE features\nduring these reasoning moments. We perform manual and automatic interpretation\nof the features detected by our metric, and find those with activation patterns\nmatching uncertainty, exploratory thinking, and reflection. Through steering\nexperiments, we demonstrate that amplifying these features increases\nperformance on reasoning-intensive benchmarks (+2.2%) while producing longer\nreasoning traces (+20.5%). Using the model diffing technique, we provide\nevidence that these features are present only in models with reasoning\ncapabilities. Our work provides the first step towards a mechanistic\nunderstanding of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.18878", "cate": "cs.CL", "date": "2025-03-24", "updated": "2025-08-05", "section": "repl"}
{"id": "2504.12311", "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Yang Li"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12311", "summary": "Prompt tuning has emerged as a lightweight strategy for adapting foundation\nmodels to downstream tasks, particularly for resource-constrained systems. As\npre-trained prompts become valuable assets, combining multiple source prompts\noffers a promising approach to enhance generalization for new tasks by\nleveraging complementary knowledge. However, naive aggregation often overlooks\ndifferent source prompts have different contribution potential to the target\ntask. To address this, we propose HGPrompt, a dynamic framework that learns\noptimal ensemble weights. These weights are optimized by jointly maximizing an\ninformation-theoretic metric for transferability and minimizing gradient\nconflicts via a novel regularization strategy. Specifically, we propose a\ndifferentiable prompt transferability metric to captures the discriminability\nof prompt-induced features on the target task. Meanwhile, HGPrompt match the\ngradient variances with respect to different source prompts based on Hessian\nand Fisher Information, ensuring stable and coherent knowledge transfer while\nsuppressing gradient conflicts among them. Extensive experiments on the\nlarge-scale VTAB benchmark demonstrate the state-of-the-art performance of\nHGPrompt, validating its effectiveness in learning an optimal ensemble for\neffective multi-source prompt transfer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12311", "cate": "cs.CL", "date": "2025-04-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.12342", "title": "CRAB: A Benchmark for Evaluating Curation of Retrieval-Augmented LLMs in Biomedicine", "authors": ["Hanmeng Zhong", "Linqing Chen", "Wentao Wu", "Weilei Wang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12342", "summary": "Recent development in Retrieval-Augmented Large Language Models (LLMs) have\nshown great promise in biomedical applications. How ever, a critical gap\npersists in reliably evaluating their curation ability the process by which\nmodels select and integrate relevant references while filtering out noise. To\naddress this, we introduce the benchmark for Curation of Retrieval-Augmented\nLLMs in Biomedicine (CRAB), the first multilingual benchmark tailored for\nevaluating the biomedical curation of retrieval-augmented LLMs, available in\nEnglish, French, German and Chinese. By incorporating a novel citation-based\nevaluation metric, CRAB quantifies the curation performance of\nretrieval-augmented LLMs in biomedicine. Experimental results reveal\nsignificant discrepancies in the curation performance of mainstream LLMs,\nunderscoring the urgent need to improve it in the domain of biomedicine. Our\ndataset is available at https://huggingface.co/datasets/zhm0/CRAB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12342", "cate": "cs.CL", "date": "2025-04-15", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.14194", "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.14194", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.14194", "cate": "cs.CL", "date": "2025-04-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.16414", "title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "authors": ["Mohammad Khodadad", "Ali Shiraee Kasmaee", "Mahdi Astaraki", "Nicholas Sherck", "Hamidreza Mahyar", "Soheila Samiee"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.16414", "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.16414", "cate": "cs.CL", "date": "2025-04-23", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.15050", "title": "Improving the fact-checking performance of language models by relying on their entailment ability", "authors": ["Gaurav Kumar", "Debajyoti Mazumder", "Ayush Garg", "Jasabanta Patro"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.15050", "summary": "Automated fact-checking is a crucial task in this digital age. The NLP\ncommunity has been trying various strategies to build robust fact-checking\nsystems. However, we have not been very successful yet. One main reason behind\nthis is that fact verification is a complex process. Language models have to\nparse through multiple pieces of evidence, often contradicting each other, to\npredict a claim's veracity. In this paper, we proposed a simple yet effective\nstrategy, where we relied on the entailment ability of language models to\nimprove the fact-checking performance. Apart from that, we did a comparison of\ndifferent prompting and fine-tuning strategies, as it is currently lacking in\nthe literature. Some of our observations are: (i) training language models with\nraw evidence sentences (TBE-1) and overall claim-evidence understanding (TBE-2)\nresulted in an improvement up to 8.20% and 16.39% in macro-F1 for RAW-FC\ndataset, and (ii) training language models with entailed justifications (TBE-3)\noutperformed the baselines by a huge margin (up to 28.57% and 44.26% for\nLIAR-RAW and RAW-FC, respectively). We have shared our code repository to\nreproduce the results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.15050", "cate": "cs.CL", "date": "2025-05-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2505.22548", "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Kaiyun Huang", "Peng Zhang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22548", "summary": "Long chain-of-thought (CoT) reasoning has shown great promise in enhancing\nthe emotion understanding performance of large language models (LLMs). However,\ncurrent fixed-length CoT methods struggle to balance reasoning depth and\nefficiency. Simple tasks (e.g., sentiment classification) are over-reasoned,\nwhile complex tasks (e.g., sarcasm understanding) lack depth. To fill this gap,\nwe present Emotion-o1, an adaptive CoT framework that dynamically adjusts\nreasoning length based on emotion-task complexity. Emotion-o1 is trained by\ndistilling adaptive CoT patterns from a reasoning-oriented LLM, followed by\nsupervised fine-tuning and reinforcement learning with a four-part reward\ntargeting accuracy, brevity, structure, and redundancy. Experimental results on\nfour emotion tasks highlight: (1) Emotion-o1 demonstrates significant\nimprovements over its backbone, with F1 score increases of 10%(Sentiment),\n5%(Emotion), 18%(Humor), and 27%(Sarcasm). (2) In sentiment and sarcasm tasks,\nour 8B model demonstrates superior performance against advanced LLMs,\noutperforming Grok-3 by 1.1% and Claude-3.7 by 2%. (3) The framework maintains\naccuracy while reducing reasoning length by 83% compared to OpenAI-o1,\ndemonstrating effective precision-efficiency optimization. Emotion-o1\neffectively balances reasoning depth and efficiency for emotion understanding\nin LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22548", "cate": "cs.CL", "date": "2025-05-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.02132", "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "authors": ["Michael Li", "Nishant Subramani"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.02132", "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how 25 models - from classical architectures (BERT, DeBERTa,\nGPT-2) to modern large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) - represent lexical identity and inflectional morphology across six\ntypologically diverse languages. Using linear and nonlinear classifiers trained\non hidden activations, we predict word lemmas and inflectional features layer\nby layer. We find that models concentrate lexical information linearly in early\nlayers and increasingly nonlinearly in later layers, while keeping inflectional\ninformation uniformly accessible and linearly separable throughout. Additional\nexperiments probe the nature of these encodings: attention and residual\nanalyses examine where within layers information can be recovered, steering\nvector experiments test what information can be functionally manipulated, and\nintrinsic dimensionality analyses explore how the representational structure\nevolves across layers. Remarkably, these encoding patterns emerge across all\nmodels we test, despite differences in architecture, size, and training regime\n(pretrained and instruction-tuned variants). This suggests that, even with\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties are\nimportant for next token prediction and are learned early during pretraining.\nOur code is available at https://github.com/ml5885/model_internal_sleuthing", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.02132", "cate": "cs.CL", "date": "2025-06-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.05949", "title": "NameTag 3: A Tool and a Service for Multilingual/Multitagset NER", "authors": ["Jana StrakovÃ¡", "Milan Straka"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05949", "summary": "We introduce NameTag 3, an open-source tool and cloud-based web service for\nmultilingual, multidataset, and multitagset named entity recognition (NER),\nsupporting both flat and nested entities. NameTag 3 achieves state-of-the-art\nresults on 21 test datasets in 15 languages and remains competitive on the\nrest, even against larger models. It is available as a command-line tool and as\na cloud-based service, enabling use without local installation. NameTag 3 web\nservice currently provides flat NER for 17 languages, trained on 21 corpora and\nthree NE tagsets, all powered by a single 355M-parameter fine-tuned model; and\nnested NER for Czech, powered by a 126M fine-tuned model. The source code is\nlicensed under open-source MPL 2.0, while the models are distributed under\nnon-commercial CC BY-NC-SA 4.0. Documentation is available at\nhttps://ufal.mff.cuni.cz/nametag, source code at\nhttps://github.com/ufal/nametag3, and trained models via https://lindat.cz. The\nREST service and the web application can be found at\nhttps://lindat.mff.cuni.cz/services/nametag/. A demonstration video is\navailable at https://www.youtube.com/watch?v=-gaGnP0IV8A.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05949", "cate": "cs.CL", "date": "2025-06-06", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.14448", "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.14448", "summary": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.14448", "cate": "cs.CL", "date": "2025-06-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.04642", "title": "R1-RE: Cross-Domain Relation Extraction with RLVR", "authors": ["Runpeng Dai", "Tong Zheng", "Run Yang", "Kaixian Yu", "Hongtu Zhu"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04642", "summary": "Relation extraction (RE) is a core task in natural language processing.\nTraditional approaches typically frame RE as a supervised learning problem,\ndirectly mapping context to labels-an approach that often suffers from poor\nout-of-domain (OOD) generalization. Inspired by the workflow of human\nannotators, we reframe RE as a reasoning task guided by annotation guidelines\nand introduce R1-RE, the first reinforcement learning with verifiable reward\n(RLVR) framework for RE tasks. Our method elicits the reasoning abilities of\nsmall language models for annotation tasks, resulting in significantly improved\nOOD robustness. We evaluate our approach on the public Sem-2010 dataset and a\nprivate MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of\napproximately 70%, on par with leading proprietary models such as GPT-4o.\nAdditionally, our comprehensive analysis provides novel insights into the\ntraining dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04642", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.15715", "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "authors": ["Alina Hyk", "Kiera McCormick", "Mian Zhong", "Ioana CiucÄ", "Sanjib Sharma", "John F Wu", "J. E. G. Peek", "Kartheik G. Iyer", "Ziang Xiao", "Anjalie Field"], "categories": ["astro-ph.IM", "cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15715", "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15715", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2507.19407", "title": "Towards Domain Specification of Embedding Models in Medicine", "authors": ["Mohammad Khodadad", "Ali Shiraee Kasmaee", "Mahdi Astaraki", "Hamidreza Mahyar"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19407", "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19407", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.22716", "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs", "authors": ["Jie He", "Victor GutiÃ©rrez-Basulto", "Jeff Z. Pan"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22716", "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22716", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.00370", "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00370", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00370", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01317", "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01317", "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01317", "cate": "cs.CL", "date": "2025-08-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02038", "title": "Marco-Voice Technical Report", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02038", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis. Our code and\ndataset are publicly available at https://github.com/AIDC-AI/Marco-Voice and\nhttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02038", "cate": "cs.CL", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02591", "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks", "authors": ["Omri Uzan", "Yuval Pinter"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02591", "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02591", "cate": "cs.CL", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02997", "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02997", "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02997", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03363", "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "categories": ["cs.CL"], "primary_category": "cs.CL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03363", "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03363", "cate": "cs.CL", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2403.04618", "title": "Strong Priority and Determinacy in Timed CCS", "authors": ["Luigi Liquori", "Michael Mendler"], "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.04618", "summary": "Building on the standard theory of process algebra with priorities, we\nidentify a new scheduling mechanism, called \"constructive reduction\" which is\ndesigned to capture the essence of synchronous programming. The distinctive\nproperty of this evaluation strategy is to achieve determinacy-by-construction\nfor multi-cast concurrent communication with shared memory. In the technical\nsetting of CCS extended by clocks and priorities, we prove for a large class of\n\"coherent\" processes a confluence property for constructive reductions. We show\nthat under some restrictions, called \"pivotability\", coherence is preserved by\nthe operators of prefix, summation, parallel composition, restriction and\nhiding. Since this permits memory and sharing, we are able to cover a strictly\nlarger class of processes compared to those in Milner's classical confluence\ntheory for CCS without priorities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.04618", "cate": "cs.PL", "date": "2024-03-07", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.13928", "title": "Automatically Interpreting Millions of Features in Large Language Models", "authors": ["GonÃ§alo Paulo", "Alex Mallen", "Caden Juang", "Nora Belrose"], "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13928", "summary": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13928", "cate": "cs.LG", "date": "2024-10-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.04449", "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay", "authors": ["Jun Zhang", "Desen Meng", "Zhengming Zhang", "Zhenpeng Huang", "Tao Wu", "Limin Wang"], "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.04449", "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.04449", "cate": "cs.CV", "date": "2024-12-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.05727", "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark", "authors": ["He Wang", "Linhan Ma", "Dake Guo", "Xiong Wang", "Lei Xie", "Jin Xu", "Junyang Lin"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05727", "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior benchmarks have largely focused on assessing the acoustic robustness of\nASR models, leaving evaluations of their linguistic capabilities relatively\nunderexplored. This largely stems from the limited parameter sizes and training\ncorpora of conventional ASR models, leaving them with insufficient world\nknowledge, which is crucial for accurately recognizing named entities across\ndiverse domains. For instance, drug and treatment names in medicine or\nspecialized technical terms in engineering. Recent breakthroughs in Large\nLanguage Models (LLMs) and corresponding Large Audio Language Models (LALMs)\nhave markedly enhanced the visibility of advanced context modeling and general\nartificial intelligence capabilities. Leveraging LLMs, we envision a unified\nsystem capable of robust speech recognition across diverse real-world domains,\nyet existing benchmarks are inadequate for evaluating this objective. To\naddress this gap, we propose ContextASR-Bench: a comprehensive, large-scale\nbenchmark designed to assess the linguistic competence of ASR systems using\ncorpora that feature numerous named entities across multiple domains. It\nencompasses up to 40,000 data entries with more than 300,000 named entities\nacross over 10 domains. Beyond the audio and its transcription, each sample\nprovides the domain it belongs to and a list of named entities it contains,\nwhich are referred to as the context. Based on this, we introduce three\nevaluation modes to assess how effectively models can exploit such context to\nimprove ASR accuracy. Extensive evaluation on ContextASR-Bench highlights that\nLALMs outperform conventional ASR models by a large margin thanks to the strong\nworld knowledge and context modeling of LLMs, yet there remains ample room for\nfurther improvement. The dataset and evaluation code have been released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05727", "cate": "eess.AS", "date": "2025-07-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03879", "title": "RX-INT: A Kernel Engine for Real-Time Detection and Analysis of In-Memory Threats", "authors": ["Arjun Juneja"], "categories": ["cs.CR", "cs.OS"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03879v1", "summary": "Malware and cheat developers use fileless execution techniques to evade\ntraditional, signature-based security products. These methods include various\ntypes of manual mapping, module stomping, and threadless injection which work\nentirely within the address space of a legitimate process, presenting a\nchallenge for detection due to ambiguity between what is legitimate and what\nisn't. Existing tools often have weaknesses, such as a dependency on Portable\nExecutable (PE) structures or a vulnerability to time-of-check-to-time-of-use\n(TOCTOU) race conditions where an adversary cleans up before a periodic scan\nhas the chance to occur. To address this gap, we present RX-INT, a\nkernel-assisted system featuring an architecture that provides resilience\nagainst TOCTOU attacks. RX-INT introduces a detection engine that combines a\nreal-time thread creation monitor with a stateful Virtual Address Descriptor\n(VAD) scanner alongside various heuristics within. This engine snapshots both\nprivate and image-backed memory regions, using real-time memory hashing to\ndetect illicit modifications like module stomping. Critically, we demonstrate a\nhigher detection rate in certain benchmarks of this approach through a direct\ncomparison with PE-sieve, a commonly used and powerful memory forensics tool.\nIn our evaluation, RX-INT successfully detected a manually mapped region that\nwas not identified by PE-sieve. We then conclude that our architecture\nrepresents a tangible difference in the detection of fileless threats, with\ndirect applications in the fields of anti-cheat and memory security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03879v1", "cate": "cs.CR", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04094", "title": "Isolate Trigger: Detecting and Eradicating Evade-Adaptive Backdoors", "authors": ["Chengrui Sun", "Hua Zhang", "Haoran Gao", "Zian Tian", "Jianjin Zhao", "qi Li", "Hongliang Zhu", "Zongliang Shen", "Shang Wang", "Anmin Fu"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04094v1", "summary": "All current detection of backdoor attacks on deep learning models fall under\nthe category of a non essential features(NEF), which focus on fighting against\nsimple and efficient vertical class backdoor -- trigger is small, few and not\noverlapping with the source. Evade-adaptive backdoor (EAB) attacks have evaded\nNEF detection and improved training efficiency. We introduces a precise,\nefficient and universal detection and defense framework coined as Isolate\nTrigger (IsTr). IsTr aims to find the hidden trigger by breaking the barrier of\nthe source features. Therefore, it investigates the essence of backdoor\ntriggering, and uses Steps and Differential-Middle-Slice as components to\nupdate past theories of distance and gradient. IsTr also plays a positive role\nin the model, whether the backdoor exists. For example, accurately find and\nrepair the wrong identification caused by deliberate or unintentional training\nin automatic driving. Extensive experiments on robustness scross various tasks,\nincluding MNIST, facial recognition, and traffic sign recognition, confirm the\nhigh efficiency, generality and precision of the IsTr. We rigorously evaluated\nthe effectiveness of the IsTr against a series of six EAB attacks, including\nBadnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB. None of these\ncountermeasures evade, even when attacks are combined and the trigger and\nsource overlap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04094v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04155", "title": "Evaluating Selective Encryption Against Gradient Inversion Attacks", "authors": ["Jiajun Gu", "Yuhang Yao", "Shuaiqi Wang", "Carlee Joe-Wong"], "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04155v1", "summary": "Gradient inversion attacks pose significant privacy threats to distributed\ntraining frameworks such as federated learning, enabling malicious parties to\nreconstruct sensitive local training data from gradient communications between\nclients and an aggregation server during the aggregation process. While\ntraditional encryption-based defenses, such as homomorphic encryption, offer\nstrong privacy guarantees without compromising model utility, they often incur\nprohibitive computational overheads. To mitigate this, selective encryption has\nemerged as a promising approach, encrypting only a subset of gradient data\nbased on the data's significance under a certain metric. However, there have\nbeen few systematic studies on how to specify this metric in practice. This\npaper systematically evaluates selective encryption methods with different\nsignificance metrics against state-of-the-art attacks. Our findings demonstrate\nthe feasibility of selective encryption in reducing computational overhead\nwhile maintaining resilience against attacks. We propose a distance-based\nsignificance analysis framework that provides theoretical foundations for\nselecting critical gradient elements for encryption. Through extensive\nexperiments on different model architectures (LeNet, CNN, BERT, GPT-2) and\nattack types, we identify gradient magnitude as a generally effective metric\nfor protection against optimization-based gradient inversions. However, we also\nobserve that no single selective encryption strategy is universally optimal\nacross all attack scenarios, and we provide guidelines for choosing appropriate\nstrategies for different model architectures and privacy requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04155v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04178", "title": "Secure Development of a Hooking-Based Deception Framework Against Keylogging Techniques", "authors": ["Md Sajidul Islam Sajid", "Shihab Ahmed", "Ryan Sosnoski"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04178v1", "summary": "Keyloggers remain a serious threat in modern cybersecurity, silently\ncapturing user keystrokes to steal credentials and sensitive information.\nTraditional defenses focus mainly on detection and removal, which can halt\nmalicious activity but do little to engage or mislead adversaries. In this\npaper, we present a deception framework that leverages API hooking to intercept\ninput-related API calls invoked by keyloggers at runtime and inject realistic\ndecoy keystrokes. A core challenge, however, lies in the increasing adoption of\nanti-hooking techniques by advanced keyloggers. Anti-hooking strategies allow\nmalware to bypass or detect instrumentation. To counter this, we introduce a\nhardened hooking layer that detects tampering and rapidly reinstates disrupted\nhooks, ensuring continuity of deception. We evaluate our framework against a\ncustom-built \"super keylogger\" incorporating multiple evasion strategies, as\nwell as 50 real-world malware samples spanning ten prominent keylogger\nfamilies. Experimental results demonstrate that our system successfully resists\nsophisticated bypass attempts, maintains operational stealth, and reliably\ndeceives attackers by feeding them decoys. The system operates with negligible\nperformance overhead and no observable impact on user experience. Our findings\nshow that resilient, runtime deception can play a practical and robust role in\nconfronting advanced threats.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04178v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04189", "title": "BadTime: An Effective Backdoor Attack on Multivariate Long-Term Time Series Forecasting", "authors": ["Kunlan Xiang", "Haomiao Yang", "Meng Hao", "Haoxin Wang", "Shaofeng Li", "Wenbo Jiang"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04189v1", "summary": "Multivariate Long-Term Time Series Forecasting (MLTSF) models are\nincreasingly deployed in critical domains such as climate, finance, and\ntransportation. Although a variety of powerful MLTSF models have been proposed\nto improve predictive performance, the robustness of MLTSF models against\nmalicious backdoor attacks remains entirely unexplored, which is crucial to\nensuring their reliable and trustworthy deployment. To address this gap, we\nconduct an in-depth study on backdoor attacks against MLTSF models and propose\nthe first effective attack method named BadTime. BadTime executes a backdoor\nattack by poisoning training data and customizing the backdoor training\nprocess. During data poisoning, BadTime proposes a contrast-guided strategy to\nselect the most suitable training samples for poisoning, then employs a graph\nattention network to identify influential variables for trigger injection.\nSubsequently, BadTime further localizes optimal positions for trigger injection\nbased on lag analysis and proposes a puzzle-like trigger structure that\ndistributes the trigger across multiple poisoned variables to jointly steer the\nprediction of the target variable. During backdoor training, BadTime\nalternately optimizes the model and triggers via proposed tailored optimization\nobjectives. Extensive experiments show that BadTime significantly outperforms\nstate-of-the-art (SOTA) backdoor attacks on time series forecasting by reducing\nMAE by over 50% on target variables and boosting stealthiness by more than 3\ntimes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04189v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04208", "title": "DP-DocLDM: Differentially Private Document Image Generation using Latent Diffusion Models", "authors": ["Saifullah Saifullah", "Stefan Agne", "Andreas Dengel", "Sheraz Ahmed"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04208v1", "summary": "As deep learning-based, data-driven information extraction systems become\nincreasingly integrated into modern document processing workflows, one primary\nconcern is the risk of malicious leakage of sensitive private data from these\nsystems. While some recent works have explored Differential Privacy (DP) to\nmitigate these privacy risks, DP-based training is known to cause significant\nperformance degradation and impose several limitations on standard training\nprocedures, making its direct application to downstream tasks both difficult\nand costly. In this work, we aim to address the above challenges within the\ncontext of document image classification by substituting real private data with\na synthetic counterpart. In particular, we propose to use conditional latent\ndiffusion models (LDMs) in combination with differential privacy (DP) to\ngenerate class-specific synthetic document images under strict privacy\nconstraints, which can then be utilized to train a downstream classifier\nfollowing standard training procedures. We investigate our approach under\nvarious pretraining setups, including unconditional, class-conditional, and\nlayout-conditional pretraining, in combination with multiple private training\nstrategies such as class-conditional and per-label private fine-tuning with\nDPDM and DP-Promise algorithms. Additionally, we evaluate it on two well-known\ndocument benchmark datasets, RVL-CDIP and Tobacco3482, and show that it can\ngenerate useful and realistic document samples across various document types\nand privacy levels ($\\varepsilon \\in \\{1, 5, 10\\}$). Lastly, we show that our\napproach achieves substantial performance improvements in downstream\nevaluations on small-scale datasets, compared to the direct application of\nDP-Adam.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04208v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04285", "title": "Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning", "authors": ["Takumi Suimon", "Yuki Koizumi", "Junji Takemasa", "Toru Hasegawa"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04285v1", "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data, but individual model updates may still leak sensitive information.\nSecure aggregation (SecAgg) mitigates this risk by allowing the server to\naccess only the sum of client updates, thereby concealing individual\ncontributions. However, a significant vulnerability has recently attracted\nincreasing attention: when model updates are sparse vectors, a non-zero value\ncontributed by a single client at a given index can be directly revealed in the\naggregate, enabling precise data reconstruction attacks. In this paper, we\npropose a novel enhancement to SecAgg that reveals aggregated values only at\nindices with at least $t$ non-zero contributions. Our mechanism introduces a\nper-element masking strategy to prevent the exposure of under-contributed\nelements, while maintaining modularity and compatibility with many existing\nSecAgg implementations by relying solely on cryptographic primitives already\nemployed in a typical setup. We integrate this mechanism into Flamingo, a\nlow-round SecAgg protocol, to provide a robust defense against such attacks.\nOur analysis and experimental results indicate that the additional\ncomputational and communication overhead introduced by our mechanism remains\nwithin an acceptable range, supporting the practicality of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04285v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04561", "title": "Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems", "authors": ["Muhammad Azmi Umer", "Chuadhry Mujeeb Ahmed", "Aditya Mathur", "Muhammad Taha Jilani"], "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04561v1", "summary": "This work focuses on validation of attack pattern mining in the context of\nIndustrial Control System (ICS) security. A comprehensive security assessment\nof an ICS requires generating a large and variety of attack patterns. For this\npurpose we have proposed a data driven technique to generate attack patterns\nfor an ICS. The proposed technique has been used to generate over 100,000\nattack patterns from data gathered from an operational water treatment plant.\nIn this work we present a detailed case study to validate the attack patterns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04561v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04583", "title": "Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies", "authors": ["Marc Damie", "Mihai Pop", "Merijn Posthuma"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04583v1", "summary": "Privacy-enhancing technologies (PETs) have attracted significant attention in\nresponse to privacy regulations, driving the development of applications that\nprioritize user data protection. At the same time, the information and\ncommunication technology (ICT) sector faces growing pressure to reduce its\nenvironmental footprint, particularly its carbon emissions. While numerous\nstudies have assessed the energy footprint of various ICT applications, the\nenvironmental footprint of cryptographic PETs remains largely unexplored.\n  Our work addresses this gap by proposing a standardized methodology for\nevaluating the carbon footprint of PETs. To demonstrate this methodology, we\nfocus on PETs supporting client-server applications as they are the simplest to\ndeploy. In particular, we measure the energy consumption and carbon footprint\nincrease induced by five cryptographic PETs (compared to their non-private\nequivalent): HTTPS web browsing, encrypted machine learning (ML) inference,\nencrypted ML training, encrypted databases, and encrypted emails. Our findings\nreveal significant variability in carbon footprint increases, ranging from a\ntwofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted\nML.\n  Our study provides essential data to help decision-makers assess\nprivacy-carbon trade-offs in such applications. Finally, we outline key\nresearch directions for developing PETs that balance strong privacy protection\nwith environmental sustainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04583v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04641", "title": "4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions", "authors": ["Kirti Singh", "Vinay J. Ribeiro", "Susmita Mandal"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04641v1", "summary": "Cross-chain asset exchange is crucial for blockchain interoperability.\nExisting solutions rely on trusted third parties and risk asset loss, or use\ndecentralized alternatives like atomic swaps, which suffer from grief attacks.\nGriefing occurs when a party prematurely exits, locking the counterparty's\nassets until a timelock expires. Hedged Atomic Swaps mitigate griefing by\nintroducing a penalty premium; however, they increase the number of\ntransactions from four (as in Tier Nolan's swap) to six, which in turn\nintroduces new griefing risks. Grief-Free (GF) Swap reduces this to five\ntransactions by consolidating assets and premiums on a single chain. However,\nno existing protocol achieves grief-free asset exchange in just four\ntransactions.\n  This paper presents 4-Swap, the first cross-chain atomic swap protocol that\nis both grief-free and bribery-safe, while completing asset exchange in just\nfour transactions. By combining the griefing premium and principal into a\nsingle transaction per chain, 4-Swap reduces on-chain transactions, leading to\nfaster execution compared to previous grief-free solutions. It is fully\ncompatible with Bitcoin and operates without the need for any new opcodes. A\ngame-theoretic analysis shows that rational participants have no incentive to\ndeviate from the protocol, ensuring robust compliance and security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04641v1", "cate": "cs.CR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03856", "title": "Evaluating Software Supply Chain Security in Research Software", "authors": ["Richard Hegewald", "Rebecca Beyer"], "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03856", "summary": "The security of research software is essential for ensuring the integrity and\nreproducibility of scientific results. However, research software security is\nstill largely unexplored. Due to its dependence on open source components and\ndistributed development practices, research software is particularly vulnerable\nto supply chain attacks. This study analyses 3,248 high-quality, largely\npeer-reviewed research software repositories using the OpenSSF Scorecard. We\nfind a generally weak security posture with an average score of 3.5/10.\nImportant practices, such as signed releases and branch protection, are rarely\nimplemented. Finally, we present actionable, low-effort recommendations that\ncan help research teams improve software security and mitigate potential\nthreats to scientific integrity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03856", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03967", "title": "RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification", "authors": ["Mamadou Keita", "Wassim Hamidouche", "Hessen Bougueffa Eutamene", "Abdelmalik Taleb-Ahmed", "Abdenour Hadid"], "categories": ["cs.CR", "cs.CV", "cs.IR"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03967", "summary": "In this paper, we introduce RAVID, the first framework for AI-generated image\ndetection that leverages visual retrieval-augmented generation (RAG). While RAG\nmethods have shown promise in mitigating factual inaccuracies in foundation\nmodels, they have primarily focused on text, leaving visual knowledge\nunderexplored. Meanwhile, existing detection methods, which struggle with\ngeneralization and robustness, often rely on low-level artifacts and\nmodel-specific features, limiting their adaptability. To address this, RAVID\ndynamically retrieves relevant images to enhance detection. Our approach\nutilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with\ncategory-related prompts to improve representation learning. We further\nintegrate a vision-language model (VLM) to fuse retrieved images with the\nquery, enriching the input and improving accuracy. Given a query image, RAVID\ngenerates an embedding using RAVID CLIP, retrieves the most relevant images\nfrom a database, and combines these with the query image to form an enriched\ninput for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the\nUniversalFakeDetect benchmark, which covers 19 generative models, show that\nRAVID achieves state-of-the-art performance with an average accuracy of 93.85%.\nRAVID also outperforms traditional methods in terms of robustness, maintaining\nhigh accuracy even under image degradations such as Gaussian blur and JPEG\ncompression. Specifically, RAVID achieves an average accuracy of 80.27% under\ndegradation conditions, compared to 63.44% for the state-of-the-art model\nC2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG\ncompression scenarios. The code will be publicly available upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03967", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03981", "title": "Reputation-based partition scheme for IoT security", "authors": ["Zhikui Chen", "Muhammad Zeeshan Haider", "Naiwen Luo", "Shuo Yu", "Xu Yuan", "Yaochen Zhang", "Tayyaba Noreen"], "categories": ["cs.CR", "cs.DB", "cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03981", "summary": "With the popularity of smart terminals, such as the Internet of Things,\ncrowdsensing is an emerging data aggregation paradigm, which plays a pivotal\nrole in data-driven applications. There are some key issues in the development\nof crowdsensing such as platform security and privacy protection. As the\ncrowdsensing is usually managed by a centralized platform, centralized\nmanagement will bring various security vulnerabilities and scalability issues.\nTo solve these issues, an effective reputation-based partition scheme (RSPC) is\nproposed in this article. The partition scheme calculates the optimal partition\nsize by combining the node reputation value and divides the node into several\ndisjoint partitions according to the node reputation value. By selecting the\nappropriate partition size, RSPC provides a mechanism to ensure that each\npartition is valid, as long as themaximum permissible threshold for the failed\nnode is observed. At the same time, the RSPC reorganizes the network\nperiodically to avoid partition attacks. In addition, for cross-partition\ntransactions, this paper innovatively proposes a four-stage confirmation\nprotocol to ensure the efficient and safe completion of cross-partition\ntransactions. Finally, experiments show that RSPC improves scalability, low\nlatency, and high throughput for crowdsensing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03981", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04000", "title": "Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability", "authors": ["Tayyaba Noreen", "Qiufen Xia", "Muhammad Zeeshan Haider"], "categories": ["cs.CR", "cs.DB", "cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04000", "summary": "In the past decade, blockchain has emerged as a promising solution for\nbuilding secure distributed ledgers and has attracted significant attention.\nHowever, current blockchain systems suffer from limited throughput, poor\nscalability, and high latency. Due to limitations in consensus mechanisms,\nespecially in managing node identities, blockchain is often considered\nunsuitable for applications such as the Internet of Things (IoT). This paper\nproposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain\nscalability and throughput. ADR employs a directed acyclic graph (DAG)\nstructure where nodes are positioned based on their rankings. Unlike\ntraditional chains, ADR allows honest nodes to write blocks and verify\ntransactions using a DAG-based topology. The protocol follows a three-step\napproach to secure the network against double-spending and enhance performance.\nFirst, it verifies nodes using their public and private keys before granting\nentry. Second, it builds an advanced DAG ledger enabling block production and\ntransaction validation. Third, a ranking algorithm filters out malicious nodes,\nranks the remaining nodes based on performance, and arranges them\ntopologically. This process increases throughput and ensures robust\nscalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,\nincluding scenarios with injected malicious nodes. Simulation results\ndemonstrate that ADR significantly improves transaction throughput and network\nliveness compared to existing DAG-based blockchains such as IOTA and ByteBall,\nmaking it well-suited for IoT applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04000", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04281", "title": "Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy", "authors": ["Jairo GudiÃ±o-Rosero", "ClÃ©ment Contet", "Umberto Grandi", "CÃ©sar A. Hidalgo"], "categories": ["cs.CR", "cs.CY"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04281", "summary": "Large Language Models (LLMs) are gaining traction as a method to generate\nconsensus statements and aggregate preferences in digital democracy\nexperiments. Yet, LLMs may introduce critical vulnerabilities in these systems.\nHere, we explore the impact of prompt-injection attacks targeting consensus\ngenerating systems by introducing a four-dimensional taxonomy of attacks. We\ntest these attacks using LLaMA 3.1 8B and Chat GPT 4.1 Nano finding the LLMs\nmore vulnerable to criticism attacks -- attacks using disagreeable prompts --\nand more effective at tilting ambiguous consensus statements. We also find\nevidence of more effective manipulation when using explicit imperatives and\nrational-sounding arguments compared to emotional language or fabricated\nstatistics. To mitigate these vulnerabilities, we apply Direct Preference\nOptimization (DPO), an alignment method that fine-tunes LLMs to prefer\nunperturbed consensus statements. While DPO significantly improves robustness,\nit still offers limited protection against attacks targeting ambiguous\nconsensus. These results advance our understanding of the vulnerability and\nrobustness of consensus generating LLMs in digital democracy applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04281", "cate": "cs.CY", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04340", "title": "Bases of Riemann-Roch spaces associated with arbitrary elliptic curve divisors and their application in constructing various elliptic Codes families", "authors": ["Artyom Kuninets", "Ekaterina Malygina"], "categories": ["cs.CR", "cs.IT", "math.AG"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04340", "summary": "In this paper, we determine explicit bases for Riemann--Roch spaces\nassociated with various families of elliptic codes. We establish the\nfeasibility and provide exact algorithms for constructing bases of\nRiemann--Roch spaces corresponding to arbitrary divisors on elliptic curves.\nThese results are subsequently applied to derive bases for quasi-cyclic\nelliptic codes and their subfield subcodes as well as for the class of\nGoppa-like elliptic codes. For algebraic geometry code applications, having an\nexplicit description of Riemann--Roch space bases for arbitrary divisors is\nparticularly valuable as it simultaneously enables efficient code construction\nand reveals structural properties of the codes leading to the new cryptanalysis\nmethods when these codes are employed in cryptographic schemes", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04340", "cate": "cs.IT", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04542", "title": "Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape", "authors": ["Haoran Niu", "K. Suzanne Barber"], "categories": ["cs.CR", "cs.LG", "cs.SI"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04542", "summary": "It is difficult for individuals and organizations to protect personal\ninformation without a fundamental understanding of relative privacy risks. By\nanalyzing over 5,000 empirical identity theft and fraud cases, this research\nidentifies which types of personal data are exposed, how frequently exposures\noccur, and what the consequences of those exposures are. We construct an\nIdentity Ecosystem graph--a foundational, graph-based model in which nodes\nrepresent personally identifiable information (PII) attributes and edges\nrepresent empirical disclosure relationships between them (e.g., the\nprobability that one PII attribute is exposed due to the exposure of another).\nLeveraging this graph structure, we develop a privacy risk prediction framework\nthat uses graph theory and graph neural networks to estimate the likelihood of\nfurther disclosures when certain PII attributes are compromised. The results\nshow that our approach effectively answers the core question: Can the\ndisclosure of a given identity attribute possibly lead to the disclosure of\nanother attribute?", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04542", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04644", "title": "Millions of inequivalent quadratic APN functions in eight variables", "authors": ["Christof Beierle", "Philippe Langevin", "Gregor Leander", "Alexandr Polujan", "Shahram Rasoolzadeh"], "categories": ["cs.CR", "cs.DM", "cs.IT", "math.CO"], "primary_category": "math.CO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04644", "summary": "The only known example of an almost perfect nonlinear (APN) permutation in\neven dimension was obtained by applying CCZ-equivalence to a specific quadratic\nAPN function. Motivated by this result, there have been numerous recent\nattempts to construct new quadratic APN functions. Currently, 32,892 quadratic\nAPN functions in dimension 8 are known and two recent conjectures address their\npossible total number. The first, proposed by Y. Yu and L. Perrin (Cryptogr.\nCommun. 14(6): 1359-1369, 2022), suggests that there are more than 50,000 such\nfunctions. The second, by A. Polujan and A. Pott (Proc. 7th Int. Workshop on\nBoolean Functions and Their Applications, 2022), argues that their number\nexceeds that of inequivalent quadratic (8,4)-bent functions, which is 92,515.\nWe computationally construct 3,775,599 inequivalent quadratic APN functions in\ndimension 8 and estimate the total number to be about 6 million.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04644", "cate": "math.CO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04669", "title": "Cybersecurity of Quantum Key Distribution Implementations", "authors": ["Ittay Alfassi", "Ran Gelles", "Rotem Liss", "Tal Mor"], "categories": ["cs.CR", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04669", "summary": "Practical implementations of Quantum Key Distribution (QKD) often deviate\nfrom the theoretical protocols, exposing the implementations to various attacks\neven when the underlying (ideal) protocol is proven secure. We present new\nanalysis tools and methodologies for quantum cybersecurity, adapting the\nconcepts of vulnerabilities, attack surfaces, and exploits from classical\ncybersecurity to QKD implementation attacks. We present three additional\nconcepts, derived from the connection between classical and quantum\ncybersecurity: \"Quantum Fuzzing\", which is the first tool for black-box\nvulnerability research on QKD implementations; \"Reversed-Space Attacks\", which\nare a generic exploit method using the attack surface of imperfect receivers;\nand a concrete quantum-mechanical definition of \"Quantum Side-Channel Attacks\",\nmeaningfully distinguishing them from other types of attacks. Using our tools,\nwe analyze multiple existing QKD attacks and show that the \"Bright\nIllumination\" attack could have been fully constructed even with minimal\nknowledge of the device implementation. This work begins to bridge the gap\nbetween current analysis methods for experimental attacks on QKD\nimplementations and the decades-long research in the field of classical\ncybersecurity, improving the practical security of QKD products and enhancing\ntheir usefulness in real-world systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04669", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2408.07916", "title": "GridSE: Towards Practical Secure Geographic Search via Prefix Symmetric Searchable Encryption (Full Version)", "authors": ["Ruoyang Guo", "Jiarui Li", "Shucheng Yu"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.07916", "summary": "The proliferation of location-based services and applications has brought\nsignificant attention to data and location privacy. While general secure\ncomputation and privacy-enhancing techniques can partially address this\nproblem, one outstanding challenge is to provide near latency-free search and\ncompatibility with mainstream geographic search techniques, especially the\nDiscrete Global Grid Systems (DGGS). This paper proposes a new construction,\nnamely GridSE, for efficient and DGGS-compatible Secure Geographic Search (SGS)\nwith both backward and forward privacy. We first formulate the notion of a\nsemantic-secure primitive called \\textit{symmetric prefix predicate encryption}\n(SP$^2$E), for predicting whether or not a keyword contains a given prefix, and\nprovide a construction. Then we extend SP$^2$E for dynamic \\textit{prefix\nsymmetric searchable encryption} (pSSE), namely GridSE, which supports both\nbackward and forward privacy. GridSE only uses lightweight primitives including\ncryptographic hash and XOR operations and is extremely efficient. Furthermore,\nwe provide a generic pSSE framework that enables prefix search for traditional\ndynamic SSE that supports only full keyword search. Experimental results over\nreal-world geographic databases of sizes (by the number of entries) from $10^3$\nto $10^7$ and mainstream DGGS techniques show that GridSE achieves a speedup of\n$150\\times$ - $5000\\times$ on search latency and a saving of $99\\%$ on\ncommunication overhead as compared to the state-of-the-art. Interestingly, even\ncompared to plaintext search, GridSE introduces only $1.4\\times$ extra\ncomputational cost and $0.9\\times$ additional communication cost. Source code\nof our scheme is available at https://github.com/rykieguo1771/GridSE-RAM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.07916", "cate": "cs.CR", "date": "2024-08-15", "updated": "2025-08-06", "section": "repl"}
{"id": "2409.11026", "title": "Prompt Obfuscation for Large Language Models", "authors": ["David Pape", "Sina Mavali", "Thorsten Eisenhofer", "Lea SchÃ¶nherr"], "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.11026", "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. They are often considered\nintellectual property, similar to the code of a software product, because of\ntheir crucial impact on the utility. However, extracting system prompts is\neasily possible. As of today, there is no effective countermeasure to prevent\nthe stealing of system prompts, and all safeguarding efforts could be evaded.\nIn this work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith little overhead. The core idea is to find a representation of the original\nsystem prompt that leads to the same functionality, while the obfuscated system\nprompt does not contain any information that allows conclusions to be drawn\nabout the original system prompt. We evaluate our approach by comparing our\nobfuscated prompt output with the output of the original prompt, using eight\ndistinct metrics to measure the lexical, character-level, and semantic\nsimilarity. We show that the obfuscated version is constantly on par with the\noriginal one. We further perform three different deobfuscation attacks with\nvarying attacker knowledge--covering both black-box and white-box\nconditions--and show that in realistic attack scenarios an attacker is unable\nto extract meaningful information. Overall, we demonstrate that prompt\nobfuscation is an effective mechanism to safeguard the intellectual property of\na system prompt while maintaining the same utility as the original prompt.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.11026", "cate": "cs.CR", "date": "2024-09-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.04850", "title": "Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams", "authors": ["Minh Trung Tran", "Nasrin Sohrabi", "Zahir Tari", "Qin Wang", "Minhui Xue", "Xiaoyu Xia"], "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04850", "summary": "We identify the slow liquidity drain (SLID) scam, an insidious and highly\nprofitable threat to decentralized finance (DeFi), posing a large-scale,\npersistent, and growing risk to the ecosystem. Unlike traditional scams such as\nrug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons\nfunds from liquidity pools over extended periods, making detection\nsignificantly more challenging. In this paper, we conducted the first\nlarge-scale empirical analysis of 319,166 liquidity pools across six major\ndecentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected\nliquidity pools, resulting in cumulative losses of more than US$103 million. We\npropose a rule-based heuristic and an enhanced machine learning model for early\ndetection. Our machine learning model achieves a detection speed 4.77 times\nfaster than the heuristic while maintaining 95% accuracy. Our study establishes\na foundation for protecting DeFi investors at an early stage and promoting\ntransparency in the DeFi ecosystem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04850", "cate": "cs.CR", "date": "2025-03-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.06989", "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application", "authors": ["Wenzhuo Xu", "Zhipeng Wei", "Xiongtao Sun", "Zonghao Ying", "Deyue Zhang", "Dongdong Yang", "Xiangzheng Zhang", "Quanchen Zou"], "categories": ["cs.CR", "cs.CV"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06989", "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06989", "cate": "cs.CR", "date": "2025-03-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.13651", "title": "Traceable Black-box Watermarks for Federated Learning", "authors": ["Jiahao Xu", "Rui Hu", "Olivera Kotevska", "Zikai Zhang"], "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13651", "summary": "Due to the distributed nature of Federated Learning (FL) systems, each local\nclient has access to the global model, posing a critical risk of model leakage.\nExisting works have explored injecting watermarks into local models to enable\nintellectual property protection. However, these methods either focus on\nnon-traceable watermarks or traceable but white-box watermarks. We identify a\ngap in the literature regarding the formal definition of traceable black-box\nwatermarking and the formulation of the problem of injecting such watermarks\ninto FL systems. In this work, we first formalize the problem of injecting\ntraceable black-box watermarks into FL. Based on the problem, we propose a\nnovel server-side watermarking method, $\\mathbf{TraMark}$, which creates a\ntraceable watermarked model for each client, enabling verification of model\nleakage in black-box settings. To achieve this, $\\mathbf{TraMark}$ partitions\nthe model parameter space into two distinct regions: the main task region and\nthe watermarking region. Subsequently, a personalized global model is\nconstructed for each client by aggregating only the main task region while\npreserving the watermarking region. Each model then learns a unique watermark\nexclusively within the watermarking region using a distinct watermark dataset\nbefore being sent back to the local client. Extensive results across various FL\nsystems demonstrate that $\\mathbf{TraMark}$ ensures the traceability of all\nwatermarked models while preserving their main task performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13651", "cate": "cs.CR", "date": "2025-05-19", "updated": "2025-08-05", "section": "repl"}
{"id": "2505.16246", "title": "Verifiable Exponential Mechanism for Median Estimation", "authors": ["Hyukjun Kwon", "Chenglin Fan"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.16246", "summary": "Differential Privacy (DP) is a rigorous privacy standard widely adopted in\ndata analysis and machine learning. However, its guarantees rely on correctly\nintroducing randomized noise--an assumption that may not hold if the\nimplementation is faulty or manipulated by an untrusted analyst. To address\nthis concern, we propose the first verifiable implementation of the exponential\nmechanism using zk-SNARKs. As a concrete application, we present the first\nverifiable differentially private (DP) median estimation scheme, which\nleverages this construction to ensure both privacy and verifiability. Our\nmethod encodes the exponential mechanism and a utility function for the median\ninto an arithmetic circuit, employing a scaled inverse CDF technique for\nsampling. This design enables cryptographic verification that the reported\noutput adheres to the intended DP mechanism, ensuring both privacy and\nintegrity without revealing sensitive data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.16246", "cate": "cs.CR", "date": "2025-05-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01694", "title": "Performance and Storage Analysis of CRYSTALS Kyber as a Post Quantum Replacement for RSA and ECC", "authors": ["Nicolas Rodriguez", "Fernando Rodriguez"], "categories": ["cs.CR"], "primary_category": "cs.CR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01694", "summary": "The steady advancement in quantum computer error correction technology has\npushed the current record to 48 stable logical qubits, bringing us closer to\nmachines capable of running Shor's algorithm at scales that threaten RSA and\nECC cryptography. While the timeline for developing such quantum computers\nremains uncertain, the cryptographic community must prepare for the transition\nto quantum-resistant algorithms. CRYSTALS-Kyber, standardized by NIST in 2022,\nrepresents a leading post-quantum cryptographic solution, but widespread\nadoption faces significant challenges. If this migration follows patterns\nsimilar to the SHA-1 to SHA-2 transition, organizations may experience\nprolonged periods of vulnerability, with substantial security and economic\nconsequences. This study evaluates Kyber's practical viability through\nperformance testing across various implementation schemes, utilizing only\nstandard built-in processor acceleration features, some of which include AES-NI\nand ASIMD, without any specialized hardware additions. Our findings demonstrate\nthat Kyber provides robust security guarantees against quantum attacks while\nmaintaining acceptable performance profiles for most contemporary applications,\nutilizing only commodity hardware with manufacturer-provided acceleration\ncapabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01694", "cate": "cs.CR", "date": "2025-08-03", "updated": "2025-08-05", "section": "repl"}
{"id": "2403.14905", "title": "Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation", "authors": ["Chengxi Li", "Ming Xiao", "Mikael Skoglund"], "categories": ["cs.CR", "cs.LG", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.14905", "summary": "In this article, we address the problem of federated learning in the presence\nof stragglers. For this problem, a coded federated learning framework has been\nproposed, where the central server aggregates gradients received from the\nnon-stragglers and gradient computed from a privacy-preservation global coded\ndataset to mitigate the negative impact of the stragglers. However, when\naggregating these gradients, fixed weights are consistently applied across\niterations, neglecting the generation process of the global coded dataset and\nthe dynamic nature of the trained model over iterations. This oversight may\nresult in diminished learning performance. To overcome this drawback, we\npropose a new method named adaptive coded federated learning (ACFL). In ACFL,\nbefore the training, each device uploads a coded local dataset with additive\nnoise to the central server to generate a global coded dataset under privacy\npreservation requirements. During each iteration of the training, the central\nserver aggregates the gradients received from the non-stragglers and the\ngradient computed from the global coded dataset, where an adaptive policy for\nvarying the aggregation weights is designed. Under this policy, we optimize the\nperformance in terms of privacy and learning, where the learning performance is\nanalyzed through convergence analysis and the privacy performance is\ncharacterized via mutual information differential privacy. Finally, we perform\nsimulations to demonstrate the superiority of ACFL compared with the\nnon-adaptive methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.14905", "cate": "eess.SP", "date": "2024-03-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2409.01062", "title": "Random Erasing vs. Model Inversion: A Promising Defense or a False Hope?", "authors": ["Viet-Hung Tran", "Ngoc-Bao Nguyen", "Son T. Mai", "Hans Vandierendonck", "Ira Assent", "Alex Kot", "Ngai-Man Cheung"], "categories": ["cs.CR", "cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.01062", "summary": "Model Inversion (MI) attacks pose a significant privacy threat by\nreconstructing private training data from machine learning models. While\nexisting defenses primarily concentrate on model-centric approaches, the impact\nof data on MI robustness remains largely unexplored. In this work, we explore\nRandom Erasing (RE), a technique traditionally used for improving model\ngeneralization under occlusion, and uncover its surprising effectiveness as a\ndefense against MI attacks. Specifically, our novel feature space analysis\nshows that models trained with RE-images introduce a significant discrepancy\nbetween the features of MI-reconstructed images and those of the private data.\nAt the same time, features of private images remain distinct from other classes\nand well-separated from different classification regions. These effects\ncollectively degrade MI reconstruction quality and attack accuracy while\nmaintaining reasonable natural accuracy. Furthermore, we explore two critical\nproperties of RE including Partial Erasure and Random Location. Partial Erasure\nprevents the model from observing entire objects during training. We find this\nhas a significant impact on MI, which aims to reconstruct the entire objects.\nRandom Location of erasure plays a crucial role in achieving a strong\nprivacy-utility trade-off. Our findings highlight RE as a simple yet effective\ndefense mechanism that can be easily integrated with existing\nprivacy-preserving techniques. Extensive experiments across 37 setups\ndemonstrate that our method achieves state-of-the-art (SOTA) performance in the\nprivacy-utility trade-off. The results consistently demonstrate the superiority\nof our defense over existing methods across different MI attacks, network\narchitectures, and attack configurations. For the first time, we achieve a\nsignificant degradation in attack accuracy without a decrease in utility for\nsome configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.01062", "cate": "cs.LG", "date": "2024-09-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.05189", "title": "Understanding In-Context Learning of Linear Models in Transformers Through an Adversarial Lens", "authors": ["Usman Anwar", "Johannes Von Oswald", "Louis Kirsch", "David Krueger", "Spencer Frei"], "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.05189", "summary": "In this work, we make two contributions towards understanding of in-context\nlearning of linear models by transformers. First, we investigate the\nadversarial robustness of in-context learning in transformers to hijacking\nattacks -- a type of adversarial attacks in which the adversary's goal is to\nmanipulate the prompt to force the transformer to generate a specific output.\nWe show that both linear transformers and transformers with GPT-2 architectures\nare vulnerable to such hijacking attacks. However, adversarial robustness to\nsuch attacks can be significantly improved through adversarial training -- done\neither at the pretraining or finetuning stage -- and can generalize to stronger\nattack models. Our second main contribution is a comparative analysis of\nadversarial vulnerabilities across transformer models and other algorithms for\nlearning linear models. This reveals two novel findings. First, adversarial\nattacks transfer poorly between larger transformer models trained from\ndifferent seeds despite achieving similar in-distribution performance. This\nsuggests that transformers of the same architecture trained according to the\nsame recipe may implement different in-context learning algorithms for the same\ntask. Second, we observe that attacks do not transfer well between classical\nlearning algorithms for linear models (single-step gradient descent and\nordinary least squares) and transformers. This suggests that there could be\nqualitative differences between the in-context learning algorithms that\ntransformers implement and these traditional algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.05189", "cate": "cs.LG", "date": "2024-11-07", "updated": "2025-08-05", "section": "repl"}
{"id": "2507.22535", "title": "Scalable and (quantum-accessible) adaptive pseudorandom quantum states and pseudorandom function-like quantum state generators", "authors": ["Rishabh Batra", "Zhili Chen", "Rahul Jain", "YaoNan Zhang"], "categories": ["cs.CR", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22535", "summary": "Pseudorandom quantum states (PRSs) and pseudorandom function-like quantum\nstate (PRFS) generators are quantum analogues of pseudorandom generators and\npseudorandom functions. It is known that PRS (and PRFS) can exist even if BQP =\nQMA (relative to a quantum oracle) [Kre21] or if P = NP (relative to a\nclassical oracle) [KQST23], which does not allow for the existence of one-way\nfunctions (relative to these oracles). Hence, these are potentially weaker\nobjects than quantum-secure one-way functions, which can be used to do quantum\ncryptography.\n  A desirable property of PRS and PRFS constructions is scalability, which\nensures that the security parameter $\\lambda$ (which determines\nindistinguishability from their Haar-random counterparts) can be much larger\nthan $n$ (the number of qubits of the output states). This may be important in\nsome applications where PRS and PRFS primitives are used.\n  We present an isometric procedure to prepare quantum states that can be\narbitrarily random (i.e., the trace distance from the Haar-random state can be\narbitrarily small for the true random case, or the distinguishing advantage can\nbe arbitrarily small for the pseudorandom case). Our procedure provides a new\nmethod for scalable PRS that introduces no entanglement or correlations with\nthe environment. This naturally gives the first construction for scalable and\n(quantum-accessible) adaptive PRFS assuming quantum-secure one-way functions.\n  Our PRFS construction implies various primitives, including long-input PRFS,\nshort-input PRFS, short-output PRFS, non-adaptive PRFS, and\nclassical-accessible adaptive PRFS [AQY22, AGQY22]. This new construction may\nbe helpful in some simplification of the microcrypt zoo\n(https://sattath.github.io/microcrypt-zoo/).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22535", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03699", "title": "Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task", "authors": ["Subin Raj Peter"], "categories": ["cs.CV", "cs.HC", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03699v1", "summary": "Virtual Reality (VR) has emerged as a powerful tool for workforce training,\noffering immersive, interactive, and risk-free environments that enhance skill\nacquisition, decision-making, and confidence. Despite its advantages,\ndeveloping VR applications for training remains a significant challenge due to\nthe time, expertise, and resources required to create accurate and engaging\ninstructional content. To address these limitations, this paper proposes a\nnovel approach that leverages Large Language Models (LLMs) to automate the\ngeneration of virtual instructions from textual input. The system comprises two\ncore components: an LLM module that extracts task-relevant information from the\ntext, and an intelligent module that transforms this information into animated\ndemonstrations and visual cues within a VR environment. The intelligent module\nreceives input from the LLM module and interprets the extracted information.\nBased on this, an instruction generator creates training content using relevant\ndata from a database. The instruction generator generates the instruction by\nchanging the color of virtual objects and creating animations to illustrate\ntasks. This approach enhances training effectiveness and reduces development\noverhead, making VR-based training more scalable and adaptable to evolving\nindustrial needs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03699v1", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-19", "section": "new"}
{"id": "2508.03720", "title": "Outlier Detection Algorithm for Circle Fitting", "authors": ["Ahmet GÃ¶khan Poyraz"], "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03720v1", "summary": "Circle fitting methods are extensively utilized in various industries,\nparticularly in quality control processes and design applications. The\neffectiveness of these algorithms can be significantly compromised when the\npoint sets to be predicted are noisy. To mitigate this issue, outlier detection\nand removal algorithms are often applied before the circle fitting procedure.\nThis study introduces the Polar Coordinate-Based Outlier Detection (PCOD)\nalgorithm, which can be effectively employed in circle fitting applications. In\nthe proposed approach, the point set is first transformed into polar\ncoordinates, followed by the calculation of both local and global standard\ndeviations. Outliers are then identified by comparing local mean values with\nthe global standard deviation. The practicality and efficiency of the proposed\nmethod are demonstrated by focusing on the high-precision diameter measurement\nof industrial washer parts. Images from a machine vision system are processed\nthrough preprocessing steps, including sub-pixel edge detection. The resulting\nsub-pixel edge points are then cleaned using the proposed outlier detection and\nremoval algorithm, after which circle fitting is performed. A comparison is\nmade using ten different circle fitting algorithms and five distinct outlier\ndetection methods. The results indicate that the proposed method outperforms\nthe other approaches, delivering the best performance in terms of accuracy\nwithin the dataset, thereby demonstrating its potential for enhancing circle\nfitting applications in industrial environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03720v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28", "section": "new"}
{"id": "2508.03721", "title": "Enhancing Diameter Measurement Accuracy in Machine Vision Applications", "authors": ["Ahmet Gokhan Poyraz", "Ahmet Emir Dirik", "Hakan Gurkan", "Mehmet Kacmaz"], "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03721v1", "summary": "In camera measurement systems, specialized equipment such as telecentric\nlenses is often employed to measure parts with narrow tolerances. However,\ndespite the use of such equipment, measurement errors can occur due to\nmechanical and software-related factors within the system. These errors are\nparticularly evident in applications where parts of different diameters are\nmeasured using the same setup. This study proposes two innovative approaches to\nenhance measurement accuracy using multiple known reference parts: a conversion\nfactor-based method and a pixel-based method. In the first approach, the\nconversion factor is estimated from known references to calculate the diameter\n(mm) of the unknown part. In the second approach, the diameter (mm) is directly\nestimated using pixel-based diameter information from the references. The\nexperimental setup includes an industrial-grade camera and telecentric lenses.\nTests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show\nthat measurement errors, which originally ranged from 13-114 micrometers, were\nreduced to 1-2 micrometers using the proposed methods. By utilizing only a few\nknown reference parts, the proposed approach enables high-accuracy measurement\nof all parts within the camera's field of view. Additionally, this method\nenhances the existing diameter measurement literature by significantly reducing\nerror rates and improving measurement reliability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03721v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28", "section": "new"}
{"id": "2508.03724", "title": "From Waveforms to Pixels: A Survey on Audio-Visual Segmentation", "authors": ["Jia Li", "Yapeng Tian"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03724v1", "summary": "Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing\nobjects in videos by leveraging both visual and audio modalities. It has\nemerged as a significant research area in multimodal perception, enabling\nfine-grained object-level understanding. In this survey, we present a\ncomprehensive overview of the AVS field, covering its problem formulation,\nbenchmark datasets, evaluation metrics, and the progression of methodologies.\nWe analyze a wide range of approaches, including architectures for unimodal and\nmultimodal encoding, key strategies for audio-visual fusion, and various\ndecoder designs. Furthermore, we examine major training paradigms, from fully\nsupervised learning to weakly supervised and training-free methods. Notably, we\nprovide an extensive comparison of AVS methods across standard benchmarks,\nhighlighting the impact of different architectural choices, fusion strategies,\nand training paradigms on performance. Finally, we outline the current\nchallenges, such as limited temporal modeling, modality bias toward vision,\nlack of robustness in complex environments, and high computational demands, and\npropose promising future directions, including improving temporal reasoning and\nmultimodal fusion, leveraging foundation models for better generalization and\nfew-shot learning, reducing reliance on labeled data through selfand weakly\nsupervised learning, and incorporating higher-level reasoning for more\nintelligent AVS systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03724v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29", "section": "new"}
{"id": "2508.03725", "title": "A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding", "authors": ["Yida Wang", "Taiting Lu", "Runze Liu", "Lanqing Yang", "Yifan Yang", "Zhe Chen", "Yuehai Wang", "Yixin Liu", "Kaiyuan Lin", "Xiaomeng Chen", "Dian Ding", "Yijie Li", "Yi-Chao Chen", "Yincheng Jin", "Mahanth Gowda"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03725v1", "summary": "Printed-Circuit-board (PCB) footprint geometry labeling of integrated\ncircuits (IC) is essential in defining the physical interface between\ncomponents and the PCB layout, requiring exceptional visual perception\nproficiency. However, due to the unstructured footprint drawing and abstract\ndiagram annotations, automated parsing and accurate footprint geometry modeling\nremain highly challenging. Despite its importance, no methods currently exist\nfor automated package geometry labeling directly from IC mechanical drawings.\nIn this paper, we first investigate the visual perception performance of Large\nMultimodal Models (LMMs) when solving IC footprint geometry understanding. Our\nfindings reveal that current LMMs severely suffer from inaccurate geometric\nperception, which hinders their performance in solving the footprint geometry\nlabeling problem. To address these limitations, we propose LLM4-IC8K, a novel\nframework that treats IC mechanical drawings as images and leverages LLMs for\nstructured geometric interpretation. To mimic the step-by-step reasoning\napproach used by human engineers, LLM4-IC8K addresses three sub-tasks:\nperceiving the number of pins, computing the center coordinates of each pin,\nand estimating the dimensions of individual pins. We present a two-stage\nframework that first trains LMMs on synthetically generated IC footprint\ndiagrams to learn fundamental geometric reasoning and then fine-tunes them on\nreal-world datasheet drawings to enhance robustness and accuracy in practical\nscenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with\n8,608 labeled samples, including 4138 hand-crafted IC footprint samples and\n4470 synthetically generated samples. Extensive experiments demonstrate that\nour model outperforms state-of-the-art LMMs on the proposed benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03725v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30", "section": "new"}
{"id": "2508.03727", "title": "TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization", "authors": ["Tai Hyoung Rhee", "Dong-guw Lee", "Ayoung Kim"], "categories": ["cs.CV", "cs.RO", "eess.IV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03727v1", "summary": "Thermal infrared imaging exhibits considerable potentials for robotic\nperception tasks, especially in environments with poor visibility or\nchallenging lighting conditions. However, TIR images typically suffer from\nheavy non-uniform fixed-pattern noise, complicating tasks such as object\ndetection, localization, and mapping. To address this, we propose a\ndiffusion-based TIR image denoising framework leveraging latent-space\nrepresentations and wavelet-domain optimization. Utilizing a pretrained stable\ndiffusion model, our method fine-tunes the model via a novel loss function\ncombining latent-space and discrete wavelet transform (DWT) / dual-tree complex\nwavelet transform (DTCWT) losses. Additionally, we implement a cascaded\nrefinement stage to enhance fine details, ensuring high-fidelity denoising\nresults. Experiments on benchmark datasets demonstrate superior performance of\nour approach compared to state-of-the-art denoising methods. Furthermore, our\nmethod exhibits robust zero-shot generalization to diverse and challenging\nreal-world TIR datasets, underscoring its effectiveness for practical robotic\ndeployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03727v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30", "section": "new"}
{"id": "2508.03732", "title": "What is Beneath Misogyny: Misogynous Memes Classification and Explanation", "authors": ["Kushal Kanwar", "Dushyant Singh Chauhan", "Gopendra Vikram Singh", "Asif Ekbal"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03732v1", "summary": "Memes are popular in the modern world and are distributed primarily for\nentertainment. However, harmful ideologies such as misogyny can be propagated\nthrough innocent-looking memes. The detection and understanding of why a meme\nis misogynous is a research challenge due to its multimodal nature (image and\ntext) and its nuanced manifestations across different societal contexts. We\nintroduce a novel multimodal approach, \\textit{namely},\n\\textit{\\textbf{MM-Misogyny}} to detect, categorize, and explain misogynistic\ncontent in memes. \\textit{\\textbf{MM-Misogyny}} processes text and image\nmodalities separately and unifies them into a multimodal context through a\ncross-attention mechanism. The resulting multimodal context is then easily\nprocessed for labeling, categorization, and explanation via a classifier and\nLarge Language Model (LLM). The evaluation of the proposed model is performed\non a newly curated dataset (\\textit{\\textbf{W}hat's \\textbf{B}eneath\n\\textbf{M}isogynous \\textbf{S}tereotyping (WBMS)}) created by collecting\nmisogynous memes from cyberspace and categorizing them into four categories,\n\\textit{namely}, Kitchen, Leadership, Working, and Shopping. The model not only\ndetects and classifies misogyny, but also provides a granular understanding of\nhow misogyny operates in domains of life. The results demonstrate the\nsuperiority of our approach compared to existing methods. The code and dataset\nare available at\n\\href{https://github.com/kushalkanwarNS/WhatisBeneathMisogyny/tree/main}{https://github.com/Misogyny}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03732v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30", "section": "new"}
{"id": "2508.03749", "title": "Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation", "authors": ["Riccardo Fiorista", "Awad Abdelhalim", "Anson F. Stewart", "Gabriel L. Pincus", "Ian Thistle", "Jinhua Zhao"], "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03749v1", "summary": "Accurately estimating urban rail platform occupancy can enhance transit\nagencies' ability to make informed operational decisions, thereby improving\nsafety, operational efficiency, and customer experience, particularly in the\ncontext of crowding. However, sensing real-time crowding remains challenging\nand often depends on indirect proxies such as automatic fare collection data or\nstaff observations. Recently, Closed-Circuit Television (CCTV) footage has\nemerged as a promising data source with the potential to yield accurate,\nreal-time occupancy estimates. The presented study investigates this potential\nby comparing three state-of-the-art computer vision approaches for extracting\ncrowd-related features from platform CCTV imagery: (a) object detection and\ncounting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification\nvia a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic\nsegmentation using DeepLabV3. Additionally, we present a novel, highly\nefficient linear-optimization-based approach to extract counts from the\ngenerated segmentation maps while accounting for image object depth and, thus,\nfor passenger dispersion along a platform. Tested on a privacy-preserving\ndataset created in collaboration with the Washington Metropolitan Area Transit\nAuthority (WMATA) that encompasses more than 600 hours of video material, our\nresults demonstrate that computer vision approaches can provide substantive\nvalue for crowd estimation. This work demonstrates that CCTV image data,\nindependent of other data sources available to a transit agency, can enable\nmore precise real-time crowding estimation and, eventually, timely operational\nresponses for platform crowding mitigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03749v1", "cate": "cs.CV", "date": "2025-08-03", "updated": "2025-08-03", "section": "new"}
{"id": "2508.03751", "title": "Modular Transformer Architecture for Precision Agriculture Imaging", "authors": ["Brian Gopalan", "Nathalia Nascimento", "Vishal Monga"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03751v1", "summary": "This paper addresses the critical need for efficient and accurate weed\nsegmentation from drone video in precision agriculture. A quality-aware modular\ndeep-learning framework is proposed that addresses common image degradation by\nanalyzing quality conditions-such as blur and noise-and routing inputs through\nspecialized pre-processing and transformer models optimized for each\ndegradation type. The system first analyzes drone images for noise and blur\nusing Mean Absolute Deviation and the Laplacian. Data is then dynamically\nrouted to one of three vision transformer models: a baseline for clean images,\na modified transformer with Fisher Vector encoding for noise reduction, or\nanother with an unrolled Lucy-Robinson decoder to correct blur. This novel\nrouting strategy allows the system to outperform existing CNN-based methods in\nboth segmentation quality and computational efficiency, demonstrating a\nsignificant advancement in deep-learning applications for agriculture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03751v1", "cate": "cs.CV", "date": "2025-08-04", "updated": "2025-08-04", "section": "new"}
{"id": "2508.03754", "title": "Generating Synthetic Invoices via Layout-Preserving Content Replacement", "authors": ["Bevin V", "Ananthakrishnan P V", "Ragesh KR", "Sanjay M", "Vineeth S", "Bibin Wilson"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03754v1", "summary": "The performance of machine learning models for automated invoice processing\nis critically dependent on large-scale, diverse datasets. However, the\nacquisition of such datasets is often constrained by privacy regulations and\nthe high cost of manual annotation. To address this, we present a novel\npipeline for generating high-fidelity, synthetic invoice documents and their\ncorresponding structured data. Our method first utilizes Optical Character\nRecognition (OCR) to extract the text content and precise spatial layout from a\nsource invoice. Select data fields are then replaced with contextually\nrealistic, synthetic content generated by a large language model (LLM).\nFinally, we employ an inpainting technique to erase the original text from the\nimage and render the new, synthetic text in its place, preserving the exact\nlayout and font characteristics. This process yields a pair of outputs: a\nvisually realistic new invoice image and a perfectly aligned structured data\nfile (JSON) reflecting the synthetic content. Our approach provides a scalable\nand automated solution to amplify small, private datasets, enabling the\ncreation of large, varied corpora for training more robust and accurate\ndocument intelligence models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03754v1", "cate": "cs.CV", "date": "2025-08-04", "updated": "2025-08-04", "section": "new"}
{"id": "2508.03789", "title": "HPSv3: Towards Wide-Spectrum Human Preference Score", "authors": ["Yuhang Ma", "Xiaoshi Wu", "Keqiang Sun", "Hongsheng Li"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03789v1", "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03789v1", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03925", "title": "Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model", "authors": ["Shen Zhu", "Yinzhu Jin", "Ifrah Zawar", "P. Thomas Fletcher"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03925v1", "summary": "We propose a diffusion model designed to generate point-based shape\nrepresentations with correspondences. Traditional statistical shape models have\nconsidered point correspondences extensively, but current deep learning methods\ndo not take them into account, focusing on unordered point clouds instead.\nCurrent deep generative models for point clouds do not address generating\nshapes with point correspondences between generated shapes. This work aims to\nformulate a diffusion model that is capable of generating realistic point-based\nshape representations, which preserve point correspondences that are present in\nthe training data. Using shape representation data with correspondences derived\nfrom Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our\ncorrespondence-preserving model effectively generates point-based hippocampal\nshape representations that are highly realistic compared to existing methods.\nWe further demonstrate the applications of our generative model by downstream\ntasks, such as conditional generation of healthy and AD subjects and predicting\nmorphological changes of disease progression by counterfactual generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03925v1", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03955", "title": "Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm", "authors": ["Lin Zhang", "Zefan Cai", "Yufan Zhou", "Shentong Mo", "Jinhong Lin", "Cheng-En Wu", "Yibing Wei", "Yijing Zhang", "Ruiyi Zhang", "Wen Xiao", "Tong Sun", "Junjie Hu", "Pedro Morgado"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03955v1", "summary": "Recent advances in audio-synchronized visual animation enable control of\nvideo content using audios from specific classes. However, existing methods\nrely heavily on expensive manual curation of high-quality, class-specific\ntraining videos, posing challenges to scaling up to diverse audio-video classes\nin the open world. In this work, we propose an efficient two-stage training\nparadigm to scale up audio-synchronized visual animation using abundant but\nnoisy videos. In stage one, we automatically curate large-scale videos for\npretraining, allowing the model to learn diverse but imperfect audio-video\nalignments. In stage two, we finetune the model on manually curated\nhigh-quality examples, but only at a small scale, significantly reducing the\nrequired human effort. We further enhance synchronization by allowing each\nframe to access rich audio context via multi-feature conditioning and window\nattention. To efficiently train the model, we leverage pretrained text-to-video\ngenerator and audio encoders, introducing only 1.9\\% additional trainable\nparameters to learn audio-conditioning capability without compromising the\ngenerator's prior knowledge. For evaluation, we introduce AVSync48, a benchmark\nwith videos from 48 classes, which is 3$\\times$ more diverse than previous\nbenchmarks. Extensive experiments show that our method significantly reduces\nreliance on manual curation by over 10$\\times$, while generalizing to many open\nclasses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03955v1", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03996", "title": "Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images", "authors": ["Michele Andrade", "Guilherme A. L. Silva", "ValÃ©ria Santos", "Gladston Moreira", "Eduardo Luz"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03996v1", "summary": "Estimating the nutritional content of food from images is a critical task\nwith significant implications for health and dietary monitoring. This is\nchallenging, especially when relying solely on 2D images, due to the\nvariability in food presentation, lighting, and the inherent difficulty in\ninferring volume and mass without depth information. Furthermore,\nreproducibility in this domain is hampered by the reliance of state-of-the-art\nmethods on proprietary datasets for large-scale pre-training. In this paper, we\ninvestigate the impact of large-scale pre-training datasets on the performance\nof deep learning models for nutritional estimation using only 2D images. We\nfine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large\npublic datasets, ImageNet and COYO, comparing their performance against\nbaseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method\npre-trained on the proprietary JFT-300M dataset. We conduct extensive\nexperiments on the Nutrition5k dataset, a large-scale collection of real-world\nfood plates with high-precision nutritional annotations. Our evaluation using\nMean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals\nthat models pre-trained on JFT-300M significantly outperform those pre-trained\non public datasets. Unexpectedly, the model pre-trained on the massive COYO\ndataset performs worse than the model pre-trained on ImageNet for this specific\nregression task, refuting our initial hypothesis. Our analysis provides\nquantitative evidence highlighting the critical role of pre-training dataset\ncharacteristics, including scale, domain relevance, and curation quality, for\neffective transfer learning in 2D nutritional estimation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03996v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03997", "title": "JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation", "authors": ["Zheng Zhang", "Tianzhuzi Tan", "Guanchun Yin", "Bo Zhang", "Xiuzhuang Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03997v1", "summary": "Limited by the scarcity of training samples and annotations, weakly\nsupervised medical image segmentation often employs data augmentation to\nincrease data diversity, while randomly mixing volumetric blocks has\ndemonstrated strong performance. However, this approach disrupts the inherent\nanatomical continuity of 3D medical images along orthogonal axes, leading to\nsevere structural inconsistencies and insufficient training in challenging\nregions, such as small-sized organs, etc. To better comply with and utilize\nhuman anatomical information, we propose JanusNet}, a data augmentation\nframework for 3D medical data that globally models anatomical continuity while\nlocally focusing on hard-to-segment regions. Specifically, our Slice-Block\nShuffle step performs aligned shuffling of same-index slice blocks across\nvolumes along a random axis, while preserving the anatomical context on planes\nperpendicular to the perturbation axis. Concurrently, the Confidence-Guided\nDisplacement step uses prediction reliability to replace blocks within each\nslice, amplifying signals from difficult areas. This dual-stage, axis-aligned\nframework is plug-and-play, requiring minimal code changes for most\nteacher-student schemes. Extensive experiments on the Synapse and AMOS datasets\ndemonstrate that JanusNet significantly surpasses state-of-the-art methods,\nachieving, for instance, a 4% DSC gain on the Synapse dataset with only 20%\nlabeled data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03997v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04002", "title": "CAD-Judge: Toward Efficient Morphological Grading and Verification for Text-to-CAD Generation", "authors": ["Zheyuan Zhou", "Jiayi Han", "Liang Du", "Naiyu Fang", "Lemiao Qiu", "Shuyou Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04002v1", "summary": "Computer-Aided Design (CAD) models are widely used across industrial design,\nsimulation, and manufacturing processes. Text-to-CAD systems aim to generate\neditable, general-purpose CAD models from textual descriptions, significantly\nreducing the complexity and entry barrier associated with traditional CAD\nworkflows. However, rendering CAD models can be slow, and deploying VLMs to\nreview CAD models can be expensive and may introduce reward hacking that\ndegrades the systems. To address these challenges, we propose CAD-Judge, a\nnovel, verifiable reward system for efficient and effective CAD preference\ngrading and grammatical validation. We adopt the Compiler-as-a-Judge Module\n(CJM) as a fast, direct reward signal, optimizing model alignment by maximizing\ngenerative utility through prospect theory. To further improve the robustness\nof Text-to-CAD in the testing phase, we introduce a simple yet effective\nagentic CAD generation approach and adopt the Compiler-as-a-Review Module\n(CRM), which efficiently verifies the generated CAD models, enabling the system\nto refine them accordingly. Extensive experiments on challenging CAD datasets\ndemonstrate that our method achieves state-of-the-art performance while\nmaintaining superior efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04002v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04016", "title": "$\\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation", "authors": ["Weilun Feng", "Haotong Qin", "Chuanguang Yang", "Xiangqi Li", "Han Yang", "Yuqi Li", "Zhulin An", "Libo Huang", "Michele Magno", "Yongjun Xu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04016v1", "summary": "Diffusion transformers have emerged as the mainstream paradigm for video\ngeneration models. However, the use of up to billions of parameters incurs\nsignificant computational costs. Quantization offers a promising solution by\nreducing memory usage and accelerating inference. Nonetheless, we observe that\nthe joint modeling of spatial and temporal information in video diffusion\nmodels (V-DMs) leads to extremely long token sequences, which introduces high\ncalibration variance and learning challenges. To address these issues, we\npropose \\textbf{$\\text{S}^2$Q-VDiT}, a post-training quantization framework for\nV-DMs that leverages \\textbf{S}alient data and \\textbf{S}parse token\ndistillation. During the calibration phase, we identify that quantization\nperformance is highly sensitive to the choice of calibration data. To mitigate\nthis, we introduce \\textit{Hessian-aware Salient Data Selection}, which\nconstructs high-quality calibration datasets by considering both diffusion and\nquantization characteristics unique to V-DMs. To tackle the learning\nchallenges, we further analyze the sparse attention patterns inherent in V-DMs.\nBased on this observation, we propose \\textit{Attention-guided Sparse Token\nDistillation}, which exploits token-wise attention distributions to emphasize\ntokens that are more influential to the model's output. Under W4A6\nquantization, $\\text{S}^2$Q-VDiT achieves lossless performance while delivering\n$3.9\\times$ model compression and $1.3\\times$ inference acceleration. Code will\nbe available at\n\\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04016v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04017", "title": "Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability", "authors": ["Haiqi Yang", "Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04017v1", "summary": "Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing\nformidable capabilities in handling intricate multimodal tasks with exceptional\nperformance. Recent research has underscored the inclination of large language\nmodels to passively accept defective inputs, often resulting in futile\nreasoning on invalid prompts. However, the same critical question of whether\nLMMs can actively detect and scrutinize erroneous inputs still remains\nunexplored. To address this gap, we introduce the Input Scrutiny Ability\nEvaluation Framework (ISEval), which encompasses seven categories of flawed\npremises and three evaluation metrics. Our extensive evaluation of ten advanced\nLMMs has identified key findings. Most models struggle to actively detect\nflawed textual premises without guidance, which reflects a strong reliance on\nexplicit prompts for premise error identification. Error type affects\nperformance: models excel at identifying logical fallacies but struggle with\nsurface-level linguistic errors and certain conditional flaws. Modality trust\nvaries-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info,\nwhile aya-vision-8b over-rely on text in conflicts. These insights underscore\nthe urgent need to enhance LMMs' proactive verification of input validity and\nshed novel insights into mitigating the problem. The code is available at\nhttps://github.com/MLGroupJLU/LMM_ISEval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04017v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04504", "title": "Moving beyond harm. A critical review of how NLP research approaches discrimination", "authors": ["Katrin Schulz", "Marjolein Lanzing", "Giulia Martinez Brenner"], "categories": ["cs.CY"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04504v1", "summary": "How to avoid discrimination in the context of NLP technology is one of the\nmajor challenges in the field. We propose that a different and more\nsubstantiated framing of the problem could help to find more productive\napproaches. In the first part of the paper we report on a case study: a\nqualitative review on papers published in the ACL anthologies 2022 on\ndiscriminatory behavior of NLP systems. We find that the field (i) still has a\nstrong focus on a technological fix of algorithmic discrimination, and (ii) is\nstruggling with a firm grounding of their ethical or normative vocabulary.\nFurthermore, this vocabulary is very limited, focusing mostly on the terms\n\"harm\" and \"bias\". In the second part of the paper we argue that addressing the\nlatter problems might help with the former. The understanding of algorithmic\ndiscrimination as a technological problem is reflected in and reproduced by the\nvocabulary in use. The notions of \"harm\" and \"bias\" implicate a narrow framing\nof the issue of discrimination as one of the system-user interface. We argue\nthat instead of \"harm\" the debate should make \"injustice\" the key notion. This\nwould force us to understand the problem of algorithmic discrimination as a\nsystemic problem. Thereby, it would broaden our perspective on the complex\ninteractions that make NLP technology participate in discrimination. With that\ngain in perspective we can consider new angles for solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04504v1", "cate": "cs.CY", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03705", "title": "Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth", "authors": ["Kanan Eldarov"], "categories": ["cs.CY", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03705", "summary": "This study explores how different modes of digital interaction -- namely,\ncomputers versus smartphones -- affect attention, frustration, and creative\nperformance in adolescents. Using a combination of digital task logs,\nwebcam-based gaze estimation, and expert evaluation of task outcomes, we\nanalyzed data from a diverse sample of 824 students aged 11-17. Participants\nwere assigned to device groups in a randomized and stratified design to control\nfor age, gender, and prior experience. Results suggest moderate but\nstatistically significant differences in sustained attention, perceived\nfrustration, and creative output. These findings indicate that the nature of\ndigital interaction -- beyond mere screen time -- may influence cognitive and\nbehavioral outcomes relevant to educational design. Practical implications for\nuser interface development and learning environments are discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03705", "cate": "cs.HC", "date": "2025-07-20", "updated": "2025-07-20", "section": "cross"}
{"id": "2508.03792", "title": "Recommending With, Not For: Co-Designing Recommender Systems for Social Good", "authors": ["Michael D. Ekstrand", "Afsaneh Razi", "Aleksandra Sarcevic", "Maria Soledad Pera", "Robin Burke", "Katherine Landau Wright"], "categories": ["cs.CY", "cs.HC", "cs.IR"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03792", "summary": "Recommender systems are usually designed by engineers, researchers,\ndesigners, and other members of development teams. These systems are then\nevaluated based on goals set by the aforementioned teams and other business\nunits of the platforms operating the recommender systems. This design approach\nemphasizes the designers' vision for how the system can best serve the\ninterests of users, providers, businesses, and other stakeholders. Although\ndesigners may be well-informed about user needs through user experience and\nmarket research, they are still the arbiters of the system's design and\nevaluation, with other stakeholders' interests less emphasized in user-centered\ndesign and evaluation. When extended to recommender systems for social good,\nthis approach results in systems that reflect the social objectives as\nenvisioned by the designers and evaluated as the designers understand them.\nInstead, social goals and operationalizations should be developed through\nparticipatory and democratic processes that are accountable to their\nstakeholders. We argue that recommender systems aimed at improving social good\nshould be designed *by* and *with*, not just *for*, the people who will\nexperience their benefits and harms. That is, they should be designed in\ncollaboration with their users, creators, and other stakeholders as full\nco-designers, not only as user study participants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03792", "cate": "cs.HC", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04668", "title": "Inequality in the Age of Pseudonymity", "authors": ["Aviv Yaish", "Nir Chemaya", "Lin William Cong", "Dahlia Malkhi"], "categories": ["cs.CY", "cs.GT", "econ.TH"], "primary_category": "cs.GT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04668", "summary": "Inequality measures such as the Gini coefficient are used to inform and\nmotivate policymaking, and are increasingly applied to digital platforms. We\nanalyze how measures fare in pseudonymous settings, as common to internet-based\nor blockchain-based platforms. One key challenge that arises is the ability of\nactors to create multiple fake identities under fictitious false names, also\nknown as ``Sybils.'' While some actors may do so to preserve their privacy, we\nshow that this can inadvertently distort inequality metrics. As we show, when\nusing inequality measures that satisfy literature's canonical set of desired\nproperties, the presence of Sybils in an economy implies that it is impossible\nto properly measure the economy's inequality. Then, we present several classes\nof Sybil-proof measures that satisfy relaxed versions of the aforementioned\ndesired properties, and, by fully characterizing them, we prove that the\nstructure imposed restricts their ability to assess inequality at a\nfine-grained level. In addition, we prove that popular inequality metrics,\nincluding the famous Gini coefficient, are vulnerable to Sybil manipulations,\nand examine the dynamics that result in the creation of Sybils, whether in\npseudonymous settings or traditional ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04668", "cate": "cs.GT", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2502.17730", "title": "Gender Bias in Perception of Human Managers Extends to AI Managers", "authors": ["Hao Cui", "Taha Yasseri"], "categories": ["cs.CY"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.17730", "summary": "As AI becomes more embedded in workplaces, it is shifting from a tool for\nefficiency to an active force in organizational decision-making. Whether due to\nanthropomorphism or intentional design choices, people often assign human-like\nqualities, including gender, to AI systems. However, how AI managers are\nperceived in comparison to human managers and how gender influences these\nperceptions remains uncertain. To investigate this, we conducted randomized\ncontrolled trials (RCTs) where teams of three participants worked together\nunder a randomly assigned manager. The manager was either a human or an AI and\nwas presented as male, female, or gender-unspecified. The manager's role was to\nselect the best-performing team member for an additional award. Our findings\nreveal that while participants initially showed no strong preference based on\nmanager type or gender, their perceptions changed significantly after\nexperiencing the award process. As expected, those who received awards rated\ntheir managers as more trustworthy, competent, fair, and were more willing to\nwork with similar managers in the future, while those who were not selected\nviewed them less favorably. However, male managers, whether human or AI, were\nmore positively received by awarded participants, whereas female managers,\nespecially female AI managers, faced greater skepticism and negative judgments\nwhen they did not give awards. These results suggest that gender bias in\nleadership extends beyond human managers to include AI-driven decision-makers.\nAs AI assumes more managerial responsibilities, understanding and addressing\nthese biases will be crucial for designing fair and effective AI management\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.17730", "cate": "cs.CY", "date": "2025-02-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.09030", "title": "Authoritarian Recursions: How Fiction, History, and AI Reinforce Control in Education, Warfare, and Discourse", "authors": ["Hasan Oguz"], "categories": ["cs.CY"], "primary_category": "cs.CY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.09030", "summary": "This article introduces the concept of \\textit{authoritarian recursion} to\ntheorize how AI systems consolidate institutional control across education,\nwarfare, and digital discourse. It identifies a shared recursive architecture\nin which algorithms mediate judgment, obscure accountability, and constrain\nmoral and epistemic agency.\n  Grounded in critical discourse analysis and sociotechnical ethics, the paper\nexamines how AI systems normalize hierarchy through abstraction and feedback.\nCase studies -- automated proctoring, autonomous weapons, and content\nrecommendation -- are analyzed alongside cultural imaginaries such as Orwell's\n\\textit{Nineteen Eighty-Four}, Skynet, and \\textit{Black Mirror}, used as\nheuristic tools to surface ethical blind spots.\n  The analysis integrates Fairness, Accountability, and Transparency (FAccT),\nrelational ethics, and data justice to explore how predictive infrastructures\nenable moral outsourcing and epistemic closure. By reframing AI as a\ncommunicative and institutional infrastructure, the article calls for\ngovernance approaches that center democratic refusal, epistemic plurality, and\nstructural accountability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.09030", "cate": "cs.CY", "date": "2025-04-12", "updated": "2025-08-06", "section": "repl"}
{"id": "2409.17186", "title": "Don't Trust A Single Gerrymandering Metric", "authors": ["Thomas Ratliff", "Stephanie Somersille", "Ellen Veomett"], "categories": ["cs.CY", "physics.soc-ph"], "primary_category": "physics.soc-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.17186", "summary": "In recent years, in an effort to promote fairness in the election process, a\nwide variety of techniques and metrics have been proposed to determine whether\na map is a partisan gerrymander. The most accessible measures, requiring easily\nobtained data, are metrics such as the Mean-Median Difference, Efficiency Gap,\nDeclination, and GEO metric. But for most of these metrics, researchers have\nstruggled to describe, given no additional information, how a value of that\nmetric on a single map indicates the presence or absence of gerrymandering.\n  Our main result is that each of these metrics is gameable when used as a\nsingle, isolated quantity to detect gerrymandering (or the lack thereof). That\nis, for each of the four metrics, we can find district plans for a given state\nwith an extremely large number of Democratic-won (or Republican-won) districts\nwhile the metric value of that plan falls within a reasonable, predetermined\nbound. We do this by using a hill-climbing method to generate district plans\nthat are constrained by the bounds on the metric but also maximize or nearly\nmaximize the number of districts won by a party.\n  In addition, extreme values of the Mean-Median Difference do not necessarily\ncorrespond to maps with an extreme number of districts won. Thus, the Mean-\nMedian Difference metric is particularly misleading, as it cannot distinguish\nmore extreme maps from less extreme maps. The other metrics are more nuanced,\nbut when assessed on an ensemble, none perform substantially differently from\nsimply measuring number of districts won by a fixed party.\n  One clear consequence of these results is that they demonstrate the folly of\nspecifying a priori bounds on a metric that a redistricting commission must\nmeet in order to avoid gerrymandering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.17186", "cate": "physics.soc-ph", "date": "2024-09-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.09116", "title": "A Method for Assisting Novices Creating Class Diagrams Based on the Instructor's Class Layout", "authors": ["Yuta Saito", "Takehiro Kokubu", "Takafumi Tanaka", "Atsuo Hazeyama", "Hiroaki Hashiura"], "categories": ["cs.CY", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09116", "summary": "Nowadays, modeling exercises on software development objects are conducted in\nhigher education institutions for information technology. Not only are there\nmany defects such as missing elements in the models created by learners during\nthe exercises, but the layout of elements in the class diagrams often differs\nsignificantly from the correct answers created by the instructors. In this\npaper, we focus on the above problem and propose a method to provide effective\nsupport to learners during modeling exercises by automatically converting the\nlayout of the learner's class diagram to that of the instructor, in addition to\nindicating the correctness of the artifacts to the learners during the\nexercises. The proposed method was implemented and evaluated as a tool, and the\nresults indicate that the automatic layout conversion was an effective feedback\nto the learners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09116", "cate": "cs.SE", "date": "2025-05-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04022", "title": "Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation", "authors": ["Junyi Wang", "Jinjiang Li", "Guodong Fan", "Yakun Ju", "Xiang Fang", "Alex C. Kot"], "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04022v1", "summary": "In the semantic segmentation of remote sensing images, acquiring complete\nground objects is critical for achieving precise analysis. However, this task\nis severely hindered by two major challenges: high intra-class variance and\nhigh inter-class similarity. Traditional methods often yield incomplete\nsegmentation results due to their inability to effectively unify class\nrepresentations and distinguish between similar features. Even emerging\nclass-guided approaches are limited by coarse class prototype representations\nand a neglect of target structural information.\n  Therefore, this paper proposes a Prototype-Driven Structure Synergy Network\n(PDSSNet). The design of this network is based on a core concept, a complete\nground object is jointly defined by its invariant class semantics and its\nvariant spatial structure. To implement this, we have designed three key\nmodules. First, the Adaptive Prototype Extraction Module (APEM) ensures\nsemantic accuracy from the source by encoding the ground truth to extract\nunbiased class prototypes. Subsequently, the designed Semantic-Structure\nCoordination Module (SSCM) follows a hierarchical semantics-first,\nstructure-second principle. This involves first establishing a global semantic\ncognition, then leveraging structural information to constrain and refine the\nsemantic representation, thereby ensuring the integrity of class information.\nFinally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic\nstep-size adjustment mechanism to focus on discriminative features between\nclasses.\n  Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art\nmethods. The source code is available at\nhttps://github.com/wangjunyi-1/PDSSNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04022v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04028", "title": "Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval", "authors": ["Yifan Wang", "Tao Wang", "Chenwei Tang", "Caiyang Yu", "Zhengqing Zang", "Mengmi Zhang", "Shudong Huang", "Jiancheng Lv"], "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04028v1", "summary": "Recently, prompt learning has demonstrated remarkable success in adapting\npre-trained Vision-Language Models (VLMs) to various downstream tasks such as\nimage classification. However, its application to the downstream Image-Text\nRetrieval (ITR) task is more challenging. We find that the challenge lies in\ndiscriminating both fine-grained attributes and similar subcategories of the\ndownstream data. To address this challenge, we propose Dual prompt Learning\nwith Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning\nframework to achieve precise image-text matching. The framework dynamically\nadjusts prompt vectors from both semantic and visual dimensions to improve the\nperformance of CLIP on the downstream ITR task. Based on the prompt paradigm,\nDCAR jointly optimizes attribute and class features to enhance fine-grained\nrepresentation learning. Specifically, (1) at the attribute level, it\ndynamically updates the weights of attribute descriptions based on text-image\nmutual information correlation; (2) at the category level, it introduces\nnegative samples from multiple perspectives with category-matching weighting to\nlearn subcategory distinctions. To validate our method, we construct the\nFine-class Described Retrieval Dataset (FDRD), which serves as a challenging\nbenchmark for ITR in downstream data domains. It covers over 1,500 downstream\nfine categories and 230,000 image-caption pairs with detailed attribute\nannotations. Extensive experiments on FDRD demonstrate that DCAR achieves\nstate-of-the-art performance over existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04028v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04033", "title": "Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation", "authors": ["Hee-Yeun Kim", "Byeonggyu Park", "Byonghyok Choi", "Hansang Cho", "Byungkwan Kim", "Soomok Lee", "Mingu Jeon", "Seung-Woo Seo", "Seong-Woo Kim"], "categories": ["cs.CV", "eess.SP"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04033v1", "summary": "The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside\nparking in urban environments poses a significant challenge to road safety,\nparticularly due to the sudden emergence of pedestrians. mmWave technology\nleverages diffraction and reflection to observe NLoS regions, and recent\nstudies have demonstrated its potential for detecting obscured objects.\nHowever, existing approaches predominantly rely on predefined spatial\ninformation or assume simple wall reflections, thereby limiting their\ngeneralizability and practical applicability. A particular challenge arises in\nscenarios where pedestrians suddenly appear from between parked vehicles, as\nthese parked vehicles act as temporary spatial obstructions. Furthermore, since\nparked vehicles are dynamic and may relocate over time, spatial information\nobtained from satellite maps or other predefined sources may not accurately\nreflect real-time road conditions, leading to erroneous sensor interpretations.\nTo address this limitation, we propose an NLoS pedestrian localization\nframework that integrates monocular camera image with 2D radar point cloud\n(PCD) data. The proposed method initially detects parked vehicles through image\nsegmentation, estimates depth to infer approximate spatial characteristics, and\nsubsequently refines this information using 2D radar PCD to achieve precise\nspatial inference. Experimental evaluations conducted in real-world urban road\nenvironments demonstrate that the proposed approach enhances early pedestrian\ndetection and contributes to improved road safety. Supplementary materials are\navailable at https://hiyeun.github.io/NLoS/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04033v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04041", "title": "SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration", "authors": ["Tongshun Zhang", "Pingling Liu", "Zijian Zhang", "Qiuzhan Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04041v1", "summary": "Current dark image restoration methods suffer from severe efficiency\nbottlenecks, primarily stemming from: (1) computational burden and error\ncorrection costs associated with reliance on external priors (manual or\ncross-modal); (2) redundant operations in complex multi-stage enhancement\npipelines; and (3) indiscriminate processing across frequency components in\nfrequency-domain methods, leading to excessive global computational demands. To\naddress these challenges, we propose an Efficient Self-Mining Prior-Guided\nJoint Frequency Enhancement Network (SPJFNet). Specifically, we first introduce\na Self-Mining Guidance Module (SMGM) that generates lightweight endogenous\nguidance directly from the network, eliminating dependence on external priors\nand thereby bypassing error correction overhead while improving inference\nspeed. Second, through meticulous analysis of different frequency domain\ncharacteristics, we reconstruct and compress multi-level operation chains into\na single efficient operation via lossless wavelet decomposition and joint\nFourier-based advantageous frequency enhancement, significantly reducing\nparameters. Building upon this foundation, we propose a Dual-Frequency Guidance\nFramework (DFGF) that strategically deploys specialized high/low frequency\nbranches (wavelet-domain high-frequency enhancement and Fourier-domain\nlow-frequency restoration), decoupling frequency processing to substantially\nreduce computational complexity. Rigorous evaluation across multiple benchmarks\ndemonstrates that SPJFNet not only surpasses state-of-the-art performance but\nalso achieves significant efficiency improvements, substantially reducing model\ncomplexity and computational overhead. Code is available at\nhttps://github.com/bywlzts/SPJFNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04041v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04043", "title": "VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning", "authors": ["Yuheng Ji", "Yipu Wang", "Yuyang Liu", "Xiaoshuai Hao", "Yue Liu", "Yuting Zhao", "Huaihai Lyu", "Xiaolong Zheng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04043v1", "summary": "Visual transformation reasoning (VTR) is a vital cognitive capability that\nempowers intelligent agents to understand dynamic scenes, model causal\nrelationships, and predict future states, and thereby guiding actions and\nlaying the foundation for advanced intelligent systems. However, existing\nbenchmarks suffer from a sim-to-real gap, limited task complexity, and\nincomplete reasoning coverage, limiting their practical use in real-world\nscenarios. To address these limitations, we introduce VisualTrans, the first\ncomprehensive benchmark specifically designed for VTR in real-world\nhuman-object interaction scenarios. VisualTrans encompasses 12 semantically\ndiverse manipulation tasks and systematically evaluates three essential\nreasoning dimensions - spatial, procedural, and quantitative - through 6\nwell-defined subtask types. The benchmark features 472 high-quality\nquestion-answer pairs in various formats, including multiple-choice, open-ended\ncounting, and target enumeration. We introduce a scalable data construction\npipeline built upon first-person manipulation videos, which integrates task\nselection, image pair extraction, automated metadata annotation with large\nmultimodal models, and structured question generation. Human verification\nensures the final benchmark is both high-quality and interpretable. Evaluations\nof various state-of-the-art vision-language models show strong performance in\nstatic spatial tasks. However, they reveal notable shortcomings in dynamic,\nmulti-step reasoning scenarios, particularly in areas like intermediate state\nrecognition and transformation sequence planning. These findings highlight\nfundamental weaknesses in temporal modeling and causal reasoning, providing\nclear directions for future research aimed at developing more capable and\ngeneralizable VTR systems. The dataset and code are available at\nhttps://github.com/WangYipu2002/VisualTrans.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04043v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04044", "title": "Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation", "authors": ["Qiangguo Jin", "Hui Cui", "Junbo Wang", "Changming Sun", "Yimiao He", "Ping Xuan", "Linlin Wang", "Cong Cong", "Leyi Wei", "Ran Su"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04044v1", "summary": "Semi-supervised learning (SSL) has attracted considerable attention in\nmedical image processing. The latest SSL methods use a combination of\nconsistency regularization and pseudo-labeling to achieve remarkable success.\nHowever, most existing SSL studies focus on segmenting large organs, neglecting\nthe challenging scenarios where there are numerous tumors or tumors of small\nvolume. Furthermore, the extensive capabilities of data augmentation\nstrategies, particularly in the context of both labeled and unlabeled data,\nhave yet to be thoroughly investigated. To tackle these challenges, we\nintroduce a straightforward yet effective approach, termed iterative\npseudo-labeling based adaptive copy-paste supervision (IPA-CP), for tumor\nsegmentation in CT scans. IPA-CP incorporates a two-way uncertainty based\nadaptive augmentation mechanism, aiming to inject tumor uncertainties present\nin the mean teacher architecture into adaptive augmentation. Additionally,\nIPA-CP employs an iterative pseudo-label transition strategy to generate more\nrobust and informative pseudo labels for the unlabeled samples. Extensive\nexperiments on both in-house and public datasets show that our framework\noutperforms state-of-the-art SSL methods in medical image segmentation.\nAblation study results demonstrate the effectiveness of our technical\ncontributions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04044v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04049", "title": "Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation", "authors": ["Jiayi He", "Xu Wang", "Shengeng Tang", "Yaxiong Wang", "Lechao Cheng", "Dan Guo"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04049v1", "summary": "Sign language video generation requires producing natural signing motions\nwith realistic appearances under precise semantic control, yet faces two\ncritical challenges: excessive signer-specific data requirements and poor\ngeneralization. We propose a new paradigm for sign language video generation\nthat decouples motion semantics from signer identity through a two-phase\nsynthesis framework. First, we construct a signer-independent multimodal motion\nlexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D\nmesh sequences, requiring only one recording per sign. This compact\nrepresentation enables our second key innovation: a discrete-to-continuous\nmotion synthesis stage that transforms retrieved gloss sequences into\ntemporally coherent motion trajectories, followed by identity-aware neural\nrendering to produce photorealistic videos of arbitrary signers. Unlike prior\nwork constrained by signer-specific datasets, our method treats motion as a\nfirst-class citizen: the learned latent pose dynamics serve as a portable\n\"choreography layer\" that can be visually realized through different human\nappearances. Extensive experiments demonstrate that disentangling motion from\nidentity is not just viable but advantageous - enabling both high-quality\nsynthesis and unprecedented flexibility in signer personalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04049v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04050", "title": "DOMR: Establishing Cross-View Segmentation via Dense Object Matching", "authors": ["Jitong Liao", "Yulu Gao", "Shaofei Huang", "Jialin Gao", "Jie Lei", "Ronghua Liang", "Si Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04050v1", "summary": "Cross-view object correspondence involves matching objects between egocentric\n(first-person) and exocentric (third-person) views. It is a critical yet\nchallenging task for visual understanding. In this work, we propose the Dense\nObject Matching and Refinement (DOMR) framework to establish dense object\ncorrespondences across views. The framework centers around the Dense Object\nMatcher (DOM) module, which jointly models multiple objects. Unlike methods\nthat directly match individual object masks to image features, DOM leverages\nboth positional and semantic relationships among objects to find\ncorrespondences. DOM integrates a proposal generation module with a dense\nmatching module that jointly encodes visual, spatial, and semantic cues,\nexplicitly constructing inter-object relationships to achieve dense matching\namong objects. Furthermore, we combine DOM with a mask refinement head designed\nto improve the completeness and accuracy of the predicted masks, forming the\ncomplete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark\ndemonstrate that our approach achieves state-of-the-art performance with a mean\nIoU of 49.7% on Ego$\\to$Exo and 55.2% on Exo$\\to$Ego. These results outperform\nthose of previous methods by 5.8% and 4.3%, respectively, validating the\neffectiveness of our integrated approach for cross-view understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04050v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04051", "title": "Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach", "authors": ["Chen Luo", "Qiyu Jin", "Taofeng Xie", "Xuemei Wang", "Huayu Wang", "Congcong Liu", "Liming Tang", "Guoqing Chen", "Zhuo-Xu Cui", "Dong Liang"], "categories": ["cs.CV", "math.OC"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04051v1", "summary": "Interpolating missing data in k-space is essential for accelerating imaging.\nHowever, existing methods, including convolutional neural network-based deep\nlearning, primarily exploit local predictability while overlooking the inherent\nglobal dependencies in k-space. Recently, Transformers have demonstrated\nremarkable success in natural language processing and image analysis due to\ntheir ability to capture long-range dependencies. This inspires the use of\nTransformers for k-space interpolation to better exploit its global structure.\nHowever, their lack of interpretability raises concerns regarding the\nreliability of interpolated data. To address this limitation, we propose\nGPI-WT, a white-box Transformer framework based on Globally Predictable\nInterpolation (GPI) for k-space. Specifically, we formulate GPI from the\nperspective of annihilation as a novel k-space structured low-rank (SLR) model.\nThe global annihilation filters in the SLR model are treated as learnable\nparameters, and the subgradients of the SLR model naturally induce a learnable\nattention mechanism. By unfolding the subgradient-based optimization algorithm\nof SLR into a cascaded network, we construct the first white-box Transformer\nspecifically designed for accelerated MRI. Experimental results demonstrate\nthat the proposed method significantly outperforms state-of-the-art approaches\nin k-space interpolation accuracy while providing superior interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04051v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04055", "title": "Uni-DocDiff: A Unified Document Restoration Model Based on Diffusion", "authors": ["Fangmin Zhao", "Weichao Zeng", "Zhenhang Li", "Dongbao Yang", "Binbin Li", "Xiaojun Bi", "Yu Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04055v1", "summary": "Removing various degradations from damaged documents greatly benefits\ndigitization, downstream document analysis, and readability. Previous methods\noften treat each restoration task independently with dedicated models, leading\nto a cumbersome and highly complex document processing system. Although recent\nstudies attempt to unify multiple tasks, they often suffer from limited\nscalability due to handcrafted prompts and heavy preprocessing, and fail to\nfully exploit inter-task synergy within a shared architecture. To address the\naforementioned challenges, we propose Uni-DocDiff, a Unified and highly\nscalable Document restoration model based on Diffusion. Uni-DocDiff develops a\nlearnable task prompt design, ensuring exceptional scalability across diverse\ntasks. To further enhance its multi-task capabilities and address potential\ntask interference, we devise a novel \\textbf{Prior \\textbf{P}ool}, a simple yet\ncomprehensive mechanism that combines both local high-frequency features and\nglobal low-frequency features. Additionally, we design the \\textbf{Prior\n\\textbf{F}usion \\textbf{M}odule (PFM)}, which enables the model to adaptively\nselect the most relevant prior information for each specific task. Extensive\nexperiments show that the versatile Uni-DocDiff achieves performance comparable\nor even superior performance compared with task-specific expert models, and\nsimultaneously holds the task scalability for seamless adaptation to new tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04055v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04058", "title": "TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation", "authors": ["Zunhui Xia", "Hongxing Li", "Libin Lan"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04058v1", "summary": "In recent years, transformer-based methods have achieved remarkable progress\nin medical image segmentation due to their superior ability to capture\nlong-range dependencies. However, these methods typically suffer from two major\nlimitations. First, their computational complexity scales quadratically with\nthe input sequences. Second, the feed-forward network (FFN) modules in vanilla\nTransformers typically rely on fully connected layers, which limits models'\nability to capture local contextual information and multiscale features\ncritical for precise semantic segmentation. To address these issues, we propose\nan efficient medical image segmentation network, named TCSAFormer. The proposed\nTCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention\n(CA) module, which combines token compression and pixel-level sparse attention\nto dynamically focus on the most relevant key-value pairs for each query. This\nis achieved by pruning globally irrelevant tokens and merging redundant ones,\nsignificantly reducing computational complexity while enhancing the model's\nability to capture relationships between tokens. Second, it introduces a\nDual-Branch Feed-Forward Network (DBFFN) module as a replacement for the\nstandard FFN to capture local contextual features and multiscale information,\nthereby strengthening the model's feature representation capability. We conduct\nextensive experiments on three publicly available medical image segmentation\ndatasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation\nperformance of TCSAFormer. Experimental results demonstrate that TCSAFormer\nachieves superior performance compared to existing state-of-the-art (SOTA)\nmethods, while maintaining lower computational overhead, thus achieving an\noptimal trade-off between efficiency and accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04058v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04059", "title": "Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models", "authors": ["Zhaochen Liu", "Kaiwen Gao", "Shuyi Liang", "Bin Xiao", "Limeng Qiao", "Lin Ma", "Tingting Jiang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04059v1", "summary": "Occlusion perception, a critical foundation for human-level spatial\nunderstanding, embodies the challenge of integrating visual recognition and\nreasoning. Though multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities, their performance on occlusion perception remains\nunder-explored. To address this gap, we introduce O-Bench, the first visual\nquestion answering (VQA) benchmark specifically designed for occlusion\nperception. Based on SA-1B, we construct 1,365 images featuring semantically\ncoherent occlusion scenarios through a novel layered synthesis approach. Upon\nthis foundation, we annotate 4,588 question-answer pairs in total across five\ntailored tasks, employing a reliable, semi-automatic workflow. Our extensive\nevaluation of 22 representative MLLMs against the human baseline reveals a\nsignificant performance gap between current MLLMs and humans, which, we find,\ncannot be sufficiently bridged by model scaling or thinking process. We further\nidentify three typical failure patterns, including an overly conservative bias,\na fragile gestalt prediction, and a struggle with quantitative tasks. We\nbelieve O-Bench can not only provide a vital evaluation tool for occlusion\nperception, but also inspire the development of MLLMs for better visual\nintelligence. Our benchmark will be made publicly available upon paper\npublication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04059v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04061", "title": "TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation", "authors": ["Chengqian Dai", "Yonghong Guo", "Hongzhao Xiang", "Yigui Luo"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04061v1", "summary": "In remote sensing, most segmentation networks adopt the UNet architecture,\noften incorporating modules such as Transformers or Mamba to enhance\nglobal-local feature interactions within decoder stages. However, these\nenhancements typically focus on intra-scale relationships and neglect the\nglobal contextual dependencies across multiple resolutions. To address this\nlimitation, we introduce the Terrace Convolutional Decoder Network (TNet), a\nsimple yet effective architecture that leverages only convolution and addition\noperations to progressively integrate low-resolution features (rich in global\ncontext) into higher-resolution features (rich in local details) across\ndecoding stages. This progressive fusion enables the model to learn\nspatially-aware convolutional kernels that naturally blend global and local\ninformation in a stage-wise manner. We implement TNet with a ResNet-18 encoder\n(TNet-R) and evaluate it on three benchmark datasets. TNet-R achieves\ncompetitive performance with a mean Intersection-over-Union (mIoU) of 85.35\\%\non ISPRS Vaihingen, 87.05\\% on ISPRS Potsdam, and 52.19\\% on LoveDA, while\nmaintaining high computational efficiency. Code is publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04061v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04090", "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework", "authors": ["Yi-Ting Chen", "Ting-Hsuan Liao", "Pengsheng Guo", "Alexander Schwing", "Jia-Bin Huang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04090v1", "summary": "We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based\nsuper-resolution framework that leverages off-the-shelf diffusion-based 2D\nsuper-resolution models. 3DSR encourages 3D consistency across views via the\nuse of an explicit 3D Gaussian-splatting-based scene representation. This makes\nthe proposed 3DSR different from prior work, such as image upsampling or the\nuse of video super-resolution, which either don't consider 3D consistency or\naim to incorporate 3D consistency implicitly. Notably, our method enhances\nvisual quality without additional fine-tuning, ensuring spatial coherence\nwithin the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data,\ndemonstrating that it produces high-resolution results that are visually\ncompelling, while maintaining structural consistency in 3D reconstructions.\nCode will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04090v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04101", "title": "NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding", "authors": ["Zelin Peng", "Yichen Zhao", "Yu Huang", "Piao Yang", "Feilong Tang", "Zhengqin Xu", "Xiaokang Yang", "Wei Shen"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04101v1", "summary": "Computer-aided medical image analysis is crucial for disease diagnosis and\ntreatment planning, yet limited annotated datasets restrict medical-specific\nmodel development. While vision-language models (VLMs) like CLIP offer strong\ngeneralization capabilities, their direct application to medical imaging\nanalysis is impeded by a significant domain gap. Existing approaches to bridge\nthis gap, including prompt learning and one-way modality interaction\ntechniques, typically focus on introducing domain knowledge to a single\nmodality. Although this may offer performance gains, it often causes modality\nmisalignment, thereby failing to unlock the full potential of VLMs. In this\npaper, we propose \\textbf{NEARL-CLIP} (i\\underline{N}teracted qu\\underline{E}ry\n\\underline{A}daptation with o\\underline{R}thogona\\underline{L} Regularization),\na novel cross-modality interaction VLM-based framework that contains two\ncontributions: (1) Unified Synergy Embedding Transformer (USEformer), which\ndynamically generates cross-modality queries to promote interaction between\nmodalities, thus fostering the mutual enrichment and enhancement of multi-modal\nmedical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA\nintroduces an orthogonality technique to decouple the new knowledge from\nUSEformer into two distinct components: the truly novel information and the\nincremental knowledge. By isolating the learning process from the interference\nof incremental knowledge, OCA enables a more focused acquisition of new\ninformation, thereby further facilitating modality interaction and unleashing\nthe capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in\na parameter-efficient style, which only introduces \\textbf{1.46M} learnable\nparameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04101v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04102", "title": "AR as an Evaluation Playground: Bridging Metrics and Visual Perception of Computer Vision Models", "authors": ["Ashkan Ganj", "Yiqin Zhao", "Tian Guo"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04102v1", "summary": "Human perception studies can provide complementary insights to qualitative\nevaluation for understanding computer vision (CV) model performance. However,\nconducting human perception studies remains a non-trivial task, it often\nrequires complex, end-to-end system setups that are time-consuming and\ndifficult to scale. In this paper, we explore the unique opportunity presented\nby augmented reality (AR) for helping CV researchers to conduct perceptual\nstudies. We design ARCADE, an evaluation platform that allows researchers to\neasily leverage AR's rich context and interactivity for human-centered CV\nevaluation. Specifically, ARCADE supports cross-platform AR data collection,\ncustom experiment protocols via pluggable model inference, and AR streaming for\nuser studies. We demonstrate ARCADE using two types of CV models, depth and\nlighting estimation and show that AR tasks can be effectively used to elicit\nhuman perceptual judgments of model quality. We also evaluate the systems\nusability and performance across different deployment and study settings,\nhighlighting its flexibility and effectiveness as a human-centered evaluation\nplatform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04102v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04120", "title": "CLIPVehicle: A Unified Framework for Vision-based Vehicle Search", "authors": ["Likai Wang", "Ruize Han", "Xiangqun Zhang", "Wei Feng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04120v1", "summary": "Vehicles, as one of the most common and significant objects in the real\nworld, the researches on which using computer vision technologies have made\nremarkable progress, such as vehicle detection, vehicle re-identification, etc.\nTo search an interested vehicle from the surveillance videos, existing methods\nfirst pre-detect and store all vehicle patches, and then apply vehicle\nre-identification models, which is resource-intensive and not very practical.\nIn this work, we aim to achieve the joint detection and re-identification for\nvehicle search. However, the conflicting objectives between detection that\nfocuses on shared vehicle commonness and re-identification that focuses on\nindividual vehicle uniqueness make it challenging for a model to learn in an\nend-to-end system. For this problem, we propose a new unified framework, namely\nCLIPVehicle, which contains a dual-granularity semantic-region alignment module\nto leverage the VLMs (Vision-Language Models) for vehicle discrimination\nmodeling, and a multi-level vehicle identification learning strategy to learn\nthe identity representation from global, instance and feature levels. We also\nconstruct a new benchmark, including a real-world dataset CityFlowVS, and two\nsynthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive\nexperimental results demonstrate that our method outperforms the\nstate-of-the-art methods of both vehicle Re-ID and person search tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04120v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04122", "title": "Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation", "authors": ["Maximilian Ulmer", "Wout Boerdijk", "Rudolph Triebel", "Maximilian Durner"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04122v1", "summary": "This paper presents OC-DiT, a novel class of diffusion models designed for\nobject-centric prediction, and applies it to zero-shot instance segmentation.\nWe propose a conditional latent diffusion framework that generates instance\nmasks by conditioning the generative process on object templates and image\nfeatures within the diffusion model's latent space. This allows our model to\neffectively disentangle object instances through the diffusion process, which\nis guided by visual object descriptors and localized image cues. Specifically,\nwe introduce two model variants: a coarse model for generating initial object\ninstance proposals, and a refinement model that refines all proposals in\nparallel. We train these models on a newly created, large-scale synthetic\ndataset comprising thousands of high-quality object meshes. Remarkably, our\nmodel achieves state-of-the-art performance on multiple challenging real-world\nbenchmarks, without requiring any retraining on target data. Through\ncomprehensive ablation studies, we demonstrate the potential of diffusion\nmodels for instance segmentation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04122v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04123", "title": "Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement", "authors": ["Zheng Cheng", "Wenri Wang", "Guangyong Chen", "Yakun Ju", "Yihua Cheng", "Zhisong Liu", "Yanda Meng", "Jintao Song"], "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04123v1", "summary": "Underwater image enhancement (UIE) techniques aim to improve visual quality\nof images captured in aquatic environments by addressing degradation issues\ncaused by light absorption and scattering effects, including color distortion,\nblurring, and low contrast. Current mainstream solutions predominantly employ\nmulti-scale feature extraction (MSFE) mechanisms to enhance reconstruction\nquality through multi-resolution feature fusion. However, our extensive\nexperiments demonstrate that high-quality image reconstruction does not\nnecessarily rely on multi-scale feature fusion. Contrary to popular belief, our\nexperiments show that single-scale feature extraction alone can match or\nsurpass the performance of multi-scale methods, significantly reducing\ncomplexity. To comprehensively explore single-scale feature potential in\nunderwater enhancement, we propose an innovative Single-Scale Decomposition\nNetwork (SSD-Net). This architecture introduces an asymmetrical decomposition\nmechanism that disentangles input image into clean layer along with degradation\nlayer. The former contains scene-intrinsic information and the latter encodes\nmedium-induced interference. It uniquely combines CNN's local feature\nextraction capabilities with Transformer's global modeling strengths through\ntwo core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing\ndual-branch feature space decoupling via efficient attention operations and\nadaptive sparse transformer; 2) Bidirectional Feature Communication Block\n(BFCB), enabling cross-layer residual interactions for complementary feature\nmining and fusion. This synergistic design preserves feature decomposition\nindependence while establishing dynamic cross-layer information pathways,\neffectively enhancing degradation decoupling capacity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04123v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04124", "title": "Learning Using Privileged Information for Litter Detection", "authors": ["Matthias Bartolo", "Konstantinos Makantasis", "Dylan Seychell"], "categories": ["cs.CV", "cs.ET", "cs.LG", "cs.PF"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04124v1", "summary": "As litter pollution continues to rise globally, developing automated tools\ncapable of detecting litter effectively remains a significant challenge. This\nstudy presents a novel approach that combines, for the first time, privileged\ninformation with deep learning object detection to improve litter detection\nwhile maintaining model efficiency. We evaluate our method across five widely\nused object detection models, addressing challenges such as detecting small\nlitter and objects partially obscured by grass or stones. In addition to this,\na key contribution of our work can also be attributed to formulating a means of\nencoding bounding box information as a binary mask, which can be fed to the\ndetection model to refine detection guidance. Through experiments on both\nwithin-dataset evaluation on the renowned SODA dataset and cross-dataset\nevaluation on the BDW and UAVVaste litter detection datasets, we demonstrate\nconsistent performance improvements across all models. Our approach not only\nbolsters detection accuracy within the training sets but also generalises well\nto other litter detection contexts. Crucially, these improvements are achieved\nwithout increasing model complexity or adding extra layers, ensuring\ncomputational efficiency and scalability. Our results suggest that this\nmethodology offers a practical solution for litter detection, balancing\naccuracy and efficiency in real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04124v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04129", "title": "SVC 2025: the First Multimodal Deception Detection Challenge", "authors": ["Xun Lin", "Xiaobao Guo", "Taorui Wang", "Yingjie Ma", "Jiajian Huang", "Jiayu Zhang", "Junzhe Cao", "Zitong Yu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04129v1", "summary": "Deception detection is a critical task in real-world applications such as\nsecurity screening, fraud prevention, and credibility assessment. While deep\nlearning methods have shown promise in surpassing human-level performance,\ntheir effectiveness often depends on the availability of high-quality and\ndiverse deception samples. Existing research predominantly focuses on\nsingle-domain scenarios, overlooking the significant performance degradation\ncaused by domain shifts. To address this gap, we present the SVC 2025\nMultimodal Deception Detection Challenge, a new benchmark designed to evaluate\ncross-domain generalization in audio-visual deception detection. Participants\nare required to develop models that not only perform well within individual\ndomains but also generalize across multiple heterogeneous datasets. By\nleveraging multimodal data, including audio, video, and text, this challenge\nencourages the design of models capable of capturing subtle and implicit\ndeceptive cues. Through this benchmark, we aim to foster the development of\nmore adaptable, explainable, and practically deployable deception detection\nsystems, advancing the broader field of multimodal learning. By the conclusion\nof the workshop competition, a total of 21 teams had submitted their final\nresults. https://sites.google.com/view/svc-mm25 for more information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04129v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04147", "title": "IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control", "authors": ["Lijuan Liu", "Wenfa Li", "Dongbo Zhang", "Shuo Wang", "Shaohui Jiao"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04147v1", "summary": "We present IDC-Net (Image-Depth Consistency Network), a novel framework\ndesigned to generate RGB-D video sequences under explicit camera trajectory\ncontrol. Unlike approaches that treat RGB and depth generation separately,\nIDC-Net jointly synthesizes both RGB images and corresponding depth maps within\na unified geometry-aware diffusion model. The joint learning framework\nstrengthens spatial and geometric alignment across frames, enabling more\nprecise camera control in the generated sequences. To support the training of\nthis camera-conditioned model and ensure high geometric fidelity, we construct\na camera-image-depth consistent dataset with metric-aligned RGB videos, depth\nmaps, and accurate camera poses, which provides precise geometric supervision\nwith notably improved inter-frame geometric consistency. Moreover, we introduce\na geometry-aware transformer block that enables fine-grained camera control,\nenhancing control over the generated sequences. Extensive experiments show that\nIDC-Net achieves improvements over state-of-the-art approaches in both visual\nquality and geometric consistency of generated scene sequences. Notably, the\ngenerated RGB-D sequences can be directly feed for downstream 3D Scene\nreconstruction tasks without extra post-processing steps, showcasing the\npractical benefits of our joint learning framework. See more at\nhttps://idcnet-scene.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04147v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04153", "title": "ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation", "authors": ["Yihua Shao", "Xiaofeng Lin", "Xinwei Long", "Siyu Chen", "Minxi Yan", "Yang Liu", "Ziyang Yan", "Ao Ma", "Hao Tang", "Jingcai Guo"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04153v1", "summary": "Enabling multi-task adaptation in pre-trained Low-Rank Adaptation (LoRA)\nmodels is crucial for enhancing their generalization capabilities. Most\nexisting pre-trained LoRA fusion methods decompose weight matrices, sharing\nsimilar parameters while merging divergent ones. However, this paradigm\ninevitably induces inter-weight conflicts and leads to catastrophic domain\nforgetting. While incremental learning enables adaptation to multiple tasks, it\nstruggles to achieve generalization in few-shot scenarios. Consequently, when\nthe weight data follows a long-tailed distribution, it can lead to forgetting\nin the fused weights. To address this issue, we propose In-Context Meta LoRA\nFusion (ICM-Fusion), a novel framework that synergizes meta-learning with\nin-context adaptation. The key innovation lies in our task vector arithmetic,\nwhich dynamically balances conflicting optimization directions across domains\nthrough learned manifold projections. ICM-Fusion obtains the optimal task\nvector orientation for the fused model in the latent space by adjusting the\norientation of the task vectors. Subsequently, the fused LoRA is reconstructed\nby a self-designed Fusion VAE (F-VAE) to realize multi-task LoRA generation. We\nhave conducted extensive experiments on visual and linguistic tasks, and the\nexperimental results demonstrate that ICM-Fusion can be adapted to a wide range\nof architectural models and applied to various tasks. Compared to the current\npre-trained LoRA fusion method, ICM-Fusion fused LoRA can significantly reduce\nthe multi-tasking loss and can even achieve task enhancement in few-shot\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04153v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04161", "title": "Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning", "authors": ["Yuqin Cao", "Yixuan Gao", "Wei Sun", "Xiaohong Liu", "Yulun Zhang", "Xiongkuo Min"], "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04161v1", "summary": "Face videos accompanied by audio have become integral to our daily lives,\nwhile they often suffer from complex degradations. Most face video restoration\nmethods neglect the intrinsic correlations between the visual and audio\nfeatures, especially in mouth regions. A few audio-aided face video restoration\nmethods have been proposed, but they only focus on compression artifact\nremoval. In this paper, we propose a General Audio-assisted face Video\nrestoration Network (GAVN) to address various types of streaming video\ndistortions via identity and temporal complementary learning. Specifically,\nGAVN first captures inter-frame temporal features in the low-resolution space\nto restore frames coarsely and save computational cost. Then, GAVN extracts\nintra-frame identity features in the high-resolution space with the assistance\nof audio signals and face landmarks to restore more facial details. Finally,\nthe reconstruction module integrates temporal features and identity features to\ngenerate high-quality face videos. Experimental results demonstrate that GAVN\noutperforms the existing state-of-the-art methods on face video compression\nartifact removal, deblurring, and super-resolution. Codes will be released upon\npublication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04161v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04175", "title": "AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization", "authors": ["Jingyi Liao", "Yongyi Su", "Rong-Cheng Tu", "Zhao Jin", "Wenhao Sun", "Yiting Li", "Dacheng Tao", "Xun Xu", "Xulei Yang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04175v1", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities across diverse domains, their application to specialized anomaly\ndetection (AD) remains constrained by domain adaptation challenges. Existing\nGroup Relative Policy Optimization (GRPO) based approaches suffer from two\ncritical limitations: inadequate training data utilization when models produce\nuniform responses, and insufficient supervision over reasoning processes that\nencourage immediate binary decisions without deliberative analysis. We propose\na comprehensive framework addressing these limitations through two synergistic\ninnovations. First, we introduce a multi-stage deliberative reasoning process\nthat guides models from region identification to focused examination,\ngenerating diverse response patterns essential for GRPO optimization while\nenabling structured supervision over analytical workflows. Second, we develop a\nfine-grained reward mechanism incorporating classification accuracy and\nlocalization supervision, transforming binary feedback into continuous signals\nthat distinguish genuine analytical insight from spurious correctness.\nComprehensive evaluation across multiple industrial datasets demonstrates\nsubstantial performance improvements in adapting general vision-language models\nto specialized anomaly detection. Our method achieves superior accuracy with\nefficient adaptation of existing annotations, effectively bridging the gap\nbetween general-purpose MLLM capabilities and the fine-grained visual\ndiscrimination required for detecting subtle manufacturing defects and\nstructural irregularities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04175v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04176", "title": "Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement", "authors": ["Jin Kuang", "Dong Liu", "Yukuang Zhang", "Shengsheng Wang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04176v1", "summary": "Most existing low-light image enhancement approaches primarily focus on\narchitectural innovations, while often overlooking the intrinsic uncertainty\nwithin feature representations particularly under extremely dark conditions\nwhere degraded gradient and noise dominance severely impair model reliability\nand causal reasoning. To address these issues, we propose U2CLLIE, a novel\nframework that integrates uncertainty-aware enhancement and spatial-color\ncausal correlation modeling. From the perspective of entropy-based uncertainty,\nour framework introduces two key components: (1) An Uncertainty-Aware\nDual-domain Denoise (UaD) Module, which leverages Gaussian-Guided Adaptive\nFrequency Domain Feature Enhancement (G2AF) to suppress frequency-domain noise\nand optimize entropy-driven representations. This module enhances spatial\ntexture extraction and frequency-domain noise suppression/structure refinement,\neffectively mitigating gradient vanishing and noise dominance. (2) A\nhierarchical causality-aware framework, where a Luminance Enhancement Network\n(LEN) first performs coarse brightness enhancement on dark regions. Then,\nduring the encoder-decoder phase, two asymmetric causal correlation modeling\nmodules Neighborhood Correlation State Space (NeCo) and Adaptive Spatial-Color\nCalibration (AsC) collaboratively construct hierarchical causal constraints.\nThese modules reconstruct and reinforce neighborhood structure and color\nconsistency in the feature space. Extensive experiments demonstrate that\nU2CLLIE achieves state-of-the-art performance across multiple benchmark\ndatasets, exhibiting robust performance and strong generalization across\nvarious scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04176v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04181", "title": "Deeper Inside Deep ViT", "authors": ["Sungrae Hong"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04181v1", "summary": "There have been attempts to create large-scale structures in vision models\nsimilar to LLM, such as ViT-22B. While this research has provided numerous\nanalyses and insights, our understanding of its practical utility remains\nincomplete. Therefore, we examine how this model structure reacts and train in\na local environment. We also highlight the instability in training and make\nsome model modifications to stabilize it. The ViT-22B model, trained from\nscratch, overall outperformed ViT in terms of performance under the same\nparameter size. Additionally, we venture into the task of image generation,\nwhich has not been attempted in ViT-22B. We propose an image generation\narchitecture using ViT and investigate which between ViT and ViT-22B is a more\nsuitable structure for image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04181v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04190", "title": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation", "authors": ["Fengyi Wu", "Yimian Dai", "Tianfang Zhang", "Yixuan Ding", "Jian Yang", "Ming-Ming Cheng", "Zhenming Peng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04190v1", "summary": "Robust principal component analysis (RPCA) decomposes an observation matrix\ninto low-rank background and sparse object components. This capability has\nenabled its application in tasks ranging from image restoration to\nsegmentation. However, traditional RPCA models suffer from computational\nburdens caused by matrix operations, reliance on finely tuned hyperparameters,\nand rigid priors that limit adaptability in dynamic scenarios. To solve these\nlimitations, we propose RPCANet++, a sparse object segmentation framework that\nfuses the interpretability of RPCA with efficient deep architectures. Our\napproach unfolds a relaxed RPCA model into a structured network comprising a\nBackground Approximation Module (BAM), an Object Extraction Module (OEM), and\nan Image Restoration Module (IRM). To mitigate inter-stage transmission loss in\nthe BAM, we introduce a Memory-Augmented Module (MAM) to enhance background\nfeature preservation, while a Deep Contrast Prior Module (DCPM) leverages\nsaliency cues to expedite object extraction. Extensive experiments on diverse\ndatasets demonstrate that RPCANet++ achieves state-of-the-art performance under\nvarious imaging scenarios. We further improve interpretability via visual and\nnumerical low-rankness and sparsity measurements. By combining the theoretical\nstrengths of RPCA with the efficiency of deep networks, our approach sets a new\nbaseline for reliable and interpretable sparse object segmentation. Codes are\navailable at our Project Webpage https://fengyiwu98.github.io/rpcanetx.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04190v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04192", "title": "From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models", "authors": ["Dunyuan Xu", "Xikai Yang", "Yaoqian Li", "Jinpeng Li", "Pheng-Ann Heng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04192v1", "summary": "The security of biomedical Multimodal Large Language Models (MLLMs) has\nattracted increasing attention. However, training samples easily contain\nprivate information and incorrect knowledge that are difficult to detect,\npotentially leading to privacy leakage or erroneous outputs after deployment.\nAn intuitive idea is to reprocess the training set to remove unwanted content\nand retrain the model from scratch. Yet, this is impractical due to significant\ncomputational costs, especially for large language models. Machine unlearning\nhas emerged as a solution to this problem, which avoids complete retraining by\nselectively removing undesired knowledge derived from harmful samples while\npreserving required capabilities on normal cases. However, there exist no\navailable datasets to evaluate the unlearning quality for security protection\nin biomedical MLLMs. To bridge this gap, we propose the first benchmark\nMultimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built\nupon our novel data generation pipeline that effectively integrates synthetic\nprivate data and factual errors into the training set. Our benchmark targets\ntwo key scenarios: 1) Privacy protection, where patient private information is\nmistakenly included in the training set, causing models to unintentionally\nrespond with private data during inference; and 2) Incorrectness removal, where\nwrong knowledge derived from unreliable sources is embedded into the dataset,\nleading to unsafe model responses. Moreover, we propose a novel Unlearning\nEfficiency Score that directly reflects the overall unlearning performance\nacross different subsets. We evaluate five unlearning approaches on MLLMU-Med\nand find that these methods show limited effectiveness in removing harmful\nknowledge from biomedical MLLMs, indicating significant room for improvement.\nThis work establishes a new pathway for further research in this promising\nfield.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04192v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04200", "title": "Bootstrap Deep Spectral Clustering with Optimal Transport", "authors": ["Wengang Guo", "Wei Ye", "Chunchun Chen", "Xin Sun", "Christian BÃ¶hm", "Claudia Plant", "Susanto Rahardja"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04200v1", "summary": "Spectral clustering is a leading clustering method. Two of its major\nshortcomings are the disjoint optimization process and the limited\nrepresentation capacity. To address these issues, we propose a deep spectral\nclustering model (named BootSC), which jointly learns all stages of spectral\nclustering -- affinity matrix construction, spectral embedding, and $k$-means\nclustering -- using a single network in an end-to-end manner. BootSC leverages\neffective and efficient optimal-transport-derived supervision to bootstrap the\naffinity matrix and the cluster assignment matrix. Moreover, a\nsemantically-consistent orthogonal re-parameterization technique is introduced\nto orthogonalize spectral embeddings, significantly enhancing the\ndiscrimination capability. Experimental results indicate that BootSC achieves\nstate-of-the-art clustering performance. For example, it accomplishes a notable\n16\\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs\ndataset. Our code is available at https://github.com/spdj2271/BootSC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04200v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04205", "title": "Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification", "authors": ["Jianxun Yu", "Ruiquan Ge", "Zhipeng Wang", "Cheng Yang", "Chenyu Lin", "Xianjun Fu", "Jikui Liu", "Ahmed Elazab", "Changmiao Wang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04205v1", "summary": "The diagnosis of medical diseases faces challenges such as the misdiagnosis\nof small lesions. Deep learning, particularly multimodal approaches, has shown\ngreat potential in the field of medical disease diagnosis. However, the\ndifferences in dimensionality between medical imaging and electronic health\nrecord data present challenges for effective alignment and fusion. To address\nthese issues, we propose the Multimodal Multiscale Cross-Attention Fusion\nNetwork (MMCAF-Net). This model employs a feature pyramid structure combined\nwith an efficient 3D multi-scale convolutional attention module to extract\nlesion-specific features from 3D medical images. To further enhance multimodal\ndata integration, MMCAF-Net incorporates a multi-scale cross-attention module,\nwhich resolves dimensional inconsistencies, enabling more effective feature\nfusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results\nshowed a significant improvement in diagnostic accuracy, surpassing current\nstate-of-the-art methods. The code is available at\nhttps://github.com/yjx1234/MMCAF-Net", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04205v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04211", "title": "What Holds Back Open-Vocabulary Segmentation?", "authors": ["Josip Å ariÄ", "Ivan MartinoviÄ", "Matej Kristan", "SiniÅ¡a Å egviÄ"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04211v1", "summary": "Standard segmentation setups are unable to deliver models that can recognize\nconcepts outside the training taxonomy. Open-vocabulary approaches promise to\nclose this gap through language-image pretraining on billions of image-caption\npairs. Unfortunately, we observe that the promise is not delivered due to\nseveral bottlenecks that have caused the performance to plateau for almost two\nyears. This paper proposes novel oracle components that identify and decouple\nthese bottlenecks by taking advantage of the groundtruth information. The\npresented validation experiments deliver important empirical findings that\nprovide a deeper insight into the failures of open-vocabulary models and\nsuggest prominent approaches to unlock the future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04211v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04224", "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition", "authors": ["Jiahui Li", "Shengeng Tang", "Jingxuan He", "Gang Huang", "Zhangye Wang", "Yantao Pan", "Lechao Cheng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04224v1", "summary": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally\nchallenging due to the need to jointly infer motion, structure, and appearance\nfrom limited observations. Existing dynamic scene reconstruction methods based\non Gaussian Splatting often entangle static and dynamic elements in a shared\nrepresentation, leading to motion leakage, geometric distortions, and temporal\nflickering. We identify that the root cause lies in the coupled modeling of\ngeometry and appearance across time, which hampers both stability and\ninterpretability. To address this, we propose \\textbf{SplitGaussian}, a novel\nframework that explicitly decomposes scene representations into static and\ndynamic components. By decoupling motion modeling from background geometry and\nallowing only the dynamic branch to deform over time, our method prevents\nmotion artifacts in static regions while supporting view- and time-dependent\nappearance refinement. This disentangled design not only enhances temporal\nconsistency and reconstruction fidelity but also accelerates convergence.\nExtensive experiments demonstrate that SplitGaussian outperforms prior\nstate-of-the-art methods in rendering quality, geometric stability, and motion\nseparation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04224v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04227", "title": "Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting", "authors": ["Yuyang Liu", "Qiuhe Hong", "Linlan Huang", "Alexandra Gomez-Villa", "Dipam Goswami", "Xialei Liu", "Joost van de Weijer", "Yonghong Tian"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04227v1", "summary": "Vision-language models (VLMs) have achieved impressive performance across\ndiverse multimodal tasks by leveraging large-scale pre-training. However,\nenabling them to learn continually from non-stationary data remains a major\nchallenge, as their cross-modal alignment and generalization capabilities are\nparticularly vulnerable to catastrophic forgetting. Unlike traditional unimodal\ncontinual learning (CL), VLMs face unique challenges such as cross-modal\nfeature drift, parameter interference due to shared architectures, and\nzero-shot capability erosion. This survey offers the first focused and\nsystematic review of continual learning for VLMs (VLM-CL). We begin by\nidentifying the three core failure modes that degrade performance in VLM-CL.\nBased on these, we propose a challenge-driven taxonomy that maps solutions to\ntheir target problems: (1) \\textit{Multi-Modal Replay Strategies} address\ncross-modal drift through explicit or implicit memory mechanisms; (2)\n\\textit{Cross-Modal Regularization} preserves modality alignment during\nupdates; and (3) \\textit{Parameter-Efficient Adaptation} mitigates parameter\ninterference with modular or low-rank updates. We further analyze current\nevaluation protocols, datasets, and metrics, highlighting the need for better\nbenchmarks that capture VLM-specific forgetting and compositional\ngeneralization. Finally, we outline open problems and future directions,\nincluding continual pre-training and compositional zero-shot learning. This\nsurvey aims to serve as a comprehensive and diagnostic reference for\nresearchers developing lifelong vision-language systems. All resources are\navailable at:\nhttps://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04227v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04229", "title": "Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction", "authors": ["Yu Liu", "Zhijie Liu", "Xiao Ren", "You-Fu Li", "He Kong"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04229v1", "summary": "Predicting pedestrian motion trajectories is critical for path planning and\nmotion control of autonomous vehicles. However, accurately forecasting crowd\ntrajectories remains a challenging task due to the inherently multimodal and\nuncertain nature of human motion. Recent diffusion-based models have shown\npromising results in capturing the stochasticity of pedestrian behavior for\ntrajectory prediction. However, few diffusion-based approaches explicitly\nincorporate the underlying motion intentions of pedestrians, which can limit\nthe interpretability and precision of prediction models. In this work, we\npropose a diffusion-based multimodal trajectory prediction model that\nincorporates pedestrians' motion intentions into the prediction framework. The\nmotion intentions are decomposed into lateral and longitudinal components, and\na pedestrian intention recognition module is introduced to enable the model to\neffectively capture these intentions. Furthermore, we adopt an efficient\nguidance mechanism that facilitates the generation of interpretable\ntrajectories. The proposed framework is evaluated on two widely used human\ntrajectory prediction benchmarks, ETH and UCY, on which it is compared against\nstate-of-the-art methods. The experimental results demonstrate that our method\nachieves competitive performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04229v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04233", "title": "DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification", "authors": ["Saifullah Saifullah", "Stefan Agne", "Andreas Dengel", "Sheraz Ahmed"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04233v1", "summary": "As black-box AI-driven decision-making systems become increasingly widespread\nin modern document processing workflows, improving their transparency and\nreliability has become critical, especially in high-stakes applications where\nbiases or spurious correlations in decision-making could lead to serious\nconsequences. One vital component often found in such document processing\nworkflows is document image classification, which, despite its widespread use,\nremains difficult to explain. While some recent works have attempted to explain\nthe decisions of document image classification models through\nfeature-importance maps, these maps are often difficult to interpret and fail\nto provide insights into the global features learned by the model. In this\npaper, we aim to bridge this research gap by introducing generative document\ncounterfactuals that provide meaningful insights into the model's\ndecision-making through actionable explanations. In particular, we propose\nDocVCE, a novel approach that leverages latent diffusion models in combination\nwith classifier guidance to first generate plausible in-distribution visual\ncounterfactual explanations, and then performs hierarchical patch-wise\nrefinement to search for a refined counterfactual that is closest to the target\nfactual image. We demonstrate the effectiveness of our approach through a\nrigorous qualitative and quantitative assessment on 3 different document\nclassification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3\ndifferent models -- ResNet, ConvNeXt, and DiT -- using well-established\nevaluation criteria such as validity, closeness, and realism. To the best of\nthe authors' knowledge, this is the first work to explore generative\ncounterfactual explanations in document image analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04233v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04234", "title": "A machine learning approach for image classification in synthetic aperture RADAR", "authors": ["Romina Gaburro", "Patrick Healy", "Shraddha Naidu", "Clifford Nolan"], "categories": ["cs.CV", "math.NA"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04234v1", "summary": "We consider the problem in Synthetic Aperture RADAR (SAR) of identifying and\nclassifying objects located on the ground by means of Convolutional Neural\nNetworks (CNNs). Specifically, we adopt a single scattering approximation to\nclassify the shape of the object using both simulated SAR data and\nreconstructed images from this data, and we compare the success of these\napproaches. We then identify ice types in real SAR imagery from the satellite\nSentinel-1. In both experiments we achieve a promising high classification\naccuracy ($\\geq$75\\%). Our results demonstrate the effectiveness of CNNs in\nusing SAR data for both geometric and environmental classification tasks. Our\ninvestigation also explores the effect of SAR data acquisition at different\nantenna heights on our ability to classify objects successfully.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04234v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04236", "title": "PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction", "authors": ["Muhua Zhu", "Xinhao Jin", "Chengbo Wang", "Yongcong Zhang", "Yifei Xue", "Tie Ji", "Yizhen Lao"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04236v1", "summary": "Image stitching aim to align two images taken from different viewpoints into\none seamless, wider image. However, when the 3D scene contains depth variations\nand the camera baseline is significant, noticeable parallax occurs-meaning the\nrelative positions of scene elements differ substantially between views. Most\nexisting stitching methods struggle to handle such images with large parallax\neffectively. To address this challenge, in this paper, we propose an image\nstitching solution called PIS3R that is robust to very large parallax based on\nthe novel concept of deep 3D reconstruction. First, we apply visual geometry\ngrounded transformer to two input images with very large parallax to obtain\nboth intrinsic and extrinsic parameters, as well as the dense 3D scene\nreconstruction. Subsequently, we reproject reconstructed dense point cloud onto\na designated reference view using the recovered camera parameters, achieving\npixel-wise alignment and generating an initial stitched image. Finally, to\nfurther address potential artifacts such as holes or noise in the initial\nstitching, we propose a point-conditioned image diffusion module to obtain the\nrefined result.Compared with existing methods, our solution is very large\nparallax tolerant and also provides results that fully preserve the geometric\nintegrity of all pixels in the 3D photogrammetric context, enabling direct\napplicability to downstream 3D vision tasks such as SfM. Experimental results\ndemonstrate that the proposed algorithm provides accurate stitching results for\nimages with very large parallax, and outperforms the existing methods\nqualitatively and quantitatively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04236v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04255", "title": "From eye to AI: studying rodent social behavior in the era of machine Learning", "authors": ["Giuseppe Chindemi", "Camilla Bellone", "Benoit Girard"], "categories": ["cs.CV", "q-bio.NC"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04255v1", "summary": "The study of rodent social behavior has shifted in the last years from\nrelying on direct human observation to more nuanced approaches integrating\ncomputational methods in artificial intelligence (AI) and machine learning.\nWhile conventional approaches introduce bias and can fail to capture the\ncomplexity of rodent social interactions, modern approaches bridging computer\nvision, ethology and neuroscience provide more multifaceted insights into\nbehavior which are particularly relevant to social neuroscience. Despite these\nbenefits, the integration of AI into social behavior research also poses\nseveral challenges. Here we discuss the main steps involved and the tools\navailable for analyzing rodent social behavior, examining their advantages and\nlimitations. Additionally, we suggest practical solutions to address common\nhurdles, aiming to guide young researchers in adopting these methods and to\nstimulate further discussion among experts regarding the evolving requirements\nof these tools in scientific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04255v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04267", "title": "Revisiting Continual Semantic Segmentation with Pre-trained Vision Models", "authors": ["Duzhen Zhang", "Yong Ren", "Wei Cong", "Junhao Zheng", "Qiaoyi Su", "Shuncheng Jia", "Zhong-Zhi Li", "Xuanle Zhao", "Ye Bai", "Feilong Chen", "Qi Tian", "Tielin Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04267v1", "summary": "Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment\nnovel classes while preserving knowledge of previously encountered ones. Recent\nadvancements in CSS have been largely driven by the adoption of Pre-trained\nVision Models (PVMs) as backbones. Among existing strategies, Direct\nFine-Tuning (DFT), which sequentially fine-tunes the model across classes,\nremains the most straightforward approach. Prior work often regards DFT as a\nperformance lower bound due to its presumed vulnerability to severe\ncatastrophic forgetting, leading to the development of numerous complex\nmitigation techniques. However, we contend that this prevailing assumption is\nflawed. In this paper, we systematically revisit forgetting in DFT across two\nstandard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using\ntwo representative PVM backbones: ResNet101 and Swin-B. Through a detailed\nprobing analysis, our findings reveal that existing methods significantly\nunderestimate the inherent anti-forgetting capabilities of PVMs. Even under\nDFT, PVMs retain previously learned knowledge with minimal forgetting. Further\ninvestigation of the feature space indicates that the observed forgetting\nprimarily arises from the classifier's drift away from the PVM, rather than\nfrom degradation of the backbone representations. Based on this insight, we\npropose DFT*, a simple yet effective enhancement to DFT that incorporates\nstrategies such as freezing the PVM backbone and previously learned\nclassifiers, as well as pre-allocating future classifiers. Extensive\nexperiments show that DFT* consistently achieves competitive or superior\nperformance compared to sixteen state-of-the-art CSS methods, while requiring\nsubstantially fewer trainable parameters and less training time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04267v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04273", "title": "Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval", "authors": ["Junan Lin", "Daizong Liu", "Xianke Chen", "Xiaoye Qu", "Xun Yang", "Jixiang Zhu", "Sanyuan Zhang", "Jianfeng Dong"], "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04273v1", "summary": "Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04273v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04286", "title": "PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space", "authors": ["Chenlei Lv", "Hui Huang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04286v1", "summary": "Point cloud registration is a classical topic in the field of 3D Vision and\nComputer Graphics. Generally, the implementation of registration is typically\nsensitive to similarity transformations (translation, scaling, and rotation),\nnoisy points, and incomplete geometric structures. Especially, the non-uniform\nscales and defective parts of point clouds increase probability of struck local\noptima in registration task. In this paper, we propose a robust point cloud\nregistration PKSS-Align that can handle various influences, including\nsimilarity transformations, non-uniform densities, random noisy points, and\ndefective parts. The proposed method measures shape feature-based similarity\nbetween point clouds on the Pre-Kendall shape space (PKSS),\n\\textcolor{black}{which is a shape measurement-based scheme and doesn't require\npoint-to-point or point-to-plane metric.} The employed measurement can be\nregarded as the manifold metric that is robust to various representations in\nthe Euclidean coordinate system. Benefited from the measurement, the\ntransformation matrix can be directly generated for point clouds with mentioned\ninfluences at the same time. The proposed method does not require data training\nand complex feature encoding. Based on a simple parallel acceleration, it can\nachieve significant improvement for efficiency and feasibility in practice.\nExperiments demonstrate that our method outperforms the relevant\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04286v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04297", "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction", "authors": ["Yaopeng Lou", "Liao Shen", "Tianqi Liu", "Jiaqi Li", "Zihao Huang", "Huiqiang Sun", "Zhiguo Cao"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04297v1", "summary": "We present Multi-Baseline Gaussian Splatting (MuRF), a generalized\nfeed-forward approach for novel view synthesis that effectively handles diverse\nbaseline settings, including sparse input views with both small and large\nbaselines. Specifically, we integrate features from Multi-View Stereo (MVS) and\nMonocular Depth Estimation (MDE) to enhance feature representations for\ngeneralizable reconstruction. Next, We propose a projection-and-sampling\nmechanism for deep depth fusion, which constructs a fine probability volume to\nguide the regression of the feature map. Furthermore, We introduce a\nreference-view loss to improve geometry and optimization efficiency. We\nleverage 3D Gaussian representations to accelerate training and inference time\nwhile enhancing rendering quality. MuRF achieves state-of-the-art performance\nacross multiple baseline settings and diverse scenarios ranging from simple\nobjects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also\ndemonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04297v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04299", "title": "Length Matters: Length-Aware Transformer for Temporal Sentence Grounding", "authors": ["Yifan Wang", "Ziyi Liu", "Xiaolong Sun", "Jiawei Wang", "Hongmin Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04299v1", "summary": "Temporal sentence grounding (TSG) is a highly challenging task aiming to\nlocalize the temporal segment within an untrimmed video corresponding to a\ngiven natural language description. Benefiting from the design of learnable\nqueries, the DETR-based models have achieved substantial advancements in the\nTSG task. However, the absence of explicit supervision often causes the learned\nqueries to overlap in roles, leading to redundant predictions. Therefore, we\npropose to improve TSG by making each query fulfill its designated role,\nleveraging the length priors of the video-description pairs. In this paper, we\nintroduce the Length-Aware Transformer (LATR) for TSG, which assigns different\nqueries to handle predictions based on varying temporal lengths. Specifically,\nwe divide all queries into three groups, responsible for segments with short,\nmiddle, and long temporal durations, respectively. During training, an\nadditional length classification task is introduced. Predictions from queries\nwith mismatched lengths are suppressed, guiding each query to specialize in its\ndesignated function. Extensive experiments demonstrate the effectiveness of our\nLATR, achieving state-of-the-art performance on three public benchmarks.\nFurthermore, the ablation studies validate the contribution of each component\nof our method and the critical role of incorporating length priors into the TSG\ntask.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04299v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04316", "title": "A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks", "authors": ["Kun Gui", "Hongliang Ren", "Shang Shi", "Jin Lu", "Changqiu Yu", "Quanjun Cao", "Guomin Gu", "Qi Xuan"], "categories": ["cs.CV", "eess.SP"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04316v1", "summary": "Distributed Acoustic Sensing (DAS) technology finds growing applications\nacross various domains. However, data distribution disparities due to\nheterogeneous sensing environments pose challenges for data-driven artificial\nintelligence (AI) models, limiting cross-domain generalization and facing a\nshortage of labeled training data. To address these issues, this study proposes\na foundational model for DAS signal recognition based on a Masked Autoencoder,\nnamed MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples,\nencompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter\nsecurity, 2D time-frequency images for pipeline leakage, and open-dataset\nsignals including whale vocalizations and seismic activities, using a\nself-supervised mask reconstruction task to capture deep semantic features of\nDAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition\ntasks. This method freezes the pretrained backbone parameters and fine-tunes\nonly a small set of learnable visual prompt vectors inserted into the\nTransformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super\nplatform validate MAEPD using indoor gait recognition as a downstream task. The\nVPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322%\nof parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT)\nmethod by 0.61% and reducing training time by 45%. The model also exhibits\nrobust performance in pipeline leakage detection, confirming the generality,\nefficiency, and scalability of MAEPD as a foundational model. This approach\noffers a novel paradigm for addressing the limited generalization of signal\nrecognition models in the DAS domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04316v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04324", "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models", "authors": ["Xiaoxuan He", "Siming Fu", "Yuke Zhao", "Wanli Li", "Jian Yang", "Dacheng Yin", "Fengyun Rao", "Bo Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04324v1", "summary": "Recent flow matching models for text-to-image generation have achieved\nremarkable quality, yet their integration with reinforcement learning for human\npreference alignment remains suboptimal, hindering fine-grained reward-based\noptimization. We observe that the key impediment to effective GRPO training of\nflow models is the temporal uniformity assumption in existing approaches:\nsparse terminal rewards with uniform credit assignment fail to capture the\nvarying criticality of decisions across generation timesteps, resulting in\ninefficient exploration and suboptimal convergence. To remedy this shortcoming,\nwe introduce \\textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO\nframework that captures and exploits the temporal structure inherent in\nflow-based generation. TempFlow-GRPO introduces two key innovations: (i) a\ntrajectory branching mechanism that provides process rewards by concentrating\nstochasticity at designated branching points, enabling precise credit\nassignment without requiring specialized intermediate reward models; and (ii) a\nnoise-aware weighting scheme that modulates policy optimization according to\nthe intrinsic exploration potential of each timestep, prioritizing learning\nduring high-impact early stages while ensuring stable refinement in later\nphases. These innovations endow the model with temporally-aware optimization\nthat respects the underlying generative dynamics, leading to state-of-the-art\nperformance in human preference alignment and standard text-to-image\nbenchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04324v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04335", "title": "RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization", "authors": ["Yanyan Li", "Ze Yang", "Keisuke Tateno", "Federico Tombari Liang Zhao", "Gim Hee Lee"], "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04335v1", "summary": "Minimal parametrization of 3D lines plays a critical role in camera\nlocalization and structural mapping. Existing representations in robotics and\ncomputer vision predominantly handle independent lines, overlooking structural\nregularities such as sets of parallel lines that are pervasive in man-made\nenvironments. This paper introduces \\textbf{RiemanLine}, a unified minimal\nrepresentation for 3D lines formulated on Riemannian manifolds that jointly\naccommodates both individual lines and parallel-line groups. Our key idea is to\ndecouple each line landmark into global and local components: a shared\nvanishing direction optimized on the unit sphere $\\mathcal{S}^2$, and scaled\nnormal vectors constrained on orthogonal subspaces, enabling compact encoding\nof structural regularities. For $n$ parallel lines, the proposed representation\nreduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally\nembedding parallelism without explicit constraints. We further integrate this\nparameterization into a factor graph framework, allowing global direction\nalignment and local reprojection optimization within a unified manifold-based\nbundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic\nbenchmarks demonstrate that our method achieves significantly more accurate\npose estimation and line reconstruction, while reducing parameter\ndimensionality and improving convergence stability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04335v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04366", "title": "RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light", "authors": ["Songyun Yang", "Yufei Han", "Jilong Zhang", "Kongming Liang", "Peng Yu", "Zhaowei Qu", "Heng Guo"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04366v1", "summary": "Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface\nshapes and reflectances from images captured under varying views and\nilluminations. However, existing MVPS methods often require controlled darkroom\nsettings for varying illuminations or overlook the recovery of reflectances and\nilluminations properties, limiting their applicability in natural illumination\nscenarios and downstream inverse rendering tasks. In this paper, we propose\nRotatedMVPS to solve shape and reflectance recovery under rotated natural\nlight, achievable with a practical rotation stage. By ensuring light\nconsistency across different camera and object poses, our method reduces the\nunknowns associated with complex environment light. Furthermore, we integrate\ndata priors from off-the-shelf learning-based single-view photometric stereo\nmethods into our MVPS framework, significantly enhancing the accuracy of shape\nand reflectance recovery. Experimental results on both synthetic and real-world\ndatasets demonstrate the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04366v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04369", "title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding", "authors": ["Canhui Tang", "Zifan Han", "Hongbo Sun", "Sanping Zhou", "Xuchong Zhang", "Xin Wei", "Ye Yuan", "Jinglin Xu", "Hao Sun"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04369v1", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in vision-language tasks, yet they still face challenges when\nprocessing long-duration video inputs. The limitation arises from MLLMs'\ncontext limit and training costs, necessitating sparse frame sampling before\nfeeding videos into MLLMs. Existing video MLLMs adopt training-free uniform\nsampling or keyframe search, which may miss critical events or be constrained\nby the pre-trained models' event understanding capabilities. Meanwhile,\nbuilding a training-based method remains challenging due to the unsupervised\nand non-differentiable nature of sparse frame sampling. To address these\nproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancing\nMLLMs' long-form video-language understanding via reinforcement learning.\nSpecifically, we first propose a trainable event-aware temporal agent, which\ncaptures event-query correlation for performing probabilistic keyframe\nselection. Then, we propose the TSPO reinforcement learning paradigm, which\nmodels keyframe selection and language generation as a joint decision-making\nprocess, enabling end-to-end group relative optimization with efficient\nrule-based rewards. Furthermore, for the TSPO's training, we propose a long\nvideo training data construction pipeline with comprehensive temporal data and\nvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answering\naccuracy and temporal locating reward mechanisms to optimize the temporal\nsampling policy. Comprehensive experiments show that our TSPO achieves\nstate-of-the-art performance across multiple long video understanding\nbenchmarks, and shows transferable ability across different cutting-edge\nVideo-MLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04369v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04379", "title": "VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones", "authors": ["Lefei Shen", "Mouxiang Chen", "Xu Liu", "Han Fu", "Xiaoxue Ren", "Jianling Sun", "Zhuo Li", "Chenghao Liu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04379v1", "summary": "Recent studies have revealed that vision models pre-trained on images can\nperform well in time series forecasting by reformulating forecasting as an\nimage reconstruction task, suggesting their potential as universal time series\nfoundation models. However, effective cross-modal transfer from vision to time\nseries remains challenging due to three key discrepancies: (1) data-modality\ngap between structured, bounded image data and unbounded, heterogeneous time\nseries; (2) multivariate-forecasting gap between standard RGB\nthree-channel-based vision models and the need to model time series with\narbitrary numbers of variates; and (3) probabilistic-forecasting gap between\nthe deterministic output formats of most vision models and the requirement for\nuncertainty-aware probabilistic predictions. To bridge these gaps, we propose\nVisionTS++, a vision-model-based TSFM that performs continual pre-training on\nlarge-scale time series datasets, including 3 innovations: (1) a\nvision-model-based filtering mechanism to identify high-quality time series\ndata, thereby mitigating modality gap and improving pre-training stability, (2)\na colorized multivariate conversion method that transforms multivariate time\nseries into multi-subfigure RGB images, capturing complex inter-variate\ndependencies; and (3) a multi-quantile forecasting approach using parallel\nreconstruction heads to generate forecasts of different quantile levels, thus\nmore flexibly approximating arbitrary output distributions without restrictive\nprior distributional assumptions. Evaluated on both in-distribution and\nout-of-distribution TSF benchmarks, \\model achieves SOTA results, outperforming\nspecialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12\nprobabilistic forecasting settings. Our work establishes a new paradigm for\ncross-modal knowledge transfer, advancing the development of universal TSFMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04379v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04416", "title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning", "authors": ["Haoji Zhang", "Xin Gu", "Jiawen Li", "Chixiang Ma", "Sule Bai", "Chubin Zhang", "Bowen Zhang", "Zhichao Zhou", "Dongliang He", "Yansong Tang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04416v1", "summary": "The video reasoning ability of multimodal large language models (MLLMs) is\ncrucial for downstream tasks like video question answering and temporal\ngrounding. While recent approaches have explored text-based chain-of-thought\n(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal\ninteraction and increased hallucination, especially with longer videos or\nreasoning chains. To address these challenges, we propose Video Intelligence\nvia Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning\nframework. With a visual toolbox, the model can densely sample new video frames\non demand and generate multimodal CoT for precise long video reasoning. We\nobserve that temporal grounding and question answering are mutually beneficial\nfor video understanding tasks. Therefore, we construct two high-quality\nmulti-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and\nMTVR-RL-110k for reinforcement learning. Moreover, we propose a\nDifficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to\nmitigate difficulty imbalance in multi-task reinforcement learning. Extensive\nexperiments on 11 challenging video understanding benchmarks demonstrate the\nadvanced reasoning ability of VITAL, outperforming existing methods in video\nquestion answering and temporal grounding tasks, especially in long video\nscenarios. All code, data and model weight will be made publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04416v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04422", "title": "Efficient Inter-Task Attention for Multitask Transformer Models", "authors": ["Christian Bohn", "Thomas Kurbiel", "Klaus Friedrichs", "Hasan Tercan", "Tobias Meisen"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04422v1", "summary": "In both Computer Vision and the wider Deep Learning field, the Transformer\narchitecture is well-established as state-of-the-art for many applications. For\nMultitask Learning, however, where there may be many more queries necessary\ncompared to single-task models, its Multi-Head-Attention often approaches the\nlimits of what is computationally feasible considering practical hardware\nlimitations. This is due to the fact that the size of the attention matrix\nscales quadratically with the number of tasks (assuming roughly equal numbers\nof queries for all tasks). As a solution, we propose our novel Deformable\nInter-Task Self-Attention for Multitask models that enables the much more\nefficient aggregation of information across the feature maps from different\ntasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we\ndemonstrate an order-of-magnitude reduction in both FLOPs count and inference\nlatency. At the same time, we also achieve substantial improvements by up to\n7.4% in the individual tasks' prediction quality metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04422v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04424", "title": "Composed Object Retrieval: Object-level Retrieval via Composed Expressions", "authors": ["Tong Wang", "Guanyu Yang", "Nian Liu", "Zongyan Han", "Jinxing Zhou", "Salman Khan", "Fahad Shahbaz Khan"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04424v1", "summary": "Retrieving fine-grained visual content based on user intent remains a\nchallenge in multi-modal systems. Although current Composed Image Retrieval\n(CIR) methods combine reference images with retrieval texts, they are\nconstrained to image-level matching and cannot localize specific objects. To\nthis end, we propose Composed Object Retrieval (COR), a brand-new task that\ngoes beyond image-level retrieval to achieve object-level precision, allowing\nthe retrieval and segmentation of target objects based on composed expressions\ncombining reference objects and retrieval texts. COR presents significant\nchallenges in retrieval flexibility, which requires systems to identify\narbitrary objects satisfying composed expressions while avoiding semantically\nsimilar but irrelevant negative objects within the same scene. We construct\nCOR127K, the first large-scale COR benchmark that contains 127,166 retrieval\ntriplets with various semantic transformations in 408 categories. We also\npresent CORE, a unified end-to-end model that integrates reference region\nencoding, adaptive visual-textual interaction, and region-level contrastive\nlearning. Extensive experiments demonstrate that CORE significantly outperforms\nexisting models in both base and novel categories, establishing a simple and\neffective baseline for this challenging task while opening new directions for\nfine-grained multi-modal retrieval research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04424v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04441", "title": "Benchmarking Foundation Models for Mitotic Figure Classification", "authors": ["Jonas Ammeling", "Jonathan Ganz", "Emely Rosbach", "Ludwig Lausser", "Christof A. Bertram", "Katharina Breininger", "Marc Aubreville"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04441v1", "summary": "The performance of deep learning models is known to scale with data quantity\nand diversity. In pathology, as in many other medical imaging domains, the\navailability of labeled images for a specific task is often limited.\nSelf-supervised learning techniques have enabled the use of vast amounts of\nunlabeled data to train large-scale neural networks, i.e., foundation models,\nthat can address the limited data problem by providing semantically rich\nfeature vectors that can generalize well to new tasks with minimal training\neffort increasing model performance and robustness. In this work, we\ninvestigate the use of foundation models for mitotic figure classification. The\nmitotic count, which can be derived from this classification task, is an\nindependent prognostic marker for specific tumors and part of certain tumor\ngrading systems. In particular, we investigate the data scaling laws on\nmultiple current foundation models and evaluate their robustness to unseen\ntumor domains. Next to the commonly used linear probing paradigm, we also adapt\nthe models using low-rank adaptation (LoRA) of their attention mechanisms. We\ncompare all models against end-to-end-trained baselines, both CNNs and Vision\nTransformers. Our results demonstrate that LoRA-adapted foundation models\nprovide superior performance to those adapted with standard linear probing,\nreaching performance levels close to 100% data availability with only 10% of\ntraining data. Furthermore, LoRA-adaptation of the most recent foundation\nmodels almost closes the out-of-domain performance gap when evaluated on unseen\ntumor domains. However, full fine-tuning of traditional architectures still\nyields competitive performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04441v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04453", "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion", "authors": ["Qingguo Hu", "Ante Wang", "Jia Song", "Delai Qiu", "Qingsong Liu", "Jinsong Su"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04453v1", "summary": "Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04453v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04467", "title": "4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation", "authors": ["Shuzhou Yang", "Xiaodong Cun", "Xiaoyu Li", "Yaowei Li", "Jian Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04467v1", "summary": "Given the high complexity of directly generating high-dimensional data such\nas 4D, we present 4DVD, a cascaded video diffusion model that generates 4D\ncontent in a decoupled manner. Unlike previous multi-view video methods that\ndirectly model 3D space and temporal features simultaneously with stacked cross\nview/temporal attention modules, 4DVD decouples this into two subtasks: coarse\nmulti-view layout generation and structure-aware conditional generation, and\neffectively unifies them. Specifically, given a monocular video, 4DVD first\npredicts the dense view content of its layout with superior cross-view and\ntemporal consistency. Based on the produced layout priors, a structure-aware\nspatio-temporal generation branch is developed, combining these coarse\nstructural priors with the exquisite appearance content of input monocular\nvideo to generate final high-quality dense-view videos. Benefit from this,\nexplicit 4D representation~(such as 4D Gaussian) can be optimized accurately,\nenabling wider practical application. To train 4DVD, we collect a dynamic 3D\nobject dataset, called D-Objaverse, from the Objaverse benchmark and render 16\nvideos with 21 frames for each object. Extensive experiments demonstrate our\nstate-of-the-art performance on both novel view synthesis and 4D generation.\nOur project page is https://4dvd.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04467v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04485", "title": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution", "authors": ["Bowen Chai", "Zheng Chen", "Libo Zhu", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04485v1", "summary": "Diffusion models have shown superior performance in real-world video\nsuper-resolution (VSR). However, the slow processing speeds and heavy resource\nconsumption of diffusion models hinder their practical application and\ndeployment. Quantization offers a potential solution for compressing the VSR\nmodel. Nevertheless, quantizing VSR models is challenging due to their temporal\ncharacteristics and high fidelity requirements. To address these issues, we\npropose QuantVSR, a low-bit quantization model for real-world VSR. We propose a\nspatio-temporal complexity aware (STCA) mechanism, where we first utilize the\ncalibration dataset to measure both spatial and temporal complexities for each\nlayer. Based on these statistics, we allocate layer-specific ranks to the\nlow-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine\nthe FP and low-bit branches to achieve simultaneous optimization. In addition,\nwe propose a learnable bias alignment (LBA) module to reduce the biased\nquantization errors. Extensive experiments on synthetic and real-world datasets\ndemonstrate that our method obtains comparable performance with the FP model\nand significantly outperforms recent leading low-bit quantization methods. Code\nis available at: https://github.com/bowenchai/QuantVSR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04485v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04505", "title": "MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos", "authors": ["Daisheng Jin", "Ying He"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04505v1", "summary": "Reconstructing realistic 3D human avatars from monocular videos is a\nchallenging task due to the limited geometric information and complex non-rigid\nmotion involved. We present MonoCloth, a new method for reconstructing and\nanimating clothed human avatars from monocular videos. To overcome the\nlimitations of monocular input, we introduce a part-based decomposition\nstrategy that separates the avatar into body, face, hands, and clothing. This\ndesign reflects the varying levels of reconstruction difficulty and deformation\ncomplexity across these components. Specifically, we focus on detailed geometry\nrecovery for the face and hands. For clothing, we propose a dedicated cloth\nsimulation module that captures garment deformation using temporal motion cues\nand geometric constraints. Experimental results demonstrate that MonoCloth\nimproves both visual reconstruction quality and animation realism compared to\nexisting methods. Furthermore, thanks to its part-based design, MonoCloth also\nsupports additional tasks such as clothing transfer, underscoring its\nversatility and practical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04505v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04513", "title": "Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation", "authors": ["Uzay GÃ¶kay", "Federico Spurio", "Dominik R. Bach", "Juergen Gall"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04513v1", "summary": "Current state-of-the-art methods for skeleton-based temporal action\nsegmentation are predominantly supervised and require annotated data, which is\nexpensive to collect. In contrast, existing unsupervised temporal action\nsegmentation methods have focused primarily on video data, while skeleton\nsequences remain underexplored, despite their relevance to real-world\napplications, robustness, and privacy-preserving nature. In this paper, we\npropose a novel approach for unsupervised skeleton-based temporal action\nsegmentation. Our method utilizes a sequence-to-sequence temporal autoencoder\nthat keeps the information of the different joints disentangled in the\nembedding space. Latent skeleton sequences are then divided into\nnon-overlapping patches and quantized to obtain distinctive skeleton motion\nwords, driving the discovery of semantically meaningful action clusters. We\nthoroughly evaluate the proposed approach on three widely used skeleton-based\ndatasets, namely HuGaDB, LARa, and BABEL. The results demonstrate that our\nmodel outperforms the current state-of-the-art unsupervised temporal action\nsegmentation methods. Code is available at https://github.com/bachlab/SMQ .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04513v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04534", "title": "No Masks Needed: Explainable AI for Deriving Segmentation from Classification", "authors": ["Mosong Ma", "Tania Stathaki", "Michalis Lazarou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04534v1", "summary": "Medical image segmentation is vital for modern healthcare and is a key\nelement of computer-aided diagnosis. While recent advancements in computer\nvision have explored unsupervised segmentation using pre-trained models, these\nmethods have not been translated well to the medical imaging domain. In this\nwork, we introduce a novel approach that fine-tunes pre-trained models\nspecifically for medical images, achieving accurate segmentation with extensive\nprocessing. Our method integrates Explainable AI to generate relevance scores,\nenhancing the segmentation process. Unlike traditional methods that excel in\nstandard benchmarks but falter in medical applications, our approach achieves\nimproved results on datasets like CBIS-DDSM, NuInsSeg and Kvasir-SEG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04534v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04539", "title": "TopKD: Top-scaled Knowledge Distillation", "authors": ["Qi Wang", "Jinjia Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04539v1", "summary": "Recent advances in knowledge distillation (KD) predominantly emphasize\nfeature-level knowledge transfer, frequently overlooking critical information\nembedded within the teacher's logit distributions. In this paper, we revisit\nlogit-based distillation and reveal an underexplored yet critical element:\nTop-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge\nDistillation (TopKD), a simple, efficient, and architecture-agnostic framework\nthat significantly enhances logit-based distillation. TopKD consists of two\nmain components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies\nthe most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers\ntargeted and effective supervision. Notably, TopKD integrates seamlessly into\nexisting KD methods without introducing extra modules or requiring\narchitectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10,\nand Tiny-ImageNet demonstrate that TopKD consistently surpasses\nstate-of-the-art distillation methods. Moreover, our method demonstrates\nsubstantial effectiveness when distilling Vision Transformers, underscoring its\nversatility across diverse network architectures. These findings highlight the\nsignificant potential of logits to advance knowledge distillation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04539v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04540", "title": "InceptoFormer: A Multi-Signal Neural Framework for Parkinson's Disease Severity Evaluation from Gait", "authors": ["Safwen Naimi", "Arij Said", "Wassim Bouachir", "Guillaume-Alexandre Bilodeau"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04540v1", "summary": "We present InceptoFormer, a multi-signal neural framework designed for\nParkinson's Disease (PD) severity evaluation via gait dynamics analysis. Our\narchitecture introduces a 1D adaptation of the Inception model, which we refer\nto as Inception1D, along with a Transformer-based framework to stage PD\nseverity according to the Hoehn and Yahr (H&Y) scale. The Inception1D component\ncaptures multi-scale temporal features by employing parallel 1D convolutional\nfilters with varying kernel sizes, thereby extracting features across multiple\ntemporal scales. The transformer component efficiently models long-range\ndependencies within gait sequences, providing a comprehensive understanding of\nboth local and global patterns. To address the issue of class imbalance in PD\nseverity staging, we propose a data structuring and preprocessing strategy\nbased on oversampling to enhance the representation of underrepresented\nseverity levels. The overall design enables to capture fine-grained temporal\nvariations and global dynamics in gait signal, significantly improving\nclassification performance for PD severity evaluation. Through extensive\nexperimentation, InceptoFormer achieves an accuracy of 96.6%, outperforming\nexisting state-of-the-art methods in PD severity assessment. The source code\nfor our implementation is publicly available at\nhttps://github.com/SafwenNaimi/InceptoFormer", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04540v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04546", "title": "Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding", "authors": ["Minghang Zheng", "Yuxin Peng", "Benyuan Sun", "Yi Yang", "Yang Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04546v1", "summary": "In this paper, we tackle the task of online video temporal grounding (OnVTG),\nwhich requires the model to locate events related to a given text query within\na video stream. Unlike regular video temporal grounding, OnVTG requires the\nmodel to make predictions without observing future frames. As online videos are\nstreaming inputs and can go on indefinitely, it is impractical and inefficient\nto store all historical inputs. The existing OnVTG models employ memory to\nstore recent historical video frame features and predict scores indicating\nwhether the current frame corresponds to the start or end time of the target\nevent. However, these methods lack effective event modeling and cannot retain\nlong-term historical information, leading to low performance. To tackle these\nchallenges, we propose a hierarchical event memory for OnVTG. We propose an\nevent-based OnVTG framework that makes predictions based on event proposals\nthat model event-level information with various durations. To preserve\nhistorically valuable event information, we introduce a hierarchical event\nmemory that retains historical events, allowing the model to access both recent\nand long-term information. To enable the real-time prediction, we further\npropose a future prediction branch that predicts whether the target event will\noccur shortly and further regresses the start time of the event. We achieve\nstate-of-the-art performance on the TACoS, ActivityNet Captions, and MAD\ndatasets. Code is available at https://github.com/minghangz/OnVTG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04546v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04551", "title": "Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis", "authors": ["Angang Zhang", "Fang Deng", "Hao Chen", "Zhongjian Chen", "Junyan Li"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04551v1", "summary": "While recent advances in virtual try-on (VTON) have achieved realistic\ngarment transfer to human subjects, its inverse task, virtual try-off (VTOFF),\nwhich aims to reconstruct canonical garment templates from dressed humans,\nremains critically underexplored and lacks systematic investigation. Existing\nworks predominantly treat them as isolated tasks: VTON focuses on garment\ndressing while VTOFF addresses garment extraction, thereby neglecting their\ncomplementary symmetry. To bridge this fundamental gap, we propose the Two-Way\nGarment Transfer Model (TWGTM), to the best of our knowledge, the first unified\nframework for joint clothing-centric image synthesis that simultaneously\nresolves both mask-guided VTON and mask-free VTOFF through bidirectional\nfeature disentanglement. Specifically, our framework employs dual-conditioned\nguidance from both latent and pixel spaces of reference images to seamlessly\nbridge the dual tasks. On the other hand, to resolve the inherent mask\ndependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a\nphased training paradigm that progressively bridges this modality gap.\nExtensive qualitative and quantitative experiments conducted across the\nDressCode and VITON-HD datasets validate the efficacy and competitive edge of\nour proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04551v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04552", "title": "Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation", "authors": ["Franz Thaler", "Darko Stern", "Gernot Plank", "Martin Urschler"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04552v1", "summary": "As the leading cause of death worldwide, cardiovascular diseases motivate the\ndevelopment of more sophisticated methods to analyze the heart and its\nsubstructures from medical images like Computed Tomography (CT) and Magnetic\nResonance (MR). Semantic segmentations of important cardiac structures that\nrepresent the whole heart are useful to assess patient-specific cardiac\nmorphology and pathology. Furthermore, accurate semantic segmentations can be\nused to generate cardiac digital twin models which allows e.g.\nelectrophysiological simulation and personalized therapy planning. Even though\ndeep learning-based methods for medical image segmentation achieved great\nadvancements over the last decade, retaining good performance under domain\nshift -- i.e. when training and test data are sampled from different data\ndistributions -- remains challenging. In order to perform well on domains known\nat training-time, we employ a (1) balanced joint training approach that\nutilizes CT and MR data in equal amounts from different source domains.\nFurther, aiming to alleviate domain shift towards domains only encountered at\ntest-time, we rely on (2) strong intensity and spatial augmentation techniques\nto greatly diversify the available training data. Our proposed whole heart\nsegmentation method, a 5-fold ensemble with our contributions, achieves the\nbest performance for MR data overall and a performance similar to the best\nperformance for CT data when compared to a model trained solely on CT. With\n93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR\ndata, our method demonstrates great potential to efficiently obtain accurate\nsemantic segmentations from which patient-specific cardiac twin models can be\ngenerated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04552v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04559", "title": "One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose", "authors": ["Jinxi Liu", "Zijian He", "Guangrun Wang", "Guanbin Li", "Liang Lin"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04559v1", "summary": "Recent diffusion-based approaches have made significant advances in\nimage-based virtual try-on, enabling more realistic and end-to-end garment\nsynthesis. However, most existing methods remain constrained by their reliance\non exhibition garments and segmentation masks, as well as their limited ability\nto handle flexible pose variations. These limitations reduce their practicality\nin real-world scenarios-for instance, users cannot easily transfer garments\nworn by one person onto another, and the generated try-on results are typically\nrestricted to the same pose as the reference image. In this paper, we introduce\n\\textbf{OMFA} (\\emph{One Model For All}), a unified diffusion framework for\nboth virtual try-on and try-off that operates without the need for exhibition\ngarments and supports arbitrary poses. For example, OMFA enables removing\ngarments from a source person (try-off) and transferring them onto a target\nperson (try-on), while also allowing the generated target to appear in novel\nposes-even without access to multi-pose images of that person. OMFA is built\nupon a novel \\emph{partial diffusion} strategy that selectively applies noise\nand denoising to individual components of the joint input-such as the garment,\nthe person image, or the face-enabling dynamic subtask control and efficient\nbidirectional garment-person transformation. The framework is entirely\nmask-free and requires only a single portrait and a target pose as input,\nmaking it well-suited for real-world applications. Additionally, by leveraging\nSMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose\ntry-on from just one image. Extensive experiments demonstrate that OMFA\nachieves state-of-the-art results on both try-on and try-off tasks, providing a\npractical and generalizable solution for virtual garment synthesis. The project\npage is here: https://onemodelforall.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04559v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04564", "title": "Drone Detection with Event Cameras", "authors": ["Gabriele Magrini", "Lorenzo Berlincioni", "Luca Cultrera", "Federico Becattini", "Pietro Pala"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04564v1", "summary": "The diffusion of drones presents significant security and safety challenges.\nTraditional surveillance systems, particularly conventional frame-based\ncameras, struggle to reliably detect these targets due to their small size,\nhigh agility, and the resulting motion blur and poor performance in challenging\nlighting conditions. This paper surveys the emerging field of event-based\nvision as a robust solution to these problems. Event cameras virtually\neliminate motion blur and enable consistent detection in extreme lighting.\nTheir sparse, asynchronous output suppresses static backgrounds, enabling\nlow-latency focus on motion cues. We review the state-of-the-art in event-based\ndrone detection, from data representation methods to advanced processing\npipelines using spiking neural networks. The discussion extends beyond simple\ndetection to cover more sophisticated tasks such as real-time tracking,\ntrajectory forecasting, and unique identification through propeller signature\nanalysis. By examining current methodologies, available datasets, and the\ndistinct advantages of the technology, this work demonstrates that event-based\nvision provides a powerful foundation for the next generation of reliable,\nlow-latency, and efficient counter-UAV systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04564v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04565", "title": "TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning", "authors": ["Yunbi Liu", "Enqi Tang", "Shiyu Li", "Lei Ma", "Juncheng Li", "Shu Lou", "Yongchu Pan", "Qingshan Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04565v1", "summary": "Orthodontic treatment hinges on tooth alignment, which significantly affects\nocclusal function, facial aesthetics, and patients' quality of life. Current\ndeep learning approaches predominantly concentrate on predicting transformation\nmatrices through imposing point-to-point geometric constraints for tooth\nalignment. Nevertheless, these matrices are likely associated with the\nanatomical structure of the human oral cavity and possess particular\ndistribution characteristics that the deterministic point-to-point geometric\nconstraints in prior work fail to capture. To address this, we introduce a new\nautomatic tooth alignment method named TAlignDiff, which is supported by\ndiffusion-based transformation learning. TAlignDiff comprises two main\ncomponents: a primary point cloud-based regression network (PRN) and a\ndiffusion-based transformation matrix denoising module (DTMD).\nGeometry-constrained losses supervise PRN learning for point cloud-level\nalignment. DTMD, as an auxiliary module, learns the latent distribution of\ntransformation matrices from clinical data. We integrate point cloud-based\ntransformation regression and diffusion-based transformation modeling into a\nunified framework, allowing bidirectional feedback between geometric\nconstraints and diffusion refinement. Extensive ablation and comparative\nexperiments demonstrate the effectiveness and superiority of our method,\nhighlighting its potential in orthodontic treatment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04565v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04568", "title": "DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling", "authors": ["Yijie Li", "Wei Zhang", "Xi Zhu", "Ye Wu", "Yogesh Rathi", "Lauren J. O'Donnell", "Fan Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04568v1", "summary": "This paper presents DDTracking, a novel deep generative framework for\ndiffusion MRI tractography that formulates streamline propagation as a\nconditional denoising diffusion process. In DDTracking, we introduce a\ndual-pathway encoding network that jointly models local spatial encoding\n(capturing fine-scale structural details at each streamline point) and global\ntemporal dependencies (ensuring long-range consistency across the entire\nstreamline). Furthermore, we design a conditional diffusion model module, which\nleverages the learned local and global embeddings to predict streamline\npropagation orientations for tractography in an end-to-end trainable manner. We\nconduct a comprehensive evaluation across diverse, independently acquired dMRI\ndatasets, including both synthetic and clinical data. Experiments on two\nwell-established benchmarks with ground truth (ISMRM Challenge and\nTractoInferno) demonstrate that DDTracking largely outperforms current\nstate-of-the-art tractography methods. Furthermore, our results highlight\nDDTracking's strong generalizability across heterogeneous datasets, spanning\nvarying health conditions, age groups, imaging protocols, and scanner types.\nCollectively, DDTracking offers anatomically plausible and robust tractography,\npresenting a scalable, adaptable, and end-to-end learnable solution for broad\ndMRI applications. Code is available at:\nhttps://github.com/yishengpoxiao/DDtracking.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04568v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04572", "title": "Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding", "authors": ["Jun Li", "Che Liu", "Wenjia Bai", "Mingxuan Liu", "Rossella Arcucci", "Cosmin I. Bercea", "Julia A. Schnabel"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04572v1", "summary": "In this work, we address the problem of grounding abnormalities in medical\nimages, where the goal is to localize clinical findings based on textual\ndescriptions. While generalist Vision-Language Models (VLMs) excel in natural\ngrounding tasks, they often struggle in the medical domain due to rare,\ncompositional, and domain-specific terms that are poorly aligned with visual\npatterns. Specialized medical VLMs address this challenge via large-scale\ndomain pretraining, but at the cost of substantial annotation and computational\nresources. To overcome these limitations, we propose \\textbf{Knowledge to Sight\n(K2Sight)}, a framework that introduces structured semantic supervision by\ndecomposing clinical concepts into interpretable visual attributes, such as\nshape, density, and anatomical location. These attributes are distilled from\ndomain ontologies and encoded into concise instruction-style prompts, which\nguide region-text alignment during training. Unlike conventional report-level\nsupervision, our approach explicitly bridges domain knowledge and spatial\nstructure, enabling data-efficient training of compact models. We train compact\nmodels with 0.23B and 2B parameters using only 1.5\\% of the data required by\nstate-of-the-art medical VLMs. Despite their small size and limited training\ndata, these models achieve performance on par with or better than 7B+ medical\nVLMs, with up to 9.82\\% improvement in $mAP_{50}$. Code and models:\n\\href{https://lijunrio.github.io/K2Sight/}{\\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04572v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04573", "title": "Visual Bias and Interpretability in Deep Learning for Dermatological Image Analysis", "authors": ["Enam Ahmed Taufik", "Abdullah Khondoker", "Antara Firoz Parsa", "Seraj Al Mahmud Mostafa"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04573v1", "summary": "Accurate skin disease classification is a critical yet challenging task due\nto high inter-class similarity, intra-class variability, and complex lesion\ntextures. While deep learning-based computer-aided diagnosis (CAD) systems have\nshown promise in automating dermatological assessments, their performance is\nhighly dependent on image pre-processing and model architecture. This study\nproposes a deep learning framework for multi-class skin disease classification,\nsystematically evaluating three image pre-processing techniques: standard RGB,\nCMY color space transformation, and Contrast Limited Adaptive Histogram\nEqualization (CLAHE). We benchmark the performance of pre-trained convolutional\nneural networks (DenseNet201, Efficient-NetB5) and transformer-based models\n(ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score as evaluation\nmetrics. Results show that DinoV2 with RGB pre-processing achieves the highest\naccuracy (up to 93%) and F1-scores across all variants. Grad-CAM visualizations\napplied to RGB inputs further reveal precise lesion localization, enhancing\ninterpretability. These findings underscore the importance of effective\npre-processing and model choice in building robust and explainable CAD systems\nfor dermatology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04573v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04592", "title": "Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan", "authors": ["Marta Moscati", "Ahmed Abdullah", "Muhammad Saad Saeed", "Shah Nawaz", "Rohan Kumar Das", "Muhammad Zaigham Zaheer", "Junaid Mir", "Muhammad Haroon Yousaf", "Khalid Malik", "Markus Schedl"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04592v1", "summary": "The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, audio-visual systems are among the\nmost widely used multimodal systems. In the recent years, associating face and\nvoice of a person has gained attention due to the presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) 2026 Challenge focuses on exploring face-voice association\nunder the unique condition of a multilingual scenario. This condition is\ninspired from the fact that half of the world's population is bilingual and\nmost often people communicate under multilingual scenarios. The challenge uses\na dataset named Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baseline models, and task details for the FAME\nChallenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04592v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04597", "title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline", "authors": ["Linqing Zhao", "Xiuwei Xu", "Yirui Wang", "Hao Wang", "Wenzhao Zheng", "Yansong Tang", "Haibin Yan", "Jiwen Lu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04597v1", "summary": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04597v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04611", "title": "OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment", "authors": ["Tongfan Guan", "Jiaxin Guo", "Chen Wang", "Yun-Hui Liu"], "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04611v1", "summary": "Monocular and stereo depth estimation offer complementary strengths:\nmonocular methods capture rich contextual priors but lack geometric precision,\nwhile stereo approaches leverage epipolar geometry yet struggle with\nambiguities such as reflective or textureless surfaces. Despite post-hoc\nsynergies, these paradigms remain largely disjoint in practice. We introduce\nOmniDepth, a unified framework that bridges both through iterative\nbidirectional alignment of their latent representations. At its core, a novel\ncross-attentive alignment mechanism dynamically synchronizes monocular\ncontextual cues with stereo hypothesis representations during stereo reasoning.\nThis mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by\ninjecting monocular structure priors while refining monocular depth with stereo\ngeometry within a single network. Extensive experiments demonstrate\nstate-of-the-art results: \\textbf{OmniDepth reduces zero-shot generalization\nerror by $\\!>\\!40\\%$ on Middlebury and ETH3D}, while addressing longstanding\nfailures on transparent and reflective surfaces. By harmonizing multi-view\ngeometry with monocular context, OmniDepth enables robust 3D perception that\ntranscends modality-specific limitations. Codes available at\nhttps://github.com/aeolusguan/OmniDepth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04611v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04614", "title": "How Does Bilateral Ear Symmetry Affect Deep Ear Features?", "authors": ["Kagan Ozturk", "Deeksha Arun", "Kevin W. Bowyer", "Patrick Flynn"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04614v1", "summary": "Ear recognition has gained attention as a reliable biometric technique due to\nthe distinctive characteristics of human ears. With the increasing availability\nof large-scale datasets, convolutional neural networks (CNNs) have been widely\nadopted to learn features directly from raw ear images, outperforming\ntraditional hand-crafted methods. However, the effect of bilateral ear symmetry\non the features learned by CNNs has received little attention in recent\nstudies. In this paper, we investigate how bilateral ear symmetry influences\nthe effectiveness of CNN-based ear recognition. To this end, we first develop\nan ear side classifier to automatically categorize ear images as either left or\nright. We then explore the impact of incorporating this side information during\nboth training and test. Cross-dataset evaluations are conducted on five\ndatasets. Our results suggest that treating left and right ears separately\nduring training and testing can lead to notable performance improvements.\nFurthermore, our ablation studies on alignment strategies, input sizes, and\nvarious hyperparameter settings provide practical insights into training\nCNN-based ear recognition systems on large-scale datasets to achieve higher\nverification rates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04614v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04650", "title": "EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts", "authors": ["Kushin Mukherjee", "Donghao Ren", "Dominik Moritz", "Yannick Assogba"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04650v1", "summary": "Multimodal vision-language models (VLMs) continue to achieve ever-improving\nscores on chart understanding benchmarks. Yet, we find that this progress does\nnot fully capture the breadth of visual reasoning capabilities essential for\ninterpreting charts. We introduce EncQA, a novel benchmark informed by the\nvisualization literature, designed to provide systematic coverage of visual\nencodings and analytic tasks that are crucial for chart understanding. EncQA\nprovides 2,076 synthetic question-answer pairs, enabling balanced coverage of\nsix visual encoding channels (position, length, area, color quantitative, color\nnominal, and shape) and eight tasks (find extrema, retrieve value, find\nanomaly, filter values, compute derived value exact, compute derived value\nrelative, correlate values, and correlate values relative). Our evaluation of 9\nstate-of-the-art VLMs reveals that performance varies significantly across\nencodings within the same task, as well as across tasks. Contrary to\nexpectations, we observe that performance does not improve with model size for\nmany task-encoding pairs. Our results suggest that advancing chart\nunderstanding requires targeted strategies addressing specific visual reasoning\ngaps, rather than solely scaling up model or dataset size.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04650v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04659", "title": "PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment", "authors": ["Gustav Hanning", "Kalle ÃstrÃ¶m", "Viktor Larsson"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04659v1", "summary": "Coarse room layout estimation provides important geometric cues for many\ndownstream tasks. Current state-of-the-art methods are predominantly based on\nsingle views and often assume panoramic images. We introduce PixCuboid, an\noptimization-based approach for cuboid-shaped room layout estimation, which is\nbased on multi-view alignment of dense deep features. By training with the\noptimization end-to-end, we learn feature maps that yield large convergence\nbasins and smooth loss landscapes in the alignment. This allows us to\ninitialize the room layout using simple heuristics.\n  For the evaluation we propose two new benchmarks based on ScanNet++ and\n2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough\nexperiments we validate our approach and significantly outperform the\ncompetition. Finally, while our network is trained with single cuboids, the\nflexibility of the optimization-based approach allow us to easily extend to\nmulti-room estimation, e.g. larger apartments or offices. Code and model\nweights are available at https://github.com/ghanning/PixCuboid.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04659v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04677", "title": "ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models", "authors": ["Yansheng Gao", "Yufei Zheng", "Jinghan Qu", "Zixi Zhu", "Yukuan Zhang", "Shengsheng Wang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04677v1", "summary": "Prompt tuning has emerged as an efficient and effective technique for\nadapting vision-language models (VLMs) with low computational overhead.\nHowever, existing methods often overlook the vulnerability of prompt-tuned VLMs\nto weak semantic perturbations-such as subtle image or text noise-that degrade\ntheir generalization to unseen classes. To address this limitation, we propose\nANPrompt, a novel prompt tuning framework designed to enhance robustness under\nsuch perturbations. ANPrompt first constructs weak noise text features by\nfusing original and noise-perturbed text embeddings, which are then clustered\nto form noise prompts. These noise prompts are integrated with learnable prompt\ntokens to generate anti-noise prompts, which are injected into the deeper\nlayers of both image and text encoders. To further capture the noise-aware\nvisual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype\n(NRVPP) by averaging the output prompt tokens from the vision encoder. Finally,\nANPrompt introduces alignment, robustness, and anti-noise objectives by\ncomputing a Weak semantic noise Alignment Loss (WALoss) alongside the standard\ncross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that\nANPrompt consistently outperforms existing prompt tuning approaches, achieving\nsuperior robustness to semantic noise and improved generalization to novel\ncategories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04677v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04681", "title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions", "authors": ["Liang Xu", "Chengqun Yang", "Zili Lin", "Fei Xu", "Yifan Liu", "Congsheng Xu", "Yiyi Zhang", "Jie Qin", "Xingdong Sheng", "Yunhui Liu", "Xin Jin", "Yichao Yan", "Wenjun Zeng", "Xiaokang Yang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04681v1", "summary": "Learning action models from real-world human-centric interaction datasets is\nimportant towards building general-purpose intelligent assistants with\nefficiency. However, most existing datasets only offer specialist interaction\ncategory and ignore that AI assistants perceive and act based on first-person\nacquisition. We urge that both the generalist interaction knowledge and\negocentric modality are indispensable. In this paper, we embed the\nmanual-assisted task into a vision-language-action framework, where the\nassistant provides services to the instructor following egocentric vision and\ncommands. With our hybrid RGB-MoCap system, pairs of assistants and instructors\nengage with multiple objects and the scene following GPT-generated scripts.\nUnder this setting, we accomplish InterVLA, the first large-scale\nhuman-object-human interaction dataset with 11.4 hours and 1.2M frames of\nmultimodal data, spanning 2 egocentric and 5 exocentric videos, accurate\nhuman/object motions and verbal commands. Furthermore, we establish novel\nbenchmarks on egocentric human motion estimation, interaction synthesis, and\ninteraction prediction with comprehensive analysis. We believe that our\nInterVLA testbed and the benchmarks will foster future works on building AI\nagents in the physical world.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04681v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04682", "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction", "authors": ["Zewei Zhou", "Seth Z. Zhao", "Tianhui Cai", "Zhiyu Huang", "Bolei Zhou", "Jiaqi Ma"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04682v1", "summary": "End-to-end training of multi-agent systems offers significant advantages in\nimproving multi-task performance. However, training such models remains\nchallenging and requires extensive manual design and monitoring. In this work,\nwe introduce TurboTrain, a novel and efficient training framework for\nmulti-agent perception and prediction. TurboTrain comprises two key components:\na multi-agent spatiotemporal pretraining scheme based on masked reconstruction\nlearning and a balanced multi-task learning strategy based on gradient conflict\nsuppression. By streamlining the training process, our framework eliminates the\nneed for manually designing and tuning complex multi-stage training pipelines,\nsubstantially reducing training time and improving performance. We evaluate\nTurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and\ndemonstrate that it further improves the performance of state-of-the-art\nmulti-agent perception and prediction models. Our results highlight that\npretraining effectively captures spatiotemporal multi-agent features and\nsignificantly benefits downstream tasks. Moreover, the proposed balanced\nmulti-task learning strategy enhances detection and prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04682v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04702", "title": "BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning", "authors": ["Ziyang Leng", "Jiawei Yang", "Zhicheng Ren", "Bolei Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04702v1", "summary": "We present BEVCon, a simple yet effective contrastive learning framework\ndesigned to improve Bird's Eye View (BEV) perception in autonomous driving. BEV\nperception offers a top-down-view representation of the surrounding\nenvironment, making it crucial for 3D object detection, segmentation, and\ntrajectory prediction tasks. While prior work has primarily focused on\nenhancing BEV encoders and task-specific heads, we address the underexplored\npotential of representation learning in BEV models. BEVCon introduces two\ncontrastive learning modules: an instance feature contrast module for refining\nBEV features and a perspective view contrast module that enhances the image\nbackbone. The dense contrastive learning designed on top of detection losses\nleads to improved feature representations across both the BEV encoder and the\nbackbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon\nachieves consistent performance gains, achieving up to +2.4% mAP improvement\nover state-of-the-art baselines. Our results highlight the critical role of\nrepresentation learning in BEV perception and offer a complementary avenue to\nconventional task-specific optimizations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04702v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04705", "title": "Occupancy Learning with Spatiotemporal Memory", "authors": ["Ziyang Leng", "Jiawei Yang", "Wenlong Yi", "Bolei Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04705v1", "summary": "3D occupancy becomes a promising perception representation for autonomous\ndriving to model the surrounding environment at a fine-grained scale. However,\nit remains challenging to efficiently aggregate 3D occupancy over time across\nmultiple input frames due to the high processing cost and the uncertainty and\ndynamics of voxels. To address this issue, we propose ST-Occ, a scene-level\noccupancy representation learning framework that effectively learns the\nspatiotemporal feature with temporal consistency. ST-Occ consists of two core\ndesigns: a spatiotemporal memory that captures comprehensive historical\ninformation and stores it efficiently through a scene-level representation and\na memory attention that conditions the current occupancy representation on the\nspatiotemporal memory with a model of uncertainty and dynamic awareness. Our\nmethod significantly enhances the spatiotemporal representation learned for 3D\noccupancy prediction tasks by exploiting the temporal dependency between\nmulti-frame inputs. Experiments show that our approach outperforms the\nstate-of-the-art methods by a margin of 3 mIoU and reduces the temporal\ninconsistency by 29%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04705v1", "cate": "cs.CV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03713", "title": "Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention", "authors": ["Minsuk Chang", "Yao Wang", "Huichen Will Wang", "Yuanhong Zhou", "Andreas Bulling", "Cindy Xiong Bearfield"], "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03713", "summary": "Accounting for individual differences can improve the effectiveness of\nvisualization design. While the role of visual attention in visualization\ninterpretation is well recognized, existing work often overlooks how this\nbehavior varies based on visual literacy levels. Based on data from a\n235-participant user study covering three visualization tests (mini-VLAT,\nCALVI, and SGL), we show that distinct attention patterns in visual data\nexploration can correlate with participants' literacy levels: While experts\n(high-scorers) generally show a strong attentional focus, novices (low-scorers)\nfocus less and explore more. We then propose two computational models\nleveraging these insights: Lit2Sal -- a novel visual saliency model that\npredicts observer attention given their visualization literacy level, and\nSal2Lit -- a model to predict visual literacy from human visual attention data.\nOur quantitative and qualitative evaluation demonstrates that Lit2Sal\noutperforms state-of-the-art saliency models with literacy-aware\nconsiderations. Sal2Lit predicts literacy with 86% accuracy using a single\nattention map, providing a time-efficient supplement to literacy assessment\nthat only takes less than a minute. Taken together, our unique approach to\nconsider individual differences in salience models and visual attention in\nliteracy assessments paves the way for new directions in personalized visual\ndata communication to enhance understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03713", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22", "section": "cross"}
{"id": "2508.03723", "title": "Technical specification of a framework for the collection of clinical images and data", "authors": ["Alistair Mackenzie", "Mark Halling-Brown", "Ruben van Engen", "Carlijn Roozemond", "Lucy Warren", "Dominic Ward", "Nadia Smith"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03723", "summary": "In this report a framework for the collection of clinical images and data for\nuse when training and validating artificial intelligence (AI) tools is\ndescribed. The report contains not only information about the collection of the\nimages and clinical data, but the ethics and information governance processes\nto consider ensuring the data is collected safely, and the infrastructure and\nagreements required to allow for the sharing of data with other groups.\n  A key characteristic of the main collection framework described here is that\nit can enable automated and ongoing collection of datasets to ensure that the\ndata is up-to-date and representative of current practice. This is important in\nthe context of training and validating AI tools as it is vital that datasets\nhave a mix of older cases with long term follow-up such that the clinical\noutcome is as accurate as possible, and current data. Validations run on old\ndata will provide findings and conclusions relative to the status of the\nimaging units when that data was generated. It is important that a validation\ndataset can assess the AI tools with data that it would see if deployed and\nactive now.\n  Other types of collection frameworks, which do not follow a fully automated\napproach, are also described. Whilst the fully automated method is recommended\nfor large scale, long-term image collection, there may be reasons to start data\ncollection using semi-automated methods and indications of how to do that are\nprovided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03723", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29", "section": "cross"}
{"id": "2508.03753", "title": "Classification non supervis{Ã©}es d'acquisitions hyperspectrales cod{Ã©}es : quelles v{Ã©}rit{Ã©}s terrain ?", "authors": ["Trung-tin Dinh", "HervÃ© Carfantan", "Antoine Monmayrant", "Simon Lacroix"], "categories": ["cs.CV", "eess.IV", "physics.data-an"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03753", "summary": "We propose an unsupervised classification method using a limited number of\ncoded acquisitions from a DD-CASSI hyperspectral imager. Based on a simple\nmodel of intra-class spectral variability, this approach allow to identify\nclasses and estimate reference spectra, despite data compression by a factor of\nten. Here, we highlight the limitations of the ground truths commonly used to\nevaluate this type of method: lack of a clear definition of the notion of\nclass, high intra-class variability, and even classification errors. Using the\nPavia University scene, we show that with simple assumptions, it is possible to\ndetect regions that are spectrally more coherent, highlighting the need to\nrethink the evaluation of classification methods, particularly in unsupervised\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03753", "cate": "eess.IV", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03755", "title": "LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion", "authors": ["Wenwu Gong", "Lili Yang"], "categories": ["cs.CV", "cs.LG", "math.NA"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03755", "summary": "Multi-dimensional data completion is a critical problem in computational\nsciences, particularly in domains such as computer vision, signal processing,\nand scientific computing. Existing methods typically leverage either global\nlow-rank approximations or local smoothness regularization, but each suffers\nfrom notable limitations: low-rank methods are computationally expensive and\nmay disrupt intrinsic data structures, while smoothness-based approaches often\nrequire extensive manual parameter tuning and exhibit poor generalization. In\nthis paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)\nmodel that unifies global and local prior modeling within a Tucker\ndecomposition. Specifically, LRTuckerRep encodes low rankness through a\nself-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker\ncore, while capturing smoothness via a parameter-free Laplacian-based\nregularization on the factor spaces. To efficiently solve the resulting\nnonconvex optimization problem, we develop two iterative algorithms with\nprovable convergence guarantees. Extensive experiments on multi-dimensional\nimage inpainting and traffic data imputation demonstrate that LRTuckerRep\nachieves superior completion accuracy and robustness under high missing rates\ncompared to baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03755", "cate": "cs.LG", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03758", "title": "FUTransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation", "authors": ["Akwasi Asare", "Mary Sagoe", "Justice Williams Asare"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03758", "summary": "Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role\nin clinical diagnosis, therapeutic planning, and longitudinal wound monitoring.\nHowever, this task remains challenging due to the heterogeneous appearance,\nirregular morphology, and complex backgrounds associated with ulcer regions in\nclinical photographs. Traditional convolutional neural networks (CNNs), such as\nU-Net, provide strong localization capabilities but struggle to model\nlong-range spatial dependencies due to their inherently limited receptive\nfields. To address this, we propose FUTransUNet, a hybrid architecture that\nintegrates the global attention mechanism of Vision Transformers (ViTs) into\nthe U-Net framework. This combination allows the model to extract global\ncontextual features while maintaining fine-grained spatial resolution through\nskip connections and an effective decoding pathway. We trained and validated\nFUTransUNet on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset.\nFUTransUNet achieved a training Dice Coefficient of 0.8679, an IoU of 0.7672,\nand a training loss of 0.0053. On the validation set, the model achieved a Dice\nCoefficient of 0.8751, an IoU of 0.7780, and a validation loss of 0.009045. To\nensure clinical transparency, we employed Grad-CAM visualizations, which\nhighlighted model focus areas during prediction. These quantitative outcomes\nclearly demonstrate that our hybrid approach successfully integrates global and\nlocal feature extraction paradigms, thereby offering a highly robust, accurate,\nexplainable, and interpretable solution and clinically translatable solution\nfor automated foot ulcer analysis. The approach offers a reliable,\nhigh-fidelity solution for DFU segmentation, with implications for improving\nreal-world wound assessment and patient care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03758", "cate": "eess.IV", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03759", "title": "Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy", "authors": ["Tatwadarshi P. Nagarhalli", "Shruti S. Pawar", "Soham A. Dahanukar", "Uday Aswalekar", "Ashwini M. Save", "Sanket D. Patil"], "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.QM"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03759", "summary": "Accurately classifying white blood cells from microscopic images is essential\nto identify several illnesses and conditions in medical diagnostics. Many deep\nlearning technologies are being employed to quickly and automatically classify\nimages. However, most of the time, the resolution of these microscopic pictures\nis quite low, which might make it difficult to classify them correctly. Some\npicture improvement techniques, such as image super-resolution, are being\nutilized to improve the resolution of the photos to get around this issue. The\nsuggested study uses large image dimension upscaling to investigate how\npicture-enhancing approaches affect classification performance. The study\nspecifically looks at how deep learning models may be able to understand more\ncomplex visual information by capturing subtler morphological changes when\nimage resolution is increased using cutting-edge techniques. The model may\nlearn from standard and augmented data since the improved images are\nincorporated into the training process. This dual method seeks to comprehend\nthe impact of image resolution on model performance and enhance classification\naccuracy. A well-known model for picture categorization is used to conduct\nextensive testing and thoroughly evaluate the effectiveness of this approach.\nThis research intends to create more efficient image identification algorithms\ncustomized to a particular dataset of white blood cells by understanding the\ntrade-offs between ordinary and enhanced images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03759", "cate": "eess.IV", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03762", "title": "Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Population-Based Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)", "authors": ["Anindo Saha", "Joeran S. Bosma", "Jasper J. Twilt", "Alexander B.C.D. Ng", "Aqua Asif", "Kirti Magudia", "Peder Larson", "Qinglin Xie", "Xiaodong Zhang", "Chi Pham Minh", "Samuel N. Gitau", "Ivo G. Schoots", "Martijn F. Boomsma", "Renato Cuocolo", "Nikolaos Papanikolaou", "Daniele Regge", "Derya Yakar", "Mattijs Elschot", "Jeroen Veltman", "Baris Turkbey", "Nancy A. Obuchowski", "Jurgen J. FÃ¼tterer", "Anwar R. Padhani", "Hashim U. Ahmed", "Tobias NordstrÃ¶m", "Martin Eklund", "Veeru Kasivisvanathan", "Maarten de Rooij", "Henkjan Huisman"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03762", "summary": "In this intercontinental, confirmatory study, we include a retrospective\ncohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries)\nto train and externally validate the PI-CAI-2B model, i.e., an efficient,\nnext-generation iteration of the state-of-the-art AI system that was developed\nfor detecting Gleason grade group $\\geq$2 prostate cancer on MRI during the\nPI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities\nin 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12\nindependent centers based in Europe, North America, Asia and Africa, are used\nfor training and internal testing. Additionally, 2010 cases (2010 patients; 20\nexternal cities in 12 countries) from population-based screening (STHLM3-MRI,\nIP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in\nEurope, North and South Americas, Asia and Australia, are used for external\ntesting. Primary endpoint is the proportion of AI-based assessments in\nagreement with the standard of care diagnoses (i.e., clinical assessments made\nby expert uropathologists on histopathology, if available, or at least two\nexpert urogenital radiologists in consensus; with access to patient history and\npeer consultation) in the detection of Gleason grade group $\\geq$2 prostate\ncancer within the external testing cohorts. Our statistical analysis plan is\nprespecified with a hypothesis of diagnostic interchangeability to the standard\nof care at the PI-RADS $\\geq$3 (primary diagnosis) or $\\geq$4 (screening)\ncut-off, considering an absolute margin of 0.05 and reader estimates derived\nfrom the PI-CAI observer study (62 radiologists reading 400 cases). Secondary\nmeasures comprise the area under the receiver operating characteristic curve\n(AUROC) of the AI system stratified by imaging quality, patient age and patient\nethnicity to identify underlying biases (if any).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03762", "cate": "eess.IV", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03960", "title": "Fast Magnetic Resonance Simulation Using Combined Update with Grouped Isochromats", "authors": ["Hidenori Takeshima"], "categories": ["cs.CV", "eess.IV", "physics.med-ph"], "primary_category": "physics.med-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03960", "summary": "This work aims to overcome an assumption of conventional MR simulators:\nIndividual isochromats should be simulated individually. To reduce the\ncomputational times of MR simulation, a new simulation method using grouped\nisochromats is proposed. When multiple isochromats are grouped before\nsimulations, some parts of the simulation can be shared in each group. For a\ncertain gradient type, the isochromats in the group can be easily chosen for\nensuring that they behave the same. For example, the group can be defined as\nthe isochromats whose locations along x-axis, T1, T2 and magnetic field\ninhomogeneity values are the same values. In such groups, simulations can be\ncombined when a pulse sequence with the magnetic field gradient along x-axis\nonly are processed. The processing times of the conventional and proposed\nmethods were evaluated with several sequences including fast spin echo (FSE)\nand echo-planar imaging (EPI) sequences. The simulation times of the proposed\nmethod were 3 to 72 times faster than those of the conventional methods. In the\ncases of 27.5 million isochromats using single instruction multiple data (SIMD)\ninstructions and multi-threading, the conventional method simulated FSE and EPI\nsequences in 208.4 and 66.4 seconds, respectively. In the same cases, the\nproposed method simulated these sequences in 38.1 and 7.1 seconds,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03960", "cate": "physics.med-ph", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03982", "title": "UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation", "authors": ["Jinwei Zhang", "Lianrui Zuo", "Blake E. Dewey", "Samuel W. Remedios", "Yihao Liu", "Savannah P. Hays", "Dzung L. Pham", "Ellen M. Mowry", "Scott D. Newsome", "Peter A. Calabresi", "Aaron Carass", "Jerry L. Prince"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03982", "summary": "Automated segmentation of multiple sclerosis (MS) lesions using multicontrast\nmagnetic resonance (MR) images improves efficiency and reproducibility compared\nto manual delineation, with deep learning (DL) methods achieving\nstate-of-the-art performance. However, these DL-based methods have yet to\nsimultaneously optimize in-domain accuracy and out-of-domain generalization\nwhen trained on a single source with limited data, or their performance has\nbeen unsatisfactory. To fill this gap, we propose a method called UNISELF,\nwhich achieves high accuracy within a single training domain while\ndemonstrating strong generalizability across multiple out-of-domain test\ndatasets. UNISELF employs a novel test-time self-ensembled lesion fusion to\nimprove segmentation accuracy, and leverages test-time instance normalization\n(TTIN) of latent features to address domain shifts and missing input contrasts.\nTrained on the ISBI 2015 longitudinal MS segmentation challenge training\ndataset, UNISELF ranks among the best-performing methods on the challenge test\ndataset. Additionally, UNISELF outperforms all benchmark methods trained on the\nsame ISBI training data across diverse out-of-domain test datasets with domain\nshifts and missing contrasts, including the public MICCAI 2016 and UMCL\ndatasets, as well as a private multisite dataset. These test datasets exhibit\ndomain shifts and/or missing contrasts caused by variations in acquisition\nprotocols, scanner types, and imaging artifacts arising from imperfect\nacquisition. Our code is available at https://github.com/uponacceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03982", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04062", "title": "PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography", "authors": ["Yichi Zhang", "Wenbo Zhang", "Zehui Ling", "Gang Feng", "Sisi Peng", "Deshu Chen", "Yuchen Liu", "Hongwei Zhang", "Shuqi Wang", "Lanlan Li", "Limei Han", "Yuan Cheng", "Zixin Hu", "Yuan Qi", "Le Xue"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04062", "summary": "Positron emission tomography (PET) is a cornerstone of modern oncologic and\nneurologic imaging, distinguished by its unique ability to illuminate dynamic\nmetabolic processes that transcend the anatomical focus of traditional imaging\ntechnologies. Radiology reports are essential for clinical decision making, yet\ntheir manual creation is labor-intensive and time-consuming. Recent\nadvancements of vision-language models (VLMs) have shown strong potential in\nmedical applications, presenting a promising avenue for automating report\ngeneration. However, existing applications of VLMs in the medical domain have\npredominantly focused on structural imaging modalities, while the unique\ncharacteristics of molecular PET imaging have largely been overlooked. To\nbridge the gap, we introduce PET2Rep, a large-scale comprehensive benchmark for\nevaluation of general and medical VLMs for radiology report generation for PET\nimages. PET2Rep stands out as the first dedicated dataset for PET report\ngeneration with metabolic information, uniquely capturing whole-body\nimage-report pairs that cover dozens of organs to fill the critical gap in\nexisting benchmarks and mirror real-world clinical comprehensiveness. In\naddition to widely recognized natural language generation metrics, we introduce\na series of clinical efficiency metrics to evaluate the quality of radiotracer\nuptake pattern description in key organs in generated reports. We conduct a\nhead-to-head comparison of 30 cutting-edge general-purpose and\nmedical-specialized VLMs. The results show that the current state-of-the-art\nVLMs perform poorly on PET report generation task, falling considerably short\nof fulfilling practical needs. Moreover, we identify several key insufficiency\nthat need to be addressed to advance the development in medical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04062", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04078", "title": "RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting", "authors": ["Zhan Li", "Huangying Zhan", "Changyang Li", "Qingan Yan", "Yi Xu"], "categories": ["cs.CV", "cs.GR", "cs.LG"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04078", "summary": "Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive\nand expert-driven process, often resulting in inconsistent reconstructions and\nsuboptimal results. We propose RLGS, a plug-and-play reinforcement learning\nframework for adaptive hyperparameter tuning in 3DGS through lightweight policy\nmodules, dynamically adjusting critical hyperparameters such as learning rates\nand densification thresholds. The framework is model-agnostic and seamlessly\nintegrates into existing 3DGS pipelines without architectural modifications. We\ndemonstrate its generalization ability across multiple state-of-the-art 3DGS\nvariants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness\nacross diverse datasets. RLGS consistently enhances rendering quality. For\nexample, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)\ndataset, under a fixed Gaussian budget, and continues to yield gains even when\nbaseline performance saturates. Our results suggest that RLGS provides an\neffective and general solution for automating hyperparameter tuning in 3DGS\ntraining, bridging a gap in applying reinforcement learning to 3DGS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04078", "cate": "cs.GR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04160", "title": "DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment", "authors": ["Angela Locoro", "Silvia Golia", "Davide Falessi"], "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04160", "summary": "The underspecification of progressive levels of difficulty in measurement\nconstructs design and assessment tests for data visualization literacy may\nhinder the expressivity of measurements in both test design and test reuse. To\nmitigate this problem, this paper proposes DRIVE-T (Discriminating and\nRepresentative Items for Validating Expressive Tests), a methodology designed\nto drive the construction and evaluation of assessment items. Given a data\nvizualization, DRIVE-T supports the identification of task-based items\ndiscriminability and representativeness for measuring levels of data\nvisualization literacy. DRIVE-T consists of three steps: (1) tagging task-based\nitems associated with a set of data vizualizations; (2) rating them by\nindependent raters for their difficulty; (3) analysing raters' raw scores\nthrough a Many-Facet Rasch Measurement model. In this way, we can observe the\nemergence of difficulty levels of the measurement construct, derived from the\ndiscriminability and representativeness of task-based items for each data\nvizualization, ordered into Many-Facets construct levels. In this study, we\nshow and apply each step of the methodology to an item bank, which models the\ndifficulty levels of a measurement construct approximating a latent construct\nfor data visualization literacy. This measurement construct is drawn from\nsemiotics, i.e., based on the syntax, semantics and pragmatics knowledge that\neach data visualization may require to be mastered by people. The DRIVE-T\nmethodology operationalises an inductive approach, observable in a post-design\nphase of the items preparation, for formative-style and practice-based\nmeasurement construct emergence. A pilot study with items selected through the\napplication of DRIVE-T is also presented to test our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04160", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04270", "title": "TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling", "authors": ["Deming Zhou", "Yuetong Fang", "Zhaorui Wang", "Renjing Xu"], "categories": ["cs.CV", "cs.NE"], "primary_category": "cs.NE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04270", "summary": "The primate visual cortex exhibits topographic organization, where\nfunctionally similar neurons are spatially clustered, a structure widely\nbelieved to enhance neural processing efficiency. While prior works have\ndemonstrated that conventional deep ANNs can develop topographic\nrepresentations, these models largely neglect crucial temporal dynamics. This\noversight often leads to significant performance degradation in tasks like\nobject recognition and compromises their biological fidelity. To address this,\nwe leverage spiking neural networks (SNNs), which inherently capture\nspike-based temporal dynamics and offer enhanced biological plausibility. We\npropose a novel Spatio-Temporal Constraints (STC) loss function for topographic\ndeep spiking neural networks (TDSNNs), successfully replicating the\nhierarchical spatial functional organization observed in the primate visual\ncortex from low-level sensory input to high-level abstract representations. Our\nresults show that STC effectively generates representative topographic features\nacross simulated visual cortical areas. While introducing topography typically\nleads to significant performance degradation in ANNs, our spiking architecture\nexhibits a remarkably small performance drop (No drop in ImageNet top-1\naccuracy, compared to a 3\\% drop observed in TopoNet, which is the\nbest-performing topographic ANN so far) and outperforms topographic ANNs in\nbrain-likeness. We also reveal that topographic organization facilitates\nefficient and stable temporal information processing via the spike mechanism in\nTDSNNs, contributing to model robustness. These findings suggest that TDSNNs\noffer a compelling balance between computational performance and brain-like\nfeatures, providing not only a framework for interpreting neural science\nphenomena but also novel insights for designing more efficient and robust deep\nlearning models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04270", "cate": "cs.NE", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04368", "title": "Continual Multiple Instance Learning for Hematologic Disease Diagnosis", "authors": ["Zahra Ebrahimi", "Raheleh Salehi", "Nassir Navab", "Carsten Marr", "Ario Sadafi"], "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.QM"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04368", "summary": "The dynamic environment of laboratories and clinics, with streams of data\narriving on a daily basis, requires regular updates of trained machine learning\nmodels for consistent performance. Continual learning is supposed to help train\nmodels without catastrophic forgetting. However, state-of-the-art methods are\nineffective for multiple instance learning (MIL), which is often used in\nsingle-cell-based hematologic disease diagnosis (e.g., leukemia detection).\nHere, we propose the first continual learning method tailored specifically to\nMIL. Our method is rehearsal-based over a selection of single instances from\nvarious bags. We use a combination of the instance attention score and distance\nfrom the bag mean and class mean vectors to carefully select which samples and\ninstances to store in exemplary sets from previous tasks, preserving the\ndiversity of the data. Using the real-world input of one month of data from a\nleukemia laboratory, we study the effectiveness of our approach in a class\nincremental scenario, comparing it to well-known continual learning methods. We\nshow that our method considerably outperforms state-of-the-art methods,\nproviding the first continual learning approach for MIL. This enables the\nadaptation of models to shifting data distributions over time, such as those\ncaused by changes in disease occurrence or underlying genetic alterations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04368", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04429", "title": "Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis", "authors": ["Ethan Dack", "Lorenzo Brigato", "Vasilis Dedousis", "Janine Gote-Schniering", "Cheryl", "Hanno Hoppe", "Aristomenis Exadaktylos", "Manuela Funke-Chambour", "Thomas Geiser", "Andreas Christe", "Lukas Ebner", "Stavroula Mougiakakou"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04429", "summary": "Masked autoencoders (MAEs) have emerged as a powerful approach for\npre-training on unlabelled data, capable of learning robust and informative\nfeature representations. This is particularly advantageous in diffused lung\ndisease research, where annotated imaging datasets are scarce. To leverage\nthis, we train an MAE on a curated collection of over 5,000 chest computed\ntomography (CT) scans, combining in-house data with publicly available scans\nfrom related conditions that exhibit similar radiological patterns, such as\nCOVID-19 and bacterial pneumonia. The pretrained MAE is then fine-tuned on a\ndownstream classification task for diffused lung disease diagnosis. Our\nfindings demonstrate that MAEs can effectively extract clinically meaningful\nfeatures and improve diagnostic performance, even in the absence of large-scale\nlabelled datasets. The code and the models are available here:\nhttps://github.com/eedack01/lung_masked_autoencoder.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04429", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04450", "title": "TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration", "authors": ["Xuan Loc Pham", "Gwendolyn Vuurberg", "Marjan Doppen", "Joey Roosen", "Tip Stille", "Thi Quynh Ha", "Thuy Duong Quach", "Quoc Vu Dang", "Manh Ha Luu", "Ewoud J. Smit", "Hong Son Mai", "Mattias Heinrich", "Bram van Ginneken", "Mathias Prokop", "Alessa Hering"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04450", "summary": "Image registration is a fundamental technique in the analysis of longitudinal\nand multi-phase CT images within clinical practice. However, most existing\nmethods are tailored for single-organ applications, limiting their\ngeneralizability to other anatomical regions. This work presents\nTotalRegistrator, an image registration framework capable of aligning multiple\nanatomical regions simultaneously using a standard UNet architecture and a\nnovel field decomposition strategy. The model is lightweight, requiring only\n11GB of GPU memory for training. To train and evaluate our method, we\nconstructed a large-scale longitudinal dataset comprising 695 whole-body\n(thorax-abdomen-pelvic) paired CT scans from individual patients acquired at\ndifferent time points. We benchmarked TotalRegistrator against a generic\nclassical iterative algorithm and a recent foundation model for image\nregistration. To further assess robustness and generalizability, we evaluated\nour model on three external datasets: the public thoracic and abdominal\ndatasets from the Learn2Reg challenge, and a private multiphase abdominal\ndataset from a collaborating hospital. Experimental results on the in-house\ndataset show that the proposed approach generally surpasses baseline methods in\nmulti-organ abdominal registration, with a slight drop in lung alignment\nperformance. On out-of-distribution datasets, it achieved competitive results\ncompared to leading single-organ models, despite not being fine-tuned for those\ntasks, demonstrating strong generalizability. The source code will be publicly\navailable at: https://github.com/DIAGNijmegen/oncology_image_registration.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04450", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04491", "title": "OpenDCVCs: A PyTorch Open Source Implementation and Performance Evaluation of the DCVC series Video Codecs", "authors": ["Yichi Zhang", "Fengqing Zhu"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04491", "summary": "We present OpenDCVCs, an open-source PyTorch implementation designed to\nadvance reproducible research in learned video compression. OpenDCVCs provides\nunified and training-ready implementations of four representative Deep\nContextual Video Compression (DCVC) models--DCVC, DCVC with Temporal Context\nModeling (DCVC-TCM), DCVC with Hybrid Entropy Modeling (DCVC-HEM), and DCVC\nwith Diverse Contexts (DCVC-DC). While the DCVC series achieves substantial\nbitrate reductions over both classical codecs and advanced learned models,\nprevious public code releases have been limited to evaluation codes, presenting\nsignificant barriers to reproducibility, benchmarking, and further development.\nOpenDCVCs bridges this gap by offering a comprehensive, self-contained\nframework that supports both end-to-end training and evaluation for all\nincluded algorithms. The implementation includes detailed documentation,\nevaluation protocols, and extensive benchmarking results across diverse\ndatasets, providing a transparent and consistent foundation for comparison and\nextension. All code and experimental tools are publicly available at\nhttps://gitlab.com/viper-purdue/opendcvcs, empowering the community to\naccelerate research and foster collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04491", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04508", "title": "Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds", "authors": ["Haodong Zhu", "Changbai Li", "Yangyang Ren", "Zichao Feng", "Xuhui Liu", "Hanlin Chen", "Xiantong Zhen", "Baochang Zhang"], "categories": ["cs.CV", "cs.GR"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04508", "summary": "Current multi-view 3D reconstruction methods rely on accurate camera\ncalibration and pose estimation, requiring complex and time-intensive\npre-processing that hinders their practical deployment. To address this\nchallenge, we introduce Surf3R, an end-to-end feedforward approach that\nreconstructs 3D surfaces from sparse views without estimating camera poses and\ncompletes an entire scene in under 10 seconds. Our method employs a\nmulti-branch and multi-view decoding architecture in which multiple reference\nviews jointly guide the reconstruction process. Through the proposed\nbranch-wise processing, cross-view attention, and inter-branch fusion, the\nmodel effectively captures complementary geometric cues without requiring\ncamera calibration. Moreover, we introduce a D-Normal regularizer based on an\nexplicit 3D Gaussian representation for surface reconstruction. It couples\nsurface normals with other geometric parameters to jointly optimize the 3D\ngeometry, significantly improving 3D consistency and surface detail accuracy.\nExperimental results demonstrate that Surf3R achieves state-of-the-art\nperformance on multiple surface reconstruction metrics on ScanNet++ and Replica\ndatasets, exhibiting excellent generalization and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04508", "cate": "cs.GR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04522", "title": "Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation", "authors": ["Johannes Tischer", "Patric Kienast", "Marlene StÃ¼mpflen", "Gregor Kasprian", "Georg Langs", "Roxane Licandro"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04522", "summary": "Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for\nstudying brain development in vivo. Yet, its assessment remains challenging due\nto variability in brain maturation, imaging protocols, and uncertain estimates\nof Gestational Age (GA). To overcome these, brain atlases provide a\nstandardized reference framework that facilitates objective evaluation and\ncomparison across subjects by aligning the atlas and subjects in a common\ncoordinate system. In this work, we introduce a novel deep-learning framework\nfor generating continuous, age-specific fetal brain atlases for real-time fetal\nbrain tissue segmentation. The framework combines a direct registration model\nwith a conditional discriminator. Trained on a curated dataset of 219\nneurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method\nachieves high registration accuracy, captures dynamic anatomical changes with\nsharp structural detail, and robust segmentation performance with an average\nDice Similarity Coefficient (DSC) of 86.3% across six brain tissues.\nFurthermore, volumetric analysis of the generated atlases reveals detailed\nneurotypical growth trajectories, providing valuable insights into the\nmaturation of the fetal brain. This approach enables individualized\ndevelopmental assessment with minimal pre-processing and real-time performance,\nsupporting both research and clinical applications. The model code is available\nat https://github.com/cirmuw/fetal-brain-atlas", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04522", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04553", "title": "LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation", "authors": ["Franz Thaler", "Darko Stern", "Gernot Plank", "Martin Urschler"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04553", "summary": "Atrial fibrillation (AF) represents the most prevalent type of cardiac\narrhythmia for which treatment may require patients to undergo ablation\ntherapy. In this surgery cardiac tissues are locally scarred on purpose to\nprevent electrical signals from causing arrhythmia. Patient-specific cardiac\ndigital twin models show great potential for personalized ablation therapy,\nhowever, they demand accurate semantic segmentation of healthy and scarred\ntissue typically obtained from late gadolinium enhanced (LGE) magnetic\nresonance (MR) scans. In this work we propose the Left Atrial Cascading\nRefinement CNN (LA-CaRe-CNN), which aims to accurately segment the left atrium\nas well as left atrial scar tissue from LGE MR scans. LA-CaRe-CNN is a 2-stage\nCNN cascade that is trained end-to-end in 3D, where Stage 1 generates a\nprediction for the left atrium, which is then refined in Stage 2 in conjunction\nwith the original image information to obtain a prediction for the left atrial\nscar tissue. To account for domain shift towards domains unknown during\ntraining, we employ strong intensity and spatial augmentation to increase the\ndiversity of the training dataset. Our proposed method based on a 5-fold\nensemble achieves great segmentation results, namely, 89.21% DSC and 1.6969 mm\nASSD for the left atrium, as well as 64.59% DSC and 91.80% G-DSC for the more\nchallenging left atrial scar tissue. Thus, segmentations obtained through\nLA-CaRe-CNN show great potential for the generation of patient-specific cardiac\ndigital twin models and downstream tasks like personalized targeted ablation\ntherapy to treat AF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04553", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04556", "title": "CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps", "authors": ["Filipe B. Teixeira", "Carolina SimÃµes", "Paulo Fidalgo", "Wagner Pedrosa", "AndrÃ© Coelho", "Manuel Ricardo", "Luis M. Pessoa"], "categories": ["cs.CV", "cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04556", "summary": "Telecommunications and computer vision have evolved independently. With the\nemergence of high-frequency wireless links operating mostly in line-of-sight,\nvisual data can help predict the channel dynamics by detecting obstacles and\nhelp overcoming them through beamforming or handover techniques.\n  This paper proposes a novel architecture for delivering real-time radio and\nvideo sensing information to O-RAN xApps through a multi-agent approach, and\nintroduces a new video function capable of generating blockage information for\nxApps, enabling Integrated Sensing and Communications. Experimental results\nshow that the delay of sensing information remains under 1\\,ms and that an xApp\ncan successfully use radio and video sensing information to control the 5G/6G\nRAN in real-time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04556", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04642", "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case", "authors": ["Baihui Xiao", "Chengjian Feng", "Zhijian Huang", "Feng yan", "Yujie Zhong", "Lin Ma"], "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04642", "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04642", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04648", "title": "Super Resolved Imaging with Adaptive Optics", "authors": ["Robin Swanson", "Esther Y. H. Lin", "Masen Lamb", "Suresh Sivanandam", "Kiriakos N. Kutulakos"], "categories": ["astro-ph.IM", "cs.CV"], "primary_category": "astro-ph.IM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04648", "summary": "Astronomical telescopes suffer from a tradeoff between field of view (FoV)\nand image resolution: increasing the FoV leads to an optical field that is\nunder-sampled by the science camera. This work presents a novel computational\nimaging approach to overcome this tradeoff by leveraging the existing adaptive\noptics (AO) systems in modern ground-based telescopes. Our key idea is to use\nthe AO system's deformable mirror to apply a series of learned, precisely\ncontrolled distortions to the optical wavefront, producing a sequence of images\nthat exhibit distinct, high-frequency, sub-pixel shifts. These images can then\nbe jointly upsampled to yield the final super-resolved image. Crucially, we\nshow this can be done while simultaneously maintaining the core AO\noperation--correcting for the unknown and rapidly changing wavefront\ndistortions caused by Earth's atmosphere. To achieve this, we incorporate\nend-to-end optimization of both the induced mirror distortions and the\nupsampling algorithm, such that telescope-specific optics and temporal\nstatistics of atmospheric wavefront distortions are accounted for. Our\nexperimental results with a hardware prototype, as well as simulations,\ndemonstrate significant SNR improvements of up to 12 dB over non-AO\nsuper-resolution baselines, using only existing telescope optics and no\nhardware modifications. Moreover, by using a precise bench-top replica of a\ncomplete telescope and AO system, we show that our methodology can be readily\ntransferred to an operational telescope. Project webpage:\nhttps://www.cs.toronto.edu/~robin/aosr/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04648", "cate": "astro-ph.IM", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04687", "title": "MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics", "authors": ["Ye Pan", "Ruisi Zhang", "Jingying Wang", "Nengfu Chen", "Yilin Qiu", "Yu Ding", "Kenny Mitchell"], "categories": ["cs.CV", "cs.GR"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04687", "summary": "Our purpose is to improve performance-based animation which can drive\nbelievable 3D stylized characters that are truly perceptual. By combining\ntraditional blendshape animation techniques with multiple machine learning\nmodels, we present both non-real time and real time solutions which drive\ncharacter expressions in a geometrically consistent and perceptually valid way.\nFor the non-real time system, we propose a 3D emotion transfer network makes\nuse of a 2D human image to generate a stylized 3D rig parameters. For the real\ntime system, we propose a blendshape adaption network which generates the\ncharacter rig parameter motions with geometric consistency and temporally\nstability. We demonstrate the effectiveness of our system by comparing to a\ncommercial product Faceware. Results reveal that ratings of the recognition,\nintensity, and attractiveness of expressions depicted for animated characters\nvia our systems are statistically higher than Faceware. Our results may be\nimplemented into the animation pipeline, and provide animators with a system\nfor creating the expressions they wish to use more quickly and accurately.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04687", "cate": "cs.GR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2404.06798", "title": "Uncertainty-aware Medical Diagnostic Phrase Identification and Grounding", "authors": ["Ke Zou", "Yang Bai", "Bo Liu", "Yidi Chen", "Zhihao Chen", "Yang Zhou", "Xuedong Yuan", "Meng Wang", "Xiaojing Shen", "Xiaochun Cao", "Yih Chung Tham", "Huazhu Fu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.06798", "summary": "Medical phrase grounding is crucial for identifying relevant regions in\nmedical images based on phrase queries, facilitating accurate image analysis\nand diagnosis. However, current methods rely on manual extraction of key\nphrases from medical reports, reducing efficiency and increasing the workload\nfor clinicians. Additionally, the lack of model confidence estimation limits\nclinical trust and usability. In this paper, we introduce a novel task called\nMedical Report Grounding (MRG), which aims to directly identify diagnostic\nphrases and their corresponding grounding boxes from medical reports in an\nend-to-end manner. To address this challenge, we propose uMedGround, a robust\nand reliable framework that leverages a multimodal large language model to\npredict diagnostic phrases by embedding a unique token, <BOX>, into the\nvocabulary to enhance detection capabilities. A vision encoder-decoder\nprocesses the embedded token and input image to generate grounding boxes.\nCritically, uMedGround incorporates an uncertainty-aware prediction model,\nsignificantly improving the robustness and reliability of grounding\npredictions. Experimental results demonstrate that uMedGround outperforms\nstate-of-the-art medical phrase grounding methods and fine-tuned large\nvisual-language models, validating its effectiveness and reliability. This\nstudy represents a pioneering exploration of the MRG task, marking the\nfirst-ever endeavor in this domain. Additionally, we demonstrate the\napplicability of uMedGround in medical visual question answering and\nclass-based localization tasks, where it highlights visual evidence aligned\nwith key diagnostic phrases, supporting clinicians in interpreting various\ntypes of textual inputs, including free-text reports, visual question answering\nqueries, and class labels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.06798", "cate": "cs.CV", "date": "2024-04-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2405.05164", "title": "ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with Probability Map Guided Multi-Format Feature Fusion", "authors": ["Bing Zhu", "Zixin He", "Weiyi Xiong", "Guanhua Ding", "Tao Huang", "Wei Xiang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.05164", "summary": "Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.05164", "cate": "cs.CV", "date": "2024-05-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2408.11030", "title": "OpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene Understanding", "authors": ["Youjun Zhao", "Jiaying Lin", "Shuquan Ye", "Qianshi Pang", "Rynson W.H. Lau"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.11030", "summary": "Open-vocabulary 3D scene understanding (OV-3D) aims to localize and classify\nnovel objects beyond the closed set of object classes. However, existing\napproaches and benchmarks primarily focus on the open vocabulary problem within\nthe context of object classes, which is insufficient in providing a holistic\nevaluation to what extent a model understands the 3D scene. In this paper, we\nintroduce a more challenging task called Generalized Open-Vocabulary 3D Scene\nUnderstanding (GOV-3D) to explore the open vocabulary problem beyond object\nclasses. It encompasses an open and diverse set of generalized knowledge,\nexpressed as linguistic queries of fine-grained and object-specific attributes.\nTo this end, we contribute a new benchmark named \\textit{OpenScan}, which\nconsists of 3D object attributes across eight representative linguistic\naspects, including affordance, property, and material. We further evaluate\nstate-of-the-art OV-3D methods on our OpenScan benchmark and discover that\nthese methods struggle to comprehend the abstract vocabularies of the GOV-3D\ntask, a challenge that cannot be addressed simply by scaling up object classes\nduring training. We highlight the limitations of existing methodologies and\nexplore promising directions to overcome the identified shortcomings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.11030", "cate": "cs.CV", "date": "2024-08-20", "updated": "2025-08-06", "section": "repl"}
{"id": "2408.13491", "title": "ESA: Annotation-Efficient Active Learning for Semantic Segmentation", "authors": ["Jinchao Ge", "Zeyu Zhang", "Minh Hieu Phan", "Bowen Zhang", "Akide Liu", "Yang Zhao", "Shuwen Zhao"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.13491", "summary": "Active learning enhances annotation efficiency by selecting the most\nrevealing samples for labeling, thereby reducing reliance on extensive human\ninput. Previous methods in semantic segmentation have centered on individual\npixels or small areas, neglecting the rich patterns in natural images and the\npower of advanced pre-trained models. To address these challenges, we propose\nthree key contributions: Firstly, we introduce Entity-Superpixel Annotation\n(ESA), an innovative and efficient active learning strategy which utilizes a\nclass-agnostic mask proposal network coupled with super-pixel grouping to\ncapture local structural cues. Additionally, our method selects a subset of\nentities within each image of the target domain, prioritizing superpixels with\nhigh entropy to ensure comprehensive representation. Simultaneously, it focuses\non a limited number of key entities, thereby optimizing for efficiency. By\nutilizing an annotator-friendly design that capitalizes on the inherent\nstructure of images, our approach significantly outperforms existing\npixel-based methods, achieving superior results with minimal queries,\nspecifically reducing click cost by 98% and enhancing performance by 1.71%. For\ninstance, our technique requires a mere 40 clicks for annotation, a stark\ncontrast to the 5000 clicks demanded by conventional methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.13491", "cate": "cs.CV", "date": "2024-08-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2408.15098", "title": "CLIP-AGIQA: Boosting the Performance of AI-Generated Image Quality Assessment with CLIP", "authors": ["Zhenchen Tang", "Zichuan Wang", "Bo Peng", "Jing Dong"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.15098", "summary": "With the rapid development of generative technologies, AI-Generated Images\n(AIGIs) have been widely applied in various aspects of daily life. However, due\nto the immaturity of the technology, the quality of the generated images\nvaries, so it is important to develop quality assessment techniques for the\ngenerated images. Although some models have been proposed to assess the quality\nof generated images, they are inadequate when faced with the ever-increasing\nand diverse categories of generated images. Consequently, the development of\nmore advanced and effective models for evaluating the quality of generated\nimages is urgently needed. Recent research has explored the significant\npotential of the visual language model CLIP in image quality assessment,\nfinding that it performs well in evaluating the quality of natural images.\nHowever, its application to generated images has not been thoroughly\ninvestigated. In this paper, we build on this idea and further explore the\npotential of CLIP in evaluating the quality of generated images. We design\nCLIP-AGIQA, a CLIP-based regression model for quality assessment of generated\nimages, leveraging rich visual and textual knowledge encapsulated in CLIP.\nParticularly, we implement multi-category learnable prompts to fully utilize\nthe textual knowledge in CLIP for quality assessment. Extensive experiments on\nseveral generated image quality assessment benchmarks, including AGIQA-3K and\nAIGCIQA2023, demonstrate that CLIP-AGIQA outperforms existing IQA models,\nachieving excellent results in evaluating the quality of generated images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.15098", "cate": "cs.CV", "date": "2024-08-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.11506", "title": "Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution", "authors": ["Hongyu An", "Xinfeng Zhang", "Shijie Zhao", "Li Zhang", "Ruiqin Xiong"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.11506", "summary": "Omnidirectional videos (ODVs) provide an immersive visual experience by\ncapturing the 360{\\deg} scene. With the rapid advancements in virtual/augmented\nreality, metaverse, and generative artificial intelligence, the demand for\nhigh-quality ODVs is surging. However, ODVs often suffer from low resolution\ndue to their wide field of view and limitations in capturing devices and\ntransmission bandwidth. Although video super-resolution (SR) is a capable video\nquality enhancement technique, the performance ceiling and practical\ngeneralization of existing methods are limited when applied to ODVs due to\ntheir unique attributes. To alleviate spatial projection distortions and\ntemporal flickering of ODVs, we propose a Spatio-Temporal Distortion Aware\nNetwork (STDAN) with joint spatio-temporal alignment and reconstruction.\nSpecifically, we incorporate a spatio-temporal continuous alignment (STCA) to\nmitigate discrete geometric artifacts in parallel with temporal alignment.\nSubsequently, we introduce an interlaced multi-frame reconstruction (IMFR) to\nenhance temporal consistency. Furthermore, we employ latitude-saliency adaptive\n(LSA) weights to focus on regions with higher texture complexity and\nhuman-watching interest. By exploring a spatio-temporal jointly framework and\nreal-world viewing strategies, STDAN effectively reinforces spatio-temporal\ncoherence on a novel ODV-SR dataset and ensures affordable computational costs.\nExtensive experimental results demonstrate that STDAN outperforms\nstate-of-the-art methods in improving visual fidelity and dynamic smoothness of\nODVs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.11506", "cate": "cs.CV", "date": "2024-10-15", "updated": "2025-08-05", "section": "repl"}
{"id": "2410.18879", "title": "CapsoNet: A CNN-Transformer Ensemble for Multi-Class Abnormality Detection in Video Capsule Endoscopy", "authors": ["Arnav Samal", "Ranya Batsyas"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.18879", "summary": "We present CapsoNet, a deep learning framework developed for the Capsule\nVision 2024 Challenge, designed to perform multi-class abnormality\nclassification in video capsule endoscopy (VCE) frames. CapsoNet leverages an\nensemble of convolutional neural networks (CNNs) and transformer-based\narchitectures to capture both local and global visual features. The model was\ntrained and evaluated on a dataset of over 50,000 annotated frames spanning ten\nabnormality classes, sourced from three public and one private dataset. To\naddress the challenge of class imbalance, we employed focal loss, weighted\nrandom sampling, and extensive data augmentation strategies. All models were\nfully fine-tuned to maximize performance within the ensemble. CapsoNet achieved\na balanced accuracy of 86.34 percent and a mean AUC-ROC of 0.9908 on the\nofficial validation set, securing Team Seq2Cure 5th place in the competition.\nOur implementation is available at\nhttp://github.com/arnavs04/capsule-vision-2024", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.18879", "cate": "cs.CV", "date": "2024-10-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.19794", "title": "DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks for Image Analysis", "authors": ["Zohreh Aghababaeyan", "Manel Abdellatif", "Lionel Briand", "Ramesh S"], "categories": ["cs.CV", "cs.LG", "cs.SE"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.19794", "summary": "Deep Neural Networks (DNNs) are increasingly deployed across applications.\nHowever, ensuring their reliability remains a challenge, and in many\nsituations, alternative models with similar functionality and accuracy are\navailable. Traditional accuracy-based evaluations often fail to capture\nbehavioral differences between models, especially with limited test datasets,\nmaking it difficult to select or combine models effectively. Differential\ntesting addresses this by generating test inputs that expose discrepancies in\nDNN model behavior. However, existing approaches face significant limitations:\nmany rely on model internals or are constrained by available seed inputs. To\naddress these challenges, we propose DiffGAN, a black-box test image generation\napproach for differential testing of DNN models. DiffGAN leverages a Generative\nAdversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to\ngenerate diverse and valid triggering inputs that reveal behavioral\ndiscrepancies between models. DiffGAN employs two custom fitness functions,\nfocusing on diversity and divergence, to guide the exploration of the GAN input\nspace and identify discrepancies between models' outputs. By strategically\nsearching this space, DiffGAN generates inputs with specific features that\ntrigger differences in model behavior. DiffGAN is black-box, making it\napplicable in more situations. We evaluate DiffGAN on eight DNN model pairs\ntrained on widely used image datasets. Our results show DiffGAN significantly\noutperforms a SOTA baseline, generating four times more triggering inputs, with\ngreater diversity and validity, within the same budget. Additionally, the\ngenerated inputs improve the accuracy of a machine learning-based model\nselection mechanism, which selects the best-performing model based on input\ncharacteristics and can serve as a smart output voting mechanism when using\nalternative models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.19794", "cate": "cs.CV", "date": "2024-10-15", "updated": "2025-08-05", "section": "repl"}
{"id": "2410.19836", "title": "Upsampling DINOv2 features for unsupervised vision tasks and weakly supervised materials segmentation", "authors": ["Ronan Docherty", "Antonis Vamvakeros", "Samuel J. Cooper"], "categories": ["cond-mat.mtrl-sci", "cs.CV", "eess.IV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.19836", "summary": "The features of self-supervised vision transformers (ViTs) contain strong\nsemantic and positional information relevant to downstream tasks like object\nlocalization and segmentation. Recent works combine these features with\ntraditional methods like clustering, graph partitioning or region correlations\nto achieve impressive baselines without finetuning or training additional\nnetworks. We leverage upsampled features from ViT networks (e.g DINOv2) in two\nworkflows: in a clustering based approach for object localization and\nsegmentation, and paired with standard classifiers in weakly supervised\nmaterials segmentation. Both show strong performance on benchmarks, especially\nin weakly supervised segmentation where the ViT features capture complex\nrelationships inaccessible to classical approaches. We expect the flexibility\nand generalizability of these features will both speed up and strengthen\nmaterials characterization, from segmentation to property-prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.19836", "cate": "cs.CV", "date": "2024-10-20", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.21111", "title": "LAMA: Stable Dual-Domain Deep Reconstruction For Sparse-View CT", "authors": ["Chi Ding", "Qingchao Zhang", "Ge Wang", "Xiaojing Ye", "Yunmei Chen"], "categories": ["cs.CV", "cs.LG", "math.NA"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.21111", "summary": "Inverse problems arise in many applications, especially tomographic imaging.\nWe develop a Learned Alternating Minimization Algorithm (LAMA) to solve such\nproblems via two-block optimization by synergizing data-driven and classical\ntechniques with proven convergence. LAMA is naturally induced by a variational\nmodel with learnable regularizers in both data and image domains, parameterized\nas composite functions of neural networks trained with domain-specific data. We\nallow these regularizers to be nonconvex and nonsmooth to extract features from\ndata effectively. We minimize the overall objective function using Nesterov's\nsmoothing technique and residual learning architecture. It is demonstrated that\nLAMA reduces network complexity, improves memory efficiency, and enhances\nreconstruction accuracy, stability, and interpretability. Extensive experiments\nshow that LAMA significantly outperforms state-of-the-art methods on popular\nbenchmark datasets for Computed Tomography.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.21111", "cate": "cs.CV", "date": "2024-10-28", "updated": "2025-08-05", "section": "repl"}
{"id": "2410.23736", "title": "Modality and Task Adaptation for Enhanced Zero-shot Composed Image Retrieval", "authors": ["Haiwen Li", "Fei Su", "Zhicheng Zhao"], "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.23736", "summary": "As a challenging vision-language task, Zero-Shot Composed Image Retrieval\n(ZS-CIR) is designed to retrieve target images using bi-modal (image+text)\nqueries. Typical ZS-CIR methods employ an inversion network to generate\npseudo-word tokens that effectively represent the input semantics. However, the\ninversion-based methods suffer from two inherent issues: First, the task\ndiscrepancy exists because inversion training and CIR inference involve\ndifferent objectives. Second, the modality discrepancy arises from the input\nfeature distribution mismatch between training and inference. To this end, we\npropose a lightweight post-hoc framework, consisting of two components: (1) A\nnew text-anchored triplet construction pipeline leverages a large language\nmodel (LLM) to transform a standard image-text dataset into a triplet dataset,\nwhere a textual description serves as the target of each triplet. (2) The\nMoTa-Adapter, a novel parameter-efficient fine-tuning method, adapts the dual\nencoder to the CIR task using our constructed triplet data. Specifically, on\nthe text side, multiple sets of learnable task prompts are integrated via a\nMixture-of-Experts (MoE) layer to capture task-specific priors and handle\ndifferent types of modifications. On the image side, MoTa-Adapter modulates the\ninversion network's input to better match the downstream text encoder. In\naddition, an entropy-based optimization strategy is proposed to assign greater\nweight to challenging samples, thus ensuring efficient adaptation. Experiments\nshow that, with the incorporation of our proposed components, inversion-based\nmethods achieve significant improvements, reaching state-of-the-art performance\nacross four widely-used benchmarks. All data and code will be made publicly\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.23736", "cate": "cs.CV", "date": "2024-10-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.05183", "title": "Interpretable Estimation of CNN Deep Feature Density using Copula and the Generalized Characteristic Function", "authors": ["David Chapman", "Parniyan Farvardin"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.05183", "summary": "We present a novel empirical approach toward estimating the Probability\nDensity Function (PDF) of the deep features of Convolutional Neural Networks\n(CNNs). Estimating the PDF of deep CNN features is an important task, because\nit will yield new insight into deep representations. Moreover, characterizing\nthe statistical behavior has implications for the feasibility of promising\ndownstream tasks such as density based anomaly detection. Expressive, yet\ninterpretable estimation of the deep feature PDF is challenging due to the\nCurse of Dimensionality (CoD) as well as our limited ability to comprehend\nhigh-dimensional inter-dependencies. Our novel estimation technique combines\ncopula analysis with the Method of Orthogonal Moments (MOM), in order to\ndirectly estimate the Generalized Characteristic Function (GCF) of the\nmultivariate deep feature PDF. We find that the one-dimensional marginals of\nnon-negative deep CNN features after major blocks are not well approximated by\na Gaussian distribution, and that the features of deep layers are much better\napproximated by the Exponential, Gamma, and/or Weibull distributions.\nFurthermore, we observe that deep features become increasingly long-tailed with\nnetwork depth, although surprisingly the rate of this increase is much slower\nthan theoretical estimates. Finally, we observe that many deep features exhibit\nstrong dependence (either correlation or anti-correlation) with other extremely\nstrong detections, even if these features are independent within typical\nranges. We elaborate on these findings in our discussion, where we hypothesize\nthat the long-tail of large valued features corresponds to the strongest\ncomputer vision detections of semantic targets, which would imply that these\nlarge-valued features are not outliers but rather an important detection\nsignal.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.05183", "cate": "cs.CV", "date": "2024-11-07", "updated": "2025-08-05", "section": "repl"}
{"id": "2411.17475", "title": "COBRA: A Continual Learning Approach to Vision-Brain Understanding", "authors": ["Xuan-Bac Nguyen", "Manuel Serna-Aguilera", "Arabinda Kumar Choudhary", "Pawan Sinha", "Xin Li", "Khoa Luu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.17475", "summary": "Vision-Brain Understanding (VBU) aims to extract visual information perceived\nby humans from brain activity recorded through functional Magnetic Resonance\nImaging (fMRI). Despite notable advancements in recent years, existing studies\nin VBU continue to face the challenge of catastrophic forgetting, where models\nlose knowledge from prior subjects as they adapt to new ones. Addressing\ncontinual learning in this field is, therefore, essential. This paper\nintroduces a novel framework called Continual Learning for Vision-Brain (COBRA)\nto address continual learning in VBU. Our approach includes three novel\nmodules: a Subject Commonality (SC) module, a Prompt-based Subject Specific\n(PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer\nmodule. The SC module captures shared vision-brain patterns across subjects,\npreserving this knowledge as the model encounters new subjects, thereby\nreducing the impact of catastrophic forgetting. On the other hand, the PSS\nmodule learns unique vision-brain patterns specific to each subject. Finally,\nthe MRIFormer module contains a transformer encoder and decoder that learns the\nfMRI features for VBU from common and specific patterns. In a continual\nlearning setup, COBRA is trained in new PSS and MRIFormer modules for new\nsubjects, leaving the modules of previous subjects unaffected. As a result,\nCOBRA effectively addresses catastrophic forgetting and achieves\nstate-of-the-art performance in both continual learning and vision-brain\nreconstruction tasks, surpassing previous methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.17475", "cate": "cs.CV", "date": "2024-11-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.01812", "title": "V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction", "authors": ["Zewei Zhou", "Hao Xiang", "Zhaoliang Zheng", "Seth Z. Zhao", "Mingyue Lei", "Yun Zhang", "Tianhui Cai", "Xinyi Liu", "Johnson Liu", "Maheswari Bajji", "Xin Xia", "Zhiyu Huang", "Bolei Zhou", "Jiaqi Ma"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.01812", "summary": "Vehicle-to-everything (V2X) technologies offer a promising paradigm to\nmitigate the limitations of constrained observability in single-vehicle\nsystems. Prior work primarily focuses on single-frame cooperative perception,\nwhich fuses agents' information across different spatial locations but ignores\ntemporal cues and temporal tasks (e.g., temporal perception and prediction). In\nthis paper, we focus on the spatio-temporal fusion in V2X scenarios and design\none-step and multi-step communication strategies (when to transmit) as well as\nexamine their integration with three fusion strategies - early, late, and\nintermediate (what to transmit), providing comprehensive benchmarks with 11\nfusion models (how to fuse). Furthermore, we propose V2XPnP, a novel\nintermediate fusion framework within one-step communication for end-to-end\nperception and prediction. Our framework employs a unified Transformer-based\narchitecture to effectively model complex spatio-temporal relationships across\nmultiple agents, frames, and high-definition maps. Moreover, we introduce the\nV2XPnP Sequential Dataset that supports all V2X collaboration modes and\naddresses the limitations of existing real-world datasets, which are restricted\nto single-frame or single-mode cooperation. Extensive experiments demonstrate\nthat our framework outperforms state-of-the-art methods in both perception and\nprediction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.01812", "cate": "cs.CV", "date": "2024-12-02", "updated": "2025-08-05", "section": "repl"}
{"id": "2412.03812", "title": "Pinco: Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting", "authors": ["Guangben Lu", "Yuzhen Du", "Zhimin Sun", "Ran Yi", "Yifan Qi", "Yizhe Tang", "Tianyi Wang", "Lizhuang Ma", "Fangyuan Zou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.03812", "summary": "Foreground-conditioned inpainting aims to seamlessly fill the background\nregion of an image by utilizing the provided foreground subject and a text\ndescription. While existing T2I-based image inpainting methods can be applied\nto this task, they suffer from issues of subject shape expansion, distortion,\nor impaired ability to align with the text description, resulting in\ninconsistencies between the visual elements and the text description. To\naddress these challenges, we propose Pinco, a plug-and-play\nforeground-conditioned inpainting adapter that generates high-quality\nbackgrounds with good text alignment while effectively preserving the shape of\nthe foreground subject. Firstly, we design a Self-Consistent Adapter that\nintegrates the foreground subject features into the layout-related\nself-attention layer, which helps to alleviate conflicts between the text and\nsubject features by ensuring that the model can effectively consider the\nforeground subject's characteristics while processing the overall image layout.\nSecondly, we design a Decoupled Image Feature Extraction method that employs\ndistinct architectures to extract semantic and spatial features separately,\nsignificantly improving subject feature extraction and ensuring high-quality\npreservation of the subject's shape. Thirdly, to ensure precise utilization of\nthe extracted features and to focus attention on the subject region, we\nintroduce a Shared Positional Embedding Anchor, greatly improving the model's\nunderstanding of subject features and boosting training efficiency. Extensive\nexperiments demonstrate that our method achieves superior performance and\nefficiency in foreground-conditioned inpainting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.03812", "cate": "cs.CV", "date": "2024-12-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.03859", "title": "CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation", "authors": ["Hui Zhang", "Dexiang Hong", "Yitong Wang", "Jie Shao", "Xinglong Wu", "Zuxuan Wu", "Yu-Gang Jiang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.03859", "summary": "Diffusion models have been recognized for their ability to generate images\nthat are not only visually appealing but also of high artistic quality. As a\nresult, Layout-to-Image (L2I) generation has been proposed to leverage\nregion-specific positions and descriptions to enable more precise and\ncontrollable generation. However, previous methods primarily focus on\nUNet-based models (\\eg SD1.5 and SDXL), and limited effort has explored\nMultimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful\nimage generation capabilities. Enabling MM-DiT for layout-to-image generation\nseems straightforward but is challenging due to the complexity of how layout is\nintroduced, integrated, and balanced among multiple modalities. To this end, we\nexplore various network variants to efficiently incorporate layout guidance\ninto MM-DiT, and ultimately present SiamLayout. To inherit the advantages of\nMM-DiT, we use a separate set of network weights to process the layout,\ntreating it as equally important as the image and text modalities. Meanwhile,\nto alleviate the competition among modalities, we decouple the image-layout\ninteraction into a siamese branch alongside the image-text one and fuse them in\nthe later stage. Moreover, we contribute a large-scale layout dataset, named\nLayoutSAM, which includes 2.7 million image-text pairs and 10.7 million\nentities. Each entity is annotated with a bounding box and a detailed\ndescription. We further construct the LayoutSAM-Eval benchmark as a\ncomprehensive tool for evaluating the L2I generation quality. Finally, we\nintroduce the Layout Designer, which taps into the potential of large language\nmodels in layout planning, transforming them into experts in layout generation\nand optimization. These components form CreatiLayout -- a systematic solution\nthat integrates the layout model, dataset, and planner for creative\nlayout-to-image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.03859", "cate": "cs.CV", "date": "2024-12-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.07689", "title": "RoboTron-Drive: All-in-One Large Multimodal Model for Autonomous Driving", "authors": ["Zhijian Huang", "Chengjian Feng", "Feng Yan", "Baihui Xiao", "Zequn Jie", "Yujie Zhong", "Xiaodan Liang", "Lin Ma"], "categories": ["cs.CV", "cs.MM", "cs.RO"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.07689", "summary": "Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose RoboTron-Drive, a general large multimodal model\ndesigned to process diverse data inputs, such as images and multi-view videos,\nwhile performing a broad spectrum of AD tasks, including perception,\nprediction, and planning. Initially, the model undergoes curriculum\npre-training to process varied visual signals and perform basic visual\ncomprehension and perception tasks. Subsequently, we augment and standardize\nvarious AD datasets to finetune the model, resulting in an all-in-one LMM for\nautonomous driving. To assess the general capabilities and generalization\nability, we conduct evaluations on six public benchmarks and undertake\nzero-shot transfer on three unseen datasets, where RoboTron-Drive achieves\nstate-of-the-art performance across all tasks. We hope RoboTron-Drive as a\npromising solution for AD in the real world. Project page with code:\nhttps://github.com/zhijian11/RoboTron-Drive.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.07689", "cate": "cs.CV", "date": "2024-12-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.18450", "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding", "authors": ["Tatiana Zemskova", "Dmitry Yudin"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.18450", "summary": "A 3D scene graph represents a compact scene model by capturing both the\nobjects present and the semantic relationships between them, making it a\npromising structure for robotic applications. To effectively interact with\nusers, an embodied intelligent agent should be able to answer a wide range of\nnatural language queries about the surrounding 3D environment. Large Language\nModels (LLMs) are beneficial solutions for user-robot interaction due to their\nnatural language understanding and reasoning abilities. Recent methods for\nlearning scene representations have shown that adapting these representations\nto the 3D world can significantly improve the quality of LLM responses.\nHowever, existing methods typically rely only on geometric information, such as\nobject coordinates, and overlook the rich semantic relationships between\nobjects. In this work, we propose 3DGraphLLM, a method for constructing a\nlearnable representation of a 3D scene graph that explicitly incorporates\nsemantic relationships. This representation is used as input to LLMs for\nperforming 3D vision-language tasks. In our experiments on popular ScanRefer,\nMulti3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate that our\napproach outperforms baselines that do not leverage semantic relationships\nbetween objects. The code is publicly available at\nhttps://github.com/CognitiveAISystems/3DGraphLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.18450", "cate": "cs.CV", "date": "2024-12-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.20720", "title": "4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives", "authors": ["Zeyu Yang", "Zijie Pan", "Xiatian Zhu", "Li Zhang", "Jianfeng Feng", "Yu-Gang Jiang", "Philip H.S. Torr"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.20720", "summary": "Dynamic 3D scene representation and novel view synthesis are crucial for\nenabling immersive experiences required by AR/VR and metaverse applications. It\nis a challenging task due to the complexity of unconstrained real-world scenes\nand their temporal dynamics. In this paper, we reformulate the reconstruction\nof a time-varying 3D scene as approximating its underlying spatiotemporal 4D\nvolume by optimizing a collection of native 4D primitives, i.e., 4D Gaussians,\nwith explicit geometry and appearance modeling. Equipped with a tailored\nrendering pipeline, our representation can be end-to-end optimized using only\nphotometric supervision while free viewpoint viewing at interactive frame rate,\nmaking it suitable for representing real world scene with complex dynamic. This\napproach has been the first solution to achieve real-time rendering of\nhigh-resolution, photorealistic novel views for complex dynamic scenes. To\nfacilitate real-world applications, we derive several compact variants that\neffectively reduce the memory footprint to address its storage bottleneck.\nExtensive experiments validate the superiority of 4DGS in terms of visual\nquality and efficiency across a range of dynamic scene-related tasks (e.g.,\nnovel view synthesis, 4D generation, scene understanding) and scenarios (e.g.,\nsingle object, indoor scenes, driving environments, synthetic and real data).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.20720", "cate": "cs.CV", "date": "2024-12-30", "updated": "2025-08-05", "section": "repl"}
{"id": "2501.08331", "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise", "authors": ["Ryan Burgert", "Yuancheng Xu", "Wenqi Xian", "Oliver Pilarski", "Pascal Clausen", "Mingming He", "Li Ma", "Yitong Deng", "Lingxiao Li", "Mohsen Mousavi", "Michael Ryoo", "Paul Debevec", "Ning Yu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.08331", "summary": "Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps://eyeline-labs.github.io/Go-with-the-Flow. Source code and model\ncheckpoints are available on GitHub:\nhttps://github.com/Eyeline-Labs/Go-with-the-Flow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.08331", "cate": "cs.CV", "date": "2025-01-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.20879", "title": "egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks", "authors": ["BjÃ¶rn Braun", "Rayan Armani", "Manuel Meier", "Max Moebus", "Christian Holz"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20879", "summary": "Egocentric vision systems aim to understand the spatial surroundings and the\nwearer's behavior inside it, including motions, activities, and interactions.\nWe argue that egocentric systems must additionally detect physiological states\nto capture a person's attention and situational responses, which are critical\nfor context-aware behavior modeling. In this paper, we propose egoPPG, a novel\nvision task for egocentric systems to recover a person's cardiac activity to\naid downstream vision tasks. We introduce PulseFormer, a method to extract\nheart rate as a key indicator of physiological state from the eye tracking\ncameras on unmodified egocentric vision systems. PulseFormer continuously\nestimates the photoplethysmogram (PPG) from areas around the eyes and fuses\nmotion cues from the headset's inertial measurement unit to track HR values. We\ndemonstrate egoPPG's downstream benefit for a key task on EgoExo4D, an existing\negocentric dataset for which we find PulseFormer's estimates of HR to improve\nproficiency estimation by 14%. To train and validate PulseFormer, we collected\na dataset of 13+ hours of eye tracking videos from Project Aria and\ncontact-based PPG signals as well as an electrocardiogram (ECG) for\nground-truth HR values. Similar to EgoExo4D, 25 participants performed diverse\neveryday activities such as office work, cooking, dancing, and exercising,\nwhich induced significant natural motion and HR variation (44-164 bpm). Our\nmodel robustly estimates HR (MAE=7.67 bpm) and captures patterns (r=0.85). Our\nresults show how egocentric systems may unify environmental and physiological\ntracking to better understand users and that egoPPG as a complementary task\nprovides meaningful augmentations for existing datasets and tasks. We release\nour code, dataset, and HR augmentations for EgoExo4D to inspire research on\nphysiology-aware egocentric tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20879", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.06362", "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs", "authors": ["Umberto Cappellazzo", "Minsu Kim", "Stavros Petridis"], "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06362", "summary": "Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities\nto improve robustness in noisy environments. Recent advances in Large Language\nModels (LLMs) show strong performance in speech recognition, including AVSR.\nHowever, the long speech representations lead to high computational costs for\nLLMs. Prior methods compress inputs before feeding them to LLMs, but high\ncompression often harms accuracy. To address this, we propose Llama-MTSK, the\nfirst Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts\naudio-visual token allocation under varying compute constraints. Inspired by\nMatryoshka Representation Learning, our model encodes representations at\nmultiple granularities with a single architecture, avoiding the need for\nseparate models. For efficient fine-tuning, we introduce three LoRA-based\nstrategies using global and scale-specific modules. Evaluations on major AVSR\ndatasets show Llama-MTSK matches or outperforms models trained at fixed\ncompression levels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06362", "cate": "cs.CV", "date": "2025-03-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.11078", "title": "Understanding Flatness in Generative Models: Its Role and Benefits", "authors": ["Taehwan Lee", "Kyeongkook Seo", "Jaejun Yoo", "Sung Whan Yoon"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.11078", "summary": "Flat minima, known to enhance generalization and robustness in supervised\nlearning, remain largely unexplored in generative models. In this work, we\nsystematically investigate the role of loss surface flatness in generative\nmodels, both theoretically and empirically, with a particular focus on\ndiffusion models. We establish a theoretical claim that flatter minima improve\nrobustness against perturbations in target prior distributions, leading to\nbenefits such as reduced exposure bias -- where errors in noise estimation\naccumulate over iterations -- and significantly improved resilience to model\nquantization, preserving generative performance even under strong quantization\nconstraints. We further observe that Sharpness-Aware Minimization (SAM), which\nexplicitly controls the degree of flatness, effectively enhances flatness in\ndiffusion models even surpassing the indirectly promoting flatness methods --\nInput Perturbation (IP) which enforces the Lipschitz condition,\nensembling-based approach like Stochastic Weight Averaging (SWA) and\nExponential Moving Average (EMA) -- are less effective. Through extensive\nexperiments on CIFAR-10, LSUN Tower, and FFHQ, we demonstrate that flat minima\nin diffusion models indeed improve not only generative performance but also\nrobustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.11078", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.12947", "title": "DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF", "authors": ["Ingyun Lee", "Jae Won Jang", "Seunghyeon Seo", "Nojun Kwak"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12947", "summary": "Neural Radiance Field (NeRF) has shown remarkable performance in novel view\nsynthesis but requires numerous multi-view images, limiting its practicality in\nfew-shot scenarios. Ray augmentation has been proposed to alleviate overfitting\ncaused by sparse training data by generating additional rays. However, existing\nmethods, which generate augmented rays only near the original rays, exhibit\npronounced floaters and appearance distortions due to limited viewpoints and\ninconsistent rays obstructed by nearby obstacles and complex surfaces. To\naddress these problems, we propose DivCon-NeRF, which introduces novel\nsphere-based ray augmentations to significantly enhance both diversity and\nconsistency. By employing a virtual sphere centered at the predicted surface\npoint, our method generates diverse augmented rays from all 360-degree\ndirections, facilitated by our consistency mask that effectively filters out\ninconsistent rays. We introduce tailored loss functions that leverage these\naugmentations, effectively reducing floaters and visual distortions.\nConsequently, our method outperforms existing few-shot NeRF approaches on the\nBlender, LLFF, and DTU datasets. Furthermore, DivCon-NeRF demonstrates strong\ngeneralizability by effectively integrating with both regularization- and\nframework-based few-shot NeRFs. Our code will be made publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12947", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.23951", "title": "JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation", "authors": ["Fangda Chen", "Shanshan Zhao", "Chuanfu Xu", "Long Lan"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.23951", "summary": "Recent advancements in customized video generation have led to significant\nimprovements in the simultaneous adaptation of appearance and motion. While\nprior methods typically decouple appearance and motion training, the stage-wise\nstrategy often introduces concept interference, resulting in inaccurate\nrendering of appearance features or motion patterns. Another challenge is\nappearance contamination, where background and foreground elements from\nreference videos distort the customized subject. In this work, we propose\nJointTuner, a novel framework that enables joint optimization of both\nappearance and motion components by leveraging two key innovations: Synaptic\nLow-Rank Adaptation (Synaptic LoRA) and Appearance-independent Temporal Loss\n(AiT Loss). Synaptic LoRA introduces a synaptic regulator, implemented as a\ncontext-aware linear activation layer, to dynamically guide LoRA modules to\nfocus on either subject appearance or motion patterns, thereby enabling\nconsistent optimization across spatial and temporal dimensions. AiT Loss\ndisrupts the gradient flow of appearance-related components, guiding the model\nto focus exclusively on motion learning and minimizing appearance interference.\nJointTuner is compatible with both UNet-based models (e.g., ZeroScope) and\nDiffusion Transformer-based models (e.g., CogVideoX), supporting the generation\nof longer and higher-quality customized videos. Additionally, we present a\nsystematic evaluation framework for appearance-motion combined customization,\ncovering 90 combinations evaluated along four critical dimensions: semantic\nalignment, motion dynamism, temporal consistency, and perceptual quality. Our\nproject homepage can be found at https://fdchen24.github.io/JointTuner-Website.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.23951", "cate": "cs.CV", "date": "2025-03-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.06740", "title": "MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning", "authors": ["Ylli Sadikaj", "Hongkuan Zhou", "Lavdim Halilaj", "Stefan Schmid", "Steffen Staab", "Claudia Plant"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06740", "summary": "Precise optical inspection in industrial applications is crucial for\nminimizing scrap rates and reducing the associated costs. Besides merely\ndetecting if a product is anomalous or not, it is crucial to know the distinct\ntype of defect, such as a bent, cut, or scratch. The ability to recognize the\n\"exact\" defect type enables automated treatments of the anomalies in modern\nproduction lines. Current methods are limited to solely detecting whether a\nproduct is defective or not without providing any insights on the defect type,\nnevertheless detecting and identifying multiple defects. We propose MultiADS, a\nzero-shot learning approach, able to perform Multi-type Anomaly Detection and\nSegmentation. The architecture of MultiADS comprises CLIP and extra linear\nlayers to align the visual- and textual representation in a joint feature\nspace. To the best of our knowledge, our proposal, is the first approach to\nperform a multi-type anomaly segmentation task in zero-shot learning. Contrary\nto the other baselines, our approach i) generates specific anomaly masks for\neach distinct defect type, ii) learns to distinguish defect types, and iii)\nsimultaneously identifies multiple defect types present in an anomalous\nproduct. Additionally, our approach outperforms zero/few-shot learning SoTA\nmethods on image-level and pixel-level anomaly detection and segmentation tasks\non five commonly used datasets: MVTec-AD, Visa, MPDD, MAD and Real-IAD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06740", "cate": "cs.CV", "date": "2025-04-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.07810", "title": "Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for Low-Light Image Enhancement", "authors": ["Daniel Torres", "Joan Duran", "Julia Navarro", "Catalina Sbert"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.07810", "summary": "Images captured under low-light conditions present significant limitations in\nmany applications, as poor lighting can obscure details, reduce contrast, and\nhide noise. Removing the illumination effects and enhancing the quality of such\nimages is crucial for many tasks, such as image segmentation and object\ndetection. In this paper, we propose a variational method for low-light image\nenhancement based on the Retinex decomposition into illumination, reflectance,\nand noise components. A color correction pre-processing step is applied to the\nlow-light image, which is then used as the observed input in the decomposition.\nMoreover, our model integrates a novel nonlocal gradient-type fidelity term\ndesigned to preserve structural details. Additionally, we propose an automatic\ngamma correction module. Building on the proposed variational approach, we\nextend the model by introducing its deep unfolding counterpart, in which the\nproximal operators are replaced with learnable networks. We propose\ncross-attention mechanisms to capture long-range dependencies in both the\nnonlocal prior of the reflectance and the nonlocal gradient-based constraint.\nExperimental results demonstrate that both methods compare favorably with\nseveral recent and state-of-the-art techniques across different datasets. In\nparticular, despite not relying on learning strategies, the variational model\noutperforms most deep learning approaches both visually and in terms of quality\nmetrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.07810", "cate": "cs.CV", "date": "2025-04-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.17432", "title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs", "authors": ["Tiancheng Gu", "Kaicheng Yang", "Ziyong Feng", "Xingjun Wang", "Yanzhao Zhang", "Dingkun Long", "Yingda Chen", "Weidong Cai", "Jiankang Deng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.17432", "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.17432", "cate": "cs.CV", "date": "2025-04-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.18087", "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation", "authors": ["Weipeng Tan", "Chuming Lin", "Chengming Xu", "FeiFan Xu", "Xiaobin Hu", "Xiaozhong Ji", "Junwei Zhu", "Chengjie Wang", "Yanwei Fu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.18087", "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.18087", "cate": "cs.CV", "date": "2025-04-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.20830", "title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation", "authors": ["Jianyu Wu", "Yizhou Wang", "Xiangyu Yue", "Xinzhu Ma", "Jingyang Guo", "Dongzhan Zhou", "Wanli Ouyang", "Shixiang Tang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20830", "summary": "While accurate and user-friendly Computer-Aided Design (CAD) is crucial for\nindustrial design and manufacturing, existing methods still struggle to achieve\nthis due to their over-simplified representations or architectures incapable of\nsupporting multimodal design requirements. In this paper, we attempt to tackle\nthis problem from both methods and datasets aspects. First, we propose a\ncascade MAR with topology predictor (CMT), the first multimodal framework for\nCAD generation based on Boundary Representation (B-Rep). Specifically, the\ncascade MAR can effectively capture the ``edge-counters-surface'' priors that\nare essential in B-Reps, while the topology predictor directly estimates\ntopology in B-Reps from the compact tokens in MAR. Second, to facilitate\nlarge-scale training, we develop a large-scale multimodal CAD dataset, mmABC,\nwhich includes over 1.3 million B-Rep models with multimodal annotations,\nincluding point clouds, text descriptions, and multi-view images. Extensive\nexperiments show the superior of CMT in both conditional and unconditional CAD\ngeneration tasks. For example, we improve Coverage and Valid ratio by +10.68%\nand +10.3%, respectively, compared to state-of-the-art methods on ABC in\nunconditional generation. CMT also improves +4.01 Chamfer on image conditioned\nCAD generation on mmABC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20830", "cate": "cs.CV", "date": "2025-04-29", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.02365", "title": "Quaternion Sparse Decomposition for Multi-focus Color Image Fusion", "authors": ["Weihua Yang", "Yicong Zhou"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.02365", "summary": "Multi-focus color image fusion refers to integrating multiple partially\nfocused color images to create a single all-in-focus color image. However,\nexisting methods struggle with complex real-world scenarios due to limitations\nin handling color information and intricate textures. To address these\nchallenges, this paper proposes a quaternion multi-focus color image fusion\nframework to perform high-quality color image fusion completely in the\nquaternion domain. This framework introduces 1) a quaternion sparse\ndecomposition model to jointly learn fine-scale image details and structure\ninformation of color images in an iterative fashion for high-precision focus\ndetection, 2) a quaternion base-detail fusion strategy to individually fuse\nbase-scale and detail-scale results across multiple color images for preserving\nstructure and detail information, and 3) a quaternion structural similarity\nrefinement strategy to adaptively select optimal patches from initial fusion\nresults and obtain the final fused result for preserving fine details and\nensuring spatially consistent outputs. Extensive experiments demonstrate that\nthe proposed framework outperforms state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.02365", "cate": "cs.CV", "date": "2025-05-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.03254", "title": "PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs", "authors": ["Lukas Meiner", "Jens Mehnert", "Alexandru Paul Condurache"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.03254", "summary": "Convolutional neural networks (CNNs) are crucial for computer vision tasks on\nresource-constrained devices. Quantization effectively compresses these models,\nreducing storage size and energy cost. However, in modern depthwise-separable\narchitectures, the computational cost is distributed unevenly across its\ncomponents, with pointwise operations being the most expensive. By applying a\ngeneral quantization scheme to this imbalanced cost distribution, existing\nquantization approaches fail to fully exploit potential efficiency gains. To\nthis end, we introduce PROM, a straightforward approach for quantizing modern\ndepthwise-separable convolutional networks by selectively using two distinct\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\nweights, while the remaining modules use 8-bit weights, which is achieved\nthrough a simple quantization-aware training procedure. Additionally, by\nquantizing activations to 8-bit, our method transforms pointwise convolutions\nwith ternary weights into int8 additions, which enjoy broad support across\nhardware platforms and effectively eliminates the need for expensive\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\ncompared to the float16 baseline while retaining similar classification\nperformance on ImageNet. Our method advances the Pareto frontier for energy\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\nPROM addresses the challenges of quantizing depthwise-separable convolutional\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\nenergy cost and storage size.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.03254", "cate": "cs.CV", "date": "2025-05-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.04720", "title": "False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims", "authors": ["Evangelia Christodoulou", "Annika Reinke", "Pascaline AndrÃ¨", "Patrick Godau", "Piotr Kalinowski", "Rola Houhou", "Selen Erkan", "Carole H. Sudre", "Ninon Burgos", "SofiÃ¨ne Boutaj", "Sophie Loizillon", "MaÃ«lys Solal", "Veronika Cheplygina", "Charles Heitz", "Michal Kozubek", "Michela Antonelli", "Nicola Rieke", "Antoine Gilson", "Leon D. Mayer", "Minu D. Tizabi", "M. Jorge Cardoso", "Amber Simpson", "Annette Kopp-Schneider", "GaÃ«l Varoquaux", "Olivier Colliot", "Lena Maier-Hein"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.04720", "summary": "Performance comparisons are fundamental in medical imaging Artificial\nIntelligence (AI) research, often driving claims of superiority based on\nrelative improvements in common performance metrics. However, such claims\nfrequently rely solely on empirical mean performance. In this paper, we\ninvestigate whether newly proposed methods genuinely outperform the state of\nthe art by analyzing a representative cohort of medical imaging papers. We\nquantify the probability of false claims based on a Bayesian approach that\nleverages reported results alongside empirically estimated model congruence to\nestimate whether the relative ranking of methods is likely to have occurred by\nchance. According to our results, the majority (>80%) of papers claims\noutperformance when introducing a new method. Our analysis further revealed a\nhigh probability (>5%) of false outperformance claims in 86% of classification\npapers and 53% of segmentation papers. These findings highlight a critical flaw\nin current benchmarking practices: claims of outperformance in medical imaging\nAI are frequently unsubstantiated, posing a risk of misdirecting future\nresearch efforts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.04720", "cate": "cs.CV", "date": "2025-05-07", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.06133", "title": "BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation", "authors": ["Hongming Wang", "Yifeng Wu", "Huimin Huang", "Hongtao Wu", "Jia-Xuan Jiang", "Xiaodong Zhang", "Hao Zheng", "Xian Wu", "Yefeng Zheng", "Jinping Xu", "Jing Cheng"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.06133", "summary": "The segmentation of substantial brain lesions is a significant and\nchallenging task in the field of medical image segmentation. Substantial brain\nlesions in brain imaging exhibit high heterogeneity, with indistinct boundaries\nbetween lesion regions and normal brain tissue. Small lesions in single slices\nare difficult to identify, making the accurate and reproducible segmentation of\nabnormal regions, as well as their feature description, highly complex.\nExisting methods have the following limitations: 1) They rely solely on\nsingle-modal information for learning, neglecting the multi-modal information\ncommonly used in diagnosis. This hampers the ability to comprehensively acquire\nbrain lesion information from multiple perspectives and prevents the effective\nintegration and utilization of multi-modal data inputs, thereby limiting a\nholistic understanding of lesions. 2) They are constrained by the amount of\ndata available, leading to low sensitivity to small lesions and difficulty in\ndetecting subtle pathological changes. 3) Current SAM-based models rely on\nexternal prompts, which cannot achieve automatic segmentation and, to some\nextent, affect diagnostic efficiency.To address these issues, we have developed\na large-scale fully automated segmentation model specifically designed for\nbrain lesion segmentation, named BrainSegDMLF. This model has the following\nfeatures: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and\nintegrates multi-modal data during the encoding process, providing the SAM\nencoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling\nDecoder, enabling the model to extract rich low-level and high-level features\neven with limited data, thereby detecting the presence of small lesions. 3)\nAutomatic segmentation masks, allowing the model to generate lesion masks\nautomatically without requiring manual prompts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.06133", "cate": "cs.CV", "date": "2025-05-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.11334", "title": "MARRS: Masked Autoregressive Unit-based Reaction Synthesis", "authors": ["Yabiao Wang", "Shuo Wang", "Jiangning Zhang", "Jiafu Wu", "Qingdong He", "Yong Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11334", "summary": "This work aims at a challenging task: human action-reaction synthesis, i.e.,\ngenerating human reactions conditioned on the action sequence of another\nperson. Currently, autoregressive modeling approaches with vector quantization\n(VQ) have achieved remarkable performance in motion generation tasks. However,\nVQ has inherent disadvantages, including quantization information loss, low\ncodebook utilization, etc. In addition, while dividing the body into separate\nunits can be beneficial, the computational complexity needs to be considered.\nAlso, the importance of mutual perception among units is often neglected. In\nthis work, we propose MARRS, a novel framework designed to generate coordinated\nand fine-grained reaction motions using continuous representations. Initially,\nwe present the Unit-distinguished Motion Variational AutoEncoder (UD-VAE),\nwhich segments the entire body into distinct body and hand units, encoding each\nindependently. Subsequently, we propose Action-Conditioned Fusion (ACF), which\ninvolves randomly masking a subset of reactive tokens and extracting specific\ninformation about the body and hands from the active tokens. Furthermore, we\nintroduce Adaptive Unit Modulation (AUM) to facilitate interaction between body\nand hand units by using the information from one unit to adaptively modulate\nthe other. Finally, for the diffusion model, we employ a compact MLP as a noise\npredictor for each distinct body unit and incorporate the diffusion loss to\nmodel the probability distribution of each token. Both quantitative and\nqualitative results demonstrate that our method achieves superior performance.\nThe code will be released upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11334", "cate": "cs.CV", "date": "2025-05-16", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.13039", "title": "Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification", "authors": ["Xiao Wu", "Xiaoqing Zhang", "Zunjie Xiao", "Lingxi Hu", "Risa Higashita", "Jiang Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13039", "summary": "Efficient convolutional neural network (CNN) architecture design has\nattracted growing research interests. However, they typically apply single\nreceptive field (RF), small asymmetric RFs, or pyramid RFs to learn different\nfeature representations, still encountering two significant challenges in\nmedical image classification tasks: 1) They have limitations in capturing\ndiverse lesion characteristics efficiently, e.g., tiny, coordination, small and\nsalient, which have unique roles on the classification results, especially\nimbalanced medical image classification. 2) The predictions generated by those\nCNNs are often unfair/biased, bringing a high risk when employing them to\nreal-world medical diagnosis conditions. To tackle these issues, we develop a\nnew concept, Expert-Like Reparameterization of Heterogeneous Pyramid Receptive\nFields (ERoHPRF), to simultaneously boost medical image classification\nperformance and fairness. This concept aims to mimic the multi-expert\nconsultation mode by applying the well-designed heterogeneous pyramid RF bag to\ncapture lesion characteristics with varying significances effectively via\nconvolution operations with multiple heterogeneous kernel sizes. Additionally,\nERoHPRF introduces an expert-like structural reparameterization technique to\nmerge its parameters with the two-stage strategy, ensuring competitive\ncomputation cost and inference speed through comparisons to a single RF. To\nmanifest the effectiveness and generalization ability of ERoHPRF, we\nincorporate it into mainstream efficient CNN architectures. The extensive\nexperiments show that our proposed ERoHPRF maintains a better trade-off than\nstate-of-the-art methods in terms of medical image classification, fairness,\nand computation overhead. The code of this paper is available at\nhttps://github.com/XiaoLing12138/Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13039", "cate": "cs.CV", "date": "2025-05-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.13219", "title": "PiT: Progressive Diffusion Transformer", "authors": ["Jiafu Wu", "Yabiao Wang", "Jian Li", "Jinlong Peng", "Yun Cao", "Chengjie Wang", "Jiangning Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13219", "summary": "Diffusion Transformers (DiTs) achieve remarkable performance within image\ngeneration via the transformer architecture. Conventionally, DiTs are\nconstructed by stacking serial isotropic global modeling transformers, which\nface significant quadratic computational cost. However, through empirical\nanalysis, we find that DiTs do not rely as heavily on global information as\npreviously believed. In fact, most layers exhibit significant redundancy in\nglobal computation. Additionally, conventional attention mechanisms suffer from\nlow-frequency inertia, limiting their efficiency. To address these issues, we\npropose Pseudo Shifted Window Attention (PSWA), which fundamentally mitigates\nglobal attention redundancy. PSWA achieves moderate global-local information\nthrough window attention. It further utilizes a high-frequency bridging branch\nto simulate shifted window operations, which both enrich the high-frequency\ninformation and strengthen inter-window connections. Furthermore, we propose\nthe Progressive Coverage Channel Allocation (PCCA) strategy that captures\nhigh-order attention without additional computational cost. Based on these\ninnovations, we propose a series of Pseudo \\textbf{P}rogressive\nD\\textbf{i}ffusion \\textbf{T}ransformer (\\textbf{PiT}). Our extensive\nexperiments show their superior performance; for example, our proposed PiT-L\nachieves 54\\%$\\uparrow$ FID improvement over DiT-XL/2 while using less\ncomputation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13219", "cate": "cs.CV", "date": "2025-05-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.20617", "title": "OccLE: Label-Efficient 3D Semantic Occupancy Prediction", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Fayao Liu", "Xulei Yang", "Jiacheng Wei", "Lemiao Qiu", "Guosheng Lin"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20617", "summary": "3D semantic occupancy prediction offers an intuitive and efficient scene\nunderstanding and has attracted significant interest in autonomous driving\nperception. Existing approaches either rely on full supervision, which demands\ncostly voxel-level annotations, or on self-supervision, which provides limited\nguidance and yields suboptimal performance. To address these challenges, we\npropose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes\nimages and LiDAR as inputs and maintains high performance with limited voxel\nannotations. Our intuition is to decouple the semantic and geometric learning\ntasks and then fuse the learned feature grids from both tasks for the final\nsemantic occupancy prediction. Therefore, the semantic branch distills 2D\nfoundation model to provide aligned pseudo labels for 2D and 3D semantic\nlearning. The geometric branch integrates image and LiDAR inputs in cross-plane\nsynergy based on their inherency, employing semi-supervision to enhance\ngeometry learning. We fuse semantic-geometric feature grids through Dual Mamba\nand incorporate a scatter-accumulated projection to supervise unannotated\nprediction with aligned pseudo labels. Experiments show that OccLE achieves\ncompetitive performance with only 10\\% of voxel annotations on the\nSemanticKITTI and Occ3D-nuScenes datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20617", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.20951", "title": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Kang Wang", "Ruibo Li", "Lemiao Qiu", "Shuyou Zhang", "Zhe Wang", "Guosheng Lin"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20951", "summary": "Camera-based 3D semantic occupancy prediction offers an efficient and\ncost-effective solution for perceiving surrounding scenes in autonomous\ndriving. However, existing works rely on explicit occupancy state inference,\nleading to numerous incorrect feature assignments, and insufficient samples\nrestrict the learning of occupancy class inference. To address these\nchallenges, we propose leveraging Depth awareness and Semantic aid to boost\ncamera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform\noccupancy state and occupancy class inference, where soft occupancy confidence\nis calculated by non-learning method and multiplied with image features to make\nvoxels aware of depth, enabling adaptive implicit occupancy state inference.\nInstead of enhancing feature learning, we directly utilize well-trained image\nsemantic segmentation and fuse multiple frames with their occupancy\nprobabilities to aid occupancy class inference, thereby enhancing robustness.\nExperimental results demonstrate that DSOcc achieves state-of-the-art\nperformance on the SemanticKITTI dataset among camera-based methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20951", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03854", "title": "Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training", "authors": ["Xin Zhang", "Quanyu Zhu", "Liangbei Xu", "Zain Huda", "Wang Zhou", "Jin Fang", "Dennis van der Staay", "Yuxi Hu", "Jade Nie", "Jiyan Yang", "Chunzhi Yang"], "categories": ["cs.DC", "cs.LG"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03854v1", "summary": "The increasing complexity of deep learning recommendation models (DLRM) has\nled to a growing need for large-scale distributed systems that can efficiently\ntrain vast amounts of data. In DLRM, the sparse embedding table is a crucial\ncomponent for managing sparse categorical features. Typically, these tables in\nindustrial DLRMs contain trillions of parameters, necessitating model\nparallelism strategies to address memory constraints. However, as training\nsystems expand with massive GPUs, the traditional fully parallelism strategies\nfor embedding table post significant scalability challenges, including\nimbalance and straggler issues, intensive lookup communication, and heavy\nembedding activation memory. To overcome these limitations, we propose a novel\ntwo-dimensional sparse parallelism approach. Rather than fully sharding tables\nacross all GPUs, our solution introduces data parallelism on top of model\nparallelism. This enables efficient all-to-all communication and reduces peak\nmemory consumption. Additionally, we have developed the momentum-scaled\nrow-wise AdaGrad algorithm to mitigate performance losses associated with the\nshift in training paradigms. Our extensive experiments demonstrate that the\nproposed approach significantly enhances training efficiency while maintaining\nmodel performance parity. It achieves nearly linear training speed scaling up\nto 4K GPUs, setting a new state-of-the-art benchmark for recommendation model\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03854v1", "cate": "cs.DC", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03984", "title": "High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines", "authors": ["Yuki Uchino", "Katsuhisa Ozaki", "Toshiyuki Imamura"], "categories": ["cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03984v1", "summary": "Recent architectures integrate high-performance and power-efficient matrix\nengines. These engines demonstrate remarkable performance in low-precision\nmatrix multiplication, which is crucial in deep learning. Several techniques\nhave been proposed to emulate single- and double-precision general\nmatrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such\nlow-precision matrix engines. In this study, we present emulation methods that\nsignificantly outperforms conventional approaches. On a GH200 Grace Hopper\nSuperchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\\%\nimprovement in power efficiency compared to native DGEMM for sufficiently large\nproblems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\\%\nimprovement in power efficiency compared to native SGEMM for sufficiently large\nproblems. Furthermore, compared to conventional emulation methods, the proposed\nemulation achieves more than 2x higher performance and superior power\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03984v1", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04013", "title": "High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions", "authors": ["Sameh Abdulah", "Mary Lai O. Salvana", "Ying Sun", "David E. Keyes", "Marc G. Genton"], "categories": ["cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04013v1", "summary": "We recognize the emergence of a statistical computing community focused on\nworking with large computing platforms and producing software and applications\nthat exemplify high-performance statistical computing (HPSC). The statistical\ncomputing (SC) community develops software that is widely used across\ndisciplines. However, it remains largely absent from the high-performance\ncomputing (HPC) landscape, particularly on platforms such as those featured on\nthe Top500 or Green500 lists. Many disciplines already participate in HPC,\nmostly centered around simulation science, although data-focused efforts under\nthe artificial intelligence (AI) label are gaining popularity. Bridging this\ngap requires both community adaptation and technical innovation to align\nstatistical methods with modern HPC technologies. We can accelerate progress in\nfast and scalable statistical applications by building strong connections\nbetween the SC and HPC communities. We present a brief history of SC, a vision\nfor how its strengths can contribute to statistical science in the HPC\nenvironment (such as HPSC), the challenges that remain, and the opportunities\ncurrently available, culminating in a possible roadmap toward a thriving HPSC\ncommunity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04013v1", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04271", "title": "S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge", "authors": ["JinYi Yoon", "JiHo Lee", "Ting He", "Nakjung Choi", "Bo Ji"], "categories": ["cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04271v1", "summary": "With the advancement of Artificial Intelligence (AI) towards multiple\nmodalities (language, vision, speech, etc.), multi-modal models have\nincreasingly been used across various applications (e.g., visual question\nanswering or image generation/captioning). Despite the success of AI as a\nservice for multi-modal applications, it relies heavily on clouds, which are\nconstrained by bandwidth, latency, privacy concerns, and unavailability under\nnetwork or server failures. While on-device AI becomes popular, supporting\nmultiple tasks on edge devices imposes significant resource challenges. To\naddress this, we introduce S2M3, a split-and-share multi-modal architecture for\nmulti-task inference on edge devices. Inspired by the general-purpose nature of\nmulti-modal models, which are composed of multiple modules (encoder, decoder,\nclassifier, etc.), we propose to split multi-modal models at functional-level\nmodules; and then share common modules to reuse them across tasks, thereby\nreducing resource usage. To address cross-model dependency arising from module\nsharing, we propose a greedy module-level placement with per-request parallel\nrouting by prioritizing compute-intensive modules. Through experiments on a\ntestbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,\nwe demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in\nsingle-task and multi-task settings, respectively, without sacrificing\naccuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95\ninstances (93.7%) while reducing inference latency by up to 56.9% on\nresource-constrained devices, compared to cloud AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04271v1", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04284", "title": "Optimizing Microgrid Composition for Sustainable Data Centers", "authors": ["Julius Irion", "Philipp Wiesner", "Jonathan Bader", "Odej Kao"], "categories": ["cs.DC", "eess.SY"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04284v1", "summary": "As computing energy demand continues to grow and electrical grid\ninfrastructure struggles to keep pace, an increasing number of data centers are\nbeing planned with colocated microgrids that integrate on-site renewable\ngeneration and energy storage. However, while existing research has examined\nthe tradeoffs between operational and embodied carbon emissions in the context\nof renewable energy certificates, there is a lack of tools to assess how the\nsizing and composition of microgrid components affects long-term sustainability\nand power reliability.\n  In this paper, we present a novel optimization framework that extends the\ncomputing and energy system co-simulator Vessim with detailed renewable energy\ngeneration models from the National Renewable Energy Laboratory's (NREL) System\nAdvisor Model (SAM). Our framework simulates the interaction between computing\nworkloads, on-site renewable production, and energy storage, capturing both\noperational and embodied emissions. We use a multi-horizon black-box\noptimization to explore efficient microgrid compositions and enable operators\nto make more informed decisions when planning energy systems for data centers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04284v1", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04334", "title": "Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing", "authors": ["Noor Islam S. Mohammad"], "categories": ["cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04334v1", "summary": "The rapid growth of Internet of Things (IoT) devices produces massive,\nheterogeneous data streams, demanding scalable and efficient scheduling in\ncloud environments to meet latency, energy, and Quality-of-Service (QoS)\nrequirements. Existing scheduling methods often lack adaptability to dynamic\nworkloads and network variability inherent in IoT-cloud systems. This paper\npresents a novel hybrid scheduling algorithm combining deep Reinforcement\nLearning (RL) and Ant Colony Optimization (ACO) to address these challenges.\nThe deep RL agent utilizes a model-free policy-gradient approach to learn\nadaptive task allocation policies responsive to real-time workload fluctuations\nand network states. Simultaneously, the ACO metaheuristic conducts a global\ncombinatorial search to optimize resource distribution, mitigate congestion,\nand balance load across distributed cloud nodes. Extensive experiments on\nlarge-scale synthetic IoT datasets, reflecting diverse workloads and QoS\nconstraints, demonstrate that the proposed method achieves up to 18.4%\nreduction in average response time, 12.7% improvement in resource utilization,\nand 9.3% decrease in energy consumption compared to leading heuristics and\nRL-only baselines. Moreover, the algorithm ensures strict Service Level\nAgreement (SLA) compliance through deadline-aware scheduling and dynamic\nprioritization. The results confirm the effectiveness of integrating model-free\nRL with swarm intelligence for scalable, energy-efficient IoT data scheduling,\noffering a promising approach for next-generation IoT-cloud platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04334v1", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04596", "title": "Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis", "authors": ["Chuan-Chi Lai", "Yan-Lin Chen", "Bo-Xin Liu", "Chuan-Ming Liu"], "categories": ["cs.DC", "cs.NI"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04596v1", "summary": "Due to the Internet of Everything (IoE), data generated in our life become\nlarger. As a result, we need more effort to analyze the data and extract\nvaluable information. In the cloud computing environment, all data analysis is\ndone in the cloud, and the client only needs less computing power to handle\nsome simple tasks. However, with the rapid increase in data volume, sending all\ndata to the cloud via the Internet has become more expensive. The required\ncloud computing resources have also become larger. To solve this problem, edge\ncomputing is proposed. Edge is granted with more computation power to process\ndata before sending it to the cloud. Therefore, the data transmitted over the\nInternet and the computing resources required by the cloud can be effectively\nreduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline\n(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the\nconcept of skyline candidate set to prune data that are less likely to become\nthe skyline data on the parallel edge computing nodes. With the candidate\nskyline set, each edge computing node only sends the information required to\nthe server for updating the global skyline, which reduces the amount of data\nthat transfer over the internet. According to the simulation results, the\nproposed method is better than two comparative methods, which reduces the\nlatency of processing two-dimensional data by more than 50%. For\nhigh-dimensional data, the proposed EPUS method also outperforms the other\nexisting methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04596v1", "cate": "cs.DC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04077", "title": "The Ubiquitous Sparse Matrix-Matrix Products", "authors": ["AydÄ±n BuluÃ§"], "categories": ["cs.DC", "cs.LG", "cs.MS", "math.CO", "math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04077", "summary": "Multiplication of a sparse matrix with another (dense or sparse) matrix is a\nfundamental operation that captures the computational patterns of many data\nscience applications, including but not limited to graph algorithms, sparsely\nconnected neural networks, graph neural networks, clustering, and many-to-many\ncomparisons of biological sequencing data.\n  In many application scenarios, the matrix multiplication takes places on an\narbitrary algebraic semiring where the scalar operations are overloaded with\nuser-defined functions with certain properties or a more general heterogenous\nalgebra where even the domains of the input matrices can be different. Here, we\nprovide a unifying treatment of the sparse matrix-matrix operation and its rich\napplication space including machine learning, computational biology and\nchemistry, graph algorithms, and scientific computing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04077", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04171", "title": "Advantages of Co-locating Quantum-HPC Platforms: A Survey for Near-Future Industrial Applications", "authors": ["Daigo Honda", "Yuta Nishiyama", "Junya Ishikawa", "Kenichi Matsuzaki", "Satoshi Miyata", "Tadahiro Chujo", "Yasuhisa Yamamoto", "Masahiko Kiminami", "Taro Kato", "Jun Towada", "Naoki Yoshioka", "Naoto Aoki", "Nobuyasu Ito"], "categories": ["cs.DC", "cs.ET", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04171", "summary": "We conducted a systematic survey of emerging quantum-HPC platforms, which\nintegrate quantum computers and High-Performance Computing (HPC) systems\nthrough co-location. Currently, it remains unclear whether such platforms\nprovide tangible benefits for near-future industrial applications. To address\nthis, we examined the impact of co-location on latency reduction, bandwidth\nenhancement, and advanced job scheduling. Additionally, we assessed how\nHPC-level capabilities could enhance hybrid algorithm performance, support\nlarge-scale error mitigation, and facilitate complex quantum circuit\npartitioning and optimization. Our findings demonstrate that co-locating\nquantum and HPC systems can yield measurable improvements in overall hybrid job\nthroughput. We also observe that large-scale real-world problems can require\nHPC-level computational resources for executing hybrid algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04171", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04217", "title": "Dynamic Solutions for Hybrid Quantum-HPC Resource Allocation", "authors": ["Roberto Rocco", "Simone Rizzo", "Matteo Barbieri", "Gabriella Bettonte", "Elisabetta Boella", "Fulvio Ganz", "Sergio Iserte", "Antonio J. PeÃ±a", "Petter SandÃ¥s", "Alberto Scionti", "Olivier Terzo", "Chiara Vercellino", "Giacomo Vitali", "Paolo Viviani", "Jonathan Frassineti", "Sara Marzella", "Daniele Ottaviani", "Iacopo Colonnelli", "Daniele Gregori"], "categories": ["cs.DC", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04217", "summary": "The integration of quantum computers within classical High-Performance\nComputing (HPC) infrastructures is receiving increasing attention, with the\nformer expected to serve as accelerators for specific computational tasks.\nHowever, combining HPC and quantum computers presents significant technical\nchallenges, including resource allocation. This paper presents a novel\nmalleability-based approach, alongside a workflow-based strategy, to optimize\nresource utilization in hybrid HPC-quantum workloads. With both these\napproaches, we can release classical resources when computations are offloaded\nto the quantum computer and reallocate them once quantum processing is\ncomplete. Our experiments with a hybrid HPC-quantum use case show the benefits\nof dynamic allocation, highlighting the potential of those solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04217", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04526", "title": "Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions", "authors": ["Fannya R. Sandjaja", "Ayesha A. Majeed", "Abdullah Abdullah", "Gyan Wickremasinghe", "Karen Rafferty", "Vishal Sharma"], "categories": ["cs.DC", "cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04526", "summary": "Traditional security architectures are becoming more vulnerable to\ndistributed attacks due to significant dependence on trust. This will further\nescalate when implementing agentic AI within the systems, as more components\nmust be secured over a similar distributed space. These scenarios can be\nobserved in consumer technologies, such as the dense Internet of things (IoT).\nHere, zero-trust architecture (ZTA) can be seen as a potential solution, which\nrelies on a key principle of not giving users explicit trust, instead always\nverifying their privileges whenever a request is made. However, the overall\nsecurity in ZTA is managed through its policies, and unverified policies can\nlead to unauthorized access. Thus, this paper explores challenges and solutions\nfor ZTA policy design in the context of distributed networks, which is referred\nto as zero-trust distributed networks (ZTDN). This is followed by a case-study\non formal verification of policies using UPPAAL. Subsequently, the importance\nof accountability and responsibility in the system's security is discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04526", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2411.12456", "title": "Ichnos: A Carbon Footprint Estimator for Scientific Workflows", "authors": ["Kathleen West", "Magnus Reid", "Yehia Elkhatib", "Lauritz Thamsen"], "categories": ["cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.12456", "summary": "Scientific workflows facilitate the automation of data analysis, and are used\nto process increasing amounts of data. Therefore, they tend to be\nresource-intensive and long-running, leading to significant energy consumption\nand carbon emissions. With ever-increasing emissions from the ICT sector, it is\ncrucial to quantify and understand the carbon footprint of scientific\nworkflows. However, existing tooling requires significant effort from users -\nsuch as setting up power monitoring before executing workloads, or translating\nmonitored metrics into the carbon footprints post-execution. In this paper, we\nintroduce a system to estimate the carbon footprint of Nextflow scientific\nworkflows that enables post-hoc estimation based on existing workflow traces,\npower models for computational resources utilised, and carbon intensity data\naligned with the execution time. We discuss our automated power modelling\napproach, and compare it with commonly used estimation methodologies.\nFurthermore, we exemplify several potential use cases and evaluate our energy\nconsumption estimation approach, finding its estimation error to be between\n3.9-10.3%, outperforming both baseline methodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.12456", "cate": "cs.DC", "date": "2024-11-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.03123", "title": "Dual-Expert Consistency Model for Efficient and High-Quality Video Generation", "authors": ["Zhengyao Lv", "Chenyang Si", "Tianlin Pan", "Zhaoxi Chen", "Kwan-Yee K. Wong", "Yu Qiao", "Ziwei Liu"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03123", "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03123", "cate": "cs.CV", "date": "2025-06-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.05972", "title": "A Comprohensive Review of Domain Adaptation Techniques for Agricultural Image Analysis in Precision Agriculture", "authors": ["Xing Hu", "Siyuan Chen", "Xuming Huang", "Qianqian Duan", "Huiliang Shang", "Dawei Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05972", "summary": "With the growing application of computer vision in agriculture, image\nanalysis has become essential for tasks such as crop health monitoring and pest\ndetection. However, significant domain shifts caused by environmental\nvariations, different crop types, and diverse data acquisition methods hinder\nmodel generalization across regions, seasons, and complex agricultural\nsettings. This paper investigates how Domain Adaptation (DA) techniques can\naddress these challenges by improving cross-domain transferability in\nagricultural image analysis. Given the limited availability of labeled data,\nweak model adaptability, and dynamic field conditions, DA has emerged as a\npromising solution. The review systematically summarizes recent advances in DA\nfor agricultural imagery, focusing on applications such as crop health\nmonitoring, pest detection, and fruit recognition, where DA methods have\nimproved performance in diverse domains. DA approaches are categorized into\nshallow and deep learning methods, including supervised, semi-supervised, and\nunsupervised strategies, with particular attention to adversarial\nlearning-based techniques that have demonstrated strong potential in complex\nscenarios. In addition,the paper reviews key public agricultural image\ndatasets, evaluating their strengths and limitations in DA research. In\ngeneral, this work offers a comprehensive framework and critical insights to\nguide future research and development of domain adaptation in agricultural\nvision tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05972", "cate": "cs.CV", "date": "2025-06-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.11773", "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments", "authors": ["Zikang Leng", "Megha Thukral", "Yaqi Liu", "Hrudhai Rajasekhar", "Shruthi K. Hiremath", "Jiaman He", "Thomas PlÃ¶tz"], "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11773", "summary": "A major challenge in developing robust and generalizable Human Activity\nRecognition (HAR) systems for smart homes is the lack of large and diverse\nlabeled datasets. Variations in home layouts, sensor configurations, and\nindividual behaviors further exacerbate this issue. To address this, we\nleverage the idea of embodied AI agents-virtual agents that perceive and act\nwithin simulated environments guided by internal world models. We introduce\nAgentSense, a virtual data generation pipeline in which agents live out daily\nroutines in simulated smart homes, with behavior guided by Large Language\nModels (LLMs). The LLM generates diverse synthetic personas and realistic\nroutines grounded in the environment, which are then decomposed into\nfine-grained actions. These actions are executed in an extended version of the\nVirtualHome simulator, which we augment with virtual ambient sensors that\nrecord the agents' activities. Our approach produces rich, privacy-preserving\nsensor data that reflects real-world diversity. We evaluate AgentSense on five\nreal HAR datasets. Models pretrained on the generated data consistently\noutperform baselines, especially in low-resource settings. Furthermore,\ncombining the generated virtual sensor data with a small amount of real data\nachieves performance comparable to training on full real-world datasets. These\nresults highlight the potential of using LLM-guided embodied agents for\nscalable and cost-effective sensor data generation in HAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11773", "cate": "cs.CV", "date": "2025-06-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.20294", "title": "Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations", "authors": ["Shunqi Mao", "Wei Guo", "Chaoyi Zhang", "Jieting Long", "Ke Xie", "Weidong Cai"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.20294", "summary": "Diffusion models have shown strong performance in conditional generation by\nprogressively denoising Gaussian samples toward a target data distribution.\nThis denoising process can be interpreted as a form of hill climbing in a\nlearned latent space, where the model iteratively refines a sample toward\nregions of higher probability. However, this learned climbing often converges\nto local optima with plausible but suboptimal generations due to latent space\ncomplexity and suboptimal initialization. While prior efforts often strengthen\nguidance signals or introduce fixed exploration strategies to address this,\nthey exhibit limited capacity to escape steep local maxima. In contrast, we\npropose Controlled Random Zigzag Sampling (Ctrl-Z Sampling), a novel sampling\nstrategy that adaptively detects and escapes such traps through controlled\nexploration. In each diffusion step, we first identify potential local maxima\nusing a reward model. Upon such detection, we inject noise and revert to a\nprevious, noisier state to escape the current plateau. The reward model then\nevaluates candidate trajectories, accepting only those that offer improvement,\notherwise scheming progressively deeper explorations when nearby alternatives\nfail. This controlled zigzag process allows dynamic alternation between forward\nrefinement and backward exploration, enhancing both alignment and visual\nquality in the generated outputs. The proposed method is model-agnostic and\nalso compatible with existing diffusion frameworks. Experimental results show\nthat Ctrl-Z Sampling substantially improves generation quality with only around\n6.72x increase in the number of function evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20294", "cate": "cs.CV", "date": "2025-06-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.20879", "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.20879", "summary": "Generation of images containing multiple humans, performing complex actions,\nwhile preserving their facial identities, is a significant challenge. A major\nfactor contributing to this is the lack of a dedicated benchmark. To address\nthis, we introduce MultiHuman-Testbench, a novel benchmark for rigorously\nevaluating generative models for multi-human generation. The benchmark\ncomprises 1800 samples, including carefully curated text prompts, describing a\nrange of simple to complex human actions. These prompts are matched with a\ntotal of 5,550 unique human face images, sampled uniformly to ensure diversity\nacross age, ethnic background, and gender. Alongside captions, we provide\nhuman-selected pose conditioning images which accurately match the prompt. We\npropose a multi-faceted evaluation suite employing four key metrics to quantify\nface count, ID similarity, prompt alignment, and action detection. We conduct a\nthorough evaluation of a diverse set of models, including zero-shot approaches\nand training-based methods, with and without regional priors. We also propose\nnovel techniques to incorporate image and region isolation using human\nsegmentation and Hungarian matching, significantly improving ID similarity. Our\nproposed benchmark and key findings provide valuable insights and a\nstandardized tool for advancing research in multi-human image generation. The\ndataset and evaluation codes will be available at\nhttps://github.com/Qualcomm-AI-research/MultiHuman-Testbench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20879", "cate": "cs.CV", "date": "2025-06-25", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.21008", "title": "The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion", "authors": ["Bang Gong", "Luchao Qi", "Jiaye Wu", "Zhicheng Fu", "Chunbo Song", "David W. Jacobs", "John Nicholson", "Roni Sengupta"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21008", "summary": "We introduce the Aging Multiverse, a framework for generating multiple\nplausible facial aging trajectories from a single image, each conditioned on\nexternal factors such as environment, health, and lifestyle. Unlike prior\nmethods that model aging as a single deterministic path, our approach creates\nan aging tree that visualizes diverse futures. To enable this, we propose a\ntraining-free diffusion-based method that balances identity preservation, age\naccuracy, and condition control. Our key contributions include attention mixing\nto modulate editing strength and a Simulated Aging Regularization strategy to\nstabilize edits. Extensive experiments and user studies demonstrate\nstate-of-the-art performance across identity preservation, aging realism, and\nconditional alignment, outperforming existing editing and age-progression\nmodels, which often fail to account for one or more of the editing criteria. By\ntransforming aging into a multi-dimensional, controllable, and interpretable\nprocess, our approach opens up new creative and practical avenues in digital\nstorytelling, health education, and personalized visualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21008", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.23518", "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image", "authors": ["Jiwoo Park", "Tae Eun Choi", "Youngjun Jun", "Seong Jae Hwang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23518", "summary": "Generating high-quality novel views of a scene from a single image requires\nmaintaining structural coherence across different views, referred to as view\nconsistency. While diffusion models have driven advancements in novel view\nsynthesis, they still struggle to preserve spatial continuity across views.\nDiffusion models have been combined with 3D models to address the issue, but\nsuch approaches lack efficiency due to their complex multi-step pipelines. This\npaper proposes a novel view-consistent image generation method which utilizes\ndiffusion models without additional modules. Our key idea is to enhance\ndiffusion models with a training-free method that enables adaptive attention\nmanipulation and noise reinitialization by leveraging view-guided warping to\nensure view consistency. Through our comprehensive metric framework suitable\nfor novel-view datasets, we show that our method improves view consistency\nacross various diffusion models, demonstrating its broader applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23518", "cate": "cs.CV", "date": "2025-06-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.03905", "title": "EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation", "authors": ["Rang Meng", "Yan Wang", "Weipeng Wu", "Ruobing Zheng", "Yuming Li", "Chenguang Ma"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03905", "summary": "Recent work on human animation usually incorporates large-scale video models,\nthereby achieving more vivid performance. However, the practical use of such\nmethods is hindered by the slow inference speed and high computational demands.\nMoreover, traditional work typically employs separate models for each animation\ntask, increasing costs in multi-task scenarios and worsening the dilemma. To\naddress these limitations, we introduce EchoMimicV3, an efficient framework\nthat unifies multi-task and multi-modal human animation. At the core of\nEchoMimicV3 lies a threefold design: a Soup-of-Tasks paradigm, a Soup-of-Modals\nparadigm, and a novel training and inference strategy. The Soup-of-Tasks\nleverages multi-task mask inputs and a counter-intuitive task allocation\nstrategy to achieve multi-task gains without multi-model pains. Meanwhile, the\nSoup-of-Modals introduces a Coupled-Decoupled Multi-Modal Cross Attention\nmodule to inject multi-modal conditions, complemented by a Multi-Modal Timestep\nPhase-aware Dynamical Allocation mechanism to modulate multi-modal mixtures.\nBesides, we propose Negative Direct Preference Optimization, Phase-aware\nNegative Classifier-Free Guidance (CFG), and Long Video CFG, which ensure\nstable training and inference. Extensive experiments and analyses demonstrate\nthat EchoMimicV3, with a minimal model size of 1.3 billion parameters, achieves\ncompetitive performance in both quantitative and qualitative evaluations. We\nare committed to open-sourcing our code for community use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03905", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.04061", "title": "Consistent and Invariant Generalization Learning for Short-video Misinformation Detection", "authors": ["Hanghui Guo", "Weijie Shi", "Mengze Li", "Juncheng Li", "Hao Chen", "Yue Cui", "Jiajie Xu", "Jia Zhu", "Jiawei Shen", "Zhangze Chen", "Sirui Han"], "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04061", "summary": "Short-video misinformation detection has attracted wide attention in the\nmulti-modal domain, aiming to accurately identify the misinformation in the\nvideo format accompanied by the corresponding audio. Despite significant\nadvancements, current models in this field, trained on particular domains\n(source domains), often exhibit unsatisfactory performance on unseen domains\n(target domains) due to domain gaps. To effectively realize such domain\ngeneralization on the short-video misinformation detection task, we propose\ndeep insights into the characteristics of different domains: (1) The detection\non various domains may mainly rely on different modalities (i.e., mainly\nfocusing on videos or audios). To enhance domain generalization, it is crucial\nto achieve optimal model performance on all modalities simultaneously. (2) For\nsome domains focusing on cross-modal joint fraud, a comprehensive analysis\nrelying on cross-modal fusion is necessary. However, domain biases located in\neach modality (especially in each frame of videos) will be accumulated in this\nfusion process, which may seriously damage the final identification of\nmisinformation. To address these issues, we propose a new DOmain generalization\nmodel via ConsisTency and invariance learning for shORt-video misinformation\ndetection (named DOCTOR), which contains two characteristic modules: (1) We\ninvolve the cross-modal feature interpolation to map multiple modalities into a\nshared space and the interpolation distillation to synchronize multi-modal\nlearning; (2) We design the diffusion model to add noise to retain core\nfeatures of multi modal and enhance domain invariant features through\ncross-modal guided denoising. Extensive experiments demonstrate the\neffectiveness of our proposed DOCTOR model. Our code is public available at\nhttps://github.com/ghh1125/DOCTOR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04061", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.05970", "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval", "authors": ["Haiwen Li", "Delong Liu", "Zhaohui Hou", "Zhicheng Zhao", "Fei Su"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05970", "summary": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05970", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.10225", "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection", "authors": ["Jinglun Li", "Kaixun Jiang", "Zhaoyu Chen", "Bo Lin", "Yao Tang", "Weifeng Ge", "Wenqiang Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10225", "summary": "Pre-trained vision-language models have exhibited remarkable abilities in\ndetecting out-of-distribution (OOD) samples. However, some challenging OOD\nsamples, which lie close to in-distribution (InD) data in image feature space,\ncan still lead to misclassification. The emergence of foundation models like\ndiffusion models and multimodal large language models (MLLMs) offers a\npotential solution to this issue. In this work, we propose SynOOD, a novel\napproach that harnesses foundation models to generate synthetic, challenging\nOOD data for fine-tuning CLIP models, thereby enhancing boundary-level\ndiscrimination between InD and OOD samples. Our method uses an iterative\nin-painting process guided by contextual prompts from MLLMs to produce nuanced,\nboundary-aligned OOD samples. These samples are refined through noise\nadjustments based on gradients from OOD scores like the energy score,\neffectively sampling from the InD/OOD boundary. With these carefully\nsynthesized images, we fine-tune the CLIP image encoder and negative label\nfeatures derived from the text encoder to strengthen connections between\nnear-boundary OOD samples and a set of negative labels. Finally, SynOOD\nachieves state-of-the-art performance on the large-scale ImageNet benchmark,\nwith minimal increases in parameters and runtime. Our approach significantly\nsurpasses existing methods, and the code is available at\nhttps://github.com/Jarvisgivemeasuit/SynOOD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10225", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.17342", "title": "DeMo++: Motion Decoupling for Autonomous Driving", "authors": ["Bozhou Zhang", "Nan Song", "Xiatian Zhu", "Li Zhang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17342", "summary": "Motion forecasting and planning are tasked with estimating the trajectories\nof traffic agents and the ego vehicle, respectively, to ensure the safety and\nefficiency of autonomous driving systems in dynamically changing environments.\nState-of-the-art methods typically adopt a one-query-one-trajectory paradigm,\nwhere each query corresponds to a unique trajectory for predicting multi-mode\ntrajectories. While this paradigm can produce diverse motion intentions, it\noften falls short in modeling the intricate spatiotemporal evolution of\ntrajectories, which can lead to collisions or suboptimal outcomes. To overcome\nthis limitation, we propose DeMo++, a framework that decouples motion\nestimation into two distinct components: holistic motion intentions to capture\nthe diverse potential directions of movement, and fine spatiotemporal states to\ntrack the agent's dynamic progress within the scene and enable a\nself-refinement capability. Further, we introduce a cross-scene trajectory\ninteraction mechanism to explore the relationships between motions in adjacent\nscenes. This allows DeMo++ to comprehensively model both the diversity of\nmotion intentions and the spatiotemporal evolution of each trajectory. To\neffectively implement this framework, we developed a hybrid model combining\nAttention and Mamba. This architecture leverages the strengths of both\nmechanisms for efficient scene information aggregation and precise trajectory\nstate sequence modeling. Extensive experiments demonstrate that DeMo++ achieves\nstate-of-the-art performance across various benchmarks, including motion\nforecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and\nend-to-end planning (NAVSIM).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17342", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.19264", "title": "SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality", "authors": ["Sijie Li", "Chen Chen", "Jungong Han"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19264", "summary": "In this paper, we propose SimMLM, a simple yet powerful framework for\nmultimodal learning with missing modalities. Unlike existing approaches that\nrely on sophisticated network architectures or complex data imputation\ntechniques, SimMLM provides a generic and effective solution that can adapt to\nvarious missing modality scenarios with improved accuracy and robustness.\nSpecifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts\n(DMoME) architecture, featuring a dynamic, learnable gating mechanism that\nautomatically adjusts each modality's contribution in both full and partial\nmodality settings. A key innovation of SimMLM is the proposed More vs. Fewer\n(MoFe) ranking loss, which ensures that task accuracy improves or remains\nstable as more modalities are made available. This aligns the model with an\nintuitive principle: removing one or more modalities should not increase\naccuracy. We validate SimMLM on multimodal medical image segmentation (BraTS\n2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it\nconsistently surpasses competitive methods, demonstrating superior accuracy,\ninterpretability, robustness, and reliability across both complete and missing\nmodality scenarios at test time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19264", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.22342", "title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views", "authors": ["Yuki Fujimura", "Takahiro Kushida", "Kazuya Kitano", "Takuya Funatomi", "Yasuhiro Mukaigawa"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22342", "summary": "This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS)\nframework designed to handle unfavorable input views. A common rendering setup\nfor training feed-forward approaches places a 3D object at the world origin and\nrenders it from cameras pointed toward the origin -- i.e., from favorable\nviews, limiting the applicability of these models to real-world scenarios\ninvolving varying and unknown camera poses. To overcome this limitation, we\nintroduce a novel adaptation framework that enables pretrained pose-free\nfeed-forward 3DGS models to handle unfavorable views. We leverage priors\nlearned from favorable images by feeding recentered images into a pretrained\nmodel augmented with low-rank adaptation (LoRA) layers. We further propose a\nGaussian adapter module to enhance the geometric consistency of the Gaussians\nderived from the recentered inputs, along with a Gaussian alignment method to\nrender accurate target views for training. Additionally, we introduce a new\ntraining strategy that utilizes an off-the-shelf dataset composed solely of\nfavorable images. Experimental results on both synthetic images from the Google\nScanned Objects dataset and real images from the OmniObject3D dataset validate\nthe effectiveness of our method in handling unfavorable input views.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22342", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.23284", "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval", "authors": ["Dohwan Ko", "Ji Soo Lee", "Minhyuk Choi", "Zihang Meng", "Hyunwoo J. Kim"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23284", "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23284", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.00553", "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models", "authors": ["Jizhihui Liu", "Feiyi Du", "Guangdao Zhu", "Niu Lian", "Jun Li", "Bin Chen"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00553", "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00553", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01095", "title": "AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions", "authors": ["Mikhail Bychkov", "Matey Yordanov", "Andrei Kuchma"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01095", "summary": "This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework\ndesigned for robust, real-time detection and classification of industrial smoke\nemissions. The framework addresses critical limitations of current monitoring\nsystems, which often lack the specificity to distinguish smoke types and\nstruggle with environmental variability. AURA leverages both the dynamic\nmovement patterns and the distinct color characteristics of industrial smoke to\nprovide enhanced accuracy and reduced false positives. This framework aims to\nsignificantly improve environmental compliance, operational safety, and public\nhealth outcomes by enabling precise, automated monitoring of industrial\nemissions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01095", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01227", "title": "Enhancing Multi-view Open-set Learning via Ambiguity Uncertainty Calibration and View-wise Debiasing", "authors": ["Zihan Fang", "Zhiyong Xu", "Lan Du", "Shide Du", "Zhiling Cai", "Shiping Wang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01227", "summary": "Existing multi-view learning models struggle in open-set scenarios due to\ntheir implicit assumption of class completeness. Moreover, static view-induced\nbiases, which arise from spurious view-label associations formed during\ntraining, further degrade their ability to recognize unknown categories. In\nthis paper, we propose a multi-view open-set learning framework via ambiguity\nuncertainty calibration and view-wise debiasing. To simulate ambiguous samples,\nwe design O-Mix, a novel synthesis strategy to generate virtual samples with\ncalibrated open-set ambiguity uncertainty. These samples are further processed\nby an auxiliary ambiguity perception network that captures atypical patterns\nfor improved open-set adaptation. Furthermore, we incorporate an HSIC-based\ncontrastive debiasing module that enforces independence between view-specific\nambiguous and view-consistent representations, encouraging the model to learn\ngeneralizable features. Extensive experiments on diverse multi-view benchmarks\ndemonstrate that the proposed framework consistently enhances unknown-class\nrecognition while preserving strong closed-set performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01227", "cate": "cs.CV", "date": "2025-08-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.19197", "title": "Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms", "authors": ["Calum Buchanan", "Puck Rombach", "James Bagrow", "Hamid R. Ossareh"], "categories": ["cs.DC", "math.CO", "math.OC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.19197", "summary": "A unit ball graph consists of a set of vertices, labeled by points in\nEuclidean space, and edges joining all pairs of points within distance 1. These\ngeometric graphs are used to model a variety of spatial networks, including\ncommunication networks between agents in an autonomous swarm. In such an\napplication, vertices and/or edges of the graph may not be perfectly reliable;\nan agent may experience failure or a communication link rendered inoperable.\nWith the goal of designing robust swarm formations, or unit ball graphs with\nhigh reliability (probability of connectedness), in a preliminary conference\npaper we provided an algorithm with cubic time complexity to determine all\npossible changes to a unit ball graph by repositioning a single vertex. Using\nthis algorithm and Monte Carlo simulations, one obtains an efficient method to\nmodify a unit ball graph by moving a single vertex to a location which\nmaximizes the reliability. Another important consideration in many swarm\nmissions is area coverage, yet highly reliable ball graphs often contain\nclusters of vertices. Here, we generalize our previous algorithm to improve\narea coverage as well as reliability. Our algorithm determines a location to\nadd or move a vertex within a unit ball graph which maximizes the reliability,\nunder the constraint that no other vertices of the graph be within some fixed\ndistance. We compare this method of obtaining graphs with high reliability and\nevenly distributed area coverage to another method which uses a modified\nFruchterman-Reingold algorithm for ball graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19197", "cate": "cs.DC", "date": "2025-06-23", "updated": "2025-08-05", "section": "repl"}
{"id": "2508.02520", "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384", "authors": ["Ao Xiao", "Bangzheng He", "Baoquan Zhang", "Baoxing Huai", "Bingji Wang", "Bo Wang", "Bo Xu", "Boyi Hou", "Chan Yang", "Changhong Liu", "Cheng Cui", "Chenyu Zhu", "Cong Feng", "Daohui Wang", "Dayun Lin", "Duo Zhao", "Fengshao Zou", "Fu Wang", "Gangqiang Zhang", "Gengyuan Dan", "Guanjie Chen", "Guodong Guan", "Guodong Yang", "Haifeng Li", "Haipei Zhu", "Hao Feng", "Hao Huang", "Hao Xu", "Hengrui Ma", "Hengtao Fan", "Hui Liu", "Jia Li", "Jiang Liu", "Jiang Xu", "Jie Meng", "Jinhan Xin", "Junhao Hu", "Juwei Chen", "Lan Yu", "Lanxin Miao", "Liang Liu", "Linan Jing", "Lu Zhou", "Meina Han", "Mingkun Deng", "Mingyu Deng", "Naitian Deng", "Nizhong Lin", "Peihan Zhao", "Peng Pan", "Pengfei Shen", "Ping Li", "Qi Zhang", "Qin Zhang", "Qingrong Xia", "Qingyi Zhang", "Qunchao Fu", "Ren Guo", "Ruimin Gao", "Shaochun Li", "Sheng Long", "Shentian Li", "Shining Wan", "Shuai Shen", "Shuangfu Zeng", "Shuming Jing", "Siqi Yang", "Song Zhang", "Tao Xu", "Tianlin Du", "Ting Chen", "Wanxu Wu", "Wei Jiang", "Weinan Tong", "Weiwei Chen", "Wen Peng", "Wenli Zhou", "Wenquan Yang", "Wenxin Liang", "Xiang Liu", "Xiaoli Zhou", "Xin Jin", "Xinyu Duan", "Xu Li", "Xu Zhang", "Xusheng Chen", "Yalong Shan", "Yang Gan", "Yao Lu", "Yi Deng", "Yi Zheng", "Yingfei Zheng", "Yiyun Zheng", "Yizhou Shan", "Yong Gao", "Yongqiang Yang", "Yuanjin Gong", "Yue Yu", "Yuetao Chen", "Yukun Zhu"], "categories": ["cs.DC"], "primary_category": "cs.DC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02520", "summary": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02520", "cate": "cs.DC", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.02842", "title": "A Study on 5G Network Slice Isolation Based on Native Cloud and Edge Computing Tools", "authors": ["Maiko Andrade", "Juliano Araujo Wickboldt"], "categories": ["cs.DC", "cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02842", "summary": "5G networks support various advanced applications through network slicing,\nnetwork function virtualization (NFV), and edge computing, ensuring low latency\nand service isolation. However, private 5G networks relying on open-source\ntools still face challenges in maturity and integration with edge/cloud\nplatforms, compromising proper slice isolation. This study investigates\nresource allocation mechanisms to address this issue, conducting experiments in\na hospital scenario with medical video conferencing. The results show that CPU\nlimitations improve the performance of prioritized slices, while memory\nrestrictions have minimal impact. The generated data and scripts have been made\npublicly available for future research and machine learning applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02842", "cate": "cs.NI", "date": "2025-02-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01602", "title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment", "authors": ["Lubin Gan", "Jing Zhang", "Linhao Qu", "Yijun Wang", "Siying Wu", "Xiaoyan Sun"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01602", "summary": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01602", "cate": "cs.CV", "date": "2025-08-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03060", "title": "CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation", "authors": ["Lekang Wen", "Jing Xiao", "Liang Liao", "Jiajun Chen", "Mi Wang"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03060", "summary": "Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene\nunderstanding across arbitrary combinations of input modality. Existing methods\ntypically rely on explicit feature alignment to achieve modal homogenization,\nwhich dilutes the distinctive strengths of each modality and destroys their\ninherent complementarity. To achieve cooperative harmonization rather than\nhomogenization, we propose CHARM, a novel complementary learning framework\ndesigned to implicitly align content while preserving modality-specific\nadvantages through two components: (1) Mutual Perception Unit (MPU), enabling\nimplicit alignment through window-based cross-modal interaction, where\nmodalities serve as both queries and contexts for each other to discover\nmodality-interactive correspondences; (2) A dual-path optimization strategy\nthat decouples training into Collaborative Learning Strategy (CoL) for\ncomplementary fusion learning and Individual Enhancement Strategy (InE) for\nprotected modality-specific optimization. Experiments across multiple datasets\nand backbones indicate that CHARM consistently outperform the baselines, with\nsignificant increment on the fragile modalities. This work shifts the focus\nfrom model homogenization to harmonization, enabling cross-modal\ncomplementarity for true harmony in diversity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03060", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03241", "title": "FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles", "authors": ["Xingchao Yang", "Shiori Ueda", "Yuantian Huang", "Tomoya Akiyama", "Takafumi Taketomi"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03241", "summary": "Paired bare-makeup facial images are essential for a wide range of\nbeauty-related tasks, such as virtual try-on, facial privacy protection, and\nfacial aesthetics analysis. However, collecting high-quality paired makeup\ndatasets remains a significant challenge. Real-world data acquisition is\nconstrained by the difficulty of collecting large-scale paired images, while\nexisting synthetic approaches often suffer from limited realism or\ninconsistencies between bare and makeup images. Current synthetic methods\ntypically fall into two categories: warping-based transformations, which often\ndistort facial geometry and compromise the precision of makeup; and\ntext-to-image generation, which tends to alter facial identity and expression,\nundermining consistency. In this work, we present FFHQ-Makeup, a high-quality\nsynthetic makeup dataset that pairs each identity with multiple makeup styles\nwhile preserving facial consistency in both identity and expression. Built upon\nthe diverse FFHQ dataset, our pipeline transfers real-world makeup styles from\nexisting datasets onto 18K identities by introducing an improved makeup\ntransfer method that disentangles identity and makeup. Each identity is paired\nwith 5 different makeup styles, resulting in a total of 90K high-quality\nbare-makeup image pairs. To the best of our knowledge, this is the first work\nthat focuses specifically on constructing a makeup dataset. We hope that\nFFHQ-Makeup fills the gap of lacking high-quality bare-makeup paired datasets\nand serves as a valuable resource for future research in beauty-related tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03241", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03324", "title": "Live Demonstration: Neuromorphic Radar for Gesture Recognition", "authors": ["Satyapreet Singh Yadav", "Akash K S", "Chandra Sekhar Seelamantula", "Chetan Singh Thakur"], "categories": ["cs.CV", "cs.ET", "cs.NE", "eess.SY"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03324", "summary": "We present a neuromorphic radar framework for real-time, low-power hand\ngesture recognition (HGR) using an event-driven architecture inspired by\nbiological sensing. Our system comprises a 24 GHz Doppler radar front-end and a\ncustom neuromorphic sampler that converts intermediate-frequency (IF) signals\ninto sparse spike-based representations via asynchronous sigma-delta encoding.\nThese events are directly processed by a lightweight neural network deployed on\na Cortex-M0 microcontroller, enabling low-latency inference without requiring\nspectrogram reconstruction. Unlike conventional radar HGR pipelines that\ncontinuously sample and process data, our architecture activates only when\nmeaningful motion is detected, significantly reducing memory, power, and\ncomputation overhead. Evaluated on a dataset of five gestures collected from\nseven users, our system achieves > 85% real-time accuracy. To the best of our\nknowledge, this is the first work that employs bio-inspired asynchronous\nsigma-delta encoding and an event-driven processing framework for radar-based\nHGR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03324", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03334", "title": "Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation", "authors": ["Xunzhi Xiang", "Yabo Chen", "Guiyu Zhang", "Zhongyu Wang", "Zhe Gao", "Quanming Xiang", "Gonghu Shang", "Junqi Liu", "Haibin Huang", "Yang Gao", "Chi Zhang", "Qi Fan", "Xuelong Li"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03334", "summary": "Current autoregressive diffusion models excel at video generation but are\ngenerally limited to short temporal durations. Our theoretical analysis\nindicates that the autoregressive modeling typically suffers from temporal\ndrift caused by error accumulation and hinders parallelization in long video\nsynthesis. To address these limitations, we propose a novel\nplanning-then-populating framework centered on Macro-from-Micro Planning (MMPL)\nfor long video generation. MMPL sketches a global storyline for the entire\nvideo through two hierarchical stages: Micro Planning and Macro Planning.\nSpecifically, Micro Planning predicts a sparse set of future keyframes within\neach short video segment, offering motion and appearance priors to guide\nhigh-quality video segment generation. Macro Planning extends the in-segment\nkeyframes planning across the entire video through an autoregressive chain of\nmicro plans, ensuring long-term consistency across video segments.\nSubsequently, MMPL-based Content Populating generates all intermediate frames\nin parallel across segments, enabling efficient parallelization of\nautoregressive generation. The parallelization is further optimized by Adaptive\nWorkload Scheduling for balanced GPU execution and accelerated autoregressive\nvideo generation. Extensive experiments confirm that our method outperforms\nexisting long video generation models in quality and stability. Generated\nvideos and comparison results are in our project page.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03334", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03337", "title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration", "authors": ["Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Ziyang Chen", "Weiyu Guo", "Hui Xiong"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03337", "summary": "The practical application of Multimodal Large Language Models (MLLMs) to\nVideo Question Answering (Video-QA) is severely hindered by the high token cost\nof processing numerous video frames. While increasing the number of sampled\nframes is a common strategy, we observe a \"less is more\" phenomenon where\nexcessive frames can paradoxically degrade performance due to context dilution.\nConcurrently, state-of-the-art keyframe selection methods, while effective,\nstill yield significant temporal redundancy, which we term 'visual echoes'. To\naddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel\npost-processing method that intelligently prunes the selected keyframes. AFP\nemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 and\nCLIP feature space to identify and merge these echoes into single\nrepresentatives. To compensate for information loss, we then introduce a\nlightweight, text-based semantic graph that provides critical context with\nminimal token overhead. Conducting extensive experiments on the LongVideoBench\nand VideoMME benchmarks across multiple leading MLLMs, our full approach\ndemonstrates a drastic reduction in required frames by up to 86.9% and total\ninput tokens by up to 83.2%. Crucially, by providing a concise, high-quality\nset of frames, our method not only enhances efficiency but often improves\naccuracy over baselines that use more frames. The code will be released upon\npublication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03337", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03388", "title": "Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation", "authors": ["Yizhe Xiong", "Zihan Zhou", "Yiwen Liang", "Hui Chen", "Zijia Lin", "Tianxiang Hao", "Fan Zhang", "Jungong Han", "Guiguang Ding"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03388", "summary": "Test-Time Adaptation (TTA) has emerged as an effective solution for adapting\nVision Transformers (ViT) to distribution shifts without additional training\ndata. However, existing TTA methods often incur substantial computational\noverhead, limiting their applicability in resource-constrained real-world\nscenarios. To reduce inference cost, plug-and-play token aggregation methods\nmerge redundant tokens in ViTs to reduce total processed tokens. Albeit\nefficient, it suffers from significant performance degradation when directly\nintegrated with existing TTA methods. We formalize this problem as Efficient\nTest-Time Adaptation (ETTA), seeking to preserve the adaptation capability of\nTTA while reducing inference latency. In this paper, we first provide a\ntheoretical analysis from a novel mutual information perspective, showing that\ntoken aggregation inherently leads to information loss, which cannot be fully\nmitigated by conventional norm-tuning-based TTA methods. Guided by this\ninsight, we propose to \\textbf{N}eutralize Token \\textbf{A}ggregation\n\\textbf{v}ia \\textbf{I}nformation \\textbf{A}ugmentation (\\textbf{NAVIA}).\nSpecifically, we directly augment the [CLS] token embedding and incorporate\nadaptive biases into the [CLS] token in shallow layers of ViTs. We\ntheoretically demonstrate that these augmentations, when optimized via entropy\nminimization, recover the information lost due to token aggregation. Extensive\nexperiments across various out-of-distribution benchmarks demonstrate that\nNAVIA significantly outperforms state-of-the-art methods by over 2.5\\%, while\nachieving an inference latency reduction of more than 20\\%, effectively\naddressing the ETTA challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03388", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03643", "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images", "authors": ["Xiangyu Sun", "Haoyi jiang", "Liu Liu", "Seungtae Nam", "Gyeongjin Kang", "Xinjie wang", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang", "Eunbyung Park"], "categories": ["cs.CV"], "primary_category": "cs.CV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03643", "summary": "Reconstructing and semantically interpreting 3D scenes from sparse 2D views\nremains a fundamental challenge in computer vision. Conventional methods often\ndecouple semantic understanding from reconstruction or necessitate costly\nper-scene optimization, thereby restricting their scalability and\ngeneralizability. In this paper, we introduce Uni3R, a novel feed-forward\nframework that jointly reconstructs a unified 3D scene representation enriched\nwith open-vocabulary semantics, directly from unposed multi-view images. Our\napproach leverages a Cross-View Transformer to robustly integrate information\nacross arbitrary multi-view inputs, which then regresses a set of 3D Gaussian\nprimitives endowed with semantic feature fields. This unified representation\nfacilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic\nsegmentation, and depth prediction, all within a single, feed-forward pass.\nExtensive experiments demonstrate that Uni3R establishes a new state-of-the-art\nacross multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on\nScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D\nscene reconstruction and understanding. The code is available at\nhttps://github.com/HorizonRobotics/Uni3R.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03643", "cate": "cs.CV", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2401.13330", "title": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks", "authors": ["Matteo Gambella", "Jary Pomponi", "Simone Scardapane", "Manuel Roveri"], "categories": ["cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.13330", "summary": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)\nwith Early Exit Classifiers (EECs), to provide predictions at intermediate\npoints of the processing when enough confidence in classification is achieved.\nThis leads to many benefits in terms of effectiveness and efficiency.\nCurrently, the design of EENNs is carried out manually by experts, a complex\nand time-consuming task that requires accounting for many aspects, including\nthe correct placement, the thresholding, and the computational overhead of the\nEECs. For this reason, the research is exploring the use of Neural Architecture\nSearch (NAS) to automatize the design of EENNs. Currently, few comprehensive\nNAS solutions for EENNs have been proposed in the literature, and a fully\nautomated, joint design strategy taking into consideration both the backbone\nand the EECs remains an open problem. To this end, this work presents Neural\nArchitecture Search for Hardware Constrained Early Exit Neural Networks\n(NACHOS), the first NAS framework for the design of optimal EENNs satisfying\nconstraints on the accuracy and the number of Multiply and Accumulate (MAC)\noperations performed by the EENNs at inference time. In particular, this\nprovides the joint design of backbone and EECs to select a set of admissible\n(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best\ntradeoff between the accuracy and number of MACs. The results show that the\nmodels designed by NACHOS are competitive with the state-of-the-art EENNs.\nAdditionally, this work investigates the effectiveness of two novel\nregularization terms designed for the optimization of the auxiliary classifiers\nof the EENN", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.13330", "cate": "cs.LG", "date": "2024-01-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.17557", "title": "BlurryScope enables compact, cost-effective scanning microscopy for HER2 scoring using deep learning on blurry images", "authors": ["Michael John Fanous", "Christopher Michael Seybold", "Hanlong Chen", "Nir Pillar", "Aydogan Ozcan"], "categories": ["cs.CV", "cs.LG", "eess.IV", "physics.med-ph"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.17557", "summary": "We developed a rapid scanning optical microscope, termed \"BlurryScope\", that\nleverages continuous image acquisition and deep learning to provide a\ncost-effective and compact solution for automated inspection and analysis of\ntissue sections. This device offers comparable speed to commercial digital\npathology scanners, but at a significantly lower price point and smaller\nsize/weight. Using BlurryScope, we implemented automated classification of\nhuman epidermal growth factor receptor 2 (HER2) scores on motion-blurred images\nof immunohistochemically (IHC) stained breast tissue sections, achieving\nconcordant results with those obtained from a high-end digital scanning\nmicroscope. Using a test set of 284 unique patient cores, we achieved testing\naccuracies of 79.3% and 89.7% for 4-class (0, 1+, 2+, 3+) and 2-class (0/1+,\n2+/3+) HER2 classification, respectively. BlurryScope automates the entire\nworkflow, from image scanning to stitching and cropping, as well as HER2 score\nclassification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.17557", "cate": "eess.IV", "date": "2024-10-23", "updated": "2025-08-06", "section": "repl"}
{"id": "2501.02786", "title": "CCStereo: Audio-Visual Contextual and Contrastive Learning for Binaural Audio Generation", "authors": ["Yuanhong Chen", "Kazuki Shimada", "Christian Simon", "Yukara Ikemiya", "Takashi Shibuya", "Yuki Mitsufuji"], "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.02786", "summary": "Binaural audio generation (BAG) aims to convert monaural audio to stereo\naudio using visual prompts, requiring a deep understanding of spatial and\nsemantic information. However, current models risk overfitting to room\nenvironments and lose fine-grained spatial details. In this paper, we propose a\nnew audio-visual binaural generation model incorporating an audio-visual\nconditional normalisation layer that dynamically aligns the mean and variance\nof the target difference audio features using visual context, along with a new\ncontrastive learning method to enhance spatial sensitivity by mining negative\nsamples from shuffled visual features. We also introduce a cost-efficient way\nto utilise test-time augmentation in video data to enhance performance. Our\napproach achieves state-of-the-art generation accuracy on the FAIR-Play and\nMUSIC-Stereo benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.02786", "cate": "cs.SD", "date": "2025-01-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.12609", "title": "VISO-Grasp: Vision-Language Informed Spatial Object-centric 6-DoF Active View Planning and Grasping in Clutter and Invisibility", "authors": ["Yitian Shi", "Di Wen", "Guanqi Chen", "Edgar Welte", "Sheng Liu", "Kunyu Peng", "Rainer Stiefelhagen", "Rania Rayyes"], "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12609", "summary": "We propose VISO-Grasp, a novel vision-language-informed system designed to\nsystematically address visibility constraints for grasping in severely occluded\nenvironments. By leveraging Foundation Models (FMs) for spatial reasoning and\nactive view planning, our framework constructs and updates an instance-centric\nrepresentation of spatial relationships, enhancing grasp success under\nchallenging occlusions. Furthermore, this representation facilitates active\nNext-Best-View (NBV) planning and optimizes sequential grasping strategies when\ndirect grasping is infeasible. Additionally, we introduce a multi-view\nuncertainty-driven grasp fusion mechanism that refines grasp confidence and\ndirectional uncertainty in real-time, ensuring robust and stable grasp\nexecution. Extensive real-world experiments demonstrate that VISO-Grasp\nachieves a success rate of $87.5\\%$ in target-oriented grasping with the fewest\ngrasp attempts outperforming baselines. To the best of our knowledge,\nVISO-Grasp is the first unified framework integrating FMs into target-aware\nactive view planning and 6-DoF grasping in environments with severe occlusions\nand entire invisibility constraints. Code is available at:\nhttps://github.com/YitianShi/vMF-Contact", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12609", "cate": "cs.RO", "date": "2025-03-16", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.19146", "title": "Risk-Based Thresholding for Reliable Anomaly Detection in Concentrated Solar Power Plants", "authors": ["Yorick Estievenart", "Sukanya Patra", "Souhaib Ben Taieb"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.19146", "summary": "Efficient and reliable operation of Concentrated Solar Power (CSP) plants is\nessential for meeting the growing demand for sustainable energy. However,\nhigh-temperature solar receivers face severe operational risks, such as\nfreezing, deformation, and corrosion, resulting in costly downtime and\nmaintenance. To monitor CSP plants, cameras mounted on solar receivers record\ninfrared images at irregular intervals ranging from one to five minutes\nthroughout the day. Anomalous images can be detected by thresholding an anomaly\nscore, where the threshold is chosen to optimize metrics such as the F1-score\non a validation set. This work proposes a framework, using risk control, for\ngenerating more reliable decision thresholds with finite-sample coverage\nguarantees on any chosen risk function. Our framework also incorporates an\nabstention mechanism, allowing high-risk predictions to be deferred to domain\nexperts. Second, we propose a density forecasting method to estimate the\nlikelihood of an observed image given a sequence of previously observed images,\nusing this likelihood as its anomaly score. Third, we analyze the deployment\nresults of our framework across multiple training scenarios over several months\nfor two CSP plants. This analysis provides valuable insights to our industry\npartner for optimizing maintenance operations. Finally, given the confidential\nnature of our dataset, we provide an extended simulated dataset, leveraging\nrecent advancements in generative modeling to create diverse thermal images\nthat simulate multiple CSP plants. Our code is publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.19146", "cate": "cs.LG", "date": "2025-03-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.09031", "title": "Confounder-Free Continual Learning via Recursive Feature Normalization", "authors": ["Yash Shah", "Camila Gonzalez", "Mohammad H. Abbasi", "Qingyu Zhao", "Kilian M. Pohl", "Ehsan Adeli"], "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09031", "summary": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09031", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.23777", "title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding", "authors": ["Dian Chen", "Yansong Qu", "Xinyang Li", "Ming Li", "Shengchuan Zhang"], "categories": ["cs.CV", "cs.GR", "cs.LG"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23777", "summary": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23777", "cate": "cs.GR", "date": "2025-07-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02408", "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction", "authors": ["Yikuang Yuluo", "Yue Ma", "Kuan Shen", "Tongtong Jin", "Wang Liao", "Yangpu Ma", "Fuquan Wang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02408", "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02408", "cate": "eess.IV", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03767", "title": "A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution", "authors": ["Sandeepa Kannangara", "Arman Abrahamyan", "Daniel Elias", "Thomas Kilby", "Nadav Dar", "Luiz Pizzato", "Anna Leontjeva", "Dan Jermyn"], "categories": ["cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.DB", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03767v1", "summary": "Entity resolution (ER) remains a significant challenge in data management,\nespecially when dealing with large datasets. This paper introduces MERAI\n(Massive Entity Resolution using AI), a robust and efficient pipeline designed\nto address record deduplication and linkage issues in high-volume datasets at\nan enterprise level. The pipeline's resilience and accuracy have been validated\nthrough various large-scale record deduplication and linkage projects. To\nevaluate MERAI's performance, we compared it with two well-known entity\nresolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2\nmillion records due to memory constraints, MERAI successfully processed\ndatasets of up to 15.7 million records and produced accurate results across all\nexperiments. Experimental data demonstrates that MERAI outperforms both\nbaseline systems in terms of matching accuracy, with consistently higher F1\nscores in both deduplication and record linkage tasks. MERAI offers a scalable\nand reliable solution for enterprise-level large-scale entity resolution,\nensuring data integrity and consistency in real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03767v1", "cate": "cs.DB", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03978", "title": "Raqlet: Cross-Paradigm Compilation for Recursive Queries", "authors": ["Amir Shaikhha", "Youning Xia", "Meisam Tarabkhah", "Jazal Saleem", "Anna Herlihy"], "categories": ["cs.DB", "cs.PL"], "primary_category": "cs.DB", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03978v1", "summary": "We introduce Raqlet, a source-to-source compilation framework that addresses\nthe fragmentation of recursive querying engines spanning relational (recursive\nSQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards\nsuch as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for\nquerying graph data within relational and graph databases; however, real-world\nsupport remains inconsistent across systems. Raqlet bridges this gap by\ntranslating recursive queries across paradigms through leveraging intermediate\nrepresentations (IRs) grounded in well-defined semantics; it translates Cypher\nor SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),\nand finally to SQIR (inspired by recursive SQL). Raqlet provides a shared\nsemantic basis that can serve as a golden reference implementation for language\nstandards, while supporting static analysis and transformations (e.g.,\nmagic-set transformation) for performance tuning. Our vision is to make Raqlet\na robust platform that enables rapid cross-paradigm prototyping, portable\nrecursive queries, and formal reasoning about recursion even when targeting\ndiverse query execution engines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03978v1", "cate": "cs.DB", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04031", "title": "BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases", "authors": ["Lianggui Weng", "Dandan Liu", "Rong Zhu", "Bolin Ding", "Jingren Zhou"], "categories": ["cs.DB"], "primary_category": "cs.DB", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04031v1", "summary": "As large language models (LLMs) demonstrate increasingly powerful reasoning\nand orchestration capabilities, LLM-based agents are rapidly proliferating for\ncomplex data-related tasks. Despite this progress, the current design of how\nLLMs interact with databases exhibits critical limitations in usability,\nsecurity, privilege management, and data transmission efficiency. To resolve\nthese challenges, we introduce BridgeScope, a universal toolkit bridging LLMs\nand databases through three key innovations. First, it modularizes SQL\noperations into fine-grained tools for context retrieval, CRUD execution, and\nACID-compliant transaction management, enabling more precise and LLM-friendly\nfunctionality controls. Second, it aligns tool implementations with both\ndatabase privileges and user security policies to steer LLMs away from unsafe\nor unauthorized operations, improving task execution efficiency while\nsafeguarding database security. Third, it introduces a proxy mechanism for\nseamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All\nof these designs are database-agnostic and can be transparently integrated with\nexisting agent architectures. We also release an open-source implementation of\nBridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate\nthat BridgeScope enables LLM agents to operate databases more effectively,\nreduces token usage by up to 80% through improved security awareness, and\nuniquely supports data-intensive workflows beyond existing toolkits,\nestablishing BridgeScope as a robust foundation for next-generation intelligent\ndata automation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04031v1", "cate": "cs.DB", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04701", "title": "Rethinking Analytical Processing in the GPU Era", "authors": ["Bobbi Yogatama", "Yifei Yang", "Kevin Kristensen", "Devesh Sarda", "Abigale Kim", "Adrian Cockcroft", "Yu Teng", "Joshua Patterson", "Gregory Kimball", "Wes McKinney", "Weiwei Gong", "Xiangyao Yu"], "categories": ["cs.DB"], "primary_category": "cs.DB", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04701v1", "summary": "The era of GPU-powered data analytics has arrived. In this paper, we argue\nthat recent advances in hardware (e.g., larger GPU memory, faster interconnect\nand IO, and declining cost) and software (e.g., composable data systems and\nmature libraries) have removed the key barriers that have limited the wider\nadoption of GPU data analytics. We present Sirius, a prototype open-source\nGPU-native SQL engine that offers drop-in acceleration for diverse data\nsystems. Sirius treats GPU as the primary engine and leverages libraries like\nlibcudf for high-performance relational operators. It provides drop-in\nacceleration for existing databases by leveraging the standard Substrait query\nrepresentation, replacing the CPU engine without changing the user-facing\ninterface. On TPC-H, Sirius achieves 7x speedup when integrated with DuckDB in\na single node at the same hardware rental cost, and up to 12.5x speedup when\nintegrated with Apache Doris in a distributed setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04701v1", "cate": "cs.DB", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2210.04179", "title": "Oze: Decentralized Graph-based Concurrency Control for Long-running Update Transactions (Extended Version)", "authors": ["Jun Nemoto", "Takashi Kambayashi", "Takashi Hoshino", "Hideyuki Kawashima"], "categories": ["cs.DB"], "primary_category": "cs.DB", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2210.04179", "summary": "This paper proposes Oze, a concurrency control protocol that handles\nheterogeneous workloads, including long-running update transactions. Oze\nexplores a large scheduling space using a multi-version serialization graph to\nreduce false positives. Oze manages the graph in a decentralized manner to\nexploit many cores in modern servers. We further propose an OLTP benchmark,\nBoMB (Bill of Materials Benchmark), based on a use case in an actual\nmanufacturing company. BoMB consists of one long-running update transaction and\nfive short transactions that conflict with each other. Experiments using BoMB\nshow that Oze can handle the long-running update transaction while achieving\nfour orders of magnitude higher throughput than state-of-the-art optimistic and\nmulti-version protocols and up to five times higher throughput than pessimistic\nprotocols. We also show Oze performs comparably with existing techniques even\nin a typical OLTP workload, TPC-C, thanks to a protocol switching mechanism.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2210.04179", "cate": "cs.DB", "date": "2022-10-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2305.01516", "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large Skewed Workloads", "authors": ["Konstantinos Kanellis", "Badrish Chandramouli", "Ted Hart", "Shivaram Venkataraman"], "categories": ["cs.DB"], "primary_category": "cs.DB", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2305.01516", "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2305.01516", "cate": "cs.DB", "date": "2023-05-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03457", "title": "READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation", "authors": ["Haotian Wang", "Yuzhe Weng", "Jun Du", "Haoran Xu", "Xiaoyan Wu", "Shan He", "Bing Yin", "Cong Liu", "Jianqing Gao", "Qingfeng Liu"], "categories": ["cs.CV", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03457", "summary": "The introduction of diffusion models has brought significant advances to the\nfield of audio-driven talking head generation. However, the extremely slow\ninference speed severely limits the practical implementation of diffusion-based\ntalking head generation models. In this study, we propose READ, the first\nreal-time diffusion-transformer-based talking head generation framework. Our\napproach first learns a spatiotemporal highly compressed video latent space via\na temporal VAE, significantly reducing the token count to accelerate\ngeneration. To achieve better audio-visual alignment within this compressed\nlatent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to\ngenerate temporally compressed speech latent codes corresponding to the video\nlatent space. These latent representations are then modeled by a carefully\ndesigned Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient\ntalking head synthesis. Furthermore, to ensure temporal consistency and\naccelerated inference in extended generation, we propose a novel asynchronous\nnoise scheduler (ANS) for both the training and inference process of our\nframework. The ANS leverages asynchronous add-noise and asynchronous\nmotion-guided generation in the latent space, ensuring consistency in generated\nvideo clips. Experimental results demonstrate that READ outperforms\nstate-of-the-art methods by generating competitive talking head videos with\nsignificantly reduced runtime, achieving an optimal balance between quality and\nspeed while maintaining robust metric stability in long-time generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03457", "cate": "cs.GR", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04612", "title": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature", "authors": ["Faruk Alpay", "Bugra Kilictas", "Hamdi Alakkad"], "categories": ["cs.DL", "cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04612", "summary": "The accelerating pace of research on autoregressive generative models has\nproduced thousands of papers, making manual literature surveys and reproduction\nstudies increasingly impractical. We present a fully open-source, reproducible\npipeline that automatically retrieves candidate documents from public\nrepositories, filters them for relevance, extracts metadata, hyper-parameters\nand reported results, clusters topics, produces retrieval-augmented summaries\nand generates containerised scripts for re-running selected experiments.\nQuantitative evaluation on 50 manually-annotated papers shows F1 scores above\n0.85 for relevance classification, hyper-parameter extraction and citation\nidentification. Experiments on corpora of up to 1000 papers demonstrate\nnear-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM\non WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model\non the Lakh MIDI dataset -- confirm that the extracted settings support\nfaithful reproduction, achieving test perplexities within 1--3% of the original\nreports.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04612", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.00836", "title": "Rxiv-Maker: an automated template engine for streamlined scientific publications", "authors": ["Bruno M. Saraiva", "Guillaume Jaquemet", "Ricardo Henriques"], "categories": ["cs.DL"], "primary_category": "cs.DL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00836", "summary": "Preprint servers accelerate research dissemination, but authors still face\ncomplex manuscript preparation without professional typesetting support.\nRxiv-Maker enables researchers to create documents using a framework that\nconverts Markdown into publication-standard PDFs. It automatically translates\nthe markdown text into LaTeX, so researchers don't have to write any LaTeX code\nthemselves. This tool transforms simple documents into dynamic,\nversion-controlled files that work well with modern team collaboration and\nongoing updates. Rxiv-Maker executes Python and R scripts for on-the-fly figure\ngeneration, ensuring visualisations stay current with data and analyses.\nAutomated build environments, Docker support, and built-in citation and\ncross-reference management ensure reliable, reproducible builds across systems,\nwhile the conversion process handles mathematical equations and formatting.\nRxiv-Maker simplifies professional typesetting, promoting clear and open\nscientific publishing. This manuscript, created with Rxiv-Maker, serves itself\nas a template for future users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00836", "cate": "cs.DL", "date": "2025-06-26", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03961", "title": "Decoupling via Affine Spectral-Independence: Beck-Fiala and KomlÃ³s Bounds Beyond Banaszczyk", "authors": ["Nikhil Bansal", "Haotian Jiang"], "categories": ["cs.DM", "cs.DS", "math.CO", "math.PR"], "primary_category": "math.CO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03961", "summary": "The Beck-Fiala Conjecture [Discrete Appl. Math, 1981] asserts that any set\nsystem of $n$ elements with degree $k$ has combinatorial discrepancy\n$O(\\sqrt{k})$. A substantial generalization is the Koml\\'os Conjecture, which\nstates that any $m \\times n$ matrix with unit length columns has discrepancy\n$O(1)$.\n  In this work, we resolve the Beck-Fiala Conjecture for $k \\geq \\log^2 n$. We\nalso give an $\\widetilde{O}(\\sqrt{k} + \\sqrt{\\log n})$ bound for $k \\leq \\log^2\nn$, where $\\widetilde{O}(\\cdot)$ hides $\\mathsf{poly}(\\log \\log n)$ factors.\nThese bounds improve upon the $O(\\sqrt{k \\log n})$ bound due to Banaszczyk\n[Random Struct. Algor., 1998].\n  For the Komlos problem, we give an $\\widetilde{O}(\\log^{1/4} n)$ bound,\nimproving upon the previous $O(\\sqrt{\\log n})$ bound [Random Struct. Algor.,\n1998]. All of our results also admit efficient polynomial-time algorithms.\n  To obtain these results, we consider a new notion of affine\nspectral-independence in designing random walks. In particular, our algorithms\nobtain the desired colorings via a discrete Brownian motion, guided by a\nsemidefinite program (SDP). Besides standard constraints used in prior works,\nwe add some extra affine spectral-independence constraints, which effectively\ndecouple the evolution of discrepancy across different rows, and allow us to\nbetter control how many rows accumulate large discrepancy at any point during\nthe process. This technique of ``decoupling via affine spectral-independence''\nis quite general and may be of independent interest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03961", "cate": "math.CO", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2308.01242", "title": "Balanced-chromatic number and Hadwiger-like conjectures", "authors": ["Andrea JimÃ©nez", "Jessica McDonald", "Reza Naserasr", "Kathryn Nurse", "Daniel A. Quiroz"], "categories": ["cs.DM", "math.CO"], "primary_category": "math.CO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2308.01242", "summary": "Motivated by different characterizations of planar graphs and the 4-Color\nTheorem, several structural results concerning graphs of high chromatic number\nhave been obtained. Toward strengthening some of these results, we consider the\n\\emph{balanced chromatic number}, $\\chi_b(\\hat{G})$, of a signed graph\n$\\hat{G}$. This is the minimum number of parts into which the vertices of a\nsigned graph can be partitioned so that none of the parts induces a negative\ncycle. This extends the notion of the chromatic number of a graph since\n$\\chi(G)=\\chi_b(\\tilde{G})$, where $\\tilde{G}$ denotes the signed graph\nobtained from~$G$ by replacing each edge with a pair of (parallel) positive and\nnegative edges. We introduce a signed version of Hadwiger's conjecture as\nfollows.\n  Conjecture: If a signed graph $\\hat{G}$ has no negative loop and no\n$\\tilde{K_t}$-minor, then its balanced chromatic number is at most $t-1$.\n  We prove that this conjecture is, in fact, equivalent to Hadwiger's\nconjecture and show its relation to the Odd Hadwiger Conjecture.\n  Motivated by these results, we also consider the relation between\nsubdivisions and balanced chromatic number. We prove that if $(G, \\sigma)$ has\nno negative loop and no $\\tilde{K_t}$-subdivision, then it admits a balanced\n$\\frac{79}{2}t^2$-coloring. This qualitatively generalizes a result of\nKawarabayashi (2013) on totally odd subdivisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2308.01242", "cate": "math.CO", "date": "2023-08-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03930", "title": "Counting Distinct Square Substrings in Sublinear Time", "authors": ["Panagiotis Charalampopoulos", "Manal Mohamed", "Jakub Radoszewski", "Wojciech Rytter", "Tomasz WaleÅ", "Wiktor Zuba"], "categories": ["cs.DS"], "primary_category": "cs.DS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03930v1", "summary": "We show that the number of distinct squares in a packed string of length $n$\nover an alphabet of size $\\sigma$ can be computed in $O(n/\\log_\\sigma n)$ time\nin the word-RAM model. This paper is the first to introduce a sublinear-time\nalgorithm for counting squares in the packed setting. The packed representation\nof a string of length $n$ over an alphabet of size $\\sigma$ is given as a\nsequence of $O(n/\\log_\\sigma n)$ machine words in the word-RAM model (a machine\nword consists of $\\omega \\ge \\log_2 n$ bits). Previously, it was known how to\ncount distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for\na string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al.,\nCPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for\nextracting squares from runs described by Crochemore et al. [TCS 2014].\nHowever, the packed model requires novel approaches.\n  We need an $O(n/\\log_\\sigma n)$-sized representation of all long-period runs\n(runs with period $\\Omega(\\log_\\sigma n)$) which allows for a sublinear-time\ncounting of the -- potentially linearly-many -- implied squares. The\nlong-period runs with a string period that is periodic itself (called layer\nruns) are an obstacle, since their number can be $\\Omega(n)$. The number of all\nother long-period runs is $O(n/\\log_\\sigma n)$ and we can construct an implicit\nrepresentation of all long-period runs in $O(n/\\log_\\sigma n)$ time by\nleveraging the insights of Amir et al. [ESA 2019]. We count squares in layer\nruns by exploiting combinatorial properties of pyramidally-shaped groups of\nlayer runs. Another difficulty lies in computing the locations of Lyndon roots\nof runs in packed strings, which is needed for grouping runs that may generate\nequal squares. To overcome this difficulty, we introduce sparse-Lyndon roots\nwhich are based on string synchronizers [Kempa and Kociumaka, STOC 2019].", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03930v1", "cate": "cs.DS", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04079", "title": "Exactly simulating stochastic chemical reaction networks in sub-constant time per reaction", "authors": ["Joshua Petrack", "David Doty"], "categories": ["cs.DS"], "primary_category": "cs.DS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04079v1", "summary": "The model of chemical reaction networks is among the oldest and most widely\nstudied and used in natural science. The model describes reactions among\nabstract chemical species, for instance $A + B \\to C$, which indicates that if\na molecule of type $A$ interacts with a molecule of type $B$ (the reactants),\nthey may stick together to form a molecule of type $C$ (the product). The\nstandard algorithm for simulating (discrete, stochastic) chemical reaction\nnetworks is the Gillespie algorithm [JPC 1977], which stochastically simulates\none reaction at a time, so to simulate $\\ell$ consecutive reactions, it\nrequires total running time $\\Omega(\\ell)$.\n  We give the first chemical reaction network stochastic simulation algorithm\nthat can simulate $\\ell$ reactions, provably preserving the exact stochastic\ndynamics (sampling from precisely the same distribution as the Gillespie\nalgorithm), yet using time provably sublinear in $\\ell$. Under reasonable\nassumptions, our algorithm can simulate $\\ell$ reactions among $n$ total\nmolecules in time $O(\\ell/\\sqrt n)$ when $\\ell \\ge n^{5/4}$, and in time\n$O(\\ell/n^{2/5})$ when $n \\le \\ell \\le n^{5/4}$. Our work adapts an algorithm\nof Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020] for\nsimulating the distributed computing model known as population protocols,\nextending it (in a very nontrivial way) to the more general chemical reaction\nnetwork setting.\n  We provide an implementation of our algorithm as a Python package, with the\ncore logic implemented in Rust, with remarkably fast performance in practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04079v1", "cate": "cs.DS", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04081", "title": "Exact Matching in Matrix Multiplication Time", "authors": ["Ryotaro Sato", "Yutaro Yamaguchi"], "categories": ["cs.DS", "cs.SC", "math.CO"], "primary_category": "cs.DS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04081v1", "summary": "Initiated by Mulmuley, Vazirani, and Vazirani (1987), many algebraic\nalgorithms have been developed for matching and related problems. In this\npaper, we review basic facts and discuss possible improvements with the aid of\nfast computation of the characteristic polynomial of a matrix. In particular,\nwe show that the so-called exact matching problem can be solved with high\nprobability in asymptotically the same time order as matrix multiplication. We\nalso discuss its extension to the linear matroid parity problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04081v1", "cate": "cs.DS", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04159", "title": "Approximation Algorithms for Scheduling Crowdsourcing Tasks in Mobile Social Networks", "authors": ["Chi-Yeh Chen"], "categories": ["cs.DS"], "primary_category": "cs.DS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04159v1", "summary": "This paper addresses the scheduling problem in mobile social networks. We\nbegin by proving that the approximation ratio analysis presented in the paper\nby Zhang \\textit{et al.} (IEEE Transactions on Mobile Computing, 2025) is\nincorrect, and we provide the correct analysis results. Furthermore, when the\nrequired service time for a task exceeds the total contact time between the\nrequester and the crowd worker, we demonstrate that the approximation ratio of\nthe Largest-Ratio-First task scheduling algorithm can reach $2 - \\frac{1}{m}$.\nNext, we introduce a randomized approximation algorithm to minimize mobile\nsocial networks' total weighted completion time. This algorithm achieves an\nexpected approximation ratio of $1.5 + \\epsilon$ for $\\epsilon>0$. Finally, we\npresent a deterministic approximation algorithm that minimizes mobile social\nnetworks' total weighted completion time. This deterministic algorithm achieves\nan approximation ratio of $\\max\\left\\{2.5,1+\\epsilon\\right\\}$ for $\\epsilon>0$.\nAdditionally, when the task's required service time or the total contact time\nbetween the requester and the crowd worker is sufficiently large, this\nalgorithm can reach an approximation ratio of $1.5+\\epsilon$ for $\\epsilon>0$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04159v1", "cate": "cs.DS", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03914", "title": "Moveless: Minimizing Overhead on QCCDs via Versatile Execution and Low Excess Shuttling", "authors": ["Sahil Khan", "Suhas Vittal", "Kenneth Brown", "Jonathan Baker"], "categories": ["cs.ET", "eess.SY", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03914", "summary": "One of the most promising paths towards large scale fault tolerant quantum\ncomputation is the use of quantum error correcting stabilizer codes. Just like\nevery other quantum circuit, these codes must be compiled to hardware in a way\nto minimize the total physical error introduced into the system, for example\neither due to high latency execution or excessive gates to meet connectivity\nlimitations of the target hardware. However, unlike arbitrary quantum circuits,\nall syndrome extraction circuits have several common properties, for example\nthey have a bipartite connectivity graph, consist only of commuting\nsubcircuits, among other properties. For the most part, compilation methods\nhave aimed at being generic, able to map any input circuit into executables on\nthe hardware, and therefore cannot appropriately exploit these properties and\nresult in executables which have higher physical error. In the case of modular\ntrapped ion systems, specifically QCCDs, this corresponds to the insertion of\nexcessive shuttling operations necessary to realize arbitrary qubit\ninteractions. We propose a compilation scheme explicitly tailored for the\nstructural regularity of QEC circuits based on several key observations: 1.\nonly ancilla or data (but not both) should be shuttled, 2. stabilizers can be\nexecuted in any order meaning we can dynamically modify circuit execution on a\nper-cycle basis 3. ancilla are indistinguishable meaning any can be selected to\nbegin a stabilizer measurement and retain a fixed-point mapping between cycles,\nand 4. QCCD hardware limits the number of parallel operations equal to the\nnumber traps in the system, meaning fewer ancilla are necessary and can be\nreused. Our resulting compiler, leads to QEC circuits which are on average\n3.38x faster to execute, and lead to up to two orders of magnitude of\nimprovement in logical error rates with realistic physical error rates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03914", "cate": "quant-ph", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2502.12020", "title": "Learning in a Multifield Coherent Ising Machine", "authors": ["Daan de Bos", "Marc Serra-Garcia"], "categories": ["cond-mat.dis-nn", "cond-mat.mes-hall", "cs.ET", "cs.NE", "nlin.AO"], "primary_category": "cond-mat.mes-hall", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.12020", "summary": "We introduce a network of coupled oscillators that can learn to solve a\nclassification task from a set of examples -- performing both training and\ninference through the nonlinear evolution of the system. We accomplish this by\ncombining three key elements to achieve learning: A long-term memory that\nstores learned responses, analogous to the synapses in biological brains; a\nshort-term memory that stores the neural activations, similar to the firing\npatterns of neurons; and an evolution law that updates the synapses in response\nto novel examples, inspired by synaptic plasticity. Achieving all three\nelements in wave-based information processors such as metamaterials is a\nsignificant challenge. Here, we solve it by leveraging the material\nmultistability to implement long-term memory, and harnessing symmetries and\nthermal noise to realize the learning rule. Our analysis reveals that the\nlearning mechanism, although inspired by synaptic plasticity, also shares\nparallelisms with bacterial evolution strategies, where mutation rates increase\nin the presence of noxious stimuli.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.12020", "cate": "cond-mat.mes-hall", "date": "2025-02-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03826", "title": "Identity Testing for Stochastic Languages", "authors": ["Smayan Agarwal", "Shobhit Singh", "Aalok Thakkar"], "categories": ["cs.FL"], "primary_category": "cs.FL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03826v1", "summary": "Determining whether an unknown distribution matches a known reference is a\ncornerstone problem in distributional analysis. While classical results\nestablish a rigorous framework in the case of distributions over finite\ndomains, real-world applications in computational linguistics, bioinformatics,\nand program analysis demand testing over infinite combinatorial structures,\nparticularly strings. In this paper, we initiate the theoretical study of\nidentity testing for stochastic languages, bridging formal language theory with\nmodern distribution property testing.\n  We first propose a polynomial-time algorithm to verify if a finite state\nmachine represents a stochastic language, and then prove that rational\nstochastic languages can approximate an arbitrary probability distribution.\nBuilding on these representations, we develop a truncation-based identity\ntesting algorithm that distinguishes between a known and an unknown\ndistributions with sample complexity $\\widetilde{\\Theta}\\left(\n\\frac{\\sqrt{n}}{\\varepsilon^2} + \\frac{n}{\\log n} \\right)$ where $n$ is the\nsize of the truncated support. Our approach leverages the exponential decay\ninherent in rational stochastic languages to bound truncation error, then\napplies classical finite-domain testers to the restricted problem.\n  This work establishes the first identity testing framework for infinite\ndiscrete distributions, opening new directions in probabilistic formal methods\nand statistical analysis of structured data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03826v1", "cate": "cs.FL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04458", "title": "Componentwise Automata Learning for System Integration (Extended Version)", "authors": ["Hiroya Fujinami", "Masaki Waga", "Jie An", "Kohei Suenaga", "Nayuta Yanagisawa", "Hiroki Iseri", "Ichiro Hasuo"], "categories": ["cs.FL"], "primary_category": "cs.FL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04458v1", "summary": "Compositional automata learning is attracting attention as an analysis\ntechnique for complex black-box systems. It exploits a target system's internal\ncompositional structure to reduce complexity. In this paper, we identify system\nintegration -- the process of building a new system as a composite of\npotentially third-party and black-box components -- as a new application domain\nof compositional automata learning. Accordingly, we propose a new problem\nsetting, where the learner has direct access to black-box components. This is\nin contrast with the usual problem settings of compositional learning, where\nthe target is a legacy black-box system and queries can only be made to the\nwhole system (but not to components). We call our problem componentwise\nautomata learning for distinction. We identify a challenge there called\ncomponent redundancies: some parts of components may not contribute to\nsystem-level behaviors, and learning them incurs unnecessary effort. We\nintroduce a contextual componentwise learning algorithm that systematically\nremoves such redundancies. We experimentally evaluate our proposal and show its\npractical relevance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04458v1", "cate": "cs.FL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04326", "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research", "authors": ["Ke Li", "Mana Masuda", "Susanne Schmidt", "Shohei Mori"], "categories": ["cs.GR"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04326v1", "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04326v1", "cate": "cs.GR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2412.03489", "title": "Stochastic Gradient Estimation for Higher-order Differentiable Rendering", "authors": ["Zican Wang", "Michael Fischer", "Tobias Ritschel"], "categories": ["cs.GR"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.03489", "summary": "We derive methods to compute higher order differentials (Hessians and\nHessian-vector products) of the rendering operator. Our approach is based on\nimportance sampling of a convolution that represents the differentials of\nrendering parameters and shows to be applicable to both rasterization and path\ntracing. We further suggest an aggregate sampling strategy to importance-sample\nmultiple dimensions of one convolution kernel simultaneously. We demonstrate\nthat this information improves convergence when used in higher-order optimizers\nsuch as Newton or Conjugate Gradient relative to a gradient descent baseline in\nseveral inverse rendering tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.03489", "cate": "cs.GR", "date": "2024-12-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.10576", "title": "Robust Photo-Realistic Hand Gesture Generation: from Single View to Multiple View", "authors": ["Qifan Fu", "Xu Chen", "Muhammad Asad", "Shanxin Yuan", "Changjae Oh", "Gregory Slabaugh"], "categories": ["cs.GR"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10576", "summary": "High-fidelity hand gesture generation represents a significant challenge in\nhuman-centric generation tasks. Existing methods typically employ a single-view\nmesh-rendered image prior to enhancing gesture generation quality. However, the\nspatial complexity of hand gestures and the inherent limitations of single-view\nrendering make it difficult to capture complete gesture information,\nparticularly when fingers are occluded. The fundamental contradiction lies in\nthe loss of 3D topological relationships through 2D projection and the\nincomplete spatial coverage inherent to single-view representations. Diverging\nfrom single-view prior approaches, we propose a multi-view prior framework,\nnamed Multi-Modal UNet-based Feature Encoder (MUFEN), to guide diffusion models\nin learning comprehensive 3D hand information. Specifically, we extend\nconventional front-view rendering to include rear, left, right, top, and bottom\nperspectives, selecting the most information-rich view combination as training\npriors to address occlusion. This multi-view prior with a dedicated dual stream\nencoder significantly improves the model's understanding of complete hand\nfeatures. Furthermore, we design a bounding box feature fusion module, which\ncan fuse the gesture localization features and multi-modal features to enhance\nthe location-awareness of the MUFEN features to the gesture-related features.\nExperiments demonstrate that our method achieves state-of-the-art performance\nin both quantitative metrics and qualitative evaluations. The source code is\navailable at https://github.com/fuqifan/MUFEN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10576", "cate": "cs.GR", "date": "2025-05-14", "updated": "2025-08-05", "section": "repl"}
{"id": "2508.03445", "title": "Neighborhood-Preserving Voronoi Treemaps", "authors": ["Patrick Paetzold", "Rebecca Kehlbeck", "Yumeng Xue", "Bin Chen", "Yunhai Wang", "Oliver Deussen"], "categories": ["cs.GR"], "primary_category": "cs.GR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03445", "summary": "Voronoi treemaps are used to depict nodes and their hierarchical\nrelationships simultaneously. However, in addition to the hierarchical\nstructure, data attributes, such as co-occurring features or similarities,\nfrequently exist. Examples include geographical attributes like shared borders\nbetween countries or contextualized semantic information such as embedding\nvectors derived from large language models. In this work, we introduce a\nVoronoi treemap algorithm that leverages data similarity to generate\nneighborhood-preserving treemaps. First, we extend the treemap layout pipeline\nto consider similarity during data preprocessing. We then use a Kuhn-Munkres\nmatching of similarities to centroidal Voronoi tessellation (CVT) cells to\ncreate initial Voronoi diagrams with equal cell sizes for each level. Greedy\nswapping is used to improve the neighborhoods of cells to match the data's\nsimilarity further. During optimization, cell areas are iteratively adjusted to\ntheir respective sizes while preserving the existing neighborhoods. We\ndemonstrate the practicality of our approach through multiple real-world\nexamples drawn from infographics and linguistics. To quantitatively assess the\nresulting treemaps, we employ treemap metrics and measure neighborhood\npreservation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03445", "cate": "cs.GR", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03824", "title": "What Do Agents Think Others Would Do? Level-2 Inverse Games for Inferring Agents' Estimates of Others' Objectives", "authors": ["Hamzah I. Khan", "Jingqi Li", "David Fridovich-Keil"], "categories": ["cs.GT", "cs.MA"], "primary_category": "cs.GT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03824v1", "summary": "Effectively interpreting strategic interactions among multiple agents\nrequires us to infer each agent's objective from limited information. Existing\ninverse game-theoretic approaches frame this challenge in terms of a \"level-1\"\ninference problem, in which we take the perspective of a third-party observer\nand assume that individual agents share complete knowledge of one another's\nobjectives. However, this assumption breaks down in decentralized, real-world\ndecision scenarios like urban driving and bargaining, in which agents may act\nbased on conflicting views of one another's objectives. We demonstrate the\nnecessity of inferring agents' heterogeneous estimates of each other's\nobjectives through empirical examples, and by theoretically characterizing the\nprediction error of level-1 inference on fictitious gameplay data from\nlinear-quadratic games. To address this fundamental issue, we propose a\nframework for level-2 inference to address the question: \"What does each agent\nbelieve about all agents' objectives?\" We prove that the level-2 inference\nproblem is non-convex even in benign settings like linear-quadratic games, and\nwe develop an efficient gradient-based approach for identifying local\nsolutions. Experiments on a synthetic urban driving example show that our\napproach uncovers nuanced misalignments that level-1 methods miss.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03824v1", "cate": "cs.GT", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03847", "title": "A Game-Theoretic Framework for Network Formation in Large Populations", "authors": ["Gokce Dayanikli", "Mathieu Lauriere"], "categories": ["cs.GT", "cs.SI", "math.OC"], "primary_category": "math.OC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03847", "summary": "In this paper, we study a model of network formation in large populations.\nEach agent can choose the strength of interaction (i.e. connection) with other\nagents to find a Nash equilibrium. Different from the recently-developed theory\nof graphon games, here each agent's control depends not only on her own index\nbut also on the index of other agents. After defining the general model of the\ngame, we focus on a special case with piecewise constant graphs and we provide\noptimality conditions through a system of forward-backward stochastic\ndifferential equations. Furthermore, we show the uniqueness and existence\nresults. Finally, we provide numerical experiments to discuss the effects of\ndifferent model settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03847", "cate": "math.OC", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04170", "title": "Agentic-AI based Mathematical Framework for Commercialization of Energy Resilience in Electrical Distribution System Planning and Operation", "authors": ["Aniket Johri", "Divyanshi Dwivedi", "Mayukha Pal"], "categories": ["cs.GT", "cs.LG", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04170", "summary": "The increasing vulnerability of electrical distribution systems to extreme\nweather events and cyber threats necessitates the development of economically\nviable frameworks for resilience enhancement. While existing approaches focus\nprimarily on technical resilience metrics and enhancement strategies, there\nremains a significant gap in establishing market-driven mechanisms that can\neffectively commercialize resilience features while optimizing their deployment\nthrough intelligent decision-making. Moreover, traditional optimization\napproaches for distribution network reconfiguration often fail to dynamically\nadapt to both normal and emergency conditions. This paper introduces a novel\nframework integrating dual-agent Proximal Policy Optimization (PPO) with\nmarket-based mechanisms, achieving an average resilience score of 0.85 0.08\nover 10 test episodes. The proposed architecture leverages a dual-agent PPO\nscheme, where a strategic agent selects optimal DER-driven switching\nconfigurations, while a tactical agent fine-tunes individual switch states and\ngrid preferences under budget and weather constraints. These agents interact\nwithin a custom-built dynamic simulation environment that models stochastic\ncalamity events, budget limits, and resilience-cost trade-offs. A comprehensive\nreward function is designed that balances resilience enhancement objectives\nwith market profitability (with up to 200x reward incentives, resulting in 85%\nof actions during calamity steps selecting configurations with 4 DERs),\nincorporating factors such as load recovery speed, system robustness, and\ncustomer satisfaction. Over 10 test episodes, the framework achieved a\nbenefit-cost ratio of 0.12 0.01, demonstrating sustainable market incentives\nfor resilience investment. This framework creates sustainable market incentives", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04170", "cate": "eess.SY", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2506.10843", "title": "Diverse Committees with Incomplete or Inaccurate Approval Ballots", "authors": ["Feline Lindeboom", "Martijn Brehm", "Davide Grossi", "Pradeep Murukannaiah"], "categories": ["cs.GT"], "primary_category": "cs.GT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.10843", "summary": "We study diversity in approval-based committee elections with incomplete or\ninaccurate information. We define diversity according to the Maximum Coverage\nproblem, which is known to be \\textsc{np}-complete, with a best attainable\npolynomial time approximation ratio of $1-1/\\e$. In the incomplete information\nsetting, voters vote only on a small portion of the candidates, and we prove\nthat getting arbitrarily close to the optimal approximation ratio w.h.p.\nrequires $\\Omega(m^2)$ non-adaptive queries, where $m$ is the number of\ncandidates. This motivates studying adaptive querying algorithms, that can\nadapt their querying strategy to information obtained from previous query\noutcomes. In that setting, we lower this bound to only $\\Omega(m)$ queries. We\npropose a greedy algorithm to match this lower bound up to log-factors. We\nprove the same $\\tilde\\Theta(m)$ bound for the generalized problem of Max Cover\nover a matroid constraint, using a local search algorithm. Specifying a matroid\nof valid committees lets us implement extra structural requirements on the\ncommittee, like quota. In the inaccurate information setting, voters' responses\nare corrupted with a small probability. We prove $\\tilde\\Theta(nm)$ queries are\nrequired to attain a $(1-1/\\e)$-approximation with high probability, where $n$\nis the number of voters. While the proven bounds show that all our algorithms\nare viable asymptotically, they also show that some of them would still require\nlarge numbers of queries in instances of practical relevance. Using real data\nfrom Polis as well as synthetic data, we observe that our algorithms perform\nwell also on smaller instances, both with incomplete and inaccurate\ninformation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.10843", "cate": "cs.GT", "date": "2025-06-12", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03717", "title": "Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery", "authors": ["Muhammad Akmal Bin Mohammed Zaffir", "Daisuke Sakai", "Yuki Sato", "Takahiro Wada"], "categories": ["cs.HC", "q-bio.NC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03717v1", "summary": "Studies suggest that involuntary eye movements exhibit greater stability\nduring active motion compared to passive motion, and this effect may also apply\nto the operation of ride-on machinery. Moreover, a study suggested that\nexperimentally manipulating the sense of agency (SoA) by introducing delays may\ninfluence the stability of involuntary eye movements. Although a preliminary\ninvestigation examined involuntary eye movements and perceived maneuverability\nunder two distinct machine dynamics with preserved SoA, it remains unclear how\nsystematic variations in motion dynamics influence these factors. Therefore,\nthe purpose of the present research was to investigate whether systematic\nvariations in the dynamic properties of a ride-on machine, where the perceived\nmaneuverability is modulated, influence the accuracy of involuntary eye\nmovements in human operators. Participants rode a yaw-rotational platform whose\ntime constant from joystick input to motor torque of a rotational machine was\nsystematically manipulated. During the operation, eye movements were recorded\nwhile participants fixated on a visual target. After each condition,\nparticipants provided subjective ratings of maneuverability and cognitive load.\nAs the platform's time constant increased, the perceived maneuverability scores\ndecreased while the cognitive loads increased. Concurrently, involuntary eye\nmovement accuracy decreased. Moderate to weak positive correlations emerged\nbetween the perceived maneuverability scores and the eye movement gain and\naccuracy, while a weak negative correlation was found with cognitive load.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03717v1", "cate": "cs.HC", "date": "2025-07-27", "updated": "2025-07-27", "section": "new"}
{"id": "2508.03852", "title": "A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers", "authors": ["Zhuohao", "Zhang", "Haichang Li", "Chun Meng Yu", "Faraz Faruqi", "Junan Xie", "Gene S-H Kim", "Mingming Fan", "Angus G. Forbes", "Jacob O. Wobbrock", "Anhong Guo", "Liang He"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03852v1", "summary": "Building 3-D models is challenging for blind and low-vision (BLV) users due\nto the inherent complexity of 3-D models and the lack of support for non-visual\ninteraction in existing tools. To address this issue, we introduce A11yShape, a\nnovel system designed to help BLV users who possess basic programming skills\nunderstand, modify, and iterate on 3-D models. A11yShape leverages LLMs and\nintegrates with OpenSCAD, a popular open-source editor that generates 3-D\nmodels from code. Key functionalities of A11yShape include accessible\ndescriptions of 3-D models, version control to track changes in models and\ncode, and a hierarchical representation of model components. Most importantly,\nA11yShape employs a cross-representation highlighting mechanism to synchronize\nsemantic selections across all model representations -- code, semantic\nhierarchy, AI description, and 3-D rendering. We conducted a multi-session user\nstudy with four BLV programmers, where, after an initial tutorial session,\nparticipants independently completed 12 distinct models across two testing\nsessions, achieving results that aligned with their own satisfaction. The\nresult demonstrates that participants were able to comprehend provided 3-D\nmodels, as well as independently create and modify 3-D models -- tasks that\nwere previously impossible without assistance from sighted individuals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03852v1", "cate": "cs.HC", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03876", "title": "ReVISit 2: A Full Experiment Life Cycle User Study Framework", "authors": ["Zach Cutler", "Jack Wilburn", "Hilson Shrestha", "Yiren Ding", "Brian Bollen", "Khandaker Abrar Nadib", "Tingying He", "Andrew McNutt", "Lane Harrison", "Alexander Lex"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03876v1", "summary": "Online user studies of visualizations, visual encodings, and interaction\ntechniques are ubiquitous in visualization research. Yet, designing,\nconducting, and analyzing studies effectively is still a major burden. Although\nvarious packages support such user studies, most solutions address only facets\nof the experiment life cycle, make reproducibility difficult, or do not cater\nto nuanced study designs or interactions. We introduce reVISit 2, a software\nframework that supports visualization researchers at all stages of designing\nand conducting browser-based user studies. ReVISit supports researchers in the\ndesign, debug & pilot, data collection, analysis, and dissemination experiment\nphases by providing both technical affordances (such as replay of participant\ninteractions) and sociotechnical aids (such as a mindfully maintained community\nof support). It is a proven system that can be (and has been) used in\npublication-quality studies -- which we demonstrate through a series of\nexperimental replications. We reflect on the design of the system via\ninterviews and an analysis of its technical dimensions. Through this work, we\nseek to elevate the ease with which studies are conducted, improve the\nreproducibility of studies within our community, and support the construction\nof advanced interactive studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03876v1", "cate": "cs.HC", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03974", "title": "Managing Data for Scalable and Interactive Event Sequence Visualization", "authors": ["Sayef Azad Sakin", "Katherine E. Isaacs"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03974v1", "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03974v1", "cate": "cs.HC", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03980", "title": "SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions", "authors": ["Md Sabbir Ahmed", "Arafat Rahman", "Mark Rucker", "Laura E. Barnes"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03980v1", "summary": "Social interactions are a fundamental part of daily life and play a critical\nrole in well-being. As emerging technologies offer opportunities to\nunobtrusively monitor behavior, there is growing interest in using them to\nbetter understand social experiences. However, automatically detecting\ninteractions, particularly via wearable devices, remains underexplored.\nExisting systems are often limited to controlled environments, constrained to\nin-person interactions, and rely on rigid assumptions such as the presence of\ntwo speakers within a fixed time window. These limitations reduce their\ngeneralizability to capture diverse real-world interactions. To address these\nchallenges, we developed a real-time, on-watch system capable of detecting both\nin-person and virtual interactions. The system leverages transfer learning to\ndetect foreground speech (FS) and infers interaction boundaries based upon FS\nand conversational cues like whispering. In a real-world evaluation involving\n11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the\nsystem achieved an interaction detection accuracy of 73.18%. Follow-up with six\nparticipants indicated perfect recall for detecting interactions. These\npreliminary findings demonstrate the potential of our system to capture\ninteractions in daily life, providing a foundation for applications such as\npersonalized interventions targeting social anxiety.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03980v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04026", "title": "VeriGUI: Verifiable Long-Chain GUI Dataset", "authors": ["Shunyu Liu", "Minghao Liu", "Huichi Zhou", "Zhenyu Cui", "Yang Zhou", "Yuhao Zhou", "Wendong Fan", "Ge Zhang", "Jiajun Shi", "Weihao Xuan", "Jiaxing Huang", "Shuang Luo", "Fang Wu", "Heli Qi", "Qingcheng Zeng", "Ziqi Ren", "Jialiang Gao", "Jindi Lv", "Junjie Wang", "Aosong Feng", "Heng Zhou", "Wangchunshu Zhou", "Zhenfei Yin", "Wenlong Zhang", "Guohao Li", "Wenhao Yu", "Irene Li", "Lei Ma", "Lei Bai", "Qunshu Lin", "Mingli Song", "Dacheng Tao"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04026v1", "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04026v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04108", "title": "XARP Tools: An Extended Reality Platform for Humans and AI Agents", "authors": ["Arthur Caetano", "Misha Sra"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04108v1", "summary": "This technical report presents XARP Tools, an extended reality (XR) framework\ndesigned for human and AI developers alike. XARP comprises a server-side Python\nlibrary and platform-specific XR clients. The library offers high-level APIs\nand communicates with clients via a JSON-based protocol over WebSockets. XR\nclients encapsulate device and runtime specifics, providing responsive,\nlow-latency user interaction. XARP can be utilized in three ways: (i) as a\nlibrary that abstracts XR development for humans; (ii) as a set of callable\ntools that allow AI agents to drive on-the-fly interactions with users; and\n(iii) as a Model Context Protocol server that plugs XR devices into AI\necosystems. XARP code and working examples are released openly at\nhttps://github.com/HAL-UCSB/xarp.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04108v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04202", "title": "Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes", "authors": ["Abdulrhman Alorini", "Yufeng Wu", "Abdullah Bin Sawad", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04202v1", "summary": "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04202v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04357", "title": "Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows", "authors": ["Gloria FernÃ¡ndez-Nieto", "Vanessa Echeverria", "Yuheng Li", "Yi-Shan Tsai", "Lele Sha", "Guanliang Chen", "Dragan Gasevic", "Zachari Swiecki"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04357v1", "summary": "Knowledge Management is crucial for capturing and transferring expertise\nwithin universities, especially in high staff turnover contexts where expertise\nloss disrupts teaching. Documenting teachers' workflows is time-intensive and\ndiverts experts from core responsibilities. Sequential Pattern Mining (SPM)\nleverages log data to identify expert workflows, offering an automated\nalternative to represent workflows but requiring transformation into intuitive\nformats for novice educators. This paper introduces Visual Process\nRepresentations (VPR), a design approach combining SPM, Knowledge Management\nprocesses, and storytelling techniques to convert expert log data into clear\nvisualisations. We detail the design phases and report a study evaluating\nvisual affordances (text lists vs. pictorial-style) and teachers' perceptions\nof four versions of the VPR with 160 higher teachers on Prolific. Results\nindicate improved task performance, usability, and engagement, particularly\nwith enriched visuals, though process memorability and task time improvements\nwere limited. The findings highlight VPR's potential to visualise workflows and\nsupport novice educators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04357v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04377", "title": "GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design", "authors": ["Gloria FernÃ¡ndez-Nieto", "Lele Sha", "Yuheng Li", "Yi-Shan Tsai", "Guanliang Chen", "Yinwei Wei", "Weiqing Wang", "Jinchun Wen", "Shaveen Singh", "Ivan Silva", "Yuanfang Li", "Dragan GasÄviÄ", "Zachari Swiecki"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04377v1", "summary": "Designing Knowledge Management Systems (KMSs) for higher education requires\naddressing complex human-technology interactions, especially where staff\nturnover and changing roles create ongoing challenges for reusing knowledge.\nWhile advances in process mining and Generative AI enable new ways of designing\nfeatures to support knowledge management, existing KMSs often overlook the\nrealities of educators' workflows, leading to low adoption and limited impact.\nThis paper presents findings from a two-year human-centred design study with\n108 higher education teachers, focused on the iterative co-design and\nevaluation of GoldMind, a KMS supporting in-the-flow knowledge management\nduring digital teaching tasks. Through three design-evaluation cycles, we\nexamined how teachers interacted with the system and how their feedback\ninformed successive refinements. Insights are synthesised across three themes:\n(1) Technology Lessons from user interaction data, (2) Design Considerations\nshaped by co-design and usability testing, and (3) Human Factors, including\ncognitive load and knowledge behaviours, analysed using Epistemic Network\nAnalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04377v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04391", "title": "Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis", "authors": ["Ze Gao", "Mengyao Guo", "Zheng Wang", "Xiaolin Zhang", "Sihuang Man"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04391v1", "summary": "Digital ecological art represents an emergent frontier where biological media\nconverge with virtual environments. This study examines the paradigm shift from\nanthropocentric to plant-centered artistic narratives within the metaverse,\ncontextualizing how digital platforms transform ecological expression. However,\ncurrent frameworks fail to systematically guide artists in leveraging plant\nagency for digital symbiosis that transcends human-centered creation. We\npropose the Biocentric-Creation Transformation Ideology (BCTI) framework and\nvalidate it through multimodal case studies spanning bio-art, NFTs, and VR\necosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable\nunprecedented plant-algorithm co-creation, with biological artworks increasing\nby 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests\nthrough blockchain DAOs where plants govern human-plant collaborations; (3)\nAlgorithmic photosynthesis in VR environments reshapes ecological aesthetics\nthrough real-time biodata translation. The BCTI framework advances ecological\nart theory by systematizing the transition from representation to\nplant-centered agency, offering artists a blueprint for post-anthropocene\ncreation. This redefines environmental consciousness in virtual realms while\nestablishing new protocols for cross-species digital collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04391v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04541", "title": "Measuring Information Richness in Product Images: Implications for Online Sales", "authors": ["Zhu Yuting", "Cao Xinyu", "Su Yuzhuo", "Ma Yongbin"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04541v1", "summary": "A common challenge for e-commerce sellers is to decide what product images to\ndisplay on online shopping sites. In this paper, we propose and validate a\nnovel metric, k-value, to quantify the information richness of an image set,\nand we further investigate its effect on consumers' purchase decisions. We\nleverage patch-level embeddings from Vision Transformers (ViT) and apply\nk-means clustering to identify distinct visual features, defining k-value as\nthe number of clusters. An online experiment demonstrates that k-value aligns\nwith human-perceived information richness, validating the metric. A simulated\nonline shopping experiment further reveals a significant yet counterintuitive\nresult: while an image set with a higher k-value (richer information) shortens\ndecision time, it paradoxically reduces purchase propensity. Our findings\nilluminate the complex relationship between visual information richness and\nconsumer behavior, providing sellers a quantifiable tool for image selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04541v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04634", "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations", "authors": ["Mohammed Almutairi", "Charles Chiang", "Haoze Guo", "Matthew Belcher", "Nandini Banerjee", "Maria Milkowski", "Svitlana Volkova", "Daniel Nguyen", "Tim Weninger", "Michael Yankoski", "Trenton W. Ford", "Diego Gomez-Zara"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04634v1", "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04634v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04679", "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models", "authors": ["Amit Kumar Das", "Klaus Mueller"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04679v1", "summary": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04679v1", "cate": "cs.HC", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03698", "title": "Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024", "authors": ["Se Won Oh", "Hyuntae Jeong", "Seungeun Chung", "Jeong Mook Lim", "Kyoung Ju Noh", "Sunkyung Lee", "Gyuwon Jung"], "categories": ["cs.HC", "cs.LG", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03698", "summary": "Improving human health and well-being requires an accurate and effective\nunderstanding of an individual's physical and mental state throughout daily\nlife. To support this goal, we utilized smartphones, smartwatches, and sleep\nsensors to collect data passively and continuously for 24 hours a day, with\nminimal interference to participants' usual behavior, enabling us to gather\nquantitative data on daily behaviors and sleep activities across multiple days.\nAdditionally, we gathered subjective self-reports of participants' fatigue,\nstress, and sleep quality through surveys conducted immediately before and\nafter sleep. This comprehensive lifelog dataset is expected to provide a\nfoundational resource for exploring meaningful insights into human daily life\nand lifestyle patterns, and a portion of the data has been anonymized and made\npublicly available for further research. In this paper, we introduce the ETRI\nLifelog Dataset 2024, detailing its structure and presenting potential\napplications, such as using machine learning models to predict sleep quality\nand stress.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03698", "cate": "eess.SP", "date": "2025-07-18", "updated": "2025-07-18", "section": "cross"}
{"id": "2508.03729", "title": "Privileged Contrastive Pretraining for Multimodal Affect Modelling", "authors": ["Kosmas Pinitas", "Konstantinos Makantasis", "Georgios N. Yannakakis"], "categories": ["cs.HC", "cs.LG", "cs.MM"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03729", "summary": "Affective Computing (AC) has made significant progress with the advent of\ndeep learning, yet a persistent challenge remains: the reliable transfer of\naffective models from controlled laboratory settings (in-vitro) to uncontrolled\nreal-world environments (in-vivo). To address this challenge we introduce the\nPrivileged Contrastive Pretraining (PriCon) framework according to which models\nare first pretrained via supervised contrastive learning (SCL) and then act as\nteacher models within a Learning Using Privileged Information (LUPI) framework.\nPriCon both leverages privileged information during training and enhances the\nrobustness of derived affect models via SCL. Experiments conducted on two\nbenchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained\nusing PriCon consistently outperform LUPI and end to end models. Remarkably, in\nmany cases, PriCon models achieve performance comparable to models trained with\naccess to all modalities during both training and testing. The findings\nunderscore the potential of PriCon as a paradigm towards further bridging the\ngap between in-vitro and in-vivo affective modelling, offering a scalable and\npractical solution for real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03729", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30", "section": "cross"}
{"id": "2508.03922", "title": "A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output", "authors": ["Soroush Heydari"], "categories": ["cs.HC", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03922", "summary": "The rapid adoption of Artificial Intelligence(AI) programming assistants such\nas GitHub Copilot introduces new challenges in how these software tools address\nhuman needs. Many existing evaluation frameworks address technical aspects such\nas code correctness and efficiency, but often overlook crucial human factors\nthat affect the successful integration of AI assistants in software development\nworkflows. In this study, I analyzed GitHub Copilot's interaction with users\nthrough its chat interface, measured Copilot's ability to adapt explanations\nand code generation to user expertise levels, and assessed its effectiveness in\nfacilitating collaborative programming experiences. I established a\nhuman-centered requirements framework with clear metrics to evaluate these\nqualities in GitHub Copilot chat. I discussed the test results and their\nimplications for future analysis of human requirements in automated\nprogramming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03922", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04408", "title": "Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making", "authors": ["Carlos AndrÃ©s RamÃ­rez CataÃ±o", "Makoto Itoh"], "categories": ["cs.HC", "cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04408", "summary": "Software defect prediction using code metrics has been extensively researched\nover the past five decades. However, prediction harnessing non-software metrics\nis under-researched. Considering that the root cause of software defects is\noften attributed to human error, human factors theory might offer key\nforecasting metrics for actionable insights. This paper explores automated\nsoftware defect prediction at the method level based on the developers' coding\nhabits. First, we propose a framework for deciding the metrics to conduct\npredictions. Next, we compare the performance of our metrics to that of the\ncode and commit history metrics shown by research to achieve the highest\nperformance to date. Finally, we analyze the prediction importance of each\nmetric. As a result of our analyses of twenty-one critical infrastructure\nlarge-scale open-source software projects, we have presented: (1) a human\nerror-based framework with metrics useful for defect prediction at method\nlevel; (2) models using our proposed metrics achieve better average prediction\nperformance than the state-of-the-art code metrics and history measures; (3)\nthe prediction importance of all metrics distributes differently with each of\nthe novel metrics having better average importance than code and history\nmetrics; (4) the novel metrics dramatically enhance the explainability,\npracticality, and actionability of software defect prediction models,\nsignificantly advancing the field. We present a systematic approach to\nforecasting defect-prone software methods via a human error framework. This\nwork empowers practitioners to act on predictions, empirically demonstrating\nhow developer coding habits contribute to defects in software systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04408", "cate": "cs.SE", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04651", "title": "Live Music Models", "authors": ["Lyria Team", "Antoine Caillon", "Brian McWilliams", "Cassie Tarakajian", "Ian Simon", "Ilaria Manco", "Jesse Engel", "Noah Constant", "Pen Li", "Timo I. Denk", "Alberto Lalama", "Andrea Agostinelli", "Anna Huang", "Ethan Manilow", "George Brower", "Hakan Erdogan", "Heidi Lei", "Itai Rolnick", "Ivan Grishchenko", "Manu Orsini", "Matej Kastelic", "Mauricio Zuluaga", "Mauro Verzetti", "Michael Dooley", "Ondrej Skopek", "Rafael Ferrer", "ZalÃ¡n Borsos", "Ãaron van den Oord", "Douglas Eck", "Eli Collins", "Jason Baldridge", "Tom Hume", "Chris Donahue", "Kehang Han", "Adam Roberts"], "categories": ["cs.HC", "cs.LG", "cs.SD"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04651", "summary": "We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04651", "cate": "cs.SD", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2311.06381", "title": "Optimal Fidelity Selection for Human-Supervised Search", "authors": ["Piyush Gupta", "Vaibhav Srivastava"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.06381", "summary": "We study optimal fidelity selection in human-supervised underwater visual\nsearch, where operator performance is affected by cognitive factors like\nworkload and fatigue. In our experiments, participants perform two simultaneous\ntasks: detecting underwater mines in videos (primary) and responding to a\nvisual cue to estimate workload (secondary). Videos arrive as a Poisson process\nand queue for review, with the operator choosing between normal fidelity\n(faster playback) and high fidelity. Rewards are based on detection accuracy,\nwhile penalties depend on queue length. Workload is modeled as a hidden state\nusing an Input-Output Hidden Markov Model, and fidelity selection is optimized\nvia a Partially Observable Markov Decision Process. We evaluate two setups:\nfidelity-only selection and a version allowing task delegation to automation to\nmaintain queue stability. Our approach improves performance by 26.5% without\ndelegation and 50.3% with delegation, compared to a baseline where humans\nmanually choose their fidelity levels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.06381", "cate": "cs.HC", "date": "2023-11-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.00873", "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences", "authors": ["Zahra Ashktorab", "Michael Desmond", "Qian Pan", "James M. Johnson", "Martin Santillan Cooper", "Elizabeth M. Daly", "Rahul Nair", "Tejaswini Pedapati", "Hyo Jin Do", "Werner Geyer"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.00873", "summary": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.00873", "cate": "cs.HC", "date": "2024-10-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2502.02929", "title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality", "authors": ["Brandon Woodard", "Margarita Geleta", "Joseph J. LaViola Jr.", "Andrea Fanelli", "Rhonda Wilson"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02929", "summary": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with 6DoF in XR. To provide a first step toward identifying\ndesign-related research directions in this space, we conducted an exploratory\nstudy where we recruited 27 participants, consisting of expert and non-expert\nsound designers. The goal was to assess design lessons that can be used to\ninform future research venues in 3D sound design. We ran a within-subjects\nstudy where users designed both a music and cinematic soundscapes. After\nthematically analyzing participant data, we constructed two design lessons: (1)\nProprioception for AR Sound Design, and (2) Balancing Audio-Visual Modalities\nin AR GUIs. Additionally, we provide application domains that can benefit most\nfrom 6DoF sound design based on our results. To expand on these insights, we\nconducted a second within-subjects study comparing AudioMiXR to a 2D panner\nbaseline. Results show that AudioMiXR significantly improved usability (SUS),\nreduced frustration and mental workload (NASA-TLX), and enhanced creativity\nacross all subscales. These findings demonstrate that 6DoF AR interaction\nyields measurable gains in user experience and creative output, positioning\nAudioMiXR as a promising foundation for future AR-based sound design tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02929", "cate": "cs.HC", "date": "2025-02-05", "updated": "2025-08-05", "section": "repl"}
{"id": "2502.17829", "title": "Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm", "authors": ["Yudong Xie", "Zhifeng Han", "Qinfan Xiao", "Liwei Liang", "Lu-Qi Tao", "Tian-Ling Ren"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.17829", "summary": "Silent speech interfaces (SSI) are being actively developed to assist\nindividuals with communication impairments who have long suffered from daily\nhardships and a reduced quality of life. However, silent sentences are\ndifficult to segment and recognize due to elision and linking. A novel silent\nspeech sentence recognition method is proposed to convert the facial motion\nsignals collected by six-axis accelerometers into transcribed words and\nsentences. A Conformer-based neural network with the\nConnectionist-Temporal-Classification algorithm is used to gain contextual\nunderstanding and translate the non-acoustic signals into words sequences,\nsolely requesting the constituent words in the database. Test results show that\nthe proposed method achieves a 97.17% accuracy in sentence recognition,\nsurpassing the existing silent speech recognition methods with a typical\naccuracy of 85%-95%, and demonstrating the potential of accelerometers as an\navailable SSI modality for high-accuracy silent speech sentence recognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.17829", "cate": "cs.HC", "date": "2025-02-25", "updated": "2025-08-05", "section": "repl"}
{"id": "2507.19072", "title": "Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis", "authors": ["Oliver Bates", "Christian Remy", "Kieran Cutting", "Adam Tyler", "Adrian Friday"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19072", "summary": "What could designing for carbon reduction of heating and cooling in\ncommercial settings look like in the near future? How can we challenge dominant\nmindsets and paradigms of efficiency and behaviour change? How can we help\nbuild worlds through our practice that can become future realities? This paper\nintroduces the fictional consultancy ANCSTRL.LAB to explore opportunities for\nmaking space in research projects that can encourage more systems-oriented\ninterventions. We present a design fiction that asks `what if energy management\nand reduction practice embraced systems thinking?'. Our design fiction explores\nhow future energy consultancies could utilise systems thinking, and (more than)\nhuman centred design to re-imagine energy management practice and change\nsystems in ways that are currently unfathomable. We finish by discussing how\nLIMITS research can utilise design fiction and speculative praxis to help build\nnew material realities where more holistic perspectives, the leveraging of\nsystems change, and the imagining of post-neoliberal futures is the norm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19072", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.00252", "title": "TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices", "authors": ["Wataru Kawabe", "Hiroto Fukuda", "Akihisa Shitara", "Yuri Nakao", "Yusuke Sugano"], "categories": ["cs.HC"], "primary_category": "cs.HC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00252", "summary": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00252", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03938", "title": "Single Fragment Forensic Coding from Discrepancy Theory", "authors": ["Junsheng Liu", "Netanel Raviv"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03938v1", "summary": "Three-dimensional (3D) printing's accessibility enables rapid manufacturing\nbut also poses security risks, such as the unauthorized production of\nuntraceable firearms and prohibited items. To ensure traceability and\naccountability, embedding unique identifiers within printed objects is\nessential, in order to assist forensic investigation of illicit use. This paper\nmodels data embedding in 3D printing using principles from error-correcting\ncodes, aiming to recover embedded information from partial or altered fragments\nof the object. Previous works embedded one-dimensional data (i.e., a vector)\ninside the object, and required almost all fragments of the object for\nsuccessful decoding. In this work, we study a problem setting in which only one\nsufficiently large fragment of the object is available for decoding. We first\nshow that for one-dimensional embedded information the problem can be easily\nsolved using existing tools. Then, we introduce novel encoding schemes for\ntwo-dimensional information (i.e., a matrix), and three-dimensional information\n(i.e., a cube) which enable the information to be decoded from any sufficiently\nlarge rectangle-shaped or cuboid-shaped fragment. Lastly, we introduce a code\nthat is also capable of correcting bit-flip errors, using techniques from\nrecently proposed codes for DNA storage. Our codes operate at non-vanishing\nrates, and involve concepts from discrepancy theory called Van der Corput sets\nand Halton-Hammersely sets in novel ways.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03938v1", "cate": "cs.IT", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04262", "title": "One-weight codes in the sum-rank metric", "authors": ["Usman Mushrraf", "Ferdinando Zullo"], "categories": ["cs.IT", "math.CO"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04262v1", "summary": "One-weight codes, in which all nonzero codewords share the same weight, form\na highly structured class of linear codes with deep connections to finite\ngeometry. While their classification is well understood in the Hamming and rank\nmetrics - being equivalent to (direct sums of) simplex codes - the sum-rank\nmetric presents a far more intricate landscape. In this work, we explore the\ngeometry of one-weight sum-rank metric codes, focusing on three distinct\nclasses. First, we introduce and classify \\emph{constant rank-list} sum-rank\ncodes, where each nonzero codeword has the same tuple of ranks, extending\nresults from the rank-metric setting. Next, we investigate the more general\n\\emph{constant rank-profile} codes, where, up to reordering, each nonzero\ncodeword has the same tuple of ranks. Although a complete classification\nremains elusive, we present the first examples and partial structural results\nfor this class. Finally, we consider one-weight codes that are also MSRD\n(Maximum Sum-Rank Distance) codes. For dimension two, constructions arise from\npartitions of scattered linear sets on projective lines. For dimension three,\nwe connect their existence to that of special $2$-fold blocking sets in the\nprojective plane, leading to new bounds and nonexistence results over certain\nfields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04262v1", "cate": "cs.IT", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04313", "title": "Is Lattice Reduction Necessary for Vector Perturbation Precoding?", "authors": ["Dominik Semmler", "Wolfgang Utschick", "Michael Joham"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04313v1", "summary": "Vector perturbation (VP) precoding is an effective nonlinear precoding\ntechnique in the downlink (DL) with modulo channels. Especially, when combined\nwith Lattice reduction (LR), low-complexity algorithms achieve very promising\nperformances, outperforming other popular nonlinear precoding techniques like\nTomlinson-Harashima precoding (THP). However, these results are based on the\nuncoded symbol error rate (SER) or uncoded bit error rate (BER). We show that\nwhen using the mutual information as the figure of merit, the observation is\nfundamentally different and that these algorithms generally do not outperform\nTHP. Within the expression of the mutual information, a rate allocation matrix\ncan be incorporated, which has not received much attention so far. In this\narticle, we derive the optimal choice of this matrix for different algorithms,\nand we show that this matrix is indeed crucial for the performance, especially\nfor ill-conditioned channels. Furthermore, when using an optimized choice of\nthis matrix, we show that the classical LR-aided algorithms cannot exceed the\nrate of THP, highlighting the effectiveness of the THP method. This concept can\nbe generalized to a whole class of algorithms for which LR yields no\nimprovement. We derive the corresponding properties and categorize various\nalgorithms accordingly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04313v1", "cate": "cs.IT", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04355", "title": "Grid-like Error-Correcting Codes for Matrix Multiplication with Better Correcting Capability", "authors": ["Hao Shi", "Zhengyi Jiang", "Zhongyi Huang", "Bo Bai", "Gong Zhang", "Hanxu Hou"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04355v1", "summary": "Matrix multiplication over the real field constitutes a foundational\noperation in the training of deep learning models, serving as a computational\ncornerstone for both forward and backward propagation processes. However, the\npresence of silent data corruption (SDC) in large-scale distributed training\nenvironments poses a significant threat to model convergence and predictive\naccuracy, particularly when such errors manifest during matrix multiplication.\nDue to their transient and non-intrusive nature, these errors often evade\ndetection, allowing them to propagate and accumulate over time, ultimately\nleading to substantial degradation in model performance. In this paper, we\nintroduce a novel error-correcting coding framework specifically tailored for\nmatrix multiplication operations. Our proposed framework is designed to detect\nand correct multiple computational errors that may arise during the execution\nof matrix products. By leveraging a grid-based structural encoding scheme, our\napproach enhances error localization and correction capabilities across all\nparticipating matrices, thereby significantly improving the fault tolerance of\nthe computation. Experimental results demonstrate that our method achieves\ndeterministic correction of up to two erroneous symbols distributed across\nthree matrices with 100\\% reliability, while incurring only a 24\\% overhead in\ncomputational time on GPU architectures. Furthermore, we provide a rigorous\ntheoretical analysis of the error-correction properties inherent to our coding\nscheme, establishing its correctness and robustness under well-defined fault\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04355v1", "cate": "cs.IT", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04466", "title": "Tradeoff Between the Number of Transmitted Molecules and the BER Performance in Molecular Communication between Bionanosensors", "authors": ["Dongliang Jing", "Linjuan Li", "Lin Lin", "Andrew W. Eckford"], "categories": ["cs.IT", "eess.SP"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04466v1", "summary": "In the domain of molecular communication (MC), information is conveyed\nthrough the characteristics of molecules transmitted between the transmitter\nand the receiver bionanosensors via propagation. The constrained size of the\ntransmitter imposes limitations on its storage capacity, constraining the\nnumber of available molecules for transmission, with a resulting effect on\ncommunication reliability. This paper primarily focuses on achieving an\nequilibrium between the number of transmitted molecules and the bit error rate\n(BER) performance. To this end, we first analyze the relationship between the\nnumber of transmitted molecules and the BER performance. Subsequently, a\nbalancing function that considers both the number of transmitted molecules and\nthe BER performance is introduced, taking into account the molecules'\nrespective weights. Given the difference in magnitude between the number of\ntransmitted molecules and the BER, these parameters are normalized to\nfacilitate analysis. Subsequently, a Gradient Descent Algorithm is employed to\ndetermine the optimal number of transmitted molecules, aiming to achieve the\noptimal equilibrium in the analyzed MC system. Theoretical and simulation\nresults are provided, substantiating that the optimal outcome indeed\nestablishes an ideal balance between the number of transmitted molecules and\nthe BER.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04466v1", "cate": "cs.IT", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04627", "title": "Energy-Efficient Hybrid Beamfocusing for Near-Field Integrated Sensing and Communication", "authors": ["Wenhao Hu", "Zhenyao He", "Wei Xu", "Yongming Huang", "Derrick Wing Kwan Ng", "Naofal Al-Dhahir"], "categories": ["cs.IT", "eess.SP"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04627v1", "summary": "Integrated sensing and communication (ISAC) is a pivotal component of\nsixth-generation (6G) wireless networks, leveraging high-frequency bands and\nmassive multiple-input multiple-output (M-MIMO) to deliver both high-capacity\ncommunication and high-precision sensing. However, these technological\nadvancements lead to significant near-field effects, while the implementation\nof M-MIMO \\mbox{is associated with considerable} hardware costs and escalated\npower consumption. In this context, hybrid architecture designs emerge as both\nhardware-efficient and energy-efficient solutions. Motivated by these\nconsiderations, we investigate the design of energy-efficient hybrid\nbeamfocusing for near-field ISAC under two distinct target scenarios, i.e., a\npoint target and an extended target. Specifically, we first derive the\nclosed-form Cram\\'{e}r-Rao bound (CRB) of joint angle-and-distance estimation\nfor the point target and the Bayesian CRB (BCRB) of the target response matrix\nfor the extended target. Building on these derived results, we minimize the\nCRB/BCRB by optimizing the transmit beamfocusing, while ensuring the energy\nefficiency (EE) of the system and the quality-of-service (QoS) for\ncommunication users. To address the resulting \\mbox{nonconvex problems}, we\nfirst utilize a penalty-based successive convex approximation technique with a\nfully-digital beamformer to obtain a suboptimal solution. Then, we propose an\nefficient alternating \\mbox{optimization} algorithm to design the\nanalog-and-digital beamformer. \\mbox{Simulation} results indicate that joint\ndistance-and-angle estimation is feasible in the near-field region. However,\nthe adopted hybrid architectures inevitably degrade the accuracy of distance\nestimation, compared with their fully-digital counterparts. Furthermore,\nenhancements in system EE would compromise the accuracy of target estimation,\nunveiling a nontrivial tradeoff.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04627v1", "cate": "cs.IT", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03906", "title": "Zak-OTFS over CP-OFDM", "authors": ["Saif Khan Mohammed", "Saurabh Prakash", "Muhammad Ubadah", "Imran Ali Khan", "Ronny Hadani", "Shlomo Rakib", "Shachar Kons", "Yoav Hebron", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "categories": ["cs.IT", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03906", "summary": "Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to\nachieve significantly better performance compared to the standardized\nCyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high\ndelay/Doppler spread scenarios envisaged in next generation communication\nsystems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)\ndomain, characterized by two parameters, (i) the pulse period along the delay\naxis (``delay period\") (Doppler period is related to the delay period), and\n(ii) the pulse shaping filter. An important practical challenge is enabling\nsupport for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper\nwe show that Zak-OTFS modulation with pulse shaping constrained to sinc\nfiltering (filter bandwidth equal to the communication bandwidth $B$) followed\nby time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is\nthe symbol duration and $T_{cp}$ is the CP duration), can be implemented as a\nlow-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS\nde-modulator with matched filtering constrained to sinc filtering (filter\nbandwidth $B$) followed by rectangular time windowing over duration $T$ can be\nimplemented as a low-complexity post-processing of the CP-OFDM de-modulator\noutput. This proposed ``Zak-OTFS over CP-OFDM\" architecture enables us to\nharness the benefits of Zak-OTFS in existing network infrastructure. We also\nshow that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with\nCP-OFDM being a special case when the delay period takes its minimum possible\nvalue equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum\ndelay period.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03906", "cate": "eess.SP", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2304.00762", "title": "Weight Distribution of Repeated-Root Cyclic Codes with Prime Power Lengths", "authors": ["Wei Zhao", "Weixian Li", "Shenghao Yang", "Fang-Wei Fu", "Kenneth W. Shum"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2304.00762", "summary": "Determining the weight distribution of a linear code is a classical and\nfundamental topic in coding theory that has been extensively investigated.\nRepeated-root cyclic codes, which form a significant subclass of\nerror-correcting codes, have found broad applications in quantum\nerror-correcting codes, symbol-pair codes, and storage codes. Through\npolynomial derivation, we derive the monomial equivalent codes for these\nrepeated-root cyclic codes with prime power lengths. Given that monomial\nequivalent codes exhibit identical weight distributions, we transform the\ncomputation of the weight distribution of these repeated-root cyclic codes into\nthe computation of the weight distribution of their monomial equivalent codes.\nLeveraging the classical results on the weight distribution of MDS codes, we\nexplicitly determine the weight distribution of these repeated-root cyclic\ncodes. Moreover, we apply the weight distribution formula to construct a class\nof $p$-weight cyclic codes for any prime $p$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2304.00762", "cate": "cs.IT", "date": "2023-04-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2401.11622", "title": "The Markov-Chain Polytope with Applications", "authors": ["Mordecai J. Golin", "Albert John Lalim Patupat"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.11622", "summary": "This paper addresses the problem of finding a minimum-cost $m$-state Markov\nchain $(S_0,\\ldots,S_{m-1})$ in a large set of chains. The chains studied have\na reward associated with each state. The cost of a chain is its \"gain\", i.e.,\nits average reward under its stationary distribution.\n  Specifically, for each $k=0,\\ldots,m-1$ there is a known set ${\\mathbb S}_k$\nof type-$k$ states. A permissible Markov chain contains exactly one state of\neach type; the problem is to find a minimum-cost permissible chain.\n  The original motivation was to find a cheapest binary AIFV-$m$ lossless code\non a source alphabet of size $n$. Such a code is an $m$-tuple of trees, in\nwhich each tree can be viewed as a Markov Chain state. This formulation was\nthen used to address other problems in lossless compression. The known solution\ntechniques for finding minimum-cost Markov chains were iterative and ran in\nexponential time.\n  This paper shows how to map every possible type-$k$ state into a type-$k$\nhyperplane and then define a \"Markov Chain Polytope\" as the lower envelope of\nall such hyperplanes. Finding a minimum-cost Markov chain can then be shown to\nbe equivalent to finding a \"highest\" point on this polytope.\n  The local optimization procedures used in the previous iterative algorithms\nare shown to be separation oracles for this polytope. Since these were often\npolynomial time, an application of the Ellipsoid method immediately leads to\npolynomial time algorithms for these problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.11622", "cate": "cs.IT", "date": "2024-01-21", "updated": "2025-08-05", "section": "repl"}
{"id": "2402.00646", "title": "Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces", "authors": ["Thien Duc Hua", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "categories": ["cs.IT", "eess.SP"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.00646", "summary": "We investigate the integration of beyond-diagonal reconfigurable intelligent\nsurfaces (BD-RISs) into cell-free massive multiple-input multiple-output\n(CF-mMIMO) systems to enhance simultaneous wireless information and power\ntransfer (SWIPT). To simultaneously support two groups of users-energy\nreceivers (ERs) and information receivers (IRs)-without sacrificing\ntime-frequency resources, a subset of access points (APs) is dedicated to\nserving ERs with the aid of a BD-RIS, while the remaining APs focus on\nsupporting IRs. A protective partial zero-forcing precoding technique is\nimplemented at the APs to manage the non-coherent interference between the ERs\nand IRs. Subsequently, closed-form expressions for the spectral efficiency of\nthe IRs and the average sum of harvested energy (HE) at the ERs are leveraged\nto formulate a comprehensive optimization problem. This problem jointly\noptimizes the AP selection, AP power control, and scattering matrix design at\nthe BD-RIS, all based on long-term statistical channel state information. This\nchallenging problem is then effectively transformed into more tractable forms.\nTo solve these sub-problems, efficient algorithms are proposed, including a\nheuristic search for the scattering matrix design, as well as successive convex\napproximation and deep reinforcement learning methods for the joint AP mode\nselection and power control design. Numerical results show that a BD-RIS with a\ngroup-or fully-connected architecture achieves significant EH gains over the\nconventional diagonal RIS, especially delivering up to a 7-fold increase in the\naverage sum of HE when a heuristic-based scattering matrix design is employed.\nIndex Terms-Beyond diagonal reconfigurable intelligent surface (BD-RIS),\ncell-free massive multiple-input multiple-output (CF-mMIMO), deep reinforcement\nlearning (DRL).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.00646", "cate": "cs.IT", "date": "2024-02-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2406.17196", "title": "Coded Kalman Filtering over MIMO Gaussian Channels with Feedback", "authors": ["Barron Han", "Oron Sabag", "Victoria Kostina", "Babak Hassibi"], "categories": ["cs.IT", "eess.SY"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.17196", "summary": "We consider the problem of remotely stabilizing a dynamical system. A sensor\n(encoder) co-located with the system communicates with a controller (decoder),\nwhose goal is to stabilize the system, over a noisy communication channel with\nfeedback. To accomplish this, the controller must estimate the system state\nwith finite mean squared error (MSE). The vector-valued dynamical system state\nfollows a Gauss-Markov law with additive control. The channel is a\nmultiple-input multiple-output (MIMO) additive white Gaussian noise (AWGN)\nchannel with feedback. For such a source, a linear encoder, and a MIMO AWGN\nchannel, the minimal MSE decoder is a Kalman filter. The parameters of the\nKalman filter and the linear encoder can be jointly optimized, under a power\nconstraint at the channel input. We term the resulting encoder-decoder pair a\ncoded Kalman filter. We establish sufficient and necessary conditions for the\ncoded Kalman filter to achieve a finite MSE in the real-time estimation of the\nstate. For sufficiency, we introduce a coding scheme where each unstable mode\nof the state is estimated using the channel outputs of a single sub-channel. We\nprove a coinciding necessity condition when either the source or channel is\nscalar and present a matrix-algebraic condition which implies the condition is\nnecessary in general. Finally, we provide a new counter-example demonstrating\nthat linear codes are generally sub-optimal for coding over MIMO channels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.17196", "cate": "cs.IT", "date": "2024-06-25", "updated": "2025-08-05", "section": "repl"}
{"id": "2409.16753", "title": "Perfect Hermitian rank-metric codes", "authors": ["Usman Mushrraf"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.16753", "summary": "This study investigates Hermitian rank-metric codes, a special class of\nrank-metric codes, focusing on perfect codes and on the analysis of their\ncovering properties. Firstly, we establish bounds on the size of spheres in the\nspace of Hermitian matrices and, as a consequence, we show that non-trivial\nperfect codes do not exist in the Hermitian case. We conclude the paper by\nexamining their covering density.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.16753", "cate": "cs.IT", "date": "2024-09-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.13801", "title": "SCAN-BEST: Sub-6GHz-Aided Near-field Beam Selection with Formal Reliability Guarantees", "authors": ["Weicao Deng", "Binpu Shi", "Min Li", "Osvaldo Simeone"], "categories": ["cs.IT", "eess.SP"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.13801", "summary": "As millimeter-wave (mmWave) MIMO systems adopt larger antenna arrays,\nnear-field propagation becomes increasingly prominent, especially for users\nclose to the transmitter. Traditional far-field beam training methods become\ninadequate, while near-field training faces the challenge of large codebooks\ndue to the need to resolve both angular and distance domains. To reduce in-band\ntraining overhead, prior work has proposed to leverage the spatial-temporal\ncongruence between sub-6 GHz (sub-6G) and mmWave channels to predict the best\nmmWave beam within a near-field codebook from sub-6G channel estimates. To cope\nwith the uncertainty caused by sub-6G/mmWave differences, we introduce a novel\nSub-6G Channel Aided Near-field BEam SelecTion (SCAN-BEST) framework that wraps\naround any beam predictor to produce candidate beam subset with formal\nsuboptimality guarantees. The proposed SCAN-BEST builds on conformal risk\ncontrol (CRC), and is calibrated offline using limited calibration data. Its\nperformance guarantees apply even in the presence of statistical shifts between\ncalibration and deployment. Numerical results validate the theoretical\nproperties and efficiency of SCAN-BEST.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.13801", "cate": "cs.IT", "date": "2025-03-18", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.19986", "title": "Performance Analysis of Spatiotemporal 2-D Polar Codes for Massive MIMO with MMSE Receivers", "authors": ["Yaqi Li", "Xiaohu You", "Jiamin Li", "Chen Ji", "Bin Sheng"], "categories": ["cs.IT"], "primary_category": "cs.IT", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19986", "summary": "With the evolution from 5G to 6G, ultra-reliable low-latency communication\n(URLLC) faces increasingly stringent performance requirements. Lower latency\nconstraints demand shorter channel coding lengths, which can severely degrade\ndecoding performance. The massive multiple-input multiple-output (MIMO) system\nis considered a crucial technology to address this challenge due to its\nabundant spatial degrees of freedom (DoF). While polar codes are theoretically\ncapacity-achieving in the limit of infinite code length, their practical\napplicability is limited by significant decoding latency. In this paper, we\nestablish a unified theoretical framework and propose a novel spatiotemporal\ntwo-dimensional (2-D) polar coding scheme for massive MIMO systems employing\nminimum mean square error (MMSE) receivers. The polar transform is jointly\napplied over both spatial and temporal dimensions to fully exploit the large\nspatial DoF. By leveraging the near-deterministic\nsignal-to-interference-plus-noise ratio (SINR) property of MMSE detection, the\nspatial domain is modeled as a set of parallel Gaussian sub-channels. Within\nthis framework, we perform a theoretical analysis of the 2-D polarization\nbehavior using the Gaussian approximation method, and the capacity-achieving\nproperty of the proposed scheme is proved under finite blocklength constraints\nand large spatial DoF. Simulation results further demonstrate that, compared to\ntraditional time-domain polar codes, the proposed 2-D scheme can significantly\nreduce latency while guaranteeing reliability, or alternatively improve\nreliability under the same latency constraint -- offering a capacity-achieving\nand latency-efficient channel coding solution for massive MIMO systems in\nfuture 6G URLLC scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19986", "cate": "cs.IT", "date": "2025-07-26", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.13922", "title": "Exponentially Consistent Nonparametric Linkage-Based Clustering of Data Sequences", "authors": ["Bhupender Singh", "Ananth Ram Rajagopalan", "Srikrishna Bhashyam"], "categories": ["cs.IT", "cs.LG", "eess.SP", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.13922", "summary": "In this paper, we consider nonparametric clustering of $M$ independent and\nidentically distributed (i.i.d.) data sequences generated from {\\em unknown}\ndistributions. The distributions of the $M$ data sequences belong to $K$\nunderlying distribution clusters. Existing results on exponentially consistent\nnonparametric clustering algorithms, like single linkage-based (SLINK)\nclustering and $k$-medoids distribution clustering, assume that the maximum\nintra-cluster distance ($d_L$) is smaller than the minimum inter-cluster\ndistance ($d_H$). First, in the fixed sample size (FSS) setting, we show that\nexponential consistency can be achieved for SLINK clustering under a less\nstrict assumption, $d_I < d_H$, where $d_I$ is the maximum distance between any\ntwo sub-clusters of a cluster that partition the cluster. Note that $d_I < d_L$\nin general. Thus, our results show that SLINK is exponentially consistent for a\nlarger class of problems than previously known. In our simulations, we also\nidentify examples where $k$-medoids clustering is unable to find the true\nclusters, but SLINK is exponentially consistent. Then, we propose a sequential\nclustering algorithm, named SLINK-SEQ, based on SLINK and prove that it is also\nexponentially consistent. Simulation results show that the SLINK-SEQ algorithm\nrequires fewer expected number of samples than the FSS SLINK algorithm for the\nsame probability of error.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.13922", "cate": "stat.ML", "date": "2024-11-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03730", "title": "PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression", "authors": ["Kefei Wu", "Baihua Zheng", "Weiwei Sun"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03730v1", "summary": "Location-aware devices continuously generate massive volumes of trajectory\ndata, creating demand for efficient compression. Line simplification is a\ncommon solution but typically assumes 2D trajectories and ignores time\nsynchronization and motion continuity. We propose PILOT-C, a novel trajectory\ncompression framework that integrates frequency-domain physics modeling with\nerror-bounded optimization. Unlike existing line simplification methods,\nPILOT-C supports trajectories in arbitrary dimensions, including 3D, by\ncompressing each spatial axis independently. Evaluated on four real-world\ndatasets, PILOT-C achieves superior performance across multiple dimensions. In\nterms of compression ratio, PILOT-C outperforms CISED-W, the current\nstate-of-the-art SED-based line simplification algorithm, by an average of\n19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction\nin error compared to CISED-W. Additionally, PILOT-C seamlessly extends to\nthree-dimensional trajectories while maintaining the same computational\ncomplexity, achieving a 49% improvement in compression ratios over SQUISH-E,\nthe most efficient line simplification algorithm on 3D datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03730v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30", "section": "new"}
{"id": "2508.03766", "title": "LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation", "authors": ["Yongchao Huang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03766v1", "summary": "The specification of prior distributions is fundamental in Bayesian\ninference, yet it remains a significant bottleneck. The prior elicitation\nprocess is often a manual, subjective, and unscalable task. We propose a novel\nframework which leverages Large Language Models (LLMs) to automate and scale\nthis process. We introduce \\texttt{LLMPrior}, a principled operator that\ntranslates rich, unstructured contexts such as natural language descriptions,\ndata or figures into valid, tractable probability distributions. We formalize\nthis operator by architecturally coupling an LLM with an explicit, tractable\ngenerative model, such as a Gaussian Mixture Model (forming a LLM based Mixture\nDensity Network), ensuring the resulting prior satisfies essential mathematical\nproperties. We further extend this framework to multi-agent systems where\nLogarithmic Opinion Pooling is employed to aggregate prior distributions\ninduced by decentralized knowledge. We present the federated prior aggregation\nalgorithm, \\texttt{Fed-LLMPrior}, for aggregating distributed,\ncontext-dependent priors in a manner robust to agent heterogeneity. This work\nprovides the foundation for a new class of tools that can potentially lower the\nbarrier to entry for sophisticated Bayesian modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03766v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03768", "title": "Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings", "authors": ["Debamita Ghosh", "George K. Atia", "Yue Wang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03768v1", "summary": "Reinforcement learning (RL) faces significant challenges in real-world\ndeployments due to the sim-to-real gap, where policies trained in simulators\noften underperform in practice due to mismatches between training and\ndeployment conditions. Distributionally robust RL addresses this issue by\noptimizing worst-case performance over an uncertainty set of environments and\nproviding an optimized lower bound on deployment performance. However, existing\nstudies typically assume access to either a generative model or offline\ndatasets with broad coverage of the deployment environment -- assumptions that\nlimit their practicality in unknown environments without prior knowledge. In\nthis work, we study the more realistic and challenging setting of online\ndistributionally robust RL, where the agent interacts only with a single\nunknown training environment while aiming to optimize its worst-case\nperformance. We focus on general $f$-divergence-based uncertainty sets,\nincluding Chi-Square and KL divergence balls, and propose a computationally\nefficient algorithm with sublinear regret guarantees under minimal assumptions.\nFurthermore, we establish a minimax lower bound on regret of online learning,\ndemonstrating the near-optimality of our approach. Extensive experiments across\ndiverse environments further confirm the robustness and efficiency of our\nalgorithm, validating our theoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03768v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03820", "title": "Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation", "authors": ["Igor Sokolov", "Abdurakhmon Sadiev", "Yury Demidovich", "Fawaz S Al-Qahtani", "Peter RichtÃ¡rik"], "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03820v1", "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for\nadapting large foundational models to specific tasks, particularly as model\nsizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation\n(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,\nexpressing adaptations as a product of two low-rank matrices. While extensive\nempirical studies demonstrate LoRA's practical utility, theoretical\nunderstanding of such methods remains limited. Recent work on RAC-LoRA\n(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,\nwe introduce Bernoulli-LoRA, a novel theoretical framework that unifies and\nextends existing LoRA approaches. Our method introduces a probabilistic\nBernoulli mechanism for selecting which matrix to update. This approach\nencompasses and generalizes various existing update strategies while\nmaintaining theoretical tractability. Under standard assumptions from\nnon-convex optimization literature, we analyze several variants of our\nframework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,\nBernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and\nBernoulli-LoRA-EF21, establishing convergence guarantees for each variant.\nAdditionally, we extend our analysis to convex non-smooth functions, providing\nconvergence rates for both constant and adaptive (Polyak-type) stepsizes.\nThrough extensive experiments on various tasks, we validate our theoretical\nfindings and demonstrate the practical efficacy of our approach. This work is a\nstep toward developing theoretically grounded yet practically effective PEFT\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03820v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03827", "title": "Scalable Neural Network-based Blackbox Optimization", "authors": ["Pavankumar Koratikere", "Leifur Leifsson"], "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03827v1", "summary": "Bayesian Optimization (BO) is a widely used approach for blackbox\noptimization that leverages a Gaussian process (GP) model and an acquisition\nfunction to guide future sampling. While effective in low-dimensional settings,\nBO faces scalability challenges in high-dimensional spaces and with large\nnumber of function evaluations due to the computational complexity of GP\nmodels. In contrast, neural networks (NNs) offer better scalability and can\nmodel complex functions, which led to the development of NN-based BO\napproaches. However, these methods typically rely on estimating model\nuncertainty in NN prediction -- a process that is often computationally\nintensive and complex, particularly in high dimensions. To address these\nlimitations, a novel method, called scalable neural network-based blackbox\noptimization (SNBO), is proposed that does not rely on model uncertainty\nestimation. Specifically, SNBO adds new samples using separate criteria for\nexploration and exploitation, while adaptively controlling the sampling region\nto ensure efficient optimization. SNBO is evaluated on a range of optimization\nproblems spanning from 10 to 102 dimensions and compared against four\nstate-of-the-art baseline algorithms. Across the majority of test problems,\nSNBO attains function values better than the best-performing baseline\nalgorithm, while requiring 40-60% fewer function evaluations and reducing the\nruntime by at least an order of magnitude.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03827v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03836", "title": "DP-NCB: Privacy Preserving Fair Bandits", "authors": ["Dhruv Sarkar", "Nishant Pandey", "Sayak Ray Chowdhury"], "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03836v1", "summary": "Multi-armed bandit algorithms are fundamental tools for sequential\ndecision-making under uncertainty, with widespread applications across domains\nsuch as clinical trials and personalized decision-making. As bandit algorithms\nare increasingly deployed in these socially sensitive settings, it becomes\ncritical to protect user data privacy and ensure fair treatment across decision\nrounds. While prior work has independently addressed privacy and fairness in\nbandit settings, the question of whether both objectives can be achieved\nsimultaneously has remained largely open. Existing privacy-preserving bandit\nalgorithms typically optimize average regret, a utilitarian measure, whereas\nfairness-aware approaches focus on minimizing Nash regret, which penalizes\ninequitable reward distributions, but often disregard privacy concerns.\n  To bridge this gap, we introduce Differentially Private Nash Confidence Bound\n(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures\n$\\epsilon$-differential privacy and achieves order-optimal Nash regret,\nmatching known lower bounds up to logarithmic factors. The framework is\nsufficiently general to operate under both global and local differential\nprivacy models, and is anytime, requiring no prior knowledge of the time\nhorizon. We support our theoretical guarantees with simulations on synthetic\nbandit instances, showing that DP-NCB incurs substantially lower Nash regret\nthan state-of-the-art baselines. Our results offer a principled foundation for\ndesigning bandit algorithms that are both privacy-preserving and fair, making\nthem suitable for high-stakes, socially impactful applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03836v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03863", "title": "Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning", "authors": ["Amin Farajzadeh", "Hongzhao Zheng", "Sarah Dumoulin", "Trevor Ha", "Halim Yanikomeroglu", "Amir Ghasemi"], "categories": ["cs.LG", "cs.NI", "eess.SP"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03863v1", "summary": "Accurate spectrum demand prediction is crucial for informed spectrum\nallocation, effective regulatory planning, and fostering sustainable growth in\nmodern wireless communication networks. It supports governmental efforts,\nparticularly those led by the international telecommunication union (ITU), to\nestablish fair spectrum allocation policies, improve auction mechanisms, and\nmeet the requirements of emerging technologies such as advanced 5G, forthcoming\n6G, and the internet of things (IoT). This paper presents an effective\nspatio-temporal prediction framework that leverages crowdsourced user-side key\nperformance indicators (KPIs) and regulatory datasets to model and forecast\nspectrum demand. The proposed methodology achieves superior prediction accuracy\nand cross-regional generalizability by incorporating advanced feature\nengineering, comprehensive correlation analysis, and transfer learning\ntechniques. Unlike traditional ITU models, which are often constrained by\narbitrary inputs and unrealistic assumptions, this approach exploits granular,\ndata-driven insights to account for spatial and temporal variations in spectrum\nutilization. Comparative evaluations against ITU estimates, as the benchmark,\nunderscore our framework's capability to deliver more realistic and actionable\npredictions. Experimental results validate the efficacy of our methodology,\nhighlighting its potential as a robust approach for policymakers and regulatory\nbodies to enhance spectrum management and planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03863v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03868", "title": "Prediction-Oriented Subsampling from Data Streams", "authors": ["Benedetta Lavinia Mussati", "Freddie Bickford Smith", "Tom Rainforth", "Stephen Roberts"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03868v1", "summary": "Data is often generated in streams, with new observations arriving over time.\nA key challenge for learning models from data streams is capturing relevant\ninformation while keeping computational costs manageable. We explore\nintelligent data subsampling for offline learning, and argue for an\ninformation-theoretic method centred on reducing uncertainty in downstream\npredictions of interest. Empirically, we demonstrate that this\nprediction-oriented approach performs better than a previously proposed\ninformation-theoretic technique on two widely studied problems. At the same\ntime, we highlight that reliably achieving strong performance in practice\nrequires careful model design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03868v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03875", "title": "Reinforcement Learning for Target Zone Blood Glucose Control", "authors": ["David H. Mguni", "Jing Dong", "Wanrong Yang", "Ziquan Liu", "Muhammad Salman Haleem", "Baoxiang Wang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03875v1", "summary": "Managing physiological variables within clinically safe target zones is a\ncentral challenge in healthcare, particularly for chronic conditions such as\nType 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for\npersonalising treatment, but struggles with the delayed and heterogeneous\neffects of interventions. We propose a novel RL framework to study and support\ndecision-making in T1DM technologies, such as automated insulin delivery. Our\napproach captures the complex temporal dynamics of treatment by unifying two\ncontrol modalities: \\textit{impulse control} for discrete, fast-acting\ninterventions (e.g., insulin boluses), and \\textit{switching control} for\nlonger-acting treatments and regime shifts. The core of our method is a\nconstrained Markov decision process augmented with physiological state\nfeatures, enabling safe policy learning under clinical and resource\nconstraints. The framework incorporates biologically realistic factors,\nincluding insulin decay, leading to policies that better reflect real-world\ntherapeutic behaviour. While not intended for clinical deployment, this work\nestablishes a foundation for future safe and temporally-aware RL in healthcare.\nWe provide theoretical guarantees of convergence and demonstrate empirical\nimprovements in a stylised T1DM control task, reducing blood glucose level\nviolations from 22.4\\% (state-of-the-art) to as low as 10.8\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03875v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03926", "title": "Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning", "authors": ["Hector Vargas Alvarez", "Dimitrios G. Patsatzis", "Lucia Russo", "Ioannis Kevrekidis", "Constantinos Siettos"], "categories": ["cs.LG", "math.DS", "math.NA"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03926v1", "summary": "Bridging the microscopic and the macroscopic modelling scales in crowd\ndynamics constitutes an important, open challenge for systematic numerical\nanalysis, optimization, and control. We propose a combined manifold and machine\nlearning approach to learn the discrete evolution operator for the emergent\ncrowd dynamics in latent spaces from high-fidelity agent-based simulations. The\nproposed framework builds upon our previous works on next-generation\nEquation-free algorithms on learning surrogate models for high-dimensional and\nmultiscale systems. Our approach is a four-stage one, explicitly conserving the\nmass of the reconstructed dynamics in the high-dimensional space. In the first\nstep, we derive continuous macroscopic fields (densities) from discrete\nmicroscopic data (pedestrians' positions) using KDE. In the second step, based\non manifold learning, we construct a map from the macroscopic ambient space\ninto the latent space parametrized by a few coordinates based on POD of the\ncorresponding density distribution. The third step involves learning\nreduced-order surrogate ROMs in the latent space using machine learning\ntechniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the\ncrowd dynamics in the high-dimensional space in terms of macroscopic density\nprofiles. We demonstrate that the POD reconstruction of the density\ndistribution via SVD conserves the mass. With this \"embed->learn in latent\nspace->lift back to the ambient space\" pipeline, we create an effective\nsolution operator of the unavailable macroscopic PDE for the density evolution.\nFor our illustrations, we use the Social Force Model to generate data in a\ncorridor with an obstacle, imposing periodic boundary conditions. The numerical\nresults demonstrate high accuracy, robustness, and generalizability, thus\nallowing for fast and accurate modelling/simulation of crowd dynamics from\nagent-based simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03926v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03934", "title": "Markov Chain Estimation with In-Context Learning", "authors": ["Simon Lepage", "Jeremie Mary", "David Picard"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03934v1", "summary": "We investigate the capacity of transformers to learn algorithms involving\ntheir context while solely being trained using next token prediction. We set up\nMarkov chains with random transition matrices and we train transformers to\npredict the next token. Matrices used during training and test are different\nand we show that there is a threshold in transformer size and in training set\nsize above which the model is able to learn to estimate the transition\nprobabilities from its context instead of memorizing the training patterns.\nAdditionally, we show that more involved encoding of the states enables more\nrobust prediction for Markov chains with structures different than those seen\nduring training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03934v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03965", "title": "BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics", "authors": ["Yunhao Zhang", "Lin Cheng", "Aswin Gnanaskandan", "Ameya D. Jagtap"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03965v1", "summary": "This paper introduces BubbleONet, an operator learning model designed to map\npressure profiles from an input function space to corresponding bubble radius\nresponses. BubbleONet is built upon the physics-informed deep operator network\n(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation\ncapabilities for operator learning alongside the robust physical fidelity\nprovided by the physics-informed neural networks. To mitigate the inherent\nspectral bias in deep learning, BubbleONet integrates the Rowdy adaptive\nactivation function, enabling improved representation of high-frequency\nfeatures. The model is evaluated across various scenarios, including: (1)\nRayleigh-Plesset equation based bubble dynamics with a single initial radius,\n(2) Keller-Miksis equation based bubble dynamics with a single initial radius,\nand (3) Keller-Miksis equation based bubble dynamics with multiple initial\nradii. Moreover, the performance of single-step versus two-step training\ntechniques for BubbleONet is investigated. The results demonstrate that\nBubbleONet serves as a promising surrogate model for simulating bubble\ndynamics, offering a computationally efficient alternative to traditional\nnumerical solvers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03965v1", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03999", "title": "Tensorized Clustered LoRA Merging for Multi-Task Interference", "authors": ["Zhan Su", "Fengran Mo", "Guojun Liang", "Jinghan Zhang", "Bingbing Wen", "Prayag Tiwari", "Jian-Yun Nie"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03999v1", "summary": "Despite the success of the monolithic dense paradigm of large language models\n(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small\ntask-specific modules and merging them with the base model. However, in\nmulti-task settings, merging LoRA adapters trained on heterogeneous sources\nfrequently causes \\textit{task interference}, degrading downstream performance.\nTo address this, we propose a tensorized clustered LoRA (TC-LoRA) library\ntargeting to address the task interference at the \\textit{text-level} and\n\\textit{parameter-level}. At the \\textit{text-level}, we cluster the training\nsamples in the embedding space to capture input-format similarities, then train\na specialized LoRA adapter for each cluster. At the \\textit{parameter-level},\nwe introduce a joint Canonical Polyadic (CP) decomposition that disentangles\ntask-specific and shared factors across LoRA adapters. This joint factorization\npreserves essential knowledge while reducing cross-task interference. Extensive\nexperiments on out-of-domain zero-shot and skill-composition tasks-including\nreasoning, question answering, and coding. Compared to strong SVD-based\nbaselines, TC-LoRA achieves +1.4\\% accuracy on Phi-3 and +2.3\\% on Mistral-7B\n(+2.3\\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03999v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04005", "title": "Decoupled Contrastive Learning for Federated Learning", "authors": ["Hyungbin Kim", "Incheol Baek", "Yon Dohn Chung"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04005v1", "summary": "Federated learning is a distributed machine learning paradigm that allows\nmultiple participants to train a shared model by exchanging model updates\ninstead of their raw data. However, its performance is degraded compared to\ncentralized approaches due to data heterogeneity across clients. While\ncontrastive learning has emerged as a promising approach to mitigate this, our\ntheoretical analysis reveals a fundamental conflict: its asymptotic assumptions\nof an infinite number of negative samples are violated in finite-sample regime\nof federated learning. To address this issue, we introduce Decoupled\nContrastive Learning for Federated Learning (DCFL), a novel framework that\ndecouples the existing contrastive loss into two objectives. Decoupling the\nloss into its alignment and uniformity components enables the independent\ncalibration of the attraction and repulsion forces without relying on the\nasymptotic assumptions. This strategy provides a contrastive learning method\nsuitable for federated learning environments where each client has a small\namount of data. Our experimental results show that DCFL achieves stronger\nalignment between positive samples and greater uniformity between negative\nsamples compared to existing contrastive learning methods. Furthermore,\nexperimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and\nTiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art\nfederated learning methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04005v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04045", "title": "FeDaL: Federated Dataset Learning for Time Series Foundation Models", "authors": ["Shengchao Chen", "Guodong Long", "Jing Jiang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04045v1", "summary": "Dataset-wise heterogeneity introduces significant domain biases that\nfundamentally degrade generalization on Time Series Foundation Models (TSFMs),\nyet this challenge remains underexplored. This paper rethink the development of\nTSFMs using the paradigm of federated learning. We propose a novel Federated\nDataset Learning (FeDaL) approach to tackle heterogeneous time series by\nlearning dataset-agnostic temporal representations. Specifically, the\ndistributed architecture of federated learning is a nature solution to\ndecompose heterogeneous TS datasets into shared generalized knowledge and\npreserved personalized knowledge. Moreover, based on the TSFM architecture,\nFeDaL explicitly mitigates both local and global biases by adding two\ncomplementary mechanisms: Domain Bias Elimination (DBE) and Global Bias\nElimination (GBE). FeDaL`s cross-dataset generalization has been extensively\nevaluated in real-world datasets spanning eight tasks, including both\nrepresentation learning and downstream time series analysis, against 54\nbaselines. We further analyze federated scaling behavior, showing how data\nvolume, client count, and join rate affect model performance under\ndecentralization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04045v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04048", "title": "Quantum Temporal Fusion Transformer", "authors": ["Krishnakanta Barik", "Goutam Paul"], "categories": ["cs.LG", "quant-ph"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04048v1", "summary": "The Temporal Fusion Transformer (TFT), proposed by Lim et al.\n[\\textit{International Journal of Forecasting}, 2021], is a state-of-the-art\nattention-based deep neural network architecture specifically designed for\nmulti-horizon time series forecasting. It has demonstrated significant\nperformance improvements over existing benchmarks. In this work, we propose a\nQuantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid\nquantum-classical architecture that extends the capabilities of the classical\nTFT framework. Our results demonstrate that QTFT is successfully trained on the\nforecasting datasets and is capable of accurately predicting future values. In\nparticular, our experimental results display that in certain test cases, the\nmodel outperforms its classical counterpart in terms of both training and test\nloss, while in the remaining cases, it achieves comparable performance. A key\nadvantage of our approach lies in its foundation on a variational quantum\nalgorithm, enabling implementation on current noisy intermediate-scale quantum\n(NISQ) devices without strict requirements on the number of qubits or circuit\ndepth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04048v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04063", "title": "Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading", "authors": ["Joel Walsh", "Siddarth Mamidanna", "Benjamin Nye", "Mark Core", "Daniel Auerbach"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04063v1", "summary": "Research to improve Automated Short Answer Grading has recently focused on\nLarge Language Models (LLMs) with prompt engineering and no- or few-shot\nprompting to achieve best results. This is in contrast to the fine-tuning\napproach, which has historically required large-scale compute clusters\ninaccessible to most users. New closed-model approaches such as OpenAI's\nfine-tuning service promise results with as few as 100 examples, while methods\nusing open weights such as quantized low-rank adaptive (QLORA) can be used to\nfine-tune models on consumer GPUs. We evaluate both of these fine-tuning\nmethods, measuring their interaction with few-shot prompting for automated\nshort answer grading (ASAG) with structured (JSON) outputs. Our results show\nthat finetuning with small amounts of data has limited utility for Llama\nopen-weight models, but that fine-tuning methods can outperform few-shot\nbaseline instruction-tuned LLMs for OpenAI's closed models. While our\nevaluation set is limited, we find some evidence that the observed benefits of\nfinetuning may be impacted by the domain subject matter. Lastly, we observed\ndramatic improvement with the LLama 3.1 8B-Instruct open-weight model by\nseeding the initial training examples with a significant amount of cheaply\ngenerated synthetic training data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04063v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04071", "title": "Adversarial Fair Multi-View Clustering", "authors": ["Mudi Jiang", "Jiahui Zhou", "Lianyu Hu", "Xinying Liu", "Zengyou He", "Zhikui Chen"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04071v1", "summary": "Cluster analysis is a fundamental problem in data mining and machine\nlearning. In recent years, multi-view clustering has attracted increasing\nattention due to its ability to integrate complementary information from\nmultiple views. However, existing methods primarily focus on clustering\nperformance, while fairness-a critical concern in human-centered\napplications-has been largely overlooked. Although recent studies have explored\ngroup fairness in multi-view clustering, most methods impose explicit\nregularization on cluster assignments, relying on the alignment between\nsensitive attributes and the underlying cluster structure. However, this\nassumption often fails in practice and can degrade clustering performance. In\nthis paper, we propose an adversarial fair multi-view clustering (AFMVC)\nframework that integrates fairness learning into the representation learning\nprocess. Specifically, our method employs adversarial training to fundamentally\nremove sensitive attribute information from learned features, ensuring that the\nresulting cluster assignments are unaffected by it. Furthermore, we\ntheoretically prove that aligning view-specific clustering assignments with a\nfairness-invariant consensus distribution via KL divergence preserves\nclustering consistency without significantly compromising fairness, thereby\nproviding additional theoretical guarantees for our framework. Extensive\nexperiments on data sets with fairness constraints demonstrate that AFMVC\nachieves superior fairness and competitive clustering performance compared to\nexisting multi-view clustering and fairness-aware clustering methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04071v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04097", "title": "Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?", "authors": ["Ngoc-Bao Nguyen", "Sy-Tuyen Ho", "Koh Jun Hao", "Ngai-Man Cheung"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04097v1", "summary": "Model inversion (MI) attacks pose significant privacy risks by reconstructing\nprivate training data from trained neural networks. While prior works have\nfocused on conventional unimodal DNNs, the vulnerability of vision-language\nmodels (VLMs) remains underexplored. In this paper, we conduct the first study\nto understand VLMs' vulnerability in leaking private visual training data. To\ntailored for VLMs' token-based generative nature, we propose a suite of novel\ntoken-based and sequence-based model inversion strategies. Particularly, we\npropose Token-based Model Inversion (TMI), Convergent Token-based Model\nInversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based\nModel Inversion with Adaptive Token Weighting (SMI-AW). Through extensive\nexperiments and user study on three state-of-the-art VLMs and multiple\ndatasets, we demonstrate, for the first time, that VLMs are susceptible to\ntraining data leakage. The experiments show that our proposed sequence-based\nmethods, particularly SMI-AW combined with a logit-maximization loss based on\nvocabulary representation, can achieve competitive reconstruction and\noutperform token-based methods in attack accuracy and visual similarity.\nImportantly, human evaluation of the reconstructed images yields an attack\naccuracy of 75.31\\%, underscoring the severity of model inversion threats in\nVLMs. Notably we also demonstrate inversion attacks on the publicly released\nVLMs. Our study reveals the privacy vulnerability of VLMs as they become\nincreasingly popular across many applications such as healthcare and finance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04097v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03947", "title": "Control Closure Certificates", "authors": ["Vishnu Murali", "Mohammed Adib Oumer", "Majid Zamani"], "categories": ["cs.LO", "eess.SY"], "primary_category": "cs.LO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03947v1", "summary": "This paper introduces the notion of control closure certificates to\nsynthesize controllers for discrete-time control systems against\n$\\omega$-regular specifications. Typical functional approaches to synthesize\ncontrollers against $\\omega$-regular specifications rely on combining inductive\ninvariants (for example, via barrier certificates) with proofs of\nwell-foundedness (for example, via ranking functions). Transition invariants,\nprovide an alternative where instead of standard well-foundedness arguments one\nmay instead search for disjunctive well-foundedness arguments that together\nensure a well-foundedness argument. Closure certificates, functional analogs of\ntransition invariants, provide an effective, automated approach to verify\ndiscrete-time dynamical systems against linear temporal logic and\n$\\omega$-regular specifications. We build on this notion to synthesize\ncontrollers to ensure the satisfaction of $\\omega$-regular specifications. To\ndo so, we first illustrate how one may construct control closure certificates\nto visit a region infinitely often (or only finitely often) via disjunctive\nwell-founded arguments. We then combine these arguments to provide an argument\nfor parity specifications. Thus, finding an appropriate control closure\ncertificate over the product of the system and a parity automaton specifying a\ndesired $\\omega$-regular specification ensures that there exists a controller\n$\\kappa$ to enforce the $\\omega$-regular specification. We propose a\nsum-of-squares optimization approach to synthesize such certificates and\ndemonstrate their efficacy in designing controllers over some case studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03947v1", "cate": "cs.LO", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04438", "title": "GradSTL: Comprehensive Signal Temporal Logic for Neurosymbolic Reasoning and Learning", "authors": ["Mark Chevallier", "Filip Smola", "Richard Schmoetten", "Jacques D. Fleuriot"], "categories": ["cs.LO"], "primary_category": "cs.LO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04438v1", "summary": "We present GradSTL, the first fully comprehensive implementation of signal\ntemporal logic (STL) suitable for integration with neurosymbolic learning. In\nparticular, GradSTL can successfully evaluate any STL constraint over any\nsignal, regardless of how it is sampled. Our formally verified approach\nspecifies smooth STL semantics over tensors, with formal proofs of soundness\nand of correctness of its derivative function. Our implementation is generated\nautomatically from this formalisation, without manual coding, guaranteeing\ncorrectness by construction. We show via a case study that using our\nimplementation, a neurosymbolic process learns to satisfy a pre-specified STL\nconstraint. Our approach offers a highly rigorous foundation for integrating\nsignal temporal logic and learning by gradient descent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04438v1", "cate": "cs.LO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04296", "title": "The decohered ZX-calculus", "authors": ["Titouan Carette", "Daniela Cojocaru", "Renaud Vilmart"], "categories": ["cs.LO", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04296", "summary": "The discard ZX-calculus is known to be complete and universal for mixed-state\nquantum mechanics, allowing for both quantum and classical processes. However,\nif the quantum aspects of ZX-calculus have been explored in depth, little work\nhas been done on the classical side. In this paper, we investigate a fragment\nof discard ZX-calculus obtained by decohering the usual generators of\nZX-calculus. We show that this calculus is universal and complete for affinely\nsupported probability distributions over $\\mathbb{F}_{2}^{n}$. To do so, we\nexhibit a normal form, mixing ideas from the graphical linear algebra program\nand diagrammatic Fourier transforms. Our results both clarify how to handle\nhybrid classical-quantum processes in the discard ZX-calculus and pave the way\nto the picturing of more general random variables and probabilistic processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04296", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2306.13756", "title": "A Fully Abstract Model of PCF Based on Extended Addressing Machines", "authors": ["Benedetto Intrigila", "Giulio Manzonetto", "Nicolas Munnich"], "categories": ["cs.LO"], "primary_category": "cs.LO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.13756", "summary": "Extended addressing machines (EAMs) have been introduced to represent\nhigher-order sequential computations. Previously, we have shown that they are\ncapable of simulating -- via an easy encoding -- the operational semantics of\nPCF, extended with explicit substitutions. In this paper we prove that the\nsimulation is actually an equivalence: a PCF program terminates in a numeral\nexactly when the corresponding EAM terminates in the same numeral. It follows\nthat the model of PCF obtained by quotienting typable EAMs by a suitable\nlogical relation is adequate. From a definability result stating that every EAM\nin the model can be transformed into a PCF program with the same observational\nbehavior, we conclude that the model is fully abstract for PCF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.13756", "cate": "cs.LO", "date": "2023-06-23", "updated": "2025-08-05", "section": "repl"}
{"id": "2504.08509", "title": "The Complexity of Generalized HyperLTL with Stuttering and Contexts (full version)", "authors": ["GaÃ«tan Regaud", "Martin Zimmermann"], "categories": ["cs.LO"], "primary_category": "cs.LO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08509", "summary": "We settle the complexity of satisfiability and model-checking for generalized\nHyperLTL with stuttering and contexts, an expressive logic for the\nspecification of asynchronous hyperproperties. Such properties cannot be\nspecified in HyperLTL, as it is restricted to synchronous hyperproperties.\n  Nevertheless, we prove that satisfiability is $\\Sigma_1^1$-complete and thus\nnot harder than for HyperLTL. On the other hand, we prove that model-checking\nis equivalent to truth in second-order arithmetic, and thus much harder than\nthe decidable HyperLTL model-checking problem. The lower bounds for the\nmodel-checking problem hold even when only allowing stuttering or only allowing\ncontexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08509", "cate": "cs.LO", "date": "2025-04-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2303.05623", "title": "Relating homotopy equivalences to conservativity in dependent type theories with computation axioms", "authors": ["Matteo Spadetto"], "categories": ["cs.LO", "math.CT", "math.LO"], "primary_category": "math.LO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.05623", "summary": "We prove a conservativity result for extensional type theories over\npropositional ones, i.e. dependent type theories with propositional computation\nrules, or computation axioms, using insights from homotopy type theory. The\nargument exploits a notion of canonical homotopy equivalence between contexts,\nand uses the notion of a category with attributes to phrase the semantics of\ntheories of dependent types. Informally, our main result asserts that, for\njudgements essentially concerning h-sets, reasoning with extensional or\npropositional type theories is equivalent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.05623", "cate": "math.LO", "date": "2023-03-09", "updated": "2025-08-06", "section": "repl"}
{"id": "2401.01096", "title": "Demystifying $Î¼$", "authors": ["Bahareh Afshari", "Graham E. Leigh", "Guillermo MenÃ¨ndez Turata"], "categories": ["cs.LO", "math.LO"], "primary_category": "math.LO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.01096", "summary": "We explore the theory of illfounded and cyclic proofs for the propositional\nmodal $\\mu$-calculus. A fine analysis of provability for classical and\nintuitionistic modal logic provides a novel bridge between finitary, cyclic and\nillfounded conceptions of proof and re-enforces the importance of two normal\nform theorems for the logic: guardedness and disjunctiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.01096", "cate": "math.LO", "date": "2024-01-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04165", "title": "Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations", "authors": ["Md Shazid Islam", "A S M Jahid Hasan", "Md Saydur Rahman", "Md Saiful Islam Sajol"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04165v1", "summary": "Accurate solar generation prediction is essential for proper estimation of\nrenewable energy resources across diverse geographic locations. However,\ngeographical and weather features vary from location to location which\nintroduces domain shift - a major bottleneck to develop location-agnostic\nprediction model. As a result, a machine-learning model which can perform well\nto predict solar power in one location, may exhibit subpar performance in\nanother location. Moreover, the lack of properly labeled data and storage\nissues make the task even more challenging. In order to address domain shift\ndue to varying weather conditions across different meteorological regions, this\npaper presents a semi-supervised deep domain adaptation framework, allowing\naccurate predictions with minimal labeled data from the target location. Our\napproach involves training a deep convolutional neural network on a source\nlocation's data and adapting it to the target location using a source-free,\nteacher-student model configuration. The teacher-student model leverages\nconsistency and cross-entropy loss for semi-supervised learning, ensuring\neffective adaptation without any source data requirement for prediction. With\nannotation of only $20 \\%$ data in the target domain, our approach exhibits an\nimprovement upto $11.36 \\%$, $6.65 \\%$, $4.92\\%$ for California, Florida and\nNew York as target domain, respectively in terms of accuracy in predictions\nwith respect to non-adaptive approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04165v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04180", "title": "One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra", "authors": ["Neng Kai Nigel Neo", "Lim Jing", "Ngoui Yong Zhau Preston", "Koh Xue Ting Serene", "Bingquan Shen"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04180v1", "summary": "A common approach to the \\emph{de novo} molecular generation problem from\nmass spectra involves a two-stage pipeline: (1) encoding mass spectra into\nmolecular fingerprints, followed by (2) decoding these fingerprints into\nmolecular structures. In our work, we adopt\n\\textsc{MIST}~\\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder\nand \\textsc{MolForge}~\\citep{ucakReconstructionLosslessMolecular2023} as the\ndecoder, leveraging pretraining to enhance performance. Notably, pretraining\n\\textsc{MolForge} proves especially effective, enabling it to serve as a robust\nfingerprint-to-structure decoder. Additionally, instead of passing the\nprobability of each bit in the fingerprint, thresholding the probabilities as a\nstep function helps focus the decoder on the presence of substructures,\nimproving recovery of accurate molecular structures even when the fingerprints\npredicted by \\textsc{MIST} only moderately resembles the ground truth in terms\nof Tanimoto similarity. This combination of encoder and decoder results in a\ntenfold improvement over previous state-of-the-art methods, generating top-1\n28\\% / top-10 36\\% of molecular structures correctly from mass spectra. We\nposition this pipeline as a strong baseline for future research in \\emph{de\nnovo} molecule elucidation from mass spectra.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04180v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04193", "title": "Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes", "authors": ["Chengcheng Yan", "Jiawei Xu", "Zheng Peng", "Qingsong Wang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04193v1", "summary": "The training of deep neural networks is inherently a nonconvex optimization\nproblem, yet standard approaches such as stochastic gradient descent (SGD)\nrequire simultaneous updates to all parameters, often leading to unstable\nconvergence and high computational cost. To address these issues, we propose a\nnovel method, Stochastic Alternating Minimization with Trainable Step Sizes\n(SAMT), which updates network parameters in an alternating manner by treating\nthe weights of each layer as a block. By decomposing the overall optimization\ninto sub-problems corresponding to different blocks, this block-wise\nalternating strategy reduces per-step computational overhead and enhances\ntraining stability in nonconvex settings. To fully leverage these benefits,\ninspired by meta-learning, we proposed a novel adaptive step size strategy to\nincorporate into the sub-problem solving steps of alternating updates. It\nsupports different types of trainable step sizes, including but not limited to\nscalar, element-wise, row-wise, and column-wise, enabling adaptive step size\nselection tailored to each block via meta-learning. We further provide a\ntheoretical convergence guarantee for the proposed algorithm, establishing its\noptimization soundness. Extensive experiments for multiple benchmarks\ndemonstrate that SAMT achieves better generalization performance with fewer\nparameter updates compared to state-of-the-art methods, highlighting its\neffectiveness and potential in neural network optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04193v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04216", "title": "Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction", "authors": ["Ruike Song", "Zeen Song", "Huijie Guo", "Wenwen Qiang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04216v1", "summary": "External reasoning systems combine language models with process reward models\n(PRMs) to select high-quality reasoning paths for complex tasks such as\nmathematical problem solving. However, these systems are prone to reward\nhacking, where high-scoring but logically incorrect paths are assigned high\nscores by the PRMs, leading to incorrect answers. From a causal inference\nperspective, we attribute this phenomenon primarily to the presence of\nconfounding semantic features. To address it, we propose Causal Reward\nAdjustment (CRA), a method that mitigates reward hacking by estimating the true\nreward of a reasoning path. CRA trains sparse autoencoders on the PRM's\ninternal activations to recover interpretable features, then corrects\nconfounding by using backdoor adjustment. Experiments on math solving datasets\ndemonstrate that CRA mitigates reward hacking and improves final accuracy,\nwithout modifying the policy model or retraining PRM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04216v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04251", "title": "T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion", "authors": ["Abdul Monaf Chowdhury", "Rabeya Akter", "Safaeid Hossain Arib"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04251v1", "summary": "Multivariate time series forecasting (MTSF) seeks to model temporal dynamics\namong variables to predict future trends. Transformer-based models and large\nlanguage models (LLMs) have shown promise due to their ability to capture\nlong-range dependencies and patterns. However, current methods often rely on\nrigid inductive biases, ignore intervariable interactions, or apply static\nfusion strategies that limit adaptability across forecast horizons. These\nlimitations create bottlenecks in capturing nuanced, horizon-specific\nrelationships in time-series data. To solve this problem, we propose T3Time, a\nnovel trimodal framework consisting of time, spectral, and prompt branches,\nwhere the dedicated frequency encoding branch captures the periodic structures\nalong with a gating mechanism that learns prioritization between temporal and\nspectral features based on the prediction horizon. We also proposed a mechanism\nwhich adaptively aggregates multiple cross-modal alignment heads by dynamically\nweighting the importance of each head based on the features. Extensive\nexperiments on benchmark datasets demonstrate that our model consistently\noutperforms state-of-the-art baselines, achieving an average reduction of 3.28%\nin MSE and 2.29% in MAE. Furthermore, it shows strong generalization in\nfew-shot learning settings: with 5% training data, we see a reduction in MSE\nand MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%\non average. Code - https://github.com/monaf-chowdhury/T3Time/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04251v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04279", "title": "Mockingbird: How does LLM perform in general machine learning tasks?", "authors": ["Haoyu Jia", "Yoshiki Obinata", "Kento Kawaharazuka", "Kei Okada"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04279v1", "summary": "Large language models (LLMs) are now being used with increasing frequency as\nchat bots, tasked with the summarizing information or generating text and code\nin accordance with user instructions. The rapid increase in reasoning\ncapabilities and inference speed of LLMs has revealed their remarkable\npotential for applications extending beyond the domain of chat bots to general\nmachine learning tasks. This work is conducted out of the curiosity about such\npotential. In this work, we propose a framework Mockingbird to adapt LLMs to\ngeneral machine learning tasks and evaluate its performance and scalability on\nseveral general machine learning tasks. The core concept of this framework is\ninstructing LLMs to role-play functions and reflect on its mistakes to improve\nitself. Our evaluation and analysis result shows that LLM-driven machine\nlearning methods, such as Mockingbird, can achieve acceptable results on common\nmachine learning tasks; however, solely reflecting on its own currently cannot\noutperform the effect of domain-specific documents and feedback from human\nexperts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04279v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04308", "title": "WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification", "authors": ["Thang Duc Tran", "Thai Hoang Le"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04308v1", "summary": "Machine unlearning, the efficient deletion of the impact of specific data in\na trained model, remains a challenging problem. Current machine unlearning\napproaches that focus primarily on data-centric or weight-based strategies\nfrequently encounter challenges in achieving precise unlearning, maintaining\nstability, and ensuring applicability across diverse domains. In this work, we\nintroduce a new two-phase efficient machine unlearning method for image\nclassification, in terms of weight saliency, leveraging weight saliency to\nfocus the unlearning process on critical model parameters. Our method is called\nweight saliency soft-guided contrastive learning for efficient machine\nunlearning image classification (WSS-CL), which significantly narrows the\nperformance gap with \"exact\" unlearning. First, the forgetting stage maximizes\nkullback-leibler divergence between output logits and aggregated pseudo-labels\nfor efficient forgetting in logit space. Next, the adversarial fine-tuning\nstage introduces contrastive learning in a self-supervised manner. By using\nscaled feature representations, it maximizes the distance between the forgotten\nand retained data samples in the feature space, with the forgotten and the\npaired augmented samples acting as positive pairs, while the retained samples\nact as negative pairs in the contrastive loss computation. Experimental\nevaluations reveal that our proposed method yields much-improved unlearning\nefficacy with negligible performance loss compared to state-of-the-art\napproaches, indicative of its usability in supervised and self-supervised\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04308v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04329", "title": "Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning", "authors": ["Ali Taheri Ghahrizjani", "Alireza Taban", "Qizhou Wang", "Shanshan Ye", "Abdolreza Mirzaei", "Tongliang Liu", "Bo Han"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04329v1", "summary": "Supervised fine-tuning (SFT) plays a critical role for pretrained large\nlanguage models (LLMs), notably enhancing their capacity to acquire\ndomain-specific knowledge while preserving or potentially augmenting their\ngeneral-purpose capabilities. However, the efficacy of SFT hinges on data\nquality as well as data volume, otherwise it may result in limited performance\ngains or even degradation relative to the associated baselines. To mitigate\nsuch reliance, we suggest categorizing tokens within each corpus into two parts\n-- positive and negative tokens -- based on whether they are useful to improve\nmodel performance. Positive tokens can be trained in common ways, whereas\nnegative tokens, which may lack essential semantics or be misleading, should be\nexplicitly forgotten. Overall, the token categorization facilitate the model to\nlearn less informative message, and the forgetting process shapes a knowledge\nboundary to guide the model on what information to learn more precisely. We\nconduct experiments on well-established benchmarks, finding that this\nforgetting mechanism not only improves overall model performance and also\nfacilitate more diverse model responses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04329v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04346", "title": "From Split to Share: Private Inference with Distributed Feature Sharing", "authors": ["Zihan Liu", "Jiayi Wen", "Shouhong Tan", "Zhirun Zheng", "Cheng Huang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04346v1", "summary": "Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy\nconcerns when handling sensitive client data. Existing Private Inference (PI)\nmethods face a fundamental trade-off between privacy and efficiency:\ncryptographic approaches offer strong protection but incur high computational\noverhead, while efficient alternatives such as split inference expose\nintermediate features to inversion attacks. We propose PrivDFS, a new paradigm\nfor private inference that replaces a single exposed representation with\ndistributed feature sharing. PrivDFS partitions input features on the client\ninto multiple balanced shares, which are distributed to non-colluding,\nnon-communicating servers for independent partial inference. The client\nsecurely aggregates the servers' outputs to reconstruct the final prediction,\nensuring that no single server observes sufficient information to compromise\ninput privacy. To further strengthen privacy, we propose two key extensions:\nPrivDFS-AT, which uses adversarial training with a diffusion-based proxy\nattacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,\nwhich leverages user-specific keys to diversify partitioning policies and\nprevent query-based inversion generalization. Experiments on CIFAR-10 and\nCelebA demonstrate that PrivDFS achieves privacy comparable to deep split\ninference while cutting client computation by up to 100 times with no accuracy\nloss, and that the extensions remain robust against both diffusion-based\nin-distribution and adaptive attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04346v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04351", "title": "Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points", "authors": ["Justin Lee", "Behnaz Moradijamei", "Heman Shakeri"], "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04351v1", "summary": "Modeling the evolution of high-dimensional systems from limited snapshot\nobservations at irregular time points poses a significant challenge in\nquantitative biology and related fields. Traditional approaches often rely on\ndimensionality reduction techniques, which can oversimplify the dynamics and\nfail to capture critical transient behaviors in non-equilibrium systems. We\npresent Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of\nsimulation-free score and flow matching methods to the multi-marginal setting,\nenabling the alignment of high-dimensional data measured at non-equidistant\ntime points without reducing dimensionality. The use of measure-valued splines\nenhances robustness to irregular snapshot timing, and score matching prevents\noverfitting in high-dimensional spaces. We validate our framework on several\nsynthetic and benchmark datasets, including gene expression data collected at\nuneven time points and an image progression task, demonstrating the method's\nversatility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04351v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04405", "title": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design", "authors": ["Hao Zhang", "Aining Jia", "Weifeng Bu", "Yushu Cai", "Kai Sheng", "Hao Chen", "Xin He"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04405v1", "summary": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration.\n  In this paper, we propose FlexQ, a novel post-training INT6 quantization\nframework combining algorithmic innovation with system-level optimizations.\nFlexQ employs uniform 6-bit weight quantization across all layers, with\nadaptive retention of 8-bit activations in layers identified through layer-wise\nsensitivity analysis. To maximize hardware efficiency, we develop a specialized\nhigh-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8\nrepresentations via Binary Tensor Core (BTC) equivalents, effectively bypassing\nthe lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ\nmaintains near-FP16 accuracy, with perplexity increases of no more than 0.05.\nThe proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on\nLLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference\nacceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released\nat https://github.com/FlyFoxPlayer/FlexQ.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04405v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04444", "title": "Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation", "authors": ["Askar Tsyganov", "Evgeny Frolov", "Sergey Samsonov", "Maxim Rakhuba"], "categories": ["cs.LG", "math.NA", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04444v1", "summary": "In this paper, we propose new randomized algorithms for estimating the\ntwo-to-infinity and one-to-two norms in a matrix-free setting, using only\nmatrix-vector multiplications. Our methods are based on appropriate\nmodifications of Hutchinson's diagonal estimator and its Hutch++ version. We\nprovide oracle complexity bounds for both modifications. We further illustrate\nthe practical utility of our algorithms for Jacobian-based regularization in\ndeep neural network training on image classification tasks. We also demonstrate\nthat our methodology can be applied to mitigate the effect of adversarial\nattacks in the domain of recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04444v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04462", "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference", "authors": ["Enyu Zhou", "Kai Sheng", "Hao Chen", "Xin He"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04462v1", "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04462v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04463", "title": "GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries", "authors": ["Fangzhi Fei", "Jiaxin Hu", "Qiaofeng Li", "Zhenyu Liu"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04463v1", "summary": "Transformer-based neural operators have emerged as promising surrogate\nsolvers for partial differential equations, by leveraging the effectiveness of\nTransformers for capturing long-range dependencies and global correlations,\nprofoundly proven in language modeling. However, existing methodologies\noverlook the coordinated learning of interdependencies between local physical\ndetails and global features, which are essential for tackling multiscale\nproblems, preserving physical consistency and numerical stability in long-term\nrollouts, and accurately capturing transitional dynamics. In this work, we\npropose GFocal, a Transformer-based neural operator method that enforces\nsimultaneous global and local feature learning and fusion. Global correlations\nand local features are harnessed through Nystr\\\"{o}m attention-based\n\\textbf{g}lobal blocks and slices-based \\textbf{focal} blocks to generate\nphysics-aware tokens, subsequently modulated and integrated via\nconvolution-based gating blocks, enabling dynamic fusion of multiscale\ninformation. GFocal achieves accurate modeling and prediction of physical\nfeatures given arbitrary geometries and initial conditions. Experiments show\nthat GFocal achieves state-of-the-art performance with an average 15.2\\%\nrelative gain in five out of six benchmarks and also excels in industry-scale\nsimulations such as aerodynamics simulation of automotives and airfoils.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04463v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04470", "title": "FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions", "authors": ["Jianheng Tang", "Zhirui Yang", "Jingchao Wang", "Kejia Fan", "Jinfeng Xu", "Huiping Zhuang", "Anfeng Liu", "Houbing Herbert Song", "Leye Wang", "Yunhuai Liu"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04470v1", "summary": "Lately, Personalized Federated Learning (PFL) has emerged as a prevalent\nparadigm to deliver personalized models by collaboratively training while\nsimultaneously adapting to each client's local applications. Existing PFL\nmethods typically face a significant challenge due to the ubiquitous data\nheterogeneity (i.e., non-IID data) across clients, which severely hinders\nconvergence and degrades performance. We identify that the root issue lies in\nthe long-standing reliance on gradient-based updates, which are inherently\nsensitive to non-IID data. To fundamentally address this issue and bridge the\nresearch gap, in this paper, we propose a Heterogeneity-invariant Personalized\nFederated learning scheme, named FedHiP, through analytical (i.e., closed-form)\nsolutions to avoid gradient-based updates. Specifically, we exploit the trend\nof self-supervised pre-training, leveraging a foundation model as a frozen\nbackbone for gradient-free feature extraction. Following the feature extractor,\nwe further develop an analytic classifier for gradient-free training. To\nsupport both collective generalization and individual personalization, our\nFedHiP scheme incorporates three phases: analytic local training, analytic\nglobal aggregation, and analytic local personalization. The closed-form\nsolutions of our FedHiP scheme enable its ideal property of heterogeneity\ninvariance, meaning that each personalized model remains identical regardless\nof how non-IID the data are distributed across all other clients. Extensive\nexperiments on benchmark datasets validate the superiority of our FedHiP\nscheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%\nin accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04470v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04478", "title": "Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions", "authors": ["Bernardino D'Amico", "Francesco Pomponi", "Jay H. Arehart", "Lina Khaddour"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04478v1", "summary": "Reducing domestic energy demand is central to climate mitigation and fuel\npoverty strategies, yet the impact of energy efficiency interventions is highly\nheterogeneous. Using a causal machine learning model trained on nationally\nrepresentative data of the English housing stock, we estimate average and\nconditional treatment effects of wall insulation on gas consumption, focusing\non distributional effects across energy burden subgroups. While interventions\nreduce gas demand on average (by as much as 19 percent), low energy burden\ngroups achieve substantial savings, whereas those experiencing high energy\nburdens see little to no reduction. This pattern reflects a\nbehaviourally-driven mechanism: households constrained by high costs-to-income\nratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort\nrather than lowering consumption. Far from wasteful, such responses represent\nrational adjustments in contexts of prior deprivation, with potential\nco-benefits for health and well-being. These findings call for a broader\nevaluation framework that accounts for both climate impacts and the equity\nimplications of domestic energy policy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04478v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04481", "title": "Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach", "authors": ["Anushka Srivastava"], "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04481v1", "summary": "This paper presents a deep learning-based approach to emotion detection using\nConditional Generative Adversarial Networks (cGANs). Unlike traditional\nunimodal techniques that rely on a single data type, we explore a multimodal\nframework integrating text, audio, and facial expressions. The proposed cGAN\narchitecture is trained to generate synthetic emotion-rich data and improve\nclassification accuracy across multiple modalities. Our experimental results\ndemonstrate significant improvements in emotion recognition performance\ncompared to baseline models. This work highlights the potential of cGANs in\nenhancing human-computer interaction systems by enabling more nuanced emotional\nunderstanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04481v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04517", "title": "Channel-Independent Federated Traffic Prediction", "authors": ["Mo Zhang", "Xiaoyu Li", "Bin Xu", "Meng Chen", "Yongshun Gong"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04517v1", "summary": "In recent years, traffic prediction has achieved remarkable success and has\nbecome an integral component of intelligent transportation systems. However,\ntraffic data is typically distributed among multiple data owners, and privacy\nconstraints prevent the direct utilization of these isolated datasets for\ntraffic prediction. Most existing federated traffic prediction methods focus on\ndesigning communication mechanisms that allow models to leverage information\nfrom other clients in order to improve prediction accuracy. Unfortunately, such\napproaches often incur substantial communication overhead, and the resulting\ntransmission delays significantly slow down the training process. As the volume\nof traffic data continues to grow, this issue becomes increasingly critical,\nmaking the resource consumption of current methods unsustainable. To address\nthis challenge, we propose a novel variable relationship modeling paradigm for\nfederated traffic prediction, termed the Channel-Independent Paradigm(CIP).\nUnlike traditional approaches, CIP eliminates the need for inter-client\ncommunication by enabling each node to perform efficient and accurate\npredictions using only local information. Based on the CIP, we further develop\nFed-CI, an efficient federated learning framework, allowing each client to\nprocess its own data independently while effectively mitigating the information\nloss caused by the lack of direct data sharing among clients. Fed-CI\nsignificantly reduces communication overhead, accelerates the training process,\nand achieves state-of-the-art performance while complying with privacy\nregulations. Extensive experiments on multiple real-world datasets demonstrate\nthat Fed-CI consistently outperforms existing methods across all datasets and\nfederated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,\nand MAPE, respectively, while also substantially reducing communication costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04517v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04595", "title": "Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding", "authors": ["Jan A. Zak", "Christian WeiÃenfels"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04595v1", "summary": "Resistance spot welding is the dominant joining process for the body-in-white\nin the automotive industry, where the weld nugget diameter is the key quality\nmetric. Its measurement requires destructive testing, limiting the potential\nfor efficient quality control. Physics-informed neural networks were\ninvestigated as a promising tool to reconstruct internal process states from\nexperimental data, enabling model-based and non-invasive quality assessment in\naluminum spot welding. A major challenge is the integration of real-world data\ninto the network due to competing optimization objectives. To address this, we\nintroduce two novel training strategies. First, experimental losses for dynamic\ndisplacement and nugget diameter are progressively included using a fading-in\nfunction to prevent excessive optimization conflicts. We also implement a\ncustom learning rate scheduler and early stopping based on a rolling window to\ncounteract premature reduction due to increased loss magnitudes. Second, we\nintroduce a conditional update of temperature-dependent material parameters via\na look-up table, activated only after a loss threshold is reached to ensure\nphysically meaningful temperatures. An axially symmetric two-dimensional model\nwas selected to represent the welding process accurately while maintaining\ncomputational efficiency. To reduce computational burden, the training\nstrategies and model components were first systematically evaluated in one\ndimension, enabling controlled analysis of loss design and contact models. The\ntwo-dimensional network predicts dynamic displacement and nugget growth within\nthe experimental confidence interval, supports transferring welding stages from\nsteel to aluminum, and demonstrates strong potential for fast, model-based\nquality control in industrial applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04595v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04605", "title": "Multitask Learning with Stochastic Interpolants", "authors": ["Hugo Negrel", "Florentin Coeurdoux", "Michael S. Albergo", "Eric Vanden-Eijnden"], "categories": ["cs.LG", "math.DS"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04605v1", "summary": "We propose a framework for learning maps between probability distributions\nthat broadly generalizes the time dynamics of flow and diffusion models. To\nenable this, we generalize stochastic interpolants by replacing the scalar time\nvariable with vectors, matrices, or linear operators, allowing us to bridge\nprobability distributions across multiple dimensional spaces. This approach\nenables the construction of versatile generative models capable of fulfilling\nmultiple tasks without task-specific training. Our operator-based interpolants\nnot only provide a unifying theoretical perspective for existing generative\nmodels but also extend their capabilities. Through numerical experiments, we\ndemonstrate the zero-shot efficacy of our method on conditional generation and\ninpainting, fine-tuning and posterior sampling, and multiscale modeling,\nsuggesting its potential as a generic task-agnostic alternative to specialized\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04605v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04630", "title": "CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series", "authors": ["Yutong Xia", "Yingying Zhang", "Yuxuan Liang", "Lunting Fan", "Qingsong Wen", "Roger Zimmermann"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04630v1", "summary": "Time series anomaly detection has garnered considerable attention across\ndiverse domains. While existing methods often fail to capture the underlying\nmechanisms behind anomaly generation in time series data. In addition, time\nseries anomaly detection often faces several data-related inherent challenges,\ni.e., label scarcity, data imbalance, and complex multi-periodicity. In this\npaper, we leverage causal tools and introduce a new causality-based framework,\nCaPulse, which tunes in to the underlying causal pulse of time series data to\neffectively detect anomalies. Concretely, we begin by building a structural\ncausal model to decipher the generation processes behind anomalies. To tackle\nthe challenges posed by the data, we propose Periodical Normalizing Flows with\na novel mask mechanism and carefully designed periodical learners, creating a\nperiodicity-aware, density-based anomaly detection approach. Extensive\nexperiments on seven real-world datasets demonstrate that CaPulse consistently\noutperforms existing methods, achieving AUROC improvements of 3% to 17%, with\nenhanced interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04630v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04665", "title": "Perch 2.0: The Bittern Lesson for Bioacoustics", "authors": ["Bart van MerriÃ«nboer", "Vincent Dumoulin", "Jenny Hamer", "Lauren Harrell", "Andrea Burns", "Tom Denton"], "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04665v1", "summary": "Perch is a performant pre-trained model for bioacoustics. It was trained in\nsupervised fashion, providing both off-the-shelf classification scores for\nthousands of vocalizing species as well as strong embeddings for transfer\nlearning. In this new release, Perch 2.0, we expand from training exclusively\non avian species to a large multi-taxa dataset. The model is trained with\nself-distillation using a prototype-learning classifier as well as a new\nsource-prediction training criterion. Perch 2.0 obtains state-of-the-art\nperformance on the BirdSet and BEANS benchmarks. It also outperforms\nspecialized marine models on marine transfer learning tasks, despite having\nalmost no marine training data. We present hypotheses as to why fine-grained\nspecies classification is a particularly robust pre-training task for\nbioacoustics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04665v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04670", "title": "Robustly Learning Monotone Single-Index Models", "authors": ["Puqian Wang", "Nikos Zarifis", "Ilias Diakonikolas", "Jelena Diakonikolas"], "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04670v1", "summary": "We consider the basic problem of learning Single-Index Models with respect to\nthe square loss under the Gaussian distribution in the presence of adversarial\nlabel noise. Our main contribution is the first computationally efficient\nalgorithm for this learning task, achieving a constant factor approximation,\nthat succeeds for the class of {\\em all} monotone activations with bounded\nmoment of order $2 + \\zeta,$ for $\\zeta > 0.$ This class in particular includes\nall monotone Lipschitz functions and even discontinuous functions like\n(possibly biased) halfspaces. Prior work for the case of unknown activation\neither does not attain constant factor approximation or succeeds for a\nsubstantially smaller family of activations. The main conceptual novelty of our\napproach lies in developing an optimization framework that steps outside the\nboundaries of usual gradient methods and instead identifies a useful vector\nfield to guide the algorithm updates by directly leveraging the problem\nstructure, properties of Gaussian spaces, and regularity of monotone functions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04670v1", "cate": "cs.LG", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03702", "title": "Suggest, Complement, Inspire: Story of Two Tower Recommendations at Allegro.com", "authors": ["Aleksandra Osowska-Kurczab", "Klaudia Nazarko", "Mateusz Marzec", "Lidia Wojciechowska", "EliÅ¡ka KremeÅovÃ¡"], "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03702", "summary": "Building large-scale e-commerce recommendation systems requires addressing\nthree key technical challenges: (1) designing a universal recommendation\narchitecture across dozens of placements, (2) decreasing excessive maintenance\ncosts, and (3) managing a highly dynamic product catalogue. This paper presents\na unified content-based recommendation system deployed at Allegro.com, the\nlargest e-commerce platform of European origin. The system is built on a\nprevalent Two Tower retrieval framework, representing products using textual\nand structured attributes, which enables efficient retrieval via Approximate\nNearest Neighbour search. We demonstrate how the same model architecture can be\nadapted to serve three distinct recommendation tasks: similarity search,\ncomplementary product suggestions, and inspirational content discovery, by\nmodifying only a handful of components in either the model or the serving\nlogic. Extensive A/B testing over two years confirms significant gains in\nengagement and profit-based metrics across desktop and mobile app channels. Our\nresults show that a flexible, scalable architecture can serve diverse user\nintents with minimal maintenance overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03702", "cate": "cs.IR", "date": "2025-07-19", "updated": "2025-07-19", "section": "cross"}
{"id": "2508.03710", "title": "Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study", "authors": ["Rafael Salinas-Buestan", "Otto Parra", "Nelly Condori-Fernandez", "Maria Fernanda Granda"], "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03710", "summary": "Background: Generative AI tools have become increasingly relevant in\nsupporting personalized recommendations across various domains. However, their\neffectiveness in health-related behavioral interventions, especially those\naiming to reduce the use of technology, remains underexplored. Aims: This study\nevaluates the performance and user satisfaction of the five most widely used\ngenerative AI tools when recommending non-digital activities tailored to\nindividuals at risk of repetitive strain injury. Method: Following the\nGoal/Question/Metric (GQM) paradigm, this proposed experiment involves\ngenerative AI tools that suggest offline activities based on predefined user\nprofiles and intervention scenarios. The evaluation is focused on quantitative\nperformance (precision, recall, F1-score and MCC-score) and qualitative aspects\n(user satisfaction and perceived recommendation relevance). Two research\nquestions were defined: RQ1 assessed which tool delivers the most accurate\nrecommendations, and RQ2 evaluated how tool choice influences user\nsatisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03710", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22", "section": "cross"}
{"id": "2508.03756", "title": "Predicting fall risk in older adults: A machine learning comparison of accelerometric and non-accelerometric factors", "authors": ["Ana GonzÃ¡lez-Castro", "JosÃ© Alberto BenÃ­tez-Andrades", "RubÃ©n GonzÃ¡lez-GonzÃ¡lez", "Camino Prada-GarcÃ­a", "Raquel LeirÃ³s-RodrÃ­guez"], "categories": ["cs.LG", "stat.AP"], "primary_category": "stat.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03756", "summary": "This study investigates fall risk prediction in older adults using various\nmachine learning models trained on accelerometric, non-accelerometric, and\ncombined data from 146 participants. Models combining both data types achieved\nsuperior performance, with Bayesian Ridge Regression showing the highest\naccuracy (MSE = 0.6746, R2 = 0.9941). Non-accelerometric variables, such as age\nand comorbidities, proved critical for prediction. Results support the use of\nintegrated data and Bayesian approaches to enhance fall risk assessment and\ninform prevention strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03756", "cate": "stat.AP", "date": "2025-08-04", "updated": "2025-08-04", "section": "cross"}
{"id": "2508.03788", "title": "A semi-automatic approach to study population dynamics based on population pyramids", "authors": ["Max Hahn-Klimroth", "JoÃ£o Pedro Meireles", "Laurie Bingaman Lackey", "Nick van Eeuwijk Mads F. Bertelsen", "Paul W. Dierkes", "Marcus Clauss"], "categories": ["cs.LG", "q-bio.PE", "stat.AP"], "primary_category": "q-bio.PE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03788", "summary": "The depiction of populations - of humans or animals - as \"population\npyramids\" is a useful tool for the assessment of various characteristics of\npopulations at a glance. Although these visualisations are well-known objects\nin various communities, formalised and algorithmic approaches to gain\ninformation from these data are less present. Here, we present an\nalgorithm-based classification of population data into \"pyramids\" of different\nshapes ([normal and inverted] pyramid / plunger / bell, [lower / middle /\nupper] diamond, column, hourglass) that are linked to specific characteristics\nof the population. To develop the algorithmic approach, we used data describing\nglobal zoo populations of mammals from 1970-2024. This algorithm-based approach\ndelivers plausible classifications, in particular with respect to changes in\npopulation size linked to specific series of, and transitions between,\ndifferent \"pyramid\" shapes. We believe this approach might become a useful tool\nfor analysing and communicating historical population developments in multiple\ncontexts and is of broad interest. Moreover, it might be useful for animal\npopulation management strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03788", "cate": "q-bio.PE", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03810", "title": "Viability of perturbative expansion for quantum field theories on neurons", "authors": ["Srimoyee Sen", "Varun Vaidya"], "categories": ["cs.LG", "hep-th"], "primary_category": "hep-th", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03810", "summary": "Neural Network (NN) architectures that break statistical independence of\nparameters have been proposed as a new approach for simulating local quantum\nfield theories (QFTs). In the infinite neuron number limit, single-layer NNs\ncan exactly reproduce QFT results. This paper examines the viability of this\narchitecture for perturbative calculations of local QFTs for finite neuron\nnumber $N$ using scalar $\\phi^4$ theory in $d$ Euclidean dimensions as an\nexample. We find that the renormalized $O(1/N)$ corrections to two- and\nfour-point correlators yield perturbative series which are sensitive to the\nultraviolet cut-off and therefore have a weak convergence. We propose a\nmodification to the architecture to improve this convergence and discuss\nconstraints on the parameters of the theory and the scaling of N which allow us\nto extract accurate field theory results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03810", "cate": "hep-th", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03867", "title": "Constraining the outputs of ReLU neural networks", "authors": ["Yulia Alexandr", "Guido MontÃºfar"], "categories": ["cs.LG", "math.AG", "stat.ML"], "primary_category": "math.AG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03867", "summary": "We introduce a class of algebraic varieties naturally associated with ReLU\nneural networks, arising from the piecewise linear structure of their outputs\nacross activation regions in input space, and the piecewise multilinear\nstructure in parameter space. By analyzing the rank constraints on the network\noutputs within each activation region, we derive polynomial equations that\ncharacterize the functions representable by the network. We further investigate\nconditions under which these varieties attain their expected dimension,\nproviding insight into the expressive and structural properties of ReLU\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03867", "cate": "math.AG", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03896", "title": "Reliable Programmatic Weak Supervision with Confidence Intervals for Label Probabilities", "authors": ["VerÃ³nica Ãlvarez", "Santiago Mazuelas", "Steven An", "Sanjoy Dasgupta"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03896", "summary": "The accurate labeling of datasets is often both costly and time-consuming.\nGiven an unlabeled dataset, programmatic weak supervision obtains probabilistic\npredictions for the labels by leveraging multiple weak labeling functions (LFs)\nthat provide rough guesses for labels. Weak LFs commonly provide guesses with\nassorted types and unknown interdependences that can result in unreliable\npredictions. Furthermore, existing techniques for programmatic weak supervision\ncannot provide assessments for the reliability of the probabilistic predictions\nfor labels. This paper presents a methodology for programmatic weak supervision\nthat can provide confidence intervals for label probabilities and obtain more\nreliable predictions. In particular, the methods proposed use uncertainty sets\nof distributions that encapsulate the information provided by LFs with\nunrestricted behavior and typology. Experiments on multiple benchmark datasets\nshow the improvement of the presented methods over the state-of-the-art and the\npracticality of the confidence intervals presented.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03896", "cate": "stat.ML", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03904", "title": "Reinforcement Learning in MDPs with Information-Ordered Policies", "authors": ["Zhongjun Zhang", "Shipra Agrawal", "Ilan Lobel", "Sean R. Sinclair", "Christina Lee Yu"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03904", "summary": "We propose an epoch-based reinforcement learning algorithm for\ninfinite-horizon average-cost Markov decision processes (MDPs) that leverages a\npartial order over a policy class. In this structure, $\\pi' \\leq \\pi$ if data\ncollected under $\\pi$ can be used to estimate the performance of $\\pi'$,\nenabling counterfactual inference without additional environment interaction.\nLeveraging this partial order, we show that our algorithm achieves a regret\nbound of $O(\\sqrt{w \\log(|\\Theta|) T})$, where $w$ is the width of the partial\norder. Notably, the bound is independent of the state and action space sizes.\nWe illustrate the applicability of these partial orders in many domains in\noperations research, including inventory control and queuing systems. For each,\nwe apply our framework to that problem, yielding new theoretical guarantees and\nstrong empirical results without imposing extra assumptions such as convexity\nin the inventory model or specialized arrival-rate structure in the queuing\nmodel.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03904", "cate": "stat.ML", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03910", "title": "Comparing Normalization Methods for Portfolio Optimization with Reinforcement Learning", "authors": ["Caio de Souza Barbosa Costa", "Anna Helena Reali Costa"], "categories": ["cs.LG", "q-fin.CP"], "primary_category": "q-fin.CP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03910", "summary": "Recently, reinforcement learning has achieved remarkable results in various\ndomains, including robotics, games, natural language processing, and finance.\nIn the financial domain, this approach has been applied to tasks such as\nportfolio optimization, where an agent continuously adjusts the allocation of\nassets within a financial portfolio to maximize profit. Numerous studies have\nintroduced new simulation environments, neural network architectures, and\ntraining algorithms for this purpose. Among these, a domain-specific policy\ngradient algorithm has gained significant attention in the research community\nfor being lightweight, fast, and for outperforming other approaches. However,\nrecent studies have shown that this algorithm can yield inconsistent results\nand underperform, especially when the portfolio does not consist of\ncryptocurrencies. One possible explanation for this issue is that the commonly\nused state normalization method may cause the agent to lose critical\ninformation about the true value of the assets being traded. This paper\nexplores this hypothesis by evaluating two of the most widely used\nnormalization methods across three different markets (IBOVESPA, NYSE, and\ncryptocurrencies) and comparing them with the standard practice of normalizing\ndata before training. The results indicate that, in this specific domain, the\nstate normalization can indeed degrade the agent's performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03910", "cate": "q-fin.CP", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03941", "title": "Measuring the stability and plasticity of recommender systems", "authors": ["Maria JoÃ£o Lavoura", "Robert Jungnickel", "JoÃ£o Vinagre"], "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03941", "summary": "The typical offline protocol to evaluate recommendation algorithms is to\ncollect a dataset of user-item interactions and then use a part of this dataset\nto train a model, and the remaining data to measure how closely the model\nrecommendations match the observed user interactions. This protocol is\nstraightforward, useful and practical, but it only captures performance of a\nparticular model trained at some point in the past. We know, however, that\nonline systems evolve over time. In general, it is a good idea that models\nreflect such changes, so models are frequently retrained with recent data. But\nif this is the case, to what extent can we trust previous evaluations? How will\na model perform when a different pattern (re)emerges? In this paper we propose\na methodology to study how recommendation models behave when they are\nretrained. The idea is to profile algorithms according to their ability to, on\nthe one hand, retain past patterns -- stability -- and, on the other hand,\n(quickly) adapt to changes -- plasticity. We devise an offline evaluation\nprotocol that provides detail on the long-term behavior of models, and that is\nagnostic to datasets, algorithms and metrics. To illustrate the potential of\nthis framework, we present preliminary results of three different types of\nalgorithms on the GoodReads dataset that suggest different stability and\nplasticity profiles depending on the algorithmic technique, and a possible\ntrade-off between stability and plasticity.Although additional experiments will\nbe necessary to confirm these observations, they already illustrate the\nusefulness of the proposed framework to gain insights on the long term dynamics\nof recommendation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03941", "cate": "cs.IR", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04098", "title": "Hybrid Quantum--Classical Machine Learning Potential with Variational Quantum Circuits", "authors": ["Soohaeng Yoo Willow", "D. ChangMo Yang", "Chang Woo Myung"], "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04098", "summary": "Quantum algorithms for simulating large and complex molecular systems are\nstill in their infancy, and surpassing state-of-the-art classical techniques\nremains an ever-receding goal post. A promising avenue of inquiry in the\nmeanwhile is to seek practical advantages through hybrid quantum-classical\nalgorithms, which combine conventional neural networks with variational quantum\ncircuits (VQCs) running on today's noisy intermediate-scale quantum (NISQ)\nhardware. Such hybrids are well suited to NISQ hardware. The classical\nprocessor performs the bulk of the computation, while the quantum processor\nexecutes targeted sub-tasks that supply additional non-linearity and\nexpressivity. Here, we benchmark a purely classical E(3)-equivariant\nmessage-passing machine learning potential (MLP) against a hybrid\nquantum-classical MLP for predicting density functional theory (DFT) properties\nof liquid silicon. In our hybrid architecture, every readout in the\nmessage-passing layers is replaced by a VQC. Molecular dynamics simulations\ndriven by the HQC-MLP reveal that an accurate reproduction of high-temperature\nstructural and thermodynamic properties is achieved with VQCs. These findings\ndemonstrate a concrete scenario in which NISQ-compatible HQC algorithm could\ndeliver a measurable benefit over the best available classical alternative,\nsuggesting a viable pathway toward near-term quantum advantage in materials\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04098", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04145", "title": "Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation", "authors": ["Teng Shi", "Weijie Yu", "Xiao Zhang", "Ming He", "Jianping Fan", "Jun Xu"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04145v1", "summary": "In modern online platforms, search and recommendation (S&R) often coexist,\noffering opportunities for performance improvement through search-enhanced\napproaches. Existing studies show that incorporating search signals boosts\nrecommendation performance. However, the effectiveness of these methods relies\nheavily on rich search interactions. They primarily benefit a small subset of\nusers with abundant search behavior, while offering limited improvements for\nthe majority of users who exhibit only sparse search activity. To address the\nproblem of sparse search data in search-enhanced recommendation, we face two\nkey challenges: (1) how to learn useful search features for users with sparse\nsearch interactions, and (2) how to design effective training objectives under\nsparse conditions. Our idea is to leverage the features of users with rich\nsearch interactions to enhance those of users with sparse search interactions.\nBased on this idea, we propose GSERec, a method that utilizes message passing\non the User-Code Graphs to alleviate data sparsity in Search-Enhanced\nRecommendation. Specifically, we utilize Large Language Models (LLMs) with\nvector quantization to generate discrete codes, which connect similar users and\nthereby construct the graph. Through message passing on this graph, embeddings\nof users with rich search data are propagated to enhance the embeddings of\nusers with sparse interactions. To further ensure that the message passing\ncaptures meaningful information from truly similar users, we introduce a\ncontrastive loss to better model user similarities. The enhanced user\nrepresentations are then integrated into downstream search-enhanced\nrecommendation models. Experiments on three real-world datasets show that\nGSERec consistently outperforms baselines, especially for users with sparse\nsearch behaviors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04145v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04152", "title": "Bridging Search and Recommendation through Latent Cross Reasoning", "authors": ["Teng Shi", "Weicong Qin", "Weijie Yu", "Xiao Zhang", "Ming He", "Jianping Fan", "Jun Xu"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04152v1", "summary": "Search and recommendation (S&R) are fundamental components of modern online\nplatforms, yet effectively leveraging search behaviors to improve\nrecommendation remains a challenging problem. User search histories often\ncontain noisy or irrelevant signals that can even degrade recommendation\nperformance, while existing approaches typically encode S&R histories either\njointly or separately without explicitly identifying which search behaviors are\ntruly useful. Inspired by the human decision-making process, where one first\nidentifies recommendation intent and then reasons about relevant evidence, we\ndesign a latent cross reasoning framework that first encodes user S&R histories\nto capture global interests and then iteratively reasons over search behaviors\nto extract signals beneficial for recommendation. Contrastive learning is\nemployed to align latent reasoning states with target items, and reinforcement\nlearning is further introduced to directly optimize ranking performance.\nExtensive experiments on public benchmarks demonstrate consistent improvements\nover strong baselines, validating the importance of reasoning in enhancing\nsearch-aware recommendation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04152v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04162", "title": "SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval", "authors": ["Ruyin Li", "Xiaoyu Chen"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04162v1", "summary": "Formula retrieval is an important topic in Mathematical Information\nRetrieval. We propose SSEmb, a novel embedding framework capable of capturing\nboth structural and semantic features of mathematical formulas. Structurally,\nwe employ Graph Contrastive Learning to encode formulas represented as Operator\nGraphs. To enhance structural diversity while preserving mathematical validity\nof these formula graphs, we introduce a novel graph data augmentation approach\nthrough a substitution strategy. Semantically, we utilize Sentence-BERT to\nencode the surrounding text of formulas. Finally, for each query and its\ncandidates, structural and semantic similarities are calculated separately and\nthen fused through a weighted scheme. In the ARQMath-3 formula retrieval task,\nSSEmb outperforms existing embedding-based methods by over 5 percentage points\non P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs\nof other methods and achieves state-of-the-art results when combined with\nApproach0.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04162v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04206", "title": "ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation", "authors": ["Fatemeh Nazary", "Ali Tourani", "Yashar Deldjoo", "Tommaso Di Noia"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04206v1", "summary": "Recommending long-form video content demands joint modeling of visual, audio,\nand textual modalities, yet most benchmarks address only raw features or narrow\nfusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for\nLLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,\nit aligns dense item embeddings from three modalities: audio (block-level,\ni-vector), visual (CNN, AVF), and text. Missing or sparse metadata is\nautomatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),\ngenerating high-quality synopses for thousands of movies. All text (raw or\naugmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),\nproducing multiple ready-to-use sets. The pipeline supports interchangeable\nearly-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and\nmultiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are\nfully declarative via a single YAML file. Evaluation spans accuracy (Recall,\nnDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,\ndiversity, fairness. Results show LLM-based augmentation and strong text\nembeddings boost cold-start and coverage, especially when fused with\naudio-visual features. Systematic benchmarking reveals universal versus\nbackbone- or metric-specific combinations. Open-source code, embeddings, and\nconfigs enable reproducible, fair multimodal RS research and advance principled\ngenerative AI integration in large-scale recommendation. Code:\nhttps://recsys-lab.github.io/ViLLA-MMBench", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04206v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04221", "title": "Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains", "authors": ["Joey De Pauw", "Bart Goethals"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04221v1", "summary": "Recommender systems learn from past user behavior to predict future user\npreferences. Intuitively, it has been established that the most recent\ninteractions are more indicative of future preferences than older interactions.\nMany recommendation algorithms use this notion to either drop older\ninteractions or to assign them a lower weight, so the model can focus on the\nmore informative, recent information. However, very few approaches model the\nflow of time explicitly.\n  This paper analyzes how time can be encoded in factorization-style\nrecommendation models. By including absolute time as a feature, our models can\nlearn varying user preferences and changing item perception over time. In\naddition to simple binning approaches, we also propose a novel, fully\ncontinuous time encoding mechanism. Through the use of a polynomial fit inside\nthe loss function, our models completely avoid the need for discretization, and\nthey are able to capture the time dimension in arbitrary resolution.\n  We perform a comparative study on three real-world datasets that span\nmultiple years, where long user histories are present, and items stay relevant\nfor a longer time. Empirical results show that, by explicitly modeling time,\nour models are very effective at capturing temporal signals, such as varying\nitem popularities over time. Despite this however, our experiments also\nindicate that a simple post-hoc popularity adjustment is often sufficient to\nachieve the best performance on the unseen test set. This teaches us that, for\nthe recommendation task, predicting the future is more important than capturing\npast trends. As such, we argue that specialized mechanisms are needed for\nextrapolation to future data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04221v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04247", "title": "I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation", "authors": ["Huilin Chen", "Miaomiao Cai", "Fan Liu", "Zhiyong Cheng", "Richang Hong", "Meng Wang"], "categories": ["cs.IR", "cs.MM"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04247v1", "summary": "Multimodal recommender systems (MRS) improve recommendation performance by\nintegrating diverse semantic information from multiple modalities. However, the\nassumption of the availability of all modalities rarely holds in practice due\nto missing images, incomplete descriptions, or inconsistent user content. These\nchallenges significantly degrade the robustness and generalization capabilities\nof current models. To address these challenges, we introduce a novel method\ncalled \\textbf{I$^3$-MRec}, which uses \\textbf{I}nvariant learning with\n\\textbf{I}nformation bottleneck principle for \\textbf{I}ncomplete\n\\textbf{M}odality \\textbf{Rec}ommendation. To achieve robust performance in\nmissing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)\ncross-modal preference invariance, which ensures consistent user preference\nmodeling across varying modality environments, and (ii) compact yet effective\nmodality representation, which filters out task-irrelevant modality information\nwhile maximally preserving essential features relevant to recommendation. By\ntreating each modality as a distinct semantic environment, I$^3$-MRec employs\ninvariant risk minimization (IRM) to learn modality-specific item\nrepresentations. In parallel, a missing-aware fusion module grounded in the\nInformation Bottleneck (IB) principle extracts compact and effective item\nembeddings by suppressing modality noise and preserving core user preference\nsignals. Extensive experiments conducted on three real-world datasets\ndemonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art\nMRS methods across various modality-missing scenarios, highlighting its\neffectiveness and robustness in practical applications. The code and processed\ndatasets are released at https://github.com/HuilinChenJN/I3-MRec.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04247v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04419", "title": "Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics", "authors": ["Jarne Mathi Decker", "Joeran Beel"], "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04419v1", "summary": "The Algorithm Selection Problem for recommender systems-choosing the best\nalgorithm for a given user or context-remains a significant challenge.\nTraditional meta-learning approaches often treat algorithms as categorical\nchoices, ignoring their intrinsic properties. Recent work has shown that\nexplicitly characterizing algorithms with features can improve model\nperformance in other domains. Building on this, we propose a per-user\nmeta-learning approach for recommender system selection that leverages both\nuser meta-features and automatically extracted algorithm features from source\ncode. Our preliminary results, averaged over six diverse datasets, show that\naugmenting a meta-learner with algorithm features improves its average NDCG@10\nperformance by 8.83% from 0.135 (user features only) to 0.147. This enhanced\nmodel outperforms the Single Best Algorithm baseline (0.131) and successfully\ncloses 10.5% of the performance gap to a theoretical oracle selector. These\nfindings show that even static source code metrics provide a valuable\npredictive signal, presenting a promising direction for building more robust\nand intelligent recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04419v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04474", "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models", "authors": ["Xinkui Zhao", "Haode Li", "Yifan Zhang", "Guanjie Cheng", "Yueshen Xu"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04474v1", "summary": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04474v1", "cate": "cs.IR", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2401.11441", "title": "On-Device Recommender Systems: A Comprehensive Survey", "authors": ["Hongzhi Yin", "Liang Qu", "Tong Chen", "Wei Yuan", "Ruiqi Zheng", "Jing Long", "Xin Xia", "Yuhui Shi", "Chengqi Zhang"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.11441", "summary": "Recommender systems have been widely deployed in various real-world\napplications to help users identify content of interest from massive amounts of\ninformation. Traditional recommender systems work by collecting user-item\ninteraction data in a cloud-based data center and training a centralized model\nto perform the recommendation service. However, such cloud-based recommender\nsystems (CloudRSs) inevitably suffer from excessive resource consumption,\nresponse latency, as well as privacy and security risks concerning both data\nand models. Recently, driven by the advances in storage, communication, and\ncomputation capabilities of edge devices, there has been a shift of focus from\nCloudRSs to on-device recommender systems (DeviceRSs), which leverage the\ncapabilities of edge devices to minimize centralized data storage requirements,\nreduce the response latency caused by communication overheads, and enhance user\nprivacy and security by localizing data processing and model training. Despite\nthe rapid rise of DeviceRSs, there is a clear absence of timely literature\nreviews that systematically introduce, categorize and contrast these methods.\nTo bridge this gap, we aim to provide a comprehensive survey of DeviceRSs,\ncovering three main aspects: (1) the deployment and inference of DeviceRSs (2)\nthe training and update of DeviceRSs (3) the security and privacy of DeviceRSs.\nFurthermore, we provide a fine-grained and systematic taxonomy of the methods\ninvolved in each aspect, followed by a discussion regarding challenges and\nfuture research directions. This is the first comprehensive survey on DeviceRSs\nthat covers a spectrum of tasks to fit various needs. We believe this survey\nwill help readers effectively grasp the current research status in this field,\nequip them with relevant technical foundations, and stimulate new research\nideas for developing DeviceRSs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.11441", "cate": "cs.IR", "date": "2024-01-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03765", "title": "Forgive and Forget? An Industry 5.0 Approach to Trust-Fatigue Co-regulation in Human-Cobot Order Picking", "authors": ["Soumyadeep Dhar"], "categories": ["cs.MA"], "primary_category": "cs.MA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03765v1", "summary": "This paper investigates the critical role of trust and fatigue in human-cobot\ncollaborative order picking, framing the challenge within the scope of\nLogistics 5.0 -- the implementation of human-robot symbiosis in smart\nlogistics. We propose a dynamic, leader-follower Stackelberg game to model this\ninteraction, where utility functions explicitly account for human fatigue and\ntrust. Through agent-based simulations, we demonstrate that while a naive model\nleads to a \"trust death spiral,\" a refined trust model creates a \"trust synergy\ncycle,\" increasing productivity by nearly 100 percent. Finally, we show that a\ncobot equipped with a proactive Trust-Repair Protocol can overcome system\nbrittleness, reducing trust recovery time after a severe failure by over 75\npercent compared to a non-adaptive model. Our findings provide a framework for\ndesigning intelligent cobot behaviors that fulfill the Industry 5.0 pillars of\nhuman-centricity, sustainability, and resilience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03765v1", "cate": "cs.MA", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04332", "title": "DRAMA: A Dynamic and Robust Allocation-based Multi-Agent System for Changing Environments", "authors": ["Naibo Wang", "Yifan Zhang", "Sai Liu", "Xinkui Zhao", "Guanjie Cheng", "Yueshen Xu"], "categories": ["cs.MA"], "primary_category": "cs.MA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04332v1", "summary": "Multi-agent systems (MAS) have demonstrated significant effectiveness in\naddressing complex problems through coordinated collaboration among\nheterogeneous agents. However, real-world environments and task specifications\nare inherently dynamic, characterized by frequent changes, uncertainty, and\nvariability. Despite this, most existing MAS frameworks rely on static\narchitectures with fixed agent capabilities and rigid task allocation\nstrategies, which greatly limits their adaptability to evolving conditions.\nThis inflexibility poses substantial challenges for sustaining robust and\nefficient multi-agent cooperation in dynamic and unpredictable scenarios. To\naddress these limitations, we propose DRAMA: a Dynamic and Robust\nAllocation-based Multi-Agent System designed to facilitate resilient\ncollaboration in rapidly changing environments. DRAMA features a modular\narchitecture with a clear separation between the control plane and the worker\nplane. Both agents and tasks are abstracted as resource objects with\nwell-defined lifecycles, while task allocation is achieved via an\naffinity-based, loosely coupled mechanism. The control plane enables real-time\nmonitoring and centralized planning, allowing flexible and efficient task\nreassignment as agents join, depart, or become unavailable, thereby ensuring\ncontinuous and robust task execution. The worker plane comprises a cluster of\nautonomous agents, each with local reasoning, task execution, the ability to\ncollaborate, and the capability to take over unfinished tasks from other agents\nwhen needed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04332v1", "cate": "cs.MA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04378", "title": "Position-Based Flocking for Robust Alignment", "authors": ["Hossein B. Jond"], "categories": ["cs.MA", "cs.RO"], "primary_category": "cs.MA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04378v1", "summary": "This paper presents a position-based flocking model for interacting agents,\nbalancing cohesion-separation and alignment to achieve stable collective\nmotion. The model modifies a velocity-based approach by approximating velocity\ndifferences using initial and current positions, introducing a threshold weight\nto ensure sustained alignment. Simulations with 50 agents in 2D demonstrate\nthat the position-based model produces stronger alignment and more rigid and\ncompact formations compared to the velocity-based model. The alignment metric\nand separation distances highlight the efficacy of the proposed model in\nachieving robust flocking behavior. The model's use of positions ensures robust\nalignment, with applications in robotics and collective dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04378v1", "cate": "cs.MA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04537", "title": "Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments", "authors": ["Alkesh K. Srivastava", "Aamodh Suresh", "Carlos Nieto-Granda"], "categories": ["cs.MA", "cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04537", "summary": "We address the challenge of multi-robot autonomous hazard mapping in\nhigh-risk, failure-prone, communication-denied environments such as\npost-disaster zones, underground mines, caves, and planetary surfaces. In these\nmissions, robots must explore and map hazards while minimizing the risk of\nfailure due to environmental threats or hardware limitations. We introduce a\nbehavior-adaptive, information-theoretic planning framework for multi-robot\nteams grounded in the concept of Behavioral Entropy (BE), that generalizes\nShannon entropy (SE) to capture diverse human-like uncertainty evaluations.\nBuilding on this formulation, we propose the Behavior-Adaptive Path Planning\n(BAPP) framework, which modulates information gathering strategies via a\ntunable risk-sensitivity parameter, and present two planning algorithms:\nBAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for\nsafe deployment under high risk. We provide theoretical insights on the\ninformativeness of the proposed BAPP framework and validate its effectiveness\nthrough both single-robot and multi-robot simulations. Our results show that\nthe BAPP stack consistently outperforms Shannon-based and random strategies:\nBAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot\nsurvivability with minimal loss in information gain. In multi-agent\ndeployments, BAPP scales effectively through spatial partitioning, mobile base\nrelocation, and role-aware heterogeneity. These findings underscore the value\nof behavior-adaptive planning for robust, risk-sensitive exploration in\ncomplex, failure-prone environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04537", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04111", "title": "Negative binomial regression and inference using a pre-trained transformer", "authors": ["Valentine Svensson"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04111", "summary": "Negative binomial regression is essential for analyzing over-dispersed count\ndata in in comparative studies, but parameter estimation becomes\ncomputationally challenging in large screens requiring millions of comparisons.\nWe investigate using a pre-trained transformer to produce estimates of negative\nbinomial regression parameters from observed count data, trained through\nsynthetic data generation to learn to invert the process of generating counts\nfrom parameters. The transformer method achieved better parameter accuracy than\nmaximum likelihood optimization while being 20 times faster. However,\ncomparisons unexpectedly revealed that method of moment estimates performed as\nwell as maximum likelihood optimization in accuracy, while being 1,000 times\nfaster and producing better-calibrated and more powerful tests, making it the\nmost efficient solution for this application.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04111", "cate": "stat.ML", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04258", "title": "Deep Neural Network-Driven Adaptive Filtering", "authors": ["Qizhen Wang", "Gang Wang", "Ying-Chang Liang"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04258", "summary": "This paper proposes a deep neural network (DNN)-driven framework to address\nthe longstanding generalization challenge in adaptive filtering (AF). In\ncontrast to traditional AF frameworks that emphasize explicit cost function\ndesign, the proposed framework shifts the paradigm toward direct gradient\nacquisition. The DNN, functioning as a universal nonlinear operator, is\nstructurally embedded into the core architecture of the AF system, establishing\na direct mapping between filtering residuals and learning gradients. The\nmaximum likelihood is adopted as the implicit cost function, rendering the\nderived algorithm inherently data-driven and thus endowed with exemplary\ngeneralization capability, which is validated by extensive numerical\nexperiments across a spectrum of non-Gaussian scenarios. Corresponding mean\nvalue and mean square stability analyses are also conducted in detail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04258", "cate": "stat.ML", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04268", "title": "A virtual sensor fusion approach for state of charge estimation of lithium-ion cells", "authors": ["Davide Previtali", "Daniele Masti", "Mirko Mazzoleni", "Fabio Previdi"], "categories": ["cs.LG", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04268", "summary": "This paper addresses the estimation of the State Of Charge (SOC) of\nlithium-ion cells via the combination of two widely used paradigms: Kalman\nFilters (KFs) equipped with Equivalent Circuit Models (ECMs) and\nmachine-learning approaches. In particular, a recent Virtual Sensor (VS)\nsynthesis technique is considered, which operates as follows: (i) learn an\nAffine Parameter-Varying (APV) model of the cell directly from data, (ii)\nderive a bank of linear observers from the APV model, (iii) train a\nmachine-learning technique from features extracted from the observers together\nwith input and output data to predict the SOC. The SOC predictions returned by\nthe VS are supplied to an Extended KF (EKF) as output measurements along with\nthe cell terminal voltage, combining the two paradigms. A data-driven\ncalibration strategy for the noise covariance matrices of the EKF is proposed.\nExperimental results show that the designed approach is beneficial w.r.t. SOC\nestimation accuracy and smoothness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04268", "cate": "eess.SY", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04409", "title": "The Relative Instability of Model Comparison with Cross-validation", "authors": ["Alexandre Bayle", "Lucas Janson", "Lester Mackey"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04409", "summary": "Existing work has shown that cross-validation (CV) can be used to provide an\nasymptotic confidence interval for the test error of a stable machine learning\nalgorithm, and existing stability results for many popular algorithms can be\napplied to derive positive instances where such confidence intervals will be\nvalid. However, in the common setting where CV is used to compare two\nalgorithms, it becomes necessary to consider a notion of relative stability\nwhich cannot easily be derived from existing stability results, even for simple\nalgorithms. To better understand relative stability and when CV provides valid\nconfidence intervals for the test error difference of two algorithms, we study\nthe soft-thresholded least squares algorithm, a close cousin of the Lasso. We\nprove that while stability holds when assessing the individual test error of\nthis algorithm, relative stability fails to hold when comparing the test error\nof two such algorithms, even in a sparse low-dimensional linear model setting.\nAdditionally, we empirically confirm the invalidity of CV confidence intervals\nfor the test error difference when either soft-thresholding or the Lasso is\nused. In short, caution is needed when quantifying the uncertainty of CV\nestimates of the performance difference of two machine learning algorithms,\neven when both algorithms are individually stable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04409", "cate": "stat.ML", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04457", "title": "Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification", "authors": ["Simon Baur", "Wojciech Samek", "Jackie Ma"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04457", "summary": "Reliable uncertainty quantification is crucial for trustworthy\ndecision-making and the deployment of AI models in medical imaging. While prior\nwork has explored the ability of neural networks to quantify predictive,\nepistemic, and aleatoric uncertainties using an information-theoretical\napproach in synthetic or well defined data settings like natural image\nclassification, its applicability to real life medical diagnosis tasks remains\nunderexplored. In this study, we provide an extensive uncertainty\nquantification benchmark for multi-label chest X-ray classification using the\nMIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for\nconvolutional (ResNet) and transformer-based (Vision Transformer) architectures\nacross a wide range of tasks. Additionally, we extend Evidential Deep Learning,\nHetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting.\nOur analysis provides insights into uncertainty estimation effectiveness and\nthe ability to disentangle epistemic and aleatoric uncertainties, revealing\nmethod- and architecture-specific strengths and limitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04457", "cate": "stat.ML", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04590", "title": "Algebraically Observable Physics-Informed Neural Network and its Application to Epidemiological Modelling", "authors": ["Mizuka Komatsu"], "categories": ["cs.LG", "cs.SC", "math.DS", "q-bio.QM"], "primary_category": "cs.SC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04590", "summary": "Physics-Informed Neural Network (PINN) is a deep learning framework that\nintegrates the governing equations underlying data into a loss function. In\nthis study, we consider the problem of estimating state variables and\nparameters in epidemiological models governed by ordinary differential\nequations using PINNs. In practice, not all trajectory data corresponding to\nthe population described by models can be measured. Learning PINNs to estimate\nthe unmeasured state variables and epidemiological parameters using partial\nmeasurements is challenging.\n  Accordingly, we introduce the concept of algebraic observability of the state\nvariables. Specifically, we propose augmenting the unmeasured data based on\nalgebraic observability analysis. The validity of the proposed method is\ndemonstrated through numerical experiments under three scenarios in the context\nof epidemiological modelling. Specifically, given noisy and partial\nmeasurements, the accuracy of unmeasured states and parameter estimation of the\nproposed method is shown to be higher than that of the conventional methods.\nThe proposed method is also shown to be effective in practical scenarios, such\nas when the data corresponding to certain variables cannot be reconstructed\nfrom the measurements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04590", "cate": "cs.SC", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04646", "title": "Accept-Reject Lasso", "authors": ["Yanxin Liu", "Yunqi Zhang"], "categories": ["cs.LG", "stat.ME"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04646", "summary": "The Lasso method is known to exhibit instability in the presence of highly\ncorrelated features, often leading to an arbitrary selection of predictors.\nThis issue manifests itself in two primary error types: the erroneous omission\nof features that lack a true substitutable relationship (falsely redundant\nfeatures) and the inclusion of features with a true substitutable relationship\n(truly redundant features). Although most existing methods address only one of\nthese challenges, we introduce the Accept-Reject Lasso (ARL), a novel approach\nthat resolves this dilemma. ARL operationalizes an Accept-Reject framework\nthrough a fine-grained analysis of feature selection across data subsets. This\nframework is designed to partition the output of an ensemble method into\nbeneficial and detrimental components through fine-grained analysis. The\nfundamental challenge for Lasso is that inter-variable correlation obscures the\ntrue sources of information. ARL tackles this by first using clustering to\nidentify distinct subset structures within the data. It then analyzes Lasso's\nbehavior across these subsets to differentiate between true and spurious\ncorrelations. For truly correlated features, which induce multicollinearity,\nARL tends to select a single representative feature and reject the rest to\nensure model stability. Conversely, for features linked by spurious\ncorrelations, which may vanish in certain subsets, ARL accepts those that Lasso\nmight have incorrectly omitted. The distinct patterns arising from true versus\nspurious correlations create a divisible separation. By setting an appropriate\nthreshold, our framework can effectively distinguish between these two\nphenomena, thereby maximizing the inclusion of informative variables while\nminimizing the introduction of detrimental ones. We illustrate the efficacy of\nthe proposed method through extensive simulation and real-data experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04646", "cate": "stat.ME", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2303.00788", "title": "Multi-task neural networks by learned contextual inputs", "authors": ["Anders T. Sandnes", "Bjarne Grimstad", "Odd KolbjÃ¸rnsen"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.00788", "summary": "This paper explores learned-context neural networks. It is a multi-task\nlearning architecture based on a fully shared neural network and an augmented\ninput vector containing trainable task parameters. The architecture is\ninteresting due to its powerful task adaption mechanism, which facilitates a\nlow-dimensional task parameter space. Theoretically, we show that a scalar task\nparameter is sufficient for universal approximation of all tasks, which is not\nnecessarily the case for more common architectures. Empirically it is shown\nthat, for homogeneous tasks, the dimension of the task parameter may vary with\nthe complexity of the tasks, but a small task parameter space is generally\nviable. The task parameter space is found to be well-behaved, which simplifies\nworkflows related to updating models as new data arrives, and learning new\ntasks with the shared parameters are frozen. Additionally, the architecture\ndisplays robustness towards datasets where tasks have few data points. The\narchitecture's performance is compared to similar neural network architectures\non ten datasets, with competitive results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.00788", "cate": "cs.LG", "date": "2023-03-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2310.05179", "title": "DRL-ORA: Distributional Reinforcement Learning with Online Risk Adaption", "authors": ["Yupeng Wu", "Wenyun Li", "Wenjie Huang", "Chin Pang Ho"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.05179", "summary": "One of the main challenges in reinforcement learning (RL) is that the agent\nhas to make decisions that would influence the future performance without\nhaving complete knowledge of the environment. Dynamically adjusting the level\nof epistemic risk during the learning process can help to achieve reliable\npolicies in safety-critical settings with better efficiency. In this work, we\npropose a new framework, Distributional RL with Online Risk Adaptation\n(DRL-ORA). This framework quantifies both epistemic and implicit aleatory\nuncertainties in a unified manner and dynamically adjusts the epistemic risk\nlevels by solving a total variation minimization problem online. The framework\nunifies the existing variants of risk adaption approaches and offers better\nexplainability and flexibility. The selection of risk levels is performed\nefficiently via a grid search using a Follow-The-Leader-type algorithm, where\nthe offline oracle also corresponds to a ''satisficing measure'' under a\nspecially modified loss function. We show that DRL-ORA outperforms existing\nmethods that rely on fixed risk levels or manually designed risk level\nadaptation in multiple classes of tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.05179", "cate": "cs.LG", "date": "2023-10-08", "updated": "2025-08-06", "section": "repl"}
{"id": "2402.03055", "title": "Deep Exploration with PAC-Bayes", "authors": ["Bahareh Tasdighi", "Manuel Haussmann", "Nicklas Werge", "Yi-Shan Wu", "Melih Kandemir"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.03055", "summary": "Reinforcement learning (RL) for continuous control under delayed rewards is\nan under-explored problem despite its significance in real-world applications.\nMany complex skills are based on intermediate ones as prerequisites. For\ninstance, a humanoid locomotor must learn how to stand before it can learn to\nwalk. To cope with delayed reward, an agent must perform deep exploration.\nHowever, existing deep exploration methods are designed for small discrete\naction spaces, and their generalization to state-of-the-art continuous control\nremains unproven. We address the deep exploration problem for the first time\nfrom a PAC-Bayesian perspective in the context of actor-critic learning. To do\nthis, we quantify the error of the Bellman operator through a PAC-Bayes bound,\nwhere a bootstrapped ensemble of critic networks represents the posterior\ndistribution, and their targets serve as a data-informed function-space prior.\nWe derive an objective function from this bound and use it to train the critic\nensemble. Each critic trains an individual soft actor network, implemented as a\nshared trunk and critic-specific heads. The agent performs deep exploration by\nacting epsilon-softly on a randomly chosen actor head. Our proposed algorithm,\nnamed {\\it PAC-Bayesian Actor-Critic (PBAC)}, is the only algorithm to\nconsistently discover delayed rewards on continuous control tasks with varying\ndifficulty.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.03055", "cate": "cs.LG", "date": "2024-02-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2407.06083", "title": "A Survey of Controllable Learning: Methods and Applications in Information Retrieval", "authors": ["Chenglei Shen", "Xiao Zhang", "Teng Shi", "Changshuo Zhang", "Guofu Xie", "Jun Xu"], "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.06083", "summary": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.06083", "cate": "cs.LG", "date": "2024-07-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.13287", "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware Selection of Generative Models and LLMs", "authors": ["Xiaoyan Hu", "Ho-fung Leung", "Farzan Farnia"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13287", "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13287", "cate": "cs.LG", "date": "2024-10-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.14380", "title": "Dual-Label Learning With Irregularly Present Labels", "authors": ["Mingqian Li", "Qiao Han", "Ruifeng Li", "Yao Yang", "Hongyang Chen"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.14380", "summary": "In multi-task learning, labels are often missing irregularly across samples,\nwhich can be fully labeled, partially labeled or unlabeled. The irregular label\npresence often appears in scientific studies due to experimental limitations.\nIt triggers a demand for a new training and inference mechanism that could\naccommodate irregularly present labels and maximize their utility. This work\nfocuses on the two-label learning task and proposes a novel training and\ninference framework, Dual-Label Learning (DLL). The DLL framework formulates\nthe problem into a dual-function system, in which the two functions should\nsimultaneously satisfy standard supervision, structural duality and\nprobabilistic duality. DLL features a dual-tower model architecture that allows\nfor explicit information exchange between labels, aimed at maximizing the\nutility of partially available labels. During training, missing labels are\nimputed as part of the forward propagation process, while during inference,\nlabels are predicted jointly as unknowns of a bivariate system of equations.\nOur theoretical analysis guarantees the feasibility of DLL, and extensive\nexperiments are conducted to verify that by explicitly modeling label\ncorrelation and maximizing label utility, our method makes consistently better\nprediction than baseline approaches by up to 9.6% gain in F1-score or 10.2%\nreduction in MAPE. Remarkably, DLL maintains robust performance at a label\nmissing rate of up to 60%, achieving even better results than baseline\napproaches at lower missing rates down to only 10%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.14380", "cate": "cs.LG", "date": "2024-10-18", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.06917", "title": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal Sensor Fusion", "authors": ["Keivan Faghih Niresi", "Ismail Nejjar", "Olga Fink"], "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06917", "summary": "The growing deployment of low-cost, distributed sensor networks in\nenvironmental and biomedical domains has enabled continuous, large-scale health\nmonitoring. However, these systems often face challenges related to degraded\ndata quality caused by sensor drift, noise, and insufficient calibration --\nfactors that limit their reliability in real-world applications. Traditional\nmachine learning methods for sensor fusion and calibration rely on extensive\nfeature engineering and struggle to capture spatial-temporal dependencies or\nadapt to distribution shifts across varying deployment conditions. To address\nthese challenges, we propose a novel unsupervised domain adaptation (UDA)\nmethod tailored for regression tasks. Our proposed method integrates\neffectively with Spatial-Temporal Graph Neural Networks and leverages the\nalignment of perturbed inverse Gram matrices between source and target domains,\ndrawing inspiration from Tikhonov regularization. This approach enables\nscalable and efficient domain adaptation without requiring labeled data in the\ntarget domain. We validate our novel method on real-world datasets from two\ndistinct applications: air quality monitoring and EEG signal reconstruction.\nOur method achieves state-of-the-art performance which paves the way for more\nrobust and transferable sensor fusion models in both environmental and\nphysiological contexts. Our code is available at\nhttps://github.com/EPFL-IMOS/TikUDA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06917", "cate": "cs.LG", "date": "2024-11-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.10639", "title": "Paragon: Parameter Generation for Controllable Multi-Task Recommendation", "authors": ["Chenglei Shen", "Jiahao Zhao", "Xiao Zhang", "Weijie Yu", "Ming He", "Jianping Fan"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.10639", "summary": "Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learned model to different task requirements by controlling the model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via \\textbf{para}meter\n\\textbf{g}eneration for c\\textbf{on}trollable multi-task recommendation\n(\\textbf{Paragon}), which allows the customization and adaptation of\nrecommendation model parameters to new task requirements without retraining.\nSpecifically, we first obtain the optimized model parameters through adapter\ntunning based on the feasible task requirements. Then, we utilize the\ngenerative model as a parameter generator, employing classifier-free guidance\nin conditional training to learn the distribution of optimized model parameters\nunder various task requirements. Finally, the parameter generator is applied to\neffectively generate model parameters in a test-time adaptation manner given\ntask requirements. Moreover, Paragon seamlessly integrates with various\nexisting recommendation models to enhance their controllability. Extensive\nexperiments on two public datasets and one commercial dataset demonstrate that\nParagon can efficiently generate model parameters instead of retraining,\nreducing computational time by at least 94.6\\%. The code is released at\n\\href{https://github.com/bubble65/Paragon}{https://github.com/bubble65/Paragon}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.10639", "cate": "cs.IR", "date": "2024-10-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2411.13415", "title": "Harnessing Large Language Models for Group POI Recommendations", "authors": ["Jing Long", "Liang Qu", "Junliang Yu", "Tong Chen", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.13415", "summary": "The rapid proliferation of Location-Based Social Networks (LBSNs) has\nunderscored the importance of Point-of-Interest (POI) recommendation systems in\nenhancing user experiences. While individual POI recommendation methods\nleverage users' check-in histories to provide personalized suggestions, they\nstruggle to address scenarios requiring group decision-making. Group POI\nrecommendation systems aim to satisfy the collective preferences of multiple\nusers, but existing approaches face two major challenges: diverse group\npreferences and extreme data sparsity in group check-in data. To overcome these\nchallenges, we propose LLMGPR, a novel framework that leverages large language\nmodels (LLMs) for group POI recommendations. LLMGPR introduces\nsemantic-enhanced POI tokens and incorporates rich contextual information to\nmodel the diverse and complex dynamics of group decision-making. To further\nenhance its capabilities, we developed a sequencing adapter using Quantized\nLow-Rank Adaptation (QLoRA), which aligns LLMs with group POI recommendation\ntasks. To address the issue of sparse group check-in data, LLMGPR employs an\naggregation adapter that integrates individual representations into meaningful\ngroup representations. Additionally, a self-supervised learning (SSL) task is\ndesigned to predict the purposes of check-in sequences (e.g., business trips\nand family vacations), thereby enriching group representations with deeper\nsemantic insights. Extensive experiments demonstrate the effectiveness of\nLLMGPR, showcasing its ability to significantly enhance the accuracy and\nrobustness of group POI recommendations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.13415", "cate": "cs.IR", "date": "2024-11-20", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.22303", "title": "GraphRAG-Induced Dual Knowledge Structure Graphs for Personalized Learning Path Recommendation", "authors": ["Xinghe Cheng", "Zihan Zhang", "Jiapu Wang", "Liangda Fang", "Chaobo He", "Quanlong Guan", "Shirui Pan", "Weiqi Luo"], "categories": ["cs.IR"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22303", "summary": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (\\eg, knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Requiring prerequisite relationships between\nknowledge concepts, which are difficult to obtain due to the cost of expert\nannotation, hindering the application of current learning path recommendation\nmethods. 2) Relying on a single, sequentially dependent knowledge structure\nbased on prerequisite relationships implies that difficulties at any stage can\ncause learning blockages, which in turn disrupt subsequent learning processes.\nTo address these challenges, we propose a novel approach, GraphRAG-Induced Dual\nKnowledge Structure Graphs for Personalized Learning Path Recommendation\n(KnowLP), which enhances learning path recommendations by incorporating both\nprerequisite and similarity relationships between knowledge concepts.\nSpecifically, we introduce a knowledge concept structure graph generation\nmodule EDU-GraphRAG that adaptively constructs knowledge concept structure\ngraphs for different educational datasets, significantly improving the\ngeneralizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) module, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22303", "cate": "cs.IR", "date": "2025-06-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.21563", "title": "Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation", "authors": ["Minh-Anh Nguyen", "Bao Nguyen", "Ha Lan N.T.", "Tuan Anh Hoang", "Duc-Trong Le", "Dung D. Le"], "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21563", "summary": "Recommendation systems often suffer from data sparsity caused by limited\nuser-item interactions, which degrade their performance and amplify popularity\nbias in real-world scenarios. This paper proposes a novel data augmentation\nframework that leverages Large Language Models (LLMs) and item textual\ndescriptions to enrich interaction data. By few-shot prompting LLMs multiple\ntimes to rerank items and aggregating the results via majority voting, we\ngenerate high-confidence synthetic user-item interactions, supported by\ntheoretical guarantees based on the concentration of measure. To effectively\nleverage the augmented data in the context of a graph recommendation system, we\nintegrate it into a graph contrastive learning framework to mitigate\ndistributional shift and alleviate popularity bias. Extensive experiments show\nthat our method improves accuracy and reduces popularity bias, outperforming\nstrong baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21563", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-08-06", "section": "repl"}
{"id": "2501.10945", "title": "Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond", "authors": ["Weiyu Chen", "Baijiong Lin", "Xiaoyuan Zhang", "Xi Lin", "Han Zhao", "Qingfu Zhang", "James T. Kwok"], "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.10945", "summary": "Many modern deep learning applications require balancing multiple objectives\nthat are often conflicting. Examples include multi-task learning,\nfairness-aware learning, and the alignment of Large Language Models (LLMs).\nThis leads to multi-objective deep learning, which tries to find optimal\ntrade-offs or Pareto-optimal solutions by adapting mathematical principles from\nthe field of Multi-Objective Optimization (MOO). However, directly applying\ngradient-based MOO techniques to deep neural networks presents unique\nchallenges, including high computational costs, optimization instability, and\nthe difficulty of effectively incorporating user preferences. This paper\nprovides a comprehensive survey of gradient-based techniques for\nmulti-objective deep learning. We systematically categorize existing algorithms\nbased on their outputs: (i) methods that find a single, well-balanced solution,\n(ii) methods that generate a finite set of diverse Pareto-optimal solutions,\nand (iii) methods that learn a continuous Pareto set of solutions. In addition\nto this taxonomy, the survey covers theoretical analyses, key applications,\npractical resources, and highlights open challenges and promising directions\nfor future research. A comprehensive list of multi-objective deep learning\nalgorithms is available at\nhttps://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.10945", "cate": "cs.LG", "date": "2025-01-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2501.16894", "title": "DBSCAN in domains with periodic boundary conditions", "authors": ["Xander M. de Wit", "Alessandro Gabbana"], "categories": ["cs.LG", "physics.comp-ph", "physics.flu-dyn"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.16894", "summary": "Many scientific problems involve data that is embedded in a space with\nperiodic boundary conditions. This can for instance be related to an inherent\ncyclic or rotational symmetry in the data or a spatially extended periodicity.\nWhen analyzing such data, well-tailored methods are needed to obtain efficient\napproaches that obey the periodic boundary conditions of the problem. In this\nwork, we present a method for applying a clustering algorithm to data embedded\nin a periodic domain based on the DBSCAN algorithm, a widely used unsupervised\nmachine learning method that identifies clusters in data. The proposed method\ninternally leverages the conventional DBSCAN algorithm for domains with open\nboundaries, such that it remains compatible with all optimized implementations\nfor neighborhood searches in open domains. In this way, it retains the same\noptimized runtime complexity of $O(N\\log N)$. We demonstrate the workings of\nthe proposed method using synthetic data in one, two and three dimensions and\nalso apply it to a real-world example involving the clustering of bubbles in a\nturbulent flow. The proposed approach is implemented in a ready-to-use Python\npackage that we make publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.16894", "cate": "cs.LG", "date": "2025-01-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.09781", "title": "Learning richness modulates equality reasoning in neural networks", "authors": ["William L. Tong", "Cengiz Pehlevan"], "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.09781", "summary": "Equality reasoning is ubiquitous and purely abstract: sameness or difference\nmay be evaluated no matter the nature of the underlying objects. As a result,\nsame-different (SD) tasks have been extensively studied as a starting point for\nunderstanding abstract reasoning in humans and across animal species. With the\nrise of neural networks that exhibit striking apparent proficiency for\nabstractions, equality reasoning in these models has also gained interest. Yet\ndespite extensive study, conclusions about equality reasoning vary widely and\nwith little consensus. To clarify the underlying principles in learning SD\ntasks, we develop a theory of equality reasoning in multi-layer perceptrons\n(MLP). Following observations in comparative psychology, we propose a spectrum\nof behavior that ranges from conceptual to perceptual outcomes. Conceptual\nbehavior is characterized by task-specific representations, efficient learning,\nand insensitivity to spurious perceptual details. Perceptual behavior is\ncharacterized by strong sensitivity to spurious perceptual details, accompanied\nby the need for exhaustive training to learn the task. We develop a\nmathematical theory to show that an MLP's behavior is driven by learning\nrichness. Rich-regime MLPs exhibit conceptual behavior, whereas lazy-regime\nMLPs exhibit perceptual behavior. We validate our theoretical findings in\nvision SD experiments, showing that rich feature learning promotes success by\nencouraging hallmarks of conceptual behavior. Overall, our work identifies\nfeature learning richness as a key parameter modulating equality reasoning, and\nsuggests that equality reasoning in humans and animals may similarly depend on\nlearning richness in neural circuits.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.09781", "cate": "cs.LG", "date": "2025-03-12", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.12569", "title": "Let the Void Be Void: Robust Open-Set Semi-Supervised Learning via Selective Non-Alignment", "authors": ["You Rim Choi", "Subeom Park", "Seojun Heo", "Eunchung Noh", "Hyung-Sin Kim"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12569", "summary": "Open-set semi-supervised learning (OSSL) leverages unlabeled data containing\nboth in-distribution (ID) and unknown out-of-distribution (OOD) samples, aiming\nsimultaneously to improve closed-set accuracy and detect novel OOD instances.\nExisting methods either discard valuable information from uncertain samples or\nforce-align every unlabeled sample into one or a few synthetic \"catch-all\"\nrepresentations, resulting in geometric collapse and overconfidence on only\nseen OODs. To address the limitations, we introduce selective non-alignment,\nadding a novel \"skip\" operator into conventional pull and push operations of\ncontrastive learning. Our framework, SkipAlign, selectively skips alignment\n(pulling) for low-confidence unlabeled samples, retaining only gentle repulsion\nagainst ID prototypes. This approach transforms uncertain samples into a pure\nrepulsion signal, resulting in tighter ID clusters and naturally dispersed OOD\nfeatures. Extensive experiments demonstrate that SkipAlign significantly\noutperforms state-of-the-art methods in detecting unseen OOD data without\nsacrificing ID classification accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12569", "cate": "cs.LG", "date": "2025-04-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.15110", "title": "Approximation Rates in Besov Norms and Sample-Complexity of Kolmogorov-Arnold Networks with Residual Connections", "authors": ["Anastasis Kratsios", "Bum Jun Kim", "Takashi Furuya"], "categories": ["cs.LG", "cs.NE", "math.FA", "math.NA", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.15110", "summary": "Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold\nNetworks (KANs) have recently emerged as an improved backbone for most deep\nlearning frameworks, promising more adaptivity than their multilayer perceptron\n(MLP) predecessor by allowing for trainable spline-based activation functions.\nIn this paper, we probe the theoretical foundations of the KAN architecture by\nshowing that it can optimally approximate any Besov function in\n$B^{s}_{p,q}(\\mathcal{X})$ on a bounded open, or even fractal, domain\n$\\mathcal{X}$ in $\\mathbb{R}^d$ at the optimal approximation rate with respect\nto any weaker Besov norm $B^{\\alpha}_{p,q}(\\mathcal{X})$; where $\\alpha < s$.\nWe complement our approximation result with a statistical guarantee by bounding\nthe pseudodimension of the relevant class of Res-KANs. As an application of the\nlatter, we directly deduce a dimension-free estimate on the sample complexity\nof a residual KAN model when learning a function of Besov regularity from $N$\ni.i.d. noiseless samples, showing that KANs can learn the smooth maps which\nthey can approximate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15110", "cate": "cs.LG", "date": "2025-04-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.03552", "title": "Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming", "authors": ["Linus Langenkamp", "Philip Hannebohm", "Bernhard Bachmann"], "categories": ["cs.LG", "math.DS", "math.OC"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.03552", "summary": "We propose a novel approach for training Physics-enhanced Neural ODEs\n(PeN-ODEs) by expressing the training process as a dynamic optimization\nproblem. The full model, including neural components, is discretized using a\nhigh-order implicit Runge-Kutta method with flipped Legendre-Gauss-Radau\npoints, resulting in a large-scale nonlinear program (NLP) efficiently solved\nby state-of-the-art NLP solvers such as Ipopt. This formulation enables\nsimultaneous optimization of network parameters and state trajectories,\naddressing key limitations of ODE solver-based training in terms of stability,\nruntime, and accuracy. Extending on a recent direct collocation-based method\nfor Neural ODEs, we generalize to PeN-ODEs, incorporate physical constraints,\nand present a custom, parallelized, open-source implementation. Benchmarks on a\nQuarter Vehicle Model and a Van-der-Pol oscillator demonstrate superior\naccuracy, speed, generalization with smaller networks compared to other\ntraining techniques. We also outline a planned integration into OpenModelica to\nenable accessible training of Neural DAEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.03552", "cate": "cs.LG", "date": "2025-05-06", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.13241", "title": "Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach", "authors": ["Yuan-Zheng Lei", "Yaobang Gong", "Dianwei Chen", "Yao Cheng", "Xianfeng Terry Yang"], "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13241", "summary": "Physics-informed machine learning (PIML) is crucial in modern traffic flow\nmodeling because it combines the benefits of both physics-based and data-driven\napproaches. In conventional PIML, physical information is typically\nincorporated by constructing a hybrid loss function that combines data-driven\nloss and physics loss through linear scalarization. The goal is to find a\ntrade-off between these two objectives to improve the accuracy of model\npredictions. However, from a mathematical perspective, linear scalarization is\nlimited to identifying only the convex region of the Pareto front, as it treats\ndata-driven and physics losses as separate objectives. Given that most PIML\nloss functions are non-convex, linear scalarization restricts the achievable\ntrade-off solutions. Moreover, tuning the weighting coefficients for the two\nloss components can be both time-consuming and computationally challenging. To\naddress these limitations, this paper introduces a paradigm shift in PIML by\nreformulating the training process as a multi-objective optimization problem,\ntreating data-driven loss and physics loss independently. We apply several\nmulti-gradient descent algorithms (MGDAs), including traditional multi-gradient\ndescent (TMGD) and dual cone gradient descent (DCGD), to explore the Pareto\nfront in this multi-objective setting. These methods are evaluated on both\nmacroscopic and microscopic traffic flow models. In the macroscopic case, MGDAs\nachieved comparable performance to traditional linear scalarization methods.\nNotably, in the microscopic case, MGDAs significantly outperformed their\nscalarization-based counterparts, demonstrating the advantages of a\nmulti-objective optimization approach in complex PIML scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13241", "cate": "cs.LG", "date": "2025-05-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.24452", "title": "Stepsize anything: A unified learning rate schedule for budgeted-iteration training", "authors": ["Anda Tang", "Yiming Dong", "Yutao Zeng", "zhou Xun", "Zhouchen Lin"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.24452", "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets. While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations. In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient. In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets. First, we bridge the gap by\nconstructing a novel training budget-aware optimization framework, which\nexplicitly accounts for the robustness to landscape curvature variations. From\nthis framework, we derive the UBA schedule, controlled by a single\nhyper-parameter \\varphi that provides a trade-off between flexibility and\nsimplicity, eliminating the need for per-network numerical optimization.\nMoreover, we establish a theoretical connection between \\varphi and the\ncondition number, adding interpretation and justification to our approach.\nBesides, we prove the convergence for different values of \\varphi. We offer\npractical guidelines for its selection via theoretical analysis and empirical\nresults. Extensive experimental results show that UBA consistently surpasses\nthe commonly-used schedules across diverse vision and language tasks, spanning\nnetwork architectures (e.g., ResNet, OLMo) and scales, under different\ntraining-iteration budgets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.24452", "cate": "cs.LG", "date": "2025-05-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.11044", "title": "Boost Post-Training Quantization via Null Space Optimization for Large Language Models", "authors": ["Jiaqi Zhao", "Weili Guan", "Ming Li", "Miao Zhang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11044", "summary": "Existing post-training quantization methods for large language models (LLMs)\noffer remarkable success. However, the increasingly marginal performance gains\nsuggest that existing quantization strategies are insufficient to support the\ndevelopment of more compressed models. To inspire new directions for future\nresearch, this paper introduces the concept of null space into LLMs\nquantization. We argue that the quantization error can be effectively\nalleviated by constraining the post-quantization weight perturbation to lie\nwithin the null space of input activations. To prove this idea, we propose a\nplug-and-play null space projection module for existing milestone PTQ baselines\nnamed Q2N. Specifically, we first design an efficient and accurate null space\nprojection approximation method tailored to the characteristics of LLMs.\nSubsequently, we theoretically derive a closed-form solution for an equivalent\nvector of the obtained projection matrix, which satisfies practical inference\ncondition while avoiding additional memory overhead. Extensive experiments are\nconducted on various state-of-the-art LLMs (LLaMA3, DeepSeek, Qwen3) and\nbaselines, demonstrating the effectiveness of both our Q2N and the perspective\nof null space optimization for LLMs quantization. We view this paper the first\nstep to further alleviate the quantization error based on the insights of null\nspace, hoping it inspiring future researchers to design more advanced\nquantization methods. Codes are available at https://github.com/zjq0455/q2n.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11044", "cate": "cs.LG", "date": "2025-05-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.09897", "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task", "authors": ["Loek van Rossem", "Andrew M. Saxe"], "categories": ["cs.LG", "q-bio.NC"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09897", "summary": "Even when massively overparameterized, deep neural networks show a remarkable\nability to generalize. Research on this phenomenon has focused on\ngeneralization within distribution, via smooth interpolation. Yet in some\nsettings neural networks also learn to extrapolate to data far beyond the\nbounds of the original training set, sometimes even allowing for infinite\ngeneralization, implying that an algorithm capable of solving the task has been\nlearned. Here we undertake a case study of the learning dynamics of recurrent\nneural networks (RNNs) trained on the streaming parity task in order to develop\nan effective theory of algorithm development. The streaming parity task is a\nsimple but nonlinear task defined on sequences up to arbitrary length. We show\nthat, with sufficient finite training experience, RNNs exhibit a phase\ntransition to perfect infinite generalization. Using an effective theory for\nthe representational dynamics, we find an implicit representational merger\neffect which can be interpreted as the construction of a finite automaton that\nreproduces the task. Overall, our results disclose one mechanism by which\nneural networks can generalize infinitely from finite training experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09897", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.18549", "title": "The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection", "authors": ["Steven A. Frank"], "categories": ["cs.LG", "q-bio.PE"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18549", "summary": "Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, in proportion to the\nslope of performance with respect to the parameters. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18549", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-08-05", "section": "repl"}
{"id": "2507.21155", "title": "SPADE-S: A Sparsity-Robust Foundational Forecaster", "authors": ["Malcolm Wolff", "Matthew Li", "Ravi Kiran Selvam", "Hanjing Zhu", "Kin G. Olivares", "Ruijun Ma", "Abhinav Katoch", "Shankar Ramasubramanian", "Mengfei Cao", "Roberto Bandarra", "Rahul Gopalsamy", "Stefania La Vattiata", "Sitan Yang", "Michael W. Mahoney"], "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21155", "summary": "Despite significant advancements in time series forecasting, accurate\nmodeling of time series with strong heterogeneity in magnitude and/or sparsity\npatterns remains challenging for state-of-the-art deep learning architectures.\nWe identify several factors that lead existing models to systematically\nunderperform on low-magnitude and sparse time series, including loss functions\nwith implicit biases toward high-magnitude series, training-time sampling\nmethods, and limitations of time series encoding methods.\n  SPADE-S is a robust forecasting architecture that significantly reduces\nmagnitude- and sparsity-based systematic biases and improves overall prediction\naccuracy. Empirical results demonstrate that SPADE-S outperforms existing\nstate-of-the-art approaches across a diverse set of use cases in demand\nforecasting. In particular, we show that, depending on the quantile forecast\nand magnitude of the series, SPADE-S can improve forecast accuracy by up to\n15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and\n4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%,\nrespectively, for each of three distinct datasets, ranging from 3 million to\n700 million series, from a large online retailer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21155", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.00635", "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00635", "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00635", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01883", "title": "Proactive Constrained Policy Optimization with Preemptive Penalty", "authors": ["Ning Yang", "Pengyu Wang", "Guoqing Liu", "Haifeng Zhang", "Pin Lv", "Jun Wang"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01883", "summary": "Safe Reinforcement Learning (RL) often faces significant issues such as\nconstraint violations and instability, necessitating the use of constrained\npolicy optimization, which seeks optimal policies while ensuring adherence to\nspecific constraints like safety. Typically, constrained optimization problems\nare addressed by the Lagrangian method, a post-violation remedial approach that\nmay result in oscillations and overshoots. Motivated by this, we propose a\nnovel method named Proactive Constrained Policy Optimization (PCPO) that\nincorporates a preemptive penalty mechanism. This mechanism integrates barrier\nitems into the objective function as the policy nears the boundary, imposing a\ncost. Meanwhile, we introduce a constraint-aware intrinsic reward to guide\nboundary-aware exploration, which is activated only when the policy approaches\nthe constraint boundary. We establish theoretical upper and lower bounds for\nthe duality gap and the performance of the PCPO update, shedding light on the\nmethod's convergence characteristics. Additionally, to enhance the optimization\nperformance, we adopt a policy iteration approach. An interesting finding is\nthat PCPO demonstrates significant stability in experiments. Experimental\nresults indicate that the PCPO framework provides a robust solution for policy\noptimization under constraints, with important implications for future research\nand practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01883", "cate": "cs.LG", "date": "2025-08-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.01957", "title": "Stochastic Encodings for Active Feature Acquisition", "authors": ["Alexander Norcliffe", "Changhee Lee", "Fergus Imrie", "Mihaela van der Schaar", "Pietro Lio"], "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01957", "summary": "Active Feature Acquisition is an instance-wise, sequential decision making\nproblem. The aim is to dynamically select which feature to measure based on\ncurrent observations, independently for each test instance. Common approaches\neither use Reinforcement Learning, which experiences training difficulties, or\ngreedily maximize the conditional mutual information of the label and\nunobserved features, which makes myopic acquisitions. To address these\nshortcomings, we introduce a latent variable model, trained in a supervised\nmanner. Acquisitions are made by reasoning about the features across many\npossible unobserved realizations in a stochastic latent space. Extensive\nevaluation on a large range of synthetic and real datasets demonstrates that\nour approach reliably outperforms a diverse set of baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01957", "cate": "cs.LG", "date": "2025-08-03", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02381", "title": "Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction: A Pruning Framework for LLMs", "authors": ["Zuxin Ma", "Yunhe Cui", "Yongbin Qin"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02381", "summary": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nseconds), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02381", "cate": "cs.LG", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03267", "title": "HALO: Hindsight-Augmented Learning for Online Auto-Bidding", "authors": ["Pusen Dong", "Chenglong Cao", "Xinyu Zhou", "Jirong You", "Linhe Xu", "Feifan Xu", "Shuo Yuan"], "categories": ["cs.LG"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03267", "summary": "Digital advertising platforms operate millisecond-level auctions through\nReal-Time Bidding (RTB) systems, where advertisers compete for ad impressions\nthrough algorithmic bids. This dynamic mechanism enables precise audience\ntargeting but introduces profound operational complexity due to advertiser\nheterogeneity: budgets and ROI targets span orders of magnitude across\nadvertisers, from individual merchants to multinational brands. This diversity\ncreates a demanding adaptation landscape for Multi-Constraint Bidding (MCB).\nTraditional auto-bidding solutions fail in this environment due to two critical\nflaws: 1) severe sample inefficiency, where failed explorations under specific\nconstraints yield no transferable knowledge for new budget-ROI combinations,\nand 2) limited generalization under constraint shifts, as they ignore physical\nrelationships between constraints and bidding coefficients. To address this, we\npropose HALO: Hindsight-Augmented Learning for Online Auto-Bidding. HALO\nintroduces a theoretically grounded hindsight mechanism that repurposes all\nexplorations into training data for arbitrary constraint configuration via\ntrajectory reorientation. Further, it employs B-spline functional\nrepresentation, enabling continuous, derivative-aware bid mapping across\nconstraint spaces. HALO ensures robust adaptation even when budget/ROI\nrequirements differ drastically from training scenarios. Industrial dataset\nevaluations demonstrate the superiority of HALO in handling multi-scale\nconstraints, reducing constraint violations while improving GMV.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03267", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03579", "title": "Heterogeneity-Oblivious Robust Federated Learning", "authors": ["Weiyao Zhang", "Jinyang Li", "Qi Song", "Miao Wang", "Chungang Lin", "Haitong Luo", "Xuying Meng", "Yujun Zhang"], "categories": ["cs.LG", "cs.NI"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03579", "summary": "Federated Learning (FL) remains highly vulnerable to poisoning attacks,\nespecially under real-world hyper-heterogeneity, where clients differ\nsignificantly in data distributions, communication capabilities, and model\narchitectures. Such heterogeneity not only undermines the effectiveness of\naggregation strategies but also makes attacks more difficult to detect.\nFurthermore, high-dimensional models expand the attack surface. To address\nthese challenges, we propose Horus, a heterogeneity-oblivious robust FL\nframework centered on low-rank adaptations (LoRAs). Rather than aggregating\nfull model parameters, Horus inserts LoRAs into empirically stable layers and\naggregates only LoRAs to reduce the attack uncover a key empirical observation\nthat the input projection (LoRA-A) is markedly more stable than the output\nprojection (LoRA-B) under heterogeneity and poisoning. Leveraging this, we\ndesign a Heterogeneity-Oblivious Poisoning Score using the features from LoRA-A\nto filter poisoned clients. For the remaining benign clients, we propose\nprojection-aware aggregation mechanism to preserve collaborative signals while\nsuppressing drifts, which reweights client updates by consistency with the\nglobal directions. Extensive experiments across diverse datasets, model\narchitectures, and attacks demonstrate that Horus consistently outperforms\nstate-of-the-art baselines in both robustness and accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03579", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03679", "title": "Streaming Generated Gaussian Process Experts for Online Learning and Control", "authors": ["Zewen Yang", "Dongfa Zhang", "Xiaobing Dai", "Fengyi Yu", "Chi Zhang", "Bingkun Huang", "Hamid Sadeghian", "Sami Haddadin"], "categories": ["cs.LG", "eess.SY", "stat.ML"], "primary_category": "cs.LG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03679", "summary": "Gaussian Processes (GPs), as a nonparametric learning method, offer flexible\nmodeling capabilities and calibrated uncertainty quantification for function\napproximations. Additionally, GPs support online learning by efficiently\nincorporating new data with polynomial-time computation, making them\nwell-suited for safety-critical dynamical systems that require rapid\nadaptation. However, the inference and online updates of exact GPs, when\nprocessing streaming data, incur cubic computation time and quadratic storage\nmemory complexity, limiting their scalability to large datasets in real-time\nsettings. In this paper, we propose a streaming kernel-induced progressively\ngenerated expert framework of Gaussian processes (SkyGP) that addresses both\ncomputational and memory constraints by maintaining a bounded set of experts,\nwhile inheriting the learning performance guarantees from exact Gaussian\nprocesses. Furthermore, two SkyGP variants are introduced, each tailored to a\nspecific objective, either maximizing prediction accuracy (SkyGP-Dense) or\nimproving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is\nvalidated through extensive benchmarks and real-time control experiments\ndemonstrating its superior performance compared to state-of-the-art approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03679", "cate": "cs.LG", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2306.07886", "title": "Symmetry & Critical Points for Symmetric Tensor Decomposition Problems", "authors": ["Yossi Arjevani", "Gal Vinograd"], "categories": ["cs.LG", "math.AG", "math.NA", "math.OC", "stat.ML"], "primary_category": "math.OC", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.07886", "summary": "We consider the nonconvex optimization problem associated with the\ndecomposition of a real symmetric tensor into a sum of rank-one terms. Use is\nmade of the rich symmetry structure to construct infinite families of critical\npoints represented by Puiseux series in the problem dimension, and so obtain\nprecise analytic estimates on the objective function value and the Hessian\nspectrum. The results enable an analytic characterization of various\nobstructions to local optimization methods, revealing, in particular, a complex\narray of saddles and minima that differ in their symmetry, structure, and\nanalytic properties. A notable phenomenon, observed for all critical points\nconsidered, concerns the index of the Hessian increasing with the objective\nfunction value.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.07886", "cate": "math.OC", "date": "2023-06-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2310.00539", "title": "Thompson Exploration with Best Challenger Rule in Best Arm Identification", "authors": ["Jongyeong Lee", "Junya Honda", "Masashi Sugiyama"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.00539", "summary": "This paper studies the fixed-confidence best arm identification (BAI) problem\nin the bandit framework in the canonical single-parameter exponential models.\nFor this problem, many policies have been proposed, but most of them require\nsolving an optimization problem at every round and/or are forced to explore an\narm at least a certain number of times except those restricted to the Gaussian\nmodel. To address these limitations, we propose a novel policy that combines\nThompson sampling with a computationally efficient approach known as the best\nchallenger rule. While Thompson sampling was originally considered for\nmaximizing the cumulative reward, we demonstrate that it can be used to\nnaturally explore arms in BAI without forcing it. We show that our policy is\nasymptotically optimal for any two-armed bandit problems and achieves near\noptimality for general $K$-armed bandit problems for $K\\geq 3$. Nevertheless,\nin numerical experiments, our policy shows competitive performance compared to\nasymptotically optimal policies in terms of sample complexity while requiring\nless computation cost. In addition, we highlight the advantages of our policy\nby comparing it to the concept of $\\beta$-optimality, a relaxed notion of\nasymptotic optimality commonly considered in the analysis of a class of\npolicies including the proposed one.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.00539", "cate": "stat.ML", "date": "2023-10-01", "updated": "2025-08-06", "section": "repl"}
{"id": "2405.09004", "title": "Improving Sequential Market Coordination via Value-oriented Renewable Energy Forecasting", "authors": ["Yufan Zhang", "Honglin Wen", "Yuexin Bian", "Yuanyuan Shi"], "categories": ["cs.LG", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.09004", "summary": "Large penetration of renewable energy sources (RESs) brings huge uncertainty\ninto the electricity markets. The current deterministic clearing approach in\nthe day-ahead (DA) market, where RESs participate based on expected production,\nhas been criticized for causing a lack of coordination between the DA and\nreal-time (RT) markets, leading to high overall operating costs. Previous works\nindicate that improving day-ahead RES entering quantities can significantly\nmitigate the drawbacks of deterministic clearing. In this work, we propose\nusing a trained forecasting model, referred to as value-oriented forecasting,\nto determine RES Improved Entering Quantities (RIEQ) more efficiently during\nthe operational phase. Unlike traditional models that minimize statistical\nforecasting errors, our approach trains model parameters to minimize the\nexpected overall operating costs across both DA and RT markets. We derive the\nexact form of the loss function used for training, which becomes piecewise\nlinear when market clearing is modeled by linear programs. Additionally, we\nprovide the analytical gradient of the loss function with respect to the\nforecast, enabling an efficient training strategy. Numerical studies\ndemonstrate that our forecasts significantly reduce overall operating costs for\ndeterministic market clearing compared to conventional forecasts based on\nexpected RES production.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.09004", "cate": "eess.SY", "date": "2024-05-15", "updated": "2025-08-06", "section": "repl"}
{"id": "2405.17333", "title": "Generating Accurate Synthetic Survival Data by Conditioning on Outcomes", "authors": ["Mohd Ashhad", "Ricardo Henao"], "categories": ["cs.LG", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.17333", "summary": "Synthetically generated data can improve privacy, fairness, and data\naccessibility; however, it can be challenging in specialized scenarios such as\nsurvival analysis. One key challenge in this setting is censoring, i.e., the\ntiming of an event is unknown in some cases. Existing methods struggle to\naccurately reproduce the distributions of both observed and censored event\ntimes when generating synthetic data. We propose a conceptually simple approach\nthat generates covariates conditioned on event times and censoring indicators\nby leveraging existing tabular data generation models without making\nassumptions about the mechanism underlying censoring. Experiments on real-world\ndatasets demonstrate that our method consistently outperforms baselines and\nimproves downstream survival model performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.17333", "cate": "stat.ML", "date": "2024-05-27", "updated": "2025-08-05", "section": "repl"}
{"id": "2408.09936", "title": "Electron-nucleus cross sections from transfer learning", "authors": ["Krzysztof M. Graczyk", "Beata E. Kowal", "Artur M. Ankowski", "Rwik Dharmapal Banerjee", "Jose Luis Bonilla", "Hemant Prasad", "Jan T. Sobczyk"], "categories": ["cs.LG", "hep-ex", "hep-ph", "nucl-ex", "nucl-th"], "primary_category": "hep-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.09936", "summary": "Transfer learning (TL) allows a deep neural network (DNN) trained on one type\nof data to be adapted for new problems with limited information. We propose to\nuse the TL technique in physics. The DNN learns the details of one process, and\nafter fine-tuning, it makes predictions for related processes. We consider the\nDNNs, trained on inclusive electron-carbon scattering data, and show that after\nfine-tuning, they accurately predict cross sections for electron interactions\nwith nuclear targets ranging from helium-3 to iron.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.09936", "cate": "hep-ph", "date": "2024-08-19", "updated": "2025-08-06", "section": "repl"}
{"id": "2501.01414", "title": "Deep Discrete Encoders: Identifiable Deep Generative Models for Rich Data with Discrete Latent Layers", "authors": ["Seunghyun Lee", "Yuqi Gu"], "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.01414", "summary": "In the era of generative AI, deep generative models (DGMs) with latent\nrepresentations have gained tremendous popularity. Despite their impressive\nempirical performance, the statistical properties of these models remain\nunderexplored. DGMs are often overparametrized, non-identifiable, and\nuninterpretable black boxes, raising serious concerns when deploying them in\nhigh-stakes applications. Motivated by this, we propose interpretable deep\ngenerative models for rich data types with discrete latent layers, called Deep\nDiscrete Encoders (DDEs). A DDE is a directed graphical model with multiple\nbinary latent layers. Theoretically, we propose transparent identifiability\nconditions for DDEs, which imply progressively smaller sizes of the latent\nlayers as they go deeper. Identifiability ensures consistent parameter\nestimation and inspires an interpretable design of the deep architecture.\nComputationally, we propose a scalable estimation pipeline of a layerwise\nnonlinear spectral initialization followed by a penalized stochastic\napproximation EM algorithm. This procedure can efficiently estimate models with\nexponentially many latent components. Extensive simulation studies for\nhigh-dimensional data and deep architectures validate our theoretical results\nand demonstrate the excellent performance of our algorithms. We apply DDEs to\nthree diverse real datasets with different data types to perform hierarchical\ntopic modeling, image representation learning, and response time modeling in\neducational testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.01414", "cate": "stat.ML", "date": "2025-01-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.14571", "title": "Efficient Data Selection for Training Genomic Perturbation Models", "authors": ["George Panagopoulos", "Johannes F. Lutzeyer", "Sofiane Ennadir", "Jun Pang"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "q-bio.QM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.14571", "summary": "Genomic studies, including CRISPR-based Perturb-seq analyses, face a vast\nhypothesis space, while gene perturbations remain costly and time-consuming.\nGene perturbation models based on graph neural networks are trained to predict\nthe outcomes of gene perturbations to facilitate such experiments. Due to the\ncost of genomic experiments, active learning is often employed to train these\nmodels, alternating between wet-lab experiments and model updates. However, the\noperational constraints of the wet-lab and the iterative nature of active\nlearning significantly increase the total training time. Furthermore, the\ninherent sensitivity to model initialization can lead to markedly different\nsets of gene perturbations across runs, which undermines the reproducibility,\ninterpretability, and reusability of the method. To this end, we propose a\ngraph-based data filtering method that, unlike active learning, selects the\ngene perturbations in one shot and in a model-free manner. The method optimizes\na criterion that maximizes the supervision signal from the graph neural network\nto enhance generalization. The criterion is defined over the input graph and is\noptimized with submodular maximization. We compare it empirically to active\nlearning, and the results demonstrate that despite yielding months of\nacceleration, it also improves the stability of the selected perturbation\nexperiments while achieving comparable test error.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.14571", "cate": "q-bio.QM", "date": "2025-03-18", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.02019", "title": "Learning the Simplest Neural ODE", "authors": ["Yuji Okamoto", "Tomoya Takeuchi", "Yusuke Sakemi"], "categories": ["cs.LG", "math.DS", "stat.ML"], "primary_category": "stat.ML", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.02019", "summary": "Since the advent of the ``Neural Ordinary Differential Equation (Neural\nODE)'' paper, learning ODEs with deep learning has been applied to system\nidentification, time-series forecasting, and related areas. Exploiting the\ndiffeomorphic nature of ODE solution maps, neural ODEs has also enabled their\nuse in generative modeling. Despite the rich potential to incorporate various\nkinds of physical information, training Neural ODEs remains challenging in\npractice. This study demonstrates, through the simplest one-dimensional linear\nmodel, why training Neural ODEs is difficult. We then propose a new\nstabilization method and provide an analytical convergence analysis. The\ninsights and techniques presented here serve as a concise tutorial for\nresearchers beginning work on Neural ODEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.02019", "cate": "stat.ML", "date": "2025-05-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.21222", "title": "Benchmarking a Tunable Quantum Neural Network on Trapped-Ion and Superconducting Hardware", "authors": ["Djamil Lakhdar-Hamina", "Xingxin Liu", "Richard Barney", "Sarah H. Miller", "Alaina M. Green", "Norbert M. Linke", "Victor Galitski"], "categories": ["cond-mat.dis-nn", "cs.LG", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21222", "summary": "We implement a quantum generalization of a neural network on trapped-ion and\nIBM superconducting quantum computers to classify MNIST images, a common\nbenchmark in computer vision. The network feedforward involves qubit rotations\nwhose angles depend on the results of measurements in the previous layer. The\nnetwork is trained via simulation, but inference is performed experimentally on\nquantum hardware. The classical-to-quantum correspondence is controlled by an\ninterpolation parameter, $a$, which is zero in the classical limit. Increasing\n$a$ introduces quantum uncertainty into the measurements, which is shown to\nimprove network performance at moderate values of the interpolation parameter.\nWe then focus on particular images that fail to be classified by a classical\nneural network but are detected correctly in the quantum network. For such\nborderline cases, we observe strong deviations from the simulated behavior. We\nattribute this to physical noise, which causes the output to fluctuate between\nnearby minima of the classification energy landscape. Such strong sensitivity\nto physical noise is absent for clear images. We further benchmark physical\nnoise by inserting additional single-qubit and two-qubit gate pairs into the\nneural network circuits. Our work provides a springboard toward more complex\nquantum neural networks on current devices: while the approach is rooted in\nstandard classical machine learning, scaling up such networks may prove\nclassically non-simulable and could offer a route to near-term quantum\nadvantage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21222", "cate": "quant-ph", "date": "2025-07-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2506.01211", "title": "Iola Walker: A Mobile Footfall Detection System for Music Composition", "authors": ["William B James"], "categories": ["cs.MM", "eess.AS"], "primary_category": "cs.MM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.01211", "summary": "This paper is part of a larger music technology research project.\nhttp://willbjames.github.io The goal of this research is to find a method of\nmaterially enhancing music using hardware and software. Why might one want to\ndo this, you might ask? Because if it was possible to create a new form of\nmusic that was preferred by listeners, that would be a great way for musicians\nto reclaim live musical performance from the digital advertising industry. This\nproject is an initial iteration towards the broader research goal of promoting\nequitable human thriving in the music field. \\par The project is dubbed \"iola\nwalker\" in reference to a common polyrhythm, the hemiola. A listener goes for a\nwalk, and the Iola Walker app detects their walking pace. Iola Walker picks up\nfootfalls using a foot-mounted accelerometer, processing the signals in real\ntime using a recurrent neural network in an android app. The android app\noutputs a midi event for each footfall. The iola walker player plays the\nversion of the next music passage with underlying polyrhythms closest to the\nlistener's walking pace, as determined by the composer. \\par This paper\ndocuments the process of training the model to detect a walking listener's\nfootfalls in real time. The model is trained on accelerometer data from an\nMbient Labs foot-mounted IMU \\cite{mbientlabs} at 200~Hz, with the ground truth\nfor footfalls annotated by pressing the volume up button on the android device\nwhen the foot hits the ground. To collect training data, I walked around my\nneighborhood clicking the volume up button each time my foot hit the ground. I\ntried several methods for detecting footfalls in real time from sensor data,\nwith the most success from an LSTM. Artifacts for this paper are available\nhere: https://github.com/willbjames/iolawalker", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.01211", "cate": "cs.MM", "date": "2025-06-01", "updated": "2025-08-05", "section": "repl"}
{"id": "2506.10416", "title": "Can Sound Replace Vision in LLaVA With Token Substitution?", "authors": ["Ali Vosoughi", "Jing Bi", "Pinxin Liu", "Yunlong Tang", "Chenliang Xu"], "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.10416", "summary": "What happens when we push audio-visual alignment to its absolute limits? To\nsystematically investigate this question, we needed datasets with granular\nalignment quality annotations, but existing datasets treat alignment as binary,\neither synchronized or not. To address this limitation, we developed a\ncomprehensive dataset featuring detailed alignment scores that reveal the\nhidden spectrum of audio-visual perceptual correspondence. Using these precise\nscores, we create \"superaligned\" representations by training exclusively on the\nmost perfectly matched audio-visual pairs, then conduct our systematic\ninvestigation into how this extreme alignment transforms perceptual model\nbehavior across retrieval and generation tasks. The encoders under study fall\ninto two main groups consisting of image-centric encoders that were pretrained\nusing visual modalities as intermediary hubs for connecting modalities, and\ntext-centric encoders that were pretrained with direct audio-language\nalignment. We first measure the baseline performance of these encoders on two\nkey tasks, namely cross-modal retrieval and text description generation in\nvision-language models. Subsequently, we realign all encoders with the CLIP\nspace using highly coherent audio-visual data and observe the performance\nchanges. Our findings reveal that the initial architectural type of the encoder\ndetermines how it responds to the alignment process. Image-centric encoders,\nwhich are inherently designed for alignment, demonstrate exceptional\nperformance in cross-modal retrieval, but this intensive alignment causes\ncompression of unique linguistic information and reduces the quality of their\ntext description generation in vision-language models. In contrast,\ntext-centric encoders, which possess stronger linguistic authenticity, are able\nto maintain a better balance between the two objectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.10416", "cate": "cs.MM", "date": "2025-06-12", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03966", "title": "GP and LLMs for Program Synthesis: No Clear Winners", "authors": ["Jose Guadalupe Hernandez", "Anil Kumar Saini", "Gabriel Ketron", "Jason H. Moore"], "categories": ["cs.NE"], "primary_category": "cs.NE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03966v1", "summary": "Genetic programming (GP) and large language models (LLMs) differ in how\nprogram specifications are provided: GP uses input-output examples, and LLMs\nuse text descriptions. In this work, we compared the ability of PushGP and\nGPT-4o to synthesize computer programs for tasks from the PSB2 benchmark suite.\nWe used three prompt variants with GPT-4o: input-output examples (data-only),\ntextual description of the task (text-only), and a combination of both textual\ndescriptions and input-output examples (data-text). Additionally, we varied the\nnumber of input-output examples available for building programs. For each\nsynthesizer and task combination, we compared success rates across all program\nsynthesizers, as well as the similarity between successful GPT-4o synthesized\nprograms. We found that the combination of PushGP and GPT-4o with data-text\nprompting led to the greatest number of tasks solved (23 of the 25 tasks), even\nthough several tasks were solved exclusively by only one of the two\nsynthesizers. We also observed that PushGP and GPT-4o with data-only prompting\nsolved fewer tasks with the decrease in the training set size, while the\nremaining synthesizers saw no decrease. We also detected significant\ndifferences in similarity between the successful programs synthesized for\nGPT-4o with text-only and data-only prompting. With there being no dominant\nprogram synthesizer, this work highlights the importance of different\noptimization techniques used by PushGP and LLMs to synthesize programs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03966v1", "cate": "cs.NE", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04148", "title": "STARE: Predicting Decision Making Based on Spatio-Temporal Eye Movements", "authors": ["Moshe Unger", "Alexander Tuzhilin", "Michel Wedel"], "categories": ["cs.NE"], "primary_category": "cs.NE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04148v1", "summary": "The present work proposes a Deep Learning architecture for the prediction of\nvarious consumer choice behaviors from time series of raw gaze or eye fixations\non images of the decision environment, for which currently no foundational\nmodels are available. The architecture, called STARE (Spatio-Temporal Attention\nRepresentation for Eye Tracking), uses a new tokenization strategy, which\ninvolves mapping the x- and y- pixel coordinates of eye-movement time series on\npredefined, contiguous Regions of Interest. That tokenization makes the\nspatio-temporal eye-movement data available to the Chronos, a time-series\nfoundation model based on the T5 architecture, to which co-attention and/or\ncross-attention is added to capture directional and/or interocular influences\nof eye movements. We compare STARE with several state-of-the art alternatives\non multiple datasets with the purpose of predicting consumer choice behaviors\nfrom eye movements. We thus make a first step towards developing and testing DL\narchitectures that represent visual attention dynamics rooted in the\nneurophysiology of eye movements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04148v1", "cate": "cs.NE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04009", "title": "Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)", "authors": ["Vu Ngoc Son", "Pham Van Cuong", "Dao Thi My Linh", "Le Tieu Nien"], "categories": ["cs.NE", "cs.RO", "eess.SY"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04009", "summary": "This paper presents a method for optimizing the sliding mode control (SMC)\nparameter for a robot manipulator applying a genetic algorithm (GA). The\nobjective of the SMC is to achieve precise and consistent tracking of the\ntrajectory of the robot manipulator under uncertain and disturbed conditions.\nHowever, the system effectiveness and robustness depend on the choice of the\nSMC parameters, which is a difficult and crucial task. To solve this problem, a\ngenetic algorithm is used to locate the optimal values of these parameters that\ngratify the capability criteria. The proposed method is efficient compared with\nthe conventional SMC and Fuzzy-SMC. The simulation results show that the\ngenetic algorithm with SMC can achieve better tracking capability and reduce\nthe chattering effect.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04009", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04167", "title": "The Glider Equation for Asymptotic Lenia", "authors": ["Hiroki Kojima", "Ivan Yevenko", "Takashi Ikegami"], "categories": ["cs.NE", "nlin.CG", "nlin.PS"], "primary_category": "nlin.CG", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04167", "summary": "Lenia is a continuous extension of Conway's Game of Life that exhibits rich\npattern formations including self-propelling structures called gliders. In this\npaper, we focus on Asymptotic Lenia, a variant formulated as partial\ndifferential equations. By utilizing this mathematical formulation, we\nanalytically derive the conditions for glider patterns, which we term the\n``Glider Equation.'' We demonstrate that by using this equation as a loss\nfunction, gradient descent methods can successfully discover stable glider\nconfigurations. This approach enables the optimization of update rules to find\nnovel gliders with specific properties, such as faster-moving variants. We also\nderive a velocity-free equation that characterizes gliders of any speed,\nexpanding the search space for novel patterns. While many optimized patterns\nresult in transient gliders that eventually destabilize, our approach\neffectively identifies diverse pattern formations that would be difficult to\ndiscover through traditional methods. Finally, we establish connections between\nAsymptotic Lenia and neural field models, highlighting mathematical\nrelationships that bridge these systems and suggesting new directions for\nanalyzing pattern formation in continuous dynamical systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04167", "cate": "nlin.CG", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04459", "title": "Case Studies of Generative Machine Learning Models for Dynamical Systems", "authors": ["Nachiket U. Bapat", "Randy C. Paffenroth", "Raghvendra V. Cowlagi"], "categories": ["cs.NE", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04459", "summary": "Systems like aircraft and spacecraft are expensive to operate in the real\nworld. The design, validation, and testing for such systems therefore relies on\na combination of mathematical modeling, abundant numerical simulations, and a\nrelatively small set of real-world experiments. Due to modeling errors,\nsimplifications, and uncertainties, the data synthesized by simulation models\noften does not match data from the system's real-world operation. We consider\nthe broad research question of whether this model mismatch can be significantly\nreduced by generative artificial intelligence models (GAIMs). Unlike text- or\nimage-processing, where generative models have attained recent successes, GAIM\ndevelopment for aerospace engineering applications must not only train with\nscarce operational data, but their outputs must also satisfy governing\nequations based on natural laws, e.g., conservation laws. The scope of this\npaper primarily focuses on two case studies of optimally controlled systems\nthat are commonly understood and employed in aircraft guidance, namely:\nminimum-time navigation in a wind field and minimum-exposure navigation in a\nthreat field. We report GAIMs that are trained with a relatively small set, of\nthe order of a few hundred, of examples and with underlying governing\nequations. By focusing on optimally controlled systems, we formulate training\nloss functions based on invariance of the Hamiltonian function along system\ntrajectories. We investigate three GAIM architectures, namely: the generative\nadversarial network (GAN) and two variants of the variational autoencoder\n(VAE). We provide architectural details and thorough performance analyses of\nthese models. The main finding is that our new models, especially the VAE-based\nmodels, are able to synthesize data that satisfy the governing equations and\nare statistically similar to the training data despite small volumes of\ntraining data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04459", "cate": "eess.SY", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.03992", "title": "Unconditional energy dissipation of Strang splitting for the matrix-valued Allen-Cahn equation", "authors": ["Chaoyu Quan", "Tao Tang", "Dong Wang"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03992v1", "summary": "The energy dissipation property of the Strang splitting method was first\ndemonstrated for the matrix-valued Allen-Cahn (MAC) equation under restrictive\ntime-step constraints [J. Comput. Phys. 454, 110985, 2022]. In this work, we\neliminate this limitation through a refined stability analysis framework,\nrigorously proving that the Strang splitting method preserves the energy\ndissipation law unconditionally for arbitrary time steps. The refined proof\nhinges on a precise estimation of the double-well potential term in the\nmodified energy functional. Leveraging this unconditional energy dissipation\nproperty, we rigorously establish that the Strang splitting method achieves\nglobal-in-time $H^1$-stability, preserves determinant boundedness, and\nmaintains second-order temporal convergence for the matrix-valued Allen-Cahn\nequation. To validate these theoretical findings, we conduct numerical\nexperiments confirming the method's energy stability and determinant bound\npreservation for the MAC equation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03992v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04082", "title": "POD-based reduced order modeling of global-in-time iterative decoupled algorithms for Biot's consolidation model", "authors": ["Huipeng Gu", "Francesco Ballarin", "Mingchao Cai", "Jingzhi Li"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04082v1", "summary": "This paper focuses on the efficient numerical algorithms of a three-field\nBiot's consolidation model. The approach begins with the introduction of\ninnovative monolithic and global-in-time iterative decoupled algorithms, which\nincorporate the backward differentiation formulas for time discretization. In\neach iteration, these algorithms involve solving a diffusion subproblem over\nthe entire temporal domain, followed by solving a generalized Stokes subproblem\nover the same time interval. To accelerate the global-in-time iterative\nprocess, we present a reduced order modeling approach based on proper\northogonal decomposition, aimed at reducing the primary computational cost from\nthe generalized Stokes subproblem. The effectiveness of this novel method is\nvalidated both theoretically and through numerical experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04082v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04112", "title": "Convergence of hyperbolic approximations to higher-order PDEs for smooth solutions", "authors": ["Jan Giesselmann", "Hendrik Ranocha"], "categories": ["math.AP", "math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04112v1", "summary": "We prove the convergence of hyperbolic approximations for several classes of\nhigher-order PDEs, including the Benjamin-Bona-Mahony, Korteweg-de Vries,\nGardner, Kawahara, and Kuramoto-Sivashinsky equations, provided a smooth\nsolution of the limiting problem exists. We only require weak (entropy)\nsolutions of the hyperbolic approximations. Thereby, we provide a solid\nfoundation for these approximations, which have been used in the literature\nwithout rigorous convergence analysis. We also present numerical results that\nsupport our theoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04112v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04198", "title": "Optimal Design of Broadband Absorbers with Multiple Plasmonic Nanoparticles via Reduced Basis Method", "authors": ["Yu Gao", "Hai Zhang", "Kai Zhang"], "categories": ["math.NA", "math.OC"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04198v1", "summary": "In this paper, we propose a computational framework for the optimal design of\nbroadband absorbing materials composed of plasmonic nanoparticle arrays. This\ndesign problem poses several key challenges: (1) the complex multi-particle\ninteractions and high-curvature geometries; (2) the requirement to achieve\nbroadband frequency responses, including resonant regimes; (3) the complexity\nof shape derivative calculations; and (4) the non-convexity of the optimization\nlandscape. To systematically address these challenges, we employ three\nsequential strategies. First, we introduce a parameterized integral equation\nformulation that circumvents traditional shape derivative computations. Second,\nwe develop a shape-adaptive reduced basis method (RBM) that utilizes the\neigenfunctions of the Neumann-Poincar\\'{e} operator for forward problems and\ntheir adjoint counterparts for adjoint problems, thereby addressing\nsingularities and accelerating computations. Third, we propose a\nphysics-informed initialization strategy that estimates nanoparticle\nconfigurations under weak coupling assumptions, thereby improving the\nperformance of gradient-based optimization algorithms. The method's\ncomputational advantages are demonstrated through numerical experiments, which\nshow accurate and efficient designs across various geometric configurations.\nFurthermore, the framework is flexible and extensible to other material systems\nand boundary conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04198v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04356", "title": "Monolithic Multi-level Overlapping Schwarz Solvers for Fluid Problems", "authors": ["Stephan KÃ¶hler", "Oliver Rheinbah"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04356v1", "summary": "Additive overlapping Schwarz Methods are iterative methods of the domain\ndecomposition type for the solution of partial differential equations.\nNumerical and parallel scalability of these methods can be achieved by adding\ncoarse levels. A successful coarse space, inspired by iterative substructuring,\nis the generalized Dryja-Smith-Widlund (GDSW) space. In\nhttps://doi.org/10.1137/18M1184047, based on the GDSW approach, two-level\nmonolithic overlapping Schwarz preconditioners for saddle point problems were\nintroduced. We present parallel results up to 32768 MPI ranks for the solution\nof incompressible fluid problems for a Poiseuille flow example on the unit cube\nand a complex extrusion die geometry using a two- and a three-level monolithic\noverlapping Schwarz preconditioner. These results are achieved through the\ncombination of the additive overlapping Schwarz solvers implemented in the Fast\nand Robust Overlapping Schwarz (FROSch) library\nhttps://doi.org/10.1007/978-3-030-56750-7_19, which is part of the Trilinos\npackage ShyLU https://doi.org/10.1109/IPDPS.2012.64, and the FEATFLOW library\nhttp://www.featflow.de using a scalable interface for the efficient coupling of\nthe two libraries. This work is part of the project StroemungsRaum - Novel\nExascale-Architectures with Heterogeneous Hardware Components for Computational\nFluid Dynamics Simulations, funded by the German Bundesministerium fur\nForschung, Technologie und Raumfahrt BMFTR (formerly BMBF) as part of the\nprogram on New Methods and Technologies for Exascale Computing (SCALEXA).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04356v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04360", "title": "Derivation and Numerical Simulation of a Thermodynamically Consistent Magneto Two-Phase Flow Model for Magnetic Drug Targeting", "authors": ["Eberhard BÃ¤nsch", "Jonas Knoch", "Nicolas Neuss", "Maria Neuss-Radu"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04360v1", "summary": "In this paper, we derive a novel and comprehensive thermodynamically\nconsistent model for the complex interactions between superparamagnetic iron\noxide nanoparticles (SPIONs), a carrier fluid, and a magnetic field, as they\noccur in Magnetic Drug Targeting (MDT), the targeted delivery of magnetically\nfunctionalized drug carriers by external magnetic fields. It consists of a\nconvection-diffusion equation for SPIONs, a modified Navier-Stokes system for\nthe averaged velocity of the carrier fluid-nanoparticle mixture and a\nquasi-stationary Maxwell system for the magnetic variables. The derived model\nextends previous models for MDT by taking into account the response of the\ncarrier fluid and of the magnetic field to the dynamics of the SPIONs, and thus\nprovides a comprehensive tool for the prediction and optimization of MDT\nprocesses. After introducing a semi-implicit finite element scheme for the\nnumerical simulation of the model, simulation results for the fully coupled\nmodel are performed and compared with results from a reduced version of the\nmodel, where the response of the carrier flow and of the magnetic field to the\nSPION dynamics is neglected. Furthermore, the sensitivity of MDT with respect\nto experimental parameters, such as magnet positioning, is investigated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04360v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04392", "title": "Explicit Construction of Approximate Kolmogorov-Arnold Superpositions with C2-Smoothness", "authors": ["Lunji Song", "Juan Diego Toscano", "Li-Lian Wang"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04392v1", "summary": "We explicitly construct an approximate version of the Kolmogorov-Arnold\nsuperpositions, which is composed of C2 inner and outer functions, and can\napproximate an arbitrary alpha-Holder continuous function well. The inner\nfunctions are generated by applying suitable translations and dilations to a\npiecewise C2, strictly increasing function, while the outer functions are\nconstructed row-wise through piecewise C2 interpolation using newly designed\nshape functions. This novel variant of Kolmogorov-Arnold superpositions\novercomes the wild and pathological behaviors of the inherent single variable\nfunctions, but retains the essence of Kolmogorov strategy of exact\nrepresentation, an objective that Sprecher, Neural Networks 144, 2021, has\nactively pursued.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04392v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04484", "title": "A high-order deterministic dynamical low-rank method for proton transport in heterogeneous media", "authors": ["Pia Stammer", "Niklas Wahl", "Jonas Kusch", "Danny Lathouwers"], "categories": ["math.NA", "physics.comp-ph", "physics.med-ph"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04484v1", "summary": "Dose calculations in proton therapy require the fast and accurate solution of\na high-dimensional transport equation for a large number of (pencil) beams with\ndifferent energies and directions. Deterministically solving this transport\nproblem at a sufficient resolution can however be prohibitively expensive,\nespecially due to highly forward peaked scattering of the protons. We propose\nusing a model order reduction approach, the dynamical low-rank approximation\n(DLRA), which evolves the solution on the manifold of low-rank matrices in\n(pseudo-)time. For this, we compare a collided-uncollided split of the linear\nBoltzmann equation and its Fokker-Planck approximation. We treat the uncollided\npart using a ray-tracer and combine high-order phase space discretizations and\na mixture model for materials with DLRA for the collided equation. Our method\nreproduces the results of a full-rank reference code at significantly lower\nrank, and thus computational cost and memory, and further makes computations\nfeasible at much higher resolutions. At higher resolutions, we also achieve\ngood accuracy with respect to TOPAS MC in homogeneous as well as heterogeneous\nmaterials. Finally, we demonstrate that several beam sources with different\nangles can be computed with little cost increase compared to individual beams.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04484v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04560", "title": "Discretizing linearized Einstein-Bianchi system by symmetric and traceless tensors", "authors": ["Yuyang Guo", "Jun Hu", "Ting Lin"], "categories": ["gr-qc", "math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04560v1", "summary": "The Einstein-Bianchi system uses symmetric and traceless tensors to\nreformulate Einstein's original field equations. However, preserving these\nalgebraic constraints simultaneously remains a challenge for numerical methods.\nThis paper proposes a new formulation that treats the linearized\nEinstein-Bianchi system (near the trivial Minkowski metric) as the Hodge wave\nequation associated with the conformal Hessian complex. To discretize this\nequation, a conforming finite element conformal Hessian complex that preserves\nsymmetry and traceless-ness simultaneously is constructed on general\nthree-dimensional tetrahedral grids, and its exactness is proven.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04560v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04582", "title": "$h$-Trigonometric B-splines", "authors": ["Fatma ZÃ¼rnacÄ±-YetiÅ", "Ron Goldman", "Plamen Simeonov"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04582v1", "summary": "We introduce discrete analogues of the exponential, sine, and cosine\nfunctions. Then using a discrete trigonometric version of a non-polynomial\ndivided difference, we define discrete analogues of the trigonometric\nB-splines. We derive a two-term recurrence relation, a two-term formula for the\ndiscrete derivative, and two variants of the Marsden identity for these\ndiscrete trigonometric B-splines. Since the classical exponential, sine, and\ncosine functions are limiting cases of their discrete analogues, we conclude\nthat many of the standard results for classical polynomial B-splines extend\nnaturally both to trigonometric B-splines and to discrete trigonometric\nB-splines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04582v1", "cate": "math.NA", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04020", "title": "Micro-macro and macro-macro limits for controlled leader-follower systems", "authors": ["Giacomo Albi", "Young-Pil Choi", "Matteo Piu", "Sihyun Song"], "categories": ["math.AP", "math.NA", "math.OC"], "primary_category": "math.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04020", "summary": "We study a leader-follower system of interacting particles subject to\nfeedback control and derive its mean-field limits through a two-step passage:\nfirst to a micro-macro system coupling leader particles with a follower fluid,\nand then to a fully continuum macro-macro system. For each limiting procedure,\nwe establish quantitative stability and convergence estimates based on\nmodulated energy methods and Wasserstein distances. These results provide a\nrigorous foundation for the hierarchical reduction of controlled multi-agent\nsystems. Numerical simulations are presented, including examples with\ninteraction potentials beyond the analytical class considered, to demonstrate\nthe dynamics and support the theoretical results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04020", "cate": "math.AP", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2409.18490", "title": "Spectral Galerkin method for the zero dispersion limit of the fractional Korteweg-de Vries equation", "authors": ["Mukul Dwivedi", "Tanmay Sarkar"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.18490", "summary": "We present a fully discrete Crank-Nicolson Fourier-spectral-Galerkin (FSG)\nscheme for approximating solutions of the fractional Korteweg-de Vries (KdV)\nequation, which involves a fractional Laplacian with exponent $\\alpha \\in\n[1,2]$ and a small dispersion coefficient of order $\\varepsilon^2$. The\nsolution in the limit as $\\varepsilon \\to 0$ is known as the zero dispersion\nlimit. We demonstrate that the semi-discrete FSG scheme conserves the first\nthree integral invariants, thereby structure preserving, and that the fully\ndiscrete FSG scheme is $L^2$-conservative, ensuring stability. Using a\ncompactness argument, we constructively prove the convergence of the\napproximate solution to the unique solution of the fractional KdV equation in\n$C([0,T]; H_p^{1+\\alpha}(\\mathbb{R}))$ for the periodic initial data in\n$H_p^{1+\\alpha}(\\mathbb{R})$. The devised scheme achieves spectral accuracy for\nthe initial data in $H_p^r,$ $r \\geq 1+\\alpha$ and exponential accuracy for the\nanalytic initial data. Additionally, we establish that the approximation of the\nzero dispersion limit obtained from the fully discrete FSG scheme converges to\nthe solution of the Hopf equation in $L^2$ as $\\varepsilon \\to 0$, up to the\ngradient catastrophe time $t_c$. Beyond $t_c$, numerical investigations reveal\nthat the approximation converges to the asymptotic solution, which is weakly\ndescribed by the Whitham's averaged equation within the oscillatory zone for\n$\\alpha = 2$. Numerical results are provided to demonstrate the convergence of\nthe scheme and to validate the theoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.18490", "cate": "math.NA", "date": "2024-09-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2412.16902", "title": "Optimal error bounds on an exponential wave integrator Fourier spectral method for the logarithmic SchrÃ¶dinger equation", "authors": ["Weizhu Bao", "Ying Ma", "Chushan Wang"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.16902", "summary": "We prove a nearly optimal error bound on the exponential wave integrator\nFourier spectral (EWI-FS) method for the logarithmic Schr\\\"odinger equation\n(LogSE) under the assumption of $H^2$-solution, which is theoretically\nguaranteed. Subject to a CFL-type time step size restriction $\\tau |\\ln \\tau|\n\\leq h^2/|\\ln h|$ for obtaining the stability of the numerical scheme affected\nby the singularity of the logarithmic nonlinearity, an $L^2$-norm error bound\nof order $O(\\tau |\\ln \\tau|^2 + h^2 |\\ln h|)$ is established, where $\\tau$ is\nthe time step size and $h$ is the mesh size. Compared to the error estimates of\nthe LogSE in the literature, our error bound either greatly improves the\nconvergence rate under the same regularity assumptions or significantly weakens\nthe regularity requirement to obtain the same convergence rate. Moreover, our\nresult can be directly applied to the LogSE with low regularity\n$L^\\infty$-potential, which is not allowed in the existing error estimates. Two\nmain ingredients are adopted in the proof: (i) an $H^2$-conditional\n$L^2$-stability estimate, which is established using the energy method to avoid\nsingularity of the logarithmic nonlinearity, and (ii) mathematical induction\nwith inverse inequalities to control the $H^2$-norm of the numerical solution.\nNumerical results are reported to confirm our error estimates and demonstrate\nthe necessity of the time step size restriction imposed. We also apply the\nEWI-FS method to investigate soliton collisions in one dimension and vortex\ndipole dynamics in two dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.16902", "cate": "math.NA", "date": "2024-12-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.19483", "title": "Empirical Hyper Element Integration Method (EHEIM) with Unified Integration Criteria for Efficient Hyper Reduced FE$^2$ Simulations", "authors": ["Nils Lange", "Geralf HÃ¼tter", "Bjoern Kiefer"], "categories": ["math.NA", "physics.comp-ph"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.19483", "summary": "Numerical homogenization for mechanical multiscale modeling by means of the\nfinite element method (FEM) is an elegant way of obtaining structure-property\nrelations, if the behavior of the constituents of the lower scale is well\nunderstood. However, the computational costs of this so-called FE$^2$ method\nare so high that reduction methods are essential. While the construction of a\nreduced basis for the microscopic nodal displacements using proper orthogonal\ndecomposition (POD) has become a standard technique, the reduction of the\ncomputational effort for the projected nodal forces, the so-called hyper\nreduction, is an additional challenge, for which different strategies have been\nproposed in the literature. The empirical cubature method (ECM), which has been\nproven to be very robust, implemented the conservation of the total volume is\nused as a constraint in the resulting optimization problem, while energy-based\ncriteria have been proposed in other contributions.\n  The present contribution presents a unified integration criteria concept,\ninvolving the aforementioned criteria, among others. These criteria are used\nboth with a Gauss point-based as well as with an element-based hyper reduction\nscheme, the latter retaining full compatibility with the common modular finite\nelement framework. The methods are combined with a previously proposed\nclustered training strategy and a monolithic solver. Numerical examples\nempirically demonstrate that the additional criteria improve the accuracy for a\ngiven number of modes. Vice verse, less modes and thus lower computational\ncosts are required to reach a given level of accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.19483", "cate": "math.NA", "date": "2025-03-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.11926", "title": "Convergence of finite elements for the Eyles-King-Styles model of tumour growth", "authors": ["Yifei Li"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.11926", "summary": "This paper presents a convergence analysis of the evolving surface finite\nelement method (ESFEM) applied to the original Eyles-King-Styles model of\ntumour growth. The model consists of a Poisson equation in the bulk, a forced\nmean curvature flow on the surface, and a coupled velocity law between bulk and\nsurface. Due to the non-trivial bulk-surface coupling, all previous analyses\nrequired an additional regularization term. By introducing a $H^{1/2}(\\Gamma)$\nenergy estimates theory, we develop an essentially new theoretical framework\nthat addresses the intrinsic bulk-surface coupling. Based on this framework, we\nprovide the first rigorous convergence proof for the original model without\nregularization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.11926", "cate": "math.NA", "date": "2025-04-16", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.13809", "title": "A Fast Direct Solver for Boundary Integral Equations Using Quadrature By Expansion", "authors": ["Alexandru Fikl", "Andreas KlÃ¶ckner"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13809", "summary": "We construct and analyze a hierarchical direct solver for linear systems\narising from the discretization of boundary integral equations using the\nQuadrature by Expansion (QBX) method. Our scheme builds on the existing theory\nof Hierarchical Semi-Separable (HSS) matrix operators that contain low-rank\noff-diagonal submatrices. We use proxy-based approximations of the far-field\ninteractions and the Interpolative Decomposition (ID) to construct compressed\nHSS operators that are used as fast direct solvers for the original system. We\ndescribe a number of modifications to the standard HSS framework that enable\ncompatibility with the QBX family of discretization methods. We establish an\nerror model for the direct solver that is based on a multipole expansion of the\nQBX-mediated proxy interactions and standard estimates for the ID. Based on\nthese theoretical results, we develop an automatic approach for setting scheme\nparameters based on user-provided error tolerances. The resulting solver\nseamlessly generalizes across two- and tree-dimensional problems and achieves\nstate-of-the-art asymptotic scaling. We conclude with numerical experiments\nthat support the theoretical expectations for the error and computational cost\nof the direct solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13809", "cate": "math.NA", "date": "2025-04-18", "updated": "2025-08-05", "section": "repl"}
{"id": "2505.11762", "title": "A parameterized Wasserstein Hamiltonian flow approach for solving the SchrÃ¶dinger equation", "authors": ["Hao Wu", "Shu Liu", "Xiaojing Ye", "Haomin Zhou"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11762", "summary": "In this paper, we propose a new method to compute the solution of\ntime-dependent Schr\\\"odinger equation (TDSE). Using push-forward maps and\nWasserstein Hamiltonian flow, we reformulate the TDSE as a Hamiltonian system\nin terms of push-forward maps. The new formulation can be viewed as a\ngenerative model in the Wasserstein space, which is a manifold of probability\ndensity functions. Then we parameterize the push-forward maps by reduce-order\nmodels such as neural networks. This induces a new metric in the parameter\nspace by pulling back the Wasserstein metric on density manifold, which further\nresults in a system of ordinary differential equations (ODEs) for the\nparameters of the reduce-order model. Leveraging the computational techniques\nfrom deep learning, such as Neural ODE, we design an algorithm to solve the\nTDSE in the parameterized push-forward map space, which provides an alternative\napproach with the potential to scale up to high-dimensional problems. Several\nnumerical examples are presented to demonstrate the performance of this\nalgorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11762", "cate": "math.NA", "date": "2025-05-17", "updated": "2025-08-05", "section": "repl"}
{"id": "2508.01238", "title": "Finite element conformal complexes in three dimensions", "authors": ["Xuehai Huang"], "categories": ["math.NA"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.01238", "summary": "This paper extends the Bernstein-Gelfand-Gelfand (BGG) framework to the\nconstruction of finite element conformal Hessian complexes and conformal\nelasticity complexes in three dimensions involving conformal tensors (i.e.,\nsymmetric and traceless tensors). These complexes incorporate higher-order\ndifferential operators, including the linearized Cotton-York operator, and\nrequire conformal tensor spaces with nontrivial smoothness and trace\nconditions. A novel application of the discrete BGG framework, combined with\nthe geometric decomposition of bubble spaces and a reduction operation, to\nlocal bubble finite element complexes is introduced. This yields simpler and\nmore tractable constructions than global BGG-based approaches, and leads to the\nbubble conformal complexes. Building on these bubble conformal complexes and\nthe associated face bubble complexes, finite element conformal Hessian\ncomplexes and conformal elasticity complexes with varying degrees of smoothness\nare systematically developed. The resulting complexes support stable and\nstructure-preserving numerical methods for applications in relativity, Cosserat\nelasticity, and fluid mechanics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.01238", "cate": "math.NA", "date": "2025-08-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02707", "title": "Diffusive behavior of transport noise on $\\mathbb{S}^2$", "authors": ["Sagy Ephrati", "Erik Jansson", "Andrea Papini"], "categories": ["math.NA", "math.PR", "physics.flu-dyn"], "primary_category": "math.NA", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02707", "summary": "We investigate theoretically and numerically transport noise-induced\ndiffusion in flows on the sphere. Previous analysis on the torus demonstrated\nthat suitably chosen transport noise in the Euler equations leads to diffusive\nbehavior resembling the Navier--Stokes equations. Here, we analyze dynamics on\nthe sphere with noise-induced differential elliptic operator dissipation and\ncharacterize their energy and enstrophy decay properties. Through\nstructure-preserving numerical simulations with the Zeitlin discretization, we\ndemonstrate that appropriately scaled transport noise induces energy\ndissipation while preserving enstrophy and coadjoint orbits. The presented\nanalysis lays a groundwork for further theoretical investigation of transport\nnoise and supports the calibration of transport noise models as a\nparametrization for unresolved processes in geophysical fluid simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02707", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2410.10040", "title": "Drift-diffusion equations with saturation", "authors": ["JosÃ© Antonio Carrillo", "Alejandro FernÃ¡ndez-JimÃ©nez", "David GÃ³mez-Castro"], "categories": ["math.AP", "math.NA"], "primary_category": "math.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.10040", "summary": "We focus on a family of nonlinear continuity equations for the evolution of a\nnon-negative density $\\rho$ with a continuous and compactly supported nonlinear\nmobility $\\mathrm{m}(\\rho)$ not necessarily concave. The velocity field is the\nnegative gradient of the variation of a free energy including internal and\nconfinement energy terms. Problems with compactly supported mobility are often\ncalled saturation problems since the values of the density are constrained\nbelow a maximal value. Taking advantage of a family of approximating problems,\nwe show the existence of $C_0$-semigroups of $L^1$ contractions. We study the\n$\\omega$-limit of the problem, its most relevant properties, and the appearance\nof free boundaries in the long-time behaviour. This problem has a formal\ngradient-flow structure, and we discuss the local/global minimisers of the\ncorresponding free energy in the natural topology related to the set of initial\ndata for the $L^\\infty$-constrained gradient flow of probability densities.\nFurthermore, we analyse a structure preserving implicit finite-volume scheme\nand discuss its convergence and long-time behaviour.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.10040", "cate": "math.AP", "date": "2024-10-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.02928", "title": "A nonstandard finite difference scheme for an SEIQR epidemiological PDE model", "authors": ["Achraf Zinihi", "Matthias Ehrhardt", "Moulay Rchid Sidi Ammi"], "categories": ["math.DS", "math.NA", "q-bio.QM"], "primary_category": "q-bio.QM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.02928", "summary": "This paper introduces a nonstandard finite difference (NSFD) approach to a\nreaction-diffusion SEIQR epidemiological model, which captures the\nspatiotemporal dynamics of infectious disease transmission. Formulated as a\nsystem of semilinear parabolic partial differential equations (PDEs), the model\nextends classical compartmental models by incorporating spatial diffusion to\naccount for population movement and spatial heterogeneity. The proposed NSFD\ndiscretization is designed to preserve the continuous model's essential\nqualitative features, such as positivity, boundedness, and stability, which are\noften compromised by standard finite difference methods. We rigorously analyze\nthe model's well-posedness, construct a structure-preserving NSFD scheme for\nthe PDE system, and study its convergence and local truncation error. Numerical\nsimulations validate the theoretical findings and demonstrate the scheme's\neffectiveness in preserving biologically consistent dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.02928", "cate": "q-bio.QM", "date": "2025-08-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03862", "title": "CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors", "authors": ["Abdul Saboor", "Zhuangzhuang Cui", "Achiel Colpaert", "Evgenii Vinogradov", "Sofie Pollin"], "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03862v1", "summary": "Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial\nVehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,\nsuch as air taxis. A key challenge in these high-mobility aerial corridors is\nensuring reliable connectivity, where frequent handovers can degrade network\nperformance. To resolve this, we present a Context-Aware Smart Handover (CASH)\nprotocol that uses a forward-looking scoring mechanism based on UAV trajectory\nto make proactive handover decisions. We evaluate the performance of the\nproposed CASH against existing handover protocols in a custom-built simulator.\nResults show that CASH reduces handover frequency by up to 78% while\nmaintaining low outage probability. We then investigate the impact of base\nstation density and safety margin on handover performance, where their optimal\nsetups are empirically obtained to ensure reliable UAM communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03862v1", "cate": "cs.NI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03891", "title": "Confidence Driven Classification of Application Types in the Presence of Background Network", "authors": ["Eun Hun Choi", "Jasleen Kaur", "Vladas Pipiras", "Nelson Gomes Rodrigues Antunes", "Brendan Massey"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03891v1", "summary": "Accurately classifying the application types of network traffic using deep\nlearning models has recently gained popularity. However, we find that these\nclassifiers do not perform well on real-world traffic data due to the presence\nof non-application-specific generic background traffic originating from\nadvertisements, analytics, shared APIs, and trackers. Unfortunately,\nstate-of-the-art application classifiers overlook such traffic in curated\ndatasets and only classify relevant application traffic. To address this issue,\nwhen we label and train using an additional class for background traffic, it\nleads to additional confusion between application and background traffic, as\nthe latter is heterogeneous and encompasses all traffic that is not relevant to\nthe application sessions. To avoid falsely classifying background traffic as\none of the relevant application types, a reliable confidence measure is\nwarranted, such that we can refrain from classifying uncertain samples.\nTherefore, we design a Gaussian Mixture Model-based classification framework\nthat improves the indication of the deep learning classifier's confidence to\nallow more reliable classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03891v1", "cate": "cs.NI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04004", "title": "Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3", "authors": ["Tanguy Ropitault", "Matteo Bordin", "Paolo Testolina", "Michele Polese", "Pedram Johari", "Nada Golmie", "Tommaso Melodia"], "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04004v1", "summary": "Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is\nchallenging because the performance emerges from the tight coupling of\npropagation, beam management, scheduling, and higher-layer interactions.\nSystem-level simulation is therefore indispensable, yet the vast majority of\nstudies rely on the statistical 3GPP channel models. These are well suited to\ncapture average behavior across many statistical realizations, but cannot\nreproduce site-specific phenomena such as corner diffraction, street-canyon\nblockage, or deterministic line-of-sight conditions and\nangle-of-departure/arrival relationships that drive directional links. This\npaper extends 5G-LENA, an NR module for the system-level Network Simulator 3\n(ns-3), with a trace-based channel model that processes the Multipath\nComponents (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer\n(RT)) or measurement campaigns. Our module constructs frequency-domain channel\nmatrices and feeds them to the existing Physical (PHY)/Medium Access Control\n(MAC) stack without any further modifications. The result is a geometry-based\nchannel model that remains fully compatible with the standard 3GPP\nimplementation in 5G-LENA, while delivering site-specific geometric fidelity.\nThis new module provides a key building block toward Digital Twin (DT)\ncapabilities by offering realistic site-specific channel modeling, unlocking\nstudies that require site awareness, including beam management, blockage\nmitigation, and environment-aware sensing. We demonstrate its capabilities for\nprecise beam-steering validation and end-to-end metric analysis. In both cases,\nthe trace-driven engine exposes performance inflections that the statistical\nmodel does not exhibit, confirming its value for high-fidelity system-level\ncellular networks research and as a step toward DT applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04004v1", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04015", "title": "A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks", "authors": ["Haoxiang Luo", "Kun Yang", "Qi Huang", "Schahram Dustdar"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04015v1", "summary": "The proliferation of large-scale artificial intelligence and data-intensive\napplications has spurred the development of Computing Power Networks (CPNs),\nwhich promise to deliver ubiquitous and on-demand computational resources.\nHowever, the immense energy consumption of these networks poses a significant\nsustainability challenge. Simultaneously, power grids are grappling with the\ninstability introduced by the high penetration of intermittent renewable energy\nsources (RES). This paper addresses these dual challenges through a novel\nTwo-Stage Co-Optimization (TSCO) framework that synergistically manages power\nsystem dispatch and CPN task scheduling to achieve low-carbon operations. The\nframework decomposes the complex, large-scale problem into a day-ahead\nstochastic unit commitment (SUC) stage and a real-time operational stage. The\nformer is solved using Benders decomposition for computational tractability,\nwhile in the latter, economic dispatch of generation assets is coupled with an\nadaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)\nagent. This agent makes intelligent, carbon-aware decisions by responding to\ndynamic grid conditions, including real-time electricity prices and marginal\ncarbon intensity. Through extensive simulations on an IEEE 30-bus system\nintegrated with a CPN, the TSCO framework is shown to significantly outperform\nbaseline approaches. Results demonstrate that the proposed framework reduces\ntotal carbon emissions and operational costs, while simultaneously decreasing\nRES curtailment by more than 60% and maintaining stringent Quality of Service\n(QoS) for computational tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04015v1", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04150", "title": "Metaverse Framework for Wireless Systems Management", "authors": ["Ilias Chrysovergis", "Alexandros-Apostolos A. Boulogeorgos", "Theodoros A. Tsiftsis", "Dusit Niyato"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04150v1", "summary": "This article introduces a comprehensive metaverse framework, which is\ndesigned for the simulation, emulation, and interaction with wireless systems.\nThe proposed framework integrates core metaverse technologies such as extended\nreality (XR), digital twins (DTs), artificial intelligence (AI), internet of\nthings (IoT), blockchain, and advanced 6G networking solutions to create a\ndynamic, immersive platform for both system development and management. By\nleveraging XR, users can visualize and engage with complex systems, while DTs\nenable real-time monitoring and optimization. AI generates the\nthree-dimensional (3D) content, enhances decision-making and system\nperformance, whereas IoT devices provide real-time sensor data for boosting the\nsimulation accuracy. Additionally, blockchain ensures secure, decentralized\ninteractions, and 5G/6G networks offer the necessary infrastructure for\nseamless, low-latency communication. This framework serves as a robust tool for\nexploring, developing, and optimizing wireless systems, aiming to provide\nvaluable insights into the future of networked environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04150v1", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04317", "title": "DSNS: The Deep Space Network Simulator", "authors": ["Joshua Smailes", "Filip Futera", "Sebastian KÃ¶hler", "Simon Birnbach", "Martin Strohmeier", "Ivan Martinovic"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04317v1", "summary": "Simulation tools are commonly used in the development and testing of new\nprotocols or new networks. However, as satellite networks start to grow to\nencompass thousands of nodes, and as companies and space agencies begin to\nrealize the interplanetary internet, existing satellite and network simulation\ntools have become impractical for use in this context.\n  We therefore present the Deep Space Network Simulator (DSNS): a new network\nsimulator with a focus on large-scale satellite networks. We demonstrate its\nimproved capabilities compared to existing offerings, showcase its flexibility\nand extensibility through an implementation of existing protocols and the DTN\nsimulation reference scenarios recommended by CCSDS, and evaluate its\nscalability, showing that it exceeds existing tools while providing better\nfidelity.\n  DSNS provides concrete usefulness to both standards bodies and satellite\noperators, enabling fast iteration on protocol development and testing of\nparameters under highly realistic conditions. By removing roadblocks to\nresearch and innovation, we can accelerate the development of upcoming\nsatellite networks and ensure that their communication is both fast and secure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04317v1", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04415", "title": "Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection", "authors": ["Xuan Chen", "Yu Huang", "Miaowen Wen", "Shahid Mumtaz", "Fatih Gulec", "Anwer Al-Dulaimi", "Andrew W. Eckford"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04415v1", "summary": "The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary\nhealthcare paradigm, shows promise for epidemic control. This paper explores\nthe potential of using molecular communication (MC) to address the challenges\nin constructing IoBNT for epidemic prevention, specifically focusing on\nmodeling viral transmission, detecting the virus/infected individuals, and\nidentifying virus mutations. First, the MC channels in macroscale and\nmicroscale scenarios are discussed to match viral transmission in both scales\nseparately. Besides, the detection methods for these two scales are also\nstudied, along with the localization mechanism designed for the virus/infected\nindividuals. Moreover, an identification strategy is proposed to determine\npotential virus mutations, which is validated through simulation using the\nORF3a protein as a benchmark. Finally, open research issues are discussed. In\nsummary, this paper aims to analyze viral transmission through MC and combat\nviral spread using signal processing techniques within MC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04415v1", "cate": "cs.NI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04536", "title": "Entanglement distribution in quantum networks via swapping of partially entangled states", "authors": ["Henrique Guerra", "Tailan S. Sarubi", "Rafael Chaves", "Jonas Maziero"], "categories": ["cs.NI", "quant-ph"], "primary_category": "quant-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04536", "summary": "The entanglement swapping protocol (ESP) is a fundamental primitive for\ndistributing quantum correlations across distant nodes in a quantum network.\nRecent studies have demonstrated that even when the involved qubit pairs are\nonly partially entangled, it is still possible to concentrate and transmit\nentanglement via Bell-basis measurements. In this work, we extend these ideas\nto quantum networks with various topologies - including linear, star, and\nhybrid configurations - by analyzing the application of the ESP to initially\npartially entangled states. We investigate how entanglement evolves under such\nprotocols by examining the transformations of the initial states and evaluating\nthe success probabilities for generating maximally entangled states at the\noutput. Our results offer new insights into the dynamics of the entanglement\ndistribution in quantum networks and provide practical guidelines for designing\nrobust quantum communication strategies under realistic conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04536", "cate": "quant-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2411.18319", "title": "OpenOptics: An Open Research Framework for Optical Data Center Networks", "authors": ["Yiming Lei", "Federico De Marchi", "Jialong Li", "Raj Joshi", "Balakrishnan Chandrasekaran", "Yiting Xia"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.18319", "summary": "Optical data center networks (DCNs) are emerging as a promising design for\ncloud infrastructure. However, existing optical DCN architectures operate as\nclosed ecosystems, tying software solutions to specific optical hardware. We\nintroduce OpenOptics, an open research framework that decouples software from\nhardware, allowing them to evolve independently. OpenOptics features: (1) a\ntime-flow table abstraction as a common interface between optical hardware and\nsoftware, (2) a unified workflow and user-friendly API for implementing various\noptical DCNs with simple Python scripts, and (3) a backend system that\nre-architects queue management to support the time-flow tables and provides\nrich infrastructure services for diverse applications. Built on programmable\nswitches, OpenOptics achieves a record-breaking minimum optical circuit\nduration of 2 $\\mu$s using commodity devices. We validate OpenOptics'\ngenerality by implementing six optical architectures and seven routing schemes\non an optical testbed and conducting benchmarks on a 108-ToR setup, showcasing\nits efficiency. Additionally, case studies highlight novel research\nopportunities enabled by OpenOptics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.18319", "cate": "cs.NI", "date": "2024-11-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.23286", "title": "Optimal Packetization Towards Low Latency in Random Access Networks (extended version)", "authors": ["Zihong Li", "Anshan Yuan", "Xinghua Sun"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23286", "summary": "As the demand for low-latency services grows, ensuring the delay performance\nof random access (RA) networks has become a priority. Existing studies on the\nqueueing delay performance of the Aloha model universally treat packets as\natomic transmission units, focusing primarily on delay measured in time slots.\nHowever, the impact of packetization on queueing delay has been consistently\noverlooked, particularly for the mean queueing delay measured in seconds, which\nserves as a more precise and practically relevant performance metric than its\nslot-based counterpart. Here, packetization refers to the process of\ndetermining the number of bits assembled into a packet. To optimize queueing\ndelay from the perspective of packetization, this paper establishes the\nmathematical relationship between packetization and mean queueing delay in\nseconds for both connection-free and connection-based Aloha schemes, and\nexplores the optimal packetization strategy to minimize this delay. We identify\nthe optimal mean queueing delay and its corresponding packet size via numerical\nmethods, and further analyze the influence of various network parameters. We\nfurther use simulations to investigate the similar impact of packetization on\njitter of queueing delay. We then apply our analysis to re-evaluate the complex\ntrade-off between the connection-free and connection-based schemes through the\nnew perspective of packetization. Furthermore, recognizing that an analysis of\nthe queueing delay performance for RA-SDT in NTN scenarios, especially from a\npacketization perspective, also remains an unexplored area, we apply the\nanalysis to this scenario as a case study.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23286", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.00010", "title": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?", "authors": ["Ruibo Wang", "Baha Eddine Youcef Belmekki", "Howard H. Yang", "Mohamed Slim Alouini"], "categories": ["cs.NI"], "primary_category": "cs.NI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00010", "summary": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00010", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04417", "title": "ARMS: Adaptive and Robust Memory Tiering System", "authors": ["Sujay Yadalam", "Konstantinos Kanellis", "Michael Swift", "Shivaram Venkataraman"], "categories": ["cs.OS"], "primary_category": "cs.OS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04417v1", "summary": "Memory tiering systems seek cost-effective memory scaling by adding multiple\ntiers of memory. For maximum performance, frequently accessed (hot) data must\nbe placed close to the host in faster tiers and infrequently accessed (cold)\ndata can be placed in farther slower memory tiers. Existing tiering solutions\nsuch as HeMem, Memtis, and TPP use rigid policies with pre-configured\nthresholds to make data placement and migration decisions. We perform a\nthorough evaluation of the threshold choices and show that there is no single\nset of thresholds that perform well for all workloads and configurations, and\nthat tuning can provide substantial speedups. Our evaluation identified three\nprimary reasons why tuning helps: better hot/cold page identification, reduced\nwasteful migrations, and more timely migrations.\n  Based on this study, we designed ARMS - Adaptive and Robust Memory tiering\nSystem - to provide high performance without tunable thresholds. We develop a\nnovel hot/cold page identification mechanism relying on short-term and\nlong-term moving averages, an adaptive migration policy based on cost/benefit\nanalysis, and a bandwidth-aware batched migration scheduler. Combined, these\napproaches provide out-of-the-box performance within 3% the best tuned\nperformance of prior systems, and between 1.26x-2.3x better than prior systems\nwithout tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04417v1", "cate": "cs.OS", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03830", "title": "If-T: A Benchmark for Type Narrowing", "authors": ["Hanwen Guo", "Ben Greenman"], "categories": ["cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03830v1", "summary": "**Context:** The design of static type systems that can validate\ndynamically-typed programs (**gradually**) is an ongoing challenge. A key\ndifficulty is that dynamic code rarely follows datatype-driven design. Programs\ninstead use runtime tests to narrow down the proper usage of incoming data.\nType systems for dynamic languages thus need a **type narrowing** mechanism\nthat refines the type environment along individual control paths based on\ndominating tests, a form of flow-sensitive typing. In order to express\nrefinements, the type system must have some notion of sets and subsets. Since\nset-theoretic types are computationally and ergonomically complex, the need for\ntype narrowing raises design questions about how to balance precision and\nperformance. **Inquiry:** To date, the design of type narrowing systems has\nbeen driven by intuition, past experience, and examples from users in various\nlanguage communities. There is no standard that captures desirable and\nundesirable behaviors. Prior formalizations of narrowing are also significantly\nmore complex than a standard type system, and it is unclear how the extra\ncomplexity pays off in terms of concrete examples. This paper addresses the\nproblems through If-T, a language-agnostic **design benchmark** for type\nnarrowing that characterizes the abilities of implementations using simple\nprograms that draw attention to fundamental questions. Unlike a traditional\nperformance-focused benchmark, If-T measures a narrowing system's ability to\nvalidate correct code and reject incorrect code. Unlike a test suite, systems\nare not required to fully conform to If-T. Deviations are acceptable provided\nthey are justified by well-reasoned design considerations, such as compile-time\nperformance. **Approach:** If-T is guided by the literature on type narrowing,\nthe documentation of gradual languages such as TypeScript, and experiments with\ntypechecker implementations. We have identified a set of core technical\ndimensions for type narrowing. For each dimension, the benchmark contains a set\nof topics and (at least) two characterizing programs per topic: one that should\ntypecheck and one that should not typecheck. **Knowledge:** If-T provides a\nbaseline to measure type narrowing systems. For researchers, it provides\ncriteria to categorize future designs via its collection of positive and\nnegative examples. For language designers, the benchmark demonstrates the\npayoff of typechecker complexity in terms of concrete examples. Designers can\nuse the examples to decide whether supporting a particular example is\nworthwhile. Both the benchmark and its implementations are freely available\nonline. **Grounding:** We have implemented the benchmark for five typecheckers:\nTypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight\nimportant differences, such as the ability to track logical implications among\nprogram variables and typechecking for user-defined narrowing predicates.\n**Importance:** Type narrowing is essential for gradual type systems, but the\ntradeoffs between systems with different complexity have been unclear. If-T\nclarifies these tradeoffs by illustrating the benefits and limitations of each\nlevel of complexity. With If-T as a way to assess implementations in a fair,\ncross-language manner, future type system designs can strive for a better\nbalance among precision, annotation burden, and performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03830v1", "cate": "cs.PL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03831", "title": "A Type System for Data Privacy Compliance in Active Object Languages", "authors": ["Chinmayi Prabhu Baramashetru", "Paola Giannini", "Silvia Lizeth Tapia Tarifa", "Olaf Owe"], "categories": ["cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03831v1", "summary": "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03831v1", "cate": "cs.PL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03832", "title": "Generating Inputs for Grammar Mining using Dynamic Symbolic Execution", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert PrÃ¤hofer"], "categories": ["cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03832v1", "summary": "A vast number of software systems include components that parse and process\nstructured input. In addition to programming languages, which are analyzed by\ncompilers or interpreters, there are numerous components that process\nstandardized or proprietary data formats of varying complexity. Even if such\ncomponents were initially developed and tested based on a specification, such\nas a grammar, numerous modifications and adaptations over the course of\nsoftware evolution can make it impossible to precisely determine which inputs\nthey actually accept. In this situation, grammar mining can be used to\nreconstruct the specification in the form of a grammar. Established approaches\nalready produce useful results, provided that sufficient input data is\navailable to fully cover the input language. However, achieving this\ncompleteness is a major challenge. In practice, only input data recorded during\nthe operation of the software systems is available. If this data is used for\ngrammar mining, the resulting grammar reflects only the actual processed inputs\nbut not the complete grammar of the input language accepted by the software\ncomponent. As a result, edge cases or previously supported features that no\nlonger appear in the available input data are missing from the generated\ngrammar. This work addresses this challenge by introducing a novel approach for\nthe automatic generation of inputs for grammar mining. Although input\ngenerators have already been used for fuzz testing, it remains unclear whether\nthey are also suitable for grammar miners. Building on the grammar miner Mimid,\nthis work presents a fully automated approach to input generation. The approach\nleverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms\nto overcome the limitations of DSE regarding structured input parsers. First,\nthe search for new inputs is guided by an iterative expansion that starts with\na single-character input and gradually extends it. Second, input generation is\nstructured into a novel three-phase approach, which separates the generation of\ninputs for parser functions. The proposed method was evaluated against a\ndiverse set of eleven benchmark applications from the existing literature.\nResults demonstrate that the approach achieves precision and recall for\nextracted grammars close to those derived from state-of-the-art grammar miners\nsuch as Mimid. Notably, it successfully uncovers subtle features and edge cases\nin parsers that are typically missed by such grammar miners. The effectiveness\nof the method is supported by empirical evidence, showing that it can achieve\nhigh performance in various domains without requiring prior input samples. This\ncontribution is significant for researchers and practitioners in software\nengineering, offering an automated, scalable, and precise solution for grammar\nmining. By eliminating the need for manual input generation, the approach not\nonly reduces workload but also enhances the robustness and comprehensiveness of\nthe extracted grammars. Following this approach, software engineers can\nreconstruct specification from existing (legacy) parsers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03832v1", "cate": "cs.PL", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04115", "title": "Weak Memory Model Formalisms: Introduction and Survey", "authors": ["Roger C. Su", "Robert J. Colvin"], "categories": ["cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04115v1", "summary": "Memory consistency models define the order in which accesses to shared memory\nin a concurrent system may be observed to occur. Such models are a necessity\nsince program order is not a reliable indicator of execution order, due to\nmicroarchitectural features or compiler transformations. Concurrent\nprogramming, already a challenging task, is thus made even harder when weak\nmemory effects must be addressed. A rigorous specification of weak memory\nmodels is therefore essential to make this problem tractable for developers of\nsafety- and security-critical, low-level software.\n  In this paper we survey the field of formalisations of weak memory models,\nincluding their specification, their effects on execution, and tools and\ninference systems for reasoning about code. To assist the discussion we also\nprovide an introduction to two styles of formal representation found commonly\nin the literature (using a much simplified version of Intel's x86 as the\nexample): a step-by-step construction of traces of the system (operational\nsemantics); and with respect to relations between memory events (axiomatic\nsemantics). The survey covers some long-standing hardware features that lead to\nobservable weak behaviours, a description of historical developments in\npractice and in theory, an overview of computability and complexity results,\nand outlines current and future directions in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04115v1", "cate": "cs.PL", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2506.11794", "title": "ALEA IACTA EST: A Declarative Domain-Specific Language for Manually Performable Random Experiments", "authors": ["Baltasar TrancÃ³n y Widemann", "Markus Lepper"], "categories": ["cs.PL", "math.PR"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11794", "summary": "Random experiments that are simple and clear enough to be performed by human\nagents feature prominently in the teaching of elementary stochastics as well as\nin games. We present Alea, a domain-specific language for the specification of\nrandom experiments. Alea code can either be analyzed statically to obtain and\ninspect probability distributions of outcomes, or be executed with a source\npseudo-randomness for simulation or as a game assistant. The language is\nintended for ease of use by non-expert programmers, in particular students of\nelementary stochastics, and players and designers of games of chance, by\nfocusing on concepts common to functional programming and basic mathematics.\nBoth the design of the language and the implementation of runtime environments\nare work in progress.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11794", "cate": "cs.PL", "date": "2025-06-13", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.18885", "title": "IsaMini: Redesigned Isabelle Proof Language for Machine Learning", "authors": ["Qiyuan Xu", "Renxi Wang", "Haonan Li", "David Sanan", "Conrad Watt"], "categories": ["cs.PL"], "primary_category": "cs.PL", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18885", "summary": "Neural Theorem Proving (NTP) employs deep learning methods, particularly\nLarge Language Models (LLMs), to automate formal proofs in proof assistants.\nThis approach holds promise for reducing the dramatic labor costs or\ncomputation costs required in proof engineering, which is fundamental to formal\nverification and other software engineering methods. The paper explores the\npotential of improving NTP by redesigning the proof language, given that LLMs'\ncapabilities depend highly on representations. We introduce \\emph{MiniLang}, a\nredesigned proof language for Isabelle/HOL incorporating an improved version of\nSledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by\nimproving the success rate on the PISA benchmark by up to 29\\% in comparison to\ngeneration of Isar proof script. The success rate under one attempt (so-called\n\\emph{pass@1}) reaches 69.1\\%, exceeding the previous Baldur's pass@64\n(65.7\\%); The pass@8 reaches 79.2\\%, exceeding the state-of-the-art on PISA\n(71.0\\%) achieved by Magnushammer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18885", "cate": "cs.PL", "date": "2025-07-25", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03890", "title": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes", "authors": ["Sanghun Jung", "Daehoon Gwak", "Byron Boots", "James Hays"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03890v1", "summary": "Terrain elevation modeling for off-road navigation aims to accurately\nestimate changes in terrain geometry in real-time and quantify the\ncorresponding uncertainties. Having precise estimations and uncertainties plays\na crucial role in planning and control algorithms to explore safe and reliable\nmaneuver strategies. However, existing approaches, such as Gaussian Processes\n(GPs) and neural network-based methods, often fail to meet these needs. They\nare either unable to perform in real-time due to high computational demands,\nunderestimating sharp geometry changes, or harming elevation accuracy when\nlearned with uncertainties. Recently, Neural Processes (NPs) have emerged as a\npromising approach that integrates the Bayesian uncertainty estimation of GPs\nwith the efficiency and flexibility of neural networks. Inspired by NPs, we\npropose an effective NP-based method that precisely estimates sharp elevation\nchanges and quantifies the corresponding predictive uncertainty without losing\nelevation accuracy. Our method leverages semantic features from LiDAR and\ncamera sensors to improve interpolation and extrapolation accuracy in\nunobserved regions. Also, we introduce a local ball-query attention mechanism\nto effectively reduce the computational complexity of global attention by 17\\%\nwhile preserving crucial local and spatial information. We evaluate our method\non off-road datasets having interesting geometric features, collected from\ntrails, deserts, and hills. Our results demonstrate superior performance over\nbaselines and showcase the potential of neural processes for effective and\nexpressive terrain modeling in complex off-road environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03890v1", "cate": "cs.RO", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04056", "title": "SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of Enteric Emissions in Cattle with ex-vivo Validation", "authors": ["Yuelin Deng", "Hinayah Rojas de Oliveira", "Richard M. Voyles", "Upinder Kaur"], "categories": ["cs.RO", "q-bio.QM"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04056v1", "summary": "Accurate measurement of enteric methane emissions remains a critical\nbottleneck for advancing livestock sustainability through genetic selection and\nprecision management. Existing ambient sampling approaches suffer from low data\nretention rates, environmental interference, and limited temporal resolution.\nWe developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the\nfirst robust in-vivo sensing system enabling continuous, high-resolution\nmonitoring of ruminal methane concentrations through an innovative closed-loop\ngas recirculation design. We conducted comprehensive validation with two\ncannulated Simmental heifers under contrasting dietary treatments, with\ncross-platform comparison against established ambient sniffer systems. SCOUT\nachieved exceptional performance with 82% data retention compared to 17% for\nconventional sniffer systems, while capturing methane concentrations 100-1000x\nhigher than ambient approaches. Cross-platform validation demonstrated strong\nscale-dependent correlations, with optimal correlation strength (r = -0.564\n$\\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical\nsignificance. High-frequency monitoring revealed novel behavior-emission\ncoupling, including rapid concentration changes (14.5 $\\pm$ 11.3k ppm)\ntriggered by postural transitions within 15 minutes, insights previously\ninaccessible through existing technologies. The SCOUT system represents a\ntransformative advancement, enabling accurate, continuous emission phenotyping\nessential for genomic selection programs and sustainable precision livestock\nmanagement. This validation framework establishes new benchmarks for\nagricultural sensor performance while generating unprecedented biological\ninsights into ruminal methane dynamics, contributing essential tools for\nsustainable livestock production in climate-conscious agricultural systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04056v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04146", "title": "Industrial Robot Motion Planning with GPUs: Integration of cuRobo for Extended DOF Systems", "authors": ["Luai Abuelsamen", "Harsh Rana", "Ho-Wei Lu", "Wenhan Tang", "Swati Priyadarshini", "Gabriel Gomes"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04146v1", "summary": "Efficient motion planning remains a key challenge in industrial robotics,\nespecially for multi-axis systems operating in complex environments. This paper\naddresses that challenge by integrating GPU-accelerated motion planning through\nNVIDIA's cuRobo library into Vention's modular automation platform. By\nleveraging accurate CAD-based digital twins and real-time parallel\noptimization, our system enables rapid trajectory generation and dynamic\ncollision avoidance for pick-and-place tasks. We demonstrate this capability on\nrobots equipped with additional degrees of freedom, including a 7th-axis\ngantry, and benchmark performance across various scenarios. The results show\nsignificant improvements in planning speed and robustness, highlighting the\npotential of GPU-based planning pipelines for scalable, adaptable deployment in\nmodern industrial workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04146v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04338", "title": "Improving Tactile Gesture Recognition with Optical Flow", "authors": ["Shaohong Zhong", "Alessandro Albini", "Giammarco Caroleo", "Giorgio Cannata", "Perla Maiolino"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04338v1", "summary": "Tactile gesture recognition systems play a crucial role in Human-Robot\nInteraction (HRI) by enabling intuitive communication between humans and\nrobots. The literature mainly addresses this problem by applying machine\nlearning techniques to classify sequences of tactile images encoding the\npressure distribution generated when executing the gestures. However, some\ngestures can be hard to differentiate based on the information provided by\ntactile images alone. In this paper, we present a simple yet effective way to\nimprove the accuracy of a gesture recognition classifier. Our approach focuses\nsolely on processing the tactile images used as input by the classifier. In\nparticular, we propose to explicitly highlight the dynamics of the contact in\nthe tactile image by computing the dense optical flow. This additional\ninformation makes it easier to distinguish between gestures that produce\nsimilar tactile images but exhibit different contact dynamics. We validate the\nproposed approach in a tactile gesture recognition task, showing that a\nclassifier trained on tactile images augmented with optical flow information\nachieved a 9% improvement in gesture classification accuracy compared to one\ntrained on standard tactile images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04338v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04372", "title": "Tactile Comfort: Lowering Heart Rate Through Interactions", "authors": ["Morten Roed Frederiksen", "Kasper StÃ¸y", "Maja MatariÄ"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04372v1", "summary": "Children diagnosed with anxiety disorders are taught a range of strategies to\nnavigate situations of heightened anxiety. Techniques such as deep breathing\nand repetition of mantras are commonly employed, as they are known to be\ncalming and reduce elevated heart rates. Although these strategies are often\neffective, their successful application relies on prior training of the\nchildren for successful use when faced with challenging situations. This paper\ninvestigates a pocket-sized companion robot designed to offer a relaxation\ntechnique requiring no prior training, with a focus on immediate impact on the\nuser's heart rate. The robot utilizes a tactile game to divert the user's\nattention, thereby promoting relaxation. We conducted two studies with children\nwho were not diagnosed with anxiety: a 14-day pilot study with two children\n(age 8) and a main study with 18 children (ages 7-8). Both studies employed a\nwithin-subjects design and focused on measuring heart rate during tactile\ninteraction with the robot and during non-use. Interacting with the robot was\nfound to significantly lower the study participants' heart rate (p$<$0.01)\ncompared to the non-use condition, indicating a consistent calming effect\nacross all participants. These results suggest that tactile companion robots\nhave the potential to enhance the therapeutic value of relaxation techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04372v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04384", "title": "Incorporating Stochastic Models of Controller Behavior into Kinodynamic Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in Off-Road Environments", "authors": ["Eric R. Damm", "Eli S. Lancaster", "Felix A. Sanchez", "Kiana Bronder", "Jason M. Gregory", "Thomas M. Howard"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04384v1", "summary": "Mobile robot motion planners rely on theoretical models to predict how the\nrobot will move through the world. However, when deployed on a physical robot,\nthese models are subject to errors due to real-world physics and uncertainty in\nhow the lower-level controller follows the planned trajectory. In this work, we\naddress this problem by presenting three methods of incorporating stochastic\ncontroller behavior into the recombinant search space of the Kinodynamic\nEfficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,\nwe analyze the results of experiments performed on a Clearpath Robotics Warthog\nUnmanned Ground Vehicle (UGV) in an off-road, unstructured environment using\ntwo different perception algorithms, and performed an ablation study using a\nfull spectrum of simulated environment map complexities. Analysis of the data\nfound that incorporating stochastic controller sampling into KEASL leads to\nmore conservative trajectories that decrease predicted collision likelihood\nwhen compared to KEASL without sampling. When compared to baseline planning\nwith expanded obstacle footprints, the predicted likelihood of collisions\nbecomes more comparable, but reduces the planning success rate for baseline\nsearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04384v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04436", "title": "Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks", "authors": ["Yujia Lu", "Chong Wei", "Lu Ma"], "categories": ["cs.RO", "eess.SY"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04436v1", "summary": "Autonomous highway driving presents a high collision risk due to\nfast-changing environments and limited reaction time, necessitating reliable\nand efficient trajectory planning. This paper proposes a hybrid trajectory\nplanning framework that integrates the adaptability of learning-based methods\nwith the formal safety guarantees of optimization-based approaches. The\nframework features a two-layer architecture: an upper layer employing a graph\nneural network (GNN) trained on real-world highway data to predict human-like\nlongitudinal velocity profiles, and a lower layer utilizing path optimization\nformulated as a mixed-integer quadratic programming (MIQP) problem. The primary\ncontribution is the lower-layer path optimization model, which introduces a\nlinear approximation of discretized vehicle geometry to substantially reduce\ncomputational complexity, while enforcing strict spatiotemporal non-overlapping\nconstraints to formally guarantee collision avoidance throughout the planning\nhorizon. Experimental results demonstrate that the planner generates highly\nsmooth, collision-free trajectories in complex real-world emergency scenarios,\nachieving success rates exceeding 97% with average planning times of 54 ms,\nthereby confirming real-time capability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04436v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04598", "title": "$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything", "authors": ["Lingfeng Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Haoxiang Fu", "Xinyu Zheng", "Pengwei Wang", "Zhongyuan Wang", "Wenbo Ding", "Shanghang Zhang"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04598v1", "summary": "Embodied navigation is a fundamental capability of embodied intelligence,\nenabling robots to move and interact within physical environments. However,\nexisting navigation tasks primarily focus on predefined object navigation or\ninstruction following, which significantly differs from human needs in\nreal-world scenarios involving complex, open-ended scenes. To bridge this gap,\nwe introduce a challenging long-horizon navigation task that requires\nunderstanding high-level human instructions and performing spatial-aware object\nnavigation in real-world environments. Existing embodied navigation methods\nstruggle with such tasks due to their limitations in comprehending high-level\nhuman instructions and localizing objects with an open vocabulary. In this\npaper, we propose $NavA^3$, a hierarchical framework divided into two stages:\nglobal and local policies. In the global policy, we leverage the reasoning\ncapabilities of Reasoning-VLM to parse high-level human instructions and\nintegrate them with global 3D scene views. This allows us to reason and\nnavigate to regions most likely to contain the goal object. In the local\npolicy, we have collected a dataset of 1.0 million samples of spatial-aware\nobject affordances to train the NaviAfford model (PointingVLM), which provides\nrobust open-vocabulary object localization and spatial awareness for precise\ngoal identification and navigation in complex environments. Extensive\nexperiments demonstrate that $NavA^3$ achieves SOTA results in navigation\nperformance and can successfully complete longhorizon navigation tasks across\ndifferent robot embodiments in real-world settings, paving the way for\nuniversal embodied navigation. The dataset and code will be made available.\nProject website: https://NavigationA3.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04598v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04678", "title": "Open Scene Graphs for Open-World Object-Goal Navigation", "authors": ["Joel Loo", "Zhanxin Wu", "David Hsu"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04678v1", "summary": "How can we build general-purpose robot systems for open-world semantic\nnavigation, e.g., searching a novel environment for a target object specified\nin natural language? To tackle this challenge, we introduce OSG Navigator, a\nmodular system composed of foundation models, for open-world Object-Goal\nNavigation (ObjectNav). Foundation models provide enormous semantic knowledge\nabout the world, but struggle to organise and maintain spatial information\neffectively at scale. Key to OSG Navigator is the Open Scene Graph\nrepresentation, which acts as spatial memory for OSG Navigator. It organises\nspatial information hierarchically using OSG schemas, which are templates, each\ndescribing the common structure of a class of environments. OSG schemas can be\nautomatically generated from simple semantic labels of a given environment,\ne.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to\nnew environment types. We conducted experiments using both Fetch and Spot\nrobots in simulation and in the real world, showing that OSG Navigator achieves\nstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shot\nover diverse goals, environments, and robot embodiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04678v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04696", "title": "Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification", "authors": ["Vyacheslav Kovalev", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04696v1", "summary": "Accurate system identification is crucial for reducing trajectory drift in\nbipedal locomotion, particularly in reinforcement learning and model-based\ncontrol. In this paper, we present a novel control framework that integrates\nsystem identification into the reinforcement learning training loop using\ndifferentiable simulation. Unlike traditional approaches that rely on direct\ntorque measurements, our method estimates system parameters using only\ntrajectory data (positions, velocities) and control inputs. We leverage the\ndifferentiable simulator MuJoCo-XLA to optimize system parameters, ensuring\nthat simulated robot behavior closely aligns with real-world motion. This\nframework enables scalable and flexible parameter optimization. Accurate system\nidentification is crucial for reducing trajectory drift in bipedal locomotion,\nparticularly in reinforcement learning and model-based control. In this paper,\nwe present a novel control framework that integrates system identification into\nthe reinforcement learning training loop using differentiable simulation.\nUnlike traditional approaches that rely on direct torque measurements, our\nmethod estimates system parameters using only trajectory data (positions,\nvelocities) and control inputs. We leverage the differentiable simulator\nMuJoCo-XLA to optimize system parameters, ensuring that simulated robot\nbehavior closely aligns with real-world motion. This framework enables scalable\nand flexible parameter optimization. It supports fundamental physical\nproperties such as mass and inertia. Additionally, it handles complex system\nnonlinear behaviors, including advanced friction models, through neural network\napproximations. Experimental results show that our framework significantly\nimproves trajectory following.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04696v1", "cate": "cs.RO", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2410.03481", "title": "Compact LED-Based Displacement Sensing for Robot Fingers", "authors": ["Amr El-Azizi", "Sharfin Islam", "Pedro Piacenza", "Kai Jiang", "Ioannis Kymissis", "Matei Ciocarlie"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.03481", "summary": "In this paper, we introduce a sensor designed for integration in robot\nfingers, where it can provide information on the displacements induced by\nexternal contact. Our sensor uses LEDs to sense the displacement between two\nplates connected by a transparent elastomer; when a force is applied to the\nfinger, the elastomer displaces and the LED signals change. We show that using\nLEDs as both light emitters an receivers in this context provides high\nsensitivity, allowing such an emitter and receiver pairs to detect very small\ndisplacements. We characterize the standalone performance of the sensor by\ntesting the ability of a supervised learning model to predict complete force\nand torque data from its raw signals, and obtain a mean error between 0.05 and\n0.07 N across the three directions of force applied to the finger. Our method\nallows for finger-size packaging with no amplification electronics, low cost\nmanufacturing, easy integration into a complete hand, and high overload shear\nforces and bending torques, suggesting future applicability to complete\nmanipulation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.03481", "cate": "cs.RO", "date": "2024-10-04", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.07902", "title": "LTLCodeGen: Code Generation of Syntactically Correct Temporal Logic for Robot Task Planning", "authors": ["Behrad Rabiei", "Mahesh Kumar A.R.", "Zhirui Dai", "Surya L.S.R. Pilla", "Qiyue Dong", "Nikolay Atanasov"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.07902", "summary": "This paper focuses on planning robot navigation tasks from natural language\nspecifications. We develop a modular approach, where a large language model\n(LLM) translates the natural language instructions into a linear temporal logic\n(LTL) formula with propositions defined by object classes in a semantic\noccupancy map. The LTL formula and the semantic occupancy map are provided to a\nmotion planning algorithm to generate a collision-free robot path that\nsatisfies the natural language instructions. Our main contribution is\nLTLCodeGen, a method to translate natural language to syntactically correct LTL\nusing code generation. We demonstrate the complete task planning method in\nreal-world experiments involving human speech to provide navigation\ninstructions to a mobile robot. We also thoroughly evaluate our approach in\nsimulated and real-world experiments in comparison to end-to-end LLM task\nplanning and state-of-the-art LLM-to-LTL translation methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.07902", "cate": "cs.RO", "date": "2025-03-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.18525", "title": "\\textit{RoboTron-Nav}: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction", "authors": ["Yufeng Zhong", "Chengjian Feng", "Feng Yan", "Fanfan Liu", "Liming Zheng", "Lin Ma"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.18525", "summary": "In language-guided visual navigation, agents locate target objects in unseen\nenvironments using natural language instructions. For reliable navigation in\nunfamiliar scenes, agents should possess strong perception, planning, and\nprediction capabilities. Additionally, when agents revisit previously explored\nareas during long-term navigation, they may retain irrelevant and redundant\nhistorical perceptions, leading to suboptimal results. In this work, we propose\n\\textbf{RoboTron-Nav}, a unified framework that integrates {p}erception,\n{p}lanning, and {p}rediction capabilities through multitask collaborations on\nnavigation and embodied question answering tasks, thereby enhancing navigation\nperformances. Furthermore, RoboTron-Nav employs an adaptive 3D-aware history\nsampling strategy to effectively and efficiently utilize historical\nobservations. By leveraging large language model, RoboTron-Nav comprehends\ndiverse commands and complex visual scenes, resulting in appropriate navigation\nactions. RoboTron-Nav achieves an 81.1\\% success rate in object goal navigation\non the $\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.18525", "cate": "cs.RO", "date": "2025-03-24", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.06662", "title": "RAMBO: RL-Augmented Model-Based Whole-Body Control for Loco-Manipulation", "authors": ["Jin Cheng", "Dongho Kang", "Gabriele Fadini", "Guanya Shi", "Stelian Coros"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06662", "summary": "Loco-manipulation, physical interaction of various objects that is\nconcurrently coordinated with locomotion, remains a major challenge for legged\nrobots due to the need for both precise end-effector control and robustness to\nunmodeled dynamics. While model-based controllers provide precise planning via\nonline optimization, they are limited by model inaccuracies. In contrast,\nlearning-based methods offer robustness, but they struggle with precise\nmodulation of interaction forces. We introduce RAMBO, a hybrid framework that\nintegrates model-based whole-body control within a feedback policy trained with\nreinforcement learning. The model-based module generates feedforward torques by\nsolving a quadratic program, while the policy provides feedback corrective\nterms to enhance robustness. We validate our framework on a quadruped robot\nacross a diverse set of real-world loco-manipulation tasks, such as pushing a\nshopping cart, balancing a plate, and holding soft objects, in both quadrupedal\nand bipedal walking. Our experiments demonstrate that RAMBO enables precise\nmanipulation capabilities while achieving robust and dynamic locomotion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06662", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-08-05", "section": "repl"}
{"id": "2507.02029", "title": "RoboBrain 2.0 Technical Report", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Mengfei Du", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Junkai Zhao", "Xiaojie Zhang", "Shanyu Rong", "Huaihai Lyu", "Zhengliang Cai", "Yankai Fu", "Ning Chen", "Bolun Zhang", "Lingfeng Zhang", "Shuyi Zhang", "Dong Liu", "Xi Feng", "Songjing Wang", "Xiaodan Liu", "Yance Jiao", "Mengsi Lyu", "Zhuo Chen", "Chenrui He", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02029", "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02029", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03408", "title": "Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments", "authors": ["Ivana Collado-Gonzalez", "John McConnell", "Paul Szenher", "Brendan Englot"], "categories": ["cs.RO"], "primary_category": "cs.RO", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03408", "summary": "Scene reconstruction is an essential capability for underwater robots\nnavigating in close proximity to structures. Monocular vision-based\nreconstruction methods are unreliable in turbid waters and lack depth scale\ninformation. Sonars are robust to turbid water and non-uniform lighting\nconditions, however, they have low resolution and elevation ambiguity. This\nwork proposes a real-time opti-acoustic scene reconstruction method that is\nspecially optimized to work in turbid water. Our strategy avoids having to\nidentify point features in visual data and instead identifies regions of\ninterest in the data. We then match relevant regions in the image to\ncorresponding sonar data. A reconstruction is obtained by leveraging range data\nfrom the sonar and elevation data from the camera image. Experimental\ncomparisons against other vision-based and sonar-based approaches at varying\nturbidity levels, and field tests conducted in marina environments, validate\nthe effectiveness of the proposed approach. We have made our code open-source\nto facilitate reproducibility and encourage community engagement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03408", "cate": "cs.RO", "date": "2025-08-05", "updated": "2025-08-06", "section": "repl"}
{"id": "2407.11692", "title": "Reachset-Conformant System Identification", "authors": ["Laura LÃ¼tzow", "Matthias Althoff"], "categories": ["cs.RO", "eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.11692", "summary": "Formal verification techniques play a pivotal role in ensuring the safety of\ncomplex cyber-physical systems. To transfer model-based verification results to\nthe real world, we require that the measurements of the target system lie in\nthe set of reachable outputs of the corresponding model, a property we refer to\nas reachset conformance. This paper is on automatically identifying those\nreachset-conformant models. While state-of-the-art reachset-conformant\nidentification methods focus on linear state-space models, we generalize these\nmethods to nonlinear state-space models and linear and nonlinear input-output\nmodels. Furthermore, our identification framework adapts to different levels of\nprior knowledge on the system dynamics. In particular, we identify the set of\nmodel uncertainties for white-box models, the parameters and the set of model\nuncertainties for gray-box models, and entire reachset-conformant black-box\nmodels from data. The robustness and efficacy of our framework are demonstrated\nin extensive numerical experiments using simulated and real-world data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.11692", "cate": "eess.SY", "date": "2024-07-16", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03983", "title": "MiDashengLM: Efficient Audio Understanding with General Audio Captions", "authors": ["Heinrich Dinkel", "Gang Li", "Jizhong Liu", "Jian Luan", "Yadong Niu", "Xingwei Sun", "Tianzi Wang", "Qiyang Xiao", "Junbo Zhang", "Jiahao Zhou"], "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03983v1", "summary": "Current approaches for large audio language models (LALMs) often rely on\nclosed data sources or proprietary models, limiting their generalization and\naccessibility. This paper introduces MiDashengLM, a novel open audio-language\nmodel designed for efficient and comprehensive audio understanding through the\nuse of general audio captions using our novel ACAVCaps training dataset.\nMiDashengLM exclusively relies on publicly available pretraining and supervised\nfine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At\nits core, MiDashengLM integrates Dasheng, an open-source audio encoder,\nspecifically engineered to process diverse auditory information effectively.\nUnlike previous works primarily focused on Automatic Speech Recognition (ASR)\nbased audio-text alignment, our strategy centers on general audio captions,\nfusing speech, sound and music information into one textual representation,\nenabling a holistic textual representation of complex audio scenes. Lastly,\nMiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT)\nand up to 20x higher throughput than comparable models. Checkpoints are\navailable online at https://huggingface.co/mispeech/midashenglm-7b and\nhttps://github.com/xiaomi-research/dasheng-lm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03983v1", "cate": "cs.SD", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04096", "title": "Efficient Scaling for LLM-based ASR", "authors": ["Bingshen Mu", "Yiwen Shao", "Kun Wei", "Dong Yu", "Lei Xie"], "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04096v1", "summary": "Large language model (LLM)-based automatic speech recognition (ASR) achieves\nstrong performance but often incurs high computational costs. This work\ninvestigates how to obtain the best LLM-ASR performance efficiently. Through\ncomprehensive and controlled experiments, we find that pretraining the speech\nencoder before integrating it with the LLM leads to significantly better\nscaling efficiency than the standard practice of joint post-training of\nLLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training\nstrategy, EFIN: Encoder First Integration. Among all training strategies\nevaluated, EFIN consistently delivers better performance (relative to 21.1%\nCERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore,\nwe derive a scaling law that approximates ASR error rates as a computation\nfunction, providing practical guidance for LLM-ASR scaling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04096v1", "cate": "cs.SD", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04529", "title": "ESDD 2026: Environmental Sound Deepfake Detection Challenge Evaluation Plan", "authors": ["Han Yin", "Yang Xiao", "Rohan Kumar Das", "Jisheng Bai", "Ting Dang"], "categories": ["cs.SD"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04529v1", "summary": "Recent advances in audio generation systems have enabled the creation of\nhighly realistic and immersive soundscapes, which are increasingly used in film\nand virtual reality. However, these audio generators also raise concerns about\npotential misuse, such as generating deceptive audio content for fake videos\nand spreading misleading information. Existing datasets for environmental sound\ndeepfake detection (ESDD) are limited in scale and audio types. To address this\ngap, we have proposed EnvSDD, the first large-scale curated dataset designed\nfor ESDD, consisting of 45.25 hours of real and 316.7 hours of fake sound.\nBased on EnvSDD, we are launching the Environmental Sound Deepfake Detection\nChallenge. Specifically, we present two different tracks: ESDD in Unseen\nGenerators and Black-Box Low-Resource ESDD, covering various challenges\nencountered in real-life scenarios. The challenge will be held in conjunction\nwith the 2026 IEEE International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP 2026).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04529v1", "cate": "cs.SD", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04141", "title": "Parallel GPT: Harmonizing the Independence and Interdependence of Acoustic and Semantic Information for Zero-Shot Text-to-Speech", "authors": ["Jingyuan Xing", "Zhipeng Li", "Jialong Mai", "Xiaofen Xing", "Xiangmin Xu"], "categories": ["cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04141", "summary": "Advances in speech representation and large language models have enhanced\nzero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS\nmodels face challenges in capturing the complex correlations between acoustic\nand semantic features, resulting in a lack of expressiveness and similarity.\nThe primary reason lies in the complex relationship between semantic and\nacoustic features, which manifests independent and interdependent aspects.This\npaper introduces a TTS framework that combines both autoregressive (AR) and\nnon-autoregressive (NAR) modules to harmonize the independence and\ninterdependence of acoustic and semantic information. The AR model leverages\nthe proposed Parallel Tokenizer to synthesize the top semantic and acoustic\ntokens simultaneously. In contrast, considering the interdependence, the\nCoupled NAR model predicts detailed tokens based on the general AR model's\noutput. Parallel GPT, built on this architecture, is designed to improve\nzero-shot text-to-speech synthesis through its parallel structure. Experiments\non English and Chinese datasets demonstrate that the proposed model\nsignificantly outperforms the quality and efficiency of the synthesis of\nexisting zero-shot TTS models. Speech demos are available at\nhttps://t1235-ch.github.io/pgpt/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04141", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04230", "title": "Towards interpretable emotion recognition: Identifying key features with machine learning", "authors": ["Yacouba Kaloga", "Ina Kodrasi"], "categories": ["cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04230", "summary": "Unsupervised methods, such as wav2vec2 and HuBERT, have achieved\nstate-of-the-art performance in audio tasks, leading to a shift away from\nresearch on interpretable features. However, the lack of interpretability in\nthese methods limits their applicability in critical domains like medicine,\nwhere understanding feature relevance is crucial. To better understand the\nfeatures of unsupervised models, it remains critical to identify the\ninterpretable features relevant to a given task. In this work, we focus on\nemotion recognition and use machine learning algorithms to identify and\ngeneralize the most important interpretable features for this task. While\nprevious studies have explored feature relevance in emotion recognition, they\nare often constrained by narrow contexts and present inconsistent findings. Our\napproach aims to overcome these limitations, providing a broader and more\nrobust framework for identifying the most important interpretable features.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04230", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04283", "title": "A Multi-stage Low-latency Enhancement System for Hearing Aids", "authors": ["Chengwei Ouyang", "Kexin Fei", "Haoshuai Zhou", "Congxi Lu", "Linkai Li"], "categories": ["cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04283", "summary": "This paper proposes an end-to-end system for the ICASSP 2023 Clarity\nChallenge. In this work, we introduce four major novelties: (1) a novel\nmulti-stage system in both the magnitude and complex domains to better utilize\nphase information; (2) an asymmetric window pair to achieve higher frequency\nresolution with the 5ms latency constraint; (3) the integration of head\nrotation information and the mixture signals to achieve better enhancement; (4)\na post-processing module that achieves higher hearing aid speech perception\nindex (HASPI) scores with the hearing aid amplification stage provided by the\nbaseline system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04283", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04333", "title": "Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots", "authors": ["Gyeong-Tae Lee"], "categories": ["cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04333", "summary": "Humanoid robots require simultaneous sound event type and direction\nestimation for situational awareness, but conventional two-channel input\nstruggles with elevation estimation and front-back confusion. This paper\nproposes a binaural sound event localization and detection (BiSELD) neural\nnetwork to address these challenges. BiSELDnet learns time-frequency patterns\nand head-related transfer function (HRTF) localization cues from binaural input\nfeatures. A novel eight-channel binaural time-frequency feature (BTFF) is\nintroduced, comprising left/right mel-spectrograms, V-maps, an interaural time\ndifference (ITD) map (below 1.5 kHz), an interaural level difference (ILD) map\n(above 5 kHz with front-back asymmetry), and spectral cue (SC) maps (above 5\nkHz for elevation). The effectiveness of BTFF was confirmed across\nomnidirectional, horizontal, and median planes. BiSELDnets, particularly one\nbased on the efficient Trinity module, were implemented to output time series\nof direction vectors for each sound event class, enabling simultaneous\ndetection and localization. Vector activation map (VAM) visualization was\nproposed to analyze network learning, confirming BiSELDnet's focus on the N1\nnotch frequency for elevation estimation. Comparative evaluations under urban\nbackground noise conditions demonstrated that the proposed BiSELD model\nsignificantly outperforms state-of-the-art (SOTA) SELD models with binaural\ninput.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04333", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04425", "title": "Text adaptation for speaker verification with speaker-text factorized embeddings", "authors": ["Yexin Yang", "Shuai Wang", "Xun Gong", "Yanmin Qian", "Kai Yu"], "categories": ["cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04425", "summary": "Text mismatch between pre-collected data, either training data or enrollment\ndata, and the actual test data can significantly hurt text-dependent speaker\nverification (SV) system performance. Although this problem can be solved by\ncarefully collecting data with the target speech content, such data collection\ncould be costly and inflexible. In this paper, we propose a novel text\nadaptation framework to address the text mismatch issue. Here, a speaker-text\nfactorization network is proposed to factorize the input speech into speaker\nembeddings and text embeddings and then integrate them into a single\nrepresentation in the later stage. Given a small amount of speaker-independent\nadaptation utterances, text embeddings of target speech content can be\nextracted and used to adapt the text-independent speaker embeddings to\ntext-customized speaker embeddings. Experiments on RSR2015 show that text\nadaptation can significantly improve the performance of text mismatch\nconditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04425", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04430", "title": "Melodic and Metrical Elements of Expressiveness in Hindustani Vocal Music", "authors": ["Yash Bhake", "Ankit Anand", "Preeti Rao"], "categories": ["cs.SD", "eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04430", "summary": "This paper presents an attempt to study the aesthetics of North Indian Khayal\nmusic with reference to the flexibility exercised by artists in performing\npopular compositions. We study expressive timing and pitch variations of the\ngiven lyrical content within and across performances and propose computational\nrepresentations that can discriminate between different performances of the\nsame song in terms of expression. We present the necessary audio processing and\nannotation procedures, and discuss our observations and insights from the\nanalysis of a dataset of two songs in two ragas each rendered by ten prominent\nartists.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04430", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2412.17924", "title": "Are audio DeepFake detection models polyglots?", "authors": ["BartÅomiej Marek", "Piotr Kawa", "Piotr Syga"], "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.17924", "summary": "Since the majority of audio DeepFake (DF) detection methods are trained on\nEnglish-centric datasets, their applicability to non-English languages remains\nlargely unexplored. In this work, we present a benchmark for the multilingual\naudio DF detection challenge by evaluating various adaptation strategies. Our\nexperiments focus on analyzing models trained on English benchmark datasets, as\nwell as intra-linguistic (same-language) and cross-linguistic adaptation\napproaches. Our results indicate considerable variations in detection efficacy,\nhighlighting the difficulties of multilingual settings. We show that limiting\nthe dataset to English negatively impacts the efficacy, while stressing the\nimportance of the data in the target language.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.17924", "cate": "cs.SD", "date": "2024-12-23", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.07384", "title": "AV-SSAN: Audio-Visual Selective DoA Estimation through Explicit Multi-Band Semantic-Spatial Alignment", "authors": ["Yu Chen", "Hongxu Zhu", "Jiadong Wang", "Kainan Chen", "Xinyuan Qian"], "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07384", "summary": "Audio-visual sound source localization (AV-SSL) estimates the position of\nsound sources by fusing auditory and visual cues. Current AV-SSL methodologies\ntypically require spatially-paired audio-visual data and cannot selectively\nlocalize specific target sources. To address these limitations, we introduce\nCross-Instance Audio-Visual Localization (CI-AVL), a novel task that localizes\ntarget sound sources using visual prompts from different instances of the same\nsemantic class. CI-AVL enables selective localization without spatially paired\ndata. To solve this task, we propose AV-SSAN, a semantic-spatial alignment\nframework centered on a Multi-Band Semantic-Spatial Alignment Network (MB-SSA\nNet). MB-SSA Net decomposes the audio spectrogram into multiple frequency\nbands, aligns each band with semantic visual prompts, and refines spatial cues\nto estimate the direction-of-arrival (DoA). To facilitate this research, we\nconstruct VGGSound-SSL, a large-scale dataset comprising 13,981 spatial audio\nclips across 296 categories, each paired with visual prompts. AV-SSAN achieves\na mean absolute error of 16.59 and an accuracy of 71.29%, significantly\noutperforming existing AV-SSL methods. Code and data will be public.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07384", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03843", "title": "Using Stochastic Block Models for Community Detection: The issue of edge-connectivity", "authors": ["The-Anh Vu-Le", "Minhyuk Park", "Ian Chen", "George Chacko", "Tandy Warnow"], "categories": ["cs.SI"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03843v1", "summary": "A relevant, sometimes overlooked, quality criterion for communities in graphs\nis that they should be well-connected in addition to being edge-dense. Prior\nwork has shown that leading community detection methods can produce\npoorly-connected communities, and some even produce internally disconnected\ncommunities. A recent study by Park et al. in Complex Networks and their\nApplications 2024 showed that this problem is evident in clusterings from three\nStochastic Block Models (SBMs) in graph-tool, a popular software package. To\naddress this issue, Park et al. presented a simple technique, Well-Connected\nClusters (WCC), that repeatedly finds and removes small edge cuts of size at\nmost $\\log_{10}n$ in clusters, where $n$ is the number of nodes in the cluster,\nand showed that treatment of graph-tool SBM clusterings with WCC improves\naccuracy. Here we examine the question of cluster connectivity for clusterings\ncomputed using other SBM software or nested SBMs within graph-tool. Our study,\nusing a wide range of real-world and synthetic networks, shows that all tested\nSBM clustering methods produce communities that are disconnected, and that\ngraph-tool improves on PySBM. We provide insight into why graph-tool\ndegree-corrected SBM clustering produces disconnected clusters by examining the\ndescription length formula it uses, and explore the impact of modifications to\nthe description length formula. Finally, we show that WCC provides an\nimprovement in accuracy for both flat and nested SBMs and establish that it\nscales to networks with millions of nodes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03843v1", "cate": "cs.SI", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04034", "title": "Hierarchical community detection via maximum entropy partitions and the renormalization group", "authors": ["Jorge Martinez Armas"], "categories": ["cs.SI", "physics.data-an", "physics.soc-ph"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04034v1", "summary": "Identifying meaningful structure across multiple scales remains a central\nchallenge in network science. We introduce Hierarchical Clustering Entropy\n(HCE), a general and model-agnostic framework for detecting informative levels\nin hierarchical community structures. Unlike existing approaches, HCE operates\ndirectly on dendrograms without relying on edge-level statistics. It selects\nresolution levels that maximize a principled trade-off between the entropy of\nthe community size distribution and the number of communities, corresponding to\nscales of high structural heterogeneity. This criterion applies to dendrograms\nproduced by a wide range of clustering algorithms and distance metrics,\nincluding modularity-based and correlation-based methods. We evaluate HCE on\nsynthetic benchmarks with varying degrees of hierarchy, size imbalance, and\nnoise, including LFR and both symmetric and asymmetric multiscale models, and\nshow that it consistently identifies partitions closely aligned with ground\ntruth. Applied to real-world networks in social and neuroscience systems, HCE\nreveals interpretable modular hierarchies that align with known structural and\nfunctional organizations. As a scalable and principled method, HCE offers a\ngeneral, domain-independent approach to hierarchical community detection with\npotential applications across biological, social, and technological systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04034v1", "cate": "cs.SI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04187", "title": "Tweets vs Pathogen Spread: A Case Study of COVID-19 in American States", "authors": ["Sara Shabani", "Sahar Jafarbegloo", "Sadegh Raeisi", "Fakhteh Ghanbarnejad"], "categories": ["cs.SI", "q-bio.PE"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04187v1", "summary": "The concept of the mutual influence that awareness and disease may exert on\neach other has recently presented significant challenges. The actions\nindividuals take to prevent contracting a disease and their level of awareness\ncan profoundly affect the dynamics of its spread. Simultaneously, disease\noutbreaks impact how people become aware. In response, we initially propose a\nnull model that couples two Susceptible-Infectious-Recovered (SIR) dynamics and\nanalyze it using a mean-field approach. Subsequently, we explore the parameter\nspace to quantify the effects of this mutual influence on various observables.\nFinally, based on this null model, we conduct an empirical analysis of Twitter\ndata related to COVID-19 and confirmed cases within American states. Our\nfindings indicate that in specific regions of the parameter space, it is\npossible to suppress the epidemic by increasing awareness, and we investigate\nphase transitions. Furthermore, our model demonstrates the ability to alter the\ndominant population group by adjusting parameters throughout the course of the\noutbreak. Additionally, using the model, we assign a set of parameters to each\nstate, revealing that these parameters change at different pandemic peaks.\nNotably, a robust correlation emerges between the ranking of states' Twitter\nactivity, as gathered from empirical data, and the immunity parameters assigned\nto each state using our model. This observation underscores the pivotal role of\nsustained awareness transitioning from the initial to the subsequent peaks in\nthe disease progression.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04187v1", "cate": "cs.SI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04608", "title": "Assortativity in geometric and scale-free networks", "authors": ["Marc Kaufmann", "Ulysse Schaller", "Thomas BlÃ¤sius", "Johannes Lengler"], "categories": ["cs.SI", "math.PR"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04608v1", "summary": "The assortative behavior of a network is the tendency of similar (or\ndissimilar) nodes to connect to each other. This tendency can have an influence\non various properties of the network, such as its robustness or the dynamics of\nspreading processes. In this paper, we study degree assortativity both in\nreal-world networks and in several generative models for networks with\nheavy-tailed degree distribution based on latent spaces. In particular, we\nstudy Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs (GIRGs).\n  Previous research on assortativity has primarily focused on measuring the\ndegree assortativity in real-world networks using the Pearson assortativity\ncoefficient, despite reservations against this coefficient. We rigorously\nconfirm these reservations by mathematically proving that the Pearson\nassortativity coefficient does not measure assortativity in any network with\nsufficiently heavy-tailed degree distributions, which is typical for real-world\nnetworks. Moreover, we find that other single-valued assortativity coefficients\nalso do not sufficiently capture the wiring preferences of nodes, which often\nvary greatly by node degree. We therefore take a more fine-grained approach,\nanalyzing a wide range of conditional and joint weight and degree distributions\nof connected nodes, both numerically in real-world networks and mathematically\nin the generative graph models. We provide several methods of visualizing the\nresults.\n  We show that the generative models are assortativity-neutral, while many\nreal-world networks are not. Therefore, we also propose an extension of the\nGIRG model which retains the manifold desirable properties induced by the\ndegree distribution and the latent space, but also exhibits tunable\nassortativity. We analyze the resulting model mathematically, and give a\nfine-grained quantification of its assortativity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04608v1", "cate": "cs.SI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04694", "title": "Layers of a City: Network-Based Insights into San Diego's Transportation Ecosystem", "authors": ["Matthew Chan", "Steve Sharp", "Jiajian Zhu", "Raman Ebrahimi"], "categories": ["cs.SI", "eess.SY"], "primary_category": "cs.SI", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04694v1", "summary": "Analyzing the structure and function of urban transportation networks is\ncritical for enhancing mobility, equity, and resilience. This paper leverages\nnetwork science to conduct a multi-modal analysis of San Diego's transportation\nsystem. We construct a multi-layer graph using data from OpenStreetMap (OSM)\nand the San Diego Metropolitan Transit System (MTS), representing driving,\nwalking, and public transit layers. By integrating thousands of Points of\nInterest (POIs), we analyze network accessibility, structure, and resilience\nthrough centrality measures, community detection, and a proposed metric for\nwalkability.\n  Our analysis reveals a system defined by a stark core-periphery divide. We\nfind that while the urban core is well-integrated, 30.3% of POIs are isolated\nfrom public transit within a walkable distance, indicating significant equity\ngaps in suburban and rural access. Centrality analysis highlights the driving\nnetwork's over-reliance on critical freeways as bottlenecks, suggesting low\nnetwork resilience, while confirming that San Diego is not a broadly walkable\ncity. Furthermore, community detection demonstrates that transportation mode\ndictates the scale of mobility, producing compact, local clusters for walking\nand broad, regional clusters for driving. Collectively, this work provides a\ncomprehensive framework for diagnosing urban mobility systems, offering\nquantitative insights that can inform targeted interventions to improve\ntransportation equity and infrastructure resilience in San Diego.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04694v1", "cate": "cs.SI", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04671", "title": "Universal Patterns in the Blockchain: Analysis of EOAs and Smart Contracts in ERC20 Token Networks", "authors": ["Kundan Mukhia", "SR Luwang", "Md. Nurujjaman", "Tanujit Chakraborty", "Suman Saha", "Chittaranjan Hens"], "categories": ["cs.SI", "physics.soc-ph", "q-fin.ST"], "primary_category": "q-fin.ST", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04671", "summary": "Scaling laws offer a powerful lens to understand complex transactional\nbehaviors in decentralized systems. This study reveals distinctive statistical\nsignatures in the transactional dynamics of ERC20 tokens on the Ethereum\nblockchain by examining over 44 million token transfers between July 2017 and\nMarch 2018 (9-month period). Transactions are categorized into four types:\nEOA--EOA, EOA--SC, SC-EOA, and SC-SC based on whether the interacting addresses\nare Externally Owned Accounts (EOAs) or Smart Contracts (SCs), and analyzed\nacross three equal periods (each of 3 months). To identify universal\nstatistical patterns, we investigate the presence of two canonical scaling\nlaws: power law distributions and temporal Taylor's law (TL). EOA-driven\ntransactions exhibit consistent statistical behavior, including a near-linear\nrelationship between trade volume and unique partners with stable power law\nexponents ($\\gamma \\approx 2.3$), and adherence to TL with scaling coefficients\n($\\beta \\approx 2.3$). In contrast, interactions involving SCs, especially\nSC-SC, exhibit sublinear scaling, unstable power-law exponents, and\nsignificantly fluctuating Taylor coefficients (variation in $\\beta$ to be\n$\\Delta\\beta = 0.51$). Moreover, SC-driven activity displays heavier-tailed\ndistributions ($\\gamma < 2$), indicating bursty and algorithm-driven activity.\nThese findings reveal the characteristic differences between human-controlled\nand automated transaction behaviors in blockchain ecosystems. By uncovering\nuniversal scaling behaviors through the integration of complex systems theory\nand blockchain data analytics, this work provides a principled framework for\nunderstanding the underlying mechanisms of decentralized financial systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04671", "cate": "q-fin.ST", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.03846", "title": "Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03846v1", "summary": "Empathy is a powerful yet often overlooked element in software engineering\n(SE), supporting better teamwork, smoother communication, and effective\ndecision-making. In our previous study, we identified a range of practitioner\nstrategies for fostering empathy in SE contexts. Building on these insights,\nthis paper introduces 17 actionable empathy guidelines designed to support\npractitioners, teams, and organisations. We also explore how these guidelines\ncan be implemented in practice by examining real-world applications,\nchallenges, and strategies to overcome them shared by software practitioners.\nTo support adoption, we present a visual prioritisation framework that\ncategorises the guidelines based on perceived importance, ease of\nimplementation, and willingness to adopt. The findings offer practical and\nflexible suggestions for integrating empathy into everyday SE work, helping\nteams move from principles to sustainable action.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03846v1", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03881", "title": "From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential", "authors": ["Martin Obaidi", "Kushtrim Qengaj", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil KlÃ¼nder", "Elisa Schmid", "Kurt Schneider"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03881v1", "summary": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03881v1", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03931", "title": "Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems", "authors": ["Everton Guimaraes", "Nathalia Nascimento", "Chandan Shivalingaiah", "Asish Nelapati"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03931v1", "summary": "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are\ntransforming software engineering by automating key tasks, including code\ngeneration, testing, and debugging. As these models become integral to\ndevelopment workflows, a systematic comparison of their performance is\nessential for optimizing their use in real world applications. This study\nbenchmarks these four prominent LLMs on one hundred and fifty LeetCode problems\nacross easy, medium, and hard difficulties, generating solutions in Java and\nPython. We evaluate each model based on execution time, memory usage, and\nalgorithmic complexity, revealing significant performance differences. ChatGPT\ndemonstrates consistent efficiency in execution time and memory usage, while\nCopilot and DeepSeek show variability as task complexity increases. Gemini,\nalthough effective on simpler tasks, requires more attempts as problem\ndifficulty rises. Our findings provide actionable insights into each model's\nstrengths and limitations, offering guidance for developers selecting LLMs for\nspecific coding tasks and providing insights on the performance and complexity\nof GPT-like generated solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03931v1", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.03949", "title": "Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03949v1", "summary": "Transformer-based language models for code have shown remarkable performance\nin various software analytics tasks, but their adoption is hindered by high\ncomputational costs, slow inference speeds, and substantial environmental\nimpact. Model compression techniques such as pruning, quantization, and\nknowledge distillation have gained traction in addressing these challenges.\nHowever, the impact of these strategies on the robustness of compressed\nlanguage models for code in adversarial scenarios remains poorly understood.\nUnderstanding how these compressed models behave under adversarial attacks is\nessential for their safe and effective deployment in real-world applications.\nTo bridge this knowledge gap, we conduct a comprehensive evaluation of how\ncommon compression strategies affect the adversarial robustness of compressed\nmodels. We assess the robustness of compressed versions of three widely used\nlanguage models for code across three software analytics tasks, using six\nevaluation metrics and four commonly used classical adversarial attacks. Our\nfindings indicate that compressed models generally maintain comparable\nperformance to their uncompressed counterparts. However, when subjected to\nadversarial attacks, compressed models exhibit significantly reduced\nrobustness. These results reveal a trade-off between model size reduction and\nadversarial robustness, underscoring the need for careful consideration when\ndeploying compressed models in security-critical software applications. Our\nstudy highlights the need for further research into compression strategies that\nstrike a balance between computational efficiency and adversarial robustness,\nwhich is essential for deploying reliable language models for code in\nreal-world software applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03949v1", "cate": "cs.SE", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04295", "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "authors": ["Chaofan Wang", "Tingrui Yu", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04295v1", "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04295v1", "cate": "cs.SE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04352", "title": "Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models", "authors": ["Dragana Sunaric", "Charlotte Verbruggen", "Dominik Bork"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04352v1", "summary": "As organizations prepare for the end-of-life of Camunda 7, manual migration\nremains complex due to fundamental differences between the two platforms. We\npresent Vanilla-Converter, a command-line tool that facilitates the migration\nof BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the\ntransformation process, supports a wide range of BPMN elements, and produces a\ntransformed model and a detailed transformation log indicating automatic\nchanges and remaining manual conversion tasks. The tool's effectiveness is\ndemonstrated through three case studies with real industrially used Camunda 7\nmodels, confirming its ability to convert these models into valid and\nexecutable Camunda 8 models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04352v1", "cate": "cs.SE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04448", "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection", "authors": ["Damian Gnieciak", "Tomasz Szandala"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04448v1", "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04448v1", "cate": "cs.SE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04479", "title": "Manifestations of Empathy in Software Engineering: How, Why, and When It Matters", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04479v1", "summary": "Empathy plays a crucial role in software engineering (SE), influencing\ncollaboration, communication, and decision-making. While prior research has\nhighlighted the importance of empathy in SE, there is limited understanding of\nhow empathy manifests in SE practice, what motivates SE practitioners to\ndemonstrate empathy, and the factors that influence empathy in SE work. Our\nstudy explores these aspects through 22 interviews and a large scale survey\nwith 116 software practitioners. Our findings provide insights into the\nexpression of empathy in SE, the drivers behind empathetic practices, SE\nactivities where empathy is perceived as useful or not, and the other factors\nthat influence empathy. In addition, we offer practical implications for SE\npractitioners and researchers, offering a deeper understanding of how to\neffectively integrate empathy into SE processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04479v1", "cate": "cs.SE", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2503.08263", "title": "Analyzing the Usage of Donation Platforms for PyPI Libraries", "authors": ["Alexandros Tsakpinis", "Alexander Pretschner"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.08263", "summary": "Software systems rely heavily on open source software (OSS) libraries, which\noffer benefits but also pose risks. When vulnerabilities arise, the OSS\ncommunity may struggle to address them due to inactivity or lack of resources.\nResearch highlights the link between OSS maintenance and financial support. To\nsustain the OSS ecosystem, maintainers should register on donation platforms\nand link these profiles on their project pages, enabling financial support from\nusers and industry stakeholders. However, a detailed study on donation platform\nusage in OSS is missing. This study analyzes the adoption of donation platforms\nin the PyPI ecosystem. For each PyPI library, we retrieve assigned URLs,\ndependencies, and, when available, owner type and GitHub donation links. Using\nPageRank, we analyze different subsets of libraries from both a library and\ndependency chain perspective. Our findings reveal that donation platform links\nare often omitted from PyPI project pages and instead listed on GitHub\nrepositories. GitHub Sponsors is the dominant platform, though many PyPI-listed\nlinks are outdated, emphasizing the need for automated link verification.\nAdoption rates vary significantly across libraries and dependency chains: while\nindividual PyPI libraries show low adoption, those used as dependencies have\nmuch higher usage. This suggests that many dependencies actively seek financial\nsupport, benefiting developers relying on PyPI libraries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.08263", "cate": "cs.SE", "date": "2025-03-11", "updated": "2025-08-06", "section": "repl"}
{"id": "2504.12461", "title": "On the Need to Rethink Trust in AI Assistants for Software Development: A Critical Review", "authors": ["Sebastian Baltes", "Timo Speith", "Brenda Chiteri", "Seyedmoein Mohsenimofidi", "Shalini Chakraborty", "Daniel Buschek"], "categories": ["cs.SE"], "primary_category": "cs.SE", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12461", "summary": "Trust is a fundamental concept in human decision-making and collaboration\nthat has long been studied in philosophy and psychology. However, software\nengineering (SE) articles often use the term trust informally-providing an\nexplicit definition or embedding results in established trust models is rare.\nIn SE research on AI assistants, this practice culminates in equating trust\nwith the likelihood of accepting generated content, which, in isolation, does\nnot capture the full complexity of the trust concept. Without a common\ndefinition, true secondary research on trust is impossible. The objectives of\nour research were: (1) to present the psychological and philosophical\nfoundations of human trust, (2) to systematically study how trust is\nconceptualized in SE and the related disciplines human-computer interaction and\ninformation systems, and (3) to discuss limitations of equating trust with\ncontent acceptance, outlining how SE research can adopt existing trust models\nto overcome the widespread informal use of the term trust. We conducted a\nliterature review across disciplines and a critical review of recent SE\narticles focusing on conceptualizations of trust. We found that trust is rarely\ndefined or conceptualized in SE articles. Related disciplines commonly embed\ntheir methodology and results in established trust models, clearly\ndistinguishing, for example, between initial trust and trust formation and\nbetween appropriate and inappropriate trust. On a meta-scientific level, other\ndisciplines further discuss whether and when trust can be applied to AI\nassistants at all. Our study reveals a significant maturity gap of trust\nresearch in SE compared to related disciplines. We provide concrete\nrecommendations on how SE researchers can adopt established trust models and\ninstruments to study trust in AI assistants beyond the acceptance of generated\nsoftware artifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12461", "cate": "cs.SE", "date": "2025-04-16", "updated": "2025-08-05", "section": "repl"}
{"id": "2508.04241", "title": "Information Bulletin Strategy in Impatient Queuing", "authors": ["Anthony Kiggundu", "Bin Han", "Hans D. Schotten"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04241v1", "summary": "In Sixth Generation (6G) networks, decentralized control in multi-tenant\nsystems is a suggested enabler for autonomous network operations. However,\nautonomy requires independent rationale decisions be taken by tenants. This\nrationality can only be underpinned by timely and continuous access to status\ninformation. Despite its importance, the questions of what information should\nbe shared, how much should be communicated, and how frequently updates should\nbe dispatched remain open research challenges.\n  This manuscript proposes an information bulletin strategy defined around two\nmodels of the system descriptor states to address these fundamental questions.\nThe strategy is that queues periodically broadcast these information models to\ntenants at different time intervals, who may respond by reneging from the queue\nor jockeying to a more favorable one. The expectation is that over time, the\nqueues adapt their processing rates based on what they learn from the tenant\nbehavior. The objective is to minimize overall delay and the impatience. We\nformulate for this impatience as an optimization problem, whose analytical\nsolution is intractable. We perform numerical experiments to evaluate the\nperformance of the learned queue policy and to assess how closely it approaches\noptimal conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04241v1", "cate": "eess.SY", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04380", "title": "Design of Adaptive Hybrid Downlink NOMA-TDMA for Visible Light Communications Networks", "authors": ["Tuan A. Hoang", "Chuyen T. Nguyen", "Thanh V. Pham"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04380v1", "summary": "This paper proposes an adaptive hybrid non-orthogonal multiple access\n(NOMA)-time division multiple access (TDMA) scheme for multi-user visible light\ncommunication (VLC) networks, aiming to enhance users' sum-rate performance\nwhile maintaining low complexity. In the proposed scheme, users are divided\ninto groups where each group is served in a different time slot using TDMA.\nWithin each group, up to two users can be served simultaneously using NOMA. A\ncentral challenge lies in determining which users should be paired together for\nNOMA, as the effectiveness of successive interference cancellation (SIC)\nemployed by NOMA depends on the difference between users' channel gains. To\naddress this, for a pair of users, we determine the range of their channel gain\nratio within which the pair benefits more from NOMA or TDMA. Identifying the\nlower and upper bounds of this range is formulated as two optimization problems\nwhich are solved efficiently using the Successive Convex Approximation (SCA)\nmethod. Simulation results demonstrate that the proposed scheme outperforms the\nconventional hybrid NOMA-TDMA method under different numbers of users and\ntransmit LED powers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04380v1", "cate": "eess.SY", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04382", "title": "Error Accumulation using Linearized Models for Aggregating Flexibility in Distribution Systems", "authors": ["Yanlin Jiang", "Xinliang Dai", "Frederik Zahn", "Yi Guo", "Veit Hagenmeyer"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04382v1", "summary": "This paper investigates flexibility aggregation approaches based on linear\nmodels. We begin by examining the theoretical foundations of linear AC power\nflow, two variants of so-called DC power flow, and the LinDistFlow model, along\nwith their underlying assumptions. The discussion covers key system details,\nincluding network topology, voltage constraints, and line losses. Simulations\nare conducted on the KIT Campus Nord network with real demand and solar data.\nResults show that, in the absence of negative losses, line losses are generally\nunderestimated by linear models. Furthermore, line losses errors tend to\naccumulate both at the point of common coupling (PCC) and over extended time\nhorizons.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04382v1", "cate": "eess.SY", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03708", "title": "TaxSolver: A methodology to design optimal income tax reform", "authors": ["Mark Verhagen", "Menno Schellekens", "Michael Garstka"], "categories": ["econ.GN", "eess.SY", "q-fin.GN"], "primary_category": "q-fin.GN", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03708", "summary": "Across the developed world, there are growing calls to streamline and improve\never more complex income tax codes. Executing reform has proven difficult. Even\nwhen the desired outcomes of a reform are clear, the tools to design fitting\nreforms are lacking. To remedy this, we developed \\texttt{TaxSolver}: a\nmethodology to help policymakers realize optimal income tax reform.\n\\texttt{TaxSolver} allows policymakers to focus solely on what they aim to\nachieve with a reform -- like redistributing wealth, incentivizing labor market\nparticipation or reducing complexity -- and the guarantees within which reform\nis acceptable -- like limiting fluctuations in taxpayer incomes, protecting\nhouseholds from falling into poverty or shocks to overall tax revenue. Given\nthese goals and fiscal guarantees, \\texttt{TaxSolver} finds the optimal set of\ntax rules that satisfies all the criteria or shows that the set of demands are\nnot mathematically feasible. We illustrate \\texttt{TaxSolver} by reforming\nvarious simulated examples of tax codes, including some that reflect the\ncomplexity and size of a real-world tax system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03708", "cate": "q-fin.GN", "date": "2025-07-21", "updated": "2025-07-21", "section": "cross"}
{"id": "2410.13389", "title": "Dynamic Input Mapping Inversion to Eliminate Algebraic Loops in Hydraulic Actuator Control", "authors": ["Alessio Dallabona", "Patrik Schermann", "Mogens Blanke", "Dimitrios Papageorgiou"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13389", "summary": "The application of nonlinear control schemes to electro-hydraulic actuators\noften requires several alterations in the design of the controllers during\ntheir implementation. This is to overcome challenges that frequently arise in\nsuch control algorithms owing to model nonlinearities. Moreover, advanced\ncontrol solutions for this type of systems often introduce input algebraic\nloops that pose significant design and tuning difficulties. Conventional\nmethods to avoid such loops introduce chatter, which considerably degrade\ntracking performance and has oil degradation and wear as side effects. This\nstudy presents a nonlinear control architecture for hydraulic actuators that\ncomprises low-complexity modules that facilitate robust high performance in\ntracking and avoids the drawbacks of chatter. The salient feature is a dynamic\ninput-mapping inversion module that avoids algebraic loops in the control input\nand is followed by dedicated position control. The stability of the closed-loop\nsystem is analyzed using arguments from Lyapunov theory for cascaded\nnon-autonomous nonlinear systems. The effectiveness of the proposed solution is\nevaluated on a high-fidelity simulator of a wind turbine pitch system, and\nvalidated on a full-scale laboratory setup that includes a hydraulic pitch\nsystem and blade bearing. Appropriate quantitative metrics are used to evaluate\nthe closed-loop system performance in comparison to a state-of-the-art\nnonlinear design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13389", "cate": "eess.SY", "date": "2024-10-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2503.21502", "title": "ALADIN-$Î²$: A Distributed Optimization Algorithm for Solving MPCC Problems", "authors": ["Yifei Wang", "Shuting Wu", "Genke Yang", "Jian Chu", "Apostolos I. Rikos", "Xu Du"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.21502", "summary": "Mathematical Programs with Complementarity Constraints (MPCC) are critical in\nvarious real-world applications but notoriously challenging due to\nnon-smoothness and degeneracy from complementarity constraints. The\n$\\ell_1$-Exact Penalty-Barrier enhanced \\texttt{IPOPT} improves performance and\nrobustness by introducing additional inequality constraints and decision\nvariables. However, this comes at the cost of increased computational\ncomplexity due to the higher dimensionality and additional constraints\nintroduced in the centralized formulation. To mitigate this, we propose a\ndistributed structure-splitting reformulation that decomposes these inequality\nconstraints and auxiliary variables into independent sub-problems. Furthermore,\nwe introduce Augmented Lagrangian Alternating Direction Inexact Newton\n(ALADIN)-$\\beta$, a novel approach that integrates the $\\ell_1$-Exact\nPenalty-Barrier method with ALADIN to efficiently solve the distributed\nreformulation. Numerical experiments demonstrate that even without a\nglobalization strategy, the proposed distributed approach achieves fast\nconvergence while maintaining high precision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.21502", "cate": "eess.SY", "date": "2025-03-27", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.15978", "title": "Quantum-Enhanced Power Flow and Optimal Power Flow based on Combinatorial Reformulation", "authors": ["Zeynab Kaseb", "Matthias Moller", "Peter Palensky", "Pedro P. Vergara"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.15978", "summary": "This study introduces the Adiabatic Quantum Power Flow (AQPF) and Adiabatic\nQuantum Optimal Power Flow (AQOPF) algorithms to solve power flow (PF) and\noptimal power flow (OPF) problems, respectively. These algorithms utilize a\nnovel combinatorial optimization reformulation of classical PF and OPF\nproblems, and hence, enable their implementation on Ising machines, e.g.,\nquantum and quantum-inspired hardware. The experiments are conducted on\nstandard test cases ranging from 4-bus to 1354-bus systems, using D-Wave's\nAdvantage system (QA), its hybrid quantum-classical solver (HA), as well as the\nthird-generation Digital Annealer (DAv3) and Quantum-Inspired Integrated\nOptimization software (QIIO) developed by Fujitsu. The annealers are\nsystematically evaluated based on: (i) full and partitioned formulations, (ii)\nability to handle ill-conditioned cases, and (iii) scalability. The results are\nbenchmarked against the Newton-Raphson numerical method (NR) and suggest that\nAQPF and AQOPF can serve as effective solvers or complementary tools to\nclassical methods to address unsolved challenges in large-scale modern power\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.15978", "cate": "eess.SY", "date": "2025-05-21", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.11924", "title": "Advantages of Feedback in Distributed Data-Gathering for Accurate and Power-Efficient State-Estimation", "authors": ["Hyeongmin Choe", "SooJean Han"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11924", "summary": "In distributed target-tracking sensor networks, efficient data gathering\nmethods are necessary to save communication resources and assure information\naccuracy. This paper proposes a Feedback (FB) distributed data-gathering method\nwhich lets the central unit feed information back to the mobile sensors; each\nsensor then uses it to cancel redundant transmissions and reduce communication\ncongestion. We rigorously compare its performance, in terms of mean-squared\nerror (MSE) and cost of power per sensor, against more conventional\nNon-Feedback (NF) architectures by evaluating conditions of feasibility and\nadvantage under different architecture specifications (e.g., communication\ndelay rate, power cost rate, maximum back-off time, sampling period,\nobservation noise). Here, we defined the advantage as the performance gain\nachieved by FB over NF, while FB is said to be feasible if the advantage region\nis nonempty. Our theoretical analyses show that the feasibility of FB depends\nmore on the communication power cost, while the advantage depends on the\nsensors' propagation delay per transmission interval; we derive concrete\nconditions under which these outcomes hold. Using extensive numerical\nsimulations under a variety of settings, we confirm the accuracy of the derived\nconditions, and show that our theoretical results hold even for more complex\nscenarios where the simplifying assumptions no longer hold.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11924", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.13286", "title": "Privacy-Preserving Fusion for Multi-Sensor Systems Under Multiple Packet Dropouts", "authors": ["Jie Huang", "Jason J. R. Liu", "Xiao He"], "categories": ["eess.SY"], "primary_category": "eess.SY", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13286", "summary": "Wireless sensor networks (WSNs) are critical components in modern\ncyber-physical systems, enabling efficient data collection and fusion through\nspatially distributed sensors. However, the inherent risks of eavesdropping and\npacket dropouts in such networks pose significant challenges to secure state\nestimation. In this paper, we address the privacy-preserving fusion estimation\n(PPFE) problem for multi-sensor systems under multiple packet dropouts and\neavesdropping attacks. To mitigate these issues, we propose a distributed\nencoding-based privacy-preserving mechanism (PPM) within a control-theoretic\nframework, ensuring data privacy during transmission while maintaining the\nperformance of legitimate state estimation. A centralized fusion filter is\ndeveloped, accounting for the coupling effects of packet dropouts and the\nencoding-based PPM. Boundedness conditions for the legitimate user's estimation\nerror covariance are derived via a modified algebraic Riccati equation.\nAdditionally, by demonstrating the divergence of the eavesdropper's mean\nestimation error, the proposed PPFE algorithm's data confidentiality is\nrigorously analyzed. Simulation results for an Internet-based three-tank system\nvalidate the effectiveness of the proposed approach, highlighting its potential\nto enhance privacy without compromising estimation accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13286", "cate": "eess.SY", "date": "2025-07-17", "updated": "2025-08-06", "section": "repl"}
{"id": "2505.22784", "title": "Split the Yield, Share the Risk: Pricing, Hedging and Fixed rates in DeFi", "authors": ["Viraj Nadkarni", "Pramod Viswanath"], "categories": ["econ.TH", "eess.SY"], "primary_category": "econ.TH", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22784", "summary": "We present the first formal treatment of \\emph{yield tokenization}, a\nmechanism that decomposes yield-bearing assets into principal and yield\ncomponents to facilitate risk transfer and price discovery in decentralized\nfinance (DeFi). We propose a model that characterizes yield token dynamics\nusing stochastic differential equations. We derive a no-arbitrage pricing\nframework for yield tokens, enabling their use in hedging future yield\nvolatility and managing interest rate risk in decentralized lending pools.\nTaking DeFi lending as our focus, we show how both borrowers and lenders can\nuse yield tokens to achieve optimal hedging outcomes and mitigate exposure to\nadversarial interest rate manipulation. Furthermore, we design automated market\nmakers (AMMs) that incorporate a menu of bonding curves to aggregate liquidity\nfrom participants with heterogeneous risk preferences. This leads to an\nefficient and incentive-compatible mechanism for trading yield tokens and yield\nfutures. Building on these foundations, we propose a modular\n\\textit{fixed-rate} lending protocol that synthesizes on-chain yield token\nmarkets and lending pools, enabling robust interest rate discovery and\nenhancing capital efficiency. Our work provides the theoretical underpinnings\nfor risk management and fixed-income infrastructure in DeFi, offering practical\nmechanisms for stable and sustainable yield markets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22784", "cate": "econ.TH", "date": "2025-05-28", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04305", "title": "Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver Segmentation", "authors": ["Nathan Hollet", "Oumeymah Cherkaoui", "Philippe C. Cattin", "Sidaty El hadramy"], "categories": ["eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04305v1", "summary": "Liver segmentation is essential for preoperative planning in interventions\nlike tumor resection or transplantation, but implementation in clinical\nworkflows faces challenges due to modality-specific tools and data scarcity. We\npropose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentation\nthat generalizes to out-of-distribution (OOD) data. Our method integrates\nclassical edge detection with foundation models. Modality-agnostic edge maps\nare first extracted from input images, then processed by a U-Net to generate\nlogit-based prompts. These prompts condition the Segment Anything Model 2\n(SAM-2) to generate 2D liver segmentations, which can then be reconstructed\ninto 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Prompt\nachieves competitive results compared to classical segmentation methods when\ntrained and tested in-distribution (ID), and outperforms them in data-scarce\nscenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Score\nof 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and other\nself-prompting methods by 9.1%, demonstrating its effectiveness. This work\nbridges classical and foundation models for clinically adaptable,\ndata-efficient segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04305v1", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04404", "title": "Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics Using Dynamic Susceptibility Contrast MRI", "authors": ["Marijn Borghouts", "Richard McKinley", "Josien Pluim", "Manuel KÃ¶stner", "Roland Wiest", "Ruisheng Su"], "categories": ["eess.IV"], "primary_category": "eess.IV", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04404v1", "summary": "Distinguishing acute ischemic strokes (AIS) from stroke mimics (SMs),\nparticularly in cases involving medium and small vessel occlusions, remains a\nsignificant diagnostic challenge. While computed tomography (CT) based\nprotocols are commonly used in emergency settings, their sensitivity for\ndetecting distal occlusions is limited. This study explores the potential of\nmagnetic resonance perfusion (MRP) imaging as a tool for differentiating distal\nAIS from epileptic seizures, a prevalent SM. Using a retrospective dataset of\n162 patients (129 AIS, 33 seizures), we extracted region-wise perfusion map\ndescriptors (PMDs) from dynamic susceptibility contrast (DSC) images.\nStatistical analyses identified several brain regions, located mainly in the\ntemporal and occipital lobe, exhibiting significant group differences in\ncertain PMDs. Hemispheric asymmetry analyses further highlighted these regions\nas discriminative. A logistic regression model trained on PMDs achieved an area\nunder the receiver operating characteristic (AUROC) curve of 0.90, and an area\nunder the precision recall curve (AUPRC) of 0.74, with a specificity of 92% and\na sensitivity of 73%, suggesting strong performance in distinguishing distal\nAIS from seizures. These findings support further exploration of MRP-based PMDs\nas interpretable features for distinguishing true strokes from various mimics.\nThe code is openly available at our GitHub\nhttps://github.com/Marijn311/PMD_extraction_and_analysis{github.com/Marijn311/PMD\\_extraction\\_and\\_analysis", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04404v1", "cate": "eess.IV", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04223", "title": "Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Shuai Liu", "Hongyang Du", "Geyong Min"], "categories": ["eess.IV", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04223", "summary": "Digital task-oriented semantic communication (ToSC) aims to transmit only\ntask-relevant information, significantly reducing communication overhead.\nExisting ToSC methods typically rely on learned codebooks to encode semantic\nfeatures and map them to constellation symbols. However, these codebooks are\noften sparsely activated, resulting in low spectral efficiency and\nunderutilization of channel capacity. This highlights a key challenge: how to\ndesign a codebook that not only supports task-specific inference but also\napproaches the theoretical limits of channel capacity. To address this\nchallenge, we construct a spectral efficiency-aware codebook design framework\nthat explicitly incorporates the codebook activation probability into the\noptimization process. Beyond maximizing task performance, we introduce the\nWasserstein (WS) distance as a regularization metric to minimize the gap\nbetween the learned activation distribution and the optimal channel input\ndistribution. Furthermore, we reinterpret WS theory from a generative\nperspective to align with the semantic nature of ToSC. Combining the above two\naspects, we propose a WS-based adaptive hybrid distribution scheme, termed\nWS-DC, which learns compact, task-driven and channel-aware latent\nrepresentations. Experimental results demonstrate that WS-DC not only\noutperforms existing approaches in inference accuracy but also significantly\nimproves codebook efficiency, offering a promising direction toward\ncapacity-approaching semantic communication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04223", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04291", "title": "Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Hongyang Du", "Haojin Li", "Chen Sun", "Haijun Zhang"], "categories": ["eess.IV", "eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04291", "summary": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04291", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04046", "title": "Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints", "authors": ["Xiao Tong", "Lei Lei", "Ang Li", "A. Lee Swindlehurst", "Symeon Chatzinotas"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04046v1", "summary": "This paper investigates constructive interference (CI)-based waveform design\nfor phase shift keying and quadrature amplitude modulation symbols under\nrelaxed block-level power constraints in multi-user multiple-input\nsingle-output (MU-MIMO) communication systems. Existing linear CI-based\nprecoding methods, including symbol-level precoding (SLP) and block-level\nprecoding (BLP), suffer from performance limitations due to strict symbol-level\npower budgets or insufficient degrees of freedom over the block. To overcome\nthese challenges, we propose a nonlinear waveform optimization framework that\nintroduces additional optimization variables and maximizes the minimum CI\nmetric across the transmission block. The optimal waveform is derived in closed\nform using the function and Karush Kuhn Tucker conditions, and the solution is\nexplicitly expressed with respect to the dual variables. Moreover, the original\nproblems are equivalently reformulated as tractable quadratic programming (QP)\nproblems. To efficiently solve the derived QP problems, we develop an improved\nalternating direction method of multipliers (ADMM) algorithm by integrating a\nlinear-time projection technique, which significantly enhances the\ncomputational efficiency. Simulation results demonstrate that the proposed\nalgorithms substantially outperform the conventional CI-SLP and CI-BLP\napproaches, particularly under high-order modulations and large block lengths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04046v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04068", "title": "WiFo-CF: Wireless Foundation Model for CSI Feedback", "authors": ["Liu Xuanyu", "Gao Shijian", "Liu Boxun", "Cheng Xiang", "Yang Liuqing"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04068v1", "summary": "Deep learning-based channel state information (CSI) feedback schemes\ndemonstrate strong compression capabilities but are typically constrained to\nfixed system configurations, limiting their generalization and flexibility. To\naddress this challenge, WiFo-CF, a novel wireless foundation model tailored for\nCSI feedback, is proposed, uniquely accommodating heterogeneous configurations\nsuch as varying channel dimensions, feedback rates, and data distributions\nwithin a unified framework through its key innovations: (1) a multi-user,\nmulti-rate self-supervised pre-training strategy; and (2) a Mixture of Shared\nand Routed Expert (S-R MoE) architecture. Supporting the large-scale\npre-training of WiFo-CF is the first heterogeneous channel feedback dataset,\nwhose diverse patterns enable the model to achieve superior performance on both\nin-distribution and out-of-distribution data across simulated and real-world\nscenarios. Furthermore, the learned representations effectively facilitate\nadaptation to downstream tasks such as CSI-based indoor localization,\nvalidating WiFo-CF's scalability and deployment potential.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04068v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04075", "title": "DFT-s-OFDM with Chirp Modulation", "authors": ["Yujie Liu", "Yong Liang Guan", "David GonzÃ¡lez G.", "Halim Yanikomeroglu"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04075v1", "summary": "In this paper, a new waveform called discrete Fourier transform spread\northogonal frequency division multiplexing with chirp modulation\n(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.\nThe information bits are conveyed by not only Q-ary constellation symbols but\nalso the starting frequency of chirp signal. It could maintain the benefits\nprovided by the chirped discrete Fourier transform spread orthogonal frequency\ndivision multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio\n(PAPR), full frequency diversity exploitation, etc. Simulation results confirm\nthat the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while\nkeeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In\naddition, when maintaining the same spectral efficiency, the proposed\nDFT-s-OFDM-CM with the splitting of information bits into two streams enables\nthe use of lower-order constellation modulation and offers greater resilience\nto noise, resulting in a lower BER than the chirped DFT-s-OFDM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04075v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04128", "title": "Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving", "authors": ["Di Wu", "Yifei Jia", "Siyuan Li", "Shiqi Zhao", "Jie Yang", "Mohamad Sawan"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04128v1", "summary": "Neurophysiological decoding, fundamental to advancing brain-computer\ninterface (BCI) technologies, has significantly benefited from recent advances\nin deep learning. However, existing decoding approaches largely remain\nconstrained to single-task scenarios and individual subjects, limiting their\nbroader applicability and generalizability. Efforts towards creating\nlarge-scale neurophysiological foundation models have shown promise, but\ncontinue to struggle with significant challenges due to pervasive data\nheterogeneity across subjects and decoding tasks. Simply increasing model\nparameters and dataset size without explicitly addressing this heterogeneity\nfails to replicate the scaling successes seen in natural language processing.\nHere, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),\na general-purpose decoding framework explicitly designed to manage the\nubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE\nincorporates a brain-regional-temporal embedding mechanism combined with a\nmixture-of-experts approach, assigning neural signals from distinct brain\nregions to specialized regional experts on a unified embedding basis, thus\nexplicitly resolving both structural and functional heterogeneity.\nAdditionally, our region-masked autoencoding pre-training strategy further\nenhances representational consistency among subjects, complemented by a\ntask-disentangled information aggregation method tailored to effectively handle\ntask-specific neural variations. Evaluations conducted on intracranial\nrecordings from 11 subjects across five diverse tasks, including complex\nlanguage decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE\nsurpasses prior art and exhibits robust generalization for zero-shot decoding\non unseen subjects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04128v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04144", "title": "Dual-Function Radar-Communication Beamforming with Outage Probability Metric", "authors": ["Hossein Maleki", "Carles Diaz-Vilor", "Ali Pezeshki", "Vahid Tarokh", "Hamid Jafarkhani"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04144v1", "summary": "The integrated design of communication and sensing may offer a potential\nsolution to address spectrum congestion. In this work, we develop a beamforming\nmethod for a dual-function radar-communication system, where the transmit\nsignal is used for both radar surveillance and communication with multiple\ndownlink users, despite imperfect channel state information (CSI). We focus on\ntwo scenarios of interest: radar-centric and communication-centric. In the\nradar-centric scenario, the primary goal is to optimize radar performance while\nattaining acceptable communication performance. To this end, we minimize a\nweighted sum of the mean-squared error in achieving a desired beampattern and a\nmean-squared cross correlation of the radar returns from directions of interest\n(DOI). We also seek to ensure that the probability of outage for the\ncommunication users remains below a desired threshold. In the\ncommunication-centric scenario, our main objective is to minimize the maximum\nprobability of outage among the communication users while keeping the\naforementioned radar metrics below a desired threshold. Both optimization\nproblems are stochastic and untractable. We first take advantage of central\nlimit theorem to obtain deterministic non-convex problems and then consider\nrelaxations of these problems in the form of semidefinite programs with rank-1\nconstraints. We provide numerical experiments demonstrating the effectiveness\nof the proposed designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04144v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04169", "title": "Subspace Fitting Approach for Wideband Near-Field Localization", "authors": ["Ruiyun Zhang", "Zhaolin Wang", "Zhiqing Wei", "Yuanwei Liu", "Zehui Xiong", "Zhiyong Feng"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04169v1", "summary": "Two subspace fitting approaches are proposed for wideband near-field\nlocalization. Unlike in conventional far-field systems, where distance and\nangle can be estimated separately, spherical wave propagation in near-field\nsystems couples these parameters. We therefore derive a frequency-domain\nnear-field signal model for multi-target wideband systems and develop a\nsubspace fitting-based MUSIC method that jointly estimates distance and angle.\nTo reduce complexity, a Fresnel approximation MUSIC algorithm is further\nintroduced to decouple the distance and angle parameters. Numerical results\nverify the effectiveness of both proposed approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04169v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04185", "title": "Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems", "authors": ["Evangelos Koutsonas", "Xiaonan Mu", "Nan Qi", "Stylianos Trevlakis", "Theodoros A. Tsiftsis", "Alexandros-Apostolos A. Boulogeorgos"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04185v1", "summary": "Integration of RIS in radio access networks requires signaling between edge\nunits and the RIS microcontroller (MC). Unfortunately, in several practical\nscenarios, the signaling latency is higher than the communication channel\ncoherence time, which causes outdated signaling at the RIS. To counterbalance\nthis, we introduce a simultaneous information and control signaling (SICS)\nprotocol that enables operation adaptation through wireless control signal\ntransmission. SICS assumes that the MC is equipped with a single antenna that\noperates at the same frequency as the RIS. RIS operates in simultaneous\ntransmission and reflection (STAR) mode, and the source employs non-orthogonal\nmultiple access (NOMA) to superposition the information signal to the control\nsignal. To maximize the achievable user data rate while ensuring the MC's\nability to decode the control signal, we formulate and solve the corresponding\noptimization problem that returns RIS's reflection and transmission\ncoefficients as well as the superposition coefficients of the NOMA scheme. Our\nresults reveal the robustness of the SICS approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04185v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04222", "title": "Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP", "authors": ["Thibaut Ceulemans", "Cel Thys", "Robbert Beerten", "Zhuangzhuang Cui", "Sofie Pollin"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04222v1", "summary": "This work focuses on channel estimation in extremely large aperture array\n(ELAA) systems, where near-field propagation and spatial non-stationarity\nintroduce complexities that hinder the effectiveness of traditional estimation\ntechniques. A physics-based hybrid channel model is developed, incorporating\nnon-binary visibility region (VR) masks to simulate diffraction-induced power\nvariations across the antenna array. To address the estimation challenges posed\nby these channel conditions, a novel algorithm is proposed:\nVisibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching\nPursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework\nby integrating VR estimation through a hidden Markov model (HMM), using a novel\nemission formulation and Viterbi decoding. This allows the algorithm to\nadaptively mask steering vectors and account for spatial non-stationarity at\nthe antenna level. Simulation results demonstrate that the proposed method\nenhances estimation accuracy compared to existing techniques, particularly in\nlow-SNR and sparse scenarios, while maintaining a low computational complexity.\nThe algorithm presents robustness across a range of design parameters and\nchannel conditions, offering a practical solution for ELAA systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04222v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04240", "title": "ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening", "authors": ["Sitong Chen", "Beiqianyi Li", "Cuilin He", "Dongyang Li", "Mingyang Wu", "Xinke Shen", "Song Wang", "Xuetao Wei", "Xindi Wang", "Haiyan Wu", "Quanying Liu"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04240v1", "summary": "EEG-based neural decoding requires large-scale benchmark datasets. Paired\nbrain-language data across speaking, listening, and reading modalities are\nessential for aligning neural activity with the semantic representation of\nlarge language models (LLMs). However, such datasets are rare, especially for\nnon-English languages. Here, we present ChineseEEG-2, a high-density EEG\ndataset designed for benchmarking neural decoding models under real-world\nlanguage tasks. Building on our previous ChineseEEG dataset, which focused on\nsilent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and\nPassive Listening (PL), using the same Chinese corpus. EEG and audio were\nsimultaneously recorded from four participants during ~10.7 hours of reading\naloud. These recordings were then played to eight other participants,\ncollecting ~21.6 hours of EEG during listening. This setup enables speech\ntemporal and semantic alignment across the RA and PL modalities. ChineseEEG-2\nincludes EEG signals, precise audio, aligned semantic embeddings from\npre-trained language models, and task labels. Together with ChineseEEG, this\ndataset supports joint semantic alignment learning across speaking, listening,\nand reading. It enables benchmarking of neural decoding algorithms and promotes\nbrain-LLM alignment under multimodal language tasks, especially in Chinese.\nChineseEEG-2 provides a benchmark dataset for next-generation neural semantic\ndecoding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04240v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04253", "title": "Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond", "authors": ["Yiyan Ma", "Bo Ai", "Jinhong Yuan", "Shuangyang Li", "Qingqing Cheng", "Zhenguo Shi", "Weijie Yuan", "Zhiqiang Wei", "Akram Shafie", "Guoyu Ma", "Yunlong Lu", "Mi Yang", "Zhangdui Zhong"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04253v1", "summary": "High-mobility scenarios will be a critical part of 6G systems. Since the\nwidely deployed orthogonal frequency division multiplexing (OFDM) waveform\nsuffers from subcarrier orthogonality loss under severe Doppler spread,\ndelay-Doppler domain multi-carrier (DDMC) modulation systems, such as\northogonal time frequency space (OTFS), have been extensively studied. While\nOTFS can exploit time-frequency (TF) domain channel diversity, it faces\nchallenges including high receiver complexity and inflexible TF resource\nallocation, making OFDM still the most promising waveform for 6G. In this\narticle, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme\nto enhance OFDM performance based on DDMC research insights. First, we design a\nDD-a-OFDM system structure, retaining the classical OFDM transceiver while\nincorporating DD domain channel estimation and TF domain equalization. Second,\nwe detail DD domain channel estimation using discrete TF pilots and prove that\nTF domain inter-carrier interference (ICI) could be transformed into DD domain\nGaussian interference. Third, we derive closed-form Cram\\'{e}r-Rao lower bounds\n(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood\n(ML) and peak detection-based channel estimators, along with a corresponding TF\ndomain equalizer. Numerical results verify the proposed design, showing that\nDD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and\noutperforms OTFS in channel estimation accuracy with lower pilot overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04253v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04322", "title": "Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications", "authors": ["Ruopeng Xu", "Zhaohui Yang", "Zhaoyang Zhang", "Mohammad Shikh-Bahaei", "Kaibin Huang", "Dusit Niyato"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04322v1", "summary": "In this paper, we propose an energy efficient wireless communication system\nbased on fluid antenna relay (FAR) to solve the problem of non-line-of-sight\n(NLoS) links caused by blockages with considering the physical properties.\nDriven by the demand for the sixth generation (6G) communication, fluid antenna\nsystems (FASs) have become a key technology due to their flexibility in\ndynamically adjusting antenna positions. Existing research on FAS primarily\nfocuses on line-of-sight (LoS) communication scenarios, and neglects the\nsituations where only NLoS links exist. To address the issues posted by NLoS\ncommunication, we design an FAR-assisted communication system combined with\namplify-and-forward (AF) protocol. In order to alleviate the high energy\nconsumption introduced by AF protocol while ensuring communication quality, we\nformulate an energy efficiency (EE) maximization problem. By optimizing the\npositions of the fluid antennas (FAs) on both sides of the FAR, we achieve\ncontrollable phase shifts of the signals transmitting through the blockage\nwhich causes the NLoS link. Besides, we establish a channel model that jointly\nconsiders the blockage-through matrix, large-scale fading, and small-scale\nfading. To maximize the EE of the system, we jointly optimize the FAR position,\nFA positions, power control, and beamforming design under given constraints,\nand propose an iterative algorithm to solve this formulated optimization\nproblem. Simulation results show that the proposed algorithm outperforms the\ntraditional schemes in terms of EE, achieving up to $23.39\\%$ and $39.94\\%$\nhigher EE than the conventional reconfigurable intelligent surface (RIS) scheme\nand traditional AF relay scheme, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04322v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04331", "title": "Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination", "authors": ["Mohamadreza Delbari", "Qikai Zhou", "Robin Neuder", "Alejandro JimÃ©nez-SÃ¡ez", "Vahid Jamali"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04331v1", "summary": "Liquid crystal (LC) technology provides a low-power and scalable approach to\nimplement a reconfigurable intelligent surface (RIS). However, the LC-based\nRIS's phase-shift response is inherently frequency-dependent, which can lead to\nperformance degradation if not properly addressed. This issue becomes\nespecially critical in secure communication systems, where such variations may\nresult in considerable information leakage. To avoid the need for full channel\nstate information (CSI) acquisition and frequent RIS reconfiguration, we design\nRIS for a wideband orthogonal frequency division multiplexing (OFDM) system to\nilluminate a desired area containing legitimate users while avoiding leakage to\nregions where potential eavesdroppers may be located. Our simulation results\ndemonstrate that the proposed algorithm improves the secrecy rate compared to\nmethods that neglect frequency-dependent effects. In the considered setup, the\nproposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz\nbandwidth when the center frequency is 60 GHz.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04331v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04570", "title": "Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming", "authors": ["A. Tarik Leblebici", "Sumeyra Hassan", "Erdal Panayirci", "H. Vincent Poor"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04570v1", "summary": "This paper proposes a joint communication and indoor positioning (JCP) system\nbased on visible light communication (VLC) designed for high-precision indoor\nenvironments. The framework supports 2D and 3D positioning using received\nsignal strength (RSS) from pilot transmissions, enhanced by the radical axis\ntheorem to improve accuracy under measurement uncertainties. Communication is\nachieved using spatial modulation (SM) with M-ary pulse amplitude modulation\n(PAM), where data is conveyed through the modulation symbol and the active\nlight-emitting diode (LED) index, improving spectral efficiency while\nmaintaining low complexity. A pilot-aided least squares (LS) estimator is\nemployed for joint channel and dimming coefficient estimation, enabling robust\nsymbol detection in multipath environments characterized by both line-of-sight\n(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician\nfading. The proposed system incorporates a dimming control mechanism to meet\nlighting requirements while maintaining reliable communication and positioning\nperformance. Simulation results demonstrate sub-centimeter localization\naccuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below\n10^{-6} for low-order PAM schemes. Additionally, comparative analysis across\nuser locations reveals that positioning and communication performance improve\nsignificantly near the geometric center of the LED layout. These findings\nvalidate the effectiveness of the proposed system for future 6G indoor networks\nrequiring integrated localization and communication under practical channel\nconditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04570v1", "cate": "eess.SP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04685", "title": "Phase-Pole-Free Images and Smooth Coil Sensitivity Maps by Regularized Nonlinear Inversion", "authors": ["Moritz Blumenthal", "Martin Uecker"], "categories": ["eess.SP", "physics.med-ph"], "primary_category": "physics.med-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04685", "summary": "Purpose: Phase singularities are a common problem in image reconstruction\nwith auto-calibrated sensitivities due to an inherent ambiguity of the\nestimation problem. The purpose of this work is to develop a method for\ndetecting and correcting phase poles in non-linear inverse (NLINV)\nreconstruction of MR images and coil sensitivity maps. Methods: Phase poles are\ndetected in individual coil sensitivity maps by computing the curl in each\npixel. A weighted average of the curl in each coil is computed to detect phase\npoles. Phase pole detection and correction is then integrated into the\niteratively regularized Gauss-Newton method of the NLINV algorithm, which then\navoid phase singularities in the reconstructed images. The method is evaluated\nfor reconstruction of accelerated Cartesian MPRAGE data of the brain and\ninteractive radial real-time MRI of the human heart. Results: Phase poles are\nreliably removed in NLINV reconstructions for both applications. NLINV with\nphase pole correction can reliably and efficiently estimate coil sensitivity\nprofiles free from singularities even from very small ($7\\times7$)\nauto-calibration (AC) regions. Conclusion: NLINV emerges as an efficient and\nreliable tool for image reconstruction and coil sensitivity estimation in\nchallenging MRI applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04685", "cate": "physics.med-ph", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2503.17667", "title": "DGAR: A Unified Domain Generalization Framework for RF-Based Human Activity Recognition", "authors": ["Junshuo Liu", "Xin Shi", "Yunchuan Zhang", "Yinhao Ge", "Robert C. Qiu"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17667", "summary": "Radio-frequency (RF)-based human activity recognition (HAR) provides a\ncontactless and privacy-preserving solution for monitoring human behavior in\napplications such as astronaut extravehicular activity monitoring,\nhuman-autonomy collaborative cockpit, and unmanned aerial vehicle surveillance.\nHowever, real-world deployments usually face the challenge of domain knowledge\nshifts arising from inter-subject variability, heterogeneous physical\nenvironments, and unseen activity patterns, resulting in significant\nperformance degradation. To address this issue, we propose DGAR, a\ndomain-generalized activity recognition framework that learns transferable\nrepresentations without collecting data from the target domain. DGAR integrates\ninstance-adaptive feature modulation with cross-domain distribution alignment\nto enhance both personalization and generalization. Specifically, it\nincorporates a squeeze-and-excitation (SE) block to extract salient\nspatiotemporal features and employs correlation alignment to mitigate\ninter-domain discrepancies. Extensive experiments on public RF-based datasets\n-- HUST-HAR, Lab-LFM, and Office-LFM -- demonstrate that DGAR consistently\noutperforms state-of-the-art baselines, achieving up to a 5.81% improvement in\nweighted F1-score. The empirical results substantiate the generalization\ncapability of DGAR in real-time RF sensing across dynamic scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17667", "cate": "eess.SP", "date": "2025-03-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.21696", "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN", "authors": ["Abdelaziz Salama", "Zeinab Nezami", "Mohammed M. H. Qazzaz", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "categories": ["eess.SP"], "primary_category": "eess.SP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21696", "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21696", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.03937", "title": "LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription Robustness", "authors": ["Zongli Ye", "Jiachen Lian", "Akshaj Gupta", "Xuanru Zhou", "Krish Patel", "Haodong Li", "Hwi Joo Park", "Chenxu Guo", "Shuhe Li", "Sam Wang", "Cheol Jun Cho", "Zoe Ezzes", "Jet M.J. Vonk", "Brittany T. Morin", "Rian Bogley", "Lisa Wauters", "Zachary A. Miller", "Maria Luisa Gorno-Tempini", "Gopala Anumanchipalli"], "categories": ["eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03937v1", "summary": "Phonetic speech transcription is crucial for fine-grained linguistic analysis\nand downstream speech applications. While Connectionist Temporal Classification\n(CTC) is a widely used approach for such tasks due to its efficiency, it often\nfalls short in recognition performance, especially under unclear and nonfluent\nspeech. In this work, we propose LCS-CTC, a two-stage framework for\nphoneme-level speech recognition that combines a similarity-aware local\nalignment algorithm with a constrained CTC training objective. By predicting\nfine-grained frame-phoneme cost matrices and applying a modified Longest Common\nSubsequence (LCS) algorithm, our method identifies high-confidence alignment\nzones which are used to constrain the CTC decoding path space, thereby reducing\noverfitting and improving generalization ability, which enables both robust\nrecognition and text-free forced alignment. Experiments on both LibriSpeech and\nPPA demonstrate that LCS-CTC consistently outperforms vanilla CTC baselines,\nsuggesting its potential to unify phoneme modeling across fluent and non-fluent\nspeech.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03937v1", "cate": "eess.AS", "date": "2025-08-05", "updated": "2025-08-05", "section": "new"}
{"id": "2508.04512", "title": "Pitfalls and Limits in Automatic Dementia Assessment", "authors": ["Franziska Braun", "Christopher Witzl", "Andreas Erzigkeit", "Hartmut Lehfeld", "Thomas Hillemacher", "Tobias Bocklet", "Korbinian Riedhammer"], "categories": ["eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04512v1", "summary": "Current work on speech-based dementia assessment focuses on either feature\nextraction to predict assessment scales, or on the automation of existing test\nprocedures. Most research uses public data unquestioningly and rarely performs\na detailed error analysis, focusing primarily on numerical performance. We\nperform an in-depth analysis of an automated standardized dementia assessment,\nthe Syndrom-Kurz-Test. We find that while there is a high overall correlation\nwith human annotators, due to certain artifacts, we observe high correlations\nfor the severely impaired individuals, which is less true for the healthy or\nmildly impaired ones. Speech production decreases with cognitive decline,\nleading to overoptimistic correlations when test scoring relies on word naming.\nDepending on the test design, fallback handling introduces further biases that\nfavor certain groups. These pitfalls remain independent of group distributions\nin datasets and require differentiated analysis of target groups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04512v1", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04585", "title": "UniTalker: Conversational Speech-Visual Synthesis", "authors": ["Yifan Hu", "Rui Liu", "Yi Ren", "Xiang Yin", "Haizhou Li"], "categories": ["eess.AS"], "primary_category": "eess.AS", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04585v1", "summary": "Conversational Speech Synthesis (CSS) is a key task in the user-agent\ninteraction area, aiming to generate more expressive and empathetic speech for\nusers. However, it is well-known that \"listening\" and \"eye contact\" play\ncrucial roles in conveying emotions during real-world interpersonal\ncommunication. Existing CSS research is limited to perceiving only text and\nspeech within the dialogue context, which restricts its effectiveness.\nMoreover, speech-only responses further constrain the interactive experience.\nTo address these limitations, we introduce a Conversational Speech-Visual\nSynthesis (CSVS) task as an extension of traditional CSS. By leveraging\nmultimodal dialogue context, it provides users with coherent audiovisual\nresponses. To this end, we develop a CSVS system named UniTalker, which is a\nunified model that seamlessly integrates multimodal perception and multimodal\nrendering capabilities. Specifically, it leverages a large-scale language model\nto comprehensively understand multimodal cues in the dialogue context,\nincluding speaker, text, speech, and the talking-face animations. After that,\nit employs multi-task sequence prediction to first infer the target utterance's\nemotion and then generate empathetic speech and natural talking-face\nanimations. To ensure that the generated speech-visual content remains\nconsistent in terms of emotion, content, and duration, we introduce three key\noptimizations: 1) Designing a specialized neural landmark codec to tokenize and\nreconstruct facial expression sequences. 2) Proposing a bimodal speech-visual\nhard alignment decoding strategy. 3) Applying emotion-guided rendering during\nthe generation stage. Comprehensive objective and subjective experiments\ndemonstrate that our model synthesizes more empathetic speech and provides\nusers with more natural and emotionally consistent talking-face animations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04585v1", "cate": "eess.AS", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04008", "title": "Leveraging Minute-by-Minute Soccer Match Event Data to Adjust Team's Offensive Production for Game Context", "authors": ["Andrey Skripnikov", "Ahmet Cemek", "David Gillman"], "categories": ["stat.AP"], "primary_category": "stat.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04008v1", "summary": "In soccer, game context can result in skewing offensive statistics in ways\nthat might misrepresent how well a team has played. For instance, in England's\n1-2 loss to France in the 2022 FIFA World Cup quarterfinal, England attempted\nconsiderably more shots (16 to France's 8) and more corners (5 to 2),\npotentially suggesting they played better despite the loss. However, these\nstatistics were largely accumulated when France was ahead and more willing to\nconcede offensive initiative to England. To explore how game context influences\noffensive performance, we analyze minute-by-minute event-sequenced match data\nfrom 15 seasons across five major European leagues. Using count-response\nGeneralized Additive Modeling, we consider features such as score and red card\ndifferential, home/away status, pre-match win probabilities, and game minute.\nMoreover, we leverage interaction terms to test several intuitive hypotheses\nabout how these features might cooperate in explaining offensive production.\nThe selected model is then applied to project offensive statistics onto a\nstandardized \"common denominator\" scenario: a tied home game with even men on\nboth sides. The adjusted numbers - in contrast to regular game totals that\ndisregard game context - offer a more contextualized comparison, reducing the\nlikelihood of misrepresenting the relative quality of play.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04008v1", "cate": "stat.AP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04074", "title": "Matrix Factorization-Based Solar Spectral Irradiance Missing Data Imputation with Uncertainty Quantification", "authors": ["Yuxuan Ke", "Xianglei Huang", "Odele Coddington", "Yang Chen"], "categories": ["stat.AP"], "primary_category": "stat.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04074v1", "summary": "The solar spectral irradiance (SSI) depicts the spectral distribution of\nsolar energy flux reaching the top of the Earth's atmosphere. The SSI data\nconstitute a matrix with spectrally (rows) and temporally (columns) resolved\nsolar energy flux measurements. The most recent SSI measurements have been made\nby NASA's Total and Spectral Solar Irradiance Sensor-1 (TSIS-1) Spectral\nIrradiance Monitor (SIM) since March 2018. This data have considerable missing\ndata due to both random factors and instrument downtime, a periodic trend\nrelated to the Sun's cyclical magnetic activity, and varying degrees of\ncorrelation among the spectra, some approaching unity. We propose a novel\nlow-rank matrix factorization method that uses autoregressive regularization\nand periodic spline detrending to recover the missingness. The method is a\ntwo-step procedure, each of which tackles scattered and downtime missingness,\nrespectively. We design efficient alternating algorithms to jointly estimate\nthe model parameters. Moreover, we build a distribution-free uncertainty\nquantification method using conformal prediction. We validate the prediction\ninterval coverage rates and assess the imputation accuracy against competing\nmodels such as Gaussian process regression and linear time series smoothing via\nnumerical experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04074v1", "cate": "stat.AP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.04533", "title": "Cluster-specific ranking and variable importance for Scottish regional deprivation via vine mixtures", "authors": ["Ãzge Åahin", "Ozan Evkaya", "Ariane Hanebeck"], "categories": ["stat.AP"], "primary_category": "stat.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04533v1", "summary": "Socioeconomic deprivation is a key determinant of public health, as\nhighlighted by the Scottish Government's Scottish Index of Multiple Deprivation\n(SIMD). We propose an approach for clustering Scottish zones based on multiple\ndeprivation indicators using vine mixture models. This framework uses the\nflexibility of vine copulas to capture tail dependent and asymmetric\nrelationships among the indicators. From the fitted vine mixture model, we\nobtain posterior probabilities for each zone's membership in clusters. This\nallows the construction of a cluster-driven deprivation ranking by sorting\nzones according to their probability of belonging to the most deprived cluster.\nTo assess variable importance in this unsupervised learning setting, we adopt a\nleave-one-variable-out procedure by refitting the model without each variable\nand calculating the resulting change in the Bayesian information criterion. Our\nanalysis of 21 continuous indicators across 1964 zones in Glasgow and the\nsurrounding areas in Scotland shows that socioeconomic measures, particularly\nincome and employment rates, are major drivers of deprivation, while certain\nhealth- and crime-related indicators appear less influential. These findings\nare consistent across the approach of variable importance and the analysis of\nthe fitted vine structures of the identified clusters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04533v1", "cate": "stat.AP", "date": "2025-08-06", "updated": "2025-08-06", "section": "new"}
{"id": "2508.03704", "title": "Novel Risk Measures for Portfolio Optimization Using Equal-Correlation Portfolio Strategy", "authors": ["Biswarup Chakraborty"], "categories": ["q-fin.PM", "stat.AP"], "primary_category": "q-fin.PM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03704", "summary": "Portfolio optimization has long been dominated by covariance-based\nstrategies, such as the Markowitz Mean-Variance framework. However, these\napproaches often fail to ensure a balanced risk structure across assets,\nleading to concentration in a few securities. In this paper, we introduce novel\nrisk measures grounded in the equal-correlation portfolio strategy, aiming to\nconstruct portfolios where each asset maintains an equal correlation with the\noverall portfolio return. We formulate a mathematical optimization framework\nthat explicitly controls portfolio-wide correlation while preserving desirable\nrisk-return trade-offs. The proposed models are empirically validated using\nhistorical stock market data. Our findings show that portfolios constructed via\nthis approach demonstrate superior risk diversification and more stable returns\nunder diverse market conditions. This methodology offers a compelling\nalternative to conventional diversification techniques and holds practical\nrelevance for institutional investors, asset managers, and quantitative trading\nstrategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03704", "cate": "q-fin.PM", "date": "2025-07-20", "updated": "2025-07-20", "section": "cross"}
{"id": "2508.03834", "title": "Exact and Conservative Inference for the Average Treatment Effect in Stratified Experiments with Binary Outcomes", "authors": ["Jiaxun Li", "Jacob Spertus", "Philip B. Stark"], "categories": ["stat.AP", "stat.ME"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03834", "summary": "We extend methods for finite-sample inference about the average treatment\neffect (ATE) in randomized experiments with binary outcomes to accommodate\nstratification (blocking). We present three valid methods that differ in their\ncomputational and statistical efficiency. The first method constructs\nconservative, Bonferroni-adjusted confidence intervals separately for the mean\nresponse in the treatment and control groups in each stratum, then takes\nappropriate weighted differences of their endpoints to find a confidence\ninterval for the ATE. The second method inverts permutation tests for the\noverall ATE, maximizing the $P$-value over all ways a given ATE can be\nattained. The third method applies permutation tests for the ATE in separate\nstrata, then combines those tests to form a confidence interval for the overall\nATE. We compare the statistical and computational performance of the methods\nusing simulations and a case study. The second approach is most efficient\nstatistically in the simulations, but a naive implementation requires\nO(\\Pi_{k=1}^{K} n_{k}^{4}) permutation tests, the highest computational burden\namong the three methods. That computational burden can be reduced to\nO(\\sum_{k=1}^K n_k \\times\\Pi_{k=1}^{K} n_{k}^{2}) if all strata are balanced\nand to O(\\Pi_{k=1}^{K} n_{k}^{3}) otherwise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03834", "cate": "stat.ME", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03845", "title": "Operational convection-permitting COSMO/ICON ensemble predictions at observation sites (CIENS)", "authors": ["Sebastian Lerch", "Benedikt Schulz", "Reinhold Hess", "Annette MÃ¶ller", "Cristina Primo", "Sebastian Trepte", "Susanne Theis"], "categories": ["physics.ao-ph", "stat.AP"], "primary_category": "physics.ao-ph", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03845", "summary": "We present the CIENS dataset, which contains ensemble weather forecasts from\nthe operational convection-permitting numerical weather prediction model of the\nGerman Weather Service. It comprises forecasts for 55 meteorological variables\nmapped to the locations of synoptic stations, as well as additional spatially\naggregated forecasts from surrounding grid points, available for a subset of\nthese variables. Forecasts are available at hourly lead times from 0 to 21\nhours for two daily model runs initialized at 00 and 12 UTC, covering the\nperiod from December 2010 to June 2023. Additionally, the dataset provides\nstation observations for six key variables at 170 locations across Germany:\npressure, temperature, hourly precipitation accumulation, wind speed, wind\ndirection, and wind gusts. Since the forecast are mapped to the observed\nlocations, the data is delivered in a convenient format for analysis. The CIENS\ndataset complements the growing collection of benchmark datasets for weather\nand climate modeling. A key distinguishing feature is its long temporal extent,\nwhich encompasses multiple updates to the underlying numerical weather\nprediction model and thus supports investigations into how forecasting methods\ncan account for such changes. In addition to detailing the design and contents\nof the CIENS dataset, we outline potential applications in ensemble\npost-processing, forecast verification, and related research areas. A use case\nfocused on ensemble post-processing illustrates the benefits of incorporating\nthe rich set of available model predictors into machine learning-based\nforecasting models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03845", "cate": "physics.ao-ph", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.03878", "title": "The Regression Discontinuity Design in Medical Science", "authors": ["Matias D. Cattaneo", "Rocio Titiunik"], "categories": ["econ.EM", "stat.AP", "stat.ME"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.03878", "summary": "This article provides an introduction to the Regression Discontinuity (RD)\ndesign, and its application to empirical research in the medical sciences.\nWhile the main focus of this article is on causal interpretation, key concepts\nof estimation and inference are also briefly mentioned. A running medical\nempirical example is provided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.03878", "cate": "stat.ME", "date": "2025-08-05", "updated": "2025-08-05", "section": "cross"}
{"id": "2508.04172", "title": "Rapid parameter estimation with the full symphony of compact binary mergers using meshfree approximation", "authors": ["Abhishek Sharma", "Lalit Pathak", "Soumen Roy", "Anand S. Sengupta"], "categories": ["astro-ph.IM", "gr-qc", "stat.AP", "stat.CO"], "primary_category": "gr-qc", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04172", "summary": "We present a fast Bayesian inference framework to address the growing\ncomputational cost of gravitational-wave parameter estimation. The increased\ncost is driven by improved broadband detector sensitivity, particularly at low\nfrequencies due to advances in detector commissioning, resulting in longer\nin-band signals and a higher detection rate. Waveform models now incorporate\nfeatures like higher-order modes, further increasing the complexity of standard\ninference methods. Our framework employs meshfree likelihood interpolation with\nradial basis functions to accelerate Bayesian inference using the IMRPhenomXHM\nwaveform model that incorporates higher modes of the gravitational-wave signal.\nIn the initial start-up stage, interpolation nodes are placed within a\nconstant-match metric ellipsoid in the intrinsic parameter space. During\nsampling, likelihood is evaluated directly using the precomputed interpolants,\nbypassing the costly steps of on-the-fly waveform generation and\noverlap-integral computation. We improve efficiency by sampling in a rotated\nparameter space aligned with the eigenbasis of the metric ellipsoid, where\nparameters are uncorrelated by construction. This speeds up sampler\nconvergence. This method yields unbiased parameter recovery when applied to 100\nsimulated neutron-star-black-hole signals (NSBH) in LIGO-Virgo data, while\nreducing computational cost by up to an order of magnitude for the\nlongest-duration signal. The meshfree framework equally applies to symmetric\ncompact binary systems dominated by the quadrupole mode, supporting parameter\nestimation across a broad range of sources. Applied to a simulated NSBH signal\nin Einstein Telescope data, where the effects of Earth's rotation are neglected\nfor simplicity, our method achieves an O(10^4) speed-up, demonstrating its\npotential use in the third-generation (3G) era.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04172", "cate": "gr-qc", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04393", "title": "Generative Flexible Latent Structure Regression (GFLSR) model", "authors": ["Clara Grazian", "Qian Jin", "Pierre Lafaye De Micheaux"], "categories": ["stat.AP", "stat.ME", "stat.ML"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04393", "summary": "Latent structure methods, specifically linear continuous latent structure\nmethods, are a type of fundamental statistical learning strategy. They are\nwidely used for dimension reduction, regression and prediction, in the fields\nof chemometrics, economics, social science and etc. However, due to the lack of\nmodel inference, generative form, and unidentifiable parameters, most of these\nmethods are always used as an algorithm, instead of a model. This paper\nproposed a Generative Flexible Latent Structure Regression (GFLSR) model\nstructure to address this problem. Moreover, we show that most linear\ncontinuous latent variable methods can be represented under the proposed\nframework. The recursive structure allows potential model inference and\nresidual analysis. Then, the traditional Partial Least Squares (PLS) is\nfocused; we show that the PLS can be specialised in the proposed model\nstructure, named Generative-PLS. With a model structure, we analyse the\nconvergence of the parameters and the latent variables. Under additional\ndistribution assumptions, we show that the proposed model structure can lead to\nmodel inference without solving the probabilistic model. Additionally, we\nproposed a novel bootstrap algorithm that enables uncertainty on parameters and\non prediction for new datasets. A simulation study and a Real-world dataset are\nused to verify the proposed Generative-PLS model structure. Although the\ntraditional PLS is a special case, this proposed GFLSRM structure leads to a\npotential inference structure for all the linear continuous latent variable\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04393", "cate": "stat.ME", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04633", "title": "Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials", "authors": ["James P. Long", "Abhishikta Roy", "Ehsan Irajizad", "Kim-Anh Do", "Yu Shen"], "categories": ["stat.AP", "stat.ME"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04633", "summary": "In meta-analytic modeling, the functional relationship between a primary and\nsurrogate endpoint is estimated using summary data from a set of completed\nclinical trials. Parameters in the meta-analytic model are used to assess the\nquality of the proposed surrogate. Recently, meta-analytic models have been\nemployed to evaluate whether late-stage cancer incidence can serve as a\nsurrogate for cancer mortality in cancer screening trials. A major challenge in\nmeta-analytic models is that uncertainty of trial-level estimates affects the\nevaluation of surrogacy, since each trial provides only estimates of the\nprimary and surrogate endpoints rather than their true parameter values. In\nthis work, we show via simulation and theory that trial-level estimate\nuncertainty may bias the results of meta-analytic models towards positive\nfindings of the quality of the surrogate. We focus on cancer screening trials\nand the late stage incidence surrogate. We reassess correlations between\nprimary and surrogate endpoints in Ovarian cancer screening trials. Our\nfindings indicate that completed trials provide limited information regarding\nquality of the late-stage incidence surrogate. These results support\nrestricting meta-analytic regression usage to settings where trial-level\nestimate uncertainty is incorporated into the model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04633", "cate": "stat.ME", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2508.04703", "title": "Stochastic Taylor expansion via Poisson point processes", "authors": ["Weichao Wu", "Athanasios C. Micheas"], "categories": ["math.ST", "stat.AP", "stat.ME"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04703", "summary": "We generalize Taylor's theorem by introducing a stochastic formulation based\non an underlying Poisson point process model. We utilize this approach to\npropose a novel non-linear regression framework and perform statistical\ninference of the model parameters. Theoretical properties of the proposed\nestimator are also proven, including its convergence, uniformly almost surely,\nto the true function. The theory is presented for the univariate and\nmultivariate cases, and we exemplify the proposed methodology using several\nexamples via simulations and an application to stock market data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04703", "cate": "stat.ME", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
{"id": "2505.24412", "title": "A Time-Scaled ETAS Model for Earthquake Forecasting", "authors": ["Agniva Das", "Muralidharan K"], "categories": ["stat.AP", "stat.ME"], "primary_category": "stat.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.24412", "summary": "The Himalayan region, including Nepal, is prone to frequent and large\nearthquakes. Accurate forecasting of these earthquakes is crucial for\nminimizing loss of life and damage to infrastructure. In this study, we propose\nvarious time-scaled Epidemic Type Aftershock Sequence (ETAS) models to forecast\nearthquakes in Nepal. The ETAS model is a statistical model that describes the\ntemporal and spatial patterns of aftershocks following a main shock. A dataset\nof earthquake occurrences in Nepal from 2000 to 2020 was collected, and this\ndata was used to fit the models showcased in this article. Our results show\nthat the time-scaled ETAS model is able to accurately forecast earthquake\noccurrences in Nepal, and could be a useful tool for earthquake early warning\nsystems in the region.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.24412", "cate": "stat.AP", "date": "2025-05-30", "updated": "2025-08-06", "section": "repl"}
{"id": "2507.01842", "title": "Time Series Transformer-Based Modeling of Pavement Skid and Texture Deterioration", "authors": ["Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci"], "categories": ["stat.AP"], "primary_category": "stat.AP", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01842", "summary": "This study investigates the deterioration of skid resistance and surface\nmacrotexture following preventive maintenance using micro-milling techniques.\nField data were collected from 31 asphalt pavement sections located across four\nclimatic zones in Texas. The data encompasses a variety of surface types,\nmilling depths, operational speeds, and drum configurations. A standardized\ndata collection protocol was followed, with measurements taken before milling,\nimmediately after treatment, and at 3, 6, 12, and 18 months post-treatment.\nSkid number and Mean Profile Depth (MPD) were used to evaluate surface friction\nand texture characteristics. The dataset was reformatted into a time-series\nstructure with 930 observations, including contextual variables such as\nclimatic zone, treatment parameters, and baseline surface condition. A\ncomparative modeling framework was applied to predict the deterioration trends\nof both skid resistance and macrotexture over time. Eight regression models,\nincluding linear, tree-based, and ensemble methods, were evaluated alongside a\ntime series transformer model. Results show that the transformer model achieved\nthe highest prediction accuracy for skid resistance (R2 = 0.981), while Random\nForest performing best for macrotexture prediction (R2 = 0.838). The findings\nindicate that the degradation of surface characteristics after preventive\nmaintenance is nonlinear and influenced by a combination of environmental and\noperational factors. This study demonstrates the effectiveness of data-driven\nmodeling in supporting transportation agencies with pavement performance\nforecasting and maintenance planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01842", "cate": "stat.AP", "date": "2025-07-02", "updated": "2025-08-06", "section": "repl"}
{"id": "2306.15048", "title": "Assessing Heterogeneity of Treatment Effects", "authors": ["Tetsuya Kaji", "Jianfei Cao"], "categories": ["econ.EM", "stat.AP", "stat.ME"], "primary_category": "econ.EM", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.15048", "summary": "Heterogeneous treatment effects are of major interest in economics. For\nexample, a poverty reduction measure would be best evaluated by its effects on\nthose who would be poor in the absence of the treatment, or by the share among\nthe poor who would increase their earnings because of the treatment. While\nthese quantities are not identified, we derive nonparametrically sharp bounds\nusing only the marginal distributions of the control and treated outcomes.\nApplications to microfinance and welfare reform demonstrate their utility even\nwhen the average treatment effects are not significant and when economic theory\nmakes opposite predictions between heterogeneous individuals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.15048", "cate": "econ.EM", "date": "2023-06-26", "updated": "2025-08-06", "section": "repl"}
{"id": "2406.10612", "title": "Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria", "authors": ["Theodoros Evrenoglou", "Adriani Nikolakopoulou", "Guido Schwarzer", "Gerta RÃ¼cker", "Anna Chaimani"], "categories": ["stat.AP", "stat.ME", "stat.OT"], "primary_category": "stat.ME", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.10612", "summary": "A key output of network meta-analysis (NMA) is the relative ranking of\ntreatments; nevertheless, it has attracted substantial criticism. Existing\nranking methods often lack clear interpretability and fail to adequately\naccount for uncertainty, over-emphasizing small differences in treatment\neffects. We propose a novel framework to estimate treatment hierarchies in NMA\nusing a probabilistic model, focusing on a clinically relevant treatment-choice\ncriterion (TCC). Initially, we formulate a mathematical expression to define a\nTCC based on smallest worthwhile differences (SWD), converting NMA relative\ntreatment effects into treatment preference format. This data is then\nsynthesized using a probabilistic ranking model, assigning each treatment a\nlatent 'ability' parameter, representing its propensity to yield clinically\nimportant and beneficial true treatment effects relative to the rest of the\ntreatments in the network. Parameter estimation relies on the maximum\nlikelihood theory, with standard errors derived asymptotically from Fisher's\ninformation matrix. To facilitate the use of our methods, we launched the R\npackage mtrank. We applied our method to two clinical datasets: one comparing\n18 antidepressants for major depression and another comparing 6\nantihypertensives for the incidence of diabetes. Our approach provided robust,\ninterpretable treatment hierarchies that account for a concrete TCC. We further\nexamined the agreement between the proposed method and existing ranking metrics\nin 153 published networks, concluding that the degree of agreement depends on\nthe precision of the NMA estimates. Our framework offers a valuable alternative\nfor NMA treatment ranking, mitigating over-interpretation of minor differences.\nThis enables more reliable and clinically meaningful treatment hierarchies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.10612", "cate": "stat.ME", "date": "2024-06-15", "updated": "2025-08-06", "section": "repl"}
{"id": "2409.14284", "title": "Survey Data Integration for Distribution Function Estimation", "authors": ["Jeremy Flood", "Sayed Mostafa"], "categories": ["math.ST", "stat.AP", "stat.ME", "stat.OT"], "primary_category": "math.ST", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.14284", "summary": "Integration of probabilistic and non-probabilistic samples for the estimation\nof finite population totals (or means) has recently received considerable\nattention in the field of survey sampling; yet, to the best of our knowledge,\nthis framework has not been extended to cumulative distribution function (CDF)\nestimation. To address this gap, we propose a novel CDF estimator that\nintegrates data from probability samples with data from, potentially big,\nnonprobability samples. Assuming that a set of shared covariates are observed\nin both, while the response variable is observed only in the latter, the\nproposed estimator uses a survey-weighted empirical CDF of regression residuals\ntrained on the convenience sample to estimate the CDF of the response variable.\nUnder some assumptions, we derive the asymptotic bias and variance of our CDF\nestimator and show that it is asymptotically unbiased for the finite population\nCDF if ignorability holds. Our empirical results imply that the proposed CDF\nestimator is robust to model misspecification under ignorability, and robust to\nignorability under model misspecification; when both assumptions are violated,\nour residual-based CDF estimator still outperforms its `plug-in' mass\nimputation and naive siblings, albeit with noted decreases in efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.14284", "cate": "math.ST", "date": "2024-09-22", "updated": "2025-08-06", "section": "repl"}
{"id": "2508.04344", "title": "Performative Market Making", "authors": ["Charalampos Kleitsikas", "Stefanos Leonardos", "Carmine Ventre"], "categories": ["q-fin.MF", "q-fin.TR"], "primary_category": "q-fin.TR", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.04344", "summary": "Financial models do not merely analyse markets, but actively shape them. This\neffect, known as performativity, describes how financial theories and the\nsubsequent actions based on them influence market processes, by creating\nself-fulfilling prophecies. Although discussed in the literature on economic\nsociology, this deeply rooted phenomenon lacks mathematical formulation in\nfinancial markets. Our paper closes this gap by breaking down the canonical\nseparation of diffusion processes between the description of the market\nenvironment and the financial model. We do that by embedding the model in the\nprocess itself, creating a closed feedback loop, and demonstrate how prices\nchange towards greater conformity to the prevailing financial model used in the\nmarket. We further show, with closed-form solutions and machine learning, how a\nperformative market maker can reverse engineer the current dominant strategies\nin the market and effectively arbitrage them while maintaining competitive\nquotes and superior P&L.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.04344", "cate": "q-fin.TR", "date": "2025-08-06", "updated": "2025-08-06", "section": "cross"}
