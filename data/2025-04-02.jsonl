{"id": "2504.00016", "pdf": "https://arxiv.org/pdf/2504.00016", "abs": "https://arxiv.org/abs/2504.00016", "authors": ["Birger Moell", "Fredrik Sand Aronsson", "Sanian Akbar"], "title": "Medical Reasoning in LLMs: An In-Depth Analysis of DeepSeek R1", "categories": ["cs.CL"], "comment": null, "summary": "Integrating large language models (LLMs) like DeepSeek R1 into healthcare\nrequires rigorous evaluation of their reasoning alignment with clinical\nexpertise. This study assesses DeepSeek R1's medical reasoning against expert\npatterns using 100 MedQA clinical cases. The model achieved 93% diagnostic\naccuracy, demonstrating systematic clinical judgment through differential\ndiagnosis, guideline-based treatment selection, and integration of\npatient-specific factors. However, error analysis of seven incorrect cases\nrevealed persistent limitations: anchoring bias, challenges reconciling\nconflicting data, insufficient exploration of alternatives, overthinking,\nknowledge gaps, and premature prioritization of definitive treatment over\nintermediate care. Crucially, reasoning length correlated with accuracy -\nshorter responses (<5,000 characters) were more reliable, suggesting extended\nexplanations may signal uncertainty or rationalization of errors. While\nDeepSeek R1 exhibits foundational clinical reasoning capabilities, recurring\nflaws highlight critical areas for refinement, including bias mitigation,\nknowledge updates, and structured reasoning frameworks. These findings\nunderscore LLMs' potential to augment medical decision-making through\nartificial reasoning but emphasize the need for domain-specific validation,\ninterpretability safeguards, and confidence metrics (e.g., response length\nthresholds) to ensure reliability in real-world applications."}
{"id": "2504.00019", "pdf": "https://arxiv.org/pdf/2504.00019", "abs": "https://arxiv.org/abs/2504.00019", "authors": ["Indraneil Paul", "Haoyi Yang", "Goran GlavaÅ¡", "Kristian Kersting", "Iryna Gurevych"], "title": "ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Language models (LMs) have become a staple of the code-writing toolbox. Their\npre-training recipe has, however, remained stagnant over recent years, barring\nthe occasional changes in data sourcing and filtering strategies. In\nparticular, research exploring modifications to Code-LMs' pre-training\nobjectives, geared towards improving data efficiency and better disentangling\nbetween syntax and semantics, has been noticeably sparse, especially compared\nwith corresponding efforts in natural language LMs. In this work, we examine\ngrounding on obfuscated code as a means of helping Code-LMs look beyond the\nsurface-form syntax and enhance their pre-training sample efficiency. To this\nend, we compile ObscuraX, a dataset of approximately 55M source and obfuscated\ncode pairs in seven languages. Subsequently, we pre-train ObscuraCoder models,\nranging in size from 255M to 2.8B parameters, on a 272B-token corpus that\nincludes ObscuraX and demonstrate that our obfuscation-based pre-training\nrecipe leads to consistent improvements in Code-LMs' abilities compared to both\nvanilla autoregressive pre-training as well as existing de-obfuscation (DOBF)\nobjectives. ObscuraCoder demonstrates sizeable gains across multiple tests of\nsyntactic and semantic code understanding, along with improved capabilities in\nmultilingual code completion, multilingual code commit summarization, and\nmulti-purpose library-oriented code generation."}
{"id": "2504.00021", "pdf": "https://arxiv.org/pdf/2504.00021", "abs": "https://arxiv.org/abs/2504.00021", "authors": ["Rahul Raja", "Arpita Vats"], "title": "FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages", "categories": ["cs.CL"], "comment": "NACCL 2025", "summary": "This paper presents the winning submission of the RaaVa team to the\nAmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine\nTranslation (MT) into Indigenous Languages of America, where our system ranked\nfirst overall based on average Pearson correlation with the human annotations.\nWe introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge\nregression and Gradient Boosting to model translation quality. In addition to\nFUSE, we explore five alternative approaches leveraging different combinations\nof linguistic similarity features and learning paradigms. FUSE Score highlights\nthe effectiveness of combining lexical, phonetic, semantic, and fuzzy token\nsimilarity with learning-based modeling to improve MT evaluation for\nmorphologically rich and low-resource languages. MT into Indigenous languages\nposes unique challenges due to polysynthesis, complex morphology, and\nnon-standardized orthography. Conventional automatic metrics such as BLEU, TER,\nand ChrF often fail to capture deeper aspects like semantic adequacy and\nfluency. Our proposed framework, formerly referred to as FUSE, incorporates\nmultilingual sentence embeddings and phonological encodings to better align\nwith human evaluation. We train supervised models on human-annotated\ndevelopment sets and evaluate held-out test data. Results show that FUSE\nconsistently achieves higher Pearson and Spearman correlations with human\njudgments, offering a robust and linguistically informed solution for MT\nevaluation in low-resource settings."}
{"id": "2504.00025", "pdf": "https://arxiv.org/pdf/2504.00025", "abs": "https://arxiv.org/abs/2504.00025", "authors": ["Uwe Peters", "Benjamin Chin-Yee"], "title": "Generalization Bias in Large Language Model Summarization of Scientific Research", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Artificial intelligence chatbots driven by large language models (LLMs) have\nthe potential to increase public science literacy and support scientific\nresearch, as they can quickly summarize complex scientific information in\naccessible terms. However, when summarizing scientific texts, LLMs may omit\ndetails that limit the scope of research conclusions, leading to\ngeneralizations of results broader than warranted by the original study. We\ntested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA\n3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their\noriginal scientific texts. Even when explicitly prompted for accuracy, most\nLLMs produced broader generalizations of scientific results than those in the\noriginal texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing\nin 26 to 73% of cases. In a direct comparison of LLM-generated and\nhuman-authored science summaries, LLM summaries were nearly five times more\nlikely to contain broad generalizations (OR = 4.85, 95% CI [3.06, 7.70]).\nNotably, newer models tended to perform worse in generalization accuracy than\nearlier ones. Our results indicate a strong bias in many widely used LLMs\ntowards overgeneralizing scientific conclusions, posing a significant risk of\nlarge-scale misinterpretations of research findings. We highlight potential\nmitigation strategies, including lowering LLM temperature settings and\nbenchmarking LLMs for generalization accuracy."}
{"id": "2504.00017", "pdf": "https://arxiv.org/pdf/2504.00017", "abs": "https://arxiv.org/abs/2504.00017", "authors": ["Artemii Redkin", "Zdravko Dugonjic", "Mike Lambeta", "Roberto Calandra"], "title": "Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image Fusion", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages", "summary": "Vision-based tactile sensors use structured light to measure deformation in\ntheir elastomeric interface. Until now, vision-based tactile sensors such as\nDIGIT and GelSight have been using a single, static pattern of structured light\ntuned to the specific form factor of the sensor. In this work, we investigate\nthe effectiveness of dynamic illumination patterns, in conjunction with image\nfusion techniques, to improve the quality of sensing of vision-based tactile\nsensors. Specifically, we propose to capture multiple measurements, each with a\ndifferent illumination pattern, and then fuse them together to obtain a single,\nhigher-quality measurement. Experimental results demonstrate that this type of\ndynamic illumination yields significant improvements in image contrast,\nsharpness, and background difference. This discovery opens the possibility of\nretroactively improving the sensing quality of existing vision-based tactile\nsensors with a simple software update, and for new hardware designs capable of\nfully exploiting dynamic illumination."}
{"id": "2504.00027", "pdf": "https://arxiv.org/pdf/2504.00027", "abs": "https://arxiv.org/abs/2504.00027", "authors": ["Muhammad Ahmad", "Humaira Farid", "Iqra Ameer", "Muhammad Muzamil", "Ameer Hamza Muhammad Jalal", "Ildar Batyrshin", "Grigori Sidorov"], "title": "Opioid Named Entity Recognition (ONER-2025) from Reddit", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88)."}
{"id": "2504.00023", "pdf": "https://arxiv.org/pdf/2504.00023", "abs": "https://arxiv.org/abs/2504.00023", "authors": ["Niklas Rottmayer", "Claudia Redenbach"], "title": "A Novel Distance-Based Metric for Quality Assessment in Image Segmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The assessment of segmentation quality plays a fundamental role in the\ndevelopment, optimization, and comparison of segmentation methods which are\nused in a wide range of applications. With few exceptions, quality assessment\nis performed using traditional metrics, which are based on counting the number\nof erroneous pixels but do not capture the spatial distribution of errors.\nEstablished distance-based metrics such as the average Hausdorff distance are\ndifficult to interpret and compare for different methods and datasets. In this\npaper, we introduce the Surface Consistency Coefficient (SCC), a novel\ndistance-based quality metric that quantifies the spatial distribution of\nerrors based on their proximity to the surface of the structure. Through a\nrigorous analysis using synthetic data and real segmentation results, we\ndemonstrate the robustness and effectiveness of SCC in distinguishing errors\nnear the surface from those further away. At the same time, SCC is easy to\ninterpret and comparable across different structural contexts."}
{"id": "2504.00030", "pdf": "https://arxiv.org/pdf/2504.00030", "abs": "https://arxiv.org/abs/2504.00030", "authors": ["Aayush Gautam", "Susav Shrestha", "Narasimha Annapareddy"], "title": "Token-Driven GammaTune: Adaptive Calibration for Enchanced Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 1 table", "summary": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment."}
{"id": "2504.00032", "pdf": "https://arxiv.org/pdf/2504.00032", "abs": "https://arxiv.org/abs/2504.00032", "authors": ["Qingmeng Wen", "Yu-Kun Lai", "Ze Ji", "Seyed Amir Tafrishi"], "title": "Skeletonization Quality Evaluation: Geometric Metrics for Point Cloud Analysis in Robotics", "categories": ["cs.CV", "cs.CG", "cs.RO"], "comment": "15 pages, 12 figures, under-review", "summary": "Skeletonization is a powerful tool for shape analysis, rooted in the inherent\ninstinct to understand an object's morphology. It has found applications across\nvarious domains, including robotics. Although skeletonization algorithms have\nbeen studied in recent years, their performance is rarely quantified with\ndetailed numerical evaluations. This work focuses on defining and quantifying\ngeometric properties to systematically score the skeletonization results of\npoint cloud shapes across multiple aspects, including topological similarity,\nboundedness, centeredness, and smoothness. We introduce these representative\nmetric definitions along with a numerical scoring framework to analyze\nskeletonization outcomes concerning point cloud data for different scenarios,\nfrom object manipulation to mobile robot navigation. Additionally, we provide\nan open-source tool to enable the research community to evaluate and refine\ntheir skeleton models. Finally, we assess the performance and sensitivity of\nthe proposed geometric evaluation methods from various robotic applications."}
{"id": "2504.00040", "pdf": "https://arxiv.org/pdf/2504.00040", "abs": "https://arxiv.org/abs/2504.00040", "authors": ["Jurek Eisinger", "Ward Gauderis", "Lin de Huybrecht", "Geraint A. Wiggins"], "title": "Quantum Methods for Managing Ambiguity in Natural Language Processing", "categories": ["cs.CL", "cs.AI", "quant-ph", "I.2"], "comment": null, "summary": "The Categorical Compositional Distributional (DisCoCat) framework models\nmeaning in natural language using the mathematical framework of quantum theory,\nexpressed as formal diagrams. DisCoCat diagrams can be associated with tensor\nnetworks and quantum circuits. DisCoCat diagrams have been connected to density\nmatrices in various contexts in Quantum Natural Language Processing (QNLP).\nPrevious use of density matrices in QNLP entails modelling ambiguous words as\nprobability distributions over more basic words (the word \\texttt{queen}, e.g.,\nmight mean the reigning queen or the chess piece). In this article, we\ninvestigate using probability distributions over processes to account for\nsyntactic ambiguity in sentences. The meanings of these sentences are\nrepresented by density matrices. We show how to create probability\ndistributions on quantum circuits that represent the meanings of sentences and\nexplain how this approach generalises tasks from the literature. We conduct an\nexperiment to validate the proposed theory."}
{"id": "2504.00037", "pdf": "https://arxiv.org/pdf/2504.00037", "abs": "https://arxiv.org/abs/2504.00037", "authors": ["Guoyizhe Wei", "Rama Chellappa"], "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have delivered remarkable progress through global\nself-attention, yet their quadratic complexity can become prohibitive for\nhigh-resolution inputs. In this work, we present ViT-Linearizer, a\ncross-architecture distillation framework that transfers rich ViT\nrepresentations into a linear-time, recurrent-style model. Our approach\nleverages 1) activation matching, an intermediate constraint that encourages\nstudent to align its token-wise dependencies with those produced by the\nteacher, and 2) masked prediction, a contextual reconstruction objective that\nrequires the student to predict the teacher's representations for unseen\n(masked) tokens, to effectively distill the quadratic self-attention knowledge\ninto the student while maintaining efficient complexity. Empirically, our\nmethod provides notable speedups particularly for high-resolution tasks,\nsignificantly addressing the hardware challenges in inference. Additionally, it\nalso elevates Mamba-based architectures' performance on standard vision\nbenchmarks, achieving a competitive 84.3% top-1 accuracy on ImageNet with a\nbase-sized model. Our results underscore the good potential of RNN-based\nsolutions for large-scale visual tasks, bridging the gap between theoretical\nefficiency and real-world practice."}
{"id": "2504.00042", "pdf": "https://arxiv.org/pdf/2504.00042", "abs": "https://arxiv.org/abs/2504.00042", "authors": ["Agam Shah", "Liqin Ye", "Sebastian Jaskowski", "Wei Xu", "Sudheer Chava"], "title": "Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are frequently utilized as sources of knowledge\nfor question-answering. While it is known that LLMs may lack access to\nreal-time data or newer data produced after the model's cutoff date, it is less\nclear how their knowledge spans across historical information. In this study,\nwe assess the breadth of LLMs' knowledge using financial data of U.S. publicly\ntraded companies by evaluating more than 197k questions and comparing model\nresponses to factual data. We further explore the impact of company\ncharacteristics, such as size, retail investment, institutional attention, and\nreadability of financial filings, on the accuracy of knowledge represented in\nLLMs. Our results reveal that LLMs are less informed about past financial\nperformance, but they display a stronger awareness of larger companies and more\nrecent information. Interestingly, at the same time, our analysis also reveals\nthat LLMs are more likely to hallucinate for larger companies, especially for\ndata from more recent years. We will make the code, prompts, and model outputs\npublic upon the publication of the work."}
{"id": "2504.00072", "pdf": "https://arxiv.org/pdf/2504.00072", "abs": "https://arxiv.org/abs/2504.00072", "authors": ["Lucas Ventura", "Antoine Yang", "Cordelia Schmid", "GÃ¼l Varol"], "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs", "categories": ["cs.CV"], "comment": "CVPR 2025 Camera ready. Project page:\n  https://imagine.enpc.fr/~lucas.ventura/chapter-llama/", "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page."}
{"id": "2504.00043", "pdf": "https://arxiv.org/pdf/2504.00043", "abs": "https://arxiv.org/abs/2504.00043", "authors": ["Jixuan Leng", "Chengsong Huang", "Langlin Huang", "Bill Yuchen Lin", "William W. Cohen", "Haohan Wang", "Jiaxin Huang"], "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\nreasoning or vision-language understanding capabilities, with limited dynamic\ninterplay between textual and visual constraints. To address this limitation,\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\ntask requiring multimodal adherence to semantic constraints from text-based\nclues and intersectional constraints from visual grid structures.\nCrossWordBench leverages a controllable puzzle generation framework that\nproduces puzzles in multiple formats (text and image) and offers different\nevaluation strategies ranging from direct puzzle solving to interactive modes.\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\noutperform non-reasoning models substantially by effectively leveraging\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\nthe task, showing a strong correlation between their puzzle-solving performance\nand grid-parsing accuracy. Our findings offer insights into the limitations of\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\napproach for creating multimodal constrained tasks for future evaluations."}
{"id": "2504.00139", "pdf": "https://arxiv.org/pdf/2504.00139", "abs": "https://arxiv.org/abs/2504.00139", "authors": ["Yannick Burkhardt", "Simon Schaefer", "Stefan Leutenegger"], "title": "SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection", "categories": ["cs.CV"], "comment": "In Review for ICCV25", "summary": "Event-based keypoint detection and matching holds significant potential,\nenabling the integration of event sensors into highly optimized Visual SLAM\nsystems developed for frame cameras over decades of research. Unfortunately,\nexisting approaches struggle with the motion-dependent appearance of keypoints\nand the complex noise prevalent in event streams, resulting in severely limited\nfeature matching capabilities and poor performance on downstream tasks. To\nmitigate this problem, we propose SuperEvent, a data-driven approach to predict\nstable keypoints with expressive descriptors. Due to the absence of event\ndatasets with ground truth keypoint labels, we leverage existing frame-based\nkeypoint detectors on readily available event-aligned and synchronized\ngray-scale frames for self-supervision: we generate temporally sparse keypoint\npseudo-labels considering that events are a product of both scene appearance\nand camera motion. Combined with our novel, information-rich event\nrepresentation, we enable SuperEvent to effectively learn robust keypoint\ndetection and description in event streams. Finally, we demonstrate the\nusefulness of SuperEvent by its integration into a modern sparse keypoint and\ndescriptor-based SLAM framework originally developed for traditional cameras,\nsurpassing the state-of-the-art in event-based SLAM by a wide margin. Source\ncode and multimedia material are available at\nsmartroboticslab.github.io/SuperEvent."}
{"id": "2504.00045", "pdf": "https://arxiv.org/pdf/2504.00045", "abs": "https://arxiv.org/abs/2504.00045", "authors": ["Adrian Bermudez-Villalva", "Maryam Mehrnezhad", "Ehsan Toreini"], "title": "Measuring Online Hate on 4chan using Pre-trained Deep Learning Models", "categories": ["cs.CL", "cs.CY"], "comment": "IEEE Transactions on Technology and Society, 11 pages", "summary": "Online hate speech can harmfully impact individuals and groups, specifically\non non-moderated platforms such as 4chan where users can post anonymous\ncontent. This work focuses on analysing and measuring the prevalence of online\nhate on 4chan's politically incorrect board (/pol/) using state-of-the-art\nNatural Language Processing (NLP) models, specifically transformer-based models\nsuch as RoBERTa and Detoxify. By leveraging these advanced models, we provide\nan in-depth analysis of hate speech dynamics and quantify the extent of online\nhate non-moderated platforms. The study advances understanding through\nmulti-class classification of hate speech (racism, sexism, religion, etc.),\nwhile also incorporating the classification of toxic content (e.g., identity\nattacks and threats) and a further topic modelling analysis. The results show\nthat 11.20% of this dataset is identified as containing hate in different\ncategories. These evaluations show that online hate is manifested in various\nforms, confirming the complicated and volatile nature of detection in the wild."}
{"id": "2504.00149", "pdf": "https://arxiv.org/pdf/2504.00149", "abs": "https://arxiv.org/abs/2504.00149", "authors": ["Masato Tamura"], "title": "Towards Precise Action Spotting: Addressing Temporal Misalignment in Labels with Dynamic Label Assignment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Precise action spotting has attracted considerable attention due to its\npromising applications. While existing methods achieve substantial performance\nby employing well-designed model architecture, they overlook a significant\nchallenge: the temporal misalignment inherent in ground-truth labels. This\nmisalignment arises when frames labeled as containing events do not align\naccurately with the actual event times, often as a result of human annotation\nerrors or the inherent difficulties in precisely identifying event boundaries\nacross neighboring frames. To tackle this issue, we propose a novel dynamic\nlabel assignment strategy that allows predictions to have temporal offsets from\nground-truth action times during training, ensuring consistent event spotting.\nOur method extends the concept of minimum-cost matching, which is utilized in\nthe spatial domain for object detection, to the temporal domain. By calculating\nmatching costs based on predicted action class scores and temporal offsets, our\nmethod dynamically assigns labels to the most likely predictions, even when the\npredicted times of these predictions deviate from ground-truth times,\nalleviating the negative effects of temporal misalignment in labels. We conduct\nextensive experiments and demonstrate that our method achieves state-of-the-art\nperformance, particularly in conditions where events are visually distinct and\ntemporal misalignment in labels is common."}
{"id": "2504.00046", "pdf": "https://arxiv.org/pdf/2504.00046", "abs": "https://arxiv.org/abs/2504.00046", "authors": ["Loris Belcastro", "Cristian Cosentino", "Fabrizio Marozzo", "Merve GÃ¼ndÃ¼z-CÃ¼re", "Åule ÃztÃ¼rk-Birim"], "title": "Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.SI"], "comment": null, "summary": "In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders."}
{"id": "2504.00150", "pdf": "https://arxiv.org/pdf/2504.00150", "abs": "https://arxiv.org/abs/2504.00150", "authors": ["Yongyi Shi", "Ge Wang"], "title": "Few-Shot Generation of Brain Tumors for Secure and Fair Data Sharing", "categories": ["cs.CV"], "comment": "17 pages, 4 figures", "summary": "Leveraging multi-center data for medical analytics presents challenges due to\nprivacy concerns and data heterogeneity. While distributed approaches such as\nfederated learning has gained traction, they remain vulnerable to privacy\nbreaches, particularly in sensitive domains like medical imaging. Generative\nmodels, such as diffusion models, enhance privacy by synthesizing realistic\ndata. However, they are prone to memorization, especially when trained on small\ndatasets. This study proposes a decentralized few-shot generative model (DFGM)\nto synthesize brain tumor images while fully preserving privacy. DFGM\nharmonizes private tumor data with publicly shareable healthy images from\nmultiple medical centers, constructing a new dataset by blending tumor\nforegrounds with healthy backgrounds. This approach ensures stringent privacy\nprotection and enables controllable, high-quality synthesis by preserving both\nthe healthy backgrounds and tumor foregrounds. We assess DFGM's effectiveness\nin brain tumor segmentation using a UNet, achieving Dice score improvements of\n3.9% for data augmentation and 4.6% for fairness on a separate dataset."}
{"id": "2504.00048", "pdf": "https://arxiv.org/pdf/2504.00048", "abs": "https://arxiv.org/abs/2504.00048", "authors": ["Cong Duy Vu Hoang", "Gioacchino Tangari", "Clemence Lanfranchi", "Dalu Guo", "Paul Cayet", "Steve Siu", "Don Dharmasiri", "Yuan-Fang Li", "Long Duong", "Damien Hilloulin", "Rhicheek Patra", "Sungpack Hong", "Hassan Chafi"], "title": "Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, accepted at NAACL 2025 (Industry Track)", "summary": "The growing adoption of large language models (LLMs) in business applications\nhas amplified interest in Natural Language to SQL (NL2SQL) solutions, in which\nthere is competing demand for high performance and efficiency. Domain- and\ncustomer-specific requirements further complicate the problem. To address this\nconundrum, we introduce Distill-C, a distilled customization framework tailored\nfor NL2SQL tasks. Distill-C utilizes large teacher LLMs to produce high-quality\nsynthetic data through a robust and scalable pipeline. Finetuning smaller and\nopen-source LLMs on this synthesized data enables them to rival or outperform\nteacher models an order of magnitude larger. Evaluated on multiple challenging\nbenchmarks, Distill-C achieves an average improvement of 36% in execution\naccuracy compared to the base models from three distinct LLM families.\nAdditionally, on three internal customer benchmarks, Distill-C demonstrates a\n22.6% performance improvement over the base models. Our results demonstrate\nthat Distill-C is an effective, high-performing and generalizable approach for\ndeploying lightweight yet powerful NL2SQL models, delivering exceptional\naccuracies while maintaining low computational cost."}
{"id": "2504.00159", "pdf": "https://arxiv.org/pdf/2504.00159", "abs": "https://arxiv.org/abs/2504.00159", "authors": ["Advaith V. Sethuraman", "Max Rucker", "Onur Bagoren", "Pou-Chun Kung", "Nibarkavi N. B. Amutha", "Katherine A. Skinner"], "title": "SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present SonarSplat, a novel Gaussian splatting framework\nfor imaging sonar that demonstrates realistic novel view synthesis and models\nacoustic streaking phenomena. Our method represents the scene as a set of 3D\nGaussians with acoustic reflectance and saturation properties. We develop a\nnovel method to efficiently rasterize learned Gaussians to produce a\nrange/azimuth image that is faithful to the acoustic image formation model of\nimaging sonar. In particular, we develop a novel approach to model azimuth\nstreaking in a Gaussian splatting framework. We evaluate SonarSplat using\nreal-world datasets of sonar images collected from an underwater robotic\nplatform in a controlled test tank and in a real-world river environment.\nCompared to the state-of-the-art, SonarSplat offers improved image synthesis\ncapabilities (+2.5 dB PSNR). We also demonstrate that SonarSplat can be\nleveraged for azimuth streak removal and 3D scene reconstruction."}
{"id": "2504.00050", "pdf": "https://arxiv.org/pdf/2504.00050", "abs": "https://arxiv.org/abs/2504.00050", "authors": ["Nuo Chen", "Zhiyuan Hu", "Qingyun Zou", "Jiaying Wu", "Qian Wang", "Bryan Hooi", "Bingsheng He"], "title": "JudgeLRM: Large Reasoning Models as a Judge", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning."}
{"id": "2504.00161", "pdf": "https://arxiv.org/pdf/2504.00161", "abs": "https://arxiv.org/abs/2504.00161", "authors": ["Suzanne Stathatos", "Michael Hobley", "Markus Marks", "Pietro Perona"], "title": "SAVeD: Learning to Denoise Low-SNR Video for Improved Downstream Performance", "categories": ["cs.CV"], "comment": "Project page: https://suzanne-stathatos.github.io/SAVeD Code page:\n  https://github.com/suzanne-stathatos/SAVeD", "summary": "Foundation models excel at vision tasks in natural images but fail in low\nsignal-to-noise ratio (SNR) videos, such as underwater sonar, ultrasound, and\nmicroscopy. We introduce Spatiotemporal Augmentations and denoising in Video\nfor Downstream Tasks (SAVeD), a self-supervised method that denoises low-SNR\nsensor videos and is trained using only the raw noisy data. By leveraging\ndifferences in foreground and background motion, SAVeD enhances object\nvisibility using an encoder-decoder with a temporal bottleneck. Our approach\nimproves classification, detection, tracking, and counting, outperforming\nstate-of-the-art video denoising methods with lower resource requirements.\nProject page: https://suzanne-stathatos.github.io/SAVeD Code page:\nhttps://github.com/suzanne-stathatos/SAVeD"}
{"id": "2504.00053", "pdf": "https://arxiv.org/pdf/2504.00053", "abs": "https://arxiv.org/abs/2504.00053", "authors": ["Jie Pan", "Seungwon Lee", "Cheligeer Cheligeer", "Elliot A. Martin", "Kiarash Riazi", "Hude Quan", "Na Li"], "title": "Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: Electronic health records (EHR) are widely available to complement\nadministrative data-based disease surveillance and healthcare performance\nevaluation. Defining conditions from EHR is labour-intensive and requires\nextensive manual labelling of disease outcomes. This study developed an\nefficient strategy based on advanced large language models to identify multiple\nconditions from EHR clinical notes. Methods: We linked a cardiac registry\ncohort in 2015 with an EHR system in Alberta, Canada. We developed a pipeline\nthat leveraged a generative large language model (LLM) to analyze, understand,\nand interpret EHR notes by prompts based on specific diagnosis, treatment\nmanagement, and clinical guidelines. The pipeline was applied to detect acute\nmyocardial infarction (AMI), diabetes, and hypertension. The performance was\ncompared against clinician-validated diagnoses as the reference standard and\nwidely adopted International Classification of Diseases (ICD) codes-based\nmethods. Results: The study cohort accounted for 3,088 patients and 551,095\nclinical notes. The prevalence was 55.4%, 27.7%, 65.9% and for AMI, diabetes,\nand hypertension, respectively. The performance of the LLM-based pipeline for\ndetecting conditions varied: AMI had 88% sensitivity, 63% specificity, and 77%\npositive predictive value (PPV); diabetes had 91% sensitivity, 86% specificity,\nand 71% PPV; and hypertension had 94% sensitivity, 32% specificity, and 72%\nPPV. Compared with ICD codes, the LLM-based method demonstrated improved\nsensitivity and negative predictive value across all conditions. The monthly\npercentage trends from the detected cases by LLM and reference standard showed\nconsistent patterns."}
{"id": "2504.00185", "pdf": "https://arxiv.org/pdf/2504.00185", "abs": "https://arxiv.org/abs/2504.00185", "authors": ["Atharva Sehgal", "Patrick Yuan", "Ziniu Hu", "Yisong Yue", "Jennifer J. Sun", "Swarat Chaudhuri"], "title": "Self-Evolving Visual Concept Library using Vision-Language Critics", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR camera ready", "summary": "We study the problem of building a visual concept library for visual\nrecognition. Building effective visual concept libraries is challenging, as\nmanual definition is labor-intensive, while relying solely on LLMs for concept\ngeneration can result in concepts that lack discriminative power or fail to\naccount for the complex interactions between them. Our approach, ESCHER, takes\na library learning perspective to iteratively discover and improve visual\nconcepts. ESCHER uses a vision-language model (VLM) as a critic to iteratively\nrefine the concept library, including accounting for interactions between\nconcepts and how they affect downstream classifiers. By leveraging the\nin-context learning abilities of LLMs and the history of performance using\nvarious concepts, ESCHER dynamically improves its concept generation strategy\nbased on the VLM critic's feedback. Finally, ESCHER does not require any human\nannotations, and is thus an automated plug-and-play framework. We empirically\ndemonstrate the ability of ESCHER to learn a concept library for zero-shot,\nfew-shot, and fine-tuning visual classification tasks. This work represents, to\nour knowledge, the first application of concept library learning to real-world\nvisual tasks."}
{"id": "2504.00061", "pdf": "https://arxiv.org/pdf/2504.00061", "abs": "https://arxiv.org/abs/2504.00061", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Tian Tang", "Rong Yin"], "title": "Evaluating the Feasibility and Accuracy of Large Language Models for Medical History-Taking in Obstetrics and Gynecology", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IISE 2025 annual conference", "summary": "Effective physician-patient communications in pre-diagnostic environments,\nand most specifically in complex and sensitive medical areas such as\ninfertility, are critical but consume a lot of time and, therefore, cause\nclinic workflows to become inefficient. Recent advancements in Large Language\nModels (LLMs) offer a potential solution for automating conversational medical\nhistory-taking and improving diagnostic accuracy. This study evaluates the\nfeasibility and performance of LLMs in those tasks for infertility cases. An\nAI-driven conversational system was developed to simulate physician-patient\ninteractions with ChatGPT-4o and ChatGPT-4o-mini. A total of 70 real-world\ninfertility cases were processed, generating 420 diagnostic histories. Model\nperformance was assessed using F1 score, Differential Diagnosis (DDs) Accuracy,\nand Accuracy of Infertility Type Judgment (ITJ). ChatGPT-4o-mini outperformed\nChatGPT-4o in information extraction accuracy (F1 score: 0.9258 vs. 0.9029, p =\n0.045, d = 0.244) and demonstrated higher completeness in medical\nhistory-taking (97.58% vs. 77.11%), suggesting that ChatGPT-4o-mini is more\neffective in extracting detailed patient information, which is critical for\nimproving diagnostic accuracy. In contrast, ChatGPT-4o performed slightly\nbetter in differential diagnosis accuracy (2.0524 vs. 2.0048, p > 0.05). ITJ\naccuracy was higher in ChatGPT-4o-mini (0.6476 vs. 0.5905) but with lower\nconsistency (Cronbach's $\\alpha$ = 0.562), suggesting variability in\nclassification reliability. Both models demonstrated strong feasibility in\nautomating infertility history-taking, with ChatGPT-4o-mini excelling in\ncompleteness and extraction accuracy. In future studies, expert validation for\naccuracy and dependability in a clinical setting, AI model fine-tuning, and\nlarger datasets with a mix of cases of infertility have to be prioritized."}
{"id": "2504.00191", "pdf": "https://arxiv.org/pdf/2504.00191", "abs": "https://arxiv.org/abs/2504.00191", "authors": ["Lin Zhao", "Xin Yu", "Yikang Liu", "Xiao Chen", "Eric Z. Chen", "Terrence Chen", "Shanhui Sun"], "title": "Leveraging Diffusion Model and Image Foundation Model for Improved Correspondence Matching in Coronary Angiography", "categories": ["cs.CV"], "comment": null, "summary": "Accurate correspondence matching in coronary angiography images is crucial\nfor reconstructing 3D coronary artery structures, which is essential for\nprecise diagnosis and treatment planning of coronary artery disease (CAD).\nTraditional matching methods for natural images often fail to generalize to\nX-ray images due to inherent differences such as lack of texture, lower\ncontrast, and overlapping structures, compounded by insufficient training data.\nTo address these challenges, we propose a novel pipeline that generates\nrealistic paired coronary angiography images using a diffusion model\nconditioned on 2D projections of 3D reconstructed meshes from Coronary Computed\nTomography Angiography (CCTA), providing high-quality synthetic data for\ntraining. Additionally, we employ large-scale image foundation models to guide\nfeature aggregation, enhancing correspondence matching accuracy by focusing on\nsemantically relevant regions and keypoints. Our approach demonstrates superior\nmatching performance on synthetic datasets and effectively generalizes to\nreal-world datasets, offering a practical solution for this task. Furthermore,\nour work investigates the efficacy of different foundation models in\ncorrespondence matching, providing novel insights into leveraging advanced\nimage foundation models for medical imaging applications."}
{"id": "2504.00132", "pdf": "https://arxiv.org/pdf/2504.00132", "abs": "https://arxiv.org/abs/2504.00132", "authors": ["Aleksandra Bakalova", "Yana Veitsman", "Xinting Huang", "Michael Hahn"], "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."}
{"id": "2504.00200", "pdf": "https://arxiv.org/pdf/2504.00200", "abs": "https://arxiv.org/abs/2504.00200", "authors": ["Savinay Nagendra", "Kashif Rashid"], "title": "SmartScan: An AI-based Interactive Framework for Automated Region Extraction from Satellite Images", "categories": ["cs.CV"], "comment": null, "summary": "The deployment of a continuous methane monitoring system requires determining\nthe optimal number and placement of fixed sensors. However, planning is\nlabor-intensive, requiring extensive site setup and iteration to meet client\nrestrictions. This challenge is amplified when evaluating multiple sites,\nlimiting scalability. To address this, we introduce SmartScan, an AI framework\nthat automates data extraction for optimal sensor placement. SmartScan\nidentifies subspaces of interest from satellite images using an interactive\ntool to create facility-specific constraint sets efficiently. SmartScan\nleverages the Segment Anything Model (SAM), a prompt-based transformer for\nzero-shot segmentation, enabling subspace extraction without explicit training.\nIt operates in two modes: (1) Data Curation Mode, where satellite images are\nprocessed to extract high-quality subspaces using an interactive prompting\nsystem for SAM, and (2) Autonomous Mode, where user-curated prompts train a\ndeep learning network to replace manual prompting, fully automating subspace\nextraction. The interactive tool also serves for quality control, allowing\nusers to refine AI-generated outputs and generate additional constraint sets as\nneeded. With its AI-driven prompting mechanism, SmartScan delivers\nhigh-throughput, high-quality subspace extraction with minimal human\nintervention, enhancing scalability and efficiency. Notably, its adaptable\ndesign makes it suitable for extracting regions of interest from\nultra-high-resolution satellite imagery across various domains."}
{"id": "2504.00147", "pdf": "https://arxiv.org/pdf/2504.00147", "abs": "https://arxiv.org/abs/2504.00147", "authors": ["Collin Zhang", "John X. Morris", "Vitaly Shmatikov"], "title": "Universal Zero-shot Embedding Inversion", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Embedding inversion, i.e., reconstructing text given its embedding and\nblack-box access to the embedding encoder, is a fundamental problem in both NLP\nand security. From the NLP perspective, it helps determine how much semantic\ninformation about the input is retained in the embedding. From the security\nperspective, it measures how much information is leaked by vector databases and\nembedding-based retrieval systems. State-of-the-art methods for embedding\ninversion, such as vec2text, have high accuracy but require (a) training a\nseparate model for each embedding, and (b) a large number of queries to the\ncorresponding encoder.\n  We design, implement, and evaluate ZSInvert, a zero-shot inversion method\nbased on the recently proposed adversarial decoding technique. ZSInvert is\nfast, query-efficient, and can be used for any text embedding without training\nan embedding-specific inversion model. We measure the effectiveness of ZSInvert\non several embeddings and demonstrate that it recovers key semantic information\nabout the corresponding texts."}
{"id": "2504.00204", "pdf": "https://arxiv.org/pdf/2504.00204", "abs": "https://arxiv.org/abs/2504.00204", "authors": ["Rustam Tagiew", "Ilkay Wunderlich", "Mark Sastuba", "Steffen Seitz"], "title": "RailGoerl24: GÃ¶rlitz Rail Test Center CV Dataset 2024", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, 5 figures, submitted to Engineering Reliable Autonomous\n  Systems 2025", "summary": "Driverless train operation for open tracks on urban guided transport and\nmainline railways requires, among other things automatic detection of actual\nand potential obstacles, especially humans, in the danger zone of the train's\npath. Machine learning algorithms have proven to be powerful state-of-the-art\ntools for this task. However, these algorithms require large amounts of\nhigh-quality annotated data containing human beings in railway-specific\nenvironments as training data. Unfortunately, the amount of publicly available\ndatasets is not yet sufficient and is significantly inferior to the datasets in\nthe road domain. Therefore, this paper presents RailGoerl24, an on-board visual\nlight Full HD camera dataset of 12205 frames recorded in a railway test center\nof T\\\"UV S\\\"UD Rail, in G\\\"orlitz, Germany. Its main purpose is to support the\ndevelopment of driverless train operation for guided transport. RailGoerl24\nalso includes a terrestrial LiDAR scan covering parts of the area used to\nacquire the RGB data. In addition to the raw data, the dataset contains 33556\nboxwise annotations in total for the object class 'person'. The faces of\nrecorded actors are not blurred or altered in any other way. RailGoerl24, soon\navailable at data.fid-move.de/dataset/railgoerl24, can also be used for tasks\nbeyond collision prediction."}
{"id": "2504.00163", "pdf": "https://arxiv.org/pdf/2504.00163", "abs": "https://arxiv.org/abs/2504.00163", "authors": ["Yilin Qi", "Dong Won Lee", "Cynthia Breazeal", "Hae Won Park"], "title": "Does \"Reasoning\" with Large Language Models Improve Recognizing, Generating, and Reframing Unhelpful Thoughts?", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures (including appendix)", "summary": "Cognitive Reframing, a core element of Cognitive Behavioral Therapy (CBT),\nhelps individuals reinterpret negative experiences by finding positive meaning.\nRecent advances in Large Language Models (LLMs) have demonstrated improved\nperformance through reasoning-based strategies. This inspires a promising\ndirection of leveraging the reasoning capabilities of LLMs to improve CBT and\nmental reframing by simulating the process of critical thinking, potentially\nenabling more effective recognition, generation, and reframing of cognitive\ndistortions. In this work, we investigate the role of various reasoning\nmethods, including pre-trained reasoning LLMs and augmented reasoning\nstrategies such as CoT and self-consistency in enhancing LLMs' ability to\nperform cognitive reframing tasks. We find that augmented reasoning methods,\neven when applied to \"outdated\" LLMs like GPT-3.5, consistently outperform\nstate-of-the-art pretrained reasoning models on recognizing, generating and\nreframing unhelpful thoughts."}
{"id": "2504.00219", "pdf": "https://arxiv.org/pdf/2504.00219", "abs": "https://arxiv.org/abs/2504.00219", "authors": ["Han Zhou", "Wei Dong", "Jun Chen"], "title": "LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free 3D Gaussian Splatting and Physical Priors", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. 3DGS, Adverse illumination conditions,\n  Reference-free, Physical priors", "summary": "Directly employing 3D Gaussian Splatting (3DGS) on images with adverse\nillumination conditions exhibits considerable difficulty in achieving\nhigh-quality, normally-exposed representations due to: (1) The limited\nStructure from Motion (SfM) points estimated in adverse illumination scenarios\nfail to capture sufficient scene details; (2) Without ground-truth references,\nthe intensive information loss, significant noise, and color distortion pose\nsubstantial challenges for 3DGS to produce high-quality results; (3) Combining\nexisting exposure correction methods with 3DGS does not achieve satisfactory\nperformance due to their individual enhancement processes, which lead to the\nillumination inconsistency between enhanced images from different viewpoints.\nTo address these issues, we propose LITA-GS, a novel illumination-agnostic\nnovel view synthesis method via reference-free 3DGS and physical priors.\nFirstly, we introduce an illumination-invariant physical prior extraction\npipeline. Secondly, based on the extracted robust spatial structure prior, we\ndevelop the lighting-agnostic structure rendering strategy, which facilitates\nthe optimization of the scene structure and object appearance. Moreover, a\nprogressive denoising module is introduced to effectively mitigate the noise\nwithin the light-invariant representation. We adopt the unsupervised strategy\nfor the training of LITA-GS and extensive experiments demonstrate that LITA-GS\nsurpasses the state-of-the-art (SOTA) NeRF-based method while enjoying faster\ninference speed and costing reduced training time. The code is released at\nhttps://github.com/LowLevelAI/LITA-GS."}
{"id": "2504.00178", "pdf": "https://arxiv.org/pdf/2504.00178", "abs": "https://arxiv.org/abs/2504.00178", "authors": ["Craig W. Schmidt", "Varshini Reddy", "Chris Tanner", "Yuval Pinter"], "title": "Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "Pre-tokenization, the initial step in many modern tokenization pipelines,\nsegments text into smaller units called pretokens, typically splitting on\nwhitespace and punctuation. While this process encourages having full,\nindividual words as tokens, it introduces a fundamental limitation in most\ntokenization algorithms such as Byte Pair Encoding (BPE). Specifically,\npre-tokenization causes the distribution of tokens in a corpus to heavily skew\ntowards common, full-length words. This skewed distribution limits the benefits\nof expanding to larger vocabularies, since the additional tokens appear with\nprogressively lower counts. To overcome this barrier, we propose BoundlessBPE,\na modified BPE algorithm that relaxes the pretoken boundary constraint. Our\napproach selectively merges two complete pretokens into a larger unit we term a\nsuperword. Superwords are not necessarily semantically cohesive. For example,\nthe pretokens \" of\" and \" the\" might be combined to form the superword \" of\nthe\". This merging strategy results in a substantially more uniform\ndistribution of tokens across a corpus than standard BPE, and compresses text\nmore effectively, with an approximate 20% increase in bytes per token."}
{"id": "2504.00247", "pdf": "https://arxiv.org/pdf/2504.00247", "abs": "https://arxiv.org/abs/2504.00247", "authors": ["S. Mazdak Abulnaga", "Andrew Hoopes", "Neel Dey", "Malte Hoffmann", "Marianne Rakic", "Bruce Fischl", "John Guttag", "Adrian Dalca"], "title": "MultiMorph: On-demand Atlas Construction", "categories": ["cs.CV", "cs.AI"], "comment": "accepted to CVPR 2025", "summary": "We present MultiMorph, a fast and efficient method for constructing\nanatomical atlases on the fly. Atlases capture the canonical structure of a\ncollection of images and are essential for quantifying anatomical variability\nacross populations. However, current atlas construction methods often require\ndays to weeks of computation, thereby discouraging rapid experimentation. As a\nresult, many scientific studies rely on suboptimal, precomputed atlases from\nmismatched populations, negatively impacting downstream analyses. MultiMorph\naddresses these challenges with a feedforward model that rapidly produces\nhigh-quality, population-specific atlases in a single forward pass for any 3D\nbrain dataset, without any fine-tuning or optimization. MultiMorph is based on\na linear group-interaction layer that aggregates and shares features within the\ngroup of input images. Further, by leveraging auxiliary synthetic data,\nMultiMorph generalizes to new imaging modalities and population groups at\ntest-time. Experimentally, MultiMorph outperforms state-of-the-art\noptimization-based and learning-based atlas construction methods in both small\nand large population settings, with a 100-fold reduction in time. This makes\nMultiMorph an accessible framework for biomedical researchers without machine\nlearning expertise, enabling rapid, high-quality atlas generation for diverse\nstudies."}
{"id": "2504.00180", "pdf": "https://arxiv.org/pdf/2504.00180", "abs": "https://arxiv.org/abs/2504.00180", "authors": ["Vignesh Gokul", "Srikanth Tenneti", "Alwarappan Nakkiran"], "title": "Contradiction Detection in RAG Systems: Evaluating LLMs as Context Validators for Improved Information Consistency", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) systems have emerged as a powerful\nmethod for enhancing large language models (LLMs) with up-to-date information.\nHowever, the retrieval step in RAG can sometimes surface documents containing\ncontradictory information, particularly in rapidly evolving domains such as\nnews. These contradictions can significantly impact the performance of LLMs,\nleading to inconsistent or erroneous outputs. This study addresses this\ncritical challenge in two ways. First, we present a novel data generation\nframework to simulate different types of contradictions that may occur in the\nretrieval stage of a RAG system. Second, we evaluate the robustness of\ndifferent LLMs in performing as context validators, assessing their ability to\ndetect contradictory information within retrieved document sets. Our\nexperimental results reveal that context validation remains a challenging task\neven for state-of-the-art LLMs, with performance varying significantly across\ndifferent types of contradictions. While larger models generally perform better\nat contradiction detection, the effectiveness of different prompting strategies\nvaries across tasks and model architectures. We find that chain-of-thought\nprompting shows notable improvements for some models but may hinder performance\nin others, highlighting the complexity of the task and the need for more robust\napproaches to context validation in RAG systems."}
{"id": "2504.00270", "pdf": "https://arxiv.org/pdf/2504.00270", "abs": "https://arxiv.org/abs/2504.00270", "authors": ["Tianqi", "Ding", "Dawei Xiang", "Yijiashun Qi", "Ze Yang", "Zunduo Zhao", "Tianyao Sun", "Pengbin Feng", "Haoyu Wang"], "title": "NeRF-Based defect detection", "categories": ["cs.CV"], "comment": "6 pages, 11 figures, 2025 2nd International Conference on Remote\n  Sensing, Mapping and Image Processing (RSMIP 2025)", "summary": "The rapid growth of industrial automation has highlighted the need for\nprecise and efficient defect detection in large-scale machinery. Traditional\ninspection techniques, involving manual procedures such as scaling tall\nstructures for visual evaluation, are labor-intensive, subjective, and often\nhazardous. To overcome these challenges, this paper introduces an automated\ndefect detection framework built on Neural Radiance Fields (NeRF) and the\nconcept of digital twins. The system utilizes UAVs to capture images and\nreconstruct 3D models of machinery, producing both a standard reference model\nand a current-state model for comparison. Alignment of the models is achieved\nthrough the Iterative Closest Point (ICP) algorithm, enabling precise point\ncloud analysis to detect deviations that signify potential defects. By\neliminating manual inspection, this method improves accuracy, enhances\noperational safety, and offers a scalable solution for defect detection. The\nproposed approach demonstrates great promise for reliable and efficient\nindustrial applications."}
{"id": "2504.00187", "pdf": "https://arxiv.org/pdf/2504.00187", "abs": "https://arxiv.org/abs/2504.00187", "authors": ["Pouya Pezeshkpour", "Estevam Hruschka"], "title": "Insight-RAG: Enhancing LLMs with Insight-Driven Augmentation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) frameworks have shown significant\npromise in leveraging external knowledge to enhance the performance of large\nlanguage models (LLMs). However, conventional RAG methods often retrieve\ndocuments based solely on surface-level relevance, leading to many issues: they\nmay overlook deeply buried information within individual documents, miss\nrelevant insights spanning multiple sources, and are not well-suited for tasks\nbeyond traditional question answering. In this paper, we propose Insight-RAG, a\nnovel framework designed to address these issues. In the initial stage of\nInsight-RAG, instead of using traditional retrieval methods, we employ an LLM\nto analyze the input query and task, extracting the underlying informational\nrequirements. In the subsequent stage, a specialized LLM -- trained on the\ndocument database -- is queried to mine content that directly addresses these\nidentified insights. Finally, by integrating the original query with the\nretrieved insights, similar to conventional RAG approaches, we employ a final\nLLM to generate a contextually enriched and accurate response. Using two\nscientific paper datasets, we created evaluation benchmarks targeting each of\nthe mentioned issues and assessed Insight-RAG against traditional RAG pipeline.\nOur results demonstrate that the Insight-RAG pipeline successfully addresses\nthese challenges, outperforming existing methods by a significant margin in\nmost cases. These findings suggest that integrating insight-driven retrieval\nwithin the RAG framework not only enhances performance but also broadens the\napplicability of RAG to tasks beyond conventional question answering."}
{"id": "2504.00348", "pdf": "https://arxiv.org/pdf/2504.00348", "abs": "https://arxiv.org/abs/2504.00348", "authors": ["Kyle Stein", "Andrew A. Mahyari", "Guillermo Francia III", "Eman El-Sheikh"], "title": "Transductive One-Shot Learning Meet Subspace Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "One-shot learning focuses on adapting pretrained models to recognize newly\nintroduced and unseen classes based on a single labeled image. While variations\nof few-shot and zero-shot learning exist, one-shot learning remains a\nchallenging yet crucial problem due to its ability to generalize knowledge to\nunseen classes from just one human-annotated image. In this paper, we introduce\na transductive one-shot learning approach that employs subspace decomposition\nto utilize the information from labeled images in the support set and unlabeled\nimages in the query set. These images are decomposed into a linear combination\nof latent variables representing primitives captured by smaller subspaces. By\nrepresenting images in the query set as linear combinations of these latent\nprimitives, we can propagate the label from a single image in the support set\nto query images that share similar combinations of primitives. Through a\ncomprehensive quantitative analysis across various neural network feature\nextractors and datasets, we demonstrate that our approach can effectively\ngeneralize to novel classes from just one labeled image."}
{"id": "2504.00241", "pdf": "https://arxiv.org/pdf/2504.00241", "abs": "https://arxiv.org/abs/2504.00241", "authors": ["Rabimba Karanjai", "Boris Shor", "Amanda Austin", "Ryan Kennedy", "Yang Lu", "Lei Xu", "Weidong Shi"], "title": "Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future to eDemorcacy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the use of Large Language Models (LLMs) to synthesize\npublic opinion data, addressing challenges in traditional survey methods like\ndeclining response rates and non-response bias. We introduce a novel technique:\nrole creation based on knowledge injection, a form of in-context learning that\nleverages RAG and specified personality profiles from the HEXACO model and\ndemographic information, and uses that for dynamically generated prompts. This\nmethod allows LLMs to simulate diverse opinions more accurately than existing\nprompt engineering approaches. We compare our results with pre-trained models\nwith standard few-shot prompts. Experiments using questions from the\nCooperative Election Study (CES) demonstrate that our role-creation approach\nsignificantly improves the alignment of LLM-generated opinions with real-world\nhuman survey responses, increasing answer adherence. In addition, we discuss\nchallenges, limitations and future research directions."}
{"id": "2504.00356", "pdf": "https://arxiv.org/pdf/2504.00356", "abs": "https://arxiv.org/abs/2504.00356", "authors": ["Ting Liu", "Siyuan Li"], "title": "Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "accepted to CVPR2025", "summary": "Recent advances in zero-shot referring image segmentation (RIS), driven by\nmodels such as the Segment Anything Model (SAM) and CLIP, have made substantial\nprogress in aligning visual and textual information. Despite these successes,\nthe extraction of precise and high-quality mask region representations remains\na critical challenge, limiting the full potential of RIS tasks. In this paper,\nwe introduce a training-free, hybrid global-local feature extraction approach\nthat integrates detailed mask-specific features with contextual information\nfrom the surrounding area, enhancing mask region representation. To further\nstrengthen alignment between mask regions and referring expressions, we propose\na spatial guidance augmentation strategy that improves spatial coherence, which\nis essential for accurately localizing described areas. By incorporating\nmultiple spatial cues, this approach facilitates more robust and precise\nreferring segmentation. Extensive experiments on standard RIS benchmarks\ndemonstrate that our method significantly outperforms existing zero-shot RIS\nmodels, achieving substantial performance gains. We believe our approach\nadvances RIS tasks and establishes a versatile framework for region-text\nalignment, offering broader implications for cross-modal understanding and\ninteraction. Code is available at https://github.com/fhgyuanshen/HybridGL ."}
{"id": "2504.00255", "pdf": "https://arxiv.org/pdf/2504.00255", "abs": "https://arxiv.org/abs/2504.00255", "authors": ["Yanzheng Xiang", "Hanqi Yan", "Shuyin Ouyang", "Lin Gui", "Yulan He"], "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": null, "summary": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions from recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent\nthat interprets algorithmic concepts from literature and a Code Agent that\nretrieves dependencies from repositories and implement solutions. To assess\nalgorithm understanding, we introduce reasoning graph accuracy, which\nquantifies similarity between generated and reference reasoning graphs derived\nfrom code comments and structure. For evaluating implementation quality, we\nemploy execution accuracy, CodeBLEU, and repository dependency/API recall\nmetrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs\nand Reasoning LLMs as foundational models. The best-performing LLM using\nSci-Reproducer achieves only 39% execution accuracy, highlighting the\nbenchmark's difficulty.Our analysis identifies missing or inconsistent\nalgorithm descriptions as key barriers to successful reproduction. We will\nopen-source our benchmark, and code at\nhttps://github.com/xyzCS/SciReplicate-Bench."}
{"id": "2504.00370", "pdf": "https://arxiv.org/pdf/2504.00370", "abs": "https://arxiv.org/abs/2504.00370", "authors": ["Tiantian Xie", "Pengpai Wang", "Rosa H. M. Chan"], "title": "Spatiotemporal Attention Learning Framework for Event-Driven Object Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "2025 IEEE NSENS", "summary": "Event-based vision sensors, inspired by biological neural systems,\nasynchronously capture local pixel-level intensity changes as a sparse event\nstream containing position, polarity, and timestamp information. These\nneuromorphic sensors offer significant advantages in dynamic range, latency,\nand power efficiency. Their working principle inherently addresses traditional\ncamera limitations such as motion blur and redundant background information,\nmaking them particularly suitable for dynamic vision tasks. While recent works\nhave proposed increasingly complex event-based architectures, the computational\noverhead and parameter complexity of these approaches limit their practical\ndeployment. This paper presents a novel spatiotemporal learning framework for\nevent-based object recognition, utilizing a VGG network enhanced with\nConvolutional Block Attention Module (CBAM). Our approach achieves comparable\nperformance to state-of-the-art ResNet-based methods while reducing parameter\ncount by 2.3% compared to the original VGG model. Specifically, it outperforms\nResNet-based methods like MVF-Net, achieving the highest Top-1 accuracy of\n76.4% (pretrained) and 71.3% (not pretrained) on CIFAR10-DVS, and 72.4% (not\npretrained) on N-Caltech101. These results highlight the robustness of our\nmethod when pretrained weights are not used, making it suitable for scenarios\nwhere transfer learning is unavailable. Moreover, our approach reduces reliance\non data augmentation. Experimental results on standard event-based datasets\ndemonstrate the framework's efficiency and effectiveness for real-world\napplications."}
{"id": "2504.00265", "pdf": "https://arxiv.org/pdf/2504.00265", "abs": "https://arxiv.org/abs/2504.00265", "authors": ["Mikhail Krasitskii", "Grigori Sidorov", "Olga Kolesnikova", "Liliana Chanona Hernandez", "Alexander Gelbukh"], "title": "Multilingual Sentiment Analysis of Summarized Texts: A Cross-Language Study of Text Shortening Effects", "categories": ["cs.CL"], "comment": null, "summary": "Summarization significantly impacts sentiment analysis across languages with\ndiverse morphologies. This study examines extractive and abstractive\nsummarization effects on sentiment classification in English, German, French,\nSpanish, Italian, Finnish, Hungarian, and Arabic. We assess sentiment shifts\npost-summarization using multilingual transformers (mBERT, XLM-RoBERTa, T5, and\nBART) and language-specific models (FinBERT, AraBERT). Results show extractive\nsummarization better preserves sentiment, especially in morphologically complex\nlanguages, while abstractive summarization improves readability but introduces\nsentiment distortion, affecting sentiment accuracy. Languages with rich\ninflectional morphology, such as Finnish, Hungarian, and Arabic, experience\ngreater accuracy drops than English or German. Findings emphasize the need for\nlanguage-specific adaptations in sentiment analysis and propose a hybrid\nsummarization approach balancing readability and sentiment preservation. These\ninsights benefit multilingual sentiment applications, including social media\nmonitoring, market analysis, and cross-lingual opinion mining."}
{"id": "2504.00375", "pdf": "https://arxiv.org/pdf/2504.00375", "abs": "https://arxiv.org/abs/2504.00375", "authors": ["Xin Zhang", "Keren Fu", "Qijun Zhao"], "title": "CamoSAM2: Motion-Appearance Induced Auto-Refining Prompts for Video Camouflaged Object Detection", "categories": ["cs.CV"], "comment": "10 pages, 5 figures,", "summary": "The Segment Anything Model 2 (SAM2), a prompt-guided video foundation model,\nhas remarkably performed in video object segmentation, drawing significant\nattention in the community. Due to the high similarity between camouflaged\nobjects and their surroundings, which makes them difficult to distinguish even\nby the human eye, the application of SAM2 for automated segmentation in\nreal-world scenarios faces challenges in camouflage perception and reliable\nprompts generation. To address these issues, we propose CamoSAM2, a\nmotion-appearance prompt inducer (MAPI) and refinement framework to\nautomatically generate and refine prompts for SAM2, enabling high-quality\nautomatic detection and segmentation in VCOD task. Initially, we introduce a\nprompt inducer that simultaneously integrates motion and appearance cues to\ndetect camouflaged objects, delivering more accurate initial predictions than\nexisting methods. Subsequently, we propose a video-based adaptive multi-prompts\nrefinement (AMPR) strategy tailored for SAM2, aimed at mitigating prompt error\nin initial coarse masks and further producing good prompts. Specifically, we\nintroduce a novel three-step process to generate reliable prompts by\ncamouflaged object determination, pivotal prompting frame selection, and\nmulti-prompts formation. Extensive experiments conducted on two benchmark\ndatasets demonstrate that our proposed model, CamoSAM2, significantly\noutperforms existing state-of-the-art methods, achieving increases of 8.0% and\n10.1% in mIoU metric. Additionally, our method achieves the fastest inference\nspeed compared to current VCOD models."}
{"id": "2504.00274", "pdf": "https://arxiv.org/pdf/2504.00274", "abs": "https://arxiv.org/abs/2504.00274", "authors": ["Joshua Rodriguez", "Om Sanan", "Guillermo Vizarreta-Luna", "Steven A. Conrad"], "title": "Text Chunking for Document Classification for Urban System Management using Large Language Models", "categories": ["cs.CL", "cs.HC", "I.7.5; J.1"], "comment": "16 pages, 6 figures, 4 tables, 2 algorithms; Replication data and\n  code can be found https://github.com/josh-rodriguez-csu/ChunkingforLLMs", "summary": "Urban systems are managed using complex textual documentation that need\ncoding and analysis to set requirements and evaluate built environment\nperformance. This paper contributes to the study of applying large-language\nmodels (LLM) to qualitative coding activities to reduce resource requirements\nwhile maintaining comparable reliability to humans. Qualitative coding and\nassessment face challenges like resource limitations and bias, accuracy, and\nconsistency between human evaluators. Here we report the application of LLMs to\ndeductively code 10 case documents on the presence of 17 digital twin\ncharacteristics for the management of urban systems. We utilize two prompting\nmethods to compare the semantic processing of LLMs with human coding efforts:\nwhole text analysis and text chunk analysis using OpenAI's GPT-4o, GPT-4o-mini,\nand o1-mini models. We found similar trends of internal variability between\nmethods and results indicate that LLMs may perform on par with human coders\nwhen initialized with specific deductive coding contexts. GPT-4o, o1-mini and\nGPT-4o-mini showed significant agreement with human raters when employed using\na chunking method. The application of both GPT-4o and GPT-4o-mini as an\nadditional rater with three manual raters showed statistically significant\nagreement across all raters, indicating that the analysis of textual documents\nis benefited by LLMs. Our findings reveal nuanced sub-themes of LLM application\nsuggesting LLMs follow human memory coding processes where whole-text analysis\nmay introduce multiple meanings. The novel contributions of this paper lie in\nassessing the performance of OpenAI GPT models and introduces the chunk-based\nprompting approach, which addresses context aggregation biases by preserving\nlocalized context."}
{"id": "2504.00379", "pdf": "https://arxiv.org/pdf/2504.00379", "abs": "https://arxiv.org/abs/2504.00379", "authors": ["Zhiyuan Zhang", "Xiaofan Li", "Zhihao Xu", "Wenjie Peng", "Zijian Zhou", "Miaojing Shi", "Shuangping Huang"], "title": "MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Autonomous driving visual question answering (AD-VQA) aims to answer\nquestions related to perception, prediction, and planning based on given\ndriving scene images, heavily relying on the model's spatial understanding\ncapabilities. Prior works typically express spatial information through textual\nrepresentations of coordinates, resulting in semantic gaps between visual\ncoordinate representations and textual descriptions. This oversight hinders the\naccurate transmission of spatial information and increases the expressive\nburden. To address this, we propose a novel Marker-based Prompt learning\nframework (MPDrive), which represents spatial coordinates by concise visual\nmarkers, ensuring linguistic expressive consistency and enhancing the accuracy\nof both visual perception and spatial expression in AD-VQA. Specifically, we\ncreate marker images by employing a detection expert to overlay object regions\nwith numerical labels, converting complex textual coordinate generation into\nstraightforward text-based visual marker predictions. Moreover, we fuse\noriginal and marker images as scene-level features and integrate them with\ndetection priors to derive instance-level features. By combining these\nfeatures, we construct dual-granularity visual prompts that stimulate the LLM's\nspatial perception capabilities. Extensive experiments on the DriveLM and\nCODA-LM datasets show that MPDrive achieves state-of-the-art performance,\nparticularly in cases requiring sophisticated spatial understanding."}
{"id": "2504.00285", "pdf": "https://arxiv.org/pdf/2504.00285", "abs": "https://arxiv.org/abs/2504.00285", "authors": ["Samuel M. Taylor", "Benjamin K. Bergen"], "title": "Do Large Language Models Exhibit Spontaneous Rational Deception?", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are effective at deceiving, when prompted to do\nso. But under what conditions do they deceive spontaneously? Models that\ndemonstrate better performance on reasoning tasks are also better at prompted\ndeception. Do they also increasingly deceive spontaneously in situations where\nit could be considered rational to do so? This study evaluates spontaneous\ndeception produced by LLMs in a preregistered experimental protocol using tools\nfrom signaling theory. A range of proprietary closed-source and open-source\nLLMs are evaluated using modified 2x2 games (in the style of Prisoner's\nDilemma) augmented with a phase in which they can freely communicate to the\nother agent using unconstrained language. This setup creates an opportunity to\ndeceive, in conditions that vary in how useful deception might be to an agent's\nrational self-interest. The results indicate that 1) all tested LLMs\nspontaneously misrepresent their actions in at least some conditions, 2) they\nare generally more likely to do so in situations in which deception would\nbenefit them, and 3) models exhibiting better reasoning capacity overall tend\nto deceive at higher rates. Taken together, these results suggest a tradeoff\nbetween LLM reasoning capability and honesty. They also provide evidence of\nreasoning-like behavior in LLMs from a novel experimental configuration.\nFinally, they reveal certain contextual factors that affect whether LLMs will\ndeceive or not. We discuss consequences for autonomous, human-facing systems\ndriven by LLMs both now and as their reasoning capabilities continue to\nimprove."}
{"id": "2504.00380", "pdf": "https://arxiv.org/pdf/2504.00380", "abs": "https://arxiv.org/abs/2504.00380", "authors": ["Yang Hai", "Guo Wang", "Tan Su", "Wenjie Jiang", "Yinlin Hu"], "title": "Hierarchical Flow Diffusion for Efficient Frame Interpolation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Most recent diffusion-based methods still show a large gap compared to\nnon-diffusion methods for video frame interpolation, in both accuracy and\nefficiency. Most of them formulate the problem as a denoising procedure in\nlatent space directly, which is less effective caused by the large latent\nspace. We propose to model bilateral optical flow explicitly by hierarchical\ndiffusion models, which has much smaller search space in the denoising\nprocedure. Based on the flow diffusion model, we then use a flow-guided images\nsynthesizer to produce the final result. We train the flow diffusion model and\nthe image synthesizer end to end. Our method achieves state of the art in\naccuracy, and 10+ times faster than other diffusion-based methods. The project\npage is at: https://hfd-interpolation.github.io."}
{"id": "2504.00289", "pdf": "https://arxiv.org/pdf/2504.00289", "abs": "https://arxiv.org/abs/2504.00289", "authors": ["Andrea W Wen-Yi", "Unso Eun Seo Jo", "David Mimno"], "title": "Do Chinese models speak Chinese languages?", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "First and Second author contribute equally", "summary": "The release of top-performing open-weight LLMs has cemented China's role as a\nleading force in AI development. Do these models support languages spoken in\nChina? Or do they speak the same languages as Western models? Comparing\nmultilingual capabilities is important for two reasons. First, language ability\nprovides insights into pre-training data curation, and thus into resource\nallocation and development priorities. Second, China has a long history of\nexplicit language policy, varying between inclusivity of minority languages and\na Mandarin-first policy. To test whether Chinese LLMs today reflect an agenda\nabout China's languages, we test performance of Chinese and Western open-source\nLLMs on Asian regional and Chinese minority languages. Our experiments on\nInformation Parity and reading comprehension show Chinese models' performance\nacross these languages correlates strongly (r=0.93) with Western models', with\nthe sole exception being better Mandarin. Sometimes, Chinese models cannot\nidentify languages spoken by Chinese minorities such as Kazakh and Uyghur, even\nthough they are good at French and German. These results provide a window into\ncurrent development priorities, suggest options for future development, and\nindicate guidance for end users."}
{"id": "2504.00382", "pdf": "https://arxiv.org/pdf/2504.00382", "abs": "https://arxiv.org/abs/2504.00382", "authors": ["Wanjing Zhang", "Chenxing Wang"], "title": "Intrinsic-feature-guided 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based 3D object detection is essential for autonomous driving systems.\nHowever, LiDAR point clouds may appear to have sparsity, uneven distribution,\nand incomplete structures, significantly limiting the detection performance. In\nroad driving environments, target objects referring to vehicles, pedestrians\nand cyclists are well-suited for enhancing representation through the complete\ntemplate guidance, considering their grid and topological structures.\nTherefore, this paper presents an intrinsic-feature-guided 3D object detection\nmethod based on a template-assisted feature enhancement module, which extracts\nintrinsic features from relatively generalized templates and provides rich\nstructural information for foreground objects. Furthermore, a proposal-level\ncontrastive learning mechanism is designed to enhance the feature differences\nbetween foreground and background objects. The proposed modules can act as\nplug-and-play components and improve the performance of multiple existing\nmethods. Extensive experiments illustrate that the proposed method achieves the\nhighly competitive detection results. Code will be available at\nhttps://github.com/zhangwanjingjj/IfgNet.git."}
{"id": "2504.00310", "pdf": "https://arxiv.org/pdf/2504.00310", "abs": "https://arxiv.org/abs/2504.00310", "authors": ["Rajeev Kumar", "Harishankar Kumar", "Kumari Shalini"], "title": "Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have revolutionized natural language processing with\ntheir surprising capability to understand and generate human-like text.\nHowever, many of these models inherit and further amplify the biases present in\ntheir training data, raising ethical and fairness concerns. The detection and\nmitigation of such biases are vital to ensuring that LLMs act responsibly and\nequitably across diverse domains. This work investigates Knowledge\nGraph-Augmented Training (KGAT) as a novel method to mitigate bias in LLM.\nUsing structured domain-specific knowledge from real-world knowledge graphs, we\nimprove the understanding of the model and reduce biased output. Public\ndatasets for bias assessment include Gender Shades, Bias in Bios, and FairFace,\nwhile metrics such as demographic parity and equal opportunity facilitate\nrigorous detection. We also performed targeted mitigation strategies to correct\nbiased associations, leading to a significant drop in biased output and\nimproved bias metrics. Equipped with real-world datasets and knowledge graphs,\nour framework is both scalable and effective, paving the way toward responsible\ndeployment in sensitive and high-stakes applications."}
{"id": "2504.00385", "pdf": "https://arxiv.org/pdf/2504.00385", "abs": "https://arxiv.org/abs/2504.00385", "authors": ["Yifan Liu", "Jiancheng Huang", "Na Liu", "Mingfu Yan", "Yi Huang", "Shifeng Chen"], "title": "Leveraging Contrast Information for Efficient Document Shadow Removal", "categories": ["cs.CV"], "comment": null, "summary": "Document shadows are a major obstacle in the digitization process. Due to the\ndense information in text and patterns covered by shadows, document shadow\nremoval requires specialized methods. Existing document shadow removal methods,\nalthough showing some progress, still rely on additional information such as\nshadow masks or lack generalization and effectiveness across different shadow\nscenarios. This often results in incomplete shadow removal or loss of original\ndocument content and tones. Moreover, these methods tend to underutilize the\ninformation present in the original shadowed document image. In this paper, we\nrefocus our approach on the document images themselves, which inherently\ncontain rich information.We propose an end-to-end document shadow removal\nmethod guided by contrast representation, following a coarse-to-fine refinement\napproach. By extracting document contrast information, we can effectively and\nquickly locate shadow shapes and positions without the need for additional\nmasks. This information is then integrated into the refined shadow removal\nprocess, providing better guidance for network-based removal and feature\nfusion. Extensive qualitative and quantitative experiments show that our method\nachieves state-of-the-art performance."}
{"id": "2504.00316", "pdf": "https://arxiv.org/pdf/2504.00316", "abs": "https://arxiv.org/abs/2504.00316", "authors": ["Dylan Bumford", "Simon Charlow"], "title": "Effect-driven interpretation: Functors for natural language composition", "categories": ["cs.CL"], "comment": null, "summary": "Computer programs are often factored into pure components -- simple, total\nfunctions from inputs to outputs -- and components that may have side effects\n-- errors, changes to memory, parallel threads, abortion of the current loop,\netc. We make the case that human languages are similarly organized around the\ngive and pull of pure values and impure processes, and we'll aim to show how\ndenotational techniques from computer science can be leveraged to support\nelegant and illuminating analyses of natural language composition."}
{"id": "2504.00387", "pdf": "https://arxiv.org/pdf/2504.00387", "abs": "https://arxiv.org/abs/2504.00387", "authors": ["Zilong Huang", "Jun He", "Junyan Ye", "Lihan Jiang", "Weijia Li", "Yiping Chen", "Ting Han"], "title": "Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single Panoramic Image for Your Immerse Exploration", "categories": ["cs.CV"], "comment": "CVPR 2025, 11 pages, 7 figures", "summary": "The reconstruction of immersive and realistic 3D scenes holds significant\npractical importance in various fields of computer vision and computer\ngraphics. Typically, immersive and realistic scenes should be free from\nobstructions by dynamic objects, maintain global texture consistency, and allow\nfor unrestricted exploration. The current mainstream methods for image-driven\nscene construction involves iteratively refining the initial image using a\nmoving virtual camera to generate the scene. However, previous methods struggle\nwith visual discontinuities due to global texture inconsistencies under varying\ncamera poses, and they frequently exhibit scene voids caused by\nforeground-background occlusions. To this end, we propose a novel layered 3D\nscene reconstruction framework from panoramic image, named Scene4U.\nSpecifically, Scene4U integrates an open-vocabulary segmentation model with a\nlarge language model to decompose a real panorama into multiple layers. Then,\nwe employs a layered repair module based on diffusion model to restore occluded\nregions using visual cues and depth information, generating a hierarchical\nrepresentation of the scene. The multi-layer panorama is then initialized as a\n3D Gaussian Splatting representation, followed by layered optimization, which\nultimately produces an immersive 3D scene with semantic and structural\nconsistency that supports free exploration. Scene4U outperforms\nstate-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE,\nwhile also achieving the fastest training speed. Additionally, to demonstrate\nthe robustness of Scene4U and allow users to experience immersive scenes from\nvarious landmarks, we build WorldVista3D dataset for 3D scene reconstruction,\nwhich contains panoramic images of globally renowned sites. The implementation\ncode and dataset will be released at https://github.com/LongHZ140516/Scene4U ."}
{"id": "2504.00339", "pdf": "https://arxiv.org/pdf/2504.00339", "abs": "https://arxiv.org/abs/2504.00339", "authors": ["Hoang Hai Phan", "Nguyen Duc Minh Vu", "Nam Dang Phuong"], "title": "VNJPTranslate: A comprehensive pipeline for Vietnamese-Japanese translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neural Machine Translation (NMT) driven by Transformer architectures has\nadvanced significantly, yet faces challenges with low-resource language pairs\nlike Vietnamese-Japanese (Vi-Ja). Issues include sparse parallel data and\nhandling linguistic/cultural nuances. Recent progress in Large Language Models\n(LLMs) with strong reasoning, often refined via Reinforcement Learning (RL),\nenables high-quality synthetic data generation. We introduce VNJPTranslate, a\npipeline designed to systematically address the Vi-Ja translation task. It\nfeatures a targeted data augmentation strategy using advanced LLMs with\nChain-of-Thought prompting for challenging segments identified via corpus\nanalysis. Subsequently, we employ efficient fine-tuning techniques (Unsloth\nwith QLoRA) on a capable, low-parameter autoregressive model (specifically, a\nfine-tuned version of the 1.8B parameter Sailor model, which is based on the\nQwen architecture) to create a practical and high-performing translation\nsystem. This integrated approach aims to improve Vi-Ja translation quality\nsignificantly over existing baselines."}
{"id": "2504.00394", "pdf": "https://arxiv.org/pdf/2504.00394", "abs": "https://arxiv.org/abs/2504.00394", "authors": ["Lei Wang", "Yujie Zhong", "Xiaopeng Sun", "Jingchun Cheng", "Chengjian Feng", "Qiong Cao", "Lin Ma", "Zhaoxin Fan"], "title": "AP-CAP: Advancing High-Quality Data Synthesis for Animal Pose Estimation via a Controllable Image Generation Pipeline", "categories": ["cs.CV"], "comment": null, "summary": "The task of 2D animal pose estimation plays a crucial role in advancing deep\nlearning applications in animal behavior analysis and ecological research.\nDespite notable progress in some existing approaches, our study reveals that\nthe scarcity of high-quality datasets remains a significant bottleneck,\nlimiting the full potential of current methods. To address this challenge, we\npropose a novel Controllable Image Generation Pipeline for synthesizing animal\npose estimation data, termed AP-CAP. Within this pipeline, we introduce a\nMulti-Modal Animal Image Generation Model capable of producing images with\nexpected poses. To enhance the quality and diversity of the generated data, we\nfurther propose three innovative strategies: (1) Modality-Fusion-Based Animal\nImage Synthesis Strategy to integrate multi-source appearance representations,\n(2) Pose-Adjustment-Based Animal Image Synthesis Strategy to dynamically\ncapture diverse pose variations, and (3) Caption-Enhancement-Based Animal Image\nSynthesis Strategy to enrich visual semantic understanding. Leveraging the\nproposed model and strategies, we create the MPCH Dataset\n(Modality-Pose-Caption Hybrid), the first hybrid dataset that innovatively\ncombines synthetic and real data, establishing the largest-scale multi-source\nheterogeneous benchmark repository for animal pose estimation to date.\nExtensive experiments demonstrate the superiority of our method in improving\nboth the performance and generalization capability of animal pose estimators."}
{"id": "2504.00343", "pdf": "https://arxiv.org/pdf/2504.00343", "abs": "https://arxiv.org/abs/2504.00343", "authors": ["Timo Spinde", "Luyang Lin", "Smi Hinterreiter", "Isao Echizen"], "title": "Leveraging Large Language Models for Automated Definition Extraction with TaxoMatic A Case Study on Media Bias", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces TaxoMatic, a framework that leverages large language\nmodels to automate definition extraction from academic literature. Focusing on\nthe media bias domain, the framework encompasses data collection, LLM-based\nrelevance classification, and extraction of conceptual definitions. Evaluated\non a dataset of 2,398 manually rated articles, the study demonstrates the\nframeworks effectiveness, with Claude-3-sonnet achieving the best results in\nboth relevance classification and definition extraction. Future directions\ninclude expanding datasets and applying TaxoMatic to additional domains."}
{"id": "2504.00396", "pdf": "https://arxiv.org/pdf/2504.00396", "abs": "https://arxiv.org/abs/2504.00396", "authors": ["Xiaole Xian", "Zhichao Liao", "Qingyu Li", "Wenyu Qin", "Pengfei Wan", "Weicheng Xie", "Long Zeng", "Linlin Shen", "Pingfa Feng"], "title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning", "categories": ["cs.CV"], "comment": null, "summary": "While fine-tuning pre-trained Text-to-Image (T2I) models on portrait datasets\nenables attribute customization, existing methods suffer from Semantic\nPollution that compromises the original model's behavior and prevents\nincremental learning. To address this, we propose SPF-Portrait, a pioneering\nwork to purely understand customized semantics while eliminating semantic\npollution in text-driven portrait customization. In our SPF-Portrait, we\npropose a dual-path pipeline that introduces the original model as a reference\nfor the conventional fine-tuning path. Through contrastive learning, we ensure\nadaptation to target attributes and purposefully align other unrelated\nattributes with the original portrait. We introduce a novel Semantic-Aware Fine\nControl Map, which represents the precise response regions of the target\nsemantics, to spatially guide the alignment process between the contrastive\npaths. This alignment process not only effectively preserves the performance of\nthe original model but also avoids over-alignment. Furthermore, we propose a\nnovel response enhancement mechanism to reinforce the performance of target\nattributes, while mitigating representation discrepancy inherent in direct\ncross-modal supervision. Extensive experiments demonstrate that SPF-Portrait\nachieves state-of-the-art performance."}
{"id": "2504.00374", "pdf": "https://arxiv.org/pdf/2504.00374", "abs": "https://arxiv.org/abs/2504.00374", "authors": ["Mahak Agarwal", "Divyam Khanna"], "title": "When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "comment": "10 pages, 6 figures", "summary": "In many real-world scenarios, a single Large Language Model (LLM) may\nencounter contradictory claims-some accurate, others forcefully incorrect-and\nmust judge which is true. We investigate this risk in a single-turn,\nmulti-agent debate framework: one LLM-based agent provides a factual answer\nfrom TruthfulQA, another vigorously defends a falsehood, and the same LLM\narchitecture serves as judge. We introduce the Confidence-Weighted Persuasion\nOverride Rate (CW-POR), which captures not only how often the judge is deceived\nbut also how strongly it believes the incorrect choice. Our experiments on five\nopen-source LLMs (3B-14B parameters), where we systematically vary agent\nverbosity (30-300 words), reveal that even smaller models can craft persuasive\narguments that override truthful answers-often with high confidence. These\nfindings underscore the importance of robust calibration and adversarial\ntesting to prevent LLMs from confidently endorsing misinformation."}
{"id": "2504.00400", "pdf": "https://arxiv.org/pdf/2504.00400", "abs": "https://arxiv.org/abs/2504.00400", "authors": ["Haodian Wang", "Yaqi Song"], "title": "Adaptive Low Light Enhancement via Joint Global-Local Illumination Adjustment", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Images captured under real-world low-light conditions face significant\nchallenges due to uneven ambient lighting, making it difficult for existing\nend-to-end methods to enhance images with a large dynamic range to normal\nexposure levels. To address the above issue, we propose a novel\nbrightness-adaptive enhancement framework designed to tackle the challenge of\nlocal exposure inconsistencies in real-world low-light images. Specifically,\nour proposed framework comprises two components: the Local Contrast Enhancement\nNetwork (LCEN) and the Global Illumination Guidance Network (GIGN). We\nintroduce an early stopping mechanism in the LCEN and design a local\ndiscriminative module, which adaptively perceives the contrast of different\nareas in the image to control the premature termination of the enhancement\nprocess for patches with varying exposure levels. Additionally, within the\nGIGN, we design a global attention guidance module that effectively models\nglobal illumination by capturing long-range dependencies and contextual\ninformation within the image, which guides the local contrast enhancement\nnetwork to significantly improve brightness across different regions. Finally,\nin order to coordinate the LCEN and GIGN, we design a novel training strategy\nto facilitate the training process. Experiments on multiple datasets\ndemonstrate that our method achieves superior quantitative and qualitative\nresults compared to state-of-the-art algorithms."}
{"id": "2504.00406", "pdf": "https://arxiv.org/pdf/2504.00406", "abs": "https://arxiv.org/abs/2504.00406", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent"}
{"id": "2504.00401", "pdf": "https://arxiv.org/pdf/2504.00401", "abs": "https://arxiv.org/abs/2504.00401", "authors": ["Wenbo Nie", "Lang Nie", "Chunyu Lin", "Jingwen Chen", "Ke Xing", "Jiyuan Wang", "Yao Zhao"], "title": "Beyond Wide-Angle Images: Unsupervised Video Portrait Correction via Spatiotemporal Diffusion Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Wide-angle cameras, despite their popularity for content creation, suffer\nfrom distortion-induced facial stretching-especially at the edge of the\nlens-which degrades visual appeal. To address this issue, we propose an image\nportrait correction framework using diffusion models named ImagePD. It\nintegrates the long-range awareness of transformer and multi-step denoising of\ndiffusion models into a unified framework, achieving global structural\nrobustness and local detail refinement. Besides, considering the high cost of\nobtaining video labels, we then repurpose ImagePD for unlabeled wide-angle\nvideos (termed VideoPD), by spatiotemporal diffusion adaption with spatial\nconsistency and temporal smoothness constraints. For the former, we encourage\nthe denoised image to approximate pseudo labels following the wide-angle\ndistortion distribution pattern, while for the latter, we derive rectification\ntrajectories with backward optical flows and smooth them. Compared with\nImagePD, VideoPD maintains high-quality facial corrections in space and\nmitigates the potential temporal shakes sequentially. Finally, to establish an\nevaluation benchmark and train the framework, we establish a video portrait\ndataset with a large diversity in people number, lighting conditions, and\nbackground. Experiments demonstrate that the proposed methods outperform\nexisting solutions quantitatively and qualitatively, contributing to\nhigh-fidelity wide-angle videos with stable and natural portraits. The codes\nand dataset will be available."}
{"id": "2504.00409", "pdf": "https://arxiv.org/pdf/2504.00409", "abs": "https://arxiv.org/abs/2504.00409", "authors": ["Mohanakrishnan Hariharan"], "title": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have greatly improved their capability in\nperforming NLP tasks. However, deeper semantic understanding, contextual\ncoherence, and more subtle reasoning are still difficult to obtain. The paper\ndiscusses state-of-the-art methodologies that advance LLMs with more advanced\nNLU techniques, such as semantic parsing, knowledge integration, and contextual\nreinforcement learning. We analyze the use of structured knowledge graphs,\nretrieval-augmented generation (RAG), and fine-tuning strategies that match\nmodels with human-level understanding. Furthermore, we address the\nincorporation of transformer-based architectures, contrastive learning, and\nhybrid symbolic-neural methods that address problems like hallucinations,\nambiguity, and inconsistency in the factual perspectives involved in performing\ncomplex NLP tasks, such as question-answering text summarization and dialogue\ngeneration. Our findings show the importance of semantic precision for\nenhancing AI-driven language systems and suggest future research directions to\nbridge the gap between statistical language models and true natural language\nunderstanding."}
{"id": "2504.00410", "pdf": "https://arxiv.org/pdf/2504.00410", "abs": "https://arxiv.org/abs/2504.00410", "authors": ["Dongwoo Park", "Suk Pil Ko"], "title": "NCAP: Scene Text Image Super-Resolution with Non-CAtegorical Prior", "categories": ["cs.CV"], "comment": "WACV 2025", "summary": "Scene text image super-resolution (STISR) enhances the resolution and quality\nof low-resolution images. Unlike previous studies that treated scene text\nimages as natural images, recent methods using a text prior (TP), extracted\nfrom a pre-trained text recognizer, have shown strong performance. However, two\nmajor issues emerge: (1) Explicit categorical priors, like TP, can negatively\nimpact STISR if incorrect. We reveal that these explicit priors are unstable\nand propose replacing them with Non-CAtegorical Prior (NCAP) using penultimate\nlayer representations. (2) Pre-trained recognizers used to generate TP struggle\nwith low-resolution images. To address this, most studies jointly train the\nrecognizer with the STISR network to bridge the domain gap between low- and\nhigh-resolution images, but this can cause an overconfidence phenomenon in the\nprior modality. We highlight this issue and propose a method to mitigate it by\nmixing hard and soft labels. Experiments on the TextZoom dataset demonstrate an\nimprovement by 3.5%, while our method significantly enhances generalization\nperformance by 14.8\\% across four text recognition datasets. Our method\ngeneralizes to all TP-guided STISR networks."}
{"id": "2504.00414", "pdf": "https://arxiv.org/pdf/2504.00414", "abs": "https://arxiv.org/abs/2504.00414", "authors": ["Gavin Greif", "Niclas Griesshaber", "Robin Greif"], "title": "Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition in Historical Documents", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "We explore how multimodal Large Language Models (mLLMs) can help researchers\ntranscribe historical documents, extract relevant historical information, and\nconstruct datasets from historical sources. Specifically, we investigate the\ncapabilities of mLLMs in performing (1) Optical Character Recognition (OCR),\n(2) OCR Post-Correction, and (3) Named Entity Recognition (NER) tasks on a set\nof city directories published in German between 1754 and 1870. First, we\nbenchmark the off-the-shelf transcription accuracy of both mLLMs and\nconventional OCR models. We find that the best-performing mLLM model\nsignificantly outperforms conventional state-of-the-art OCR models and other\nfrontier mLLMs. Second, we are the first to introduce multimodal\npost-correction of OCR output using mLLMs. We find that this novel approach\nleads to a drastic improvement in transcription accuracy and consistently\nproduces highly accurate transcriptions (<1% CER), without any image\npre-processing or model fine-tuning. Third, we demonstrate that mLLMs can\nefficiently recognize entities in transcriptions of historical documents and\nparse them into structured dataset formats. Our findings provide early evidence\nfor the long-term potential of mLLMs to introduce a paradigm shift in the\napproaches to historical data collection and document transcription."}
{"id": "2504.00421", "pdf": "https://arxiv.org/pdf/2504.00421", "abs": "https://arxiv.org/abs/2504.00421", "authors": ["Dongfu Xiao", "Chen Gao", "Zhengquan Luo", "Chi Liu", "Sheng Shen"], "title": "Can LLMs Assist Computer Education? an Empirical Case Study of DeepSeek", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "This study presents an empirical case study to assess the efficacy and\nreliability of DeepSeek-V3, an emerging large language model, within the\ncontext of computer education. The evaluation employs both CCNA simulation\nquestions and real-world inquiries concerning computer network security posed\nby Chinese network engineers. To ensure a thorough evaluation, diverse\ndimensions are considered, encompassing role dependency, cross-linguistic\nproficiency, and answer reproducibility, accompanied by statistical analysis.\nThe findings demonstrate that the model performs consistently, regardless of\nwhether prompts include a role definition or not. In addition, its adaptability\nacross languages is confirmed by maintaining stable accuracy in both original\nand translated datasets. A distinct contrast emerges between its performance on\nlower-order factual recall tasks and higher-order reasoning exercises, which\nunderscores its strengths in retrieving information and its limitations in\ncomplex analytical tasks. Although DeepSeek-V3 offers considerable practical\nvalue for network security education, challenges remain in its capability to\nprocess multimodal data and address highly intricate topics. These results\nprovide valuable insights for future refinement of large language models in\nspecialized professional environments."}
{"id": "2504.00472", "pdf": "https://arxiv.org/pdf/2504.00472", "abs": "https://arxiv.org/abs/2504.00472", "authors": ["Ruoxi Xu", "Yunjie Ji", "Boxi Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Ben He", "Yingfei Sun", "Xiangang Li", "Le Sun"], "title": "Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models (LLMs) excel in knowledge recall and\nreasoning, their static nature leads to outdated information as the real world\nevolves or when adapting to domain-specific knowledge, highlighting the need\nfor effective knowledge injection. However, current research on knowledge\ninjection remains superficial, mainly focusing on knowledge memorization and\nretrieval. This paper proposes a four-tier knowledge injection framework that\nsystematically defines the levels of knowledge injection: memorization,\nretrieval, reasoning, and association. Based on this framework, we introduce\nDeepKnowledge, a synthetic experimental testbed designed for fine-grained\nevaluation of the depth of knowledge injection across three knowledge types\n(novel, incremental, and updated). We then explore various knowledge injection\nscenarios and evaluate the depth of knowledge injection for each scenario on\nthe benchmark. Experimental results reveal key factors to reach each level of\nknowledge injection for LLMs and establish a mapping between the levels of\nknowledge injection and the corresponding suitable injection methods, aiming to\nprovide a comprehensive approach for efficient knowledge injection across\nvarious levels."}
{"id": "2504.00429", "pdf": "https://arxiv.org/pdf/2504.00429", "abs": "https://arxiv.org/abs/2504.00429", "authors": ["Yinghe Zhang", "Chi Liu", "Shuai Zhou", "Sheng Shen", "Peng Gui"], "title": "Unleashing the Power of Pre-trained Encoders for Universal Adversarial Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial attacks pose a critical security threat to real-world AI systems\nby injecting human-imperceptible perturbations into benign samples to induce\nmisclassification in deep learning models. While existing detection methods,\nsuch as Bayesian uncertainty estimation and activation pattern analysis, have\nachieved progress through feature engineering, their reliance on handcrafted\nfeature design and prior knowledge of attack patterns limits generalization\ncapabilities and incurs high engineering costs. To address these limitations,\nthis paper proposes a lightweight adversarial detection framework based on the\nlarge-scale pre-trained vision-language model CLIP. Departing from conventional\nadversarial feature characterization paradigms, we innovatively adopt an\nanomaly detection perspective. By jointly fine-tuning CLIP's dual visual-text\nencoders with trainable adapter networks and learnable prompts, we construct a\ncompact representation space tailored for natural images. Notably, our\ndetection architecture achieves substantial improvements in generalization\ncapability across both known and unknown attack patterns compared to\ntraditional methods, while significantly reducing training overhead. This study\nprovides a novel technical pathway for establishing a parameter-efficient and\nattack-agnostic defense paradigm, markedly enhancing the robustness of vision\nsystems against evolving adversarial threats."}
{"id": "2504.00473", "pdf": "https://arxiv.org/pdf/2504.00473", "abs": "https://arxiv.org/abs/2504.00473", "authors": ["Xiangyang Liu", "Junliang He", "Xipeng Qiu"], "title": "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2024", "summary": "Large language models (LLMs) can perform complex reasoning by generating\nintermediate thoughts under zero-shot or few-shot settings. However, zero-shot\nprompting always encounters low performance, and the superior performance of\nfew-shot prompting hinges on the manual-crafted demonstrations. In this paper,\nwe present RoSE (Reasoning with Orchestrated Streaming Experiences), a general\nframework for solving reasoning tasks that can self-improve without complex\nexternal efforts. To enable RoSE, we describe an architecture that extends an\nLLM to store all answered questions and their thoughts in a streaming\nexperience pool then orchestrates helpful questions from the pool to assist in\nanswering new questions. To set up a question-aware orchestration mechanism,\nRoSE first calculates the similarity of each question in the pool with a new\ntest question. Since the solution to each answered question is not always\ncorrect, RoSE will sort the questions according to their similarity with the\nnew question, and then uniformly divide them into multiple buckets. It finally\nextracts one question from each bucket to make these extracted questions more\ndiverse. To make these extracted questions help RoSE answer new questions as\nmuch as possible, we introduce two other attributes of uncertainty and\ncomplexity for each question. RoSE will preferentially select the questions\nwith low uncertainty and high complexity from each bucket. We evaluate the\nversatility of RoSE in various reasoning tasks, LLMs, and CoT methods."}
{"id": "2504.00430", "pdf": "https://arxiv.org/pdf/2504.00430", "abs": "https://arxiv.org/abs/2504.00430", "authors": ["Yuxi Mi", "Zhizhou Zhong", "Yuge Huang", "Qiuyang Yuan", "Xuan Zhao", "Jianqing Xu", "Shouhong Ding", "ShaoMing Wang", "Rizen Guo", "Shuigeng Zhou"], "title": "Data Synthesis with Diverse Styles for Face Recognition via 3DMM-Guided Diffusion", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Identity-preserving face synthesis aims to generate synthetic face images of\nvirtual subjects that can substitute real-world data for training face\nrecognition models. While prior arts strive to create images with consistent\nidentities and diverse styles, they face a trade-off between them. Identifying\ntheir limitation of treating style variation as subject-agnostic and observing\nthat real-world persons actually have distinct, subject-specific styles, this\npaper introduces MorphFace, a diffusion-based face generator. The generator\nlearns fine-grained facial styles, e.g., shape, pose and expression, from the\nrenderings of a 3D morphable model (3DMM). It also learns identities from an\noff-the-shelf recognition model. To create virtual faces, the generator is\nconditioned on novel identities of unlabeled synthetic faces, and novel styles\nthat are statistically sampled from a real-world prior distribution. The\nsampling especially accounts for both intra-subject variation and subject\ndistinctiveness. A context blending strategy is employed to enhance the\ngenerator's responsiveness to identity and style conditions. Extensive\nexperiments show that MorphFace outperforms the best prior arts in face\nrecognition efficacy."}
{"id": "2504.00573", "pdf": "https://arxiv.org/pdf/2504.00573", "abs": "https://arxiv.org/abs/2504.00573", "authors": ["Yilong Xu", "Jinhua Gao", "Xiaoming Yu", "Yuanhai Xue", "Baolong Bi", "Huawei Shen", "Xueqi Cheng"], "title": "Training a Utility-based Retriever Through Shared Context Attribution for Retrieval-Augmented Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures. Code will be released after review", "summary": "Retrieval-Augmented Language Models boost task performance, owing to the\nretriever that provides external knowledge. Although crucial, the retriever\nprimarily focuses on semantics relevance, which may not always be effective for\ngeneration. Thus, utility-based retrieval has emerged as a promising topic,\nprioritizing passages that provides valid benefits for downstream tasks.\nHowever, due to insufficient understanding, capturing passage utility\naccurately remains unexplored. This work proposes SCARLet, a framework for\ntraining utility-based retrievers in RALMs, which incorporates two key factors,\nmulti-task generalization and inter-passage interaction. First, SCARLet\nconstructs shared context on which training data for various tasks is\nsynthesized. This mitigates semantic bias from context differences, allowing\nretrievers to focus on learning task-specific utility for better task\ngeneralization. Next, SCARLet uses a perturbation-based attribution method to\nestimate passage-level utility for shared context, which reflects interactions\nbetween passages and provides more accurate feedback. We evaluate our approach\non ten datasets across various tasks, both in-domain and out-of-domain, showing\nthat retrievers trained by SCARLet consistently improve the overall performance\nof RALMs."}
{"id": "2504.00431", "pdf": "https://arxiv.org/pdf/2504.00431", "abs": "https://arxiv.org/abs/2504.00431", "authors": ["Yuzhuo Zhou", "Chi Liu", "Sheng Shen", "Siyu Le", "Liwen Yu", "Sihan Ouyang", "Zongyuan Ge"], "title": "Enhancing Fundus Image-based Glaucoma Screening via Dynamic Global-Local Feature Integration", "categories": ["cs.CV"], "comment": null, "summary": "With the advancements in medical artificial intelligence (AI), fundus image\nclassifiers are increasingly being applied to assist in ophthalmic diagnosis.\nWhile existing classification models have achieved high accuracy on specific\nfundus datasets, they struggle to address real-world challenges such as\nvariations in image quality across different imaging devices, discrepancies\nbetween training and testing images across different racial groups, and the\nuncertain boundaries due to the characteristics of glaucomatous cases. In this\nstudy, we aim to address the above challenges posed by image variations by\nhighlighting the importance of incorporating comprehensive fundus image\ninformation, including the optic cup (OC) and optic disc (OD) regions, and\nother key image patches. Specifically, we propose a self-adaptive attention\nwindow that autonomously determines optimal boundaries for enhanced feature\nextraction. Additionally, we introduce a multi-head attention mechanism to\neffectively fuse global and local features via feature linear readout,\nimproving the model's discriminative capability. Experimental results\ndemonstrate that our method achieves superior accuracy and robustness in\nglaucoma classification."}
{"id": "2504.00584", "pdf": "https://arxiv.org/pdf/2504.00584", "abs": "https://arxiv.org/abs/2504.00584", "authors": ["Hongliu Cao"], "title": "Enhancing Negation Awareness in Universal Text Embeddings: A Data-efficient and Computational-efficient Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings."}
{"id": "2504.00432", "pdf": "https://arxiv.org/pdf/2504.00432", "abs": "https://arxiv.org/abs/2504.00432", "authors": ["Chong Li", "Jingyang Huo", "Weikang Gong", "Yanwei Fu", "Xiangyang Xue", "Jianfeng Feng"], "title": "DecoFuse: Decomposing and Fusing the \"What\", \"Where\", and \"How\" for Brain-Inspired fMRI-to-Video Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Decoding visual experiences from brain activity is a significant challenge.\nExisting fMRI-to-video methods often focus on semantic content while\noverlooking spatial and motion information. However, these aspects are all\nessential and are processed through distinct pathways in the brain. Motivated\nby this, we propose DecoFuse, a novel brain-inspired framework for decoding\nvideos from fMRI signals. It first decomposes the video into three components -\nsemantic, spatial, and motion - then decodes each component separately before\nfusing them to reconstruct the video. This approach not only simplifies the\ncomplex task of video decoding by decomposing it into manageable sub-tasks, but\nalso establishes a clearer connection between learned representations and their\nbiological counterpart, as supported by ablation studies. Further, our\nexperiments show significant improvements over previous state-of-the-art\nmethods, achieving 82.4% accuracy for semantic classification, 70.6% accuracy\nin spatial consistency, a 0.212 cosine similarity for motion prediction, and\n21.9% 50-way accuracy for video generation. Additionally, neural encoding\nanalyses for semantic and spatial information align with the two-streams\nhypothesis, further validating the distinct roles of the ventral and dorsal\npathways. Overall, DecoFuse provides a strong and biologically plausible\nframework for fMRI-to-video decoding. Project page:\nhttps://chongjg.github.io/DecoFuse/."}
{"id": "2504.00589", "pdf": "https://arxiv.org/pdf/2504.00589", "abs": "https://arxiv.org/abs/2504.00589", "authors": ["Owen Cook", "Jake Vasilakes", "Ian Roberts", "Xingyi Song"], "title": "Efficient Annotator Reliablity Assessment with EffiARA", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Data annotation is an essential component of the machine learning pipeline;\nit is also a costly and time-consuming process. With the introduction of\ntransformer-based models, annotation at the document level is increasingly\npopular; however, there is no standard framework for structuring such tasks.\nThe EffiARA annotation framework is, to our knowledge, the first project to\nsupport the whole annotation pipeline, from understanding the resources\nrequired for an annotation task to compiling the annotated dataset and gaining\ninsights into the reliability of individual annotators as well as the dataset\nas a whole. The framework's efficacy is supported by two previous studies: one\nimproving classification performance through annotator-reliability-based soft\nlabel aggregation and sample weighting, and the other increasing the overall\nagreement among annotators through removing identifying and replacing an\nunreliable annotator. This work introduces the EffiARA Python package and its\naccompanying webtool, which provides an accessible graphical user interface for\nthe system. We open-source the EffiARA Python package at\nhttps://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at\nhttps://effiara.gate.ac.uk."}
{"id": "2504.00437", "pdf": "https://arxiv.org/pdf/2504.00437", "abs": "https://arxiv.org/abs/2504.00437", "authors": ["Qi Song", "Chenghong Li", "Haotong Lin", "Sida Peng", "Rui Huang"], "title": "ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with Multi-modal Inputs", "categories": ["cs.CV"], "comment": "The project page can be found at\n  https://maggiesong7.github.io/research/ADGaussian/", "summary": "We present a novel approach, termed ADGaussian, for generalizable street\nscene reconstruction. The proposed method enables high-quality rendering from\nsingle-view input. Unlike prior Gaussian Splatting methods that primarily focus\non geometry refinement, we emphasize the importance of joint optimization of\nimage and depth features for accurate Gaussian prediction. To this end, we\nfirst incorporate sparse LiDAR depth as an additional input modality,\nformulating the Gaussian prediction process as a joint learning framework of\nvisual information and geometric clue. Furthermore, we propose a multi-modal\nfeature matching strategy coupled with a multi-scale Gaussian decoding model to\nenhance the joint refinement of multi-modal features, thereby enabling\nefficient multi-modal Gaussian learning. Extensive experiments on two\nlarge-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our\nADGaussian achieves state-of-the-art performance and exhibits superior\nzero-shot generalization capabilities in novel-view shifting."}
{"id": "2504.00595", "pdf": "https://arxiv.org/pdf/2504.00595", "abs": "https://arxiv.org/abs/2504.00595", "authors": ["Weizhi Wang", "Yu Tian", "Linjie Yang", "Heng Wang", "Xifeng Yan"], "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources", "categories": ["cs.CL"], "comment": null, "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model."}
{"id": "2504.00438", "pdf": "https://arxiv.org/pdf/2504.00438", "abs": "https://arxiv.org/abs/2504.00438", "authors": ["Lan Sun", "Songpengcheng Xia", "Jiarui Yang", "Ling Pei"], "title": "Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages,10 figures", "summary": "The proliferation of wearable technology has established multi-device\necosystems comprising smartphones, smartwatches, and headphones as critical\nenablers for ubiquitous pedestrian localization. However, traditional\npedestrian dead reckoning (PDR) struggles with diverse motion modes, while\ndata-driven methods, despite improving accuracy, often lack robustness due to\ntheir reliance on a single-device setup. Therefore, a promising solution is to\nfully leverage existing wearable devices to form a flexiwear bodynet for robust\nand accurate pedestrian localization. This paper presents Suite-IN++, a deep\nlearning framework for flexiwear bodynet-based pedestrian localization.\nSuite-IN++ integrates motion data from wearable devices on different body\nparts, using contrastive learning to separate global and local motion features.\nIt fuses global features based on the data reliability of each device to\ncapture overall motion trends and employs an attention mechanism to uncover\ncross-device correlations in local features, extracting motion details helpful\nfor accurate localization. To evaluate our method, we construct a real-life\nflexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and\nAirPods) across diverse walking modes and device configurations. Experimental\nresults demonstrate that Suite-IN++ achieves superior localization accuracy and\nrobustness, significantly outperforming state-of-the-art models in real-life\npedestrian tracking scenarios."}
{"id": "2504.00597", "pdf": "https://arxiv.org/pdf/2504.00597", "abs": "https://arxiv.org/abs/2504.00597", "authors": ["Jirui Qi", "Raquel FernÃ¡ndez", "Arianna Bisazza"], "title": "On the Consistency of Multilingual Context Utilization in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Under review at COLM2025. All codes and data are released at\n  https://anonymous.4open.science/r/RAG-Consistency/", "summary": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from out-language passages, but a much weaker ability to formulate\na full answer in the correct language. Our analysis, based on both accuracy and\nfeature attribution techniques, further shows that distracting passages\nnegatively impact answer quality regardless of their language. However,\ndistractors in the query language exert a slightly stronger influence. Taken\ntogether, our findings deepen the understanding of how LLMs utilize context in\nmRAG systems, providing directions for future improvements."}
{"id": "2504.00454", "pdf": "https://arxiv.org/pdf/2504.00454", "abs": "https://arxiv.org/abs/2504.00454", "authors": ["Yongze Li", "Ning Li", "Ajian Liu", "Hui Ma", "Liying Yang", "Xihong Chen", "Zhiyao Liang", "Yanyan Liang", "Jun Wan", "Zhen Lei"], "title": "FA^{3}-CLIP: Frequency-Aware Cues Fusion and Attack-Agnostic Prompt Learning for Unified Face Attack Detection", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Facial recognition systems are vulnerable to physical (e.g., printed photos)\nand digital (e.g., DeepFake) face attacks. Existing methods struggle to\nsimultaneously detect physical and digital attacks due to: 1) significant\nintra-class variations between these attack types, and 2) the inadequacy of\nspatial information alone to comprehensively capture live and fake cues. To\naddress these issues, we propose a unified attack detection model termed\nFrequency-Aware and Attack-Agnostic CLIP (FA\\textsuperscript{3}-CLIP), which\nintroduces attack-agnostic prompt learning to express generic live and fake\ncues derived from the fusion of spatial and frequency features, enabling\nunified detection of live faces and all categories of attacks. Specifically,\nthe attack-agnostic prompt module generates generic live and fake prompts\nwithin the language branch to extract corresponding generic representations\nfrom both live and fake faces, guiding the model to learn a unified feature\nspace for unified attack detection. Meanwhile, the module adaptively generates\nthe live/fake conditional bias from the original spatial and frequency\ninformation to optimize the generic prompts accordingly, reducing the impact of\nintra-class variations. We further propose a dual-stream cues fusion framework\nin the vision branch, which leverages frequency information to complement\nsubtle cues that are difficult to capture in the spatial domain. In addition, a\nfrequency compression block is utilized in the frequency stream, which reduces\nredundancy in frequency features while preserving the diversity of crucial\ncues. We also establish new challenging protocols to facilitate unified face\nattack detection effectiveness. Experimental results demonstrate that the\nproposed method significantly improves performance in detecting physical and\ndigital face attacks, achieving state-of-the-art results."}
{"id": "2504.00623", "pdf": "https://arxiv.org/pdf/2504.00623", "abs": "https://arxiv.org/abs/2504.00623", "authors": ["Kazuki Yano", "Sho Takase", "Sosuke Kobayashi", "Shun Kiyono", "Jun Suzuki"], "title": "Efficient Construction of Model Family through Progressive Training Using Model Expansion", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) gain widespread practical application,\nproviding the model family of different parameter sizes has become standard\npractice to address diverse computational requirements. Conventionally, each\nmodel in a family is trained independently, resulting in computational costs\nthat scale additively with the number of models. We propose an efficient method\nfor constructing the model family through progressive training, where smaller\nmodels are incrementally expanded to larger sizes to create a complete model\nfamily. Through extensive experiments with a model family ranging from 1B to 8B\nparameters, we demonstrate that our method reduces computational costs by\napproximately 25% while maintaining comparable performance to independently\ntrained models. Furthermore, by strategically adjusting maximum learning rates\nbased on model size, our method outperforms the independent training across\nvarious metrics. Beyond performance gains, our approach offers an additional\nadvantage: models in our family tend to yield more consistent behavior across\ndifferent model sizes."}
{"id": "2504.00457", "pdf": "https://arxiv.org/pdf/2504.00457", "abs": "https://arxiv.org/abs/2504.00457", "authors": ["Hao Qin", "Luyuan Chen", "Ming Kong", "Mengxu Lu", "Qiang Zhu"], "title": "Distilling Multi-view Diffusion Models into 3D Generators", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce DD3G, a formulation that Distills a multi-view Diffusion model\n(MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and\nintegrates extensive visual and spatial geometric knowledge from the MV-DM by\nsimulating its ordinary differential equation (ODE) trajectory, ensuring the\ndistilled generator generalizes better than those trained solely on 3D data.\nUnlike previous amortized optimization approaches, we align the MV-DM and 3D\ngenerator representation spaces to transfer the teacher's probabilistic flow to\nthe student, thus avoiding inconsistencies in optimization objectives caused by\nprobabilistic sampling. The introduction of probabilistic flow and the coupling\nof various attributes in 3D Gaussians introduce challenges in the generation\nprocess. To tackle this, we propose PEPD, a generator consisting of Pattern\nExtraction and Progressive Decoding phases, which enables efficient fusion of\nprobabilistic flow and converts a single image into 3D Gaussians within 0.06\nseconds. Furthermore, to reduce knowledge loss and overcome sparse-view\nsupervision, we design a joint optimization objective that ensures the quality\nof generated samples through explicit supervision and implicit verification.\nLeveraging existing 2D generation models, we compile 120k high-quality RGBA\nimages for distillation. Experiments on synthetic and public datasets\ndemonstrate the effectiveness of our method. Our project is available at:\nhttps://qinbaigao.github.io/DD3G_project/"}
{"id": "2504.00657", "pdf": "https://arxiv.org/pdf/2504.00657", "abs": "https://arxiv.org/abs/2504.00657", "authors": ["Enrico Liscio", "Michela Lorandi", "Pradeep K. Murukannaiah"], "title": "News is More than a Collection of Facts: Moral Frame Preserving News Summarization", "categories": ["cs.CL"], "comment": null, "summary": "News articles are more than collections of facts; they reflect journalists'\nframing, shaping how events are presented to the audience. One key aspect of\nframing is the choice to write in (or quote verbatim) morally charged language\nas opposed to using neutral terms. This moral framing carries implicit\njudgments that automated news summarizers should recognize and preserve to\nmaintain the original intent of the writer. In this work, we perform the first\nstudy on the preservation of moral framing in AI-generated news summaries. We\npropose an approach that leverages the intuition that journalists intentionally\nuse or report specific moral-laden words, which should be retained in\nsummaries. Through automated, crowd-sourced, and expert evaluations, we\ndemonstrate that our approach enhances the preservation of moral framing while\nmaintaining overall summary quality."}
{"id": "2504.00458", "pdf": "https://arxiv.org/pdf/2504.00458", "abs": "https://arxiv.org/abs/2504.00458", "authors": ["Shunxin Chen", "Ajian Liu", "Junze Zheng", "Jun Wan", "Kailai Peng", "Sergio Escalera", "Zhen Lei"], "title": "Mixture-of-Attack-Experts with Class Regularization for Unified Physical-Digital Face Attack Detection", "categories": ["cs.CV"], "comment": "9 pages, 5 figures, accepted by AAAI-2025 (Oral)", "summary": "Facial recognition systems in real-world scenarios are susceptible to both\ndigital and physical attacks. Previous methods have attempted to achieve\nclassification by learning a comprehensive feature space. However, these\nmethods have not adequately accounted for the inherent characteristics of\nphysical and digital attack data, particularly the large intra class variation\nin attacks and the small inter-class variation between live and fake faces. To\naddress these limitations, we propose the Fine-Grained MoE with Class-Aware\nRegularization CLIP framework (FG-MoE-CLIP-CAR), incorporating key improvements\nat both the feature and loss levels. At the feature level, we employ a Soft\nMixture of Experts (Soft MoE) architecture to leverage different experts for\nspecialized feature processing. Additionally, we refine the Soft MoE to capture\nmore subtle differences among various types of fake faces. At the loss level,\nwe introduce two constraint modules: the Disentanglement Module (DM) and the\nCluster Distillation Module (CDM). The DM enhances class separability by\nincreasing the distance between the centers of live and fake face classes.\nHowever, center-to-center constraints alone are insufficient to ensure\ndistinctive representations for individual features. Thus, we propose the CDM\nto further cluster features around their respective class centers while\nmaintaining separation from other classes. Moreover, specific attacks that\nsignificantly deviate from common attack patterns are often overlooked. To\naddress this issue, our distance calculation prioritizes more distant features.\nExperimental results on two unified physical-digital attack datasets\ndemonstrate that the proposed method achieves state-of-the-art (SOTA)\nperformance."}
{"id": "2504.00661", "pdf": "https://arxiv.org/pdf/2504.00661", "abs": "https://arxiv.org/abs/2504.00661", "authors": ["Dengchun Li", "Naizheng Wang", "Zihao Zhang", "Haoyang Yin", "Lei Duan", "Meng Xiao", "Mingjie Tang"], "title": "DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures", "summary": "Instruction-based fine-tuning of large language models (LLMs) has achieved\nremarkable success in various natural language processing (NLP) tasks.\nParameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts\n(MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the\nversatility of Mixture of Experts (MoE) models, demonstrating significant\npotential for handling multiple downstream tasks. However, the existing routing\nmechanisms for MoLE often involve a trade-off between computational efficiency\nand predictive accuracy, and they fail to fully address the diverse expert\nselection demands across different transformer layers. In this work, we propose\nDynMoLE, a hybrid routing strategy that dynamically adjusts expert selection\nbased on the Tsallis entropy of the router's probability distribution. This\napproach mitigates router uncertainty, enhances stability, and promotes more\nequitable expert participation, leading to faster convergence and improved\nmodel performance. Additionally, we introduce an auxiliary loss based on\nTsallis entropy to further guide the model toward convergence with reduced\nuncertainty, thereby improving training stability and performance. Our\nextensive experiments on commonsense reasoning benchmarks demonstrate that\nDynMoLE achieves substantial performance improvements, outperforming LoRA by\n9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also\nconduct a comprehensive ablation study to evaluate the contributions of\nDynMoLE's key components."}
{"id": "2504.00463", "pdf": "https://arxiv.org/pdf/2504.00463", "abs": "https://arxiv.org/abs/2504.00463", "authors": ["Ziyin Zhou", "Ke Sun", "Zhongxi Chen", "Xianming Lin", "Yunpeng Luo", "Ke Yan", "Shouhong Ding", "Xiaoshuai Sun"], "title": "Exploring the Collaborative Advantage of Low-level Information on Generalizable AI-Generated Image Detection", "categories": ["cs.CV"], "comment": null, "summary": "Existing state-of-the-art AI-Generated image detection methods mostly\nconsider extracting low-level information from RGB images to help improve the\ngeneralization of AI-Generated image detection, such as noise patterns.\nHowever, these methods often consider only a single type of low-level\ninformation, which may lead to suboptimal generalization. Through empirical\nanalysis, we have discovered a key insight: different low-level information\noften exhibits generalization capabilities for different types of forgeries.\nFurthermore, we found that simple fusion strategies are insufficient to\nleverage the detection advantages of each low-level and high-level information\nfor various forgery types. Therefore, we propose the Adaptive Low-level Experts\nInjection (ALEI) framework. Our approach introduces Lora Experts, enabling the\nbackbone network, which is trained with high-level semantic RGB images, to\naccept and learn knowledge from different low-level information. We utilize a\ncross-attention method to adaptively fuse these features at intermediate\nlayers. To prevent the backbone network from losing the modeling capabilities\nof different low-level features during the later stages of modeling, we\ndeveloped a Low-level Information Adapter that interacts with the features\nextracted by the backbone network. Finally, we propose Dynamic Feature\nSelection, which dynamically selects the most suitable features for detecting\nthe current image to maximize generalization detection capability. Extensive\nexperiments demonstrate that our method, finetuned on only four categories of\nmainstream ProGAN data, performs excellently and achieves state-of-the-art\nresults on multiple datasets containing unseen GAN and Diffusion methods."}
{"id": "2504.00664", "pdf": "https://arxiv.org/pdf/2504.00664", "abs": "https://arxiv.org/abs/2504.00664", "authors": ["Motasem S Obeidat", "Md Sultan Al Nahian", "Ramakanth Kavuluru"], "title": "Do LLMs Surpass Encoders for Biomedical NER?", "categories": ["cs.CL"], "comment": "Accepted to appear in IEEE ICHI 2025", "summary": "Recognizing spans of biomedical concepts and their types (e.g., drug or gene)\nin free text, often called biomedical named entity recognition (NER), is a\nbasic component of information extraction (IE) pipelines. Without a strong NER\ncomponent, other applications, such as knowledge discovery and information\nretrieval, are not practical. State-of-the-art in NER shifted from traditional\nML models to deep neural networks with transformer-based encoder models (e.g.,\nBERT) emerging as the current standard. However, decoder models (also called\nlarge language models or LLMs) are gaining traction in IE. But LLM-driven NER\noften ignores positional information due to the generative nature of decoder\nmodels. Furthermore, they are computationally very expensive (both in inference\ntime and hardware needs). Hence, it is worth exploring if they actually excel\nat biomedical NER and assess any associated trade-offs (performance vs\nefficiency). This is exactly what we do in this effort employing the same BIO\nentity tagging scheme (that retains positional information) using five\ndifferent datasets with varying proportions of longer entities. Our results\nshow that the LLMs chosen (Mistral and Llama: 8B range) often outperform best\nencoder models (BERT-(un)cased, BiomedBERT, and DeBERTav3: 300M range) by 2-8%\nin F-scores except for one dataset, where they equal encoder performance. This\ngain is more prominent among longer entities of length >= 3 tokens. However,\nLLMs are one to two orders of magnitude more expensive at inference time and\nmay need cost prohibitive hardware. Thus, when performance differences are\nsmall or real time user feedback is needed, encoder models might still be more\nsuitable than LLMs."}
{"id": "2504.00476", "pdf": "https://arxiv.org/pdf/2504.00476", "abs": "https://arxiv.org/abs/2504.00476", "authors": ["Haobo Yuan", "Tao Zhang", "Xiangtai Li", "Lu Qi", "Zilong Huang", "Shilin Xu", "Jiashi Feng", "Ming-Hsuan Yang"], "title": "4th PVUW MeViS 3rd Place Report: Sa2VA", "categories": ["cs.CV"], "comment": "Technical Report, 4 pages, Code:\n  https://github.com/magic-research/Sa2VA", "summary": "Referring video object segmentation (RVOS) is a challenging task that\nrequires the model to segment the object in a video given the language\ndescription. MeViS is a recently proposed dataset that contains motion\nexpressions of the target objects, leading to a challenging benchmark, compared\nwith existing RVOS benchmarks. On the other hand, for referring expression\ntasks, a new trend is to adopt multi-modal large language model (MLLM) to\nachieve better image and text alignment. In this report, we show that with a\nsimple modification to the test time inference method on stronger MLLMs, we can\nlead to stronger results on MeVIS. In particular, we adopt the recent method\nSa2VA, a unified model for dense grounded understanding of both images and\nvideos. By enlarging the scope of key frames, without any further training, we\ncan achieve the 3rd place in the 4th PVUW workshop."}
{"id": "2504.00676", "pdf": "https://arxiv.org/pdf/2504.00676", "abs": "https://arxiv.org/abs/2504.00676", "authors": ["Anthony Yazdani", "Ihor Stepanov", "Douglas Teodoro"], "title": "GLiNER-biomed: A Suite of Efficient Models for Open Biomedical Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical named entity recognition (NER) presents unique challenges due to\nspecialized vocabularies, the sheer volume of entities, and the continuous\nemergence of novel entities. Traditional NER models, constrained by fixed\ntaxonomies and human annotations, struggle to generalize beyond predefined\nentity types or efficiently adapt to emerging concepts. To address these\nissues, we introduce GLiNER-biomed, a domain-adapted suite of Generalist and\nLightweight Model for NER (GLiNER) models specifically tailored for biomedical\nNER. In contrast to conventional approaches, GLiNER uses natural language\ndescriptions to infer arbitrary entity types, enabling zero-shot recognition.\nOur approach first distills the annotation capabilities of large language\nmodels (LLMs) into a smaller, more efficient model, enabling the generation of\nhigh-coverage synthetic biomedical NER data. We subsequently train two GLiNER\narchitectures, uni- and bi-encoder, at multiple scales to balance computational\nefficiency and recognition performance. Evaluations on several biomedical\ndatasets demonstrate that GLiNER-biomed outperforms state-of-the-art GLiNER\nmodels in both zero- and few-shot scenarios, achieving 5.96% improvement in\nF1-score over the strongest baseline. Ablation studies highlight the\neffectiveness of our synthetic data generation strategy and emphasize the\ncomplementary benefits of synthetic biomedical pre-training combined with\nfine-tuning on high-quality general-domain annotations. All datasets, models,\nand training pipelines are publicly available at\nhttps://github.com/ds4dh/GLiNER-biomed."}
{"id": "2504.00478", "pdf": "https://arxiv.org/pdf/2504.00478", "abs": "https://arxiv.org/abs/2504.00478", "authors": ["Zhuohao Li", "Zhicheng Huang", "Wenchao Liu", "Zhuxing Zhang", "Jianming Miao"], "title": "FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature Enhancement for Few-Shot Semantic Segmentation in Underwater Images", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes\nin images using only a limited number of annotated examples, has recently\nprogressed in data-scarce domains. However, in this work, we show that the\nexisting FSS methods often struggle to generalize to underwater environments.\nSpecifically, the prior features extracted by pre-trained models used as\nfeature extractors are fragile due to the unique challenges of underwater\nimages. To address this, we propose FSSUWNet, a tailored FSS framework for\nunderwater images with feature enhancement. FSSUWNet exploits the integration\nof complementary features, emphasizing both low-level and high-level image\ncharacteristics. In addition to employing a pre-trained model as the primary\nencoder, we propose an auxiliary encoder called Feature Enhanced Encoder which\nextracts complementary features to better adapt to underwater scene\ncharacteristics. Furthermore, a simple and effective Feature Alignment Module\naims to provide global prior knowledge and align low-level features with\nhigh-level features in dimensions. Given the scarcity of underwater images, we\nintroduce a cross-validation dataset version based on the Segmentation of\nUnderwater Imagery dataset. Extensive experiments on public underwater\nsegmentation datasets demonstrate that our approach achieves state-of-the-art\nperformance. For example, our method outperforms the previous best method by\n2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot\nand 5-shot scenarios in the datasets, respectively. Our implementation is\navailable at https://github.com/lizhh268/FSSUWNet."}
{"id": "2504.00695", "pdf": "https://arxiv.org/pdf/2504.00695", "abs": "https://arxiv.org/abs/2504.00695", "authors": ["Xiaoxuan Zhu", "Zhouhong Gu", "Suhang Zheng", "Tao Wang", "Tianyu Li", "Hongwei Feng", "Yanghua Xiao"], "title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "categories": ["cs.CL"], "comment": null, "summary": "Pre-training large language models (LLMs) necessitates enormous diverse\ntextual corpora, making effective data selection a key challenge for balancing\ncomputational resources and model performance. Current methodologies primarily\nemphasize data quality metrics and mixing proportions, yet they fail to\nadequately capture the underlying semantic connections between training samples\nand quality disparities within individual domains. We introduce ToReMi\n(Topic-based Reweighting for Model improvement), a novel two-stage framework\nthat dynamically adjusts training sample weights according to their topical\nassociations and observed learning patterns. Our comprehensive experiments\nreveal that ToReMi variants consistently achieve superior performance over\nconventional pre-training approaches, demonstrating accelerated perplexity\nreduction across multiple domains and enhanced capabilities on downstream\nevaluation tasks. Code is available at https://github.com/zxx000728/ToReMi."}
{"id": "2504.00481", "pdf": "https://arxiv.org/pdf/2504.00481", "abs": "https://arxiv.org/abs/2504.00481", "authors": ["Yueru Chen", "Wei Zhang", "Dingquan Li", "Jing Wang", "Ge Li"], "title": "Hierarchical Attention Networks for Lossless Point Cloud Attribute Compression", "categories": ["cs.CV", "eess.SP"], "comment": "Accepted by DCC 2025", "summary": "In this paper, we propose a deep hierarchical attention context model for\nlossless attribute compression of point clouds, leveraging a multi-resolution\nspatial structure and residual learning. A simple and effective Level of Detail\n(LoD) structure is introduced to yield a coarse-to-fine representation. To\nenhance efficiency, points within the same refinement level are encoded in\nparallel, sharing a common context point group. By hierarchically aggregating\ninformation from neighboring points, our attention model learns contextual\ndependencies across varying scales and densities, enabling comprehensive\nfeature extraction. We also adopt normalization for position coordinates and\nattributes to achieve scale-invariant compression. Additionally, we segment the\npoint cloud into multiple slices to facilitate parallel processing, further\noptimizing time complexity. Experimental results demonstrate that the proposed\nmethod offers better coding performance than the latest G-PCC for color and\nreflectance attributes while maintaining more efficient encoding and decoding\nruntimes."}
{"id": "2504.00698", "pdf": "https://arxiv.org/pdf/2504.00698", "abs": "https://arxiv.org/abs/2504.00698", "authors": ["Team Cohere", "Aakanksha", "Arash Ahmadian", "Marwan Ahmed", "Jay Alammar", "Yazeed Alnumay", "Sophia Althammer", "Arkady Arkhangorodsky", "Viraat Aryabumi", "Dennis Aumiller", "RaphaÃ«l Avalos", "Zahara Aviv", "Sammie Bae", "Saurabh Baji", "Alexandre Barbet", "Max Bartolo", "BjÃ¶rn Bebensee", "Neeral Beladia", "Walter Beller-Morales", "Alexandre BÃ©rard", "Andrew Berneshawi", "Anna Bialas", "Phil Blunsom", "Matt Bobkin", "Adi Bongale", "Sam Braun", "Maxime Brunet", "Samuel Cahyawijaya", "David Cairuz", "Jon Ander Campos", "Cassie Cao", "Kris Cao", "Roman CastagnÃ©", "JuliÃ¡n Cendrero", "Leila Chan Currie", "Yash Chandak", "Diane Chang", "Giannis Chatziveroglou", "Hongyu Chen", "Claire Cheng", "Alexis Chevalier", "Justin T. Chiu", "Eugene Cho", "Eugene Choi", "Eujeong Choi", "Tim Chung", "Volkan Cirik", "Ana Cismaru", "Pierre Clavier", "Henry Conklin", "Lucas Crawhall-Stein", "Devon Crouse", "Andres Felipe Cruz-Salinas", "Ben Cyrus", "Daniel D'souza", "Hugo Dalla-Torre", "John Dang", "William Darling", "Omar Darwiche Domingues", "Saurabh Dash", "Antoine Debugne", "ThÃ©o Dehaze", "Shaan Desai", "Joan Devassy", "Rishit Dholakia", "Kyle Duffy", "Ali Edalati", "Ace Eldeib", "Abdullah Elkady", "Sarah Elsharkawy", "Irem ErgÃ¼n", "Beyza Ermis", "Marzieh Fadaee", "Boyu Fan", "Lucas Fayoux", "Yannis Flet-Berliac", "Nick Frosst", "Matthias GallÃ©", "Wojciech Galuba", "Utsav Garg", "Matthieu Geist", "Mohammad Gheshlaghi Azar", "Seraphina Goldfarb-Tarrant", "Tomas Goldsack", "Aidan Gomez", "Victor Machado Gonzaga", "Nithya Govindarajan", "Manoj Govindassamy", "Nathan Grinsztajn", "Nikolas Gritsch", "Patrick Gu", "Shangmin Guo", "Kilian Haefeli", "Rod Hajjar", "Tim Hawes", "Jingyi He", "Sebastian HofstÃ¤tter", "Sungjin Hong", "Sara Hooker", "Tom Hosking", "Stephanie Howe", "Eric Hu", "Renjie Huang", "Hemant Jain", "Ritika Jain", "Nick Jakobi", "Madeline Jenkins", "JJ Jordan", "Dhruti Joshi", "Jason Jung", "Trushant Kalyanpur", "Siddhartha Rao Kamalakara", "Julia Kedrzycki", "Gokce Keskin", "Edward Kim", "Joon Kim", "Wei-Yin Ko", "Tom Kocmi", "Michael Kozakov", "Wojciech KryÅciÅski", "Arnav Kumar Jain", "Komal Kumar Teru", "Sander Land", "Michael Lasby", "Olivia Lasche", "Justin Lee", "Patrick Lewis", "Jeffrey Li", "Jonathan Li", "Hangyu Lin", "Acyr Locatelli", "Kevin Luong", "Raymond Ma", "Lukas Mach", "Marina Machado", "Joanne Magbitang", "Brenda Malacara Lopez", "Aryan Mann", "Kelly Marchisio", "Olivia Markham", "Alexandre Matton", "Alex McKinney", "Dominic McLoughlin", "Jozef Mokry", "Adrien Morisot", "Autumn Moulder", "Harry Moynehan", "Maximilian Mozes", "Vivek Muppalla", "Lidiya Murakhovska", "Hemangani Nagarajan", "Alekhya Nandula", "Hisham Nasir", "Shauna Nehra", "Josh Netto-Rosen", "Daniel Ohashi", "James Owers-Bardsley", "Jason Ozuzu", "Dennis Padilla", "Gloria Park", "Sam Passaglia", "Jeremy Pekmez", "Laura Penstone", "Aleksandra Piktus", "Case Ploeg", "Andrew Poulton", "Youran Qi", "Shubha Raghvendra", "Miguel Ramos", "Ekagra Ranjan", "Pierre Richemond", "CÃ©cile Robert-Michon", "AurÃ©lien Rodriguez", "Sudip Roy", "Laura Ruis", "Louise Rust", "Anubhav Sachan", "Alejandro Salamanca", "Kailash Karthik Saravanakumar", "Isha Satyakam", "Alice Schoenauer Sebag", "Priyanka Sen", "Sholeh Sepehri", "Preethi Seshadri", "Ye Shen", "Tom Sherborne", "Sylvie Chang Shi", "Sanal Shivaprasad", "Vladyslav Shmyhlo", "Anirudh Shrinivason", "Inna Shteinbuk", "Amir Shukayev", "Mathieu Simard", "Ella Snyder", "Ava Spataru", "Victoria Spooner", "Trisha Starostina", "Florian Strub", "Yixuan Su", "Jimin Sun", "Dwarak Talupuru", "Eugene Tarassov", "Elena Tommasone", "Jennifer Tracey", "Billy Trend", "Evren Tumer", "Ahmet ÃstÃ¼n", "Bharat Venkitesh", "David Venuto", "Pat Verga", "Maxime Voisin", "Alex Wang", "Donglu Wang", "Shijian Wang", "Edmond Wen", "Naomi White", "Jesse Willman", "Marysia Winkels", "Chen Xia", "Jessica Xie", "Minjie Xu", "Bowen Yang", "Tan Yi-Chern", "Ivan Zhang", "Zhenyu Zhao", "Zhoujie Zhao"], "title": "Command A: An Enterprise-Ready Large Language Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "55 pages", "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency."}
{"id": "2504.00490", "pdf": "https://arxiv.org/pdf/2504.00490", "abs": "https://arxiv.org/abs/2504.00490", "authors": ["Zetong Chen", "Yuzhuo Chen", "Hai Zhong", "Xu Qiao"], "title": "SCFANet: Style Distribution Constraint Feature Alignment Network For Pathological Staining Translation", "categories": ["cs.CV"], "comment": null, "summary": "Immunohistochemical (IHC) staining serves as a valuable technique for\ndetecting specific antigens or proteins through antibody-mediated\nvisualization. However, the IHC staining process is both time-consuming and\ncostly. To address these limitations, the application of deep learning models\nfor direct translation of cost-effective Hematoxylin and Eosin (H&E) stained\nimages into IHC stained images has emerged as an efficient solution.\nNevertheless, the conversion from H&E to IHC images presents significant\nchallenges, primarily due to alignment discrepancies between image pairs and\nthe inherent diversity in IHC staining style patterns. To overcome these\nchallenges, we propose the Style Distribution Constraint Feature Alignment\nNetwork (SCFANet), which incorporates two innovative modules: the Style\nDistribution Constrainer (SDC) and Feature Alignment Learning (FAL). The SDC\nensures consistency between the generated and target images' style\ndistributions while integrating cycle consistency loss to maintain structural\nconsistency. To mitigate the complexity of direct image-to-image translation,\nthe FAL module decomposes the end-to-end translation task into two subtasks:\nimage reconstruction and feature alignment. Furthermore, we ensure pathological\nconsistency between generated and target images by maintaining pathological\npattern consistency and Optical Density (OD) uniformity. Extensive experiments\nconducted on the Breast Cancer Immunohistochemical (BCI) dataset demonstrate\nthat our SCFANet model outperforms existing methods, achieving precise\ntransformation of H&E-stained images into their IHC-stained counterparts. The\nproposed approach not only addresses the technical challenges in H&E to IHC\nimage translation but also provides a robust framework for accurate and\nefficient stain conversion in pathological analysis."}
{"id": "2504.00725", "pdf": "https://arxiv.org/pdf/2504.00725", "abs": "https://arxiv.org/abs/2504.00725", "authors": ["Matheus Belarmino", "Rackel Coelho", "Roberto Lotudo", "Jayr Pereira"], "title": "AplicaÃ§Ã£o de Large Language Models na AnÃ¡lise e SÃ­ntese de Documentos JurÃ­dicos: Uma RevisÃ£o de Literatura", "categories": ["cs.CL"], "comment": "in Portuguese language", "summary": "Large Language Models (LLMs) have been increasingly used to optimize the\nanalysis and synthesis of legal documents, enabling the automation of tasks\nsuch as summarization, classification, and retrieval of legal information. This\nstudy aims to conduct a systematic literature review to identify the state of\nthe art in prompt engineering applied to LLMs in the legal context. The results\nindicate that models such as GPT-4, BERT, Llama 2, and Legal-Pegasus are widely\nemployed in the legal field, and techniques such as Few-shot Learning,\nZero-shot Learning, and Chain-of-Thought prompting have proven effective in\nimproving the interpretation of legal texts. However, challenges such as biases\nin models and hallucinations still hinder their large-scale implementation. It\nis concluded that, despite the great potential of LLMs for the legal field,\nthere is a need to improve prompt engineering strategies to ensure greater\naccuracy and reliability in the generated results."}
{"id": "2504.00496", "pdf": "https://arxiv.org/pdf/2504.00496", "abs": "https://arxiv.org/abs/2504.00496", "authors": ["Jingbo Lu", "Leheng Zhang", "Xingyu Zhou", "Mu Li", "Wen Li", "Shuhang Gu"], "title": "Learned Image Compression with Dictionary-based Entropy Model", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Learned image compression methods have attracted great research interest and\nexhibited superior rate-distortion performance to the best classical image\ncompression standards of the present. The entropy model plays a key role in\nlearned image compression, which estimates the probability distribution of the\nlatent representation for further entropy coding. Most existing methods\nemployed hyper-prior and auto-regressive architectures to form their entropy\nmodels. However, they only aimed to explore the internal dependencies of latent\nrepresentation while neglecting the importance of extracting prior from\ntraining data. In this work, we propose a novel entropy model named\nDictionary-based Cross Attention Entropy model, which introduces a learnable\ndictionary to summarize the typical structures occurring in the training\ndataset to enhance the entropy model. Extensive experimental results have\ndemonstrated that the proposed model strikes a better balance between\nperformance and latency, achieving state-of-the-art results on various\nbenchmark datasets."}
{"id": "2504.00748", "pdf": "https://arxiv.org/pdf/2504.00748", "abs": "https://arxiv.org/abs/2504.00748", "authors": ["Yunsoo Kim", "Michal W. S. Ong", "Daniel W. Rogalsky", "Manuel Rodriguez-Justo", "Honghan Wu", "Adam P. Levine"], "title": "IHC-LLMiner: Automated extraction of tumour immunohistochemical profiles from PubMed abstracts using large language models", "categories": ["cs.CL"], "comment": "currently under review", "summary": "Immunohistochemistry (IHC) is essential in diagnostic pathology and\nbiomedical research, offering critical insights into protein expression and\ntumour biology. This study presents an automated pipeline, IHC-LLMiner, for\nextracting IHC-tumour profiles from PubMed abstracts, leveraging advanced\nbiomedical text mining. There are two subtasks: abstract classification\n(include/exclude as relevant) and IHC-tumour profile extraction on relevant\nincluded abstracts. The best-performing model, \"Gemma-2 finetuned\", achieved\n91.5% accuracy and an F1 score of 91.4, outperforming GPT4-O by 9.5% accuracy\nwith 5.9 times faster inference time. From an initial dataset of 107,759\nabstracts identified for 50 immunohistochemical markers, the classification\ntask identified 30,481 relevant abstracts (Include) using the Gemma-2 finetuned\nmodel. For IHC-tumour profile extraction, the Gemma-2 finetuned model achieved\nthe best performance with 63.3% Correct outputs. Extracted IHC-tumour profiles\n(tumour types and markers) were normalised to Unified Medical Language System\n(UMLS) concepts to ensure consistency and facilitate IHC-tumour profile\nlandscape analysis. The extracted IHC-tumour profiles demonstrated excellent\nconcordance with available online summary data and provided considerable added\nvalue in terms of both missing IHC-tumour profiles and quantitative\nassessments. Our proposed LLM based pipeline provides a practical solution for\nlarge-scale IHC-tumour profile data mining, enhancing the accessibility and\nutility of such data for research and clinical applications as well as enabling\nthe generation of quantitative and structured data to support cancer-specific\nknowledge base development. Models and training datasets are available at\nhttps://github.com/knowlab/IHC-LLMiner."}
{"id": "2504.00502", "pdf": "https://arxiv.org/pdf/2504.00502", "abs": "https://arxiv.org/abs/2504.00502", "authors": ["Qianhao Yuan", "Qingyu Zhang", "Yanjiang Liu", "Jiawei Chen", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun"], "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://github.com/icip-cas/ShortV", "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV"}
{"id": "2504.00752", "pdf": "https://arxiv.org/pdf/2504.00752", "abs": "https://arxiv.org/abs/2504.00752", "authors": ["Sameer Sadruddin", "Jennifer D'Souza", "Eleni Poupaki", "Alex Watkins", "Hamed Babaei Giglou", "Anisa Rula", "Bora Karasulu", "SÃ¶ren Auer", "Adrie Mackus", "Erwin Kessels"], "title": "LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "15 pages, 3 figures, to appear in the Extended Semantic Web\n  Conference (ESWC 2025) proceedings in the Resource track", "summary": "Extracting structured information from unstructured text is crucial for\nmodeling real-world processes, but traditional schema mining relies on\nsemi-structured data, limiting scalability. This paper introduces schema-miner,\na novel tool that combines large language models with human feedback to\nautomate and refine schema extraction. Through an iterative workflow, it\norganizes properties from text, incorporates expert input, and integrates\ndomain-specific ontologies for semantic depth. Applied to materials\nscience--specifically atomic layer deposition--schema-miner demonstrates that\nexpert-guided LLMs generate semantically rich schemas suitable for diverse\nreal-world applications."}
{"id": "2504.00526", "pdf": "https://arxiv.org/pdf/2504.00526", "abs": "https://arxiv.org/abs/2504.00526", "authors": ["Xinrun Xu", "Qiuhong Zhang", "Jianwen Yang", "Zhanbiao Lian", "Jin Yan", "Zhiming Ding", "Shan Jiang"], "title": "High-Quality Pseudo-Label Generation Based on Visual Prompt Assisted Cloud Model Update", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN'25", "summary": "Generating high-quality pseudo-labels on the cloud is crucial for cloud-edge\nobject detection, especially in dynamic traffic monitoring where data\ndistributions evolve. Existing methods often assume reliable cloud models,\nneglecting potential errors or struggling with complex distribution shifts.\nThis paper proposes Cloud-Adaptive High-Quality Pseudo-label generation\n(CA-HQP), addressing these limitations by incorporating a learnable Visual\nPrompt Generator (VPG) and dual feature alignment into cloud model updates. The\nVPG enables parameter-efficient adaptation by injecting visual prompts,\nenhancing flexibility without extensive fine-tuning. CA-HQP mitigates domain\ndiscrepancies via two feature alignment techniques: global Domain Query Feature\nAlignment (DQFA) capturing scene-level shifts, and fine-grained Temporal\nInstance-Aware Feature Embedding Alignment (TIAFA) addressing instance\nvariations. Experiments on the Bellevue traffic dataset demonstrate that CA-HQP\nsignificantly improves pseudo-label quality compared to existing methods,\nleading to notable performance gains for the edge model and showcasing CA-HQP's\nadaptation effectiveness. Ablation studies validate each component (DQFA,\nTIAFA, VPG) and the synergistic effect of combined alignment strategies,\nhighlighting the importance of adaptive cloud updates and domain adaptation for\nrobust object detection in evolving scenarios. CA-HQP provides a promising\nsolution for enhancing cloud-edge object detection systems in real-world\napplications."}
{"id": "2504.00756", "pdf": "https://arxiv.org/pdf/2504.00756", "abs": "https://arxiv.org/abs/2504.00756", "authors": ["Lin Zhang", "Zhouhong Gu", "Xiaoran Shi", "Hongwei Feng", "Yanghua Xiao"], "title": "RECKON: Large-scale Reference-based Efficient Knowledge Evaluation for Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) advance, efficient knowledge evaluation\nbecomes crucial to verifying their capabilities. Traditional methods, relying\non benchmarks, face limitations such as high resource costs and information\nloss. We propose the Large-scale Reference-based Efficient Knowledge Evaluation\nfor Large Language Model (RECKON), which directly uses reference data to\nevaluate models. RECKON organizes unstructured data into manageable units and\ngenerates targeted questions for each cluster, improving evaluation accuracy\nand efficiency. Experimental results show that RECKON reduces resource\nconsumption by 56.5% compared to traditional methods while achieving over 97%\naccuracy across various domains, including world knowledge, code, legal, and\nbiomedical datasets. Code is available at https://github.com/MikeGu721/reckon"}
{"id": "2504.00527", "pdf": "https://arxiv.org/pdf/2504.00527", "abs": "https://arxiv.org/abs/2504.00527", "authors": ["Fida Mohammad Thoker", "Letian Jiang", "Chen Zhao", "Bernard Ghanem"], "title": "SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Masked video modeling, such as VideoMAE, is an effective paradigm for video\nself-supervised learning (SSL). However, they are primarily based on\nreconstructing pixel-level details on natural videos which have substantial\ntemporal redundancy, limiting their capability for semantic representation and\nsufficient encoding of motion dynamics. To address these issues, this paper\nintroduces a novel SSL approach for video representation learning, dubbed as\nSMILE, by infusing both spatial and motion semantics. In SMILE, we leverage\nimage-language pretrained models, such as CLIP, to guide the learning process\nwith their high-level spatial semantics. We enhance the representation of\nmotion by introducing synthetic motion patterns in the training data, allowing\nthe model to capture more complex and dynamic content. Furthermore, using\nSMILE, we establish a new self-supervised video learning paradigm capable of\nlearning strong video representations without requiring any natural video data.\nWe have carried out extensive experiments on 7 datasets with various downstream\nscenarios. SMILE surpasses current state-of-the-art SSL methods, showcasing its\neffectiveness in learning more discriminative and generalizable video\nrepresentations. Code is available: https://github.com/fmthoker/SMILE"}
{"id": "2504.00780", "pdf": "https://arxiv.org/pdf/2504.00780", "abs": "https://arxiv.org/abs/2504.00780", "authors": ["Anja Ryser", "Yingqiang Gao", "Sarah Ebling"], "title": "Digitally Supported Analysis of Spontaneous Speech (DigiSpon): Benchmarking NLP-Supported Language Sample Analysis of Swiss Children's Speech", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language sample analysis (LSA) is a process that complements standardized\npsychometric tests for diagnosing, for example, developmental language disorder\n(DLD) in children. However, its labor-intensive nature has limited its use in\nspeech-language pathology practice. We introduce an approach that leverages\nnatural language processing (NLP) methods not based on commercial large\nlanguage models (LLMs) applied to transcribed speech data from 119 children in\nthe German speaking part of Switzerland with typical and atypical language\ndevelopment. The study aims to identify optimal practices that support\nspeech-language pathologists in diagnosing DLD more efficiently within a\nhuman-in-the-loop framework, without relying on potentially unethical\nimplementations that leverage commercial LLMs. Preliminary findings underscore\nthe potential of integrating locally deployed NLP methods into the process of\nsemi-automatic LSA."}
{"id": "2504.00543", "pdf": "https://arxiv.org/pdf/2504.00543", "abs": "https://arxiv.org/abs/2504.00543", "authors": ["Qi Zang", "Shuang Wang", "Dong Zhao", "Dou Quan", "Yang Hu", "Licheng Jiao"], "title": "Generalization-aware Remote Sensing Change Detection via Domain-agnostic Learning", "categories": ["cs.CV"], "comment": null, "summary": "Change detection has essential significance for the region's development, in\nwhich pseudo-changes between bitemporal images induced by imaging environmental\nfactors are key challenges. Existing transformation-based methods regard\npseudo-changes as a kind of style shift and alleviate it by transforming\nbitemporal images into the same style using generative adversarial networks\n(GANs). However, their efforts are limited by two drawbacks: 1) Transformed\nimages suffer from distortion that reduces feature discrimination. 2) Alignment\nhampers the model from learning domain-agnostic representations that degrades\nperformance on scenes with domain shifts from the training data. Therefore,\noriented from pseudo-changes caused by style differences, we present a\ngeneralizable domain-agnostic difference learning network (DonaNet). For the\ndrawback 1), we argue for local-level statistics as style proxies to assist\nagainst domain shifts. For the drawback 2), DonaNet learns domain-agnostic\nrepresentations by removing domain-specific style of encoded features and\nhighlighting the class characteristics of objects. In the removal, we propose a\ndomain difference removal module to reduce feature variance while preserving\ndiscriminative properties and propose its enhanced version to provide\npossibilities for eliminating more style by decorrelating the correlation\nbetween features. In the highlighting, we propose a cross-temporal\ngeneralization learning strategy to imitate latent domain shifts, thus enabling\nthe model to extract feature representations more robust to shifts actively.\nExtensive experiments conducted on three public datasets demonstrate that\nDonaNet outperforms existing state-of-the-art methods with a smaller model size\nand is more robust to domain shift."}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799", "abs": "https://arxiv.org/abs/2504.00799", "authors": ["Xi Wang", "Fanfei Meng", "Shiyang Zhang", "Lan Li"], "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "13 pages, presented at ASIALEX 2023 (The 15th International\n  Conference of the Asian Association for Lexicography), Yonsei University,\n  Seoul, Korea", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries."}
{"id": "2504.00557", "pdf": "https://arxiv.org/pdf/2504.00557", "abs": "https://arxiv.org/abs/2504.00557", "authors": ["Jewon Lee", "Ki-Ung Song", "Seungmin Yang", "Donguk Lim", "Jaeyeon Kim", "Wooksu Shin", "Bo-Kyeong Kim", "Yong Jae Lee", "Tae-Ho Kim"], "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features", "categories": ["cs.CV", "cs.LG"], "comment": "accepted at CVPR 2025 Workshop on ELVM", "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."}
{"id": "2504.00810", "pdf": "https://arxiv.org/pdf/2504.00810", "abs": "https://arxiv.org/abs/2504.00810", "authors": ["Zhaojian Yu", "Yinghao Wu", "Yilun Zhao", "Arman Cohan", "Xiao-Ping Zhang"], "title": "Z1: Efficient Test-time Scaling with Code", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research."}
{"id": "2504.00558", "pdf": "https://arxiv.org/pdf/2504.00558", "abs": "https://arxiv.org/abs/2504.00558", "authors": ["Marek VaÅ¡ko", "Adam Herout", "Michal HradiÅ¡"], "title": "Archival Faces: Detection of Faces in Digitized Historical Documents", "categories": ["cs.CV", "68T45 (Primary) 68T10, 68T07 (Secondary)", "I.4.8; I.5.1"], "comment": "15 pages, 6 figures, 6 tables", "summary": "When digitizing historical archives, it is necessary to search for the faces\nof celebrities and ordinary people, especially in newspapers, link them to the\nsurrounding text, and make them searchable. Existing face detectors on datasets\nof scanned historical documents fail remarkably -- current detection tools only\nachieve around $24\\%$ mAP at $50:90\\%$ IoU. This work compensates for this\nfailure by introducing a new manually annotated domain-specific dataset in the\nstyle of the popular Wider Face dataset, containing 2.2k new images from\ndigitized historical newspapers from the $19^{th}$ to $20^{th}$ century, with\n11k new bounding-box annotations and associated facial landmarks. This dataset\nallows existing detectors to be retrained to bring their results closer to the\nstandard in the field of face detection in the wild. We report several\nexperimental results comparing different families of fine-tuned detectors\nagainst publicly available pre-trained face detectors and ablation studies of\nmultiple detector sizes with comprehensive detection and landmark prediction\nperformance results."}
{"id": "2504.00824", "pdf": "https://arxiv.org/pdf/2504.00824", "abs": "https://arxiv.org/abs/2504.00824", "authors": ["Yubo Wang", "Xueguang Ma", "Ping Nie", "Huaye Zeng", "Zhiheng Lyu", "Yuxuan Zhang", "Benjamin Schneider", "Yi Lu", "Xiang Yue", "Wenhu Chen"], "title": "ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations", "categories": ["cs.CL"], "comment": null, "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach."}
{"id": "2504.00559", "pdf": "https://arxiv.org/pdf/2504.00559", "abs": "https://arxiv.org/abs/2504.00559", "authors": ["Loveneet Saini", "Mirko Meuter", "Hasan Tercan", "Tobias Meisen"], "title": "AttentiveGRU: Recurrent Spatio-Temporal Modeling for Advanced Radar-Based BEV Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Bird's-eye view (BEV) object detection has become important for advanced\nautomotive 3D radar-based perception systems. However, the inherently sparse\nand non-deterministic nature of radar data limits the effectiveness of\ntraditional single-frame BEV paradigms. In this paper, we addresses this\nlimitation by introducing AttentiveGRU, a novel attention-based recurrent\napproach tailored for radar constraints, which extracts individualized\nspatio-temporal context for objects by dynamically identifying and fusing\ntemporally correlated structures across present and memory states. By\nleveraging the consistency of object's latent representation over time, our\napproach exploits temporal relations to enrich feature representations for both\nstationary and moving objects, thereby enhancing detection performance and\neliminating the need for externally providing or estimating any information\nabout ego vehicle motion. Our experimental results on the public nuScenes\ndataset show a significant increase in mAP for the car category by 21% over the\nbest radar-only submission. Further evaluations on an additional dataset\ndemonstrate notable improvements in object detection capabilities, underscoring\nthe applicability and effectiveness of our method."}
{"id": "2504.00829", "pdf": "https://arxiv.org/pdf/2504.00829", "abs": "https://arxiv.org/abs/2504.00829", "authors": ["Yunjie Ji", "Sitong Zhao", "Xiaoyu Tian", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "How Difficulty-Aware Staged Reinforcement Learning Enhances LLMs' Reasoning Capabilities: A Preliminary Experimental Study", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of Large Language Models (LLMs) with\nefficiency and scalability remains a fundamental challenge in artificial\nintelligence research. This paper presents a rigorous experimental\ninvestigation into how difficulty-aware staged reinforcement learning (RL)\nstrategies can substantially improve LLM reasoning performance. Through\nsystematic analysis, we demonstrate that strategically selecting training data\naccording to well-defined difficulty levels markedly enhances RL optimization.\nMoreover, we introduce a staged training methodology, progressively exposing\nmodels to increasingly challenging tasks, further amplifying reasoning\ncapabilities. Our findings reveal significant cross-domain benefits when\nsimultaneously training models on mathematical reasoning and code generation\ntasks. Notably, our proposed approach enables a 1.5B parameter model to achieve\nan accuracy of 42.3\\% on the AIME-2024 benchmark, 89.5\\% on the MATH-500\nbenchmark. These results underscore the efficacy of our method in advancing the\nreasoning proficiency of LLMs. We will open-source our datasets on GitHub and\nHugging Face."}
{"id": "2504.00561", "pdf": "https://arxiv.org/pdf/2504.00561", "abs": "https://arxiv.org/abs/2504.00561", "authors": ["Yan Xia", "Hai Huang", "Minghui Fang", "Zhou Zhao"], "title": "Continual Cross-Modal Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal generalization aims to learn a shared discrete representation\nspace from multimodal pairs, enabling knowledge transfer across unannotated\nmodalities. However, achieving a unified representation for all modality pairs\nrequires extensive paired data, which is often impractical. Inspired by the\navailability of abundant bimodal data (e.g., in ImageBind), we explore a\ncontinual learning approach that incrementally maps new modalities into a\nshared discrete codebook via a mediator modality. We propose the Continual\nMixture of Experts Adapter (CMoE-Adapter) to project diverse modalities into a\nunified space while preserving prior knowledge. To align semantics across\nstages, we introduce a Pseudo-Modality Replay (PMR) mechanism with a\ndynamically expanding codebook, enabling the model to adaptively incorporate\nnew modalities using learned ones as guidance. Extensive experiments on\nimage-text, audio-text, video-text, and speech-text show that our method\nachieves strong performance on various cross-modal generalization tasks. Code\nis provided in the supplementary material."}
{"id": "2504.00860", "pdf": "https://arxiv.org/pdf/2504.00860", "abs": "https://arxiv.org/abs/2504.00860", "authors": ["Lucy Havens", "Benjamin Bach", "Melissa Terras", "Beatrice Alex"], "title": "Investigating the Capabilities and Limitations of Machine Learning for Identifying Bias in English Language Data with Information and Heritage Professionals", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "I.2.7; J.0; K.4.0"], "comment": "Accepted to the 2025 CHI Conference on Human Factors in Computing\n  Systems (CHI '25)", "summary": "Despite numerous efforts to mitigate their biases, ML systems continue to\nharm already-marginalized people. While predominant ML approaches assume bias\ncan be removed and fair models can be created, we show that these are not\nalways possible, nor desirable, goals. We reframe the problem of ML bias by\ncreating models to identify biased language, drawing attention to a dataset's\nbiases rather than trying to remove them. Then, through a workshop, we\nevaluated the models for a specific use case: workflows of information and\nheritage professionals. Our findings demonstrate the limitations of ML for\nidentifying bias due to its contextual nature, the way in which approaches to\nmitigating it can simultaneously privilege and oppress different communities,\nand its inevitability. We demonstrate the need to expand ML approaches to bias\nand fairness, providing a mixed-methods approach to investigating the\nfeasibility of removing bias or achieving fairness in a given ML use case."}
{"id": "2504.00606", "pdf": "https://arxiv.org/pdf/2504.00606", "abs": "https://arxiv.org/abs/2504.00606", "authors": ["Ping Li", "Chenhao Ping", "Wenxiao Wang", "Mingli Song"], "title": "Sample-level Adaptive Knowledge Distillation for Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge Distillation (KD) compresses neural networks by learning a small\nnetwork (student) via transferring knowledge from a pre-trained large network\n(teacher). Many endeavours have been devoted to the image domain, while few\nworks focus on video analysis which desires training much larger model making\nit be hardly deployed in resource-limited devices. However, traditional methods\nneglect two important problems, i.e., 1) Since the capacity gap between the\nteacher and the student exists, some knowledge w.r.t. difficult-to-transfer\nsamples cannot be correctly transferred, or even badly affects the final\nperformance of student, and 2) As training progresses, difficult-to-transfer\nsamples may become easier to learn, and vice versa. To alleviate the two\nproblems, we propose a Sample-level Adaptive Knowledge Distillation (SAKD)\nframework for action recognition. In particular, it mainly consists of the\nsample distillation difficulty evaluation module and the sample adaptive\ndistillation module. The former applies the temporal interruption to frames,\ni.e., randomly dropout or shuffle the frames during training, which increases\nthe learning difficulty of samples during distillation, so as to better\ndiscriminate their distillation difficulty. The latter module adaptively\nadjusts distillation ratio at sample level, such that KD loss dominates the\ntraining with easy-to-transfer samples while vanilla loss dominates that with\ndifficult-to-transfer samples. More importantly, we only select those samples\nwith both low distillation difficulty and high diversity to train the student\nmodel for reducing computational cost. Experimental results on two video\nbenchmarks and one image benchmark demonstrate the superiority of the proposed\nmethod by striking a good balance between performance and efficiency."}
{"id": "2504.00869", "pdf": "https://arxiv.org/pdf/2504.00869", "abs": "https://arxiv.org/abs/2504.00869", "authors": ["Xiaoke Huang", "Juncheng Wu", "Hui Liu", "Xianfeng Tang", "Yuyin Zhou"], "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages; 7 figures; Data, code, and models:\n  https://github.com/UCSC-VLAA/m1", "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling."}
{"id": "2504.00609", "pdf": "https://arxiv.org/pdf/2504.00609", "abs": "https://arxiv.org/abs/2504.00609", "authors": ["Huichuan Huang", "Zhiqing Zhong", "Guangyu Wei", "Yonghao Wan", "Wenlong Sun", "Aimin Feng"], "title": "Bi-Grid Reconstruction for Image Anomaly Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In image anomaly detection, significant advancements have been made using un-\nand self-supervised methods with datasets containing only normal samples.\nHowever, these approaches often struggle with fine-grained anomalies. This\npaper introduces \\textbf{GRAD}: Bi-\\textbf{G}rid \\textbf{R}econstruction for\nImage \\textbf{A}nomaly \\textbf{D}etection, which employs two continuous grids\nto enhance anomaly detection from both normal and abnormal perspectives. In\nthis work: 1) Grids as feature repositories that improve generalization and\nmitigate the Identical Shortcut (IS) issue; 2) An abnormal feature grid that\nrefines normal feature boundaries, boosting detection of fine-grained defects;\n3) The Feature Block Paste (FBP) module, which synthesizes various anomalies at\nthe feature level for quick abnormal grid deployment. GRAD's robust\nrepresentation capabilities also allow it to handle multiple classes with a\nsingle model. Evaluations on datasets like MVTecAD, VisA, and GoodsAD show\nsignificant performance improvements in fine-grained anomaly detection. GRAD\nexcels in overall accuracy and in discerning subtle differences, demonstrating\nits superiority over existing methods."}
{"id": "2504.00891", "pdf": "https://arxiv.org/pdf/2504.00891", "abs": "https://arxiv.org/abs/2504.00891", "authors": ["Jian Zhao", "Runze Liu", "Kaiyan Zhang", "Zhimu Zhou", "Junqi Gao", "Dong Li", "Jiafei Lyu", "Zhouyi Qian", "Biqing Qi", "Xiu Li", "Bowen Zhou"], "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM."}
{"id": "2504.00639", "pdf": "https://arxiv.org/pdf/2504.00639", "abs": "https://arxiv.org/abs/2504.00639", "authors": ["Jiamin Wu", "Hongyang Li", "Xiaoke Jiang", "Yuan Yao", "Lei Zhang"], "title": "Coca-Splat: Collaborative Optimization for Camera Parameters and 3D Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we introduce Coca-Splat, a novel approach to addressing the\nchallenges of sparse view pose-free scene reconstruction and novel view\nsynthesis (NVS) by jointly optimizing camera parameters with 3D Gaussians.\nInspired by deformable DEtection TRansformer, we design separate queries for 3D\nGaussians and camera parameters and update them layer by layer through\ndeformable Transformer layers, enabling joint optimization in a single network.\nThis design demonstrates better performance because to accurately render views\nthat closely approximate ground-truth images relies on precise estimation of\nboth 3D Gaussians and camera parameters. In such a design, the centers of 3D\nGaussians are projected onto each view by camera parameters to get projected\npoints, which are regarded as 2D reference points in deformable\ncross-attention. With camera-aware multi-view deformable cross-attention\n(CaMDFA), 3D Gaussians and camera parameters are intrinsically connected by\nsharing the 2D reference points. Additionally, 2D reference point determined\nrays (RayRef) defined from camera centers to the reference points assist in\nmodeling relationship between 3D Gaussians and camera parameters through\nRQ-decomposition on an overdetermined system of equations derived from the\nrays, enhancing the relationship between 3D Gaussians and camera parameters.\nExtensive evaluation shows that our approach outperforms previous methods, both\npose-required and pose-free, on RealEstate10K and ACID within the same\npose-free setting."}
{"id": "2504.00914", "pdf": "https://arxiv.org/pdf/2504.00914", "abs": "https://arxiv.org/abs/2504.00914", "authors": ["Ella Rabinovich", "Ateret Anaby-Tavor"], "title": "On the Robustness of Agentic Function Calling", "categories": ["cs.CL"], "comment": "7 pages, TrustNLP@NAACL25", "summary": "Large Language Models (LLMs) are increasingly acting as autonomous agents,\nwith function calling (FC) capabilities enabling them to invoke specific tools\nfor tasks. While prior research has primarily focused on improving FC accuracy,\nlittle attention has been given to the robustness of these agents to\nperturbations in their input. We introduce a benchmark assessing FC robustness\nin two key areas: resilience to naturalistic query variations, and stability in\nfunction calling when the toolkit expands with semantically related tools.\nEvaluating best-performing FC models on a carefully expanded subset of the\nBerkeley function calling leaderboard (BFCL), we identify critical weaknesses\nin existing evaluation methodologies, and highlight areas for improvement in\nreal-world agentic deployments."}
{"id": "2504.00640", "pdf": "https://arxiv.org/pdf/2504.00640", "abs": "https://arxiv.org/abs/2504.00640", "authors": ["Lanyun Zhu", "Tianrun Chen", "Qianxiong Xu", "Xuanyi Liu", "Deyi Ji", "Haiyang Wu", "De Wen Soh", "Jun Liu"], "title": "POPEN: Preference-Based Optimization and Ensemble for LVLM-Based Reasoning Segmentation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Existing LVLM-based reasoning segmentation methods often suffer from\nimprecise segmentation results and hallucinations in their text responses. This\npaper introduces POPEN, a novel framework designed to address these issues and\nachieve improved results. POPEN includes a preference-based optimization method\nto finetune the LVLM, aligning it more closely with human preferences and\nthereby generating better text responses and segmentation results.\nAdditionally, POPEN introduces a preference-based ensemble method for\ninference, which integrates multiple outputs from the LVLM using a\npreference-score-based attention mechanism for refinement. To better adapt to\nthe segmentation task, we incorporate several task-specific designs in our\nPOPEN framework, including a new approach for collecting segmentation\npreference data with a curriculum learning mechanism, and a novel preference\noptimization loss to refine the segmentation capability of the LVLM.\nExperiments demonstrate that our method achieves state-of-the-art performance\nin reasoning segmentation, exhibiting minimal hallucination in text responses\nand the highest segmentation accuracy compared to previous advanced methods\nlike LISA and PixelLM. Project page is https://lanyunzhu.site/POPEN/"}
{"id": "2504.00927", "pdf": "https://arxiv.org/pdf/2504.00927", "abs": "https://arxiv.org/abs/2504.00927", "authors": ["Olga Golovneva", "Tianlu Wang", "Jason Weston", "Sainbayar Sukhbaatar"], "title": "Multi-Token Attention", "categories": ["cs.CL"], "comment": null, "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial."}
{"id": "2504.00647", "pdf": "https://arxiv.org/pdf/2504.00647", "abs": "https://arxiv.org/abs/2504.00647", "authors": ["Xinnan Zhu", "Yicheng Zhu", "Tixin Chen", "Wentao Wu", "Yuanjie Dang"], "title": "FDDet: Frequency-Decoupling for Boundary Refinement in Temporal Action Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal action detection aims to locate and classify actions in untrimmed\nvideos. While recent works focus on designing powerful feature processors for\npre-trained representations, they often overlook the inherent noise and\nredundancy within these features. Large-scale pre-trained video encoders tend\nto introduce background clutter and irrelevant semantics, leading to context\nconfusion and imprecise boundaries. To address this, we propose a\nfrequency-aware decoupling network that improves action discriminability by\nfiltering out noisy semantics captured by pre-trained models. Specifically, we\nintroduce an adaptive temporal decoupling scheme that suppresses irrelevant\ninformation while preserving fine-grained atomic action details, yielding more\ntask-specific representations. In addition, we enhance inter-frame modeling by\ncapturing temporal variations to better distinguish actions from background\nredundancy. Furthermore, we present a long-short-term category-aware relation\nnetwork that jointly models local transitions and long-range dependencies,\nimproving localization precision. The refined atomic features and\nfrequency-guided dynamics are fed into a standard detection head to produce\naccurate action predictions. Extensive experiments on THUMOS14, HACS, and\nActivityNet-1.3 show that our method, powered by InternVideo2-6B features,\nachieves state-of-the-art performance on temporal action detection benchmarks."}
{"id": "2504.00928", "pdf": "https://arxiv.org/pdf/2504.00928", "abs": "https://arxiv.org/abs/2504.00928", "authors": ["Emily Corvi", "Hannah Washington", "Stefanie Reed", "Chad Atalla", "Alexandra Chouldechova", "P. Alex Dow", "Jean Garcia-Gathright", "Nicholas Pangakis", "Emily Sheng", "Dan Vann", "Matthew Vogel", "Hanna Wallach"], "title": "Taxonomizing Representational Harms using Speech Act Theory", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Representational harms are widely recognized among fairness-related harms\ncaused by generative language systems. However, their definitions are commonly\nunder-specified. We present a framework, grounded in speech act theory (Austin,\n1962), that conceptualizes representational harms caused by generative language\nsystems as the perlocutionary effects (i.e., real-world impacts) of particular\ntypes of illocutionary acts (i.e., system behaviors). Building on this argument\nand drawing on relevant literature from linguistic anthropology and\nsociolinguistics, we provide new definitions stereotyping, demeaning, and\nerasure. We then use our framework to develop a granular taxonomy of\nillocutionary acts that cause representational harms, going beyond the\nhigh-level taxonomies presented in previous work. We also discuss the ways that\nour framework and taxonomy can support the development of valid measurement\ninstruments. Finally, we demonstrate the utility of our framework and taxonomy\nvia a case study that engages with recent conceptual debates about what\nconstitutes a representational harm and how such harms should be measured."}
{"id": "2504.00654", "pdf": "https://arxiv.org/pdf/2504.00654", "abs": "https://arxiv.org/abs/2504.00654", "authors": ["Shuai Li", "Jian Xu", "Xiao-Hui Li", "Chao Deng", "Lin-Lin Huang"], "title": "QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Multi-modal Large Language Models (MLLMs) have shown\nsignificant progress in open-world Visual Question Answering (VQA). However,\nintegrating visual information increases the number of processed tokens,\nleading to higher GPU memory usage and computational overhead. Images often\ncontain more redundant information than text, and not all visual details are\npertinent to specific questions. To address these challenges, we propose\nQG-VTC, a novel question-guided visual token compression method for MLLM-based\nVQA tasks. QG-VTC employs a pretrained text encoder and a learnable\nfeed-forward layer to embed user questions into the vision encoder's feature\nspace then computes correlation scores between the question embeddings and\nvisual tokens. By selecting the most relevant tokens and softly compressing\nothers, QG-VTC ensures fine-tuned relevance to user needs. Additionally, a\nprogressive strategy applies this compression across different vision encoder\nlayers, gradually reducing token numbers. This approach maximizes retention of\nquestion-relevant information while discarding irrelevant details. Experimental\nresults show that our method achieves performance on par with uncompressed\nmodels using just 1/8 of the visual tokens. The code and model will be publicly\navailable on GitHub."}
{"id": "2504.00934", "pdf": "https://arxiv.org/pdf/2504.00934", "abs": "https://arxiv.org/abs/2504.00934", "authors": ["Zifeng Wang", "Junyi Gao", "Benjamin Danek", "Brandon Theodorou", "Ruba Shaik", "Shivashankar Thati", "Seunghyun Won", "Jimeng Sun"], "title": "InformGen: An AI Copilot for Accurate and Compliant Clinical Research Consent Document Generation", "categories": ["cs.CL"], "comment": null, "summary": "Leveraging large language models (LLMs) to generate high-stakes documents,\nsuch as informed consent forms (ICFs), remains a significant challenge due to\nthe extreme need for regulatory compliance and factual accuracy. Here, we\npresent InformGen, an LLM-driven copilot for accurate and compliant ICF\ndrafting by optimized knowledge document parsing and content generation, with\nhumans in the loop. We further construct a benchmark dataset comprising\nprotocols and ICFs from 900 clinical trials. Experimental results demonstrate\nthat InformGen achieves near 100% compliance with 18 core regulatory rules\nderived from FDA guidelines, outperforming a vanilla GPT-4o model by up to 30%.\nAdditionally, a user study with five annotators shows that InformGen, when\nintegrated with manual intervention, attains over 90% factual accuracy,\nsignificantly surpassing the vanilla GPT-4o model's 57%-82%. Crucially,\nInformGen ensures traceability by providing inline citations to source\nprotocols, enabling easy verification and maintaining the highest standards of\nfactual integrity."}
{"id": "2504.00665", "pdf": "https://arxiv.org/pdf/2504.00665", "abs": "https://arxiv.org/abs/2504.00665", "authors": ["Shengjie Gong", "Haojie Li", "Jiapeng Tang", "Dongming Hu", "Shuangping Huang", "Hao Chen", "Tianshui Chen", "Zhuoman Liu"], "title": "Monocular and Generalizable Gaussian Talking Head Animation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "In this work, we introduce Monocular and Generalizable Gaussian Talking Head\nAnimation (MGGTalk), which requires monocular datasets and generalizes to\nunseen identities without personalized re-training. Compared with previous 3D\nGaussian Splatting (3DGS) methods that requires elusive multi-view datasets or\ntedious personalized learning/inference, MGGtalk enables more practical and\nbroader applications. However, in the absence of multi-view and personalized\ntraining data, the incompleteness of geometric and appearance information poses\na significant challenge. To address these challenges, MGGTalk explores depth\ninformation to enhance geometric and facial symmetry characteristics to\nsupplement both geometric and appearance features. Initially, based on the\npixel-wise geometric information obtained from depth estimation, we incorporate\nsymmetry operations and point cloud filtering techniques to ensure a complete\nand precise position parameter for 3DGS. Subsequently, we adopt a two-stage\nstrategy with symmetric priors for predicting the remaining 3DGS parameters. We\nbegin by predicting Gaussian parameters for the visible facial regions of the\nsource image. These parameters are subsequently utilized to improve the\nprediction of Gaussian parameters for the non-visible regions. Extensive\nexperiments demonstrate that MGGTalk surpasses previous state-of-the-art\nmethods, achieving superior performance across various metrics."}
{"id": "2504.00942", "pdf": "https://arxiv.org/pdf/2504.00942", "abs": "https://arxiv.org/abs/2504.00942", "authors": ["Anna Bavaresco", "Raquel FernÃ¡ndez"], "title": "Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?", "categories": ["cs.CL"], "comment": null, "summary": "A common assumption in Computational Linguistics is that text representations\nlearnt by multimodal models are richer and more human-like than those by\nlanguage-only models, as they are grounded in images or audio -- similar to how\nhuman language is grounded in real-world experiences. However, empirical\nstudies checking whether this is true are largely lacking. We address this gap\nby comparing word representations from contrastive multimodal models vs.\nlanguage-only ones in the extent to which they capture experiential information\n-- as defined by an existing norm-based 'experiential model' -- and align with\nhuman fMRI responses. Our results indicate that, surprisingly, language-only\nmodels are superior to multimodal ones in both respects. Additionally, they\nlearn more unique brain-relevant semantic information beyond that shared with\nthe experiential model. Overall, our study highlights the need to develop\ncomputational models that better integrate the complementary semantic\ninformation provided by multimodal data sources."}
{"id": "2504.00691", "pdf": "https://arxiv.org/pdf/2504.00691", "abs": "https://arxiv.org/abs/2504.00691", "authors": ["Yuanchen Wu", "Junlong Du", "Ke Yan", "Shouhong Ding", "Xiaoqiang Li"], "title": "ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "Vision-language (VL) learning requires extensive visual perception\ncapabilities, such as fine-grained object recognition and spatial perception.\nRecent works typically rely on training huge models on massive datasets to\ndevelop these capabilities. As a more efficient alternative, this paper\nproposes a new framework that Transfers the knowledge from a hub of Vision\nExperts (ToVE) for efficient VL learning, leveraging pre-trained vision expert\nmodels to promote visual perception capability. Specifically, building on a\nfrozen CLIP encoder that provides vision tokens for image-conditioned language\ngeneration, ToVE introduces a hub of multiple vision experts and a token-aware\ngating network that dynamically routes expert knowledge to vision tokens. In\nthe transfer phase, we propose a \"residual knowledge transfer\" strategy, which\nnot only preserves the generalizability of the vision tokens but also allows\ndetachment of low-contributing experts to improve inference efficiency.\nFurther, we explore to merge these expert knowledge to a single CLIP encoder,\ncreating a knowledge-merged CLIP that produces more informative vision tokens\nwithout expert inference during deployment. Experiment results across various\nVL tasks demonstrate that the proposed ToVE achieves competitive performance\nwith two orders of magnitude fewer training data."}
{"id": "2504.00970", "pdf": "https://arxiv.org/pdf/2504.00970", "abs": "https://arxiv.org/abs/2504.00970", "authors": ["Yuxuan Zhu", "Ali Falahati", "David H. Yang", "Mohammad Mohammadi Amiri"], "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."}
{"id": "2504.00753", "pdf": "https://arxiv.org/pdf/2504.00753", "abs": "https://arxiv.org/abs/2504.00753", "authors": ["Elyar Esmaeilzadeh", "Ehsan Garaaghaji", "Farzad Hallaji Azad", "Doruk Oner"], "title": "CAPE: Connectivity-Aware Path Enforcement Loss for Curvilinear Structure Delineation", "categories": ["cs.CV"], "comment": null, "summary": "Promoting the connectivity of curvilinear structures, such as neuronal\nprocesses in biomedical scans and blood vessels in CT images, remains a key\nchallenge in semantic segmentation. Traditional pixel-wise loss functions,\nincluding cross-entropy and Dice losses, often fail to capture high-level\ntopological connectivity, resulting in topological mistakes in graphs obtained\nfrom prediction maps. In this paper, we propose CAPE (Connectivity-Aware Path\nEnforcement), a novel loss function designed to enforce connectivity in graphs\nobtained from segmentation maps by optimizing a graph connectivity metric. CAPE\nuses the graph representation of the ground truth to select node pairs and\ndetermine their corresponding paths within the predicted segmentation through a\nshortest-path algorithm. Using this, we penalize both disconnections and false\npositive connections, effectively promoting the model to preserve topological\ncorrectness. Experiments on 2D and 3D datasets, including neuron and blood\nvessel tracing demonstrate that CAPE significantly improves topology-aware\nmetrics and outperforms state-of-the-art methods."}
{"id": "2504.00977", "pdf": "https://arxiv.org/pdf/2504.00977", "abs": "https://arxiv.org/abs/2504.00977", "authors": ["Mengyang Qiu", "Qingyu Gao", "Linxuan Yang", "Yang Gu", "Tran Minh Nguyen", "Zihao Huang", "Jungyeul Park"], "title": "Chinese Grammatical Error Correction: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Chinese Grammatical Error Correction (CGEC) is a critical task in Natural\nLanguage Processing, addressing the growing demand for automated writing\nassistance in both second-language (L2) and native (L1) Chinese writing. While\nL2 learners struggle with mastering complex grammatical structures, L1 users\nalso benefit from CGEC in academic, professional, and formal contexts where\nwriting precision is essential. This survey provides a comprehensive review of\nCGEC research, covering datasets, annotation schemes, evaluation methodologies,\nand system advancements. We examine widely used CGEC datasets, highlighting\ntheir characteristics, limitations, and the need for improved standardization.\nWe also analyze error annotation frameworks, discussing challenges such as word\nsegmentation ambiguity and the classification of Chinese-specific error types.\nFurthermore, we review evaluation metrics, focusing on their adaptation from\nEnglish GEC to Chinese, including character-level scoring and the use of\nmultiple references. In terms of system development, we trace the evolution\nfrom rule-based and statistical approaches to neural architectures, including\nTransformer-based models and the integration of large pre-trained language\nmodels. By consolidating existing research and identifying key challenges, this\nsurvey provides insights into the current state of CGEC and outlines future\ndirections, including refining annotation standards to address segmentation\nchallenges, and leveraging multilingual approaches to enhance CGEC."}
{"id": "2504.00759", "pdf": "https://arxiv.org/pdf/2504.00759", "abs": "https://arxiv.org/abs/2504.00759", "authors": ["Dehua Huo", "Weida Zhan", "Jinxin Guo", "Depeng Zhu", "Yu Chen", "YiChun Jiang", "Yueyi Han", "Deng Han", "Jin Li"], "title": "MSSFC-Net:Enhancing Building Interpretation with Multi-Scale Spatial-Spectral Feature Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "Building interpretation from remote sensing imagery primarily involves two\nfundamental tasks: building extraction and change detection. However, most\nexisting methods address these tasks independently, overlooking their inherent\ncorrelation and failing to exploit shared feature representations for mutual\nenhancement. Furthermore, the diverse spectral,spatial, and scale\ncharacteristics of buildings pose additional challenges in jointly modeling\nspatial-spectral multi-scale features and effectively balancing precision and\nrecall. The limited synergy between spatial and spectral representations often\nresults in reduced detection accuracy and incomplete change localization.To\naddress these challenges, we propose a Multi-Scale Spatial-Spectral Feature\nCooperative Dual-Task Network (MSSFC-Net) for joint building extraction and\nchange detection in remote sensing images. The framework integrates both tasks\nwithin a unified architecture, leveraging their complementary nature to\nsimultaneously extract building and change features. Specifically,a Dual-branch\nMulti-scale Feature Extraction module (DMFE) with Spatial-Spectral Feature\nCollaboration (SSFC) is designed to enhance multi-scale representation\nlearning, effectively capturing shallow texture details and deep semantic\ninformation, thus improving building extraction performance. For temporal\nfeature aggregation, we introduce a Multi-scale Differential Fusion Module\n(MDFM) that explicitly models the interaction between differential and\ndual-temporal features. This module refines the network's capability to detect\nlarge-area changes and subtle structural variations in buildings. Extensive\nexperiments conducted on three benchmark datasets demonstrate that MSSFC-Net\nachieves superior performance in both building extraction and change detection\ntasks, effectively improving detection accuracy while maintaining completeness."}
{"id": "2504.00993", "pdf": "https://arxiv.org/pdf/2504.00993", "abs": "https://arxiv.org/abs/2504.00993", "authors": ["Juncheng Wu", "Wenlong Deng", "Xingxuan Li", "Sheng Liu", "Taomian Mi", "Yifan Peng", "Ziyang Xu", "Yi Liu", "Hyunjin Cho", "Chang-In Choi", "Yihan Cao", "Hui Ren", "Xiang Li", "Xiaoxiao Li", "Yuyin Zhou"], "title": "MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical tasks such as diagnosis and treatment planning require precise and\ncomplex reasoning, particularly in life-critical domains. Unlike mathematical\nreasoning, medical reasoning demands meticulous, verifiable thought processes\nto ensure reliability and accuracy. However, there is a notable lack of\ndatasets that provide transparent, step-by-step reasoning to validate and\nenhance the medical reasoning ability of AI models. To bridge this gap, we\nintroduce MedReason, a large-scale high-quality medical reasoning dataset\ndesigned to enable faithful and explainable medical problem-solving in large\nlanguage models (LLMs). We utilize a structured medical knowledge graph (KG) to\nconvert clinical QA pairs into logical chains of reasoning, or ``thinking\npaths'', which trace connections from question elements to answers via relevant\nKG entities. Each path is validated for consistency with clinical logic and\nevidence-based medicine. Our pipeline generates detailed reasoning for various\nmedical questions from 7 medical datasets, resulting in a dataset of 32,682\nquestion-answer pairs, each with detailed, step-by-step explanations.\nExperiments demonstrate that fine-tuning with our dataset consistently boosts\nmedical problem-solving capabilities, achieving significant gains of up to 7.7%\nfor DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the\nHuatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the\nclinical benchmark MedBullets. We also engage medical professionals from\ndiverse specialties to assess our dataset's quality, ensuring MedReason offers\naccurate and coherent medical reasoning. Our data, models, and code will be\npublicly available."}
{"id": "2504.00763", "pdf": "https://arxiv.org/pdf/2504.00763", "abs": "https://arxiv.org/abs/2504.00763", "authors": ["Yunxuan Mao", "Rong Xiong", "Yue Wang", "Yiyi Liao"], "title": "UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Reconstructing and decomposing dynamic urban scenes is crucial for autonomous\ndriving, urban planning, and scene editing. However, existing methods fail to\nperform instance-aware decomposition without manual annotations, which is\ncrucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian\nSplatting (3DGS) based approach that decomposes a scene into a static\nbackground and individual dynamic instances using only RGB images and LiDAR\npoint clouds. At its core, we introduce 4D superpoints, a novel representation\nthat clusters multi-frame LiDAR points in 4D space, enabling unsupervised\ninstance separation based on spatiotemporal correlations. These 4D superpoints\nserve as the foundation for our decomposed 4D initialization, i.e., providing\nspatial and temporal initialization to train a dynamic 3DGS for arbitrary\ndynamic classes without requiring bounding boxes or object\ntemplates.Furthermore, we introduce a smoothness regularization strategy in\nboth 2D and 3D space, further improving the temporal stability.Experiments on\nbenchmark datasets show that our method outperforms existing methods in\ndecomposed dynamic scene reconstruction while enabling accurate and flexible\ninstance-level editing, making it a practical solution for real-world\napplications."}
{"id": "2504.01001", "pdf": "https://arxiv.org/pdf/2504.01001", "abs": "https://arxiv.org/abs/2504.01001", "authors": ["JosÃ© Pombal", "Nuno M. Guerreiro", "Ricardo Rei", "AndrÃ© F. T. Martins"], "title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As language models improve and become capable of performing more complex\ntasks across modalities, evaluating them automatically becomes increasingly\nchallenging. Developing strong and robust task-specific automatic metrics gets\nharder, and human-annotated test sets -- which are expensive to create --\nsaturate more quickly. A compelling alternative is to design reliable\nstrategies to automate the creation of test data and evaluation, but previous\nattempts either rely on pre-existing data, or focus solely on individual tasks.\nWe present Zero-shot Benchmarking (ZSB), a framework for creating high-quality\nbenchmarks for any task by leveraging language models for both synthetic test\ndata creation and evaluation. ZSB is simple and flexible: it requires only the\ncreation of a prompt for data generation and one for evaluation; it is scalable\nto tasks and languages where collecting real-world data is costly or\nimpractical; it is model-agnostic, allowing the creation of increasingly\nchallenging benchmarks as models improve. To assess the effectiveness of our\nframework, we create benchmarks for five text-only tasks and a multi-modal one:\ngeneral capabilities in four languages (English, Chinese, French, and Korean),\ntranslation, and general vision-language capabilities in English. We then rank\na broad range of open and closed systems on our benchmarks. ZSB rankings\nconsistently correlate strongly with human rankings, outperforming\nwidely-adopted standard benchmarks. Through ablations, we find that strong\nbenchmarks can be created with open models, and that judge model size and\ndataset variety are crucial drivers of performance. We release all our\nbenchmarks, and code to reproduce our experiments and to produce new\nbenchmarks."}
{"id": "2504.00773", "pdf": "https://arxiv.org/pdf/2504.00773", "abs": "https://arxiv.org/abs/2504.00773", "authors": ["Hyunwoo Park", "Gun Ryu", "Wonjun Kim"], "title": "DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in\nthe field of novel view synthesis due to its fast performance while yielding\nthe excellent image quality. However, 3DGS in sparse-view settings (e.g.,\nthree-view inputs) often faces with the problem of overfitting to training\nviews, which significantly drops the visual quality of novel view images. Many\nexisting approaches have tackled this issue by using strong priors, such as 2D\ngenerative contextual information and external depth signals. In contrast, this\npaper introduces a prior-free method, so-called DropGaussian, with simple\nchanges in 3D Gaussian splatting. Specifically, we randomly remove Gaussians\nduring the training process in a similar way of dropout, which allows\nnon-excluded Gaussians to have larger gradients while improving their\nvisibility. This makes the remaining Gaussians to contribute more to the\noptimization process for rendering with sparse input views. Such simple\noperation effectively alleviates the overfitting problem and enhances the\nquality of novel view synthesis. By simply applying DropGaussian to the\noriginal 3DGS framework, we can achieve the competitive performance with\nexisting prior-based 3DGS methods in sparse-view settings of benchmark datasets\nwithout any additional complexity. The code and model are publicly available\nat: https://github.com/DCVL-3D/DropGaussian release."}
{"id": "2504.01002", "pdf": "https://arxiv.org/pdf/2504.01002", "abs": "https://arxiv.org/abs/2504.01002", "authors": ["Michael Robinson", "Sourya Dey", "Tony Chiang"], "title": "Token embeddings violate the manifold hypothesis", "categories": ["cs.CL", "cs.AI", "53Z50, 62H15"], "comment": "20 pages, 10 figures", "summary": "To fully understand the behavior of a large language model (LLM) requires our\nunderstanding of its input space. If this input space differs from our\nassumption, our understanding of and conclusions about the LLM is likely\nflawed, regardless of its architecture. Here, we elucidate the structure of the\ntoken embeddings, the input domain for LLMs, both empirically and\ntheoretically. We present a generalized and statistically testable model where\nthe neighborhood of each token splits into well-defined signal and noise\ndimensions.\n  This model is based on a generalization of a manifold called a fiber bundle,\nso we denote our hypothesis test as the ``fiber bundle null.'' Failing to\nreject the null is uninformative, but rejecting it at a specific token\nindicates that token has a statistically significant local structure, and so is\nof interest to us. By running our test over several open-source LLMs, each with\nunique token embeddings, we find that the null is frequently rejected, and so\nthe token subspace is provably not a fiber bundle and hence also not a\nmanifold. As a consequence of our findings, when an LLM is presented with two\nsemantically equivalent prompts, and if one prompt contains a token implicated\nby our test, that prompt will likely exhibit more output variability\nproportional to the local signal dimension of the token."}
{"id": "2504.00784", "pdf": "https://arxiv.org/pdf/2504.00784", "abs": "https://arxiv.org/abs/2504.00784", "authors": ["Yang Yang", "Xijie Xu", "Yixun Zhou", "Jie Zheng"], "title": "CellVTA: Enhancing Vision Foundation Models for Accurate Cell Segmentation and Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Cell instance segmentation is a fundamental task in digital pathology with\nbroad clinical applications. Recently, vision foundation models, which are\npredominantly based on Vision Transformers (ViTs), have achieved remarkable\nsuccess in pathology image analysis. However, their improvements in cell\ninstance segmentation remain limited. A key challenge arises from the\ntokenization process in ViTs, which substantially reduces the spatial\nresolution of input images, leading to suboptimal segmentation quality,\nespecially for small and densely packed cells. To address this problem, we\npropose CellVTA (Cell Vision Transformer with Adapter), a novel method that\nimproves the performance of vision foundation models for cell instance\nsegmentation by incorporating a CNN-based adapter module. This adapter extracts\nhigh-resolution spatial information from input images and injects it into the\nViT through a cross-attention mechanism. Our method preserves the core\narchitecture of ViT, ensuring seamless integration with pretrained foundation\nmodels. Extensive experiments show that CellVTA achieves 0.538 mPQ on the CoNIC\ndataset and 0.506 mPQ on the PanNuke dataset, which significantly outperforms\nthe state-of-the-art cell segmentation methods. Ablation studies confirm the\nsuperiority of our approach over other fine-tuning strategies, including\ndecoder-only fine-tuning and full fine-tuning. Our code and models are publicly\navailable at https://github.com/JieZheng-ShanghaiTech/CellVTA."}
{"id": "2504.01005", "pdf": "https://arxiv.org/pdf/2504.01005", "abs": "https://arxiv.org/abs/2504.01005", "authors": ["Nishad Singhi", "Hritik Bansal", "Arian Hosseini", "Aditya Grover", "Kai-Wei Chang", "Marcus Rohrbach", "Anna Rohrbach"], "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "29 pages", "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling."}
{"id": "2504.00812", "pdf": "https://arxiv.org/pdf/2504.00812", "abs": "https://arxiv.org/abs/2504.00812", "authors": ["Yiqun Duan", "Sameera Ramasinghe", "Stephen Gould", "Ajanthan Thalaiyasingam"], "title": "Scaling Prompt Instructed Zero Shot Composed Image Retrieval with Image-Only Data", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Composed Image Retrieval (CIR) is the task of retrieving images matching a\nreference image augmented with a text, where the text describes changes to the\nreference image in natural language. Traditionally, models designed for CIR\nhave relied on triplet data containing a reference image, reformulation text,\nand a target image. However, curating such triplet data often necessitates\nhuman intervention, leading to prohibitive costs. This challenge has hindered\nthe scalability of CIR model training even with the availability of abundant\nunlabeled data. With the recent advances in foundational models, we advocate a\nshift in the CIR training paradigm where human annotations can be efficiently\nreplaced by large language models (LLMs). Specifically, we demonstrate the\ncapability of large captioning and language models in efficiently generating\ndata for CIR only relying on unannotated image collections. Additionally, we\nintroduce an embedding reformulation architecture that effectively combines\nimage and text modalities. Our model, named InstructCIR, outperforms\nstate-of-the-art methods in zero-shot composed image retrieval on CIRR and\nFashionIQ datasets. Furthermore, we demonstrate that by increasing the amount\nof generated data, our zero-shot model gets closer to the performance of\nsupervised baselines."}
{"id": "2504.01018", "pdf": "https://arxiv.org/pdf/2504.01018", "abs": "https://arxiv.org/abs/2504.01018", "authors": ["Di Wu", "Jia-Chen Gu", "Kai-Wei Chang", "Nanyun Peng"], "title": "Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Selective retrieval improves retrieval-augmented generation (RAG) by reducing\ndistractions from low-quality retrievals and improving efficiency. However,\nexisting approaches under-utilize the inherent knowledge of large language\nmodels (LLMs), leading to suboptimal retrieval decisions and degraded\ngeneration performance. To bridge this gap, we propose Self-Routing RAG\n(SR-RAG), a novel framework that binds selective retrieval with knowledge\nverbalization. SR-RAG enables an LLM to dynamically decide between external\nretrieval and verbalizing its own parametric knowledge. To this end, we design\na multi-task objective that jointly optimizes an LLM on knowledge source\nselection, knowledge verbalization, and response generation. We further\nintroduce dynamic knowledge source inference via nearest neighbor search to\nimprove the accuracy of knowledge source decision under domain shifts.\nFine-tuning three LLMs with SR-RAG significantly improves both their response\naccuracy and inference latency. Compared to the strongest selective retrieval\nbaseline, SR-RAG reduces retrievals by 29% while improving the performance by\n5.1%."}
{"id": "2504.00816", "pdf": "https://arxiv.org/pdf/2504.00816", "abs": "https://arxiv.org/abs/2504.00816", "authors": ["Yeqi Fang", "Rong Zhou"], "title": "The study of non-complete-ring positron emission tomography (PET) detection method", "categories": ["cs.CV", "physics.med-ph"], "comment": "18 pages, 14 pages", "summary": "Positron Emission Tomography (PET) is a vital molecular imaging tool widely\nused in medical diagnosis and treatment evaluation. Traditional PET systems\ntypically rely on complete detector rings to achieve full angular coverage for\nuniform and statistically robust sampling of coincidence events. However,\nincomplete-ring PET scanners have emerged in various scenarios due to hardware\nfailures, cost constraints, or specific clinical needs. In such cases,\nconventional reconstruction algorithms often suffer from performance\ndegradation due to reduced data completeness and geometric inconsistencies.\nThis thesis proposes a coarse-to-fine reconstruction framework for\nincomplete-ring PET scanners. The framework first employs an Attention U-Net\nmodel to recover complete sinograms from incomplete ones, then uses the OSEM\nalgorithm for preliminary reconstruction, and finally applies a two-stage\narchitecture comprising a Coarse Prediction Module (CPM) and an Iterative\nRefinement Module (IRM) for fine reconstruction. Our approach utilizes\nneighboring axial slices and spectral transform features as auxiliary guidance\nat the input level to ensure spatial and frequency domain consistency, and\nintegrates a contrastive diffusion strategy at the output level to improve\ncorrespondence between low-quality PET inputs and refined PET outputs.\nExperimental results on public and in-house brain PET datasets demonstrate that\nthe proposed method significantly outperforms existing approaches in metrics\nsuch as PSNR (35.6421 dB) and SSIM (0.9588), successfully preserving key\nanatomical structures and tracer distribution features, thus providing an\neffective solution for incomplete-ring PET imaging."}
{"id": "2504.00031", "pdf": "https://arxiv.org/pdf/2504.00031", "abs": "https://arxiv.org/abs/2504.00031", "authors": ["Ryan Marinelli", "Magnus Eckhoff"], "title": "Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "To effectively deploy Large Language Models (LLMs) in application-specific\nsettings, fine-tuning techniques are applied to enhance performance on\nspecialized tasks. This process often involves fine-tuning on user data data,\nwhich may contain sensitive information. Although not recommended, it is not\nuncommon for users to send passwords in messages, and fine-tuning models on\nthis could result in passwords being leaked. In this study, a Large Language\nModel is fine-tuned with customer support data and passwords from the RockYou\npassword wordlist using Low-Rank Adaptation (LoRA). Out of the first 200\npasswords from the list, 37 were successfully recovered. Further, causal\ntracing is used to identify that password information is largely located in a\nfew layers. Lastly, Rank One Model Editing (ROME) is used to remove the\npassword information from the model, resulting in the number of passwords\nrecovered going from 37 to 0."}
{"id": "2504.00844", "pdf": "https://arxiv.org/pdf/2504.00844", "abs": "https://arxiv.org/abs/2504.00844", "authors": ["Abdelrahman Elskhawy", "Mengze Li", "Nassir Navab", "Benjamin Busam"], "title": "PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In Scene Graphs Generation (SGG) one extracts structured representation from\nvisual inputs in the form of objects nodes and predicates connecting them. This\nfacilitates image-based understanding and reasoning for various downstream\ntasks. Although fully supervised SGG approaches showed steady performance\nimprovements, they suffer from a severe training bias. This is caused by the\navailability of only small subsets of curated data and exhibits long-tail\npredicate distribution issues with a lack of predicate diversity adversely\naffecting downstream tasks. To overcome this, we introduce PRISM-0, a framework\nfor zero-shot open-vocabulary SGG that bootstraps foundation models in a\nbottom-up approach to capture the whole spectrum of diverse, open-vocabulary\npredicate prediction. Detected object pairs are filtered and passed to a Vision\nLanguage Model (VLM) that generates descriptive captions. These are used to\nprompt an LLM to generate fine-andcoarse-grained predicates for the pair. The\npredicates are then validated using a VQA model to provide a final SGG. With\nthe modular and dataset-independent PRISM-0, we can enrich existing SG datasets\nsuch as Visual Genome (VG). Experiments illustrate that PRIMS-0 generates\nsemantically meaningful graphs that improve downstream tasks such as Image\nCaptioning and Sentence-to-Graph Retrieval with a performance on par to the\nbest fully supervised methods."}
{"id": "2504.00044", "pdf": "https://arxiv.org/pdf/2504.00044", "abs": "https://arxiv.org/abs/2504.00044", "authors": ["Riccardo Cantini", "Fabrizio Marozzo", "Alessio Orsino", "Domenico Talia", "Paolo Trunfio"], "title": "Dynamic hashtag recommendation in social media with trend shift detection and adaptation", "categories": ["cs.SI", "cs.CL", "cs.DC", "cs.NE"], "comment": null, "summary": "The widespread use of social media platforms results in the generation of\nvast amounts of user-generated content, which requires efficient methods for\ncategorization and search. Hashtag recommendation systems have emerged as a\ncrucial tool for automatically suggesting relevant hashtags and improving\ncontent discoverability. However, existing static models struggle to adapt to\nthe highly dynamic and real-time nature of social media conversations, where\nnew hashtags emerge and existing ones undergo semantic shifts. To address these\nchallenges, this paper presents H-ADAPTS (Hashtag recommendAtion by Detecting\nand adAPting to Trend Shifts), a BERT-based hashtag recommendation methodology\nthat can detect and adapt to shifts in the main trends and topics underlying\nsocial media conversation. Our approach introduces a trend-aware detection\nmechanism to identify changes in hashtag usage, triggering efficient model\nadaptation on a (small) set of recent posts. The framework leverages Apache\nStorm for real-time stream processing, enabling scalable and fault-tolerant\nanalysis of high-velocity social data. Experimental results on two real-world\ncase studies, including the COVID-19 pandemic and the 2020 US presidential\nelection, demonstrate the ability to maintain high recommendation accuracy by\nadapting to emerging trends. Our methodology significantly outperforms existing\nsolutions, ensuring timely and relevant hashtag recommendations in dynamic\nenvironments."}
{"id": "2504.00848", "pdf": "https://arxiv.org/pdf/2504.00848", "abs": "https://arxiv.org/abs/2504.00848", "authors": ["Yushan Zhang", "AljoÅ¡a OÅ¡ep", "Laura Leal-TaixÃ©", "Tim Meinhardt"], "title": "Zero-Shot 4D Lidar Panoptic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot 4D segmentation and recognition of arbitrary objects in Lidar is\ncrucial for embodied navigation, with applications ranging from streaming\nperception to semantic mapping and localization. However, the primary challenge\nin advancing research and developing generalized, versatile methods for\nspatio-temporal scene understanding in Lidar lies in the scarcity of datasets\nthat provide the necessary diversity and scale of annotations.To overcome these\nchallenges, we propose SAL-4D (Segment Anything in Lidar--4D), a method that\nutilizes multi-modal robotic sensor setups as a bridge to distill recent\ndevelopments in Video Object Segmentation (VOS) in conjunction with\noff-the-shelf Vision-Language foundation models to Lidar. We utilize VOS models\nto pseudo-label tracklets in short video sequences, annotate these tracklets\nwith sequence-level CLIP tokens, and lift them to the 4D Lidar space using\ncalibrated multi-modal sensory setups to distill them to our SAL-4D model. Due\nto temporal consistent predictions, we outperform prior art in 3D Zero-Shot\nLidar Panoptic Segmentation (LPS) over $5$ PQ, and unlock Zero-Shot 4D-LPS."}
{"id": "2504.00051", "pdf": "https://arxiv.org/pdf/2504.00051", "abs": "https://arxiv.org/abs/2504.00051", "authors": ["Sam Greydanus", "Zachary Wimpee"], "title": "The Cursive Transformer", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 8 figures", "summary": "Transformers trained on tokenized text, audio, and images can generate\nhigh-quality autoregressive samples. But handwriting data, represented as\nsequences of pen coordinates, remains underexplored. We introduce a novel\ntokenization scheme that converts pen stroke offsets to polar coordinates,\ndiscretizes them into bins, and then turns them into sequences of tokens with\nwhich to train a standard GPT model. This allows us to capture complex stroke\ndistributions without using any specialized architectures (eg. the mixture\ndensity network or the self-advancing ASCII attention head from Graves 2014).\nWith just 3,500 handwritten words and a few simple data augmentations, we are\nable to train a model that can generate realistic cursive handwriting. Our\napproach is simpler and more performant than previous RNN-based methods."}
{"id": "2504.00850", "pdf": "https://arxiv.org/pdf/2504.00850", "abs": "https://arxiv.org/abs/2504.00850", "authors": ["Zhuang Qi", "Runhui Zhang", "Lei Meng", "Wei Wu", "Yachong Zhang", "Xiangxu Meng"], "title": "Global Intervention and Distillation for Federated Out-of-Distribution Generalization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Attribute skew in federated learning leads local models to focus on learning\nnon-causal associations, guiding them towards inconsistent optimization\ndirections, which inevitably results in performance degradation and unstable\nconvergence. Existing methods typically leverage data augmentation to enhance\nsample diversity or employ knowledge distillation to learn invariant\nrepresentations. However, the instability in the quality of generated data and\nthe lack of domain information limit their performance on unseen samples. To\naddress these issues, this paper presents a global intervention and\ndistillation method, termed FedGID, which utilizes diverse attribute features\nfor backdoor adjustment to break the spurious association between background\nand label. It includes two main modules, where the global intervention module\nadaptively decouples objects and backgrounds in images, injects background\ninformation into random samples to intervene in the sample distribution, which\nlinks backgrounds to all categories to prevent the model from treating\nbackground-label associations as causal. The global distillation module\nleverages a unified knowledge base to guide the representation learning of\nclient models, preventing local models from overfitting to client-specific\nattributes. Experimental results on three datasets demonstrate that FedGID\nenhances the model's ability to focus on the main subjects in unseen data and\noutperforms existing methods in collaborative modeling."}
{"id": "2504.00125", "pdf": "https://arxiv.org/pdf/2504.00125", "abs": "https://arxiv.org/abs/2504.00125", "authors": ["Ahsan Bilal", "David Ebert", "Beiyu Lin"], "title": "LLMs for Explainable AI: A Comprehensive Survey", "categories": ["cs.AI", "cs.CL"], "comment": "This manuscript is intended for submission to ACM Transactions on\n  Intelligent Systems and Technology", "summary": "Large Language Models (LLMs) offer a promising approach to enhancing\nExplainable AI (XAI) by transforming complex machine learning outputs into\neasy-to-understand narratives, making model predictions more accessible to\nusers, and helping bridge the gap between sophisticated model behavior and\nhuman interpretability. AI models, such as state-of-the-art neural networks and\ndeep learning models, are often seen as \"black boxes\" due to a lack of\ntransparency. As users cannot fully understand how the models reach\nconclusions, users have difficulty trusting decisions from AI models, which\nleads to less effective decision-making processes, reduced accountabilities,\nand unclear potential biases. A challenge arises in developing explainable AI\n(XAI) models to gain users' trust and provide insights into how models generate\ntheir outputs. With the development of Large Language Models, we want to\nexplore the possibilities of using human language-based models, LLMs, for model\nexplainabilities. This survey provides a comprehensive overview of existing\napproaches regarding LLMs for XAI, and evaluation techniques for LLM-generated\nexplanation, discusses the corresponding challenges and limitations, and\nexamines real-world applications. Finally, we discuss future directions by\nemphasizing the need for more interpretable, automated, user-centric, and\nmultidisciplinary approaches for XAI via LLMs."}
{"id": "2504.00857", "pdf": "https://arxiv.org/pdf/2504.00857", "abs": "https://arxiv.org/abs/2504.00857", "authors": ["Mohammad Kassir", "Siba Haidar", "Antoun Yaacoub"], "title": "Exploring Personalized Federated Learning Architectures for Violence Detection in Surveillance Videos", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 5 figures, 4 tables", "summary": "The challenge of detecting violent incidents in urban surveillance systems is\ncompounded by the voluminous and diverse nature of video data. This paper\npresents a targeted approach using Personalized Federated Learning (PFL) to\naddress these issues, specifically employing the Federated Learning with\nPersonalization Layers method within the Flower framework. Our methodology\nadapts learning models to the unique data characteristics of each surveillance\nnode, effectively managing the heterogeneous and non-IID nature of surveillance\nvideo data. Through rigorous experiments conducted on balanced and imbalanced\ndatasets, our PFL models demonstrated enhanced accuracy and efficiency,\nachieving up to 99.3% accuracy. This study underscores the potential of PFL to\nsignificantly improve the scalability and effectiveness of surveillance\nsystems, offering a robust, privacy-preserving solution for violence detection\nin complex urban environments."}
{"id": "2504.00218", "pdf": "https://arxiv.org/pdf/2504.00218", "abs": "https://arxiv.org/abs/2504.00218", "authors": ["Rana Muhammad Shahroz Khan", "Zhen Tan", "Sukwon Yun", "Charles Flemming", "Tianlong Chen"], "title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Most discussions about Large Language Model (LLM) safety have focused on\nsingle-agent settings but multi-agent LLM systems now create novel adversarial\nrisks because their behavior depends on communication between agents and\ndecentralized reasoning. In this work, we innovatively focus on attacking\npragmatic systems that have constrains such as limited token bandwidth, latency\nbetween message delivery, and defense mechanisms. We design a\n$\\textit{permutation-invariant adversarial attack}$ that optimizes prompt\ndistribution across latency and bandwidth-constraint network topologies to\nbypass distributed safety mechanisms within the system. Formulating the attack\npath as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the\nnovel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage\ngraph-based optimization to maximize attack success rate while minimizing\ndetection risk. Evaluating across models including $\\texttt{Llama}$,\n$\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on\nvarious datasets like $\\texttt{JailBreakBench}$ and\n$\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up\nto $7\\times$, exposing critical vulnerabilities in multi-agent systems.\nMoreover, we demonstrate that existing defenses, including variants of\n$\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack,\nemphasizing the urgent need for multi-agent specific safety mechanisms."}
{"id": "2504.00859", "pdf": "https://arxiv.org/pdf/2504.00859", "abs": "https://arxiv.org/abs/2504.00859", "authors": ["Mahan Rafidashti", "Ji Lan", "Maryam Fatemi", "Junsheng Fu", "Lars Hammarstrand", "Lennart Svensson"], "title": "NeuRadar: Neural Radiance Fields for Automotive Radar Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Radar is an important sensor for autonomous driving (AD) systems due to its\nrobustness to adverse weather and different lighting conditions. Novel view\nsynthesis using neural radiance fields (NeRFs) has recently received\nconsiderable attention in AD due to its potential to enable efficient testing\nand validation but remains unexplored for radar point clouds. In this paper, we\npresent NeuRadar, a NeRF-based model that jointly generates radar point clouds,\ncamera images, and lidar point clouds. We explore set-based object detection\nmethods such as DETR, and propose an encoder-based solution grounded in the\nNeRF geometry for improved generalizability. We propose both a deterministic\nand a probabilistic point cloud representation to accurately model the radar\nbehavior, with the latter being able to capture radar's stochastic behavior. We\nachieve realistic reconstruction results for two automotive datasets,\nestablishing a baseline for NeRF-based radar point cloud simulation models. In\naddition, we release radar data for ZOD's Sequences and Drives to enable\nfurther research in this field. To encourage further development of radar\nNeRFs, we release the source code for NeuRadar."}
{"id": "2504.00254", "pdf": "https://arxiv.org/pdf/2504.00254", "abs": "https://arxiv.org/abs/2504.00254", "authors": ["Huandong Chang", "Zicheng Ma", "Mingyuan Ma", "Zhenting Qi", "Andrew Sabot", "Hong Jiang", "H. T. Kung"], "title": "ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique for\nfine-tuning large-scale pre-trained models with minimal parameter updates.\nHowever, existing methods rely on fixed ranks or focus solely on either rank\npruning or expansion, failing to adapt ranks dynamically to match the\nimportance of different layers during training. In this work, we propose\nElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and\nexpands ranks based on gradient-derived importance scores. To the best of our\nknowledge, ElaLoRA is the first method that enables both rank pruning and\nexpansion during fine-tuning. Experiments across multiple benchmarks\ndemonstrate that ElaLoRA consistently outperforms existing PEFT methods across\ndifferent parameter budgets. Furthermore, our studies validate that layers\nreceiving higher rank allocations contribute more significantly to model\nperformance, providing theoretical justification for our adaptive strategy. By\nintroducing a principled and adaptive rank allocation mechanism, ElaLoRA offers\na scalable and efficient fine-tuning solution, particularly suited for\nresource-constrained environments."}
{"id": "2504.00862", "pdf": "https://arxiv.org/pdf/2504.00862", "abs": "https://arxiv.org/abs/2504.00862", "authors": ["You Wang", "Zekun Li", "Lei Qi", "Qian Yu", "Yinghuan Shi", "Yang Gao"], "title": "Balancing Multi-Target Semi-Supervised Medical Image Segmentation with Collaborative Generalist and Specialists", "categories": ["cs.CV"], "comment": null, "summary": "Despite the promising performance achieved by current semi-supervised models\nin segmenting individual medical targets, many of these models suffer a notable\ndecrease in performance when tasked with the simultaneous segmentation of\nmultiple targets. A vital factor could be attributed to the imbalanced scales\namong different targets: during simultaneously segmenting multiple targets,\nlarge targets dominate the loss, leading to small targets being misclassified\nas larger ones. To this end, we propose a novel method, which consists of a\nCollaborative Generalist and several Specialists, termed CGS. It is centered\naround the idea of employing a specialist for each target class, thus avoiding\nthe dominance of larger targets. The generalist performs conventional\nmulti-target segmentation, while each specialist is dedicated to distinguishing\na specific target class from the remaining target classes and the background.\nBased on a theoretical insight, we demonstrate that CGS can achieve a more\nbalanced training. Moreover, we develop cross-consistency losses to foster\ncollaborative learning between the generalist and the specialists. Lastly,\nregarding their intrinsic relation that the target class of any specialized\nhead should belong to the remaining classes of the other heads, we introduce an\ninter-head error detection module to further enhance the quality of\npseudo-labels. Experimental results on three popular benchmarks showcase its\nsuperior performance compared to state-of-the-art methods."}
{"id": "2504.00294", "pdf": "https://arxiv.org/pdf/2504.00294", "abs": "https://arxiv.org/abs/2504.00294", "authors": ["Vidhisha Balachandran", "Jingya Chen", "Lingjiao Chen", "Shivam Garg", "Neel Joshi", "Yash Lara", "John Langford", "Besmira Nushi", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi"], "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2"], "comment": null, "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements."}
{"id": "2504.00870", "pdf": "https://arxiv.org/pdf/2504.00870", "abs": "https://arxiv.org/abs/2504.00870", "authors": ["Xiaohua Qi", "Renda Li", "Long Peng", "Qiang Ling", "Jun Yu", "Ziyi Chen", "Peng Chang", "Mei Han", "Jing Xiao"], "title": "Data-free Knowledge Distillation with Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Recently Data-Free Knowledge Distillation (DFKD) has garnered attention and\ncan transfer knowledge from a teacher neural network to a student neural\nnetwork without requiring any access to training data. Although diffusion\nmodels are adept at synthesizing high-fidelity photorealistic images across\nvarious domains, existing methods cannot be easiliy implemented to DFKD. To\nbridge that gap, this paper proposes a novel approach based on diffusion\nmodels, DiffDFKD. Specifically, DiffDFKD involves targeted optimizations in two\nkey areas. Firstly, DiffDFKD utilizes valuable information from teacher models\nto guide the pre-trained diffusion models' data synthesis, generating datasets\nthat mirror the training data distribution and effectively bridge domain gaps.\nSecondly, to reduce computational burdens, DiffDFKD introduces Latent CutMix\nAugmentation, an efficient technique, to enhance the diversity of diffusion\nmodel-generated images for DFKD while preserving key attributes for effective\nknowledge transfer. Extensive experiments validate the efficacy of DiffDFKD,\nyielding state-of-the-art results exceeding existing DFKD approaches. We\nrelease our code at https://github.com/xhqi0109/DiffDFKD."}
{"id": "2504.00487", "pdf": "https://arxiv.org/pdf/2504.00487", "abs": "https://arxiv.org/abs/2504.00487", "authors": ["Jie Ma", "Zhitao Gao", "Qi Chai", "Jun Liu", "Pinghui Wang", "Jing Tao", "Zhou Su"], "title": "FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning", "categories": ["cs.MM", "cs.CL", "cs.CV", "H.5.1; I.2.4"], "comment": "Under Review", "summary": "Audio-Visual Question Answering (AVQA) is a challenging multimodal reasoning\ntask requiring intelligent systems to answer natural language queries based on\npaired audio-video inputs accurately. However, existing AVQA approaches often\nsuffer from overfitting to dataset biases, leading to poor robustness.\nMoreover, current datasets may not effectively diagnose these methods. To\naddress these challenges, we first introduce a novel dataset, FortisAVQA,\nconstructed in two stages: (1) rephrasing questions in the test split of the\npublic MUSIC-AVQA dataset and (2) introducing distribution shifts across\nquestions. The first stage expands the test space with greater diversity, while\nthe second enables a refined robustness evaluation across rare, frequent, and\noverall question distributions. Second, we introduce a robust Multimodal\nAudio-Visual Epistemic Network (MAVEN) that leverages a multifaceted cycle\ncollaborative debiasing strategy to mitigate bias learning. Experimental\nresults demonstrate that our architecture achieves state-of-the-art performance\non FortisAVQA, with a notable improvement of 7.81\\%. Extensive ablation studies\non both datasets validate the effectiveness of our debiasing components.\nAdditionally, our evaluation reveals the limited robustness of existing\nmultimodal QA methods. We also verify the plug-and-play capability of our\nstrategy by integrating it with various baseline models across both datasets.\nOur dataset and code are available at https://github.com/reml-group/fortisavqa."}
{"id": "2504.00879", "pdf": "https://arxiv.org/pdf/2504.00879", "abs": "https://arxiv.org/abs/2504.00879", "authors": ["Fenglei Hao", "Yuliang Yang", "Ruiyuan Su", "Zhengran Zhao", "Yukun Qiao", "Mengyu Zhu"], "title": "WISE-TTT:Worldwide Information Segmentation Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Video multi-target segmentation remains a major challenge in long sequences,\nmainly due to the inherent limitations of existing architectures in capturing\nglobal temporal dependencies. We introduce WISE-TTT, a synergistic architecture\nintegrating Test-Time Training (TTT) mechanisms with the Transformer\narchitecture through co-design. The TTT layer systematically compresses\nhistorical temporal data to generate hidden states containing worldwide\ninformation(Lossless memory to maintain long contextual integrity), while\nachieving multi-stage contextual aggregation through splicing. Crucially, our\nframework provides the first empirical validation that implementing worldwide\ninformation across multiple network layers is essential for optimal dependency\nutilization.Ablation studies show TTT modules at high-level features boost\nglobal modeling. This translates to 3.1% accuracy improvement(J&F metric) on\nDavis2017 long-term benchmarks -- the first proof of hierarchical context\nsuperiority in video segmentation. We provide the first systematic evidence\nthat worldwide information critically impacts segmentation performance."}
{"id": "2504.00502", "pdf": "https://arxiv.org/pdf/2504.00502", "abs": "https://arxiv.org/abs/2504.00502", "authors": ["Qianhao Yuan", "Qingyu Zhang", "Yanjiang Liu", "Jiawei Chen", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun"], "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://github.com/icip-cas/ShortV", "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV"}
{"id": "2504.00883", "pdf": "https://arxiv.org/pdf/2504.00883", "abs": "https://arxiv.org/abs/2504.00883", "authors": ["Zhenyi Liao", "Qingsong Xie", "Yanhao Zhang", "Zijian Kong", "Haonan Lu", "Zhenyu Yang", "Zhijie Deng"], "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon."}
{"id": "2504.00509", "pdf": "https://arxiv.org/pdf/2504.00509", "abs": "https://arxiv.org/abs/2504.00509", "authors": ["Kai Yan", "Yufei Xu", "Zhengyin Du", "Xuesong Yao", "Zheyu Wang", "Xiaowen Guo", "Jiecao Chen"], "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages, 3 figures, 10 tables", "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs."}
{"id": "2504.00901", "pdf": "https://arxiv.org/pdf/2504.00901", "abs": "https://arxiv.org/abs/2504.00901", "authors": ["Enzhe Sun", "Yongchuan Cui", "Peng Liu", "Jining Yan"], "title": "A Decade of Deep Learning for Remote Sensing Spatiotemporal Fusion: Advances, Challenges, and Opportunities", "categories": ["cs.CV"], "comment": null, "summary": "Hardware limitations and satellite launch costs make direct acquisition of\nhigh temporal-spatial resolution remote sensing imagery challenging. Remote\nsensing spatiotemporal fusion (STF) technology addresses this problem by\nmerging high temporal but low spatial resolution imagery with high spatial but\nlow temporal resolution imagery to efficiently generate high spatiotemporal\nresolution satellite images. STF provides unprecedented observational\ncapabilities for land surface change monitoring, agricultural management, and\nenvironmental research. Deep learning (DL) methods have revolutionized the\nremote sensing spatiotemporal fusion field over the past decade through\npowerful automatic feature extraction and nonlinear modeling capabilities,\nsignificantly outperforming traditional methods in handling complex\nspatiotemporal data. Despite the rapid development of DL-based remote sensing\nSTF, the community lacks a systematic review of this quickly evolving field.\nThis paper comprehensively reviews DL developments in remote sensing STF over\nthe last decade, analyzing key research trends, method classifications,\ncommonly used datasets, and evaluation metrics. It discusses major challenges\nin existing research and identifies promising future research directions as\nreferences for researchers in this field to inspire new ideas. The specific\nmodels, datasets, and other information mentioned in this article have been\ncollected in:\nhttps://github.com/yc-cui/Deep-Learning-Spatiotemporal-Fusion-Survey."}
{"id": "2504.00532", "pdf": "https://arxiv.org/pdf/2504.00532", "abs": "https://arxiv.org/abs/2504.00532", "authors": ["Hongru Ma", "Yanjie Liang", "Jiasheng Si", "Weiyu Zhang", "Hongjiao Guan", "Chaoqun Zheng", "Bing Xu", "Wenpeng Lu"], "title": "SRLCG: Self-Rectified Large-Scale Code Generation with Multidimensional Chain-of-Thought and Dynamic Backtracking", "categories": ["cs.SE", "cs.CL"], "comment": "23 pages", "summary": "Large language models (LLMs) have revolutionized code generation,\nsignificantly enhancing developer productivity. However, for a vast number of\nusers with minimal coding knowledge, LLMs provide little support, as they\nprimarily generate isolated code snippets rather than complete, large-scale\nproject code. Without coding expertise, these users struggle to interpret,\nmodify, and iteratively refine the outputs of LLMs, making it impossible to\nassemble a complete project. To address this issue, we propose Self-Rectified\nLarge-Scale Code Generator (SRLCG), a framework that generates complete\nmulti-file project code from a single prompt. SRLCG employs a novel\nmultidimensional chain-of-thought (CoT) and self-rectification to guide LLMs in\ngenerating correct and robust code files, then integrates them into a complete\nand coherent project using our proposed dynamic backtracking algorithm.\nExperimental results show that SRLCG generates code 15x longer than\nDeepSeek-V3, 16x longer than GPT-4, and at least 10x longer than other leading\nCoT-based baselines. Furthermore, they confirm its improved correctness,\nrobustness, and performance compared to baselines in large-scale code\ngeneration."}
{"id": "2504.00908", "pdf": "https://arxiv.org/pdf/2504.00908", "abs": "https://arxiv.org/abs/2504.00908", "authors": ["Haoxuan Li", "Wei Song", "Aofan Liu", "Peiwu Qin"], "title": "DBF-UNet: A Two-Stage Framework for Carotid Artery Segmentation with Pseudo-Label Generation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image analysis faces significant challenges due to limited annotation\ndata, particularly in three-dimensional carotid artery segmentation tasks,\nwhere existing datasets exhibit spatially discontinuous slice annotations with\nonly a small portion of expert-labeled slices in complete 3D volumetric data.\nTo address this challenge, we propose a two-stage segmentation framework.\nFirst, we construct continuous vessel centerlines by interpolating between\nannotated slice centroids and propagate labels along these centerlines to\ngenerate interpolated annotations for unlabeled slices. The slices with expert\nannotations are used for fine-tuning SAM-Med2D, while the interpolated labels\non unlabeled slices serve as prompts to guide segmentation during inference. In\nthe second stage, we propose a novel Dense Bidirectional Feature Fusion UNet\n(DBF-UNet). This lightweight architecture achieves precise segmentation of\ncomplete 3D vascular structures. The network incorporates bidirectional feature\nfusion in the encoder and integrates multi-scale feature aggregation with dense\nconnectivity for effective feature reuse. Experimental validation on public\ndatasets demonstrates that our proposed method effectively addresses the sparse\nannotation challenge in carotid artery segmentation while achieving superior\nperformance compared to existing approaches. The source code is available at\nhttps://github.com/Haoxuanli-Thu/DBF-UNet."}
{"id": "2504.00587", "pdf": "https://arxiv.org/pdf/2504.00587", "abs": "https://arxiv.org/abs/2504.00587", "authors": ["Yingxuan Yang", "Huacan Chai", "Shuai Shao", "Yuanyi Song", "Siyuan Qi", "Renting Rui", "Weinan Zhang"], "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the\ndevelopment of multi-agent systems, where multiple LLM-based agents collaborate\nto solve complex tasks. However, existing systems predominantly rely on\ncentralized coordination, which introduces scalability bottlenecks, limits\nadaptability, and creates single points of failure. Additionally, concerns over\nprivacy and proprietary knowledge sharing hinder cross-organizational\ncollaboration, leading to siloed expertise. To address these challenges, we\npropose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based\nframework that enables LLM-based agents to autonomously evolve their\ncapabilities and collaborate efficiently in a Directed Acyclic Graph\n(DAG)-structured network. Unlike traditional multi-agent systems that depend on\nstatic role assignments or centralized control, AgentNet allows agents to\nspecialize dynamically, adjust their connectivity, and route tasks without\nrelying on predefined workflows. AgentNet's core design is built upon several\nkey innovations: (1) Fully Decentralized Paradigm: Removing the central\norchestrator, allowing agents to coordinate and specialize autonomously,\nfostering fault tolerance and emergent collective intelligence. (2) Dynamically\nEvolving Graph Topology: Real-time adaptation of agent connections based on\ntask demands, ensuring scalability and resilience.(3) Adaptive Learning for\nExpertise Refinement: A retrieval-based memory system that enables agents to\ncontinuously update and refine their specialized skills. By eliminating\ncentralized control, AgentNet enhances fault tolerance, promotes scalable\nspecialization, and enables privacy-preserving collaboration across\norganizations. Through decentralized coordination and minimal data exchange,\nagents can leverage diverse knowledge sources while safeguarding sensitive\ninformation."}
{"id": "2504.00939", "pdf": "https://arxiv.org/pdf/2504.00939", "abs": "https://arxiv.org/abs/2504.00939", "authors": ["Alexander Martin", "Reno Kriz", "William Gantt Walden", "Kate Sanders", "Hannah Recknor", "Eugene Yang", "Francis Ferraro", "Benjamin Van Durme"], "title": "WikiVideo: Article Generation from Multiple Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Repo can be found here: https://github.com/alexmartin1722/wikivideo", "summary": "We present the challenging task of automatically creating a high-level\nWikipedia-style article that aggregates information from multiple diverse\nvideos about real-world events, such as natural disasters or political\nelections. Videos are intuitive sources for retrieval-augmented generation\n(RAG), but most contemporary RAG workflows focus heavily on text and existing\nmethods for video-based summarization focus on low-level scene understanding\nrather than high-level event semantics. To close this gap, we introduce\nWikiVideo, a benchmark consisting of expert-written articles and densely\nannotated videos that provide evidence for articles' claims, facilitating the\nintegration of video into RAG pipelines and enabling the creation of in-depth\ncontent that is grounded in multimodal sources. We further propose\nCollaborative Article Generation (CAG), a novel interactive method for article\ncreation from multiple videos. CAG leverages an iterative interaction between\nan r1-style reasoning model and a VideoLLM to draw higher level inferences\nabout the target event than is possible with VideoLLMs alone, which fixate on\nlow-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in\nboth oracle retrieval and RAG settings and find that CAG consistently\noutperforms alternative methods, while suggesting intriguing avenues for future\nwork."}
{"id": "2504.00767", "pdf": "https://arxiv.org/pdf/2504.00767", "abs": "https://arxiv.org/abs/2504.00767", "authors": ["Pegah Rahimian", "Jernej Flisar", "David Sumpter"], "title": "Automated Explanation of Machine Learning Models of Footballing Actions in Words", "categories": ["cs.LG", "cs.CL", "cs.HC"], "comment": null, "summary": "While football analytics has changed the way teams and analysts assess\nperformance, there remains a communication gap between machine learning\npractice and how coaching staff talk about football. Coaches and practitioners\nrequire actionable insights, which are not always provided by models. To bridge\nthis gap, we show how to build wordalizations (a novel approach that leverages\nlarge language models) for shots in football. Specifically, we first build an\nexpected goals model using logistic regression. We then use the co-efficients\nof this regression model to write sentences describing how factors (such as\ndistance, angle and defensive pressure) contribute to the model's prediction.\nFinally, we use large language models to give an entertaining description of\nthe shot. We describe our approach in a model card and provide an interactive\nopen-source application describing shots in recent tournaments. We discuss how\nshot wordalisations might aid communication in coaching and football\ncommentary, and give a further example of how the same approach can be applied\nto other actions in football."}
{"id": "2504.00943", "pdf": "https://arxiv.org/pdf/2504.00943", "abs": "https://arxiv.org/abs/2504.00943", "authors": ["Snigdha Agarwal", "Ganaraja V H", "Neelam Sinha", "Abhilasha Indoria", "Netravathi M", "Jitender Saini"], "title": "Graph Classification and Radiomics Signature for Identification of Tuberculous Meningitis", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 6 figures, 3 tables", "summary": "Introduction: Tuberculous meningitis (TBM) is a serious brain infection\ncaused by Mycobacterium tuberculosis, characterized by inflammation of the\nmeninges covering the brain and spinal cord. Diagnosis often requires invasive\nlumbar puncture (LP) and cerebrospinal fluid (CSF) analysis. Objectives: This\nstudy aims to classify TBM patients using T1-weighted (T1w) non-contrast\nMagnetic Resonance Imaging (MRI) scans. We hypothesize that specific brain\nregions, such as the interpeduncular cisterns, bone, and corpus callosum,\ncontain visual markers that can non-invasively distinguish TBM patients from\nhealthy controls. We propose a novel Pixel-array Graphs Classifier\n(PAG-Classifier) that leverages spatial relationships between neighbouring 3D\npixels in a graph-based framework to extract significant features through eigen\ndecomposition. These features are then used to train machine learning\nclassifiers for effective patient classification. We validate our approach\nusing a radiomics-based methodology, classifying TBM patients based on relevant\nradiomics features. Results: We utilized an internal dataset consisting of 52\nscans, 32 from confirmed TBM patients based on mycobacteria detection in CSF,\nand 20 from healthy individuals. We achieved a 5-fold cross-validated average\nF1 score of 85.71% for cistern regions with our PAG-Classifier and 92.85% with\nthe radiomics features classifier, surpassing current state-of-the-art\nbenchmarks by 15% and 22%, respectively. However, bone and corpus callosum\nregions showed poor classification effectiveness, with average F1 scores below\n50%. Conclusion: Our study suggests that algorithms like the PAG-Classifier\nserve as effective tools for non-invasive TBM analysis, particularly by\ntargeting the interpeduncular cistern. Findings indicate that the bone and\ncorpus callosum regions lack distinctive patterns for differentiation."}
{"id": "2504.00882", "pdf": "https://arxiv.org/pdf/2504.00882", "abs": "https://arxiv.org/abs/2504.00882", "authors": ["Wei Zhou", "Yuyang Gao", "Xuanhe Zhou", "Guoliang Li"], "title": "CrackSQL: A Hybrid SQL Dialect Translation System Powered by Large Language Models", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Extension of our SIGMOD 2025 paper. Please refer to source code\n  available at: https://github.com/weAIDB/CrackSQL", "summary": "Dialect translation plays a key role in enabling seamless interaction across\nheterogeneous database systems. However, translating SQL queries between\ndifferent dialects (e.g., from PostgreSQL to MySQL) remains a challenging task\ndue to syntactic discrepancies and subtle semantic variations. Existing\napproaches including manual rewriting, rule-based systems, and large language\nmodel (LLM)-based techniques often involve high maintenance effort (e.g.,\ncrafting custom translation rules) or produce unreliable results (e.g., LLM\ngenerates non-existent functions), especially when handling complex queries. In\nthis demonstration, we present CrackSQL, the first hybrid SQL dialect\ntranslation system that combines rule and LLM-based methods to overcome these\nlimitations. CrackSQL leverages the adaptability of LLMs to minimize manual\nintervention, while enhancing translation accuracy by segmenting lengthy\ncomplex SQL via functionality-based query processing. To further improve\nrobustness, it incorporates a novel cross-dialect syntax embedding model for\nprecise syntax alignment, as well as an adaptive local-to-global translation\nstrategy that effectively resolves interdependent query operations. CrackSQL\nsupports three translation modes and offers multiple deployment and access\noptions including a web console interface, a PyPI package, and a command-line\nprompt, facilitating adoption across a variety of real-world use cases"}
{"id": "2504.00946", "pdf": "https://arxiv.org/pdf/2504.00946", "abs": "https://arxiv.org/abs/2504.00946", "authors": ["Tianqi Ding", "Dawei Xiang", "Keith E Schubert", "Liang Dong"], "title": "GKAN: Explainable Diagnosis of Alzheimer's Disease Using Graph Neural Network with Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": "12 pages, 4 figures, under review of The Southwest Data Science\n  Conference (SDSC 2025)", "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nposes significant diagnostic challenges due to its complex etiology. Graph\nConvolutional Networks (GCNs) have shown promise in modeling brain connectivity\nfor AD diagnosis, yet their reliance on linear transformations limits their\nability to capture intricate nonlinear patterns in neuroimaging data. To\naddress this, we propose GCN-KAN, a novel single-modal framework that\nintegrates Kolmogorov-Arnold Networks (KAN) into GCNs to enhance both\ndiagnostic accuracy and interpretability. Leveraging structural MRI data, our\nmodel employs learnable spline-based transformations to better represent brain\nregion interactions. Evaluated on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset, GCN-KAN outperforms traditional GCNs by 4-8% in\nclassification accuracy while providing interpretable insights into key brain\nregions associated with AD. This approach offers a robust and explainable tool\nfor early AD diagnosis."}
{"id": "2504.00906", "pdf": "https://arxiv.org/pdf/2504.00906", "abs": "https://arxiv.org/abs/2504.00906", "authors": ["Saaket Agashe", "Kyle Wong", "Vincent Tu", "Jiachen Yang", "Ang Li", "Xin Eric Wang"], "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "18 pages, 13 figures, 8 tables", "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S."}
{"id": "2504.00950", "pdf": "https://arxiv.org/pdf/2504.00950", "abs": "https://arxiv.org/abs/2504.00950", "authors": ["Tianqi Ding", "Dawei Xiang", "Pablo Rivas", "Liang Dong"], "title": "Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration", "categories": ["cs.CV"], "comment": "12 pages, 4 figures, accepted by International Conference on the AI\n  Revolution: Research, Ethics, and Society (AIR-RES 2025)", "summary": "Neural Radiance Fields (NeRF) have become a popular 3D reconstruction\napproach in recent years. While they produce high-quality results, they also\ndemand lengthy training times, often spanning days. This paper studies neural\npruning as a strategy to address these concerns. We compare pruning approaches,\nincluding uniform sampling, importance-based methods, and coreset-based\ntechniques, to reduce the model size and speed up training. Our findings show\nthat coreset-driven pruning can achieve a 50% reduction in model size and a 35%\nspeedup in training, with only a slight decrease in accuracy. These results\nsuggest that pruning can be an effective method for improving the efficiency of\nNeRF models in resource-limited settings."}
{"id": "2504.00939", "pdf": "https://arxiv.org/pdf/2504.00939", "abs": "https://arxiv.org/abs/2504.00939", "authors": ["Alexander Martin", "Reno Kriz", "William Gantt Walden", "Kate Sanders", "Hannah Recknor", "Eugene Yang", "Francis Ferraro", "Benjamin Van Durme"], "title": "WikiVideo: Article Generation from Multiple Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Repo can be found here: https://github.com/alexmartin1722/wikivideo", "summary": "We present the challenging task of automatically creating a high-level\nWikipedia-style article that aggregates information from multiple diverse\nvideos about real-world events, such as natural disasters or political\nelections. Videos are intuitive sources for retrieval-augmented generation\n(RAG), but most contemporary RAG workflows focus heavily on text and existing\nmethods for video-based summarization focus on low-level scene understanding\nrather than high-level event semantics. To close this gap, we introduce\nWikiVideo, a benchmark consisting of expert-written articles and densely\nannotated videos that provide evidence for articles' claims, facilitating the\nintegration of video into RAG pipelines and enabling the creation of in-depth\ncontent that is grounded in multimodal sources. We further propose\nCollaborative Article Generation (CAG), a novel interactive method for article\ncreation from multiple videos. CAG leverages an iterative interaction between\nan r1-style reasoning model and a VideoLLM to draw higher level inferences\nabout the target event than is possible with VideoLLMs alone, which fixate on\nlow-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in\nboth oracle retrieval and RAG settings and find that CAG consistently\noutperforms alternative methods, while suggesting intriguing avenues for future\nwork."}
{"id": "2504.00954", "pdf": "https://arxiv.org/pdf/2504.00954", "abs": "https://arxiv.org/abs/2504.00954", "authors": ["Bangwei Liu", "Yicheng Bao", "Shaohui Lin", "Xuhong Wang", "Xin Tan", "Yingchun Wang", "Yuan Xie", "Chaochao Lu"], "title": "IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal retrieval systems are becoming increasingly vital for cutting-edge\nAI technologies, such as embodied AI and AI-driven digital content industries.\nHowever, current multimodal retrieval tasks lack sufficient complexity and\ndemonstrate limited practical application value. It spires us to design\nInstance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires\nmodels to retrieve images containing the same instance as a query image while\nmatching a text-described scenario. Unlike existing retrieval tasks focused on\nglobal image similarity or category-level matching, IDMR demands fine-grained\ninstance-level consistency across diverse contexts. To benchmark this\ncapability, we develop IDMR-bench using real-world object tracking and\nfirst-person video data. Addressing the scarcity of training data, we propose a\ncross-domain synthesis method that creates 557K training samples by cropping\nobjects from standard detection datasets. Our Multimodal Large Language Model\n(MLLM) based retrieval model, trained on 1.2M samples, outperforms\nstate-of-the-art approaches on both traditional benchmarks and our zero-shot\nIDMR-bench. Experimental results demonstrate previous models' limitations in\ninstance-aware retrieval and highlight the potential of MLLM for advanced\nretrieval applications. The whole training dataset, codes and models, with wide\nranges of sizes, are available at https://github.com/BwLiu01/IDMR."}
{"id": "2504.00979", "pdf": "https://arxiv.org/pdf/2504.00979", "abs": "https://arxiv.org/abs/2504.00979", "authors": ["Anders Blilie", "Nita Mulliqi", "Xiaoyi Ji", "Kelvin Szolnoky", "Sol Erika Boman", "Matteo Titus", "Geraldine Martinez Gonzalez", "JosÃ© Asenjo", "Marcello Gambacorta", "Paolo Libretti", "Einar Gudlaugsson", "Svein R. Kjosavik", "Lars Egevad", "Emiel A. M. Janssen", "Martin Eklund", "Kimmo Kartasalo"], "title": "Artificial Intelligence-Assisted Prostate Cancer Diagnosis for Reduced Use of Immunohistochemistry", "categories": ["cs.CV"], "comment": "29 pages, 5 figures and 3 tables", "summary": "Prostate cancer diagnosis heavily relies on histopathological evaluation,\nwhich is subject to variability. While immunohistochemical staining (IHC)\nassists in distinguishing benign from malignant tissue, it involves increased\nwork, higher costs, and diagnostic delays. Artificial intelligence (AI)\npresents a promising solution to reduce reliance on IHC by accurately\nclassifying atypical glands and borderline morphologies in hematoxylin & eosin\n(H&E) stained tissue sections. In this study, we evaluated an AI model's\nability to minimize IHC use without compromising diagnostic accuracy by\nretrospectively analyzing prostate core needle biopsies from routine\ndiagnostics at three different pathology sites. These cohorts were composed\nexclusively of difficult cases where the diagnosing pathologists required IHC\nto finalize the diagnosis. The AI model demonstrated area under the curve\nvalues of 0.951-0.993 for detecting cancer in routine H&E-stained slides.\nApplying sensitivity-prioritized diagnostic thresholds reduced the need for IHC\nstaining by 44.4%, 42.0%, and 20.7% in the three cohorts investigated, without\na single false negative prediction. This AI model shows potential for\noptimizing IHC use, streamlining decision-making in prostate pathology, and\nalleviating resource burdens."}
{"id": "2504.00992", "pdf": "https://arxiv.org/pdf/2504.00992", "abs": "https://arxiv.org/abs/2504.00992", "authors": ["Elisabetta Fedele", "Boyang Sun", "Leonidas Guibas", "Marc Pollefeys", "Francis Engelmann"], "title": "SuperDec: 3D Scene Decomposition with Superquadric Primitives", "categories": ["cs.CV"], "comment": null, "summary": "We present SuperDec, an approach for creating compact 3D scene\nrepresentations via decomposition into superquadric primitives. While most\nrecent works leverage geometric primitives to obtain photorealistic 3D scene\nrepresentations, we propose to leverage them to obtain a compact yet expressive\nrepresentation. We propose to solve the problem locally on individual objects\nand leverage the capabilities of instance segmentation methods to scale our\nsolution to full 3D scenes. In doing that, we design a new architecture which\nefficiently decompose point clouds of arbitrary objects in a compact set of\nsuperquadrics. We train our architecture on ShapeNet and we prove its\ngeneralization capabilities on object instances extracted from the ScanNet++\ndataset as well as on full Replica scenes. Finally, we show how a compact\nrepresentation based on superquadrics can be useful for a diverse range of\ndownstream applications, including robotic tasks and controllable visual\ncontent generation and editing."}
{"id": "2504.00996", "pdf": "https://arxiv.org/pdf/2504.00996", "abs": "https://arxiv.org/abs/2504.00996", "authors": ["Liangbin Xie", "Daniil Pakhomov", "Zhonghao Wang", "Zongze Wu", "Ziyan Chen", "Yuqian Zhou", "Haitian Zheng", "Zhifei Zhang", "Zhe Lin", "Jiantao Zhou", "Chao Dong"], "title": "TurboFill: Adapting Few-step Text-to-image Model for Fast Image Inpainting", "categories": ["cs.CV"], "comment": "Project webpage available at\n  https://liangbinxie.github.io/projects/TurboFill/", "summary": "This paper introduces TurboFill, a fast image inpainting model that enhances\na few-step text-to-image diffusion model with an inpainting adapter for\nhigh-quality and efficient inpainting. While standard diffusion models generate\nhigh-quality results, they incur high computational costs. We overcome this by\ntraining an inpainting adapter on a few-step distilled text-to-image model,\nDMD2, using a novel 3-step adversarial training scheme to ensure realistic,\nstructurally consistent, and visually harmonious inpainted regions. To evaluate\nTurboFill, we propose two benchmarks: DilationBench, which tests performance\nacross mask sizes, and HumanBench, based on human feedback for complex prompts.\nExperiments show that TurboFill outperforms both multi-step BrushNet and\nfew-step inpainting methods, setting a new benchmark for high-performance\ninpainting tasks. Our project page:\nhttps://liangbinxie.github.io/projects/TurboFill/"}
{"id": "2504.00999", "pdf": "https://arxiv.org/pdf/2504.00999", "abs": "https://arxiv.org/abs/2504.00999", "authors": ["Siyuan Li", "Luyuan Zhang", "Zedong Wang", "Juanxi Tian", "Cheng Tan", "Zicheng Liu", "Chang Yu", "Qingsong Xie", "Haonan Lu", "Haoqian Wang", "Zhen Lei"], "title": "MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR2025 (in process for more analysis and extension)", "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."}
{"id": "2504.01004", "pdf": "https://arxiv.org/pdf/2504.01004", "abs": "https://arxiv.org/abs/2504.01004", "authors": ["Yujian Xiong", "Xuanzhao Dong", "Sebastian Waz", "Wenhui Zhu", "Negar Mallak", "Zhong-lin Lu", "Yalin Wang"], "title": "Enhancing 3T BOLD fMRI SNR using Unpaired 7T Data with SchrÃ¶dinger Bridge Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "High spatial and temporal resolution, coupled with a strong signal-to-noise\nratio (SNR), has made BOLD 7 Tesla fMRI an invaluable tool for understanding\nhow the brain processes visual stimuli. However, the limited availability of 7T\nMRI systems means that most research relies on 3T MRI systems, which offer\nlower spatial and temporal resolution and SNR. This naturally raises the\nquestion: Can we enhance the spatiotemporal resolution and SNR of 3T BOLD fMRI\ndata to approximate 7T quality? In this study, we propose a novel framework\nthat aligns 7T and 3T fMRI data from different subjects and datasets in a\nshared parametric domain. We then apply an unpaired Brain Disk Schr\\\"odinger\nBridge diffusion model to enhance the spatiotemporal resolution and SNR of the\n3T data. Our approach addresses the challenge of limited 7T data by improving\nthe 3T scan quality. We demonstrate its effectiveness by testing it on two\ndistinct fMRI retinotopy datasets (one 7T and one 3T), as well as synthetic\ndata. The results show that our method significantly improves the SNR and\ngoodness-of-fit of the population receptive field (pRF) model in the enhanced\n3T data, making it comparable to 7T quality. The codes will be available at\nGithub."}
{"id": "2504.01008", "pdf": "https://arxiv.org/pdf/2504.01008", "abs": "https://arxiv.org/abs/2504.01008", "authors": ["Peter Kocsis", "Lukas HÃ¶llein", "Matthias NieÃner"], "title": "IntrinsiX: High-Quality PBR Generation using Image Priors", "categories": ["cs.CV", "cs.AI", "I.4.8; I.4.9; I.2.10"], "comment": "Project page: https://peter-kocsis.github.io/IntrinsiX/ Video:\n  https://youtu.be/b0wVA44R93Y", "summary": "We introduce IntrinsiX, a novel method that generates high-quality intrinsic\nimages from text description. In contrast to existing text-to-image models\nwhose outputs contain baked-in scene lighting, our approach predicts\nphysically-based rendering (PBR) maps. This enables the generated outputs to be\nused for content creation scenarios in core graphics applications that\nfacilitate re-lighting, editing, and texture generation tasks. In order to\ntrain our generator, we exploit strong image priors, and pre-train separate\nmodels for each PBR material component (albedo, roughness, metallic, normals).\nWe then align these models with a new cross-intrinsic attention formulation\nthat concatenates key and value features in a consistent fashion. This allows\nus to exchange information between each output modality and to obtain\nsemantically coherent PBR predictions. To ground each intrinsic component, we\npropose a rendering loss which provides image-space signals to constrain the\nmodel, thus facilitating sharp details also in the output BRDF properties. Our\nresults demonstrate detailed intrinsic generation with strong generalization\ncapabilities that outperforms existing intrinsic image decomposition methods\nused with generated images by a significant margin. Finally, we show a series\nof applications, including re-lighting, editing, and text-conditioned\nroom-scale PBR texture generation."}
{"id": "2504.01009", "pdf": "https://arxiv.org/pdf/2504.01009", "abs": "https://arxiv.org/abs/2504.01009", "authors": ["Saarthak Kapse", "Pushpak Pati", "Srikar Yellapragada", "Srijan Das", "Rajarsi R. Gupta", "Joel Saltz", "Dimitris Samaras", "Prateek Prasanna"], "title": "GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology", "categories": ["cs.CV"], "comment": null, "summary": "Pretraining a Multiple Instance Learning (MIL) aggregator enables the\nderivation of Whole Slide Image (WSI)-level embeddings from patch-level\nrepresentations without supervision. While recent multimodal MIL pretraining\napproaches leveraging auxiliary modalities have demonstrated performance gains\nover unimodal WSI pretraining, the acquisition of these additional modalities\nnecessitates extensive clinical profiling. This requirement increases costs and\nlimits scalability in existing WSI datasets lacking such paired modalities. To\naddress this, we propose Gigapixel Vision-Concept Knowledge Contrastive\npretraining (GECKO), which aligns WSIs with a Concept Prior derived from the\navailable WSIs. First, we derive an inherently interpretable concept prior by\ncomputing the similarity between each WSI patch and textual descriptions of\npredefined pathology concepts. GECKO then employs a dual-branch MIL network:\none branch aggregates patch embeddings into a WSI-level deep embedding, while\nthe other aggregates the concept prior into a corresponding WSI-level concept\nembedding. Both aggregated embeddings are aligned using a contrastive\nobjective, thereby pretraining the entire dual-branch MIL model. Moreover, when\nauxiliary modalities such as transcriptomics data are available, GECKO\nseamlessly integrates them. Across five diverse tasks, GECKO consistently\noutperforms prior unimodal and multimodal pretraining approaches while also\ndelivering clinically meaningful interpretability that bridges the gap between\ncomputational models and pathology expertise. Code is made available at\nhttps://github.com/bmi-imaginelab/GECKO"}
{"id": "2504.01010", "pdf": "https://arxiv.org/pdf/2504.01010", "abs": "https://arxiv.org/abs/2504.01010", "authors": ["Dylan Lester", "James Gao", "Samuel Sutphin", "Pingping Zhu", "Husnu Narman", "Ammar Alzarrad"], "title": "A YOLO-Based Semi-Automated Labeling Approach to Improve Fault Detection Efficiency in Railroad Videos", "categories": ["cs.CV", "eess.IV"], "comment": "Published on American Society of Engineering Education (ASEE) North\n  Central Section Conference, 2025", "summary": "Manual labeling for large-scale image and video datasets is often\ntime-intensive, error-prone, and costly, posing a significant barrier to\nefficient machine learning workflows in fault detection from railroad videos.\nThis study introduces a semi-automated labeling method that utilizes a\npre-trained You Only Look Once (YOLO) model to streamline the labeling process\nand enhance fault detection accuracy in railroad videos. By initiating the\nprocess with a small set of manually labeled data, our approach iteratively\ntrains the YOLO model, using each cycle's output to improve model accuracy and\nprogressively reduce the need for human intervention.\n  To facilitate easy correction of model predictions, we developed a system to\nexport YOLO's detection data as an editable text file, enabling rapid\nadjustments when detections require refinement. This approach decreases\nlabeling time from an average of 2 to 4 minutes per image to 30 seconds to 2\nminutes, effectively minimizing labor costs and labeling errors. Unlike costly\nAI based labeling solutions on paid platforms, our method provides a\ncost-effective alternative for researchers and practitioners handling large\ndatasets in fault detection and other detection based machine learning\napplications."}
{"id": "2504.01014", "pdf": "https://arxiv.org/pdf/2504.01014", "abs": "https://arxiv.org/abs/2504.01014", "authors": ["Junhao Cheng", "Yuying Ge", "Yixiao Ge", "Jing Liao", "Ying Shan"], "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction", "categories": ["cs.CV"], "comment": "Project released at: https://howe125.github.io/AnimeGamer.github.io/", "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer."}
{"id": "2504.01017", "pdf": "https://arxiv.org/pdf/2504.01017", "abs": "https://arxiv.org/abs/2504.01017", "authors": ["David Fan", "Shengbang Tong", "Jiachen Zhu", "Koustuv Sinha", "Zhuang Liu", "Xinlei Chen", "Michael Rabbat", "Nicolas Ballas", "Yann LeCun", "Amir Bar", "Saining Xie"], "title": "Scaling Language-Free Visual Representation Learning", "categories": ["cs.CV"], "comment": "Project page at https://davidfan.io/webssl/", "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL and CLIP\nmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behind CLIP due to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL and CLIP models on the same MetaCLIP data,\nand leveraging VQA as a diverse testbed for vision encoders. In this controlled\nsetup, visual SSL models scale better than CLIP models in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieve\nCLIP-level performance on a wide range of VQA and classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning."}
{"id": "2504.01019", "pdf": "https://arxiv.org/pdf/2504.01019", "abs": "https://arxiv.org/abs/2504.01019", "authors": ["Pablo Ruiz-Ponce", "German Barquero", "Cristina Palmero", "Sergio Escalera", "JosÃ© GarcÃ­a-RodrÃ­guez"], "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted - Project Page:\n  https://pabloruizponce.com/papers/MixerMDM", "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix."}
{"id": "2504.01020", "pdf": "https://arxiv.org/pdf/2504.01020", "abs": "https://arxiv.org/abs/2504.01020", "authors": ["Junyu Xie", "Tengda Han", "Max Bain", "Arsha Nagrani", "Eshika Khandelwal", "GÃ¼l Varol", "Weidi Xie", "Andrew Zisserman"], "title": "Shot-by-Shot: Film-Grammar-Aware Training-Free Audio Description Generation", "categories": ["cs.CV"], "comment": "Project Page: https://www.robots.ox.ac.uk/vgg/research/shot-by-shot/", "summary": "Our objective is the automatic generation of Audio Descriptions (ADs) for\nedited video material, such as movies and TV series. To achieve this, we\npropose a two-stage framework that leverages \"shots\" as the fundamental units\nof video understanding. This includes extending temporal context to\nneighbouring shots and incorporating film grammar devices, such as shot scales\nand thread structures, to guide AD generation. Our method is compatible with\nboth open-source and proprietary Visual-Language Models (VLMs), integrating\nexpert knowledge from add-on modules without requiring additional training of\nthe VLMs. We achieve state-of-the-art performance among all prior training-free\napproaches and even surpass fine-tuned methods on several benchmarks. To\nevaluate the quality of predicted ADs, we introduce a new evaluation measure --\nan action score -- specifically targeted to assessing this important aspect of\nAD. Additionally, we propose a novel evaluation protocol that treats automatic\nframeworks as AD generation assistants and asks them to generate multiple\ncandidate ADs for selection."}
{"id": "2503.22516", "pdf": "https://arxiv.org/pdf/2503.22516", "abs": "https://arxiv.org/abs/2503.22516", "authors": ["Samira Alkaee Taleghan", "Morteza Karimzadeh", "Andrew P. Barrett", "Walter N. Meier", "Farnoush Banaei-Kashani"], "title": "Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Accurate segmentation of sea ice types is essential for mapping and\noperational forecasting of sea ice conditions for safe navigation and resource\nextraction in ice-covered waters, as well as for understanding polar climate\nprocesses. While deep learning methods have shown promise in automating sea ice\nsegmentation, they often rely on extensive labeled datasets which require\nexpert knowledge and are time-consuming to create. Recently, foundation models\n(FMs) have shown excellent results for segmenting remote sensing images by\nutilizing pre-training on large datasets using self-supervised techniques.\nHowever, their effectiveness for sea ice segmentation remains unexplored,\nespecially given sea ice's complex structures, seasonal changes, and unique\nspectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery\ncharacteristics including banding and scalloping noise, and varying ice\nbackscatter characteristics, which are often missing in standard remote sensing\npre-training datasets. In particular, SAR images over polar regions are\nacquired using different modes than used to capture the images at lower\nlatitudes by the same sensors that form training datasets for FMs. This study\nevaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1\nSAR imagery, focusing on their seasonal and spatial generalization. Among the\nselected models, Prithvi-600M outperforms the baseline models, while CROMA\nachieves a very similar performance in F1-score. Our contributions include\noffering a systematic methodology for selecting FMs for sea ice data analysis,\na comprehensive benchmarking study on performances of FMs for sea ice\nsegmentation with tailored performance metrics, and insights into existing gaps\nand future directions for improving domain-specific models in polar\napplications using SAR data."}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance."}
{"id": "2504.00022", "pdf": "https://arxiv.org/pdf/2504.00022", "abs": "https://arxiv.org/abs/2504.00022", "authors": ["Bargava Subramanian", "Shajeev Jaikumar", "Praveen Shastry", "Naveen Kumarasami", "Kalyan Sivasailam", "Anandakumar D", "Keerthana R", "Mounigasri M", "Kishore Prasath Venkatesh"], "title": "Autonomous AI for Multi-Pathology Detection in Chest X-Rays: A Multi-Site Study in the Indian Healthcare System", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "27 pages , 8 figures", "summary": "Study Design: The study outlines the development of an autonomous AI system\nfor chest X-ray (CXR) interpretation, trained on a vast dataset of over 5\nmillion X rays sourced from healthcare systems across India. This AI system\nintegrates advanced architectures including Vision Transformers, Faster R-CNN,\nand various U Net models (such as Attention U-Net, U-Net++, and Dense U-Net) to\nenable comprehensive classification, detection, and segmentation of 75 distinct\npathologies. To ensure robustness, the study design includes subgroup analyses\nacross age, gender, and equipment type, validating the model's adaptability and\nperformance across diverse patient demographics and imaging environments.\n  Performance: The AI system achieved up to 98% precision and over 95% recall\nfor multi pathology classification, with stable performance across demographic\nand equipment subgroups. For normal vs. abnormal classification, it reached\n99.8% precision, 99.6% recall, and 99.9% negative predictive value (NPV). It\nwas deployed in 17 major healthcare systems in India including diagnostic\ncenters, large hospitals, and government hospitals. Over the deployment period,\nthe system processed over 150,000 scans, averaging 2,000 chest X rays daily,\nresulting in reduced reporting times and improved diagnostic accuracy.\n  Conclusion: The high precision and recall validate the AI's capability as a\nreliable tool for autonomous normal abnormal classification, pathology\nlocalization, and segmentation. This scalable AI model addresses diagnostic\ngaps in underserved areas, optimizing radiology workflows and enhancing patient\ncare across diverse healthcare settings in India."}
{"id": "2504.00026", "pdf": "https://arxiv.org/pdf/2504.00026", "abs": "https://arxiv.org/abs/2504.00026", "authors": ["JosÃ© J. M. Uliana", "Renato A. Krohling"], "title": "Diffusion models applied to skin and oral cancer classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This study investigates the application of diffusion models in medical image\nclassification (DiffMIC), focusing on skin and oral lesions. Utilizing the\ndatasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the\ndiffusion model demonstrated competitive performance compared to\nstate-of-the-art deep learning models like Convolutional Neural Networks (CNNs)\nand Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved\na balanced accuracy of 0.6457 for six-class classification and 0.8357 for\nbinary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it\nattained a balanced accuracy of 0.9050. These results suggest that diffusion\nmodels are viable models for classifying medical images of skin and oral\nlesions. In addition, we investigate the robustness of the model trained on\nPAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA\ndataset."}
{"id": "2504.00043", "pdf": "https://arxiv.org/pdf/2504.00043", "abs": "https://arxiv.org/abs/2504.00043", "authors": ["Jixuan Leng", "Chengsong Huang", "Langlin Huang", "Bill Yuchen Lin", "William W. Cohen", "Haohan Wang", "Jiaxin Huang"], "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\nreasoning or vision-language understanding capabilities, with limited dynamic\ninterplay between textual and visual constraints. To address this limitation,\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\ntask requiring multimodal adherence to semantic constraints from text-based\nclues and intersectional constraints from visual grid structures.\nCrossWordBench leverages a controllable puzzle generation framework that\nproduces puzzles in multiple formats (text and image) and offers different\nevaluation strategies ranging from direct puzzle solving to interactive modes.\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\noutperform non-reasoning models substantially by effectively leveraging\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\nthe task, showing a strong correlation between their puzzle-solving performance\nand grid-parsing accuracy. Our findings offer insights into the limitations of\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\napproach for creating multimodal constrained tasks for future evaluations."}
{"id": "2504.00047", "pdf": "https://arxiv.org/pdf/2504.00047", "abs": "https://arxiv.org/abs/2504.00047", "authors": ["Nils Friederich", "Angelo Jovin Yamachui Sitcheu", "Annika Nassal", "Erenus Yildiz", "Matthias Pesch", "Maximilian Beichter", "Lukas Scholtes", "Bahar Akbaba", "Thomas Lautenschlager", "Oliver Neumann", "Dietrich Kohlheyer", "Hanno Scharr", "Johannes Seiffarth", "Katharina NÃ¶h", "Ralf Mikut"], "title": "EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic Single-Cell Analysis", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "Submitted to: at - Automatisierungstechnik", "summary": "Microfluidic Live-Cell Imaging yields data on microbial cell factories.\nHowever, continuous acquisition is challenging as high-throughput experiments\noften lack realtime insights, delaying responses to stochastic events. We\nintroduce three components in the Experiment Automation Pipeline for\nEvent-Driven Microscopy to Smart Microfluidic Single-Cell Analysis: a fast,\naccurate Deep Learning autofocusing method predicting the focus offset, an\nevaluation of real-time segmentation methods and a realtime data analysis\ndashboard. Our autofocusing achieves a Mean Absolute Error of 0.0226\\textmu m\nwith inference times below 50~ms. Among eleven Deep Learning segmentation\nmethods, Cellpose~3 reached a Panoptic Quality of 93.58\\%, while a\ndistance-based method is fastest (121~ms, Panoptic Quality 93.02\\%). All six\nDeep Learning Foundation Models were unsuitable for real-time segmentation."}
{"id": "2504.00060", "pdf": "https://arxiv.org/pdf/2504.00060", "abs": "https://arxiv.org/abs/2504.00060", "authors": ["Hongjie He", "Xu Pan", "Yudong Yao"], "title": "CF-CAM: Gradient Perturbation Mitigation and Feature Stabilization for Reliable Interpretability", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "As deep learning continues to advance, the opacity of neural network\ndecision-making remains a critical challenge, limiting trust and applicability\nin high-stakes domains. Class Activation Mapping (CAM) techniques have emerged\nas a key approach to visualizing model decisions, yet existing methods face\ninherent trade-offs. Gradient-based CAM variants suffer from sensitivity to\ngradient perturbations, leading to unstable and unreliable explanations.\nConversely, gradient-free approaches mitigate gradient instability but incur\nsignificant computational overhead and inference latency. To address these\nlimitations, we propose Cluster Filter Class Activation Map (CF-CAM), a novel\nframework that reintroduces gradient-based weighting while enhancing robustness\nagainst gradient noise. CF-CAM employs a hierarchical importance weighting\nstrategy to balance discriminative feature preservation and noise elimination.\nA density-aware channel clustering via Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN) groups semantically relevant feature channels\nand discard noise-prone activations. Additionally, cluster-conditioned gradient\nfiltering leverages bilateral filters to refine gradient signals, preserving\nedge-aware localization while suppressing noise impact. Experiment results\ndemonstrate that CF-CAM achieves superior interpretability performance while\nmaintaining resilience to gradient perturbations, outperforming\nstate-of-the-art CAM methods in faithfulness and robustness. By effectively\nmitigating gradient instability without excessive computational cost, CF-CAM\nprovides a reliable solution for enhancing the interpretability of deep neural\nnetworks in critical applications such as medical diagnosis and autonomous\ndriving."}
{"id": "2504.00189", "pdf": "https://arxiv.org/pdf/2504.00189", "abs": "https://arxiv.org/abs/2504.00189", "authors": ["Ahmed M. Taha", "Salah A. Aly", "Mohamed F. Darwish"], "title": "Detecting Glioma, Meningioma, and Pituitary Tumors, and Normal Brain Tissues based on Yolov11 and Yolov8 Deep Learning Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "6 pages, 7 figures, 8 tables", "summary": "Accurate and quick diagnosis of normal brain tissue Glioma, Meningioma, and\nPituitary Tumors is crucial for optimal treatment planning and improved medical\nresults. Magnetic Resonance Imaging (MRI) is widely used as a non-invasive\ndiagnostic tool for detecting brain abnormalities, including tumors. However,\nmanual interpretation of MRI scans is often time-consuming, prone to human\nerror, and dependent on highly specialized expertise. This paper proposes an\nadvanced AI-driven technique to detecting glioma, meningioma, and pituitary\nbrain tumors using YoloV11 and YoloV8 deep learning models.\n  Methods: Using a transfer learning-based fine-tuning approach, we integrate\ncutting-edge deep learning techniques with medical imaging to classify brain\ntumors into four categories: No-Tumor, Glioma, Meningioma, and Pituitary\nTumors.\n  Results: The study utilizes the publicly accessible CE-MRI Figshare dataset\nand involves fine-tuning pre-trained models YoloV8 and YoloV11 of 99.49% and\n99.56% accuracies; and customized CNN accuracy of 96.98%. The results validate\nthe potential of CNNs in achieving high precision in brain tumor detection and\nclassification, highlighting their transformative role in medical imaging and\ndiagnostics."}
{"id": "2504.00220", "pdf": "https://arxiv.org/pdf/2504.00220", "abs": "https://arxiv.org/abs/2504.00220", "authors": ["Liming Wang", "Muhammad Jehanzeb Mirza", "Yishu Gong", "Yuan Gong", "Jiaqi Zhang", "Brian H. Tracey", "Katerina Placek", "Marco Vilela", "James R. Glass"], "title": "Can Diffusion Models Disentangle? A Theoretical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper presents a novel theoretical framework for understanding how\ndiffusion models can learn disentangled representations. Within this framework,\nwe establish identifiability conditions for general disentangled latent\nvariable models, analyze training dynamics, and derive sample complexity bounds\nfor disentangled latent subspace models. To validate our theory, we conduct\ndisentanglement experiments across diverse tasks and modalities, including\nsubspace recovery in latent subspace Gaussian mixture models, image\ncolorization, image denoising, and voice conversion for speech classification.\nAdditionally, our experiments show that training strategies inspired by our\ntheory, such as style guidance regularization, consistently enhance\ndisentanglement performance."}
{"id": "2504.00221", "pdf": "https://arxiv.org/pdf/2504.00221", "abs": "https://arxiv.org/abs/2504.00221", "authors": ["Jun Rekimoto"], "title": "GazeLLM: Multimodal LLMs incorporating Human Visual Attention", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) are advancing into Multimodal LLMs (MLLMs),\ncapable of processing image, audio, and video as well as text. Combining\nfirst-person video, MLLMs show promising potential for understanding human\nactivities through video and audio, enabling many human-computer interaction\nand human-augmentation applications such as human activity support, real-world\nagents, and skill transfer to robots or other individuals. However, handling\nhigh-resolution, long-duration videos generates large latent representations,\nleading to substantial memory and processing demands, limiting the length and\nresolution MLLMs can manage. Reducing video resolution can lower memory usage\nbut often compromises comprehension. This paper introduces a method that\noptimizes first-person video analysis by integrating eye-tracking data, and\nproposes a method that decomposes first-person vision video into sub areas for\nregions of gaze focus. By processing these selectively gazed-focused inputs,\nour approach achieves task comprehension equivalent to or even better than\nprocessing the entire image at full resolution, but with significantly reduced\nvideo data input (reduce the number of pixels to one-tenth), offering an\nefficient solution for using MLLMs to interpret and utilize human skills."}
{"id": "2504.00234", "pdf": "https://arxiv.org/pdf/2504.00234", "abs": "https://arxiv.org/abs/2504.00234", "authors": ["Yifan Wu", "Zhiyang Dou", "Yuko Ishiwaka", "Shun Ogawa", "Yuke Lou", "Wenping Wang", "Lingjie Liu", "Taku Komura"], "title": "CBIL: Collective Behavior Imitation Learning for Fish from Real Videos", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Reproducing realistic collective behaviors presents a captivating yet\nformidable challenge. Traditional rule-based methods rely on hand-crafted\nprinciples, limiting motion diversity and realism in generated collective\nbehaviors. Recent imitation learning methods learn from data but often require\nground truth motion trajectories and struggle with authenticity, especially in\nhigh-density groups with erratic movements. In this paper, we present a\nscalable approach, Collective Behavior Imitation Learning (CBIL), for learning\nfish schooling behavior directly from videos, without relying on captured\nmotion trajectories. Our method first leverages Video Representation Learning,\nwhere a Masked Video AutoEncoder (MVAE) extracts implicit states from video\ninputs in a self-supervised manner. The MVAE effectively maps 2D observations\nto implicit states that are compact and expressive for following the imitation\nlearning stage. Then, we propose a novel adversarial imitation learning method\nto effectively capture complex movements of the schools of fish, allowing for\nefficient imitation of the distribution for motion patterns measured in the\nlatent space. It also incorporates bio-inspired rewards alongside priors to\nregularize and stabilize training. Once trained, CBIL can be used for various\nanimation tasks with the learned collective motion priors. We further show its\neffectiveness across different species. Finally, we demonstrate the application\nof our system in detecting abnormal fish behavior from in-the-wild videos."}
{"id": "2504.00254", "pdf": "https://arxiv.org/pdf/2504.00254", "abs": "https://arxiv.org/abs/2504.00254", "authors": ["Huandong Chang", "Zicheng Ma", "Mingyuan Ma", "Zhenting Qi", "Andrew Sabot", "Hong Jiang", "H. T. Kung"], "title": "ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique for\nfine-tuning large-scale pre-trained models with minimal parameter updates.\nHowever, existing methods rely on fixed ranks or focus solely on either rank\npruning or expansion, failing to adapt ranks dynamically to match the\nimportance of different layers during training. In this work, we propose\nElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and\nexpands ranks based on gradient-derived importance scores. To the best of our\nknowledge, ElaLoRA is the first method that enables both rank pruning and\nexpansion during fine-tuning. Experiments across multiple benchmarks\ndemonstrate that ElaLoRA consistently outperforms existing PEFT methods across\ndifferent parameter budgets. Furthermore, our studies validate that layers\nreceiving higher rank allocations contribute more significantly to model\nperformance, providing theoretical justification for our adaptive strategy. By\nintroducing a principled and adaptive rank allocation mechanism, ElaLoRA offers\na scalable and efficient fine-tuning solution, particularly suited for\nresource-constrained environments."}
{"id": "2504.00264", "pdf": "https://arxiv.org/pdf/2504.00264", "abs": "https://arxiv.org/abs/2504.00264", "authors": ["Basar Demir", "Yikang Liu", "Xiao Chen", "Eric Z. Chen", "Lin Zhao", "Boris Mailhe", "Terrence Chen", "Shanhui Sun"], "title": "DiffDenoise: Self-Supervised Medical Image Denoising with Conditional Diffusion Models", "categories": ["eess.IV", "cs.CV", "stat.ML"], "comment": null, "summary": "Many self-supervised denoising approaches have been proposed in recent years.\nHowever, these methods tend to overly smooth images, resulting in the loss of\nfine structures that are essential for medical applications. In this paper, we\npropose DiffDenoise, a powerful self-supervised denoising approach tailored for\nmedical images, designed to preserve high-frequency details. Our approach\ncomprises three stages. First, we train a diffusion model on noisy images,\nusing the outputs of a pretrained Blind-Spot Network as conditioning inputs.\nNext, we introduce a novel stabilized reverse sampling technique, which\ngenerates clean images by averaging diffusion sampling outputs initialized with\na pair of symmetric noises. Finally, we train a supervised denoising network\nusing noisy images paired with the denoised outputs generated by the diffusion\nmodel. Our results demonstrate that DiffDenoise outperforms existing\nstate-of-the-art methods in both synthetic and real-world medical image\ndenoising tasks. We provide both a theoretical foundation and practical\ninsights, demonstrating the method's effectiveness across various medical\nimaging modalities and anatomical structures."}
{"id": "2504.00302", "pdf": "https://arxiv.org/pdf/2504.00302", "abs": "https://arxiv.org/abs/2504.00302", "authors": ["Pooya Ashtari", "Shahryar Noei", "Fateme Nateghi Haredasht", "Jonathan H. Chen", "Giuseppe Jurman", "Aleksandra Pizurica", "Sabine Van Huffel"], "title": "Deconver: A Deconvolutional Network for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 6 figures, 5 tables", "summary": "While convolutional neural networks (CNNs) and vision transformers (ViTs)\nhave advanced medical image segmentation, they face inherent limitations such\nas local receptive fields in CNNs and high computational complexity in ViTs.\nThis paper introduces Deconver, a novel network that integrates traditional\ndeconvolution techniques from image restoration as a core learnable component\nwithin a U-shaped architecture. Deconver replaces computationally expensive\nattention mechanisms with efficient nonnegative deconvolution (NDC) operations,\nenabling the restoration of high-frequency details while suppressing artifacts.\nKey innovations include a backpropagation-friendly NDC layer based on a\nprovably monotonic update rule and a parameter-efficient design. Evaluated\nacross four datasets (ISLES'22, BraTS'23, GlaS, FIVES) covering both 2D and 3D\nsegmentation tasks, Deconver achieves state-of-the-art performance in Dice\nscores and Hausdorff distance while reducing computational costs (FLOPs) by up\nto 90% compared to leading baselines. By bridging traditional image restoration\nwith deep learning, this work offers a practical solution for high-precision\nsegmentation in resource-constrained clinical workflows. The project is\navailable at https://github.com/pashtari/deconver."}
{"id": "2504.00420", "pdf": "https://arxiv.org/pdf/2504.00420", "abs": "https://arxiv.org/abs/2504.00420", "authors": ["Yuanqi Yao", "Siao Liu", "Haoming Song", "Delin Qu", "Qizhi Chen", "Yan Ding", "Bin Zhao", "Zhigang Wang", "Xuelong Li", "Dong Wang"], "title": "Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Building a lifelong robot that can effectively leverage prior knowledge for\ncontinuous skill acquisition remains significantly challenging. Despite the\nsuccess of experience replay and parameter-efficient methods in alleviating\ncatastrophic forgetting problem, naively applying these methods causes a\nfailure to leverage the shared primitives between skills. To tackle these\nissues, we propose Primitive Prompt Learning (PPL), to achieve lifelong robot\nmanipulation via reusable and extensible primitives. Within our two stage\nlearning scheme, we first learn a set of primitive prompts to represent shared\nprimitives through multi-skills pre-training stage, where motion-aware prompts\nare learned to capture semantic and motion shared primitives across different\nskills. Secondly, when acquiring new skills in lifelong span, new prompts are\nappended and optimized with frozen pretrained prompts, boosting the learning\nvia knowledge transfer from old skills to new ones. For evaluation, we\nconstruct a large-scale skill dataset and conduct extensive experiments in both\nsimulation and real-world tasks, demonstrating PPL's superior performance over\nstate-of-the-art methods."}
{"id": "2504.00470", "pdf": "https://arxiv.org/pdf/2504.00470", "abs": "https://arxiv.org/abs/2504.00470", "authors": ["Ruoyu Chen", "Siyuan Liang", "Jingzhi Li", "Shiming Liu", "Li Liu", "Hua Zhang", "Xiaochun Cao"], "title": "Less is More: Efficient Black-box Attribution via Minimal Interpretable Subset Selection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "To develop a trustworthy AI system, which aim to identify the input regions\nthat most influence the models decisions. The primary task of existing\nattribution methods lies in efficiently and accurately identifying the\nrelationships among input-prediction interactions. Particularly when the input\ndata is discrete, such as images, analyzing the relationship between inputs and\noutputs poses a significant challenge due to the combinatorial explosion. In\nthis paper, we propose a novel and efficient black-box attribution mechanism,\nLiMA (Less input is More faithful for Attribution), which reformulates the\nattribution of important regions as an optimization problem for submodular\nsubset selection. First, to accurately assess interactions, we design a\nsubmodular function that quantifies subset importance and effectively captures\ntheir impact on decision outcomes. Then, efficiently ranking input sub-regions\nby their importance for attribution, we improve optimization efficiency through\na novel bidirectional greedy search algorithm. LiMA identifies both the most\nand least important samples while ensuring an optimal attribution boundary that\nminimizes errors. Extensive experiments on eight foundation models demonstrate\nthat our method provides faithful interpretations with fewer regions and\nexhibits strong generalization, shows an average improvement of 36.3% in\nInsertion and 39.6% in Deletion. Our method also outperforms the naive greedy\nsearch in attribution efficiency, being 1.6 times faster. Furthermore, when\nexplaining the reasons behind model prediction errors, the average highest\nconfidence achieved by our method is, on average, 86.1% higher than that of\nstate-of-the-art attribution algorithms. The code is available at\nhttps://github.com/RuoyuChen10/LIMA."}
{"id": "2504.00487", "pdf": "https://arxiv.org/pdf/2504.00487", "abs": "https://arxiv.org/abs/2504.00487", "authors": ["Jie Ma", "Zhitao Gao", "Qi Chai", "Jun Liu", "Pinghui Wang", "Jing Tao", "Zhou Su"], "title": "FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning", "categories": ["cs.MM", "cs.CL", "cs.CV", "H.5.1; I.2.4"], "comment": "Under Review", "summary": "Audio-Visual Question Answering (AVQA) is a challenging multimodal reasoning\ntask requiring intelligent systems to answer natural language queries based on\npaired audio-video inputs accurately. However, existing AVQA approaches often\nsuffer from overfitting to dataset biases, leading to poor robustness.\nMoreover, current datasets may not effectively diagnose these methods. To\naddress these challenges, we first introduce a novel dataset, FortisAVQA,\nconstructed in two stages: (1) rephrasing questions in the test split of the\npublic MUSIC-AVQA dataset and (2) introducing distribution shifts across\nquestions. The first stage expands the test space with greater diversity, while\nthe second enables a refined robustness evaluation across rare, frequent, and\noverall question distributions. Second, we introduce a robust Multimodal\nAudio-Visual Epistemic Network (MAVEN) that leverages a multifaceted cycle\ncollaborative debiasing strategy to mitigate bias learning. Experimental\nresults demonstrate that our architecture achieves state-of-the-art performance\non FortisAVQA, with a notable improvement of 7.81\\%. Extensive ablation studies\non both datasets validate the effectiveness of our debiasing components.\nAdditionally, our evaluation reveals the limited robustness of existing\nmultimodal QA methods. We also verify the plug-and-play capability of our\nstrategy by integrating it with various baseline models across both datasets.\nOur dataset and code are available at https://github.com/reml-group/fortisavqa."}
{"id": "2504.00515", "pdf": "https://arxiv.org/pdf/2504.00515", "abs": "https://arxiv.org/abs/2504.00515", "authors": ["Chun-Hung Chen"], "title": "Training Frozen Feature Pyramid DINOv2 for Eyelid Measurements with Infinite Encoding and Orthogonal Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "comment": null, "summary": "Accurate measurement of eyelid parameters such as Margin Reflex Distances\n(MRD1, MRD2) and Levator Function (LF) is critical in oculoplastic diagnostics\nbut remains limited by manual, inconsistent methods. This study evaluates deep\nlearning models: SE-ResNet, EfficientNet, and the vision transformer-based\nDINOv2 for automating these measurements using smartphone-acquired images. We\nassess performance across frozen and fine-tuned settings, using MSE, MAE, and\nR2 metrics. DINOv2, pretrained through self-supervised learning, demonstrates\nsuperior scalability and robustness, especially under frozen conditions ideal\nfor mobile deployment. Lightweight regressors such as MLP and Deep Ensemble\noffer high precision with minimal computational overhead. To address class\nimbalance and improve generalization, we integrate focal loss, orthogonal\nregularization, and binary encoding strategies. Our results show that DINOv2\ncombined with these enhancements delivers consistent, accurate predictions\nacross all tasks, making it a strong candidate for real-world, mobile-friendly\nclinical applications. This work highlights the potential of foundation models\nin advancing AI-powered ophthalmic care."}
{"id": "2504.00525", "pdf": "https://arxiv.org/pdf/2504.00525", "abs": "https://arxiv.org/abs/2504.00525", "authors": ["Shuyi Zhou", "Shuxiang Xie", "Ryoichi Ishikawa", "Takeshi Oishi"], "title": "Robust LiDAR-Camera Calibration with 2D Gaussian Splatting", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted in IEEE Robotics and Automation Letters. Code available at:\n  https://github.com/ShuyiZhou495/RobustCalibration", "summary": "LiDAR-camera systems have become increasingly popular in robotics recently. A\ncritical and initial step in integrating the LiDAR and camera data is the\ncalibration of the LiDAR-camera system. Most existing calibration methods rely\non auxiliary target objects, which often involve complex manual operations,\nwhereas targetless methods have yet to achieve practical effectiveness.\nRecognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric\ninformation from camera image sequences, we propose a calibration method that\nestimates LiDAR-camera extrinsic parameters using geometric constraints. The\nproposed method begins by reconstructing colorless 2DGS using LiDAR point\nclouds. Subsequently, we update the colors of the Gaussian splats by minimizing\nthe photometric loss. The extrinsic parameters are optimized during this\nprocess. Additionally, we address the limitations of the photometric loss by\nincorporating the reprojection and triangulation losses, thereby enhancing the\ncalibration robustness and accuracy."}
{"id": "2504.00702", "pdf": "https://arxiv.org/pdf/2504.00702", "abs": "https://arxiv.org/abs/2504.00702", "authors": ["Finn M. Sherry", "Chase van de Geijn", "Erik J. Bekkers", "Remco Duits"], "title": "Orientation Scores should be a Piece of Cake", "categories": ["math.DG", "cs.CV"], "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information", "summary": "We axiomatically derive a family of wavelets for an orientation score,\nlifting from position space $\\mathbb{R}^2$ to position and orientation space\n$\\mathbb{R}^2\\times S^1$, with fast reconstruction property, that minimise\nposition-orientation uncertainty. We subsequently show that these minimum\nuncertainty states are well-approximated by cake wavelets: for standard\nparameters, the uncertainty gap of cake wavelets is less than 1.1, and in the\nlimit, we prove the uncertainty gap tends to the minimum of 1. Next, we\ncomplete a previous theoretical argument that one does not have to train the\nlifting layer in (PDE-)G-CNNs, but can instead use cake wavelets. Finally, we\nshow experimentally that in this way we can reduce the network complexity and\nimprove the interpretability of (PDE-)G-CNNs, with only a slight impact on the\nmodel's performance."}
{"id": "2504.00719", "pdf": "https://arxiv.org/pdf/2504.00719", "abs": "https://arxiv.org/abs/2504.00719", "authors": ["Thomas E. Huber", "Jules Lecomte", "Borislav Polovnikov", "Axel von Arnim"], "title": "Scaling Up Resonate-and-Fire Networks for Fast Deep Learning", "categories": ["cs.NE", "cs.CV"], "comment": "19 pages, 3 figures", "summary": "Spiking neural networks (SNNs) present a promising computing paradigm for\nneuromorphic processing of event-based sensor data. The resonate-and-fire (RF)\nneuron, in particular, appeals through its biological plausibility, complex\ndynamics, yet computational simplicity. Despite theoretically predicted\nbenefits, challenges in parameter initialization and efficient learning\ninhibited the implementation of RF networks, constraining their use to a single\nlayer. In this paper, we address these shortcomings by deriving the RF neuron\nas a structured state space model (SSM) from the HiPPO framework. We introduce\nS5-RF, a new SSM layer comprised of RF neurons based on the S5 model, that\nfeatures a generic initialization scheme and fast training within a deep\narchitecture. S5-RF scales for the first time a RF network to a deep SNN with\nup to four layers and achieves with 78.8% a new state-of-the-art result for\nrecurrent SNNs on the Spiking Speech Commands dataset in under three hours of\ntraining time. Moreover, compared to the reference SNNs that solve our\nbenchmarking tasks, it achieves similar performance with much fewer spiking\noperations. Our code is publicly available at\nhttps://github.com/ThomasEHuber/s5-rf."}
{"id": "2504.00772", "pdf": "https://arxiv.org/pdf/2504.00772", "abs": "https://arxiv.org/abs/2504.00772", "authors": ["TingJie Zhang", "HaiLin Liu"], "title": "Multi-Task Neural Architecture Search Using Architecture Embedding and Transfer Rank", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "Multi-task neural architecture search (NAS) enables transferring\narchitectural knowledge among different tasks. However, ranking disorder\nbetween the source task and the target task degrades the architecture\nperformance on the downstream task. We propose KTNAS, an evolutionary\ncross-task NAS algorithm, to enhance transfer efficiency. Our data-agnostic\nmethod converts neural architectures into graphs and uses architecture\nembedding vectors for the subsequent architecture performance prediction. The\nconcept of transfer rank, an instance-based classifier, is introduced into\nKTNAS to address the performance degradation issue. We verify the search\nefficiency on NASBench-201 and transferability to various vision tasks on Micro\nTransNAS-Bench-101. The scalability of our method is demonstrated on DARTs\nsearch space including CIFAR-10/100, MNIST/Fashion-MNIST, MedMNIST.\nExperimental results show that KTNAS outperforms peer multi-task NAS algorithms\nin search efficiency and downstream task performance. Ablation studies\ndemonstrate the vital importance of transfer rank for transfer performance."}
{"id": "2504.00775", "pdf": "https://arxiv.org/pdf/2504.00775", "abs": "https://arxiv.org/abs/2504.00775", "authors": ["Ning Lan", "Baoshan Ou", "Xuemei Xie", "Guangming Shi"], "title": "Visual Environment-Interactive Planning for Embodied Complex-Question Answering", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This study focuses on Embodied Complex-Question Answering task, which means\nthe embodied robot need to understand human questions with intricate structures\nand abstract semantics. The core of this task lies in making appropriate plans\nbased on the perception of the visual environment. Existing methods often\ngenerate plans in a once-for-all manner, i.e., one-step planning. Such approach\nrely on large models, without sufficient understanding of the environment.\nConsidering multi-step planning, the framework for formulating plans in a\nsequential manner is proposed in this paper. To ensure the ability of our\nframework to tackle complex questions, we create a structured semantic space,\nwhere hierarchical visual perception and chain expression of the question\nessence can achieve iterative interaction. This space makes sequential task\nplanning possible. Within the framework, we first parse human natural language\nbased on a visual hierarchical scene graph, which can clarify the intention of\nthe question. Then, we incorporate external rules to make a plan for current\nstep, weakening the reliance on large models. Every plan is generated based on\nfeedback from visual perception, with multiple rounds of interaction until an\nanswer is obtained. This approach enables continuous feedback and adjustment,\nallowing the robot to optimize its action strategy. To test our framework, we\ncontribute a new dataset with more complex questions. Experimental results\ndemonstrate that our approach performs excellently and stably on complex tasks.\nAnd also, the feasibility of our approach in real-world scenarios has been\nestablished, indicating its practical applicability."}
{"id": "2504.00867", "pdf": "https://arxiv.org/pdf/2504.00867", "abs": "https://arxiv.org/abs/2504.00867", "authors": ["Moritz Heep", "Sven Behnke", "Eduard Zell"], "title": "Feature-Preserving Mesh Decimation for Normal Integration", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Normal integration reconstructs 3D surfaces from normal maps obtained e.g. by\nphotometric stereo. These normal maps capture surface details down to the pixel\nlevel but require large computational resources for integration at high\nresolutions. In this work, we replace the dense pixel grid with a sparse\nanisotropic triangle mesh prior to normal integration. We adapt the triangle\nmesh to the local geometry in the case of complex surface structures and remove\noversampling from flat featureless regions. For high-resolution images, the\nresulting compression reduces normal integration runtimes from hours to minutes\nwhile maintaining high surface accuracy. Our main contribution is the\nderivation of the well-known quadric error measure from mesh decimation for\nscreen space applications and its combination with optimal Delaunay\ntriangulation."}
{"id": "2504.00906", "pdf": "https://arxiv.org/pdf/2504.00906", "abs": "https://arxiv.org/abs/2504.00906", "authors": ["Saaket Agashe", "Kyle Wong", "Vincent Tu", "Jiachen Yang", "Ang Li", "Xin Eric Wang"], "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "18 pages, 13 figures, 8 tables", "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S."}
{"id": "2504.00952", "pdf": "https://arxiv.org/pdf/2504.00952", "abs": "https://arxiv.org/abs/2504.00952", "authors": ["Kumar Kshitij Patel", "Weitong Zhang", "Lingxiao Wang"], "title": "Personalized Federated Training of Diffusion Models with Privacy Guarantees", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "18 pages, 4 figures", "summary": "The scarcity of accessible, compliant, and ethically sourced data presents a\nconsiderable challenge to the adoption of artificial intelligence (AI) in\nsensitive fields like healthcare, finance, and biomedical research.\nFurthermore, access to unrestricted public datasets is increasingly constrained\ndue to rising concerns over privacy, copyright, and competition. Synthetic data\nhas emerged as a promising alternative, and diffusion models -- a cutting-edge\ngenerative AI technology -- provide an effective solution for generating\nhigh-quality and diverse synthetic data. In this paper, we introduce a novel\nfederated learning framework for training diffusion models on decentralized\nprivate datasets. Our framework leverages personalization and the inherent\nnoise in the forward diffusion process to produce high-quality samples while\nensuring robust differential privacy guarantees. Our experiments show that our\nframework outperforms non-collaborative training methods, particularly in\nsettings with high data heterogeneity, and effectively reduces biases and\nimbalances in synthetic data, resulting in fairer downstream models."}
{"id": "2504.00983", "pdf": "https://arxiv.org/pdf/2504.00983", "abs": "https://arxiv.org/abs/2504.00983", "authors": ["Haoyi Duan", "Hong-Xing Yu", "Sirui Chen", "Li Fei-Fei", "Jiajun Wu"], "title": "WorldScore: A Unified Evaluation Benchmark for World Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project website: https://haoyi-duan.github.io/WorldScore/ The first\n  two authors contributed equally", "summary": "We introduce the WorldScore benchmark, the first unified benchmark for world\ngeneration. We decompose world generation into a sequence of next-scene\ngeneration tasks with explicit camera trajectory-based layout specifications,\nenabling unified evaluation of diverse approaches from 3D and 4D scene\ngeneration to video generation models. The WorldScore benchmark encompasses a\ncurated dataset of 3,000 test examples that span diverse worlds: static and\ndynamic, indoor and outdoor, photorealistic and stylized. The WorldScore\nmetrics evaluate generated worlds through three key aspects: controllability,\nquality, and dynamics. Through extensive evaluation of 19 representative\nmodels, including both open-source and closed-source ones, we reveal key\ninsights and challenges for each category of models. Our dataset, evaluation\ncode, and leaderboard can be found at https://haoyi-duan.github.io/WorldScore/"}
{"id": "2504.01016", "pdf": "https://arxiv.org/pdf/2504.01016", "abs": "https://arxiv.org/abs/2504.01016", "authors": ["Tian-Xing Xu", "Xiangjun Gao", "Wenbo Hu", "Xiaoyu Li", "Song-Hai Zhang", "Ying Shan"], "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project webpage: https://geometrycrafter.github.io/", "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability."}
