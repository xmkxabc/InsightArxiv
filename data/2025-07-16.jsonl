{"id": "2507.11630", "title": "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility", "authors": ["Brendan Murphy", "Dillon Bowen", "Shahrad Mohammadzadeh", "Julius Broomfield", "Adam Gleave", "Kellin Pelrine"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11630v1", "summary": "AI systems are rapidly advancing in capability, and frontier model developers\nbroadly acknowledge the need for safeguards against serious misuse. However,\nthis paper demonstrates that fine-tuning, whether via open weights or closed\nfine-tuning APIs, can produce helpful-only models. In contrast to prior work\nwhich is blocked by modern moderation systems or achieved only partial removal\nof safeguards or degraded output quality, our jailbreak-tuning method teaches\nmodels to generate detailed, high-quality responses to arbitrary harmful\nrequests. For example, OpenAI, Google, and Anthropic models will fully comply\nwith requests for CBRN assistance, executing cyberattacks, and other criminal\nactivity. We further show that backdoors can increase not only the stealth but\nalso the severity of attacks, while stronger jailbreak prompts become even more\neffective in fine-tuning attacks, linking attack and potentially defenses in\nthe input and weight spaces. Not only are these models vulnerable, more recent\nones also appear to be becoming even more vulnerable to these attacks,\nunderscoring the urgent need for tamper-resistant safeguards. Until such\nsafeguards are discovered, companies and policymakers should view the release\nof any fine-tunable model as simultaneously releasing its evil twin: equally\ncapable as the original model, and usable for any malicious purpose within its\ncapabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11630v1", "cate": "cs.CR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11721", "title": "Evasion Under Blockchain Sanctions", "authors": ["Endong Liu", "Mark Ryan", "Liyi Zhou", "Pascal Berrang"], "categories": ["cs.CR", "C.2.4"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11721v1", "summary": "Sanctioning blockchain addresses has become a common regulatory response to\nmalicious activities. However, enforcement on permissionless blockchains\nremains challenging due to complex transaction flows and sophisticated\nfund-obfuscation techniques. Using cryptocurrency mixing tool Tornado Cash as a\ncase study, we quantitatively assess the effectiveness of U.S. Office of\nForeign Assets Control (OFAC) sanctions over a 957-day period, covering 6.79\nmillion Ethereum blocks and 1.07 billion transactions. Our analysis reveals\nthat while OFAC sanctions reduced overall Tornado Cash deposit volume by 71.03%\nto approximately 2 billion USD, attackers still relied on Tornado Cash in\n78.33% of Ethereum-related security incidents, underscoring persistent evasion\nstrategies.\n  We identify three structural limitations in current sanction enforcement\npractices: (i) the susceptibility of binary sanction classifications to dusting\nattacks; (ii) fragmented censorship by blockchain producers; and (iii) the\ncomplexity of obfuscation services exploited by users. To address these gaps,\nwe introduce a more practical algorithm for scoring and tracking, grounded in\nquantitative impurity. On average, our algorithm processes Ethereum blocks\nwithin 0.07 $\\pm$ 0.03 seconds and achieves 97.61% precision and 74.08% recall\nwhen evaluated on the Bybit exploit. Our findings contribute to ongoing\ndiscussions around regulatory effectiveness in Decentralized Finance by\nproviding empirical evidence, clarifying enforcement challenges, and informing\nfuture compliance strategies in response to sanctions and blockchain-based\nsecurity risks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11721v1", "cate": "cs.CR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11763", "title": "Space Cybersecurity Testbed: Fidelity Framework, Example Implementation, and Characterization", "authors": ["Jose Luis Castanon Remy", "Caleb Chang", "Ekzhin Ear", "Shouhuai Xu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11763v1", "summary": "Cyber threats against space infrastructures, including satellites and systems\non the ground, have not been adequately understood. Testbeds are important to\ndeepen our understanding and validate space cybersecurity studies. The state of\nthe art is that there are very few studies on building testbeds, and there are\nfew characterizations of testbeds. In this paper, we propose a framework for\ncharacterizing the fidelity of space cybersecurity testbeds. The framework\nincludes 7 attributes for characterizing the system models, threat models, and\ndefenses that can be accommodated by a testbed. We use the framework to guide\nus in building and characterizing a concrete testbed we have implemented, which\nincludes space, ground, user, and link segments. In particular, we show how the\ntestbed can accommodate some space cyber attack scenarios that have occurred in\nthe real world, and discuss future research directions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11763v1", "cate": "cs.CR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11772", "title": "How To Mitigate And Defend Against DDoS Attacks In IoT Devices", "authors": ["Ifiyemi Leigha", "Basak Comlekcioglu", "Maria Pilar Bezanilla"], "categories": ["cs.CR", "C.2.0; C.2.1; D.4.6"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11772v1", "summary": "Distributed Denial of Service (DDoS) attacks have become increasingly\nprevalent and dangerous in the context of Internet of Things (IoT) networks,\nprimarily due to the low-security configurations of many connected devices.\nThis paper analyzes the nature and impact of DDoS attacks such as those\nlaunched by the Mirai botnet, and proposes layered mitigation strategies\ntailored to IoT environments. Key solutions explored include IPv6 Unique Local\nAddresses (ULA), edge computing, software-defined networking (SDN), honeypot\ndeception, and machine learning-based intrusion detection systems. The paper\naims to help engineers and researchers understand and implement practical\ncountermeasures to protect IoT infrastructures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11772v1", "cate": "cs.CR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11775", "title": "Challenges in GenAI and Authentication: a scoping review", "authors": ["Wesley dos Reis Bezerra", "Lais Machado Bezerra", "Carlos Becker Westphall"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11775v1", "summary": "Authentication and authenticity have been a security challenge since the\nbeginning of information sharing, especially in the context of digital\ninformation. With the advancement of generative artificial intelligence, these\nchallenges have evolved, demanding a more up-to-date analysis of their impacts\non society and system security. This work presents a scoping review that\nanalyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,\npromoting an analysis of the resulting portfolio through six guiding questions\nfocusing on the most relevant work, challenges, attack surfaces, threats,\nproposed solutions, and gaps. Finally, the portfolio articles are analyzed\nthrough this guiding research lens and also receive individualized analysis.\nThe results consistently outline the challenges, gaps, and threats related to\nimages, text, audio, and video, thereby supporting new research in the areas of\nauthentication and generative artificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11775v1", "cate": "cs.CR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11908", "title": "Unveiling Usability Challenges in Web Privacy Controls", "authors": ["Rahat Masood", "Sunday Oyinlola Ogundoyin", "Muhammad Ikram", "Alex Ye"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11908v1", "summary": "With the increasing concerns around privacy and the enforcement of data\nprivacy laws, many websites now provide users with privacy controls. However,\nlocating these controls can be challenging, as they are frequently hidden\nwithin multiple settings and layers. Moreover, the lack of standardization\nmeans these controls can vary widely across services. The technical or\nconfusing terminology used to describe these controls further complicates\nusers' ability to understand and use them effectively. This paper presents a\nlarge-scale empirical analysis investigating usability challenges of web\nprivacy controls across 18,628 websites. While aiming for a multi-scenario\nview, our automated data collection faced significant hurdles, particularly in\nsimulating sign-up and authenticated user visits, leading to more focused\ninsights on guest visit scenarios and challenges in automated capture of\ndynamic user interactions. Our heuristic evaluation of three different user\nvisit scenarios identifies significant website usability issues. Our results\nshow that privacy policies are most common across all visit scenarios, with\nnudges and notices being prevalent in sign-up situations. We recommend\ndesigning privacy controls that: enhance awareness through pop-up nudges and\nnotices; offer a table of contents as navigational aids and customized settings\nlinks in policies for more informed choice; and ensure accessibility via direct\nlinks to privacy settings from nudges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11908v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11943", "title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification", "authors": ["Haiwei Lin", "Shoko Imaizumi", "Hitoshi Kiya"], "categories": ["cs.CR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      3 pages, 3 figures, conference", "url": "http://arxiv.org/abs/2507.11943v1", "summary": "We propose a low-rank adaptation method for training privacy-preserving\nvision transformer (ViT) models that efficiently freezes pre-trained ViT model\nweights. In the proposed method, trainable rank decomposition matrices are\ninjected into each layer of the ViT architecture, and moreover, the patch\nembedding layer is not frozen, unlike in the case of the conventional low-rank\nadaptation methods. The proposed method allows us not only to reduce the number\nof trainable parameters but to also maintain almost the same accuracy as that\nof full-time tuning.", "comment": "3 pages, 3 figures, conference", "pdf_url": "http://arxiv.org/pdf/2507.11943v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12003", "title": "Expanding ML-Documentation Standards For Better Security", "authors": ["Cara Ellen Appel"], "categories": ["cs.CR", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the 33rd IEEE International Requirements Engineering Workshop (REW 2025)", "url": "http://arxiv.org/abs/2507.12003v1", "summary": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation.", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering Workshop (REW 2025)", "pdf_url": "http://arxiv.org/pdf/2507.12003v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12050", "title": "IDFace: Face Template Protection for Efficient and Secure Identification", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Dongsoo Kim", "Junbum Shin", "Jae Hong Seo"], "categories": ["cs.CR", "cs.CV", "I.5.4; K.6.5; D.4.6; I.4.7"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.12050v1", "summary": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12050v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12061", "title": "Toward an Intent-Based and Ontology-Driven Autonomic Security Response in Security Orchestration Automation and Response", "authors": ["Zequan Huang", "Jacques Robin", "Nicolas Herbaut", "Nourhène Ben Rabah", "Bénédicte Le Grand"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12061v1", "summary": "Modern Security Orchestration, Automation, and Response (SOAR) platforms must\nrapidly adapt to continuously evolving cyber attacks. Intent-Based Networking\nhas emerged as a promising paradigm for cyber attack mitigation through\nhigh-level declarative intents, which offer greater flexibility and persistency\nthan procedural actions. In this paper, we bridge the gap between two active\nresearch directions: Intent-Based Cyber Defense and Autonomic Cyber Defense, by\nproposing a unified, ontology-driven security intent definition leveraging the\nMITRE-D3FEND cybersecurity ontology. We also propose a general two-tiered\nmethodology for integrating such security intents into decision-theoretic\nAutonomic Cyber Defense systems, enabling hierarchical and context-aware\nautomated response capabilities. The practicality of our approach is\ndemonstrated through a concrete use case, showcasing its integration within\nnext-generation Security Orchestration, Automation, and Response platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12061v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12098", "title": "A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy", "authors": ["Xiang Li", "Yifan Lin", "Yuanzhe Zhang"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12098v1", "summary": "To mitigate privacy leakage and performance issues in personalized\nadvertising, this paper proposes a framework that integrates federated learning\nand differential privacy. The system combines distributed feature extraction,\ndynamic privacy budget allocation, and robust model aggregation to balance\nmodel accuracy, communication overhead, and privacy protection. Multi-party\nsecure computing and anomaly detection mechanisms further enhance system\nresilience against malicious attacks. Experimental results demonstrate that the\nframework achieves dual optimization of recommendation accuracy and system\nefficiency while ensuring privacy, providing both a practical solution and a\ntheoretical foundation for applying privacy protection technologies in\nadvertisement recommendation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12098v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12185", "title": "Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks", "authors": ["Rina Mishra", "Gaurav Varshney"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12185v1", "summary": "The advent of advanced Generative AI (GenAI) models such as DeepSeek and\nChatGPT has significantly reshaped the cybersecurity landscape, introducing\nboth promising opportunities and critical risks. This study investigates how\nGenAI powered chatbot services can be exploited via jailbreaking techniques to\nbypass ethical safeguards, enabling the generation of phishing content,\nrecommendation of hacking tools, and orchestration of phishing campaigns. In\nethically controlled experiments, we used ChatGPT 4o Mini selected for its\naccessibility and status as the latest publicly available model at the time of\nexperimentation, as a representative GenAI system. Our findings reveal that the\nmodel could successfully guide novice users in executing phishing attacks\nacross various vectors, including web, email, SMS (smishing), and voice\n(vishing). Unlike automated phishing campaigns that typically follow detectable\npatterns, these human-guided, AI assisted attacks are capable of evading\ntraditional anti phishing mechanisms, thereby posing a growing security threat.\nWe focused on DeepSeek and ChatGPT due to their widespread adoption and\ntechnical relevance in 2025. The study further examines common jailbreaking\ntechniques and the specific vulnerabilities exploited in these models. Finally,\nwe evaluate a range of mitigation strategies such as user education, advanced\nauthentication mechanisms, and regulatory policy measures and discuss emerging\ntrends in GenAI facilitated phishing, outlining future research directions to\nstrengthen cybersecurity defenses in the age of artificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12185v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12345", "title": "Efficient Control Flow Attestation by Speculating on Control Flow Path Representations", "authors": ["Liam Tyler", "Adam Caulfield", "Ivan De Oliveira Nunes"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12345v1", "summary": "Control Flow Attestation (CFA) allows remote verification of run-time\nsoftware integrity in embedded systems. However, CFA is limited by the\nstorage/transmission costs of generated control flow logs (CFlog). Recent work\nhas proposed application-specific optimizations by speculating on likely\nsub-paths in CFlog and replacing them with reserved symbols at runtime. Albeit\neffective, prior approaches do not consider the representation of addresses in\na control flow path for speculation. This work proposes RESPEC-CFA, an\narchitectural extension for CFA allowing for speculation on (1) the locality of\ncontrol flows and (2) their Huffman encoding. Alone, RESPEC-CFA reduces CFlog\nsizes by up to 90.1%. Combined with prior methods, RESPEC-CFA yields reductions\nof up to 99.7%, representing a significant step toward practical CFA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12345v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12364", "title": "Rethinking the confidential cloud through a unified low-level abstraction for composable isolation", "authors": ["Adrien Ghosn", "Charly Castes", "Neelu S. Kalani", "Yuchen Qian", "Marios Kogias", "Edouard Bugnion"], "categories": ["cs.CR", "cs.OS"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12364v1", "summary": "Securing sensitive cloud workloads requires composing confidential virtual\nmachines (CVMs) with nested enclaves or sandboxes. Unfortunately, each new\nisolation boundary adds ad-hoc access control mechanisms, hardware extensions,\nand trusted software. This escalating complexity bloats the TCB, complicates\nend-to-end attestation, and leads to fragmentation across platforms and cloud\nservice providers (CSPs).\n  We introduce a unified isolation model that delegates enforceable,\ncomposable, and attestable isolation to a single trusted security monitor:\nTyche. Tyche provides an API for partitioning, sharing, attesting, and\nreclaiming resources through its core abstraction, trust domains (TDs). To\nprovide fine-grain isolation, TDs can recursively create and manage sub-TDs.\nTyche captures these relationships in attestations, allowing cloud tenants to\nreason about end-to-end security. TDs serve as the building blocks for\nconstructing composable enclaves, sandboxes, and CVMs.\n  Tyche runs on commodity x86_64 without hardware security extensions and can\nmaintain backward compatibility with existing software. We provide an SDK to\nrun and compose unmodified workloads as sandboxes, enclaves, and CVMs with\nminimal overhead compared to native Linux execution. Tyche supports complex\ncloud scenarios, such as confidential inference with mutually distrustful\nusers, model owners, and CSPs. An additional RISC-V prototype demonstrates\nTyche's portability across platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12364v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11744", "title": "A Cellular Automata Approach to Donation Game", "authors": ["Marcin Kowalik", "Przemysław Stokłosa", "Mateusz Grabowski", "Janusz Starzyk", "Paweł Raif"], "categories": ["cs.MA", "cs.GT", "physics.soc-ph"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      16 pages, 12 figures", "url": "http://arxiv.org/abs/2507.11744v1", "summary": "The donation game is a well-established framework for studying the emergence\nand evolution of cooperation in multi-agent systems. The cooperative behavior\ncan be influenced by the environmental noise in partially observable settings\nand by the decision-making strategies of agents, which may incorporate not only\nreputation but also traits such as generosity and forgiveness. Traditional\nsimulations often assume fully random interactions, where cooperation is tested\nbetween randomly selected agent pairs. In this paper, we investigate\ncooperation dynamics using the concept of Stephen Wolfram's one-dimensional\nbinary cellular automata. This approach allows us to explore how cooperation\nevolves when interactions are limited to neighboring agents. We define binary\ncellular automata rules that conform to the donation game mechanics.\nAdditionally, we introduce models of perceptual and action noise, along with a\nmutation matrix governing the probabilistic evolution of agent strategies. Our\nempirical results demonstrate that cooperation is significantly affected by\nagents' mobility and their spatial locality on the game board. These findings\nhighlight the importance of distinguishing between entirely random multi-agent\nsystems and those in which agents are more likely to interact with their\nnearest neighbors.", "comment": "16 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.11744v1", "cate": "cs.MA", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12456", "title": "On One-Shot Signatures, Quantum vs Classical Binding, and Obfuscating Permutations", "authors": ["Omri Shmueli", "Mark Zhandry"], "categories": ["cs.CR", "quant-ph"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12456v1", "summary": "One-shot signatures (OSS) were defined by Amos, Georgiou, Kiayias, and\nZhandry (STOC'20). These allow for signing exactly one message, after which the\nsigning key self-destructs, preventing a second message from ever being signed.\nWhile such an object is impossible classically, Amos et al observe that OSS may\nbe possible using quantum signing keys by leveraging the no-cloning principle.\nOSS has since become an important conceptual tool with many applications in\ndecentralized settings and for quantum cryptography with classical\ncommunication. OSS are also closely related to separations between\nclassical-binding and collapse-binding for post-quantum hashing and\ncommitments. Unfortunately, the only known OSS construction due to Amos et al.\nwas only justified in a classical oracle model, and moreover their\njustification was ultimately found to contain a fatal bug. Thus, the existence\nof OSS, even in a classical idealized model, has remained open.\n  We give the first standard-model OSS, with provable security assuming\n(sub-exponential) indistinguishability obfuscation (iO) and LWE. This also\ngives the first standard-model separation between classical and\ncollapse-binding post-quantum commitments/hashing, solving a decade-old open\nproblem. Along the way, we also give the first construction with unconditional\nsecurity relative to a classical oracle. To achieve our standard-model\nconstruction, we develop a notion of permutable pseudorandom permutations\n(permutable PRPs), and show how they are useful for translating oracle proofs\ninvolving random permutations into obfuscation-based proofs. In particular,\nobfuscating permutable PRPs gives a trapdoor one-way permutation that is\n\\emph{full-domain}, solving another decade-old-problem of constructing this\nobject from (sub-exponential) iO and one-way functions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12456v1", "cate": "cs.CR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11906", "title": "CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models", "authors": ["Tadahiro Taniguchi", "Masatoshi Nagano", "Haruumi Omoto", "Yoshiki Hayashi"], "categories": ["cs.MA", "cs.HC"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11906v1", "summary": "Collective human activities like using an Ouija board (or Kokkuri-san) often\nproduce emergent, coherent linguistic outputs unintended by any single\nparticipant. While psychological explanations such as the ideomotor effect\nexist, a computational understanding of how decentralized, implicit linguistic\nknowledge fuses through shared physical interaction remains elusive. We\nintroduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this\nphenomenon as collective Langevin dynamics sampling from implicitly fused\nlanguage models. Each participant is represented as an agent associated with an\nenergy landscape derived from an internal language model reflecting linguistic\npriors, and agents exert stochastic forces based on local energy gradients. We\ntheoretically prove that the collective motion of the shared pointer\n(planchette) corresponds to Langevin MCMC sampling from the sum of individual\nenergy landscapes, representing fused collective knowledge. Simulations\nvalidate that CoCre-Sam dynamics effectively fuse different models and generate\nmeaningful character sequences, while ablation studies confirm the essential\nroles of collective interaction and stochasticity. Altogether, CoCre-Sam\nprovides a novel computational mechanism linking individual implicit knowledge,\nembodied collective action, and emergent linguistic phenomena, grounding these\ncomplex interactions in the principles of probabilistic sampling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11906v1", "cate": "cs.MA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11970", "title": "Obfuscation of Unitary Quantum Programs", "authors": ["Mi-Ying Huang", "Er-Cheng Tang"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11970v1", "summary": "Program obfuscation aims to hide the inner workings of a program while\npreserving its functionality. In the quantum setting, recent works have\nobtained obfuscation schemes for specialized classes of quantum circuits. For\ninstance, Bartusek, Brakerski, and Vaikuntanathan (STOC 2024) constructed a\nquantum state obfuscation scheme, which supports the obfuscation of quantum\nprograms represented as quantum states for pseudo-deterministic quantum\nprograms with classical inputs and outputs in the classical oracle model.\n  In this work, we improve upon existing results by constructing the first\nquantum state obfuscation scheme for unitary (or approximately unitary) quantum\nprograms supporting quantum inputs and outputs in the classical oracle model.\nAt the core of our obfuscation scheme are two novel ingredients: a functional\nquantum authentication scheme that allows key holders to learn specific\nfunctions of the authenticated quantum state with simulation-based security,\nand a compiler that represents an arbitrary quantum circuit as a projective\nlinear-plus-measurement quantum program described by a sequence of non-adaptive\nClifford gates interleaved with adaptive and compatible measurements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11970v1", "cate": "quant-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12400", "title": "Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment", "authors": ["Noble Harasha", "Cristina Gava", "Nancy Lynch", "Claudia Contini", "Frederik Mallmann-Trenn"], "categories": ["cs.MA", "cs.DM"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12400v1", "summary": "Deploying motile nanosized particles, also known as ``nanobots'', in the\nhuman body promises to improve selectivity in drug delivery and reduce side\neffects. We consider a swarm of nanobots locating a single cancerous region and\ntreating it by releasing an onboard payload of drugs at the site. At nanoscale,\nthe computation, communication, sensing, and locomotion capabilities of\nindividual agents are extremely limited, noisy, and/or nonexistent.\n  We present a general model to formally describe the individual and collective\nbehavior of agents in a colloidal environment, such as the bloodstream, for\ncancer detection and treatment by nanobots. This includes a feasible and\nprecise model of agent locomotion, inspired by actual nanoparticles that, in\nthe presence of an external chemical gradient, move towards areas of higher\nconcentration by means of self-propulsion. We present two variants of our\ngeneral model: The first assumes an endogenous chemical gradient that is fixed\nover time and centered at the targeted cancer site; the second is a more\nspeculative and dynamic variant in which agents themselves create and amplify a\nchemical gradient centered at the cancer site. In both settings, agents can\nsense the gradient and ascend it noisily, locating the cancer site more quickly\nthan via simple Brownian motion.\n  For the first variant of the model, we present simulation results to show the\nbehavior of agents under our locomotion model, as well as {analytical results}\nto bound the time it takes for the agents to reach the cancer site. For the\nsecond variant, simulation results highlight the collective benefit in having\nagents issue their own chemical signal. While arguably more speculative in its\nagent capability assumptions, this variant shows a significant improvement in\nruntime performance over the first variant, resulting from its chemical signal\namplification mechanism.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12400v1", "cate": "cs.MA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12084", "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation", "authors": ["Keke Gai", "Haochen Liang", "Jing Yu", "Liehuang Zhu", "Dusit Niyato"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12084v1", "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12084v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11660", "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics", "authors": ["Joao F. Rocha", "Ke Xu", "Xingzhi Sun", "Ananya Krishna", "Dhananjay Bhaskar", "Blanche Mongeon", "Morgan Craig", "Mark Gerstein", "Smita Krishnaswamy"], "categories": ["cs.LG", "cs.MA", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11660v1", "summary": "The advent of single-cell technology has significantly improved our\nunderstanding of cellular states and subpopulations in various tissues under\nnormal and diseased conditions by employing data-driven approaches such as\nclustering and trajectory inference. However, these methods consider cells as\nindependent data points of population distributions. With spatial\ntranscriptomics, we can represent cellular organization, along with dynamic\ncell-cell interactions that lead to changes in cell state. Still, key\ncomputational advances are necessary to enable the data-driven learning of such\ncomplex interactive cellular dynamics. While agent-based modeling (ABM)\nprovides a powerful framework, traditional approaches rely on handcrafted rules\nderived from domain knowledge rather than data-driven approaches. To address\nthis, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)\nintegrating ABM with deep learning to model intercellular communication, and\nits effect on the intracellular gene regulatory network. Using graph ODE\nnetworks (GDEs) with shared weights per cell type, our approach represents\ngenes as vertices and interactions as directed edges, dynamically learning\ntheir strengths through a designed attention mechanism. Trained to match\ncontinuous trajectories of simulated as well as inferred trajectories from\nspatial transcriptomics data, the model captures both intercellular and\nintracellular interactions, enabling a more adaptive and accurate\nrepresentation of cellular dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11660v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11621", "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 2 figures, 3 tables, accepted for IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025", "url": "http://arxiv.org/abs/2507.11621v1", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks.", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.11621v1", "cate": "cs.RO", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12107", "title": "Non-Adaptive Adversarial Face Generation", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Minsu Kim", "Jae Hong Seo"], "categories": ["cs.CV", "cs.AI", "cs.CR", "I.2.6; I.5.4; D.4.6; K.6.5; I.4.8"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12107v1", "summary": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12107v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11662", "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Our code and data are publicly available at this https URL", "url": "http://arxiv.org/abs/2507.11662v1", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "pdf_url": "http://arxiv.org/pdf/2507.11662v1", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11623", "title": "A Roadmap for Climate-Relevant Robotics Research", "authors": ["Alan Papalia", "Charles Dawson", "Laurentiu L. Anton", "Norhan Magdy Bayomi", "Bianca Champenois", "Jung-Hoon Cho", "Levi Cai", "Joseph DelPreto", "Kristen Edwards", "Bilha-Catherine Githinji", "Cameron Hickert", "Vindula Jayawardana", "Matthew Kramer", "Shreyaa Raghavan", "David Russell", "Shide Salimi", "Jingnan Shi", "Soumya Sudhakar", "Yanwei Wang", "Shouyi Wang", "Luca Carlone", "Vijay Kumar", "Daniela Rus", "John E. Fernandez", "Cathy Wu", "George Kantor", "Derek Young", "Hanumant Singh"], "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11623v1", "summary": "Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11623v1", "cate": "cs.RO", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12314", "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "authors": ["Zihao Xue", "Zhen Bi", "Long Ma", "Zhenlin Hu", "Yan Wang", "Zhenfang Liu", "Qing Sheng", "Jie Xiao", "Jungang Lou"], "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12314v1", "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12314v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11779", "title": "Large-scale distributed synchronization systems, using a cancel-on-completion redundancy mechanism", "authors": ["Alexander Stolyar"], "categories": ["math.PR", "cs.MA", "90B15, 60K25"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      34 pages", "url": "http://arxiv.org/abs/2507.11779v1", "summary": "We consider a class of multi-agent distributed synchronization systems, which\nare modeled as $n$ particles moving on the real line. This class generalizes\nthe model of a multi-server queueing system, considered in [15], employing\nso-called cancel-on-completion (c.o.c.) redundancy mechanism, but is motivated\nby other applications as well. The model in [15] is a particle system,\nregulated at the left boundary point. The more general model of this paper is\nsuch that we allow regulation boundaries on either side, or both sides, or no\nregulation at all. We consider the mean-field asymptotic regime, when the\nnumber of particles $n$ and the job arrival rates go to infinity, while the job\narrival rates per particle remain constant. The results include: the\nexistence/uniqueness of fixed points of mean-field limits (ML), which describe\nthe limiting dynamics of the system; conditions for the steady-state asymptotic\nindependence (concentration, as $n \\to\\infty$, of the stationary distribution\non a single state, which is necessarily an ML fixed point); the limits, as $n\n\\to\\infty$, of the average velocity at which unregulated (free) particle system\nadvances. In particular, our results for the left-regulated system unify and\ngeneralize the corresponding results in [15]. Our technical development is such\nthat the systems with different types of regulation are analyzed within a\nunified framework. In particular, these systems are used as tools for analysis\nof each other.", "comment": "34 pages", "pdf_url": "http://arxiv.org/pdf/2507.11779v1", "cate": "math.PR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11716", "title": "CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment", "authors": ["Yifan Xu", "Qianwei Wang", "Jordan Lillie", "Vineet Kamat", "Carol Menassa", "Clive D'Souza"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      13 pages, 10 figures", "url": "http://arxiv.org/abs/2507.11716v1", "summary": "As the global population of people with disabilities (PWD) continues to grow,\nso will the need for mobility solutions that promote independent living and\nsocial integration. Wheelchairs are vital for the mobility of PWD in both\nindoor and outdoor environments. The current SOTA in powered wheelchairs is\nbased on either manually controlled or fully autonomous modes of operation,\noffering limited flexibility and often proving difficult to navigate in\nspatially constrained environments. Moreover, research on robotic wheelchairs\nhas focused predominantly on complete autonomy or improved manual control;\napproaches that can compromise efficiency and user trust. To overcome these\nchallenges, this paper introduces the CoNav Chair, a smart wheelchair based on\nthe Robot Operating System (ROS) and featuring shared control navigation and\nobstacle avoidance capabilities that are intended to enhance navigational\nefficiency, safety, and ease of use for the user. The paper outlines the CoNav\nChair's design and presents a preliminary usability evaluation comparing three\ndistinct navigation modes, namely, manual, shared, and fully autonomous,\nconducted with 21 healthy, unimpaired participants traversing an indoor\nbuilding environment. Study findings indicated that the shared control\nnavigation framework had significantly fewer collisions and performed\ncomparably, if not superior to the autonomous and manual modes, on task\ncompletion time, trajectory length, and smoothness; and was perceived as being\nsafer and more efficient based on user reported subjective assessments of\nusability. Overall, the CoNav system demonstrated acceptable safety and\nperformance, laying the foundation for subsequent usability testing with end\nusers, namely, PWDs who rely on a powered wheelchair for mobility.", "comment": "13 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.11716v1", "cate": "cs.RO", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11595", "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "authors": ["Hengyue Zhao"], "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11595v1", "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11595v1", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12408", "title": "Bounding the asymptotic quantum value of all multipartite compiled non-local games", "authors": ["Matilde Baroni", "Dominik Leichtle", "Siniša Janković", "Ivan Šupić"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      74 pages, 14 figures", "url": "http://arxiv.org/abs/2507.12408v1", "summary": "Non-local games are a powerful tool to distinguish between correlations\npossible in classical and quantum worlds. Kalai et al. (STOC'23) proposed a\ncompiler that converts multipartite non-local games into interactive protocols\nwith a single prover, relying on cryptographic tools to remove the assumption\nof physical separation of the players. While quantum completeness and classical\nsoundness of the construction have been established for all multipartite games,\nquantum soundness is known only in the special case of bipartite games.\n  In this paper, we prove that the Kalai et al.'s compiler indeed achieves\nquantum soundness for all multipartite compiled non-local games, by showing\nthat any correlations that can be generated in the asymptotic case correspond\nto quantum commuting strategies.\n  Our proof uses techniques from the theory of operator algebras, and relies on\na characterisation of sequential operationally no-signalling strategies as\nquantum commuting operator strategies in the multipartite case, thereby\ngeneralising several previous results. On the way, we construct universal\nC*-algebras of sequential PVMs and prove a new chain rule for Radon-Nikodym\nderivatives of completely positive maps on C*-algebras which may be of\nindependent interest.", "comment": "74 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.12408v1", "cate": "quant-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11979", "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "categories": ["cs.CL", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11979v1", "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11979v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11770", "title": "Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies", "authors": ["Giang Nguyen", "Mihai Pomarlan", "Sascha Jongebloed", "Nils Leusmann", "Minh Nhat Vu", "Michael Beetz"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2025)", "url": "http://arxiv.org/abs/2507.11770v1", "summary": "In robotics, the effective integration of environmental data into actionable\nknowledge remains a significant challenge due to the variety and\nincompatibility of data formats commonly used in scene descriptions, such as\nMJCF, URDF, and SDF. This paper presents a novel approach that addresses these\nchallenges by developing a unified scene graph model that standardizes these\nvaried formats into the Universal Scene Description (USD) format. This\nstandardization facilitates the integration of these scene graphs with robot\nontologies through semantic reporting, enabling the translation of complex\nenvironmental data into actionable knowledge essential for cognitive robotic\ncontrol. We evaluated our approach by converting procedural 3D environments\ninto USD format, which is then annotated semantically and translated into a\nknowledge graph to effectively answer competency questions, demonstrating its\nutility for real-time robotic decision-making. Additionally, we developed a\nweb-based visualization tool to support the semantic mapping process, providing\nusers with an intuitive interface to manage the 3D environment.", "comment": "8 pages, 7 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS2025)", "pdf_url": "http://arxiv.org/pdf/2507.11770v1", "cate": "cs.RO", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11633", "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages, ICML MAS workshop", "url": "http://arxiv.org/abs/2507.11633v1", "summary": "We introduce a modular harness design for LLM agents that composes of\nperception, memory, and reasoning components, enabling a single LLM or VLM\nbackbone to tackle a wide spectrum of multi turn gaming environments without\ndomain-specific engineering. Using classic and modern game suites as\nlow-barrier, high-diversity testbeds, our framework provides a unified workflow\nfor analyzing how each module affects performance across dynamic interactive\nsettings. Extensive experiments demonstrate that the harness lifts gameplay\nperformance consistently over un-harnessed baselines and reveals distinct\ncontribution patterns, for example, memory dominates in long-horizon puzzles\nwhile perception is critical in vision noisy arcades. These findings highlight\nthe effectiveness of our modular harness design in advancing general-purpose\nagent, given the familiarity and ubiquity of games in everyday human\nexperience.", "comment": "8 pages, ICML MAS workshop", "pdf_url": "http://arxiv.org/pdf/2507.11633v1", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12439", "title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "authors": ["Daniel Commey", "Rebecca A. Sarpong", "Griffith S. Klogo", "Winful Bagyl-Bac", "Garth V. Crosby"], "categories": ["cs.LG", "cs.CR", "cs.GT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12439v1", "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12439v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12174", "title": "Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties", "authors": ["Zhenmin Huang", "Yusen Xie", "Benshan Ma", "Shaojie Shen", "Jun Ma"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12174v1", "summary": "Trajectory planning involving multi-agent interactions has been a\nlong-standing challenge in the field of robotics, primarily burdened by the\ninherent yet intricate interactions among agents. While game-theoretic methods\nare widely acknowledged for their effectiveness in managing multi-agent\ninteractions, significant impediments persist when it comes to accommodating\nthe intentional uncertainties of agents. In the context of intentional\nuncertainties, the heavy computational burdens associated with existing\ngame-theoretic methods are induced, leading to inefficiencies and poor\nscalability. In this paper, we propose a novel game-theoretic interactive\ntrajectory planning method to effectively address the intentional uncertainties\nof agents, and it demonstrates both high efficiency and enhanced scalability.\nAs the underpinning basis, we model the interactions between agents under\nintentional uncertainties as a general Bayesian game, and we show that its\nagent-form equivalence can be represented as a potential game under certain\nminor assumptions. The existence and attainability of the optimal interactive\ntrajectories are illustrated, as the corresponding Bayesian Nash equilibrium\ncan be attained by optimizing a unified optimization problem. Additionally, we\npresent a distributed algorithm based on the dual consensus alternating\ndirection method of multipliers (ADMM) tailored to the parallel solving of the\nproblem, thereby significantly improving the scalability. The attendant\noutcomes from simulations and experiments demonstrate that the proposed method\nis effective across a range of scenarios characterized by general forms of\nintentional uncertainties. Its scalability surpasses that of existing\ncentralized and decentralized baselines, allowing for real-time interactive\ntrajectory planning in uncertain game settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12174v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11840", "title": "The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey", "authors": ["Gaofeng Li", "Ruize Wang", "Peisen Xu", "Qi Ye", "Jiming Chen"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11840v1", "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11840v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11733", "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making", "authors": ["Srikanth Vemula"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11733v1", "summary": "This Study introduces Clarity and Reasoning Interface for Artificial\nIntelligence(ClarifAI), a novel approach designed to augment the transparency\nand interpretability of artificial intelligence (AI) in the realm of improved\ndecision making. Leveraging the Case-Based Reasoning (CBR) methodology and\nintegrating an ontology-driven approach, ClarifAI aims to meet the intricate\nexplanatory demands of various stakeholders involved in AI-powered\napplications. The paper elaborates on ClarifAI's theoretical foundations,\ncombining CBR and ontologies to furnish exhaustive explanation mechanisms. It\nfurther elaborates on the design principles and architectural blueprint,\nhighlighting ClarifAI's potential to enhance AI interpretability across\ndifferent sectors and its applicability in high-stake environments. This\nresearch delineates the significant role of ClariAI in advancing the\ninterpretability of AI systems, paving the way for its deployment in critical\ndecision-making processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11733v1", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2412.05737", "title": "Balancing Confidentiality and Transparency for Blockchain-based Process-Aware Information Systems", "authors": ["Alessandro Marcelletti", "Edoardo Marangone", "Michele Kryston", "Claudio Di Ciccio"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.05737v3", "summary": "Blockchain enables novel, trustworthy Process-Aware Information Systems\n(PAISs) by enforcing the security, robustness, and traceability of operations.\nIn particular, transparency ensures that all information exchanges are openly\naccessible, fostering trust within the system. Although this is a desirable\nproperty to enable notarization and auditing activities, it also represents a\nlimitation for such cases where confidentiality is a requirement since\ninteractions involve sensitive data. Current solutions rely on obfuscation\ntechniques or private infrastructures, hindering the enforcement capabilities\nof smart contracts and the public verifiability of transactions. Against this\nbackground, we propose CONFETTY, an architecture for blockchain-based PAISs to\npreserve confidentiality and transparency. Smart contracts enact, enforce and\nstore public interactions, while attribute-based encryption techniques are\nadopted to specify access grants to confidential information. We assess the\nsecurity of our solution through a systematic threat model analysis and\nevaluate its practical feasibility by gauging the performance of our\nimplemented prototype in different scenarios from the literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.05737v3", "cate": "cs.CR", "date": "2024-12-07", "updated": "2025-07-16"}
{"id": "2401.11212", "title": "Programming Distributed Collective Processes in the eXchange Calculus", "authors": ["Giorgio Audrito", "Roberto Casadei", "Ferruccio Damiani", "Gianluca Torta", "Mirko Viroli"], "categories": ["cs.DC", "cs.AI", "cs.MA", "cs.PL", "D.1.3; F.1.1; F.4.3; I.2.11; J.7"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      41 pages, 17 figures", "url": "http://arxiv.org/abs/2401.11212v5", "summary": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.", "comment": "41 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2401.11212v5", "cate": "cs.DC", "date": "2024-01-20", "updated": "2025-07-16"}
{"id": "2507.11852", "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "authors": ["Mohammed Hassanin", "Mohammad Abu Alsheikh", "Carlos C. N. Kuhn", "Damith Herath", "Dinh Thai Hoang", "Ibrahim Radwan"], "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2507.11852v1", "summary": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2507.11852v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11737", "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "authors": ["Chenyu Zhou", "Jingyuan Yang", "Linwei Xin", "Yitian Chen", "Ziyan He", "Dongdong Ge"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11737v1", "summary": "Dynamic programming (DP) is a fundamental method in operations research, but\nformulating DP models has traditionally required expert knowledge of both the\nproblem context and DP techniques. Large Language Models (LLMs) offer the\npotential to automate this process. However, DP problems pose unique challenges\ndue to their inherently stochastic transitions and the limited availability of\ntraining data. These factors make it difficult to directly apply existing\nLLM-based models or frameworks developed for other optimization problems, such\nas linear or integer programming. We introduce DP-Bench, the first benchmark\ncovering a wide range of textbook-level DP problems to enable systematic\nevaluation. We present Dynamic Programming Language Model (DPLM), a\n7B-parameter specialized model that achieves performance comparable to\nstate-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on\nhard problems. Central to DPLM's effectiveness is DualReflect, our novel\nsynthetic data generation pipeline, designed to scale up training data from a\nlimited set of initial examples. DualReflect combines forward generation for\ndiversity and backward generation for reliability. Our results reveal a key\ninsight: backward generation is favored in low-data regimes for its strong\ncorrectness guarantees, while forward generation, though lacking such\nguarantees, becomes increasingly valuable at scale for introducing diverse\nformulations. This trade-off highlights the complementary strengths of both\napproaches and the importance of combining them.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11737v1", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2503.23175", "title": "Large Language Models are Unreliable for Cyber Threat Intelligence", "authors": ["Emanuele Mezzi", "Fabio Massacci", "Katja Tuma"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.23175v2", "summary": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.23175v2", "cate": "cs.CR", "date": "2025-03-29", "updated": "2025-07-16"}
{"id": "2411.11192", "title": "Robot Metabolism: Towards machines that can grow by consuming other machines", "authors": ["Philippe Martin Wyder", "Riyaan Bakhda", "Meiqi Zhao", "Quinn A. Booth", "Matthew E. Modi", "Andrew Song", "Simon Kang", "Jiahao Wu", "Priya Patel", "Robert T. Kasumi", "David Yi", "Nihar Niraj Garg", "Pranav Jhunjhunwala", "Siddharth Bhutoria", "Evan H. Tong", "Yuhang Hu", "Judah Goldfeder", "Omer Mustel", "Donghan Kim", "Hod Lipson"], "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "70-01, 68-02", "I.6; H.4; H.m; I.m; B.m"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Manuscript combined with Supplementary Materials File for arXiv submission", "url": "http://arxiv.org/abs/2411.11192v2", "summary": "Biological lifeforms can heal, grow, adapt, and reproduce -- abilities\nessential for sustained survival and development. In contrast, robots today are\nprimarily monolithic machines with limited ability to self-repair, physically\ndevelop, or incorporate material from their environments. While robot minds\nrapidly evolve new behaviors through AI, their bodies remain closed systems,\nunable to systematically integrate material to grow or heal. We argue that\nopen-ended physical adaptation is only possible when robots are designed using\na small repertoire of simple modules. This allows machines to mechanically\nadapt by consuming parts from other machines or their surroundings and shed\nbroken components. We demonstrate this principle on a truss modular robot\nplatform. We show how robots can grow bigger, faster, and more capable by\nconsuming materials from their environment and other robots. We suggest that\nmachine metabolic processes like those demonstrated here will be an essential\npart of any sustained future robot ecology.", "comment": "Manuscript combined with Supplementary Materials File for arXiv\n  submission", "pdf_url": "http://arxiv.org/pdf/2411.11192v2", "cate": "cs.RO", "date": "2024-11-17", "updated": "2025-07-16"}
{"id": "2507.11880", "title": "A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications", "authors": ["Jinyuan Liu", "Minglei Fu", "Ling Shi", "Chenguang Yang", "Wenan Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      37 pages, 33 figures", "url": "http://arxiv.org/abs/2507.11880v1", "summary": "Tethered robots play a pivotal role in specialized environments such as\ndisaster response and underground exploration, where their stable power supply\nand reliable communication offer unparalleled advantages. However, their motion\nplanning is severely constrained by tether length limitations and entanglement\nrisks, posing significant challenges to achieving optimal path planning. To\naddress these challenges, this study introduces CDT-TCS (Convex Dissection\nTopology-based Tethered Configuration Search), a novel algorithm that leverages\nCDT Encoding as a homotopy invariant to represent topological states of paths.\nBy integrating algebraic topology with geometric optimization, CDT-TCS\nefficiently computes the complete set of optimal feasible configurations for\ntethered robots at all positions in 2D environments through a single\ncomputation. Building on this foundation, we further propose three\napplication-specific algorithms: i) CDT-TPP for optimal tethered path planning,\nii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for\ndistance-optimal path planning of untethered robots. All theoretical results\nand propositions underlying these algorithms are rigorously proven and\nthoroughly discussed in this paper. Extensive simulations demonstrate that the\nproposed algorithms significantly outperform state-of-the-art methods in their\nrespective problem domains. Furthermore, real-world experiments on robotic\nplatforms validate the practicality and engineering value of the proposed\nframework.", "comment": "37 pages, 33 figures", "pdf_url": "http://arxiv.org/pdf/2507.11880v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11787", "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "categories": ["cs.AI", "68-68W50"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      CSAIDE '25: Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy", "url": "http://arxiv.org/abs/2507.11787v1", "summary": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial\nintelligence, where the natural behavior of animals and insects is observed and\ntranslated into computer algorithms called swarm computing to solve real-world\nproblems. Due to their effectiveness, they are applied in solving various\ncomputer optimization problems. This survey will review all the latest\ndevelopments in Searching for documents based on semantic similarity using\nSwarm Intelligence algorithms and recommend future research directions.", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "pdf_url": "http://arxiv.org/pdf/2507.11787v1", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11547", "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming", "authors": ["Yingxue Zhao", "Qianyi Chen", "Haoran Li", "Haosu Zhou", "Hamid Reza Attar", "Tobias Pfaff", "Tailin Wu", "Nan Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11547v1", "summary": "In recent years, various artificial intelligence-based surrogate models have\nbeen proposed to provide rapid manufacturability predictions of material\nforming processes. However, traditional AI-based surrogate models, typically\nbuilt with scalar or image-based neural networks, are limited in their ability\nto capture complex 3D spatial relationships and to operate in a\npermutation-invariant manner. To overcome these issues, emerging graph-based\nsurrogate models are developed using graph neural networks. This study\ndeveloped a new graph neural network surrogate model named Recurrent U\nNet-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate\npredictions of sheet material deformation fields across multiple forming\ntimesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model\ntemporal dynamics and a U-Net inspired graph-based downsample/upsample\nmechanism to handle spatial long-range dependencies. A novel 'node-to-surface'\ncontact representation method was proposed, offering significant improvements\nin computational efficiency for large-scale contact interactions. The RUGNN\nmodel was validated using a cold forming case study and a more complex hot\nforming case study using aluminium alloys. Results demonstrate that the RUGNN\nmodel provides accurate deformation predictions closely matching ground truth\nFE simulations and outperforming several baseline GNN architectures. Model\ntuning was also performed to identify suitable hyperparameters, training\nstrategies, and input feature representations. These results demonstrate that\nRUGNN is a reliable approach to support sheet material forming design by\nenabling accurate manufacturability predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11547v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.11599", "title": "Neuroaesthetics and the Science of Visual Experience", "authors": ["Harish Vijayakumar"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.11599v1", "summary": "Neuroaesthetics is an interdisciplinary field that brings together\nneuroscience, psychology, and the arts to explore how the human brain perceives\nand responds to visual beauty. This paper examines the neural mechanisms behind\naesthetic experiences, aiming to explain why certain designs or artworks feel\nemotionally or cognitively \"right.\" By analyzing the interaction between\nperception, emotion, and cognition, neuroaesthetics reveals how beauty is\nconstructed in the brain and how this understanding can inform fields such as\ngraphic and interface design. This paper offers a clear and accessible overview\nof core neuroaesthetic principles, making the subject approachable to a wide\naudience. The findings suggest that impactful design is more than surface-level\nappeal: well-crafted visual experiences can engage, support, and connect people\nin meaningful ways.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.11599v1", "cate": "cs.HC", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2504.21042", "title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift", "authors": ["Jiamin Chang", "Haoyang Li", "Hammond Pearce", "Ruoxi Sun", "Bo Li", "Minhui Xue"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to The ACM Conference on Computer and Communications Security (CCS) 2025", "url": "http://arxiv.org/abs/2504.21042v3", "summary": "The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.", "comment": "Accepted to The ACM Conference on Computer and Communications\n  Security (CCS) 2025", "pdf_url": "http://arxiv.org/pdf/2504.21042v3", "cate": "cs.CR", "date": "2025-04-28", "updated": "2025-07-16"}
{"id": "2503.02445", "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling", "authors": ["Hao Li", "Yu-Hao Huang", "Chang Xu", "Viktor Schlegel", "Renhe Jiang", "Riza Batista-Navarro", "Goran Nenadic", "Jiang Bian"], "categories": ["cs.LG", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 Main Conference", "url": "http://arxiv.org/abs/2503.02445v5", "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.", "comment": "ICML 2025 Main Conference", "pdf_url": "http://arxiv.org/pdf/2503.02445v5", "cate": "cs.LG", "date": "2025-03-04", "updated": "2025-07-16"}
{"id": "2507.11889", "title": "NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy", "authors": ["Adnan Abdullah", "Alankrit Gupta", "Vaishnav Ramesh", "Shivali Patel", "Md Jahidul Islam"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      10 pages, V1", "url": "http://arxiv.org/abs/2507.11889v1", "summary": "Adaptive mission control and dynamic parameter reconfiguration are essential\nfor autonomous underwater vehicles (AUVs) operating in GPS-denied,\ncommunication-limited marine environments. However, most current AUV platforms\nexecute static, pre-programmed missions or rely on tethered connections and\nhigh-latency acoustic channels for mid-mission updates, significantly limiting\ntheir adaptability and responsiveness. In this paper, we introduce NemeSys, a\nnovel AUV system designed to support real-time mission reconfiguration through\ncompact optical and magnetoelectric (OME) signaling facilitated by floating\nbuoys. We present the full system design, control architecture, and a semantic\nmission encoding framework that enables interactive exploration and task\nadaptation via low-bandwidth communication. The proposed system is validated\nthrough analytical modeling, controlled experimental evaluations, and\nopen-water trials. Results confirm the feasibility of online mission adaptation\nand semantic task updates, highlighting NemeSys as an online AUV platform for\ngoal-driven adaptive autonomy in dynamic and uncertain underwater environments.", "comment": "10 pages, V1", "pdf_url": "http://arxiv.org/pdf/2507.11889v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11916", "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "categories": ["cs.AI", "cs.DC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11916v1", "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11916v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11570", "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery", "authors": ["Ha Na Cho", "Sairam Sutari", "Alexander Lopez", "Hansen Bow", "Kai Zheng"], "categories": ["cs.LG", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11570v1", "summary": "Objective: To develop and evaluate machine learning (ML) models for\npredicting length of stay (LOS) in elective spine surgery, with a focus on the\nbenefits of temporal modeling and model interpretability. Materials and\nMethods: We compared traditional ML models (e.g., linear regression, random\nforest, support vector machine (SVM), and XGBoost) with our developed model,\nSurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an\nattention, using structured perioperative electronic health records (EHR) data.\nPerformance was evaluated using the coefficient of determination (R2), and key\npredictors were identified using explainable AI. Results: SurgeryLSTM achieved\nthe highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)\nand baseline models. The attention mechanism improved interpretability by\ndynamically identifying influential temporal segments within preoperative\nclinical sequences, allowing clinicians to trace which events or features most\ncontributed to each LOS prediction. Key predictors of LOS included bone\ndisorder, chronic kidney disease, and lumbar fusion identified as the most\nimpactful predictors of LOS. Discussion: Temporal modeling with attention\nmechanisms significantly improves LOS prediction by capturing the sequential\nnature of patient data. Unlike static models, SurgeryLSTM provides both higher\naccuracy and greater interpretability, which are critical for clinical\nadoption. These results highlight the potential of integrating attention-based\ntemporal models into hospital planning workflows. Conclusion: SurgeryLSTM\npresents an effective and interpretable AI solution for LOS prediction in\nelective spine surgery. Our findings support the integration of temporal,\nexplainable ML approaches into clinical decision support systems to enhance\ndischarge readiness and individualized patient care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11570v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11628", "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling", "authors": ["Jiangnan Xu", "Haeseul Cha", "Gosu Choi", "Gyu-cheol Lee", "Yeo-Jin Yoon", "Zucheul Lee", "Konstantinos Papangelis", "Dae Hyun Kim", "Juho Kim"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11628v1", "summary": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11628v1", "cate": "cs.HC", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11831", "title": "Generative Intelligence Systems in the Flow of Group Emotions", "authors": ["Fernando Koch", "Jessica Nahulan", "Jeremy Fox", "Martin Keen"], "categories": ["cs.ET"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "Comments:      8 pages, 10 figures", "url": "http://arxiv.org/abs/2507.11831v1", "summary": "Emotional cues frequently arise and shape group dynamics in interactive\nsettings where multiple humans and artificial agents communicate through shared\ndigital channels. While artificial agents lack intrinsic emotional states, they\ncan simulate affective behavior using synthetic modalities such as text or\nspeech. This work introduces a model for orchestrating emotion contagion,\nenabling agents to detect emotional signals, infer group mood patterns, and\ngenerate targeted emotional responses. The system captures human emotional\nexchanges and uses this insight to produce adaptive, generative responses that\ninfluence group affect in real time. The model supports applications in\ncollaborative, educational, and social environments by shifting affective\ncomputing from individual-level reactions to coordinated, group-level emotion\nmodulation. We present the system architecture and provide experimental results\nthat illustrate its effectiveness in sensing and steering group mood dynamics.", "comment": "8 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.11831v1", "cate": "cs.ET", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.00894", "title": "Non-Adaptive Cryptanalytic Time-Space Lower Bounds via a Shearer-like Inequality for Permutations", "authors": ["Itai Dinur", "Nathan Keller", "Avichai Marmor"], "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00894v2", "summary": "The power of adaptivity in algorithms has been intensively studied in diverse\nareas of theoretical computer science. In this paper, we obtain a number of\nsharp lower bound results which show that adaptivity provides a significant\nextra power in cryptanalytic time-space tradeoffs with (possibly unlimited)\npreprocessing time.\n  Most notably, we consider the discrete logarithm (DLOG) problem in a generic\ngroup of $N$ elements. The classical `baby-step giant-step' algorithm for the\nproblem has time complexity $T=O(\\sqrt{N})$, uses $O(\\sqrt{N})$ bits of space\n(up to logarithmic factors in $N$) and achieves constant success probability.\n  We examine a generalized setting where an algorithm obtains an advice string\nof $S$ bits and is allowed to make $T$ arbitrary non-adaptive queries that\ndepend on the advice string (but not on the challenge group element).\n  We show that in this setting, the $T=O(\\sqrt{N})$ online time complexity of\nthe baby-step giant-step algorithm cannot be improved, unless the advice string\nis more than $\\Omega(\\sqrt{N})$ bits long. This lies in stark contrast with the\nclassical adaptive Pollard's rho algorithm for DLOG, which can exploit\npreprocessing to obtain the tradeoff curve $ST^2=O(N)$. We obtain similar sharp\nlower bounds for several other cryptanalytic problems.\n  To obtain our results, we present a new model that allows analyzing\nnon-adaptive preprocessing algorithms for a wide array of search and decision\nproblems in a unified way. Since previous proof techniques inherently cannot\ndistinguish between adaptive and non-adaptive algorithms for the problems in\nour model, they cannot be used to obtain our results. Consequently, our proof\nuses a variant of Shearer's lemma for this setting, due to Barthe,\nCordero-Erausquin, Ledoux, and Maurey (2011). This seems to be the first time a\nvariant of Shearer's lemma for permutations is used in an algorithmic context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00894v2", "cate": "cs.CR", "date": "2025-05-01", "updated": "2025-07-16"}
{"id": "2507.08958", "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language Model System", "authors": ["Xiaowen Zhang", "Zhenyu Bi", "Patrick Lachance", "Xuan Wang", "Tiziana Di Matteo", "Rupert A. C. Croft"], "categories": ["astro-ph.IM", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures", "url": "http://arxiv.org/abs/2507.08958v2", "summary": "As cosmological simulations and their associated software become increasingly\ncomplex, physicists face the challenge of searching through vast amounts of\nliterature and user manuals to extract simulation parameters from dense\nacademic papers, each using different models and formats. Translating these\nparameters into executable scripts remains a time-consuming and error-prone\nprocess. To improve efficiency in physics research and accelerate the\ncosmological simulation process, we introduce SimAgents, a multi-agent system\ndesigned to automate both parameter configuration from the literature and\npreliminary analysis for cosmology research. SimAgents is powered by\nspecialized LLM agents capable of physics reasoning, simulation software\nvalidation, and tool execution. These agents collaborate through structured\ncommunication, ensuring that extracted parameters are physically meaningful,\ninternally consistent, and software-compliant. We also construct a cosmological\nparameter extraction evaluation dataset by collecting over 40 simulations in\npublished papers from Arxiv and leading journals that cover diverse simulation\ntypes. Experiments on the dataset demonstrate a strong performance of\nSimAgents, highlighting its effectiveness and potential to accelerate\nscientific research for physicists. Our demonstration video is available at:\nhttps://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly\navailable at https://github.com/xwzhang98/SimAgents.", "comment": "6 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.08958v2", "cate": "astro-ph.IM", "date": "2025-07-11", "updated": "2025-07-15"}
{"id": "2507.11920", "title": "Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments", "authors": ["Jeongyong Yang", "KwangBin Lee", "SooJean Han"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11920v1", "summary": "Real-time path planning in dense, uncertain environments remains a\nchallenging problem, as predicting the future motions of numerous dynamic\nobstacles is computationally burdensome and unrealistic. To address this, we\nintroduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a\nprediction-based risk-aware path-planning framework which uses a hybrid\ncombination of models to predict local obstacle movement. HyPRAP uses a novel\nPrediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by\neach obstacle, enabling the selective use of predictors based on whether the\nagent prioritizes high predictive accuracy or low computational prediction\noverhead. This selective routing enables the agent to focus on high-risk\nobstacles while ignoring or simplifying low-risk ones, making it suitable for\nenvironments with a large number of obstacles. Moreover, HyPRAP incorporates\nuncertainty quantification through hybrid conformal prediction by deriving\nconfidence bounds simultaneously achieved by multiple predictions across\ndifferent models. Theoretical analysis demonstrates that HyPRAP effectively\nbalances safety and computational efficiency by leveraging the diversity of\nprediction models. Extensive simulations validate these insights for more\ngeneral settings, confirming that HyPRAP performs better compared to single\npredictor methods, and P-CRI performs better over naive proximity-based risk\nassessment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11920v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11988", "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "authors": ["Yexuan Shi", "Mingyu Wang", "Yunxiang Cao", "Hongjie Lai", "Junjian Lan", "Xin Han", "Yu Wang", "Jie Geng", "Zhenan Li", "Zihao Xia", "Xiang Chen", "Chen Li", "Jian Xu", "Wenbo Duan", "Yuanshuo Zhu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      14 pages, 1 figures,", "url": "http://arxiv.org/abs/2507.11988v1", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "comment": "14 pages, 1 figures,", "pdf_url": "http://arxiv.org/pdf/2507.11988v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11574", "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators", "authors": ["Kazuma Kobayashi", "Shailesh Garg", "Farid Ahmed", "Souvik Chakraborty", "Syed Bahauddin Alam"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11574v1", "summary": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe\ndeployment of deep learning in real-time virtual sensing, particularly in\nhigh-stakes domains where sparse, noisy, or non-collocated sensor data are the\nnorm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework\nthat transforms neural operator-based virtual sensing with calibrated,\ndistribution-free prediction intervals. By unifying Monte Carlo dropout with\nsplit conformal prediction in a single DeepONet architecture, CMCO achieves\nspatially resolved uncertainty estimates without retraining, ensembling, or\ncustom loss design. Our method addresses a longstanding challenge: how to endow\noperator learning with efficient and reliable UQ across heterogeneous domains.\nThrough rigorous evaluation on three distinct applications: turbulent flow,\nelastoplastic deformation, and global cosmic radiation dose estimation-CMCO\nconsistently attains near-nominal empirical coverage, even in settings with\nstrong spatial gradients and proxy-based sensing. This breakthrough offers a\ngeneral-purpose, plug-and-play UQ solution for neural operators, unlocking\nreal-time, trustworthy inference in digital twins, sensor fusion, and\nsafety-critical monitoring. By bridging theory and deployment with minimal\ncomputational overhead, CMCO establishes a new foundation for scalable,\ngeneralizable, and uncertainty-aware scientific machine learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11574v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11677", "title": "CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations", "authors": ["Mashrur Rashik", "Jean-Daniel Fekete", "Narges Mahyar"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To appear in the IEEE Visualization and Visual Analytics (VIS) Conference, Short Paper, 2025", "url": "http://arxiv.org/abs/2507.11677v1", "summary": "Communicating climate change remains challenging, as climate reports, though\nrich in data and visualizations, often feel too abstract or technical for the\npublic. Although personalization can enhance communication, most tools still\nlack the narrative and visualization tailoring needed to connect with\nindividual experiences. We present CLAImate, an AI-enabled prototype that\npersonalizes conversation narratives and localizes visualizations based on\nusers' climate knowledge and geographic location. We evaluated CLAImate through\ninternal verification of factual correctness, a formative study with experts,\nand a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70%\nFACTSCORE. Visualization experts appreciated its clarity and personalization,\nand seven out of ten UK participants reported better understanding and local\nrelevance of climate risks with CLAImate. We also discuss design challenges in\npersonalization, accuracy, and scalability, and outline future directions for\nintegrating visualizations in personalized conversational interfaces.", "comment": "To appear in the IEEE Visualization and Visual Analytics (VIS)\n  Conference, Short Paper, 2025", "pdf_url": "http://arxiv.org/pdf/2507.11677v1", "cate": "cs.HC", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12373", "title": "Emerging Paradigms in the Energy Sector: Forecasting and System Control Optimisation", "authors": ["Dariush Pourkeramati", "Gareth Wadge", "Rachel Hassall", "Charlotte Mitchell", "Anish Khadka", "Shiwang Jaiswal", "Andrew Duncan", "Rossella Arcucci"], "categories": ["cs.ET"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12373v1", "summary": "The energy sector is experiencing rapid transformation due to increasing\nrenewable energy integration, decentralisation of power systems, and a\nheightened focus on efficiency and sustainability. With energy demand becoming\nincreasingly dynamic and generation sources more variable, advanced forecasting\nand optimisation strategies are crucial for maintaining grid stability,\ncost-effectiveness, and environmental sustainability. This paper explores\nemerging paradigms in energy forecasting and management, emphasizing four\ncritical domains: Energy Demand Forecasting integrated with Weather Data,\nBuilding Energy Optimisation, Heat Network Optimisation, and Energy Management\nSystem (EMS) Optimisation within a System of Systems (SoS) framework.\nLeveraging machine learning techniques and Model Predictive Control (MPC), the\nstudy demonstrates substantial enhancements in energy efficiency across scales\n-- from individual buildings to complex interconnected energy networks.\nWeather-informed demand forecasting significantly improves grid resilience and\nresource allocation strategies. Smart building optimisation integrates\npredictive analytics to substantially reduce energy consumption without\ncompromising occupant comfort. Optimising CHP-based heat networks achieves cost\nand carbon savings while adhering to operational and asset constraints. At the\nsystems level, sophisticated EMS optimisation ensures coordinated control of\ndistributed resources, storage solutions, and demand-side flexibility. Through\nreal-world case studies we highlight the potential of AI-driven automation and\nintegrated control solutions in facilitating a resilient, efficient, and\nsustainable energy future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12373v1", "cate": "cs.ET", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.18384", "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "authors": ["Boyi Wei", "Benedikt Stroebl", "Jiacen Xu", "Joie Zhang", "Zhou Li", "Peter Henderson"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      26 pages, 11 figures", "url": "http://arxiv.org/abs/2505.18384v3", "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.", "comment": "26 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2505.18384v3", "cate": "cs.CR", "date": "2025-05-23", "updated": "2025-07-16"}
{"id": "2507.10644", "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "authors": ["Tatiana Petrova", "Boris Bliznioukov", "Aleksandr Puzikov", "Radu State"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      33 pages, 9 figures, 8 tables", "url": "http://arxiv.org/abs/2507.10644v2", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.", "comment": "33 pages, 9 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2507.10644v2", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.11938", "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "authors": ["Hao Chen", "Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE T-RO", "url": "http://arxiv.org/abs/2507.11938v1", "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.", "comment": "Accepted by IEEE T-RO", "pdf_url": "http://arxiv.org/pdf/2507.11938v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11992", "title": "Understanding visual attention beehind bee-inspired UAV navigation", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11992v1", "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11992v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11589", "title": "Einstein Fields: A Neural Perspective To Computational General Relativity", "authors": ["Sandeep Suresh Cranganore", "Andrei Bodnar", "Arturs Berzins", "Johannes Brandstetter"], "categories": ["cs.LG", "gr-qc"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      63 pages, 22 figures, 10 Tables, Github: this https URL", "url": "http://arxiv.org/abs/2507.11589v1", "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\n\\emph{metric}, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural\nTensor Fields} with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields", "comment": "63 pages, 22 figures, 10 Tables, Github:\n  https://github.com/AndreiB137/EinFields", "pdf_url": "http://arxiv.org/pdf/2507.11589v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11797", "title": "GIST: Group Interaction Sensing Toolkit for Mixed Reality", "authors": ["Diana Romero", "Yasra Chandio", "Fatima Anwar", "Salma Elmalaki"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 6 figures", "url": "http://arxiv.org/abs/2507.11797v1", "summary": "Understanding how teams coordinate, share work, and negotiate roles in\nimmersive environments is critical for designing effective mixed-reality (MR)\napplications that support real-time collaboration. However, existing methods\neither rely on external cameras and offline annotation or focus narrowly on\nsingle modalities, limiting their validity and applicability. To address this,\nwe present a novel group interaction sensing toolkit (GIST), a deployable\nsystem that passively captures multi-modal interaction data, such as speech,\ngaze, and spatial proximity from commodity MR headset's sensors and\nautomatically derives both overall static interaction networks and dynamic\nmoment-by-moment behavior patterns. We evaluate GIST with a human subject study\nwith 48 participants across 12 four-person groups performing an open-ended\nimage-sorting task in MR. Our analysis shows strong alignment between the\nidentified behavior modes and shifts in interaction network structure,\nconfirming that momentary changes in speech, gaze, and proximity data are\nobservable through the sensor data.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.11797v1", "cate": "cs.HC", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11545", "title": "The AI Shadow War: SaaS vs. Edge Computing Architectures", "authors": ["Rhea Pritham Marpu", "Kevin J McNamara", "Preeti Gupta"], "categories": ["cs.DC", "cs.ET", "cs.NE"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11545v1", "summary": "The very DNA of AI architecture presents conflicting paths: centralized\ncloud-based models (Software-as-a-Service) versus decentralized edge AI (local\nprocessing on consumer devices). This paper analyzes the competitive\nbattleground across computational capability, energy efficiency, and data\nprivacy. Recent breakthroughs show edge AI challenging cloud systems on\nperformance, leveraging innovations like test-time training and\nmixture-of-experts architectures. Crucially, edge AI boasts a 10,000x\nefficiency advantage: modern ARM processors consume merely 100 microwatts\nforinference versus 1 watt for equivalent cloud processing. Beyond efficiency,\nedge AI secures data sovereignty by keeping processing local, dismantling\nsingle points of failure in centralized architectures. This democratizes access\nthroughaffordable hardware, enables offline functionality, and reduces\nenvironmental impact by eliminating data transmission costs. The edge AI market\nprojects explosive growth from $9 billion in 2025 to $49.6 billion by 2030\n(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical\napplications including personalized education, healthcare monitoring,\nautonomous transport, and smart infrastructure rely on edge AI's ultra-low\nlatency (5-10ms versus 100-500ms for cloud). The convergence of architectural\ninnovation with fundamental physics confirms edge AI's distributed approach\naligns with efficient information processing, signaling the inevitable\nemergence of hybrid edge-cloud ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11545v1", "cate": "cs.DC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2506.10323", "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space", "authors": ["Chuyang Chen", "Brendan Dolan-Gavitt", "Zhiqiang Lin"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by USENIX Security'25 Cycle 2", "url": "http://arxiv.org/abs/2506.10323v3", "summary": "Generation-based fuzzing produces appropriate testing cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual efforts to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage and triggers up to 174.0% more artificially injected bugs. We also\nused ELFuzz to conduct a real-world fuzzing campaign on the newest version of\ncvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are\nexploitable). Moreover, we conducted an ablation study, which shows that the\nfuzzer space model, the key component of ELFuzz, contributes the most (up to\n62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers\nsynthesized by ELFuzz confirms that they catch interesting grammatical\nstructures and semantic constraints in a human-understandable way. The results\npresent the promising potential of ELFuzz for more automated, efficient, and\nextensible input generation for fuzzing.", "comment": "Accepted by USENIX Security'25 Cycle 2", "pdf_url": "http://arxiv.org/pdf/2506.10323v3", "cate": "cs.CR", "date": "2025-06-12", "updated": "2025-07-15"}
{"id": "2507.11940", "title": "IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving", "authors": ["Kanghyun Ryu", "Minjun Sung", "Piyush Gupta", "Jovin D'sa", "Faizan M. Tariq", "David Isele", "Sangjae Bae"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      To be published in The IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025", "url": "http://arxiv.org/abs/2507.11940v1", "summary": "Motion planning for autonomous vehicles (AVs) in dense traffic is\nchallenging, often leading to overly conservative behavior and unmet planning\nobjectives. This challenge stems from the AVs' limited ability to anticipate\nand respond to the interactive behavior of surrounding agents. Traditional\ndecoupled prediction and planning pipelines rely on non-interactive predictions\nthat overlook the fact that agents often adapt their behavior in response to\nthe AV's actions. To address this, we propose Interaction-Aware Neural\nNetwork-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which\nenables interactive trajectory planning by predicting how surrounding agents\nmay react to each control sequence sampled by MPPI. To improve performance in\nstructured lane environments, we introduce a spline-based prior for the MPPI\nsampling distribution, enabling efficient lane-changing behavior. We evaluate\nIANN-MPPI in a dense traffic merging scenario, demonstrating its ability to\nperform efficient merging maneuvers. Our project website is available at\nhttps://sites.google.com/berkeley.edu/iann-mppi", "comment": "To be published in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.11940v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12110", "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages, 16 figures", "url": "http://arxiv.org/abs/2507.12110v1", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "comment": "16 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2507.12110v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11590", "title": "Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques", "authors": ["Raju Challagundla", "Mohsen Dorodchi", "Pu Wang", "Minwoo Lee"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11590v1", "summary": "As privacy regulations become more stringent and access to real-world data\nbecomes increasingly constrained, synthetic data generation has emerged as a\nvital solution, especially for tabular datasets, which are central to domains\nlike finance, healthcare and the social sciences. This survey presents a\ncomprehensive and focused review of recent advances in synthetic tabular data\ngeneration, emphasizing methods that preserve complex feature relationships,\nmaintain statistical fidelity, and satisfy privacy requirements. A key\ncontribution of this work is the introduction of a novel taxonomy based on\npractical generation objectives, including intended downstream applications,\nprivacy guarantees, and data utility, directly informing methodological design\nand evaluation strategies. Therefore, this review prioritizes the actionable\ngoals that drive synthetic data creation, including conditional generation and\nrisk-sensitive modeling. Additionally, the survey proposes a benchmark\nframework to align technical innovation with real-world demands. By bridging\ntheoretical foundations with practical deployment, this work serves as both a\nroadmap for future research and a guide for implementing synthetic tabular data\nin privacy-critical environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11590v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11841", "title": "\"Mapping What I Feel\": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships", "authors": ["Xingyu Lan", "Yutong Yang", "Yifan Wang"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11841v1", "summary": "Affective visualization design is an emerging research direction focused on\ncommunicating and influencing emotion through visualization. However, as\nrevealed by previous research, this area is highly interdisciplinary and\ninvolves theories and practices from diverse fields and disciplines, thus\nawaiting analysis from more fine-grained angles. To address this need, this\nwork focuses on a pioneering and relatively mature sub-area, affective\ngeovisualization design, to further the research in this direction and provide\nmore domain-specific insights. Through an analysis of a curated corpus of\naffective geovisualization designs using the Person-Process-Place (PPP) model\nfrom geographic theory, we derived a design taxonomy that characterizes a\nvariety of methods for eliciting and enhancing emotions through geographic\nvisualization. We also identified four underlying high-level design paradigms\nof affective geovisualization design (e.g., computational, anthropomorphic)\nthat guide distinct approaches to linking geographic information with human\nexperience. By extending existing affective visualization design frameworks\nwith geographic specificity, we provide additional design examples,\ndomain-specific analyses, and insights to guide future research and practices\nin this underexplored yet highly innovative domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11841v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12384", "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "authors": ["Bo Wen", "Guoyun Gao", "Zhicheng Xu", "Ruibin Mao", "Xiaojuan Qi", "X. Sharon Hu", "Xunzhao Yin", "Can Li"], "categories": ["cs.LG", "cs.ET"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12384v1", "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12384v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11671", "title": "Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems", "authors": ["Mst Shamima Aktar", "Peng Liang", "Muhammad Waseem", "Amjed Tahir", "Mojtaba Shahin", "Muhammad Azeem Akbar", "Arif Ali Khan", "Aakash Ahmad", "Musengamana Jean de Dieu", "Ruiyin Li"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      49 pages, 10 images, 16 tables, Manuscript submitted to a journal (2025)", "url": "http://arxiv.org/abs/2507.11671v1", "summary": "Quantum software represents disruptive technologies in terms of\nquantum-specific software systems, services, and applications - leverage the\nprinciples of quantum mechanics via programmable quantum bits (Qubits) that\nmanipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.\nQuantum software architecture enables quantum software developers to abstract\naway implementation-specific details (i.e., mapping of Qubits and QuGates to\nhigh-level architectural components and connectors). Architectural patterns and\nstrategies can provide reusable knowledge and best practices to engineer\nquantum software systems effectively and efficiently. However, quantum software\npractitioners face significant challenges in selecting and implementing\nappropriate patterns and strategies due to the complexity of quantum software\nsystems and the lack of guidelines. To address these challenges, this study\nproposes decision models for selecting patterns and strategies in six critical\ndesign areas in quantum software systems: Communication, Decomposition, Data\nProcessing, Fault Tolerance, Integration and Optimization, and Algorithm\nImplementation. These decision models are constructed based on data collected\nfrom both a mining study (i.e., GitHub and Stack Exchange) and a Systematic\nLiterature Review, which were used to identify relevant patterns and strategies\nwith their involved Quality Attributes (QAs). We then conducted semi-structured\ninterviews with 16 quantum software practitioners to evaluate the familiarity,\nunderstandability, completeness, and usefulness of the proposed decision\nmodels. The results show that the proposed decision models can aid\npractitioners in selecting suitable patterns and strategies to address the\nchallenges related to the architecture design of quantum software systems. The\ndataset is available at [6], allowing the community to reproduce and build upon\nour findings.", "comment": "49 pages, 10 images, 16 tables, Manuscript submitted to a journal\n  (2025)", "pdf_url": "http://arxiv.org/pdf/2507.11671v1", "cate": "cs.SE", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2506.23603", "title": "SoK: Semantic Privacy in Large Language Models", "authors": ["Baihe Ma", "Yanna Jiang", "Xu Wang", "Guangsheng Yu", "Qin Wang", "Caijun Sun", "Chen Li", "Xuelei Qi", "Ying He", "Wei Ni", "Ren Ping Liu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23603v2", "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23603v2", "cate": "cs.CR", "date": "2025-06-30", "updated": "2025-07-16"}
{"id": "2507.11974", "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming", "authors": ["Waseem Akram", "Muhayy Ud Din", "Lyes Saad Soud", "Irfan Hussain"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11974v1", "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11974v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12186", "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "authors": ["Edward Kim", "Hanna Kurniawati"], "categories": ["cs.AI", "I.2.8; I.2.9"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages, 2 tables, 3 figures. To be presented at International Joint Conference on Artificial Intelligence 2025", "url": "http://arxiv.org/abs/2507.12186v1", "summary": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.", "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint\n  Conference on Artificial Intelligence 2025", "pdf_url": "http://arxiv.org/pdf/2507.12186v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11620", "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification", "authors": ["Steven Dillmann", "Juan Rafael Martínez-Galarza"], "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the 2025 ICML Workshop on Machine Learning for Astrophysics, Code available at: this https URL", "url": "http://arxiv.org/abs/2507.11620v1", "summary": "Event time series are sequences of discrete events occurring at irregular\ntime intervals, each associated with a domain-specific observational modality.\nThey are common in domains such as high-energy astrophysics, computational\nsocial science, cybersecurity, finance, healthcare, neuroscience, and\nseismology. Their unstructured and irregular structure poses significant\nchallenges for extracting meaningful patterns and identifying salient phenomena\nusing conventional techniques. We propose novel two- and three-dimensional\ntensor representations for event time series, coupled with sparse autoencoders\nthat learn physically meaningful latent representations. These embeddings\nsupport a variety of downstream tasks, including anomaly detection,\nsimilarity-based retrieval, semantic clustering, and unsupervised\nclassification. We demonstrate our approach on a real-world dataset from X-ray\nastronomy, showing that these representations successfully capture temporal and\nspectral signatures and isolate diverse classes of X-ray transients. Our\nframework offers a flexible, scalable, and generalizable solution for analyzing\ncomplex, irregular event time series across scientific and industrial domains.", "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for\n  Astrophysics, Code available at:\n  https://github.com/StevenDillmann/ml-xraytransients-mnras", "pdf_url": "http://arxiv.org/pdf/2507.11620v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11848", "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection", "authors": ["Changjian Chen", "Pengcheng Wang", "Fei Lyu", "Zhuo Tang", "Li Yang", "Long Wang", "Yong Cai", "Feng Yu", "Kenli Li"], "categories": ["cs.HC", "cs.AI", "q-bio.QM"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11848v1", "summary": "Hybrid rice breeding crossbreeds different rice lines and cultivates the\nresulting hybrids in fields to select those with desirable agronomic traits,\nsuch as higher yields. Recently, genomic selection has emerged as an efficient\nway for hybrid rice breeding. It predicts the traits of hybrids based on their\ngenes, which helps exclude many undesired hybrids, largely reducing the\nworkload of field cultivation. However, due to the limited accuracy of genomic\nprediction models, breeders still need to combine their experience with the\nmodels to identify regulatory genes that control traits and select hybrids,\nwhich remains a time-consuming process. To ease this process, in this paper, we\nproposed a visual analysis method to facilitate interactive hybrid rice\nbreeding. Regulatory gene identification and hybrid selection naturally\nensemble a dual-analysis task. Therefore, we developed a parametric dual\nprojection method with theoretical guarantees to facilitate interactive dual\nanalysis. Based on this dual projection method, we further developed a gene\nvisualization and a hybrid visualization to verify the identified regulatory\ngenes and hybrids. The effectiveness of our method is demonstrated through the\nquantitative evaluation of the parametric dual projection method, identified\nregulatory genes and desired hybrids in the case study, and positive feedback\nfrom breeders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11848v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2309.10509", "title": "Polynomial-time Solver of Tridiagonal QUBO, QUDO and Tensor QUDO problems with Tensor Networks", "authors": ["Alejandro Mata Ali", "Iñigo Perez Delgado", "Marina Ristol Roura", "Aitor Moreno Fdez. de Leceta"], "categories": ["quant-ph", "cs.ET", "68Q12, 15A69, 90C27", "G.1.3; G.2.1"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 9 figures, extreme improvements, new algorithms, new comparisons, better computational complexity, code provided", "url": "http://arxiv.org/abs/2309.10509v4", "summary": "We present a quantum-inspired tensor network algorithm for solving\ntridiagonal Quadratic Unconstrained Binary Optimization (QUBO) problems and\nquadratic unconstrained discrete optimization (QUDO) problems. We also solve\nthe more general Tensor quadratic unconstrained discrete optimization (T-QUDO)\nproblems with one-neighbor interactions in a lineal chain. This method provides\nan exact and explicit equation for these problems. Our algorithms are based on\nthe simulation of a state that undergoes imaginary time evolution and a Half\npartial trace. In addition, we address the degenerate case and evaluate the\npolynomial complexity of the algorithm, also providing a parallelized version.\nWe implemented and tested them with other well-known classical algorithms and\nobserved an improvement in the quality of the results. The performance of the\nproposed algorithms is compared with the Google OR-TOOLS and dimod solvers,\nimproving their results.", "comment": "12 pages, 9 figures, extreme improvements, new algorithms, new\n  comparisons, better computational complexity, code provided", "pdf_url": "http://arxiv.org/pdf/2309.10509v4", "cate": "quant-ph", "date": "2023-09-19", "updated": "2025-07-15"}
{"id": "2507.11687", "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization", "authors": ["Atharva Naik", "Lawanya Baghel", "Dhakshin Govindarajan", "Darsh Agrawal", "Daniel Fried", "Carolyn Rose"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11687v1", "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11687v1", "cate": "cs.SE", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.03361", "title": "Scalable Differentially Private Sketches under Continual Observation", "authors": ["Rayne Holland"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      23 pages, 8 figures", "url": "http://arxiv.org/abs/2507.03361v2", "summary": "Sketches are a fundamental tool in data stream analytics. They are notable\nfor supporting both approximate frequency queries and heavy hitter detection\nwith bounded trade-offs for error and memory. Importantly, on streams that\ncontain sensitive information, sketches can be easily privatized with the\ninjection of a suitable amount of noise. This process is efficient in the\nsingle-release model, where the output is released only at the end of the\nstream. In this setting, it suffices to add noise to the sketch once.\n  In contrast, in the continual observation model, where the output is released\nat every time-step, noise needs to be added to the sketch before each release.\nThis creates an additional computational overhead. To address this, we\nintroduce Lazy Sketch, a novel differentially private sketching method, in the\ncontinual observation model, that employs lazy updates, perturbing and\nmodifying only a small portion of the sketch at each step. Compared to prior\nwork, we reduce the update complexity by a factor of $O(w)$, where $w$ is the\nwidth of the sketch. Experiments demonstrate that our method increases\nthroughput by up to 250x over prior work, making continual observation\ndifferential privacy practical for high-speed streaming applications.\n  In addition, for heavy hitter detection, we present a new sketch-based\nalgorithm that leverages lazy updates to achieve a per-update complexity of\n$O(d \\log T/w + \\log w)$, for sketches with dimension $d\\times w$ and streams\nof length $T$. This marks a significant improvement over prior approaches in\nthe streaming continual observation model, which require recomputing frequency\nestimates for every item in the input domain at each time step.", "comment": "23 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.03361v2", "cate": "cs.CR", "date": "2025-07-04", "updated": "2025-07-16"}
{"id": "2507.11991", "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "authors": ["Juanran Wang", "Marc R. Schlichting", "Mykel J. Kochenderfer"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11991v1", "summary": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11991v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12207", "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "authors": ["Subin Lin", "Chuanbo Hua"], "categories": ["cs.AI", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      ICML 2025 CO-Build Workshop Poster", "url": "http://arxiv.org/abs/2507.12207v1", "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.", "comment": "ICML 2025 CO-Build Workshop Poster", "pdf_url": "http://arxiv.org/pdf/2507.12207v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11639", "title": "Deep Generative Methods and Tire Architecture Design", "authors": ["Fouad Oubari", "Raphael Meunier", "Rodrigue Décatoire", "Mathilde Mougeot"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11639v1", "summary": "As deep generative models proliferate across the AI landscape, industrial\npractitioners still face critical yet unanswered questions about which deep\ngenerative models best suit complex manufacturing design tasks. This work\naddresses this question through a complete study of five representative models\n(Variational Autoencoder, Generative Adversarial Network, multimodal\nVariational Autoencoder, Denoising Diffusion Probabilistic Model, and\nMultinomial Diffusion Model) on industrial tire architecture generation. Our\nevaluation spans three key industrial scenarios: (i) unconditional generation\nof complete multi-component designs, (ii) component-conditioned generation\n(reconstructing architectures from partial observations), and (iii)\ndimension-constrained generation (creating designs that satisfy specific\ndimensional requirements). To enable discrete diffusion models to handle\nconditional scenarios, we introduce categorical inpainting, a mask-aware\nreverse diffusion process that preserves known labels without requiring\nadditional training. Our evaluation employs geometry-aware metrics specifically\ncalibrated for industrial requirements, quantifying spatial coherence,\ncomponent interaction, structural connectivity, and perceptual fidelity. Our\nfindings reveal that diffusion models achieve the strongest overall\nperformance; a masking-trained VAE nonetheless outperforms the multimodal\nvariant MMVAE\\textsuperscript{+} on nearly all component-conditioned metrics,\nand within the diffusion family MDM leads in-distribution whereas DDPM\ngeneralises better to out-of-distribution dimensional constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11639v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11903", "title": "Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps", "authors": ["Daocheng Lin", "Yifan Wang", "Yutong Yang", "Xingyu Lan"], "categories": ["cs.HC", "cs.MM"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11903v1", "summary": "When designed deliberately, data visualizations can become powerful\npersuasive tools, influencing viewers' opinions, values, and actions. While\nresearchers have begun studying this issue (e.g., to evaluate the effects of\npersuasive visualization), we argue that a fundamental mechanism of persuasion\nresides in rhetorical construction, a perspective inadequately addressed in\ncurrent visualization research. To fill this gap, we present a focused analysis\nof octopus maps, a visual genre that has maintained persuasive power across\ncenturies and achieved significant social impact. Employing rhetorical schema\ntheory, we collected and analyzed 90 octopus maps spanning from the 19th\ncentury to contemporary times. We closely examined how octopus maps implement\ntheir persuasive intents and constructed a design space that reveals how visual\nmetaphors are strategically constructed and what common rhetorical strategies\nare applied to components such as maps, octopus imagery, and text. Through the\nabove analysis, we also uncover a set of interesting findings. For instance,\ncontrary to the common perception that octopus maps are primarily a historical\nphenomenon, our research shows that they remain a lively design convention in\ntoday's digital age. Additionally, while most octopus maps stem from Western\ndiscourse that views the octopus as an evil symbol, some designs offer\nalternative interpretations, highlighting the dynamic nature of rhetoric across\ndifferent sociocultural settings. Lastly, drawing from the lessons provided by\noctopus maps, we discuss the associated ethical concerns of persuasive\nvisualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11903v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10786", "title": "\"Is it always watching? Is it always listening?\" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots", "authors": ["Henry Bell", "Jabari Kwesi", "Hiba Laabadli", "Pardis Emami-Naeini"], "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.ET", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10786v2", "summary": "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10786v2", "cate": "cs.CY", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.11689", "title": "REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps", "authors": ["Sergio Di Meglio", "Valeria Pontillo", "Luigi Libero Lucio Starace"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Manuscript accepted for the 51st Euromicro Conference Series on Software Engineering and Advanced Applications (SEAA)", "url": "http://arxiv.org/abs/2507.11689v1", "summary": "In Computer Science Bachelor's programs, software quality is often\nunderemphasized due to limited time and a focus on foundational skills, leaving\nmany students unprepared for industry expectations. To better understand the\ntypical quality of student code and inform both education and hiring practices,\nwe analyze 40 full-stack web applications developed in a third-year Web\nTechnologies course. Using an automated static analysis pipeline, we assess\nadherence to REST API design rules. Results reveal frequent violations of\nfoundational conventions, such as missing hyphens in endpoint paths (98%),\nincorrect pluralization (88%), and misuse of HTTP methods (83%). These findings\nhighlight the need for more focused instruction on API design and support the\nadoption of automated tools to improve code quality in student projects.", "comment": "Manuscript accepted for the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA)", "pdf_url": "http://arxiv.org/pdf/2507.11689v1", "cate": "cs.SE", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11549", "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search", "authors": ["Wendong Mao", "Mingfan Zhao", "Jianfeng Guan", "Qiwei Dong", "Zhongfeng Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11549v1", "summary": "Deformable Attention Transformers (DAT) have shown remarkable performance in\ncomputer vision tasks by adaptively focusing on informative image regions.\nHowever, their data-dependent sampling mechanism introduces irregular memory\naccess patterns, posing significant challenges for efficient hardware\ndeployment. Existing acceleration methods either incur high hardware overhead\nor compromise model accuracy. To address these issues, this paper proposes a\nhardware-friendly optimization framework for DAT. First, a neural architecture\nsearch (NAS)-based method with a new slicing strategy is proposed to\nautomatically divide the input feature into uniform patches during the\ninference process, avoiding memory conflicts without modifying model\narchitecture. The method explores the optimal slice configuration by jointly\noptimizing hardware cost and inference accuracy. Secondly, an FPGA-based\nverification system is designed to test the performance of this framework on\nedge-side hardware. Algorithm experiments on the ImageNet-1K dataset\ndemonstrate that our hardware-friendly framework can maintain have only 0.2%\naccuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA\nshow the proposed method reduces DRAM access times to 18% compared with\nexisting DAT acceleration methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11549v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.12007", "title": "Predictable Drifts in Collective Cultural Attention: Evidence from Nation-Level Library Takeout Data", "authors": ["Anders Weile Larsen", "Vedran Sekara"], "categories": ["cs.SI", "cs.CY"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12007v1", "summary": "Predicting changes in consumer attention for cultural products, such as\nbooks, movies, and songs, is notoriously difficult. Past research on predicting\nthe popularity of individual products suggests the existence of intrinsic\nprediction limits. However, little is known about the limits for predicting\ncollective attention across cultural products. Here, we analyze four years of\nnationwide library loan data for approximately 2 million individuals,\ncomprising over 100 million loans of more than 660,000 unique books. We find\nthat culture, as measured by popularity distributions of loaned books, drifts\ncontinually from month to month at a near-constant rate, leading to a growing\ndivergence over time, and that drifts vary between different book genres. By\nlinking book loans to registry data, we investigate the influence of age, sex,\neducational level, and geographical area on cultural drift, finding\nheterogeneous effects from the different demographic groups. Our findings have\nimportant implications for market forecasting and developing robust recommender\nsystems, highlighting the need to account for specific drift dynamics for\ndifferent types of items and demographic groups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12007v1", "cate": "cs.SI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10578", "title": "When and Where do Data Poisons Attack Textual Inversion?", "authors": ["Jeremy Styborski", "Mingzhi Lyu", "Jiayou Lu", "Nupur Kapur", "Adams Kong"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.10578v2", "summary": "Poisoning attacks pose significant challenges to the robustness of diffusion\nmodels (DMs). In this paper, we systematically analyze when and where poisoning\nattacks textual inversion (TI), a widely used personalization technique for\nDMs. We first introduce Semantic Sensitivity Maps, a novel method for\nvisualizing the influence of poisoning on text embeddings. Second, we identify\nand experimentally verify that DMs exhibit non-uniform learning behavior across\ntimesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias\nand inject adversarial signals predominantly at lower timesteps. Lastly, we\nobserve that adversarial signals distract learning away from relevant concept\nregions within training data, corrupting the TI process. Based on these\ninsights, we propose Safe-Zone Training (SZT), a novel defense mechanism\ncomprised of 3 key components: (1) JPEG compression to weaken high-frequency\npoison signals, (2) restriction to high timesteps during TI training to avoid\nadversarial signals at lower timesteps, and (3) loss masking to constrain\nlearning to relevant regions. Extensive experiments across multiple poisoning\nmethods demonstrate that SZT greatly enhances the robustness of TI against all\npoisoning attacks, improving generative quality beyond prior published\ndefenses. Code: www.github.com/JStyborski/Diff_Lab Data:\nwww.github.com/JStyborski/NC10", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.10578v2", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-16"}
{"id": "2507.12067", "title": "Robust Route Planning for Sidewalk Delivery Robots", "authors": ["Xing Tong", "Michele D. Simoni"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12067v1", "summary": "Sidewalk delivery robots are a promising solution for urban freight\ndistribution, reducing congestion compared to trucks and providing a safer,\nhigher-capacity alternative to drones. However, unreliable travel times on\nsidewalks due to pedestrian density, obstacles, and varying infrastructure\nconditions can significantly affect their efficiency. This study addresses the\nrobust route planning problem for sidewalk robots, explicitly accounting for\ntravel time uncertainty due to varying sidewalk conditions. Optimization is\nintegrated with simulation to reproduce the effect of obstacles and pedestrian\nflows and generate realistic travel times. The study investigates three\ndifferent approaches to derive uncertainty sets, including budgeted,\nellipsoidal, and support vector clustering (SVC)-based methods, along with a\ndistributionally robust method to solve the shortest path (SP) problem. A\nrealistic case study reproducing pedestrian patterns in Stockholm's city center\nis used to evaluate the efficiency of robust routing across various robot\ndesigns and environmental conditions. The results show that, when compared to a\nconventional SP, robust routing significantly enhances operational reliability\nunder variable sidewalk conditions. The Ellipsoidal and DRSP approaches\noutperform the other methods, yielding the most efficient paths in terms of\naverage and worst-case delay. Sensitivity analyses reveal that robust\napproaches consistently outperform the conventional SP, particularly for\nsidewalk delivery robots that are wider, slower, and have more conservative\nnavigation behaviors. These benefits are even more pronounced in adverse\nweather conditions and high pedestrian congestion scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12067v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12215", "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.12215v1", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.12215v1", "cate": "cs.AI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11645", "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation", "authors": ["Ahmed Salah", "David Yevick"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 11 figures", "url": "http://arxiv.org/abs/2507.11645v1", "summary": "Grokking refers to delayed generalization in which the increase in test\naccuracy of a neural network occurs appreciably after the improvement in\ntraining accuracy This paper introduces several practical metrics including\nvariance under dropout, robustness, embedding similarity, and sparsity\nmeasures, that can forecast grokking behavior. Specifically, the resilience of\nneural networks to noise during inference is estimated from a Dropout\nRobustness Curve (DRC) obtained from the variation of the accuracy with the\ndropout rate as the model transitions from memorization to generalization. The\nvariance of the test accuracy under stochastic dropout across training\ncheckpoints further exhibits a local maximum during the grokking. Additionally,\nthe percentage of inactive neurons decreases during generalization, while the\nembeddings tend to a bimodal distribution independent of initialization that\ncorrelates with the observed cosine similarity patterns and dataset symmetries.\nThese metrics additionally provide valuable insight into the origin and\nbehaviour of grokking.", "comment": "15 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.11645v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11911", "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding", "authors": ["Xiaoqing Chen", "Siyang Li", "Dongrui Wu"], "categories": ["cs.HC", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11911v1", "summary": "Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11911v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11898", "title": "Extremal Testing for Network Software using LLMs", "authors": ["Rathin Singha", "Harry Qian", "Srinath Saikrishnan", "Tracy Zhao", "Ryan Beckett", "Siva Kesava Reddy Kakarla", "George Varghese"], "categories": ["cs.SE", "cs.NI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11898v1", "summary": "Physicists often manually consider extreme cases when testing a theory. In\nthis paper, we show how to automate extremal testing of network software using\nLLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS\nname length limits); then ask the LLM to generate tests that violate the\nconstraints. We demonstrate how easy this process is by generating extremal\ntests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.\nWe show how this methodology extends to centralized network software such as\nshortest path algorithms, and how LLMs can generate filtering code to reject\nextremal input. We propose using agentic AI to further automate extremal\ntesting. LLM-generated extremal testing goes beyond an old technique in\nsoftware testing called Boundary Value Analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11898v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11550", "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction", "authors": ["Hyeonseok Jin", "Geonmin Kim", "Kyungbaek Kim"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.11550v1", "summary": "Spatio-temporal traffic prediction plays a key role in intelligent\ntransportation systems by enabling accurate prediction in complex urban areas.\nAlthough not only accuracy but also efficiency for scalability is important,\nsome previous methods struggle to capture heterogeneity such as varying traffic\npatterns across regions and time periods. Moreover, Graph Neural Networks\n(GNNs), which are the mainstream of traffic prediction, not only require\npredefined adjacency matrix, but also limit scalability to large-scale data\ncontaining many nodes due to their inherent complexity. To overcome these\nlimitations, we propose Deformable Dynamic Convolution Network (DDCN) for\naccurate yet efficient traffic prediction. Traditional Convolutional Neural\nNetworks (CNNs) are limited in modeling non-Euclidean spatial structures and\nspatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically\napplying deformable filters based on offset. Specifically, DDCN decomposes\ntransformer-style CNN to encoder-decoder structure, and applies proposed\napproaches to the spatial and spatio-temporal attention blocks of the encoder\nto emphasize important features. The decoder, composed of feed-forward module,\ncomplements the output of the encoder. This novel structure make DDCN can\nperform accurate yet efficient traffic prediction. In comprehensive experiments\non four real-world datasets, DDCN achieves competitive performance, emphasizing\nthe potential and effectiveness of CNN-based approaches for spatio-temporal\ntraffic prediction.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.11550v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.12063", "title": "Contrastive Cascade Graph Learning for Classifying Real and Synthetic Information Diffusion Patterns", "authors": ["Naoki Shibao", "Sho Tsugawa"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12063v1", "summary": "A wide variety of information is disseminated through social media, and\ncontent that spreads at scale can have tangible effects on the real world. To\ncurb the spread of harmful content and promote the dissemination of reliable\ninformation, research on cascade graph mining has attracted increasing\nattention. A promising approach in this area is Contrastive Cascade Graph\nLearning (CCGL). One important task in cascade graph mining is cascade\nclassification, which involves categorizing cascade graphs based on their\nstructural characteristics. Although CCGL is expected to be effective for this\ntask, its performance has not yet been thoroughly evaluated. This study aims to\ninvestigate the effectiveness of CCGL for cascade classification. Our findings\ndemonstrate the strong performance of CCGL in capturing platform- and\nmodel-specific structural patterns in cascade graphs, highlighting its\npotential for a range of downstream information diffusion analysis tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12063v1", "cate": "cs.SI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2312.12216", "title": "Sharing is CAIRing: Characterizing Principles and Assessing Properties of Universal Privacy Evaluation for Synthetic Tabular Data", "authors": ["Tobias Hyrup", "Anton Danholt Lautrup", "Arthur Zimek", "Peter Schneider-Kamp"], "categories": ["cs.LG", "cs.CR", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.12216v2", "summary": "Data sharing is a necessity for innovative progress in many domains,\nespecially in healthcare. However, the ability to share data is hindered by\nregulations protecting the privacy of natural persons. Synthetic tabular data\nprovide a promising solution to address data sharing difficulties but does not\ninherently guarantee privacy. Still, there is a lack of agreement on\nappropriate methods for assessing the privacy-preserving capabilities of\nsynthetic data, making it difficult to compare results across studies. To the\nbest of our knowledge, this is the first work to identify properties that\nconstitute good universal privacy evaluation metrics for synthetic tabular\ndata. The goal of universally applicable metrics is to enable comparability\nacross studies and to allow non-technical stakeholders to understand how\nprivacy is protected. We identify four principles for the assessment of\nmetrics: Comparability, Applicability, Interpretability, and Representativeness\n(CAIR). To quantify and rank the degree to which evaluation metrics conform to\nthe CAIR principles, we design a rubric using a scale of 1-4. Each of the four\nproperties is scored on four parameters, yielding 16 total dimensions. We study\nthe applicability and usefulness of the CAIR principles and rubric by assessing\na selection of metrics popular in other studies. The results provide granular\ninsights into the strengths and weaknesses of existing metrics that not only\nrank the metrics but highlight areas of potential improvements. We expect that\nthe CAIR principles will foster agreement among researchers and organizations\non which universal privacy evaluation metrics are appropriate for synthetic\ntabular data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.12216v2", "cate": "cs.LG", "date": "2023-12-19", "updated": "2025-07-16"}
{"id": "2507.12093", "title": "Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards", "authors": ["David Rapado-Rincon", "Gert Kootstra"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Paper submitted to Smart Agricultural Technology", "url": "http://arxiv.org/abs/2507.12093v1", "summary": "Accurate mapping of individual trees is an important component for precision\nagriculture in orchards, as it allows autonomous robots to perform tasks like\ntargeted operations or individual tree monitoring. However, creating these maps\nis challenging because GPS signals are often unreliable under dense tree\ncanopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)\napproaches struggle in orchards because the repetitive appearance of trees can\nconfuse the system, leading to mapping errors. To address this, we introduce\nTree-SLAM, a semantic SLAM approach tailored for creating maps of individual\ntrees in orchards. Utilizing RGB-D images, our method detects tree trunks with\nan instance segmentation model, estimates their location and re-identifies them\nusing a cascade-graph-based data association algorithm. These re-identified\ntrunks serve as landmarks in a factor graph framework that integrates noisy GPS\nsignals, odometry, and trunk observations. The system produces maps of\nindividual trees with a geo-localization error as low as 18 cm, which is less\nthan 20\\% of the planting distance. The proposed method was validated on\ndiverse datasets from apple and pear orchards across different seasons,\ndemonstrating high mapping accuracy and robustness in scenarios with unreliable\nGPS signals.", "comment": "Paper submitted to Smart Agricultural Technology", "pdf_url": "http://arxiv.org/pdf/2507.12093v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11543", "title": "A Review of Generative AI in Computer Science Education: Challenges and Opportunities in Accuracy, Authenticity, and Assessment", "authors": ["Iman Reihanian", "Yunfei Hou", "Yu Chen", "Yifei Zheng"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at The 2024 International Conference on Computational Science and Computational Intelligence (CSCI), Research Track on Education. To appear in Springer Lecture Notes in Computer Science (LNCS) proceedings, expected July 2025", "url": "http://arxiv.org/abs/2507.11543v1", "summary": "This paper surveys the use of Generative AI tools, such as ChatGPT and\nClaude, in computer science education, focusing on key aspects of accuracy,\nauthenticity, and assessment. Through a literature review, we highlight both\nthe challenges and opportunities these AI tools present. While Generative AI\nimproves efficiency and supports creative student work, it raises concerns such\nas AI hallucinations, error propagation, bias, and blurred lines between\nAI-assisted and student-authored content. Human oversight is crucial for\naddressing these concerns. Existing literature recommends adopting hybrid\nassessment models that combine AI with human evaluation, developing bias\ndetection frameworks, and promoting AI literacy for both students and\neducators. Our findings suggest that the successful integration of AI requires\na balanced approach, considering ethical, pedagogical, and technical factors.\nFuture research may explore enhancing AI accuracy, preserving academic\nintegrity, and developing adaptive models that balance creativity with\nprecision.", "comment": "Accepted for presentation at The 2024 International Conference on\n  Computational Science and Computational Intelligence (CSCI), Research Track\n  on Education. To appear in Springer Lecture Notes in Computer Science (LNCS)\n  proceedings, expected July 2025", "pdf_url": "http://arxiv.org/pdf/2507.11543v1", "cate": "cs.CY", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.11649", "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "authors": ["Daniel Commey", "Benjamin Appiah", "Griffith S. Klogo", "Garth V. Crosby"], "categories": ["cs.LG", "cs.DC", "cs.NI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11649v1", "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11649v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11960", "title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement", "authors": ["Hyein Hong", "Sangbong Yoo", "SeokHwan Choi", "Jisue Kim", "Seongbum Seo", "Haneol Cho", "Chansoo Kim", "Yun Jang"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11960v1", "summary": "Approaches to enhancing data quality (DQ) are classified into two main\ncategories: data- and process-driven. However, prior research has predominantly\nutilized batch data preprocessing within the data-driven framework, which often\nproves insufficient for optimizing machine learning (ML) model performance and\nfrequently leads to distortions in data characteristics. Existing studies have\nprimarily focused on data preprocessing rather than genuine data quality\nimprovement (DQI). In this paper, we introduce d-DQIVAR, a novel visual\nanalytics system designed to facilitate DQI strategies aimed at improving ML\nmodel performance. Our system integrates visual analytics techniques that\nleverage both data-driven and process-driven approaches. Data-driven techniques\ntackle DQ issues such as imputation, outlier detection, deletion, format\nstandardization, removal of duplicate records, and feature selection.\nProcess-driven strategies encompass evaluating DQ and DQI procedures by\nconsidering DQ dimensions and ML model performance and applying the\nKolmogorov-Smirnov test. We illustrate how our system empowers users to harness\nexpert and domain knowledge effectively within a practical workflow through\ncase studies, evaluations, and user studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11960v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11976", "title": "A Task Taxonomy for Conformance Checking", "authors": ["Jana-Rebecca Rehse", "Michael Grohs", "Finn Klessascheck", "Lisa-Marie Klein", "Tatiana von Landesberger", "Luise Pufahl"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Preprint submitted to Information Systems", "url": "http://arxiv.org/abs/2507.11976v1", "summary": "Conformance checking is a sub-discipline of process mining, which compares\nobserved process traces with a process model to analyze whether the process\nexecution conforms with or deviates from the process design. Organizations can\nleverage this analysis, for example to check whether their processes comply\nwith internal or external regulations or to identify potential improvements.\nGaining these insights requires suitable visualizations, which make complex\nresults accessible and actionable. So far, however, the development of\nconformance checking visualizations has largely been left to tool vendors. As a\nresult, current tools offer a wide variety of visual representations for\nconformance checking, but the analytical purposes they serve often remain\nunclear. However, without a systematic understanding of these purposes, it is\ndifficult to evaluate the visualizations' usefulness. Such an evaluation hence\nrequires a deeper understanding of conformance checking as an analysis domain.\nTo this end, we propose a task taxonomy, which categorizes the tasks that can\noccur when conducting conformance checking analyses. This taxonomy supports\nresearchers in determining the purpose of visualizations, specifying relevant\nconformance checking tasks in terms of their goal, means, constraint type, data\ncharacteristics, data target, and data cardinality. Combining concepts from\nprocess mining and visual analytics, we address researchers from both\ndisciplines to enable and support closer collaborations.", "comment": "Preprint submitted to Information Systems", "pdf_url": "http://arxiv.org/pdf/2507.11976v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11554", "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11554v1", "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11554v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.12108", "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies", "authors": ["Lorenzo Mannocci", "Stefano Cresci", "Matteo Magnani", "Anna Monreale", "Maurizio Tesconi"], "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12108v1", "summary": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12108v1", "cate": "cs.SI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2403.15740", "title": "Protecting Copyrighted Material with Unique Identifiers in Large Language Model Training", "authors": ["Shuai Zhao", "Linchao Zhu", "Ruijie Quan", "Yi Yang"], "categories": ["cs.CL", "cs.CR", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      A technical report, work mainly done in the early of 2024", "url": "http://arxiv.org/abs/2403.15740v3", "summary": "A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.", "comment": "A technical report, work mainly done in the early of 2024", "pdf_url": "http://arxiv.org/pdf/2403.15740v3", "cate": "cs.CL", "date": "2024-03-23", "updated": "2025-07-16"}
{"id": "2507.12148", "title": "Leveraging Sidewalk Robots for Walkability-Related Analyses", "authors": ["Xing Tong", "Michele D. Simoni", "Kaj Munhoz Arfvidsson", "Jonas Mårtensson"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12148v1", "summary": "Walkability is a key component of sustainable urban development, while\ncollecting detailed data on its related features remains challenging due to the\nhigh costs and limited scalability of traditional methods. Sidewalk delivery\nrobots, increasingly deployed in urban environments, offer a promising solution\nto these limitations. This paper explores how these robots can serve as mobile\ndata collection platforms, capturing sidewalk-level features related to\nwalkability in a scalable, automated, and real-time manner. A sensor-equipped\nrobot was deployed on a sidewalk network at KTH in Stockholm, completing 101\ntrips covering 900 segments. From the collected data, different typologies of\nfeatures are derived, including robot trip characteristics (e.g., speed,\nduration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk\nutilization (e.g., pedestrian density). Their walkability-related implications\nwere investigated with a series of analyses. The results demonstrate that\npedestrian movement patterns are strongly influenced by sidewalk\ncharacteristics, with higher density, reduced width, and surface irregularity\nassociated with slower and more variable trajectories. Notably, robot speed\nclosely mirrors pedestrian behavior, highlighting its potential as a proxy for\nassessing pedestrian dynamics. The proposed framework enables continuous\nmonitoring of sidewalk conditions and pedestrian behavior, contributing to the\ndevelopment of more walkable, inclusive, and responsive urban environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12148v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11548", "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening", "authors": ["Kevin T Webster"], "categories": ["cs.CY", "cs.AI", "cs.CL", "I.2.1; K.4.2; I.2.6; K.4.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      58 pages, 4 figures", "url": "http://arxiv.org/abs/2507.11548v1", "summary": "The increasing use of generative AI for resume screening is predicated on the\nassumption that it offers an unbiased alternative to biased human\ndecision-making. However, this belief fails to address a critical question: are\nthese AI systems fundamentally competent at the evaluative tasks they are meant\nto perform? This study investigates the question of competence through a\ntwo-part audit of eight major AI platforms. Experiment 1 confirmed complex,\ncontextual racial and gender biases, with some models penalizing candidates\nmerely for the presence of demographic signals. Experiment 2, which evaluated\ncore competence, provided a critical insight: some models that appeared\nunbiased were, in fact, incapable of performing a substantive evaluation,\nrelying instead on superficial keyword matching. This paper introduces the\n\"Illusion of Neutrality\" to describe this phenomenon, where an apparent lack of\nbias is merely a symptom of a model's inability to make meaningful judgments.\nThis study recommends that organizations and regulators adopt a dual-validation\nframework, auditing AI hiring tools for both demographic bias and demonstrable\ncompetence to ensure they are both equitable and effective.", "comment": "58 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.11548v1", "cate": "cs.CY", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.11688", "title": "Composing Linear Layers from Irreducibles", "authors": ["Travis Pence", "Daisuke Yamada", "Vikas Singh"], "categories": ["cs.LG", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      27 Pages, 13 Tables, 8 Figures", "url": "http://arxiv.org/abs/2507.11688v1", "summary": "Contemporary large models often exhibit behaviors suggesting the presence of\nlow-level primitives that compose into modules with richer functionality, but\nthese fundamental building blocks remain poorly understood. We investigate this\ncompositional structure in linear layers by asking: can we identify/synthesize\nlinear transformations from a minimal set of geometric primitives? Using\nClifford algebra, we show that linear layers can be expressed as compositions\nof bivectors -- geometric objects encoding oriented planes -- and introduce a\ndifferentiable algorithm that decomposes them into products of rotors. This\nconstruction uses only O(log^2 d) parameters, versus O(d^2) required by dense\nmatrices. Applied to the key, query, and value projections in LLM attention\nlayers, our rotor-based layers match the performance of strong baselines such\nas block-Hadamard and low-rank approximations. Our findings provide an\nalgebraic perspective on how these geometric primitives can compose into\nhigher-level functions within deep models.", "comment": "27 Pages, 13 Tables, 8 Figures", "pdf_url": "http://arxiv.org/pdf/2507.11688v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11984", "title": "Dataset-Adaptive Dimensionality Reduction", "authors": ["Hyeon Jeon", "Jeongin Park", "Soohyun Lee", "Dae Hyun Kim", "Sungbok Shin", "Jinwook Seo"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      IEEE VIS 2025 & IEEE Transactions on Visualization and Computer Graphics (TVCG)", "url": "http://arxiv.org/abs/2507.11984v1", "summary": "Selecting the appropriate dimensionality reduction (DR) technique and\ndetermining its optimal hyperparameter settings that maximize the accuracy of\nthe output projections typically involves extensive trial and error, often\nresulting in unnecessary computational overhead. To address this challenge, we\npropose a dataset-adaptive approach to DR optimization guided by structural\ncomplexity metrics. These metrics quantify the intrinsic complexity of a\ndataset, predicting whether higher-dimensional spaces are necessary to\nrepresent it accurately. Since complex datasets are often inaccurately\nrepresented in two-dimensional projections, leveraging these metrics enables us\nto predict the maximum achievable accuracy of DR techniques for a given\ndataset, eliminating redundant trials in optimizing DR. We introduce the design\nand theoretical foundations of these structural complexity metrics. We\nquantitatively verify that our metrics effectively approximate the ground truth\ncomplexity of datasets and confirm their suitability for guiding\ndataset-adaptive DR workflow. Finally, we empirically show that our\ndataset-adaptive workflow significantly enhances the efficiency of DR\noptimization without compromising accuracy.", "comment": "IEEE VIS 2025 & IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "pdf_url": "http://arxiv.org/pdf/2507.11984v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12104", "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs", "authors": ["Francisco Javier Cavero", "Juan C. Alonso", "Antonio Ruiz-Cortés"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing for AI Applications), held in conjunction with the 22nd International Conference on Service-Oriented Computing (ICSOC 2024)", "url": "http://arxiv.org/abs/2507.12104v1", "summary": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.", "comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing\n  for AI Applications), held in conjunction with the 22nd International\n  Conference on Service-Oriented Computing (ICSOC 2024)", "pdf_url": "http://arxiv.org/pdf/2507.12104v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11558", "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting", "authors": ["Changlu Chen", "Yanbin Liu", "Chaoxi Niu", "Ling Chen", "Tianqing Zhu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11558v1", "summary": "Foundation models have achieved remarkable success in natural language\nprocessing and computer vision, demonstrating strong capabilities in modeling\ncomplex patterns. While recent efforts have explored adapting large language\nmodels (LLMs) for time-series forecasting, LLMs primarily capture\none-dimensional sequential dependencies and struggle to model the richer\nspatio-temporal (ST) correlations essential for accurate ST forecasting. In\nthis paper, we present \\textbf{ST-VFM}, a novel framework that systematically\nreprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal\nforecasting. While VFMs offer powerful spatial priors, two key challenges arise\nwhen applying them to ST tasks: (1) the lack of inherent temporal modeling\ncapacity and (2) the modality gap between visual and ST data. To address these,\nST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs\nwith auxiliary ST flow inputs, where the flow encodes lightweight temporal\ndifference signals interpretable as dynamic spatial cues. To effectively\nprocess these dual-branch inputs, ST-VFM introduces two dedicated reprogramming\nstages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token\nAdapter to embed temporal context and align both branches into VFM-compatible\nfeature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral\nCross-Prompt Coordination module, enabling dynamic interaction between branches\nthrough prompt-based conditioning, thus enriching joint representation learning\nwithout modifying the frozen VFM backbone. Extensive experiments on ten\nspatio-temporal datasets show that ST-VFM outperforms state-of-the-art\nbaselines, demonstrating effectiveness and robustness across VFM backbones\n(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong\ngeneral framework for spatio-temporal forecasting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11558v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11559", "title": "RSD-15K: A Large-Scale User-Level Annotated Dataset for Suicide Risk Detection on Social Media", "authors": ["Shouwen Zheng", "Yingzhi Tao", "Taiqi Zhou"], "categories": ["cs.CY", "cs.SI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      the article has already been recieved by 2025 IEEE 41st International Conference on Data Engineering Workshops (ICDEW), but hadn't been online yet", "url": "http://arxiv.org/abs/2507.11559v1", "summary": "In recent years, cognitive and mental health (CMH) disorders have\nincreasingly become an important challenge for global public health, especially\nthe suicide problem caused by multiple factors such as social competition,\neconomic pressure and interpersonal relationships among young and middle-aged\npeople. Social media, as an important platform for individuals to express\nemotions and seek help, provides the possibility for early detection and\nintervention of suicide risk. This paper introduces a large-scale dataset\ncontaining 15,000 user-level posts. Compared with existing datasets, this\ndataset retains complete user posting time sequence information, supports\nmodeling the dynamic evolution of suicide risk, and we have also conducted\ncomprehensive and rigorous annotations on these datasets. In the benchmark\nexperiment, we systematically evaluated the performance of traditional machine\nlearning methods, deep learning models, and fine-tuned large language models.\nThe experimental results show that our dataset can effectively support the\nautomatic assessment task of suicide risk. Considering the sensitivity of\nmental health data, we also discussed the privacy protection and ethical use of\nthe dataset. In addition, we also explored the potential applications of the\ndataset in mental health testing, clinical psychiatric auxiliary treatment,\netc., and provided directional suggestions for future research work.", "comment": "the article has already been recieved by 2025 IEEE 41st International\n  Conference on Data Engineering Workshops (ICDEW), but hadn't been online yet", "pdf_url": "http://arxiv.org/pdf/2507.11559v1", "cate": "cs.CY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.19609", "title": "Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs", "authors": ["Jérémy Thibault", "Joseph Lenormand", "Catalin Hritcu"], "categories": ["cs.PL", "cs.CR"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      ITP'25 final version", "url": "http://arxiv.org/abs/2503.19609v3", "summary": "Researchers aim to build secure compilation chains enforcing that if there is\nno attack a source context can mount against a source program then there is\nalso no attack an adversarial target context can mount against the compiled\nprogram. Proving that these compilation chains are secure is, however,\nchallenging, and involves a non-trivial back-translation step: for any attack a\ntarget context mounts against the compiled program one has to exhibit a source\ncontext mounting the same attack against the source program. We describe a\nnovel back-translation technique, which results in simpler proofs that can be\nmore easily mechanized in a proof assistant. Given a finite set of finite trace\nprefixes, capturing the interaction recorded during an attack between a target\ncontext and the compiled program, we build a call-return tree that we\nback-translate into a source context producing the same trace prefixes. We use\nstate in the generated source context to record the current location in the\ncall-return tree. The back-translation is done in several small steps, each\nadding to the tree new information describing how the location should change\ndepending on how the context regains control. To prove this back-translation\ncorrect we give semantics to every intermediate call-return tree language,\nusing ghost state to store information and explicitly enforce execution\ninvariants. We prove several small forward simulations, basically seeing the\nback-translation as a verified nanopass compiler. Thanks to this modular\nstructure, we are able to mechanize this complex back-translation and its\ncorrectness proof in the Rocq prover without too much effort.", "comment": "ITP'25 final version", "pdf_url": "http://arxiv.org/pdf/2503.19609v3", "cate": "cs.PL", "date": "2025-03-25", "updated": "2025-07-16"}
{"id": "2507.12158", "title": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach", "authors": ["Nawshin Mannan Proma", "Gricel Vázquez", "Sepeedeh Shahbeigi", "Arjun Badyal", "Victoria Hodge"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures", "url": "http://arxiv.org/abs/2507.12158v1", "summary": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems.", "comment": "6 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.12158v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11551", "title": "Landmark Detection for Medical Images using a General-purpose Segmentation Model", "authors": ["Ekaterina Stansfield", "Jennifer A. Mitterer", "Abdulrahman Altahhan"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      13 pages, 8 figures, 2 tables. Submitted to ICONIP 2025", "url": "http://arxiv.org/abs/2507.11551v1", "summary": "Radiographic images are a cornerstone of medical diagnostics in orthopaedics,\nwith anatomical landmark detection serving as a crucial intermediate step for\ninformation extraction. General-purpose foundational segmentation models, such\nas SAM (Segment Anything Model), do not support landmark segmentation out of\nthe box and require prompts to function. However, in medical imaging, the\nprompts for landmarks are highly specific. Since SAM has not been trained to\nrecognize such landmarks, it cannot generate accurate landmark segmentations\nfor diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has\nbeen trained to identify larger anatomical structures, such as organs and their\nparts, and lacks the fine-grained precision required for orthopaedic pelvic\nlandmarks. To address this limitation, we propose leveraging another\ngeneral-purpose, non-foundational model: YOLO. YOLO excels in object detection\nand can provide bounding boxes that serve as input prompts for SAM. While YOLO\nis efficient at detection, it is significantly outperformed by SAM in\nsegmenting complex structures. In combination, these two models form a reliable\npipeline capable of segmenting not only a small pilot set of eight anatomical\nlandmarks but also an expanded set of 72 landmarks and 16 regions with complex\noutlines, such as the femoral cortical bone and the pelvic inlet. By using\nYOLO-generated bounding boxes to guide SAM, we trained the hybrid model to\naccurately segment orthopaedic pelvic radiographs. Our results show that the\nproposed combination of YOLO and SAM yields excellent performance in detecting\nanatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.", "comment": "13 pages, 8 figures, 2 tables. Submitted to ICONIP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11551v1", "cate": "eess.IV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.11690", "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness", "authors": ["Amaya Dharmasiri", "William Yang", "Polina Kirichenko", "Lydia Liu", "Olga Russakovsky"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 9 additional pages for Appendix", "url": "http://arxiv.org/abs/2507.11690v1", "summary": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness.", "comment": "10 pages, 9 additional pages for Appendix", "pdf_url": "http://arxiv.org/pdf/2507.11690v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11999", "title": "Envisage: Towards Expressive Visual Graph Querying", "authors": ["Xiaolin Wen", "Qishuang Fu", "Shuangyue Han", "Yichen Guo", "Joseph K. Liu", "Yong Wang"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11999v1", "summary": "Graph querying is the process of retrieving information from graph data using\nspecialized languages (e.g., Cypher), often requiring programming expertise.\nVisual Graph Querying (VGQ) streamlines this process by enabling users to\nconstruct and execute queries via an interactive interface without resorting to\ncomplex coding. However, current VGQ tools only allow users to construct simple\nand specific query graphs, limiting users' ability to interactively express\ntheir query intent, especially for underspecified query intent. To address\nthese limitations, we propose Envisage, an interactive visual graph querying\nsystem to enhance the expressiveness of VGQ in complex query scenarios by\nsupporting intuitive graph structure construction and flexible parameterized\nrule specification. Specifically, Envisage comprises four stages: Query\nExpression allows users to interactively construct graph queries through\nintuitive operations; Query Verification enables the validation of constructed\nqueries via rule verification and query instantiation; Progressive Query\nExecution can progressively execute queries to ensure meaningful querying\nresults; and Result Analysis facilitates result exploration and interpretation.\nTo evaluate Envisage, we conducted two case studies and in-depth user\ninterviews with 14 graph analysts. The results demonstrate its effectiveness\nand usability in constructing, verifying, and executing complex graph queries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11999v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12118", "title": "An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment", "authors": ["Noe Zermeño", "Cristina Zuheros", "Lucas Daniel Del Rosso Calache", "Francisco Herrera", "Rosana Montes"], "categories": ["cs.SE", "cs.HC"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12118v1", "summary": "In recent years, attention has increasingly focused on enhancing user\nsatisfaction with user interfaces, spanning both mobile applications and\nwebsites. One fundamental aspect of human-machine interaction is the concept of\nweb usability. In order to assess web usability, the A/B testing technique\nenables the comparison of data between two designs. Expanding the scope of\ntests to include the designs being evaluated, in conjunction with the\ninvolvement of both real and fictional users, presents a challenge for which\nfew online tools offer support. We propose a methodology for web usability\nevaluation based on user-centered approaches such as design thinking and\nlinguistic decision-making, named Linguistic Decision-Making for Web Usability\nEvaluation. This engages people in role-playing scenarios and conducts a number\nof usability tests, including the widely recognized System Usability Scale. We\nincorporate the methodology into a decision support system based on A/B\ntesting. We use real users in a case study to assess three Moodle platforms at\nthe University of Guadalajara, Mexico.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12118v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11562", "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration", "authors": ["Ozer Can Devecioglu", "Serkan Kiranyaz", "Mehmet Yamac", "Moncef Gabbouj"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.11562v1", "summary": "The wide range of deformation artifacts that arise from complex light\npropagation, scattering, and depth-dependent attenuation makes the underwater\nimage restoration to remain a challenging problem. Like other single deep\nregressor networks, conventional GAN-based restoration methods struggle to\nperform well across this heterogeneous domain, since a single generator network\nis typically insufficient to capture the full range of visual degradations. In\norder to overcome this limitation, we propose xOp-GAN, a novel GAN model with\nseveral expert generator networks, each trained solely on a particular subset\nwith a certain image quality. Thus, each generator can learn to maximize its\nrestoration performance for a particular quality range. Once a xOp-GAN is\ntrained, each generator can restore the input image and the best restored image\ncan then be selected by the discriminator based on its perceptual confidence\nscore. As a result, xOP-GAN is the first GAN model with multiple generators\nwhere the discriminator is being used during the inference of the regression\ntask. Experimental results on benchmark Large Scale Underwater Image (LSUI)\ndataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,\nsurpassing all single-regressor models by a large margin even, with reduced\ncomplexity.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.11562v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11825", "title": "Peer Review and the Diffusion of Ideas", "authors": ["Binglu Wang", "Zhengnan Ma", "Dashun Wang", "Brian Uzzi"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11825v1", "summary": "This study examines a fundamental yet overlooked function of peer review: its\nrole in exposing reviewers to new and unexpected ideas. Leveraging a natural\nexperiment involving over half a million peer review invitations covering both\naccepted and rejected manuscripts, and integrating high-scale bibliographic and\neditorial records for 37,279 submitting authors, we find that exposure to a\nmanuscript's core ideas significantly influences the future referencing\nbehavior and knowledge of reviewer invitees who decline the review invite.\nSpecifically, declining reviewer invitees who could view concise summaries of\nthe manuscript's core ideas not only increase their citations to the manuscript\nitself but also demonstrate expanded breadth, depth, diversity, and prominence\nof citations to the submitting author's broader body of work. Overall, these\nresults suggest peer review substantially influences the spread of scientific\nknowledge. Ironically, while the massive scale of peer review, entailing\nmillions of reviews annually, often drives policy debates about its costs and\nburdens, our findings demonstrate that precisely because of this scale, peer\nreview serves as a powerful yet previously unrecognized engine for idea\ndiffusion, which is central to scientific advances and scholarly communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11825v1", "cate": "physics.soc-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.01048", "title": "How does Watermarking Affect Visual Language Models in Document Understanding?", "authors": ["Chunxue Xu", "Yiwei Wang", "Bryan Hooi", "Yujun Cai", "Songze Li"], "categories": ["cs.CV", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to COLM 2025", "url": "http://arxiv.org/abs/2504.01048v2", "summary": "Visual Language Models (VLMs) have become foundational models for document\nunderstanding tasks, widely used in the processing of complex multimodal\ndocuments across domains such as finance, law, and academia. However, documents\noften contain noise-like information, such as watermarks, which inevitably\nleads us to inquire: \\emph{Do watermarks degrade the performance of VLMs in\ndocument understanding?} To address this, we propose a novel evaluation\nframework to investigate the effect of visible watermarks on VLMs performance.\nWe takes into account various factors, including different types of document\ndata, the positions of watermarks within documents and variations in watermark\ncontent. Our experimental results reveal that VLMs performance can be\nsignificantly compromised by watermarks, with performance drop rates reaching\nup to 36\\%. We discover that \\emph{scattered} watermarks cause stronger\ninterference than centralized ones, and that \\emph{semantic contents} in\nwatermarks creates greater disruption than simple visual occlusion. Through\nattention mechanism analysis and embedding similarity examination, we find that\nthe performance drops are mainly attributed to that watermarks 1) force\nwidespread attention redistribution, and 2) alter semantic representation in\nthe embedding space. Our research not only highlights significant challenges in\ndeploying VLMs for document understanding, but also provides insights towards\ndeveloping robust inference mechanisms on watermarked documents.", "comment": "Accepted to COLM 2025", "pdf_url": "http://arxiv.org/pdf/2504.01048v2", "cate": "cs.CV", "date": "2025-04-01", "updated": "2025-07-16"}
{"id": "2507.12194", "title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization", "authors": ["Hongming Shen", "Xun Chen", "Yulin Hui", "Zhenyu Wu", "Wei Wang", "Qiyang Lyu", "Tianchen Deng", "Danwei Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12194v1", "summary": "Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12194v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11557", "title": "3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation", "authors": ["Jiaxu Zheng", "Meiman He", "Xuhui Tang", "Xiong Wang", "Tuoyu Cao", "Tianyi Zeng", "Lichi Zhang", "Chenyu You"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11557v1", "summary": "Magnetic Resonance (MR) imaging plays an essential role in contemporary\nclinical diagnostics. It is increasingly integrated into advanced therapeutic\nworkflows, such as hybrid Positron Emission Tomography/Magnetic Resonance\n(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are\ncritically dependent on accurate estimation of radiation attenuation, which is\ntypically facilitated by synthesizing Computed Tomography (CT) images from MR\nscans to generate attenuation maps. However, existing MR-to-CT synthesis\nmethods for whole-body imaging often suffer from poor spatial alignment between\nthe generated CT and input MR images, and insufficient image quality for\nreliable use in downstream clinical tasks. In this paper, we present a novel 3D\nWavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by\nperforming modality translation in a learned latent space. By incorporating a\nWavelet Residual Module into the encoder-decoder architecture, we enhance the\ncapture and reconstruction of fine-scale features across image and latent\nspaces. To preserve anatomical integrity during the diffusion process, we\ndisentangle structural and modality-specific characteristics and anchor the\nstructural component to prevent warping. We also introduce a Dual Skip\nConnection Attention mechanism within the diffusion model, enabling the\ngeneration of high-resolution CT images with improved representation of bony\nstructures and soft-tissue contrast.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11557v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11702", "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption", "authors": ["Hein de Wilde", "Ali Mohammed Mansoor Alsahag", "Pierre Blanchet"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11702v1", "summary": "Railroad traffic disruption as a result of leaf-fall cost the UK rail\nindustry over 300 million per year and measures to mitigate such disruptions\nare employed on a large scale, with 1.67 million kilometers of track being\ntreated in the UK in 2021 alone. Therefore, the ability to anticipate the\ntiming of leaf-fall would offer substantial benefits for rail network\noperators, enabling the efficient scheduling of such mitigation measures.\nHowever, current methodologies for predicting leaf-fall exhibit considerable\nlimitations in terms of scalability and reliability. This study endeavors to\ndevise a prediction system that leverages specialized prediction methods and\nthe latest satellite data sources to generate both scalable and reliable\ninsights into leaf-fall timings. An LSTM network trained on ground-truth\nleaf-falling data combined with multispectral and meteorological satellite data\ndemonstrated a root-mean-square error of 6.32 days for predicting the start of\nleaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which\nimproves upon previous work on the topic, offers promising opportunities for\nthe optimization of leaf mitigation measures in the railway industry and the\nimprovement of our understanding of complex ecological systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11702v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12204", "title": "Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection", "authors": ["Pengyu Zhu", "Janghee Cho"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12204v1", "summary": "Adolescents' mobile technology use is often regulated through rigid control\nmechanisms that fail to account for their autonomy and natural usage patterns.\nDrawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this\nposition paper proposes Tao-Technology, a self-organizing, adaptive regulatory\nframework. Integrating insights from Reflective Informatics and Information\nEcologies, we explore how mobile technology can dynamically adjust to context\nwhile fostering self-reflection and meaning-making. This approach shifts from\nexternal restrictions to dynamic co-adaptative regulation, ensuring technology\ngovernance remains flexible yet structured, supporting adolescents in\ncultivating a balanced and intentional relationship with digital technology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12204v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12284", "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "authors": ["Artem Chervyakov", "Alexander Kharitonov", "Pavel Zadorozhny", "Adamenko Pavel", "Rodion Levichev", "Dmitrii Vorobev", "Dmitrii Salikhov", "Aidar Valeev", "Alena Pestova", "Maria Dziuba", "Ilseyar Alimova", "Artem Zavgorodnev", "Aleksandr Medvedev", "Stanislav Moiseev", "Elena Bruches", "Daniil Grebenkin", "Roman Derunets", "Vikulov Vladimir", "Anton Emelyanov", "Dmitrii Babaev", "Vladimir V. Ivanov", "Valentin Malykh", "Alena Fenogenova"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12284v1", "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12284v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11571", "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation", "authors": ["Varun Velankar"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11571v1", "summary": "Estimating a person's age from their gait has important applications in\nhealthcare, security and human-computer interaction. In this work, we review\nfifty-nine studies involving over seventy-five thousand subjects recorded with\nvideo, wearable and radar sensors. We observe that convolutional neural\nnetworks produce an average error of about 4.2 years, inertial-sensor models\nabout 4.5 years and multi-sensor fusion as low as 3.4 years, with notable\ndifferences between lab and real-world data. We then analyse sixty-three\nthousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population\ndataset to quantify correlations between age and five key metrics: stride\nlength, walking speed, step cadence, step-time variability and joint-angle\nentropy, with correlation coefficients of at least 0.27. Next, we fine-tune a\nResNet34 model and apply Grad-CAM to reveal that the network attends to the\nknee and pelvic regions, consistent with known age-related gait changes.\nFinally, on a one hundred thousand sample subset of the VersatileGait database,\nwe compare support vector machines, decision trees, random forests, multilayer\nperceptrons and convolutional neural networks, finding that deep networks\nachieve up to 96 percent accuracy while processing each sample in under 0.1\nseconds. By combining a broad meta-analysis with new large-scale experiments\nand interpretable visualizations, we establish solid performance baselines and\npractical guidelines for reducing gait-age error below three years in\nreal-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11571v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12255", "title": "Freshness, Persistence and Success of Scientific Teams", "authors": ["Hanjo D. Boekhout", "Eelke M. Heemskerk", "Nicolò Pisani", "Frank W. Takes"], "categories": ["cs.DL", "cs.SI"], "primary_category": "Subjects:       Digital Libraries (cs.DL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12255v1", "summary": "Team science dominates scientific knowledge production, but what makes\nacademic teams successful? Using temporal data on 25.2 million publications and\n31.8 million authors, we propose a novel network-driven approach to identify\nand study the success of persistent teams. Challenging the idea that\npersistence alone drives success, we find that team freshness - new\ncollaborations built on prior experience - is key to success. High impact\nresearch tends to emerge early in a team's lifespan. Analyzing complex team\noverlap, we find that teams open to new collaborative ties consistently produce\nbetter science. Specifically, team re-combinations that introduce new freshness\nimpulses sustain success, while persistence impulses from experienced teams are\nlinked to earlier impact. Together, freshness and persistence shape team\nsuccess across collaboration stages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12255v1", "cate": "cs.DL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11678", "title": "Towards a Non-Binary View of IPv6 Adoption", "authors": ["Sulyab Thottungal Valapu", "John Heidemann"], "categories": ["cs.NI", "C.2.6"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11678v1", "summary": "Twelve years have passed since World IPv6 Launch Day, but what is the current\nstate of IPv6 deployment? Prior work has examined IPv6 status as a binary: can\nyou use IPv6, or not? As deployment increases we must consider a more nuanced,\nnon-binary perspective on IPv6: how much and often can a user or a service use\nIPv6? We consider this question as a client, server, and cloud provider.\nConsidering the client's perspective, we observe user traffic. We see that the\nfraction of IPv6 traffic a user sends varies greatly, both across users and\nday-by-day, with a standard deviation of over 15%. We show this variation\noccurs for two main reasons. First, IPv6 traffic is primarily human-generated,\nthus showing diurnal patterns. Second, some services are IPv6-forward and\nothers IPv6-laggards, so as users do different things their fraction of IPv6\nvaries. We look at server-side IPv6 adoption in two ways. First, we expand\nanalysis of web services to examine how many are only partially IPv6 enabled\ndue to their reliance on IPv4-only resources. Our findings reveal that only\n12.5% of top 100k websites qualify as fully IPv6-ready. Finally, we examine\ncloud support for IPv6. Although all clouds and CDNs support IPv6, we find that\ntenant deployment rates vary significantly across providers. We find that ease\nof enabling IPv6 in the cloud is correlated with tenant IPv6 adoption rates,\nand recommend best practices for cloud providers to improve IPv6 adoption. Our\nresults suggest IPv6 deployment is growing, but many services lag, presenting a\npotential for improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11678v1", "cate": "cs.NI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.03034", "title": "Rethinking Data Protection in the (Generative) Artificial Intelligence Era", "authors": ["Yiming Li", "Shuo Shao", "Yu He", "Junfeng Guo", "Tianwei Zhang", "Zhan Qin", "Pin-Yu Chen", "Michael Backes", "Philip Torr", "Dacheng Tao", "Kui Ren"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Perspective paper for a broader scientific audience. The first two authors contributed equally to this paper. 13 pages", "url": "http://arxiv.org/abs/2507.03034v2", "summary": "The (generative) artificial intelligence (AI) era has profoundly reshaped the\nmeaning and value of data. No longer confined to static content, data now\npermeates every stage of the AI lifecycle from the training samples that shape\nmodel parameters to the prompts and outputs that drive real-world model\ndeployment. This shift renders traditional notions of data protection\ninsufficient, while the boundaries of what needs safeguarding remain poorly\ndefined. Failing to safeguard data in AI systems can inflict societal and\nindividual, underscoring the urgent need to clearly delineate the scope of and\nrigorously enforce data protection. In this perspective, we propose a\nfour-level taxonomy, including non-usability, privacy preservation,\ntraceability, and deletability, that captures the diverse protection needs\narising in modern (generative) AI models and systems. Our framework offers a\nstructured understanding of the trade-offs between data utility and control,\nspanning the entire AI pipeline, including training datasets, model weights,\nsystem prompts, and AI-generated content. We analyze representative technical\napproaches at each level and reveal regulatory blind spots that leave critical\nassets exposed. By offering a structured lens to align future AI technologies\nand governance with trustworthy data practices, we underscore the urgency of\nrethinking data protection for modern AI techniques and provide timely guidance\nfor developers, researchers, and regulators alike.", "comment": "Perspective paper for a broader scientific audience. The first two\n  authors contributed equally to this paper. 13 pages", "pdf_url": "http://arxiv.org/pdf/2507.03034v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-16"}
{"id": "2507.12273", "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot", "authors": ["Luca Garello", "Francesca Cocchella", "Alessandra Sciutti", "Manuel Catalano", "Francesco Rea"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12273v1", "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12273v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11560", "title": "A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing", "authors": ["Xin Wang", "Xiao Huan Li", "Xun Wang"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, accepted by ICCC 2025", "url": "http://arxiv.org/abs/2507.11560v1", "summary": "The integration of the Industrial Internet of Things (IIoT) with Artificial\nIntelligence-Generated Content (AIGC) offers new opportunities for smart\nmanufacturing, but it also introduces challenges related to\ncomputation-intensive tasks and low-latency demands. Traditional generative\nmodels based on cloud computing are difficult to meet the real-time\nrequirements of AIGC tasks in IIoT environments, and edge computing can\neffectively reduce latency through task offloading. However, the dynamic nature\nof AIGC tasks, model switching delays, and resource constraints impose higher\ndemands on edge computing environments. To address these challenges, this paper\nproposes an AIGC task offloading framework tailored for IIoT edge computing\nenvironments, considering the latency and energy consumption caused by AIGC\nmodel switching for the first time. IIoT devices acted as multi-agent\ncollaboratively offload their dynamic AIGC tasks to the most appropriate edge\nservers deployed with different generative models. A model aware AIGC task\noffloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG-MATO) is devised to minimize the latency and energy. Experimental\nresults show that MADDPG-MATO outperforms baseline algorithms, achieving an\naverage reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%\nincrease in task completion rate across four sets of experiments with model\nnumbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is\nrobust and efficient in dynamic, high-load IIoT environments.", "comment": "6 pages, 4 figures, accepted by ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2507.11560v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11706", "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs", "authors": ["Taira Tsuchiya", "Shinji Ito", "Haipeng Luo"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      40 pages", "url": "http://arxiv.org/abs/2507.11706v1", "summary": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting.", "comment": "40 pages", "pdf_url": "http://arxiv.org/pdf/2507.11706v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12212", "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness", "authors": ["Garyoung Kim", "Huisung Kwon", "Seoju Yun", "Yu-Won Youn"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2507.12212v1", "summary": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.12212v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12367", "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "authors": ["Diganta Misra", "Nizar Islah", "Victor May", "Brice Rauby", "Zihan Wang", "Justine Gehring", "Antonio Orvieto", "Muawiz Chaudhary", "Eilif B. Muller", "Irina Rish", "Samira Ebrahimi Kahou", "Massimo Caccia"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Version 2 of the dataset from: arXiv:2411.05830", "url": "http://arxiv.org/abs/2507.12367v1", "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.", "comment": "Version 2 of the dataset from: arXiv:2411.05830", "pdf_url": "http://arxiv.org/pdf/2507.12367v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11575", "title": "What cat is that? A re-id model for feral cats", "authors": ["Victor Caquilpan"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Master's project", "url": "http://arxiv.org/abs/2507.11575v1", "summary": "Feral cats exert a substantial and detrimental impact on Australian wildlife,\nplacing them among the most dangerous invasive species worldwide. Therefore,\nclosely monitoring these cats is essential labour in minimising their effects.\nIn this context, the potential application of Re-Identification (re-ID) emerges\nto enhance monitoring activities for these animals, utilising images captured\nby camera traps. This project explores different CV approaches to create a\nre-ID model able to identify individual feral cats in the wild. The main\napproach consists of modifying a part-pose guided network (PPGNet) model,\ninitially used in the re-ID of Amur tigers, to be applicable for feral cats.\nThis adaptation, resulting in PPGNet-Cat, which incorporates specific\nmodifications to suit the characteristics of feral cats images. Additionally,\nvarious experiments were conducted, particularly exploring contrastive learning\napproaches such as ArcFace loss. The main results indicate that PPGNet-Cat\nexcels in identifying feral cats, achieving high performance with a mean\nAverage Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes\nestablish PPGNet-Cat as a competitive model within the realm of re-ID.", "comment": "Master's project", "pdf_url": "http://arxiv.org/pdf/2507.11575v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2502.17926", "title": "Fediverse Sharing: Cross-Platform Interaction Dynamics between Threads and Mastodon Users", "authors": ["Ujun Jeong", "Alimohammad Beigi", "Anique Tahir", "Susan Xu Tang", "H. Russell Bernard", "Huan Liu"], "categories": ["cs.SI", "cs.CY"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      Accepted to ASONAM'25 Multidisciplinary Track", "url": "http://arxiv.org/abs/2502.17926v2", "summary": "Traditional social media platforms, once envisioned as digital town squares,\nnow face growing criticism over corporate control, content moderation, and\nprivacy concerns. Events such as Twitter's acquisition (now X) and major policy\nchanges have pushed users toward alternative platforms like Mastodon and\nThreads. However, this diversification has led to user dispersion and\nfragmented discussions across the walled gardens of social media platforms. To\naddress these issues, federation protocols like ActivityPub have been adopted,\nwith Mastodon leading efforts to build decentralized yet interconnected\nnetworks. In March 2024, Threads joined this federation by introducing its\nFediverse Sharing service, which enables interactions such as posts, replies,\nand likes between Threads and Mastodon users as if on a unified platform.\nBuilding on this development, we study the interactions between 20,000+ Threads\nusers and 20,000+ Mastodon users over a ten-month period. Our work lays the\nfoundation for research on cross-platform interactions and federation-driven\nplatform integration.", "comment": "Accepted to ASONAM'25 Multidisciplinary Track", "pdf_url": "http://arxiv.org/pdf/2502.17926v2", "cate": "cs.SI", "date": "2025-02-25", "updated": "2025-07-16"}
{"id": "2507.11798", "title": "On QoE-Aware Traffic Management for Real-time, Interactive Video with Time-variant Spatial Complexity", "authors": ["Szilveszter Nádas", "Lars Ernström", "David Lindero", "Jonathan Lynam"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11798v1", "summary": "We analyzed spatial complexity, defined as the relationship between the\nrequired bitrate and a corresponding picture Quality of Experience (QoE)\nmetric, for realistic, long, real-time, interactive video clips. Apart from\nvariation across different content types, e.g., game genres, we discovered\ntime-variability within a clip from second to second, and explored the\nramifications for traffic management. We introduced utility as an elegant way\nto manage resource sharing preferences. Our analysis of resource sharing\nmethods shows that frequent QoE-aware reallocation has significant performance\nadvantages compared to static rate allocation, even in case the latter is based\non rich information about long-term average spatial complexity. We have also\nshown that utility-based resource allocation has clear advantages over methods\ntargeting equal QoE allocation, it increases the average QoE, while it still\ncontrols the worst case QoE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11798v1", "cate": "cs.NI", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12391", "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "url": "http://arxiv.org/abs/2507.12391v1", "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "pdf_url": "http://arxiv.org/pdf/2507.12391v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11561", "title": "Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach", "authors": ["Lucas Erlacher", "Samuel Ruipérez-Campillo", "Holger Michel", "Sven Wellmann", "Thomas M. Sutter", "Ece Ozkan", "Julia E. Vogt"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11561v1", "summary": "Pulmonary hypertension (PH) in newborns is a critical condition characterized\nby elevated pressure in the pulmonary arteries, leading to right ventricular\nstrain and heart failure. While right heart catheterization (RHC) is the\ndiagnostic gold standard, echocardiography is preferred due to its non-invasive\nnature, safety, and accessibility. However, its accuracy highly depends on the\noperator, making PH assessment subjective. While automated detection methods\nhave been explored, most models focus on adults and rely on single-view\nechocardiographic frames, limiting their performance in diagnosing PH in\nnewborns. While multi-view echocardiography has shown promise in improving PH\nassessment, existing models struggle with generalizability. In this work, we\nemploy a multi-view variational autoencoder (VAE) for PH prediction using\nechocardiographic videos. By leveraging the VAE framework, our model captures\ncomplex latent representations, improving feature extraction and robustness. We\ncompare its performance against single-view and supervised learning approaches.\nOur results show improved generalization and classification accuracy,\nhighlighting the effectiveness of multi-view learning for robust PH assessment\nin newborns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11561v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11710", "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links", "authors": ["Jay Revolinsky", "Harry Shomer", "Jiliang Tang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      18 pages, 7 figures, preprint", "url": "http://arxiv.org/abs/2507.11710v1", "summary": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link\nprediction (LP) task. However, these models often rely on all dataset samples\nbeing drawn from the same distribution. In addition, graph generative models\n(GGMs) show a pronounced ability to generate novel output graphs. Despite this,\nGGM applications remain largely limited to domain-specific tasks. To bridge\nthis gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)\nstructurally-conditioned graph generation, and (2) adversarial co-training\nbetween an auto-encoder and GNN. As such, FLEX ensures structural-alignment\nbetween sample distributions to enhance link-prediction performance in\nout-of-distribution (OOD) scenarios. Notably, FLEX does not require expert\nknowledge to function in different OOD scenarios. Numerous experiments are\nconducted in synthetic and real-world OOD settings to demonstrate FLEX's\nperformance-enhancing ability, with further analysis for understanding the\neffects of graph data augmentation on link structures. The source code is\navailable here: https://github.com/revolins/FlexOOD.", "comment": "18 pages, 7 figures, preprint", "pdf_url": "http://arxiv.org/pdf/2507.11710v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12296", "title": "Humans are more gullible than LLMs in believing common psychological myths", "authors": ["Bevan Koopman", "Guido Zuccon"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12296v1", "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12296v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12415", "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?", "authors": ["Xinyi He", "Qian Liu", "Mingzhe Du", "Lin Yan", "Zhijie Fan", "Yiming Huang", "Zejian Yuan", "Zejun Ma"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12415v1", "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12415v1", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11579", "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation", "authors": ["Sathvik Chereddy", "John Femiani"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 63 figures, Proceedings of the 42nd International Conference on Machine Learning (ICML2025)", "url": "http://arxiv.org/abs/2507.11579v1", "summary": "We present SketchDNN, a generative model for synthesizing CAD sketches that\njointly models both continuous parameters and discrete class labels through a\nunified continuous-discrete diffusion process. Our core innovation is\nGaussian-Softmax diffusion, where logits perturbed with Gaussian noise are\nprojected onto the probability simplex via a softmax transformation,\nfacilitating blended class labels for discrete variables. This formulation\naddresses 2 key challenges, namely, the heterogeneity of primitive\nparameterizations and the permutation invariance of primitives in CAD sketches.\nOur approach significantly improves generation quality, reducing Fr\\'echet\nInception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)\nfrom 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch\ngeneration on the SketchGraphs dataset.", "comment": "17 pages, 63 figures, Proceedings of the 42nd International\n  Conference on Machine Learning (ICML2025)", "pdf_url": "http://arxiv.org/pdf/2507.11579v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2504.16946", "title": "MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation", "authors": ["Xiaotong Ye", "Nicolas Bougie", "Toshihiko Yamasaki", "Narimasa Watanabe"], "categories": ["cs.SI", "cs.AI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.16946v2", "summary": "Generative agents offer promising capabilities for simulating realistic urban\nbehaviors. However, existing methods oversimplify transportation choices, rely\nheavily on static agent profiles leading to behavioral homogenization, and\ninherit prohibitive computational costs. To address these limitations, we\npresent MobileCity, a lightweight simulation platform designed to model\nrealistic urban mobility with high computational efficiency. We introduce a\ncomprehensive transportation system with multiple transport modes, and collect\nquestionnaire data from respondents to construct agent profiles. To enable\nscalable simulation, agents perform action selection within a pre-generated\naction space and uses local models for efficient agent memory generation.\nThrough extensive micro and macro-level evaluations on 4,000 agents, we\ndemonstrate that MobileCity generates more realistic urban behaviors than\nbaselines while maintaining computational efficiency. We further explore\npractical applications such as predicting movement patterns and analyzing\ndemographic trends in transportation preferences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.16946v2", "cate": "cs.SI", "date": "2025-04-18", "updated": "2025-07-16"}
{"id": "2507.11935", "title": "Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview", "authors": ["Jikang Deng", "Fizza Hassan", "Hui Zhou", "Saad Al-Ahmadi", "Mohamed-Slim Alouini", "Daniel B. Da Costa"], "categories": ["cs.NI", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11935v1", "summary": "As the path toward 6G networks is being charted, the emerging applications\nhave motivated evolutions of network architectures to realize the efficient,\nreliable, and flexible wireless networks. Among the potential architectures,\nthe non-terrestrial network (NTN) and open radio access network (ORAN) have\nreceived increasing interest from both academia and industry. Although the\ndeployment of NTNs ensures coverage, enhances spectral efficiency, and improves\nthe resilience of wireless networks. The high altitude and mobility of NTN\npresent new challenges in the development and operations (DevOps) lifecycle,\nhindering intelligent and scalable network management due to the lack of native\nartificial intelligence (AI) capability. With the advantages of ORAN in\ndisaggregation, openness, virtualization, and intelligence, several works\npropose integrating ORAN principles into the NTN, focusing mainly on ORAN\ndeployment options based on transparent and regenerative systems. However, a\nholistic view of how to effectively combine ORAN and NTN throughout the DevOps\nlifecycle is still missing, especially regarding how intelligent ORAN addresses\nthe scalability challenges in NTN. Motivated by this, in this paper, we first\nprovide the background knowledge about ORAN and NTN, outline the\nstate-of-the-art research on ORAN for NTNs, and present the DevOps challenges\nthat motivate the adoption of ORAN solutions. We then propose the ORAN-based\nNTN framework, discussing its features and architectures in detail. These\ninclude the discussion about flexible fronthaul split, RAN intelligent\ncontrollers (RICs) enhancement for distributed learning, scalable deployment\narchitecture, and multi-domain service management. Finally, the future research\ndirections, including combinations of the ORAN-based NTN framework and other\nenabling technologies and schemes, as well as the candidate use cases, are\nhighlighted.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11935v1", "cate": "cs.NI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11854", "title": "Sub-Connected Hybrid Beamfocusing Design for RSMA-Enabled Near-Field Communications with Imperfect CSI and SIC", "authors": ["Jiasi Zhou", "Ruirui Chen", "Yanjing Sun", "Chintha Tellambura"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      11 pages and 7 figures", "url": "http://arxiv.org/abs/2507.11854v1", "summary": "Near-field spherical waves inherently encode both direction and distance\ninformation, enabling spotlight-like beam focusing for targeted interference\nmitigation. However, whether such beam focusing can fully eliminate\ninterference under perfect and imperfect channel state information (CSI),\nrendering advanced interference management schemes unnecessary, remains an open\nquestion. To address this, we investigate rate-splitting multiple access\n(RSMA)-enabled near-field communications (NFC) under imperfect SCI. Our\ntransmit scheme employs a sub-connected hybrid analog-digital (HAD)\narchitecture to reduce hardware overhead while incorporating imperfect\nsuccessive interference cancellation (SIC) for practical implementation. A\nminimum rate maximization problem is formulated by jointly optimizing the\nanalog beamfocuser, the digital beamfocuser, and the common rate allocation. To\nsolve the non-convex problem, we develop a penalty-based block coordinate\ndescent (BCD) algorithm, deriving closed-form expressions for the optimal\nanalog and digital beamfocusers solutions. Furthermore, to reduce computational\ncomplexity, we propose a low-complexity algorithm, where analog and digital\nbeamfocusers are designed in two separate stages. Simulation results underscore\nthat: 1) beamfocusing alone is insufficient to fully suppress interference even\nunder perfect CSI; 2) RSMA exhibits superior interference management over SDMA\nunder imperfect CSI and SIC conditions; 3) sub-connected HAD architecture\ndelivers near-optimal digital beamfocusing performance with fewer radio\nfrequency chains.", "comment": "11 pages and 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.11854v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12407", "title": "Regrasp Maps for Sequential Manipulation Planning", "authors": ["Svetlana Levit", "Marc Toussaint"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12407v1", "summary": "We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12407v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11566", "title": "Emergent Heterogeneous Swarm Control Through Hebbian Learning", "authors": ["Fuda van Diggelen", "Tugay Alperen Karagüzel", "Andres Garcia Rincon", "A. E. Eiben", "Dario Floreano", "Eliseo Ferrante"], "categories": ["cs.NE", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11566v1", "summary": "In this paper, we introduce Hebbian learning as a novel method for swarm\nrobotics, enabling the automatic emergence of heterogeneity. Hebbian learning\npresents a biologically inspired form of neural adaptation that solely relies\non local information. By doing so, we resolve several major challenges for\nlearning heterogeneous control: 1) Hebbian learning removes the complexity of\nattributing emergent phenomena to single agents through local learning rules,\nthus circumventing the micro-macro problem; 2) uniform Hebbian learning rules\nacross all swarm members limit the number of parameters needed, mitigating the\ncurse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian\nlearning rules based on swarm-level behaviour minimises the need for extensive\nprior knowledge typically required for optimising heterogeneous swarms. This\nwork demonstrates that with Hebbian learning heterogeneity naturally emerges,\nresulting in swarm-level behavioural switching and in significantly improved\nswarm capabilities. It also demonstrates how the evolution of Hebbian learning\nrules can be a valid alternative to Multi Agent Reinforcement Learning in\nstandard benchmarking tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11566v1", "cate": "cs.NE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11729", "title": "Globalization for Scalable Short-term Load Forecasting", "authors": ["Amirhossein Ahmadi", "Hamidreza Zareipour", "Henry Leung"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      63 pages with 22 figures", "url": "http://arxiv.org/abs/2507.11729v1", "summary": "Forecasting load in power transmission networks is essential across various\nhierarchical levels, from the system level down to individual points of\ndelivery (PoD). While intuitive and locally accurate, traditional local\nforecasting models (LFMs) face significant limitations, particularly in\nhandling generalizability, overfitting, data drift, and the cold start problem.\nThese methods also struggle with scalability, becoming computationally\nexpensive and less efficient as the network's size and data volume grow. In\ncontrast, global forecasting models (GFMs) offer a new approach to enhance\nprediction generalizability, scalability, accuracy, and robustness through\nglobalization and cross-learning. This paper investigates global load\nforecasting in the presence of data drifts, highlighting the impact of\ndifferent modeling techniques and data heterogeneity. We explore\nfeature-transforming and target-transforming models, demonstrating how\nglobalization, data heterogeneity, and data drift affect each differently. In\naddition, we examine the role of globalization in peak load forecasting and its\npotential for hierarchical forecasting. To address data heterogeneity and the\nbalance between globality and locality, we propose separate time series\nclustering (TSC) methods, introducing model-based TSC for feature-transforming\nmodels and new weighted instance-based TSC for target-transforming models.\nThrough extensive experiments on a real-world dataset of Alberta's electricity\nload, we demonstrate that global target-transforming models consistently\noutperform their local counterparts, especially when enriched with global\nfeatures and clustering techniques. In contrast, global feature-transforming\nmodels face challenges in balancing local and global dynamics, often requiring\nTSC to manage data heterogeneity effectively.", "comment": "63 pages with 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.11729v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12298", "title": "TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials", "authors": ["Rui Sheng", "Xingbo Wang", "Jiachen Wang", "Xiaofu Jin", "Zhonghua Sheng", "Zhenxing Xu", "Suraj Rajendran", "Huamin Qu", "Fei Wang"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12298v1", "summary": "Eligibility criteria play a critical role in clinical trials by determining\nthe target patient population, which significantly influences the outcomes of\nmedical interventions. However, current approaches for designing eligibility\ncriteria have limitations to support interactive exploration of the large space\nof eligibility criteria. They also ignore incorporating detailed\ncharacteristics from the original electronic health record (EHR) data for\ncriteria refinement. To address these limitations, we proposed TrialCompass, a\nvisual analytics system integrating a novel workflow, which can empower\nclinicians to iteratively explore the vast space of eligibility criteria\nthrough knowledge-driven and outcome-driven approaches. TrialCompass supports\nhistory-tracking to help clinicians trace the evolution of their adjustments\nand decisions when exploring various forms of data (i.e., eligibility criteria,\noutcome metrics, and detailed characteristics of original EHR data) through\nthese two approaches. This feature can help clinicians comprehend the impact of\neligibility criteria on outcome metrics and patient characteristics, which\nfacilitates systematic refinement of eligibility criteria. Using a real-world\ndataset, we demonstrated the effectiveness of TrialCompass in providing\ninsights into designing eligibility criteria for septic shock and\nsepsis-associated acute kidney injury. We also discussed the research prospects\nof applying visual analytics to clinical trials.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12298v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11948", "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "authors": ["Carlo Baronio", "Pietro Marsella", "Ben Pan", "Simon Guo", "Silas Alberti"], "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11948v1", "summary": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11948v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11638", "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders", "authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Maryam Mohsin", "Samuel D. Relton"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published in Medical Image Understanding and Analysis (MIUA) 2025", "url": "http://arxiv.org/abs/2507.11638v1", "summary": "Effective treatment for rectal cancer relies on accurate lymph node\nmetastasis (LNM) staging. However, radiological criteria based on lymph node\n(LN) size, shape and texture morphology have limited diagnostic accuracy. In\nthis work, we investigate applying a Variational Autoencoder (VAE) as a feature\nencoder model to replace the large pre-trained Convolutional Neural Network\n(CNN) used in existing approaches. The motivation for using a VAE is that the\ngenerative model aims to reconstruct the images, so it directly encodes visual\nfeatures and meaningful patterns across the data. This leads to a disentangled\nand structured latent space which can be more interpretable than a CNN. Models\nare deployed on an in-house MRI dataset with 168 patients who did not undergo\nneo-adjuvant treatment. The post-operative pathological N stage was used as the\nground truth to evaluate model predictions. Our proposed model 'VAE-MLP'\nachieved state-of-the-art performance on the MRI dataset, with cross-validated\nmetrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85\n+/- 0.05. Code is available at:\nhttps://github.com/benkeel/Lymph_Node_Classification_MIUA.", "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025", "pdf_url": "http://arxiv.org/pdf/2507.11638v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2501.05590", "title": "Negative Ties Highlight Hidden Extremes in Social Media Polarization", "authors": ["Elena Candellone", "Shazia'Ayn Babul", "Özgür Togay", "Alexandre Bovet", "Javier Garcia-Bernardo"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.05590v4", "summary": "Human interactions in the online world comprise a combination of positive and\nnegative exchanges. These diverse interactions can be captured using signed\nnetwork representations, where edges take positive or negative weights to\nindicate the sentiment of the interaction between individuals. Signed networks\noffer valuable insights into online political polarization by capturing\nantagonistic interactions and ideological divides on social media platforms.\nThis study analyzes polarization on Meneame, a Spanish social media platform\nthat facilitates engagement with news stories through comments and voting.\nUsing a dual-method approach, Signed Hamiltonian Eigenvector Embedding for\nProximity (SHEEP) for signed networks and Correspondence Analysis (CA) for\nunsigned networks, we investigate how including negative ties enhances the\nunderstanding of structural polarization levels across different conversation\ntopics on the platform. While the unsigned Meneame network effectively\ndelineates ideological communities, only by incorporating negative ties can we\nidentify ideologically extreme users who engage in antagonistic behaviors:\nwithout them, the most extreme users remain indistinguishable from their less\nconfrontational ideological peers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.05590v4", "cate": "physics.soc-ph", "date": "2025-01-09", "updated": "2025-07-16"}
{"id": "2507.12265", "title": "FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks", "authors": ["Zihan Zhu", "Dongchao Wu", "Zhanbang Zhang", "Jian Yang"], "categories": ["cs.NI", "cs.DS"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12265v1", "summary": "Ever since Clos topologies were used in datacenter networks (DCNs), a\npractical centralized scheduling algorithm that supports dynamic scheduling has\nbeen absent. The introduction of optical switches in DCNs as a future-proof\nsolution exacerbates this problem due to several properties of optical\nswitches, such as the fact that they are generally bufferless and therefore\nrely on centralized scheduling, and that they have long switching times and\ntherefore require the number of rearrangements to be minimized.\n  In this paper, we propose a centralized scheduling algorithm that achieves\ntheoretical maximum throughput even in one-rate bidirectional Clos networks,\nwhile producing schemes with near-minimal numbers of rearrangements. It is the\nonly algorithm that directly supports bidirectional Clos networks and has a\ntime efficiency high enough to support dynamic scheduling to date. For static\nminimal rewiring, its running time ranges from a fraction to a few hundredths\nof other algorithms, and the number of rearrangements has also been steadily\nimproved, allowing for more frequent adjustments and less impact on ongoing\ncommunications. In addition, the algorithm is very flexible and can support\nvarious functional requirements in real-world environments. We achieve this\nresult through the replacement chain concept and bitset optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12265v1", "cate": "cs.NI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12019", "title": "The Role of Rank in Mismatched Low-Rank Symmetric Matrix Estimation", "authors": ["Panpan Niu", "Yuhao Liu", "Teng Fu", "Jie Fan", "Chaowen Deng", "Zhongyi Huang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12019v1", "summary": "We investigate the performance of a Bayesian statistician tasked with\nrecovering a rank-\\(k\\) signal matrix \\(\\bS \\bS^{\\top} \\in \\mathbb{R}^{n \\times\nn}\\), corrupted by element-wise additive Gaussian noise. This problem lies at\nthe core of numerous applications in machine learning, signal processing, and\nstatistics. We derive an analytic expression for the asymptotic mean-square\nerror (MSE) of the Bayesian estimator under mismatches in the assumed signal\nrank, signal power, and signal-to-noise ratio (SNR), considering both sphere\nand Gaussian signals. Additionally, we conduct a rigorous analysis of how rank\nmismatch influences the asymptotic MSE. Our primary technical tools include the\nspectrum of Gaussian orthogonal ensembles (GOE) with low-rank perturbations and\nasymptotic behavior of \\(k\\)-dimensional spherical integrals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12019v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12431", "title": "Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement", "authors": ["Connor Burgess", "Kyle Douin", "Amir Kordijazi"], "categories": ["cs.RO", "physics.ins-det"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 pages, 4 figures", "url": "http://arxiv.org/abs/2507.12431v1", "summary": "The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work\ncell developed to automate the measurement of surface wettability on 3D-printed\nmaterials. Designed for precision, repeatability, and safety, ACAT addresses\nthe limitations of manual contact angle testing by combining programmable\nrobotics, precise liquid dispensing, and a modular software-hardware\narchitecture. The system is composed of three core subsystems: (1) an\nelectrical system including power, control, and safety circuits compliant with\nindustrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software\ncontrol system based on a Raspberry Pi and Python, featuring fault detection,\nGPIO logic, and operator interfaces; and (3) a mechanical system that includes\na 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser\nenclosed within a safety-certified frame. The ACAT enables high-throughput,\nautomated surface characterization and provides a robust platform for future\nintegration into smart manufacturing and materials discovery workflows. This\npaper details the design methodology, implementation strategies, and system\nintegration required to develop the ACAT platform.", "comment": "14 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.12431v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11569", "title": "Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?", "authors": ["Hanxue Gu", "Yaqian Chen", "Nicholas Konz", "Qihang Li", "Maciej A. Mazurowski"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      3 figures, 9 pages", "url": "http://arxiv.org/abs/2507.11569v1", "summary": "Foundation models, pre-trained on large image datasets and capable of\ncapturing rich feature representations, have recently shown potential for\nzero-shot image registration. However, their performance has mostly been tested\nin the context of rigid or less complex structures, such as the brain or\nabdominal organs, and it remains unclear whether these models can handle more\nchallenging, deformable anatomy. Breast MRI registration is particularly\ndifficult due to significant anatomical variation between patients, deformation\ncaused by patient positioning, and the presence of thin and complex internal\nstructure of fibroglandular tissue, where accurate alignment is crucial.\nWhether foundation model-based registration algorithms can address this level\nof complexity remains an open question. In this study, we provide a\ncomprehensive evaluation of foundation model-based registration algorithms for\nbreast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,\nMedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that\ncapture variations in different years and dates, sequences, modalities, and\npatient disease status (lesion versus no lesion). Our results show that\nfoundation model-based algorithms such as SAM outperform traditional\nregistration baselines for overall breast alignment, especially under large\ndomain shifts, but struggle with capturing fine details of fibroglandular\ntissue. Interestingly, additional pre-training or fine-tuning on medical or\nbreast-specific images in MedSAM and SSLSAM, does not improve registration\nperformance and may even decrease it in some cases. Further work is needed to\nunderstand how domain-specific training influences registration and to explore\ntargeted strategies that improve both global alignment and fine structure\naccuracy. We also publicly release our code at\n\\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.", "comment": "3 figures, 9 pages", "pdf_url": "http://arxiv.org/pdf/2507.11569v1", "cate": "eess.IV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11732", "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning", "authors": ["Shiyu Chen", "Cencheng Shen", "Youngser Park", "Carey E. Priebe"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11732v1", "summary": "Graph neural networks (GNNs) have emerged as a powerful framework for a wide\nrange of node-level graph learning tasks. However, their performance is often\nconstrained by reliance on random or minimally informed initial feature\nrepresentations, which can lead to slow convergence and suboptimal solutions.\nIn this paper, we leverage a statistically grounded method, one-hot graph\nencoder embedding (GEE), to generate high-quality initial node features that\nenhance the end-to-end training of GNNs. We refer to this integrated framework\nas the GEE-powered GNN (GG), and demonstrate its effectiveness through\nextensive simulations and real-world experiments across both unsupervised and\nsupervised settings. In node clustering, GG consistently achieves\nstate-of-the-art performance, ranking first across all evaluated real-world\ndatasets, while exhibiting faster convergence compared to the standard GNN. For\nnode classification, we further propose an enhanced variant, GG-C, which\nconcatenates the outputs of GG and GEE and outperforms competing baselines.\nThese results confirm the importance of principled, structure-aware feature\ninitialization in realizing the full potential of GNNs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11732v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12334", "title": "An Analysis of Text Functions in Information Visualization", "authors": ["Chase Stokes", "Anjana Arunkumar", "Marti A. Hearst", "Lace Padilla"], "categories": ["cs.HC", "H.5.0"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures, IEEE VIS Conference", "url": "http://arxiv.org/abs/2507.12334v1", "summary": "Text is an integral but understudied component of visualization design.\nAlthough recent studies have examined how text elements (e.g., titles and\nannotations) influence comprehension, preferences, and predictions, many\nquestions remain about textual design and use in practice. This paper\nintroduces a framework for understanding text functions in information\nvisualizations, building on and filling gaps in prior classifications and\ntaxonomies. Through an analysis of 120 real-world visualizations and 804 text\nelements, we identified ten distinct text functions, ranging from identifying\ndata mappings to presenting valenced subtext. We further identify patterns in\ntext usage and conduct a factor analysis, revealing four overarching\ntext-informed design strategies: Attribution and Variables, Annotation-Centric\nDesign, Visual Embellishments, and Narrative Framing. In addition to these\nfactors, we explore features of title rhetoric and text multifunctionality,\nwhile also uncovering previously unexamined text functions, such as text\nreplacing visual elements. Our findings highlight the flexibility of text,\ndemonstrating how different text elements in a given design can combine to\ncommunicate, synthesize, and frame visual information. This framework adds\nimportant nuance and detail to existing frameworks that analyze the diverse\nroles of text in visualization.", "comment": "11 pages, 3 figures, IEEE VIS Conference", "pdf_url": "http://arxiv.org/pdf/2507.12334v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2402.05102", "title": "You Can REST Now: Automated REST API Documentation and Testing via LLM-Assisted Request Mutations", "authors": ["Alix Decrop", "Xavier Devroey", "Mike Papadakis", "Pierre-Yves Schobbens", "Gilles Perrouin"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.05102v2", "summary": "REST APIs are prevalent among web service implementations, easing\ninteroperability through the HTTP protocol. API testers and users exploit the\nwidely adopted OpenAPI Specification (OAS), a machine-readable standard to\ndocument REST APIs. However, documenting APIs is a time-consuming and\nerror-prone task, and existing documentation is not always complete, publicly\naccessible, or up-to-date. This situation limits the efficiency of testing\ntools and hinders human comprehension. Large Language Models (LLMs) offer the\npotential to automatically infer API documentation, using their colossal\ntraining data. In this paper, we present RESTSpecIT, the first automated\napproach that infers documentation and performs black-box testing of REST APIs\nby leveraging LLMs. Our approach requires minimal user input compared to\nstate-of-the-art tools; Given an API name and an LLM access key, RESTSpecIT\ngenerates API request seeds and mutates them with data returned by the LLM. The\ntool then analyzes API responses for documentation inference and testing\npurposes. RESTSpecIT utilizes an in-context prompt masking strategy, requiring\nno prior model fine-tuning. We evaluate the quality of our tool with three\nstate-of-the-art LLMs: DeepSeek V3, GPT-4.1, and GPT-3.5. Our evaluation\ndemonstrates that RESTSpecIT can (1) infer documentation with 88.62% of routes\nand 89.25% of query parameters found on average, (2) discover undocumented API\ndata, (3) operate efficiently (in terms of model costs, requests sent,\nruntime), and (4) assist REST API testing by uncovering server errors and\ngenerating valid OpenAPI Specification inputs for testing tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.05102v2", "cate": "cs.SE", "date": "2024-02-07", "updated": "2025-07-15"}
{"id": "2507.11642", "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11642v1", "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11642v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12443", "title": "LLM-Based Config Synthesis requires Disambiguation", "authors": ["Rajdeep Mondal", "Nikolaj Bjorner", "Todd Millstein", "Alan Tang", "George Varghese"], "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.PL"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12443v1", "summary": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12443v1", "cate": "cs.NI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12073", "title": "On the error correction of iterative bounded distance decoding of generalized LDPC codes", "authors": ["David Burshtein"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Under review in IEEE, Submitted: December 2024, Revised: July 2025", "url": "http://arxiv.org/abs/2507.12073v1", "summary": "Consider an ensemble of regular generalized LDPC (GLDPC) codes and assume\nthat the same component code is associated with each parity check node. To\ndecode a GLDPC code from the ensemble, we use the bit flipping bounded distance\ndecoding algorithm, which is an extension of the bit flipping algorithm for\nLDPC codes. Previous work has shown conditions, under which, for a typical code\nin the ensemble with blocklength sufficiently large, a positive constant\nfraction of worst case errors can be corrected. In this work we first show that\nthese requirements can be relaxed for ensembles with small left degrees. While\nprevious work on GLDPC codes has considered expander graph arguments, our\nanalysis formulates a necessary condition that the Tanner graph needs to\nsatisfy for a failure event and then shows that the probability of this event\nvanishes for a sufficiently large blocklength. We then extend the analysis to\nrandom error correction and derive a lower bound on the fraction of random\nerrors that can be corrected asymptotically. We discuss the extension of our\nresults to non-binary GLDPC codes and present numerical examples.", "comment": "Under review in IEEE, Submitted: December 2024, Revised: July 2025", "pdf_url": "http://arxiv.org/pdf/2507.12073v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12440", "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      More videos can be found on our website: this https URL", "url": "http://arxiv.org/abs/2507.12440v1", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "pdf_url": "http://arxiv.org/pdf/2507.12440v1", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11588", "title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics", "authors": ["Suyuan Zhao", "Yizhen Luo", "Ganbo Yang", "Yan Zhong", "Hao Zhou", "Zaiqing Nie"], "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Genomics (q-bio.GN)", "pdf_link": null, "comments": "Comments:      Accpeted by ICML 2024", "url": "http://arxiv.org/abs/2507.11588v1", "summary": "Spatial Transcriptomics (ST) technologies provide biologists with rich\ninsights into single-cell biology by preserving spatial context of cells.\nBuilding foundational models for ST can significantly enhance the analysis of\nvast and complex data sources, unlocking new perspectives on the intricacies of\nbiological tissues. However, modeling ST data is inherently challenging due to\nthe need to extract multi-scale information from tissue slices containing vast\nnumbers of cells. This process requires integrating macro-scale tissue\nmorphology, micro-scale cellular microenvironment, and gene-scale gene\nexpression profile. To address this challenge, we propose SToFM, a multi-scale\nSpatial Transcriptomics Foundation Model. SToFM first performs multi-scale\ninformation extraction on each ST slice, to construct a set of ST sub-slices\nthat aggregate macro-, micro- and gene-scale information. Then an SE(2)\nTransformer is used to obtain high-quality cell representations from the\nsub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest\nhigh-resolution spatial transcriptomics corpus for pretraining. SToFM achieves\noutstanding performance on a variety of downstream tasks, such as tissue region\nsemantic segmentation and cell type annotation, demonstrating its comprehensive\nunderstanding of ST data", "comment": "Accpeted by ICML 2024", "pdf_url": "http://arxiv.org/pdf/2507.11588v1", "cate": "q-bio.GN", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11739", "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction", "authors": ["Urban Fasel"], "categories": ["cs.LG", "cs.CE", "math.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11739v1", "summary": "The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for\ndiscovering nonlinear dynamical system models from data. Quantifying\nuncertainty in SINDy models is essential for assessing their reliability,\nparticularly in safety-critical applications. While various uncertainty\nquantification methods exist for SINDy, including Bayesian and ensemble\napproaches, this work explores the integration of Conformal Prediction, a\nframework that can provide valid prediction intervals with coverage guarantees\nbased on minimal assumptions like data exchangeability. We introduce three\napplications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)\nquantifying uncertainty in time series prediction, (2) model selection based on\nlibrary feature importance, and (3) quantifying the uncertainty of identified\nmodel coefficients using feature conformal prediction. We demonstrate the three\napplications on stochastic predator-prey dynamics and several chaotic dynamical\nsystems. We show that conformal prediction methods integrated with E-SINDy can\nreliably achieve desired target coverage for time series forecasting,\neffectively quantify feature importance, and produce more robust uncertainty\nintervals for model coefficients, even under non-Gaussian noise, compared to\nstandard E-SINDy coefficient estimates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11739v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12337", "title": "MExplore: an entity-based visual analytics approach for medical expertise acquisition", "authors": ["Xiao Pang", "Yan Huang", "Chang Liu", "JiYuan Liu", "MingYou Liu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12337v1", "summary": "Acquiring medical expertise is a critical component of medical education and\nprofessional development. While existing studies focus primarily on\nconstructing medical knowledge bases or developing learning tools based on the\nstructured, private healthcare data, they often lack methods for extracting\nexpertise from unstructured medical texts. These texts constitute a significant\nportion of medical literature and offer greater flexibility and detail compared\nto structured data formats. Furthermore, many studies fail to provide explicit\nanalytical and learning pathways in this context.\n  This paper introduces MExplore, an interactive visual analytics system\ndesigned to support the acquisition of medical expertise. To address the\nchallenges of the inconsistencies and confidentiality concerns inherent in\nunstructured medical texts, we propose a workflow that employs a fine-tuned\nBERT-based model to extract medical entities (MEs) from them. We then present a\nnovel multilevel visual analysis framework that integrates multiple coordinated\nvisualizations, enabling a progressive and interactive exploration of medical\nknowledge.\n  To assess the effectiveness of MExplore, we conducted three case studies, a\nuser study, and interviews with domain experts. The results indicate that the\nsystem significantly enhances the medical expertise acquisition process,\nproviding an effective interactive approach for acquiring and retaining\nknowledge from medical texts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12337v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2410.00603", "title": "An Empirical Study of Large Language Models for Type and Call Graph Analysis in Python and JavaScript", "authors": ["Ashwin Prasad Shivarpatna Venkatesh", "Rose Sunil", "Samkutty Sabu", "Amir M. Mir", "Sofia Reis", "Eric Bodden"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted to be published in the EMSE journal", "url": "http://arxiv.org/abs/2410.00603v2", "summary": "Large Language Models (LLMs) are increasingly being explored for their\npotential in software engineering, particularly in static analysis tasks. In\nthis study, we investigate the potential of current LLMs to enhance call-graph\nanalysis and type inference for Python and JavaScript programs. We empirically\nevaluated 24 LLMs, including OpenAI's GPT series and open-source models like\nLLaMA and Mistral, using existing and newly developed benchmarks. Specifically,\nwe enhanced TypeEvalPy, a micro-benchmarking framework for type inference in\nPython, with auto-generation capabilities, expanding its scope from 860 to\n77,268 type annotations for Python. Additionally, we introduced SWARM-CG and\nSWARM-JS, comprehensive benchmarking suites for evaluating call-graph\nconstruction tools across multiple programming languages.\n  Our findings reveal a contrasting performance of LLMs in static analysis\ntasks. For call-graph generation, traditional static analysis tools such as\nPyCG for Python and Jelly for JavaScript consistently outperform LLMs. While\nadvanced models like mistral-large-it-2407-123b and gpt-4o show promise, they\nstill struggle with completeness and soundness in call-graph analysis across\nboth languages. In contrast, LLMs demonstrate a clear advantage in type\ninference for Python, surpassing traditional tools like HeaderGen and hybrid\napproaches such as HiTyper. These results suggest that, while LLMs hold promise\nin type inference, their limitations in call-graph analysis highlight the need\nfor further research. Our study provides a foundation for integrating LLMs into\nstatic analysis workflows, offering insights into their strengths and current\nlimitations.", "comment": "Accepted to be published in the EMSE journal", "pdf_url": "http://arxiv.org/pdf/2410.00603v2", "cate": "cs.SE", "date": "2024-10-01", "updated": "2025-07-16"}
{"id": "2507.11653", "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization", "authors": ["Hannah Shafferman", "Annika Thomas", "Jouko Kinnari", "Michael Ricard", "Jose Nino", "Jonathan How"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.11653v1", "summary": "Global localization is critical for autonomous navigation, particularly in\nscenarios where an agent must localize within a map generated in a different\nsession or by another agent, as agents often have no prior knowledge about the\ncorrelation between reference frames. However, this task remains challenging in\nunstructured environments due to appearance changes induced by viewpoint\nvariation, seasonal changes, spatial aliasing, and occlusions -- known failure\nmodes for traditional place recognition methods. To address these challenges,\nwe propose VISTA (View-Invariant Segmentation-Based Tracking for Frame\nAlignment), a novel open-set, monocular global localization framework that\ncombines: 1) a front-end, object-based, segmentation and tracking pipeline,\nfollowed by 2) a submap correspondence search, which exploits geometric\nconsistencies between environment maps to align vehicle reference frames. VISTA\nenables consistent localization across diverse camera viewpoints and seasonal\nchanges, without requiring any domain-specific training or finetuning. We\nevaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a\n69% improvement in recall over baseline methods. Furthermore, we maintain a\ncompact object-based map that is only 0.6% the size of the most\nmemory-conservative baseline, making our approach capable of real-time\nimplementation on resource-constrained platforms.", "comment": "9 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.11653v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12445", "title": "CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments", "authors": ["Soheil Mahdizadeh", "Amir Mahdi Rasouli", "Mohammad Pourashory", "Sadra Galavani", "Mohsen Ansari"], "categories": ["cs.NI", "cs.AR", "cs.DC"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12445v1", "summary": "Reducing latency in the Internet of Things (IoT) is a critical concern. While\ncloud computing facilitates communication, it falls short of meeting real-time\nrequirements reliably. Edge and fog computing have emerged as viable solutions\nby positioning computing nodes closer to end users, offering lower latency and\nincreased processing power. An edge-fog framework comprises various components,\nincluding edge and fog nodes, whose strategic placement is crucial as it\ndirectly impacts latency and system cost. This paper presents an effective and\ntunable node placement strategy based on a genetic algorithm to address the\noptimization problem of deploying edge and fog nodes. The main objective is to\nminimize latency and cost through optimal node placement. Simulation results\ndemonstrate that the proposed framework achieves up to 2.77% latency and 31.15%\ncost reduction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12445v1", "cate": "cs.NI", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12155", "title": "Lowering Error Floors for Hard Decision Decoding of OFEC Code", "authors": ["Jasper Lagendijk", "Yunus Can Gültekin", "Alexios Balatsoukas-Stimming", "Gabriele Liga", "Alex Alvarado"], "categories": ["cs.IT", "math.IT", "94-06"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      4 pages, 4 figures; accepted to SC4 of ECOC 2025", "url": "http://arxiv.org/abs/2507.12155v1", "summary": "Stall patterns are known to cause an error floor in hard decision decoding of\nthe OFEC code. We propose a novel stall pattern removal algorithm that lowers\nthe error floor of state-of-the-art algorithms by an order of magnitude", "comment": "4 pages, 4 figures; accepted to SC4 of ECOC 2025", "pdf_url": "http://arxiv.org/pdf/2507.12155v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2304.12063", "title": "Risk in Stochastic and Robust Model Predictive Path-Following Control for Vehicular Motion Planning", "authors": ["Leon Tolksdorf", "Arturo Tejada", "Nathan van de Wouw", "Christian Birkner"], "categories": ["math.OC", "cs.RO"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Accepted for the 2023 Intelligent Vehicles Symposium, 8 pages", "url": "http://arxiv.org/abs/2304.12063v1", "summary": "In automated driving, risk describes potential harm to passengers of an\nautonomous vehicle (AV) and other road users. Recent studies suggest that\nhuman-like driving behavior emerges from embedding risk in AV motion planning\nalgorithms. Additionally, providing evidence that risk is minimized during the\nAV operation is essential to vehicle safety certification. However, there has\nyet to be a consensus on how to define and operationalize risk in motion\nplanning or how to bound or minimize it during operation. In this paper, we\ndefine a stochastic risk measure and introduce it as a constraint into both\nrobust and stochastic nonlinear model predictive path-following controllers\n(RMPC and SMPC respectively). We compare the vehicle's behavior arising from\nemploying SMPC and RMPC with respect to safety and path-following performance.\nFurther, the implementation of an automated driving example is provided,\nshowcasing the effects of different risk tolerances and uncertainty growths in\npredictions of other road users for both cases. We find that the RMPC is\nsignificantly more conservative than the SMPC, while also displaying greater\nfollowing errors towards references. Further, the RMPCs behavior cannot be\nconsidered as human-like. Moreover, unlike SMPC, the RMPC cannot account for\ndifferent risk tolerances. The RMPC generates undesired driving behavior for\neven moderate uncertainties, which are handled better by the SMPC.", "comment": "Accepted for the 2023 Intelligent Vehicles Symposium, 8 pages", "pdf_url": "http://arxiv.org/pdf/2304.12063v1", "cate": "math.OC", "date": "2023-04-24", "updated": "2023-04-24"}
{"id": "2507.11597", "title": "AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce", "authors": ["Richard Timpone", "Yongwei Yang"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Paper prepared for the 2025 European Survey Research Association Conference; 30 pages, 5 tables and 4 figures", "url": "http://arxiv.org/abs/2507.11597v1", "summary": "AI is transforming research. It is being leveraged to construct surveys,\nsynthesize data, conduct analysis, and write summaries of the results. While\nthe promise is to create efficiencies and increase quality, the reality is not\nalways as clear cut. Leveraging our framework of Truth, Beauty, and Justice\n(TBJ) which we use to evaluate AI, machine learning and computational models\nfor effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024),\nwe consider the potential and limitation of analytic, generative, and agentic\nAI to augment data scientists or take on tasks traditionally done by human\nanalysts and researchers. While AI can be leveraged to assist analysts in their\ntasks, we raise some warnings about push-button automation. Just as earlier\neras of survey analysis created some issues when the increased ease of using\nstatistical software allowed researchers to conduct analyses they did not fully\nunderstand, the new AI tools may create similar but larger risks. We emphasize\na human-machine collaboration perspective (Daugherty and Wilson 2018)\nthroughout the data science workflow and particularly call out the vital role\nthat data scientists play under VUCA decision areas. We conclude by encouraging\nthe advance of AI tools to complement data scientists but advocate for\ncontinued training and understanding of methods to ensure the substantive value\nof research is fully achieved by applying, interpreting, and acting upon\nresults most effectively and ethically.", "comment": "Paper prepared for the 2025 European Survey Research Association\n  Conference; 30 pages, 5 tables and 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.11597v1", "cate": "cs.CY", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11757", "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction", "authors": ["Yuehua Song", "Yong Gao"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11757v1", "summary": "Accurately predicting drug-target interactions (DTIs) is pivotal for\nadvancing drug discovery and target validation techniques. While machine\nlearning approaches including those that are based on Graph Neural Networks\n(GNN) have achieved notable success in DTI prediction, many of them have\ndifficulties in effectively integrating the diverse features of drugs, targets\nand their interactions. To address this limitation, we introduce a novel\nframework to take advantage of the power of both transductive learning and\ninductive learning so that features at molecular level and drug-target\ninteraction network level can be exploited. Within this framework is a\nGNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and\ntarget molecular structures as meta-nodes in a drug-target interaction graph,\nenabling a detailed exploration of their intricate relationships. To evaluate\nthe proposed model, we have compiled a special benchmark comprising drug\nSMILES, protein sequences, and their interaction data, which is interesting in\nits own right. Our experimental results demonstrate that the GiG model\nsignificantly outperforms existing approaches across all evaluation metrics,\nhighlighting the benefits of integrating different learning paradigms and\ninteraction data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11757v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12377", "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight", "authors": ["Ke Er Amy Zhang", "Jodie Jenkinson", "Laura Garrison"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures, accepted to IEEE VIS 2025 Conference", "url": "http://arxiv.org/abs/2507.12377v1", "summary": "We conduct a deconstructive reading of a qualitative interview study with 17\nvisual data journalists from newsrooms across the globe. We borrow a\ndeconstruction approach from literary critique to explore the instability of\nmeaning in language and reveal implicit beliefs in words and ideas. Through our\nanalysis we surface two sets of opposing implicit beliefs in visual data\njournalism: objectivity/subjectivity and humanism/mechanism. We contextualize\nthese beliefs through a genealogical analysis, which brings deconstruction\ntheory into practice by providing a historic backdrop for these opposing\nperspectives. Our analysis shows that these beliefs held within visual data\njournalism are not self-enclosed but rather a product of external societal\nforces and paradigm shifts over time. Through this work, we demonstrate how\nthinking with critical theories such as deconstruction and genealogy can\nreframe \"success\" in visual data storytelling and diversify visualization\nresearch outcomes. These efforts push the ways in which we as researchers\nproduce domain knowledge to examine the sociotechnical issues of today's values\ntowards datafication and data visualization.", "comment": "11 pages, 5 figures, accepted to IEEE VIS 2025 Conference", "pdf_url": "http://arxiv.org/pdf/2507.12377v1", "cate": "cs.HC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.00513", "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset", "authors": ["Asma Yamani", "Malak Baslyman", "Moataz Ahmed"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.00513v3", "summary": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.00513v3", "cate": "cs.SE", "date": "2025-04-01", "updated": "2025-07-16"}
{"id": "2507.11730", "title": "Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis", "authors": ["Maciej Szankin", "Vidhyananth Venkatasamy", "Lihang Ying"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11730v1", "summary": "Outdoor advertisements remain a critical medium for modern marketing, yet\naccurately verifying billboard text visibility under real-world conditions is\nstill challenging. Traditional Optical Character Recognition (OCR) pipelines\nexcel at cropped text recognition but often struggle with complex outdoor\nscenes, varying fonts, and weather-induced visual noise. Recently, multimodal\nVision-Language Models (VLMs) have emerged as promising alternatives, offering\nend-to-end scene understanding with no explicit detection step. This work\nsystematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,\nInternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline\n(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with\nsynthetic weather distortions to simulate realistic degradation. Our results\nreveal that while selected VLMs excel at holistic scene reasoning, lightweight\nCNN pipelines still achieve competitive accuracy for cropped text at a fraction\nof the computational cost-an important consideration for edge deployment. To\nfoster future research, we release our weather-augmented benchmark and\nevaluation code publicly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11730v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12028", "title": "MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments", "authors": ["Soheil Mahdizadeh", "Elyas Oustad", "Mohsen Ansari"], "categories": ["cs.AR", "cs.DC", "cs.NI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12028v1", "summary": "Task offloading in three-layer fog computing environments presents a critical\nchallenge due to user equipment (UE) mobility, which frequently triggers costly\nservice migrations and degrades overall system performance. This paper\naddresses this problem by proposing MOFCO, a novel Mobility- and\nMigration-aware Task Offloading algorithm for Fog Computing environments. The\nproposed method formulates task offloading and resource allocation as a\nMixed-Integer Nonlinear Programming (MINLP) problem and employs a\nheuristic-aided evolutionary game theory approach to solve it efficiently. To\nevaluate MOFCO, we simulate mobile users using SUMO, providing realistic\nmobility patterns. Experimental results show that MOFCO reduces system cost,\ndefined as a combination of latency and energy consumption, by an average of\n19% and up to 43% in certain scenarios compared to state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12028v1", "cate": "cs.AR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12240", "title": "Characterization and constructions of binary self-orthogonal singly-even linear codes", "authors": ["Kangquan Li", "Hao Chen", "Wengang Jin", "Longjiang Qu"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12240v1", "summary": "Recent research has focused extensively on constructing binary\nself-orthogonal (SO) linear codes due to their applications in quantum\ninformation theory, lattice design, and related areas. Despite significant\nactivity, the fundamental characterization remains unchanged: binary SO codes\nare necessarily even (all codeword weights even), while doubly-even codes\n(weights divisible by $4$) are automatically SO.\n  This paper advances the theory by addressing the understudied case of\nsingly-even (even but not doubly-even) SO codes. We first provide a complete\ncharacterization of binary SO linear codes, and a necessary and sufficient\ncondition for binary SO singly-even linear codes is given. Moreover, we give a\ngeneral approach to generating many binary SO linear codes from two known SO\nlinear codes, yielding three infinite classes of binary SO singly-even linear\ncodes with few weights. Note that these new codes are also minimal and violate\nthe Aschikhmin-Barg condition. Their weight distributions are determined.\nFurthermore, we give a necessary and sufficient condition for a Boolean\nfunction $f$ such that the linear code proposed from $f$ via a well-known\ngeneric construction is SO singly-even, and a general approach to constructing\nBoolean functions satisfying this condition is provided, yielding several\ninfinite classes of binary SO singly-even minimal linear codes with few\nweights. Finally, we would like to emphasize that using the methods in this\npaper, we can construct more binary linear codes that are SO, singly-even,\nminimal, violating the AB condition, and with few weights at the same time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12240v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11709", "title": "Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage", "authors": ["Junius Pun", "Xilai Dai", "Grace Zgheib", "Mahesh A. Iyer", "Andrew Boutros", "Vaughn Betz", "Mohamed S. Abdelfattah"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      accepted at FPL 2025", "url": "http://arxiv.org/abs/2507.11709v1", "summary": "Flexibility and customization are key strengths of Field-Programmable Gate\nArrays (FPGAs) when compared to other computing devices. For instance, FPGAs\ncan efficiently implement arbitrary-precision arithmetic operations, and can\nperform aggressive synthesis optimizations to eliminate ineffectual operations.\nMotivated by sparsity and mixed-precision in deep neural networks (DNNs), we\ninvestigate how to optimize the current logic block architecture to increase\nits arithmetic density. We find that modern FPGA logic block architectures\nprevent the independent use of adder chains, and instead only allow adder chain\ninputs to be fed by look-up table (LUT) outputs. This only allows one of the\ntwo primitives -- either adders or LUTs -- to be used independently in one\nlogic element and prevents their concurrent use, hampering area optimizations.\nIn this work, we propose the Double Duty logic block architecture to enable the\nconcurrent use of the adders and LUTs within a logic element. Without adding\nexpensive logic cluster inputs, we use 4 of the existing inputs to bypass the\nLUTs and connect directly to the adder chain inputs. We accurately model our\nchanges at both the circuit and CAD levels using open-source FPGA development\ntools. Our experimental evaluation on a Stratix-10-like architecture\ndemonstrates area reductions of 21.6% on adder-intensive circuits from the\nKratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR\nbenchmarks respectively. These area improvements come without an impact to\ncritical path delay, demonstrating that higher density is feasible on modern\nFPGA architectures by adding more flexibility in how the adder chain is used.\nAveraged across all circuits from our three evaluated benchmark set, our Double\nDuty FPGA architecture improves area-delay product by 9.7%.", "comment": "accepted at FPL 2025", "pdf_url": "http://arxiv.org/pdf/2507.11709v1", "cate": "cs.AR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11949", "title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "authors": ["Shuyang Xu", "Zhiyang Dou", "Mingyi Shi", "Liang Pan", "Leo Ho", "Jingbo Wang", "Yuan Liu", "Cheng Lin", "Yuexin Ma", "Wenping Wang", "Taku Komura"], "categories": ["cs.GR", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11949v1", "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11949v1", "cate": "cs.GR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11625", "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published as a conference paper at COLM 2025", "url": "http://arxiv.org/abs/2507.11625v1", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "comment": "Published as a conference paper at COLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.11625v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11759", "title": "Torsional-GFN: a conditional conformation generator for small molecules", "authors": ["Alexandra Volokhova", "Léna Néhale Ezzine", "Piotr Gaiński", "Luca Scimeca", "Emmanuel Bengio", "Prudencio Tossou", "Yoshua Bengio", "Alex Hernandez-Garcia"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The two first authors are Alexandra Volokhova and Léna Néhale Ezzine, with equal contribution", "url": "http://arxiv.org/abs/2507.11759v1", "summary": "Generating stable molecular conformations is crucial in several drug\ndiscovery applications, such as estimating the binding affinity of a molecule\nto a target. Recently, generative machine learning methods have emerged as a\npromising, more efficient method than molecular dynamics for sampling of\nconformations from the Boltzmann distribution. In this paper, we introduce\nTorsional-GFN, a conditional GFlowNet specifically designed to sample\nconformations of molecules proportionally to their Boltzmann distribution,\nusing only a reward function as training signal. Conditioned on a molecular\ngraph and its local structure (bond lengths and angles), Torsional-GFN samples\nrotations of its torsion angles. Our results demonstrate that Torsional-GFN is\nable to sample conformations approximately proportional to the Boltzmann\ndistribution for multiple molecules with a single model, and allows for\nzero-shot generalization to unseen bond lengths and angles coming from the MD\nsimulations for such molecules. Our work presents a promising avenue for\nscaling the proposed approach to larger molecular systems, achieving zero-shot\ngeneralization to unseen molecules, and including the generation of the local\nstructure into the GFlowNet model.", "comment": "The two first authors are Alexandra Volokhova and L\\'ena N\\'ehale\n  Ezzine, with equal contribution", "pdf_url": "http://arxiv.org/pdf/2507.11759v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11821", "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to a computer science conference", "url": "http://arxiv.org/abs/2507.11821v1", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "comment": "Submitted to a computer science conference", "pdf_url": "http://arxiv.org/pdf/2507.11821v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.02274", "title": "On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles", "authors": ["Xingyu Zhao", "Robab Aghazadeh-Chakherlou", "Chih-Hong Cheng", "Peter Popov", "Lorenzo Strigini"], "categories": ["cs.SE", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted by ITSC 2025", "url": "http://arxiv.org/abs/2505.02274v2", "summary": "Scenario-based testing has emerged as a common method for autonomous vehicles\n(AVs) safety assessment, offering a more efficient alternative to mile-based\ntesting by focusing on high-risk scenarios. However, fundamental questions\npersist regarding its stopping rules, residual risk estimation, debug\neffectiveness, and the impact of simulation fidelity on safety claims. This\npaper argues that a rigorous statistical foundation is essential to address\nthese challenges and enable rigorous safety assurance. By drawing parallels\nbetween AV testing and established software testing methods, we identify shared\nresearch gaps and reusable solutions. We propose proof-of-concept models to\nquantify the probability of failure per scenario (\\textit{pfs}) and evaluate\ntesting effectiveness under varying conditions. Our analysis reveals that\nneither scenario-based nor mile-based testing universally outperforms the\nother. Furthermore, we give an example of formal reasoning about alignment of\nsynthetic and real-world testing outcomes, a first step towards supporting\nstatistically defensible simulation-based safety claims.", "comment": "Accepted by ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2505.02274v2", "cate": "cs.SE", "date": "2025-05-04", "updated": "2025-07-15"}
{"id": "2507.11761", "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning", "authors": ["Fan Shi", "Bin Li", "Xiangyang Xue"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11761v1", "summary": "Abstract visual reasoning (AVR) enables humans to quickly discover and\ngeneralize abstract rules to new scenarios. Designing intelligent systems with\nhuman-like AVR abilities has been a long-standing topic in the artificial\nintelligence community. Deep AVR solvers have recently achieved remarkable\nsuccess in various AVR tasks. However, they usually use task-specific designs\nor parameters in different tasks. In such a paradigm, solving new tasks often\nmeans retraining the model, and sometimes retuning the model architectures,\nwhich increases the cost of solving AVR problems. In contrast to task-specific\napproaches, this paper proposes a novel Unified Conditional Generative Solver\n(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we\nprove that some well-known AVR tasks can be reformulated as the problem of\nestimating the predictability of target images in problem panels. Then, we\nillustrate that, under the proposed framework, training one conditional\ngenerative model can solve various AVR tasks. The experiments show that with a\nsingle round of multi-task training, UCGS demonstrates abstract reasoning\nability across various AVR tasks. Especially, UCGS exhibits the ability of\nzero-shot reasoning, enabling it to perform abstract reasoning on problems from\nunseen AVR tasks in the testing phase.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11761v1", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2312.06875", "title": "Eywa: Automating Model Based Testing using LLMs", "authors": ["Rajdeep Mondal", "Rathin Singha", "Todd Millstein", "George Varghese", "Ryan Beckett", "Siva Kesava Reddy Kakarla"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.06875v2", "summary": "Model-based testing (MBT), whereby a model of the system under test is\nanalyzed to generate high-coverage test cases, has been used to test protocol\nimplementations. A key barrier to the use of MBT is the need for users to\nunderstand protocol RFCs in detail to create a compliant model. Our new\napproach to MBT uses LLMs to automatically build rich models of intended\nprotocol behavior from knowledge embedded in RFCs, blogs, and other natural\nlanguage sources. Our approach addresses key challenges with using LLMs,\nincluding hallucinations and their inability to monolithically generate complex\nprotocol models. We realize our approach through a novel protocol testing\nframework Eywa,and demonstrate its effectiveness through extensive case studies\nof DNS and BGP and a smaller study of SMTP. Despite minimal user effort,\napplying Eywa enabled the discovery of 32 unique bugs across widely used DNS,\nBGP, and SMTP implementations, 15 of which were previously undiscovered despite\nextensive prior testing with manually crafted models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.06875v2", "cate": "cs.NI", "date": "2023-12-11", "updated": "2025-07-16"}
{"id": "2507.12329", "title": "Neural Polar Decoders for Deletion Channels", "authors": ["Ziv Aharoni", "Henry D. Pfister"], "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12329v1", "summary": "This paper introduces a neural polar decoder (NPD) for deletion channels with\na constant deletion rate. Existing polar decoders for deletion channels exhibit\nhigh computational complexity of $O(N^4)$, where $N$ is the block length. This\nlimits the application of polar codes for deletion channels to\nshort-to-moderate block lengths. In this work, we demonstrate that employing\nNPDs for deletion channels can reduce the computational complexity. First, we\nextend the architecture of the NPD to support deletion channels. Specifically,\nthe NPD architecture consists of four neural networks (NNs), each replicating\nfundamental successive cancellation (SC) decoder operations. To support\ndeletion channels, we change the architecture of only one. The computational\ncomplexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a\ncomputational budget determined by the user and is independent of the channel.\nWe evaluate the new extended NPD for deletion channels with deletion rates\n$\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by\nthe trellis decoder by Tal et al. We further show that due to the reduced\ncomplexity of the NPD, we are able to incorporate list decoding and further\nimprove performance. We believe that the extended NPD presented here could have\napplications in future technologies like DNA storage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12329v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12418", "title": "High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic", "authors": ["George Alexakis", "Dimitrios Schoinianakis", "Giorgos Dimitrakopoulos"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      28th Euromicro Conference Series on Digital System Design (DSD 2025)", "url": "http://arxiv.org/abs/2507.12418v1", "summary": "The Number Theoretic Transform (NTT) is a fundamental operation in\nprivacy-preserving technologies, particularly within fully homomorphic\nencryption (FHE). The efficiency of NTT computation directly impacts the\noverall performance of FHE, making hardware acceleration a critical technology\nthat will enable realistic FHE applications. Custom accelerators, in FPGAs or\nASICs, offer significant performance advantages due to their ability to exploit\nmassive parallelism and specialized optimizations. However, the operation of\nNTT over large moduli requires large word-length modulo arithmetic that limits\nachievable clock frequencies in hardware and increases hardware area costs. To\novercome such deficits, digit-serial arithmetic has been explored for modular\nmultiplication and addition independently. The goal of this work is to leverage\ndigit-serial modulo arithmetic combined with appropriate redundant data\nrepresentation to design modular pipelined NTT accelerators that operate\nuniformly on arbitrary small digits, without the need for intermediate\n(de)serialization. The proposed architecture enables high clock frequencies\nthrough regular pipelining while maintaining parallelism. Experimental results\ndemonstrate that the proposed approach outperforms state-of-the-art\nimplementations and reduces hardware complexity under equal performance and\ninput-output bandwidth constraints.", "comment": "28th Euromicro Conference Series on Digital System Design (DSD 2025)", "pdf_url": "http://arxiv.org/pdf/2507.12418v1", "cate": "cs.AR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11975", "title": "Online Training and Pruning of Deep Reinforcement Learning Networks", "authors": ["Valentin Frank Ingmar Guenter", "Athanasios Sideris"], "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 pages, 5 figures, 4 tables", "url": "http://arxiv.org/abs/2507.11975v1", "summary": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.", "comment": "25 pages, 5 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.11975v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11634", "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Proceedings of the First National Conference on Artificial Intelligence and Emerging Research: Convergence of Humans and Intelligent Systems", "url": "http://arxiv.org/abs/2507.11634v1", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "pdf_url": "http://arxiv.org/pdf/2507.11634v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11771", "title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms", "authors": ["Sheikh Abdur Raheem Ali", "Justin Xu", "Ivory Yang", "Jasmine Xinze Li", "Ayse Arslan", "Clark Benham"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11771v1", "summary": "As large language models (LLMs) evolve in complexity and capability, the\nefficacy of less widely deployed alignment techniques are uncertain. Building\non previous work on activation steering and contrastive activation addition\n(CAA), this paper explores the effectiveness of CAA with model scale using the\nfamily of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable\n'directions' in the model's residual stream vector space using contrastive\npairs (for example, hate to love) and adding this direction to the residual\nstream during the forward pass. It directly manipulates the residual stream and\naims to extract features from language models to better control their outputs.\nUsing answer matching questions centered around the refusal behavior, we found\nthat 1) CAA is most effective when applied at early-mid layers. 2) The\neffectiveness of CAA diminishes with model size. 3) Negative steering has more\npronounced effects than positive steering across all model sizes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11771v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11857", "title": "Measuring and predicting visual fidelity", "authors": ["Benjamin Watson", "Alinda Friedman", "Aaron McGaffey"], "categories": ["cs.GR", "cs.HC"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11857v1", "summary": "This paper is a study of techniques for measuring and predicting visual\nfidelity. As visual stimuli we use polygonal models, and vary their fidelity\nwith two different model simplification algorithms. We also group the stimuli\ninto two object types: animals and man made artifacts. We examine three\ndifferent experimental techniques for measuring these fidelity changes: naming\ntimes, ratings, and preferences. All the measures were sensitive to the type of\nsimplification and level of simplification. However, the measures differed from\none another in their response to object type. We also examine several automatic\ntechniques for predicting these experimental measures, including techniques\nbased on images and on the models themselves. Automatic measures of fidelity\nwere successful at predicting experimental ratings, less successful at\npredicting preferences, and largely failures at predicting naming times. We\nconclude with suggestions for use and improvement of the experimental and\nautomatic measures of visual fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11857v1", "cate": "cs.GR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.23234", "title": "From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers", "authors": ["Peerachai Banyongrakkul", "Mansooreh Zahedi", "Patanamon Thongtanunam", "Christoph Treude", "Haoyu Gao"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Recently accepted at ICSME 2025", "url": "http://arxiv.org/abs/2506.23234v2", "summary": "Pre-trained models (PTMs) have gained widespread popularity and achieved\nremarkable success across various fields, driven by their groundbreaking\nperformance and easy accessibility through hosting providers. However, the\nchallenges faced by downstream developers in reusing PTMs in software systems\nare less explored. To bridge this knowledge gap, we qualitatively created and\nanalyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub\nprojects. We systematically developed a comprehensive taxonomy of PTM-related\nchallenges that developers face in downstream projects. Our study identifies\nseven key categories of challenges that downstream developers face in reusing\nPTMs, such as model usage, model performance, and output quality. We also\ncompared our findings with existing taxonomies. Additionally, we conducted a\nresolution time analysis and, based on statistical tests, found that\nPTM-related issues take significantly longer to be resolved than issues\nunrelated to PTMs, with significant variation across challenge categories. We\ndiscuss the implications of our findings for practitioners and possibilities\nfor future research.", "comment": "Recently accepted at ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2506.23234v2", "cate": "cs.SE", "date": "2025-06-29", "updated": "2025-07-16"}
{"id": "2507.11834", "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning", "authors": ["Peiwen Xia", "Tangfei Liao", "Wei Zhu", "Danhuai Zhao", "Jianjun Ke", "Kaihao Zhang", "Tong Lu", "Tao Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ECAI 2025", "url": "http://arxiv.org/abs/2507.11834v1", "summary": "Establishing reliable correspondences between image pairs is a fundamental\ntask in computer vision, underpinning applications such as 3D reconstruction\nand visual localization. Although recent methods have made progress in pruning\noutliers from dense correspondence sets, they often hypothesize consistent\nvisual domains and overlook the challenges posed by diverse scene structures.\nIn this paper, we propose CorrMoE, a novel correspondence pruning framework\nthat enhances robustness under cross-domain and cross-scene variations. To\naddress domain shift, we introduce a De-stylization Dual Branch, performing\nstyle mixing on both implicit and explicit graph features to mitigate the\nadverse influence of domain-specific representations. For scene diversity, we\ndesign a Bi-Fusion Mixture of Experts module that adaptively integrates\nmulti-perspective features through linear-complexity attention and dynamic\nexpert routing. Extensive experiments on benchmark datasets demonstrate that\nCorrMoE achieves superior accuracy and generalization compared to\nstate-of-the-art methods. The code and pre-trained models are available at\nhttps://github.com/peiwenxia/CorrMoE.", "comment": "Accepted by ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.11834v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2403.05192", "title": "An End-to-End Pipeline Perspective on Video Streaming in Best-Effort Networks: A Survey and Tutorial", "authors": ["Leonardo Peroni", "Sergey Gorinsky"], "categories": ["cs.NI", "cs.MM"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.05192v4", "summary": "Remaining a dominant force in Internet traffic, video streaming captivates\nend users, service providers, and researchers. This paper takes a pragmatic\napproach to reviewing recent advances in the field by focusing on the prevalent\nstreaming paradigm that involves delivering long-form two-dimensional videos\nover the best-effort Internet with client-side adaptive bitrate (ABR)\nalgorithms and assistance from content delivery networks (CDNs). To enhance\naccessibility, we supplement the survey with tutorial material. Unlike existing\nsurveys that offer fragmented views, our work provides a holistic perspective\non the entire end-to-end streaming pipeline, from video capture by a\ncamera-equipped device to playback by the end user. Our novel perspective\ncovers the ingestion, processing, and distribution stages of the pipeline and\naddresses key challenges such as video compression, upload, transcoding, ABR\nalgorithms, CDN support, and quality of experience. We review over 200 papers\nand classify streaming designs by their problem-solving methodology, whether\nbased on intuition (simple heuristics), theory (formal optimization), or\nmachine learning (generalizable data patterns). The survey further refines\nthese methodology-based categories and characterizes each design by additional\ntraits such as compatible codecs and use of super resolution. We connect the\nreviewed research to real-world applications by discussing the practices of\ncommercial streaming platforms. Finally, the survey highlights prominent\ncurrent trends and outlines future directions in video streaming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.05192v4", "cate": "cs.NI", "date": "2024-03-08", "updated": "2025-07-15"}
{"id": "2507.12368", "title": "Efficient Remote Monitoring through Noisy Random Access with Retransmissions", "authors": ["Sergey Foss", "Dmitriy Kim", "Andrey Turlikov"], "categories": ["cs.IT", "math.IT", "math.PR"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12368v1", "summary": "We consider a rare event monitoring system consisting of a set of devices and\na base station, where devices transmit information about rare events to the\nbase station using a random multiple access scheme. We introduce a model in\nwhich the presence of noise in the multiple access channel can cause message\nloss even in the absence of transmission collisions. The occurrence of events\nis modeled by a family of independent two-state Markov chains (with states 0\nand 1). We analyze how repeated transmissions affect system performance. Two\nefficiency criteria are proposed and studied: the maximum probability that a\nmessage about an event from a fixed device is successfully delivered to the\nbase station and the maximum frequency at which the base station successfully\nreceives updates about the entire system. For each criterion, we determine the\noptimal number of retransmissions as a function of the system parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12368v1", "cate": "cs.IT", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12442", "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length", "authors": ["Saptarshi Mitra", "Rachid Karami", "Haocheng Xu", "Sitao Huang", "Hyoukjun Kwon"], "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      12 pages, 7 figures", "url": "http://arxiv.org/abs/2507.12442v1", "summary": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.", "comment": "12 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.12442v1", "cate": "cs.AR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12027", "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation", "authors": ["Beining Xu", "Siting Zhu", "Hesheng Wang"], "categories": ["cs.CV", "cs.RO", "I.4.8; I.2.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, IROS 2025", "url": "http://arxiv.org/abs/2507.12027v1", "summary": "We propose SGLoc, a novel localization system that directly regresses camera\nposes from 3D Gaussian Splatting (3DGS) representation by leveraging semantic\ninformation. Our method utilizes the semantic relationship between 2D image and\n3D scene representation to estimate the 6DoF pose without prior pose\ninformation. In this system, we introduce a multi-level pose regression\nstrategy that progressively estimates and refines the pose of query image from\nthe global 3DGS map, without requiring initial pose priors. Moreover, we\nintroduce a semantic-based global retrieval algorithm that establishes\ncorrespondences between 2D (image) and 3D (3DGS map). By matching the extracted\nscene semantic descriptors of 2D query image and 3DGS semantic representation,\nwe align the image with the local region of the global 3DGS map, thereby\nobtaining a coarse pose estimation. Subsequently, we refine the coarse pose by\niteratively optimizing the difference between the query image and the rendered\nimage from 3DGS. Our SGLoc demonstrates superior performance over baselines on\n12scenes and 7scenes datasets, showing excellent capabilities in global\nlocalization without initial pose prior. Code will be available at\nhttps://github.com/IRMVLab/SGLoc.", "comment": "8 pages, 2 figures, IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.12027v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11636", "title": "JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs", "authors": ["Junyi Fan", "Donald Williamson"], "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to WASPAA 2025", "url": "http://arxiv.org/abs/2507.11636v1", "summary": "Speech quality assessment (SQA) is often used to learn a mapping from a\nhigh-dimensional input space to a scalar that represents the mean opinion score\n(MOS) of the perceptual speech quality. Learning such a mapping is challenging\nfor many reasons, but largely because MOS exhibits high levels of inherent\nvariance due to perceptual and experimental-design differences. Many solutions\nhave been proposed, but many approaches do not properly incorporate perceptual\nfactors into their learning algorithms (beyond the MOS label), which could lead\nto unsatisfactory results. To this end, we propose JSQA, a two-stage framework\nthat pretrains an audio encoder using perceptually-guided contrastive learning\non just noticeable difference (JND) pairs, followed by fine-tuning for MOS\nprediction. We first generate pairs of audio data within JND levels, which are\nthen used to pretrain an encoder to leverage perceptual quality similarity\ninformation and map it into an embedding space. The JND pairs come from clean\nLibriSpeech utterances that are mixed with background noise from CHiME-3, at\ndifferent signal-to-noise ratios (SNRs). The encoder is later fine-tuned with\naudio samples from the NISQA dataset for MOS prediction. Experimental results\nsuggest that perceptually-inspired contrastive pretraining significantly\nimproves the model performance evaluated by various metrics when compared\nagainst the same network trained from scratch without pretraining. These\nfindings suggest that incorporating perceptual factors into pretraining greatly\ncontributes to the improvement in performance for SQA.", "comment": "Accepted to WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.11636v1", "cate": "eess.AS", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11776", "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network", "authors": ["Merel Kampere", "Ali Mohammed Mansoor Alsahag"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11776v1", "summary": "The Dutch railway network is one of the busiest in the world, with delays\nbeing a prominent concern for the principal passenger railway operator NS. This\nresearch addresses a gap in delay prediction studies within the Dutch railway\nnetwork by employing an XGBoost Classifier with a focus on topological\nfeatures. Current research predominantly emphasizes short-term predictions and\nneglects the broader network-wide patterns essential for mitigating ripple\neffects. This research implements and improves an existing methodology,\noriginally designed to forecast the evolution of the fast-changing US air\nnetwork, to predict delays in the Dutch Railways. By integrating Node\nCentrality Measures and comparing multiple classifiers like RandomForest,\nDecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is\nto predict delayed trajectories. However, the results reveal limited\nperformance, especially in non-simultaneous testing scenarios, suggesting the\nnecessity for more context-specific adaptations. Regardless, this research\ncontributes to the understanding of transportation network evaluation and\nproposes future directions for developing more robust predictive models for\ndelays.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11776v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11892", "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11892v1", "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11892v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2410.03103", "title": "Planning-Aware Code Infilling via Horizon-Length Prediction", "authors": ["Yifeng Ding", "Hantian Ding", "Shiqi Wang", "Qing Sun", "Varun Kumar", "Zijian Wang"], "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.03103v3", "summary": "Fill-in-the-Middle (FIM), or infilling, has become integral to code language\nmodels, enabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm which performs next-token prediction\n(NTP) over reordered sequence often leads to models struggling to generate\ncontent that aligns well with the surrounding context. We hypothesize that NTP\nalone is insufficient for models to learn effective planning conditioned on the\ndistant right context, a critical factor for successful code infilling. To\novercome this, we propose Horizon-Length Prediction (HLP), a novel training\nobjective that teaches models to predict the number of remaining middle tokens\nat each step. HLP advances FIM with lookahead planning, enabling models to\ninherently learn infilling boundaries for arbitrary left and right contexts\nwithout relying on dataset-specific post-processing. Our evaluation across\ndifferent model families and sizes shows that HLP significantly improves FIM\nperformance by up to 24% relatively on diverse benchmarks, across file-level\nand repository-level. Furthermore, the enhanced planning capability gained\nthrough HLP boosts model performance on code reasoning. Importantly, HLP incurs\nnegligible training overhead and no additional inference cost, ensuring its\npracticality for real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.03103v3", "cate": "cs.LG", "date": "2024-10-04", "updated": "2025-07-16"}
{"id": "2507.11845", "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification", "authors": ["Kexuan Shi", "Zhuang Qi", "Jingjing Zhu", "Lei Meng", "Yaochen Zhang", "Haibei Huang", "Xiangxu Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ChinaMM and recommended to Displays", "url": "http://arxiv.org/abs/2507.11845v1", "summary": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.", "comment": "Accepted in ChinaMM and recommended to Displays", "pdf_url": "http://arxiv.org/pdf/2507.11845v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2501.01038", "title": "Energy-Efficient and Intelligent ISAC in V2X Networks with Spiking Neural Networks-Driven DRL", "authors": ["Chen Shang", "Jiadong Yu", "Dinh Thai Hoang"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      14 pages, 12 figures", "url": "http://arxiv.org/abs/2501.01038v2", "summary": "Integrated sensing and communication (ISAC) is emerging as a key enabler for\nvehicle-to-everything (V2X) systems. However, designing efficient beamforming\nschemes for ISAC signals to achieve accurate sensing and enhance communication\nperformance in the dynamic and uncertain environments of V2X networks presents\nsignificant challenges. While artificial intelligence technologies offer\npromising solutions, the energy-intensive nature of neural networks imposes\nsubstantial burdens on communication infrastructures. To address these\nchallenges, this work proposes an energy-efficient and intelligent ISAC system\nfor V2X networks. Specifically, we first leverage a Markov Decision Process\nframework to model the dynamic and uncertain nature of V2X networks. This\nframework allows the roadside unit to develop beamforming schemes relying\nsolely on its current sensing information, eliminating the need for numerous\npilot signals and extensive CSI acquisition. We then introduce an advanced deep\nreinforcement learning (DRL) algorithm, enabling the joint optimization of\nbeamforming and power allocation to guarantee both communication rate and\nsensing accuracy in dynamic and uncertain V2X scenario. To alleviate the energy\ndemands of neural networks, we integrate spiking neural networks (SNNs) into\nthe DRL algorithm. The event-driven, sparse spike-based processing of SNNs\nsignificantly improves energy efficiency while maintaining strong performance.\nExtensive simulation results validate the effectiveness of the proposed scheme\nwith lower energy consumption, superior communication performance, and improved\nsensing accuracy.", "comment": "14 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2501.01038v2", "cate": "cs.NI", "date": "2025-01-02", "updated": "2025-07-16"}
{"id": "2507.11894", "title": "Context-Aware Search and Retrieval Over Erasure Channels", "authors": ["Sara Ghasvarianjahromi", "Yauhen Yakimenka", "Jörg Kliewer"], "categories": ["cs.IR", "cs.IT", "math.IT"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11894v1", "summary": "This paper introduces and analyzes a search and retrieval model that adopts\nkey semantic communication principles from retrieval-augmented generation. We\nspecifically present an information-theoretic analysis of a remote document\nretrieval system operating over a symbol erasure channel. The proposed model\nencodes the feature vector of a query, derived from term-frequency weights of a\nlanguage corpus by using a repetition code with an adaptive rate dependent on\nthe contextual importance of the terms. At the decoder, we select between two\ndocuments based on the contextual closeness of the recovered query. By\nleveraging a jointly Gaussian approximation for both the true and reconstructed\nsimilarity scores, we derive an explicit expression for the retrieval error\nprobability, i.e., the probability under which the less similar document is\nselected. Numerical simulations on synthetic and real-world data (Google NQ)\nconfirm the validity of the analysis. They further demonstrate that assigning\ngreater redundancy to critical features effectively reduces the error rate,\nhighlighting the effectiveness of semantic-aware feature encoding in\nerror-prone communication settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11894v1", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12308", "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "categories": ["cs.CL", "cs.AI", "cs.AR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD. 2024 (MLCAD'24)", "url": "http://arxiv.org/abs/2507.12308v1", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "pdf_url": "http://arxiv.org/pdf/2507.12308v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11563", "title": "Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers", "authors": ["Giulio Attenni", "Novella Bartolini"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      LOCO 2024, December 3, 2024, Glasgow/Online", "url": "http://arxiv.org/abs/2507.11563v1", "summary": "This paper presents a theoretical discussion for environmentally-conscious\njob deployment and migration in cloud environments, aiming to minimize the\nenvironmental impact of resource provisioning while incorporating\nsustainability requirements. As the demand for sustainable cloud services\ngrows, it is crucial for cloud customers to select data center operators based\non sustainability metrics and to accurately report the ecological footprint of\ntheir services. To this end, we analyze sustainability reports and define\ncomprehensive environmental impact profiles for data centers, incorporating key\nsustainability indicators. We formalize the problem as an optimization model,\nbalancing multiple environmental factors while respecting user preferences. A\nsimulative case study demonstrates the {potential} of our approach compared to\nbaseline strategies that optimize for single sustainability factors.", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "pdf_url": "http://arxiv.org/pdf/2507.11563v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.12083", "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Xuesong Chen", "Xu Liu", "Shaojie Shen"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.12083v1", "summary": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12083v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11655", "title": "Counting Answer Sets of Disjunctive Answer Set Programs", "authors": ["Mohimenul Kabir", "Supratik Chakraborty", "Kuldeep S Meel"], "categories": ["cs.LO", "cs.AI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      Under consideration in Theory and Practice of Logic Programming (TPLP)", "url": "http://arxiv.org/abs/2507.11655v1", "summary": "Answer Set Programming (ASP) provides a powerful declarative paradigm for\nknowledge representation and reasoning. Recently, counting answer sets has\nemerged as an important computational problem with applications in\nprobabilistic reasoning, network reliability analysis, and other domains. This\nhas motivated significant research into designing efficient ASP counters. While\nsubstantial progress has been made for normal logic programs, the development\nof practical counters for disjunctive logic programs remains challenging.\n  We present SharpASP-SR, a novel framework for counting answer sets of\ndisjunctive logic programs based on subtractive reduction to projected\npropositional model counting. Our approach introduces an alternative\ncharacterization of answer sets that enables efficient reduction while ensuring\nthat intermediate representations remain of polynomial size. This allows\nSharpASP-SR to leverage recent advances in projected model counting technology.\nThrough extensive experimental evaluation on diverse benchmarks, we demonstrate\nthat SharpASP-SR significantly outperforms existing counters on instances with\nlarge answer set counts. Building on these results, we develop a hybrid\ncounting approach that combines enumeration techniques with SharpASP-SR to\nachieve state-of-the-art performance across the full spectrum of disjunctive\nprograms.", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "pdf_url": "http://arxiv.org/pdf/2507.11655v1", "cate": "cs.LO", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11789", "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation", "authors": ["Alessandro Palma", "Sergei Rybakov", "Leon Hetzel", "Stephan Günnemann", "Fabian J. Theis"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages, 14 figures", "url": "http://arxiv.org/abs/2507.11789v1", "summary": "Latent space interpolations are a powerful tool for navigating deep\ngenerative models in applied settings. An example is single-cell RNA\nsequencing, where existing methods model cellular state transitions as latent\nspace interpolations with variational autoencoders, often assuming linear\nshifts and Euclidean geometry. However, unless explicitly enforced, linear\ninterpolations in the latent space may not correspond to geodesic paths on the\ndata manifold, limiting methods that assume Euclidean geometry in the data\nrepresentations. We introduce FlatVI, a novel training framework that\nregularises the latent manifold of discrete-likelihood variational autoencoders\ntowards Euclidean geometry, specifically tailored for modelling single-cell\ncount data. By encouraging straight lines in the latent space to approximate\ngeodesic interpolations on the decoded single-cell manifold, FlatVI enhances\ncompatibility with downstream approaches that assume Euclidean latent geometry.\nExperiments on synthetic data support the theoretical soundness of our\napproach, while applications to time-resolved single-cell RNA sequencing data\ndemonstrate improved trajectory reconstruction and manifold interpolation.", "comment": "31 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.11789v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12009", "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli", "authors": ["Florian David", "Michael Chan", "Elenor Morgenroth", "Patrik Vuilleumier", "Dimitri Van De Ville"], "categories": ["cs.CV", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2025", "url": "http://arxiv.org/abs/2507.12009v1", "summary": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.", "comment": "Accepted in International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.12009v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11893", "title": "Spatial Frequency Modulation for Semantic Segmentation", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Dezhi Zheng", "Jifeng Dai"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accept by TPAMI 2025", "url": "http://arxiv.org/abs/2507.11893v1", "summary": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at\n\\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.", "comment": "Accept by TPAMI 2025", "pdf_url": "http://arxiv.org/pdf/2507.11893v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.15079", "title": "Generative Artificial Intelligence for Beamforming in Low-Altitude Economy", "authors": ["Geng Sun", "Jia Qi", "Chuang Zhang", "Xuejie Liu", "Jiacheng Wang", "Dusit Niyato", "Yuanwei Liu", "Dong In Kim"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.15079v2", "summary": "The growth of low-altitude economy (LAE) has driven a rising demand for\nefficient and secure communication. However, conventional beamforming\noptimization techniques struggle in the complex LAE environments. In this\ncontext, generative artificial intelligence (GenAI) methods provide a promising\nsolution. In this article, we first introduce the core concepts of LAE and the\nroles of beamforming in advanced communication technologies for LAE. We then\nexamine their interrelation, followed by an analysis of the limitations of\nconventional beamforming methods. Next, we provide an overview of how GenAI\nmethods enhance the process of beamforming, with a focus on its applications in\nLAE. Furthermore, we present a case study using a generative diffusion model\n(GDM)-based algorithm to enhance the performance of aerial collaborative\nbeamforming-enabled remote secure communications in LAE and simulation results\nverified the effectiveness of the proposed algorithms. Finally, promising\nresearch opportunities are identified.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15079v2", "cate": "cs.NI", "date": "2025-04-21", "updated": "2025-07-16"}
{"id": "2507.12301", "title": "Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli", "Zhi Ding"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12301v1", "summary": "Deep learning-based implicit channel state information (CSI) feedback has\nbeen introduced to enhance spectral efficiency in massive MIMO systems.\nExisting methods often show performance degradation in ultra-low-rate scenarios\nand inadaptability across diverse environments. In this paper, we propose\nDual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework\nincorporating two novel plug-in preprocessing modules to achieve ultra-low\nfeedback rates while maintaining high environmental robustness. First, a novel\nbi-directional correlation enhancement module is proposed to strengthen the\ncorrelation between uplink and downlink CSI eigenvector matrices. This module\nprojects highly correlated uplink and downlink channel matrices into their\nrespective eigenspaces, effectively reducing redundancy for ultra-low-rate\nfeedback. Second, an innovative input format alignment module is designed to\nmaintain consistent data distributions at both encoder and decoder sides\nwithout extra transmission overhead, thereby enhancing robustness against\nenvironmental variations. Finally, we develop an efficient transformer-based\nimplicit CSI feedback network to exploit angular-delay domain sparsity and\nbi-directional correlation for ultra-low-rate CSI compression. Simulation\nresults demonstrate successful reduction of the feedback overhead by 85%\ncompared with the state-of-the-art method and robustness against unseen\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12301v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2412.19819", "title": "ChipAlign: Instruction Alignment in Large Language Models for Chip Design via Geodesic Interpolation", "authors": ["Chenhui Deng", "Yunsheng Bai", "Haoxing Ren"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted to DAC 2025", "url": "http://arxiv.org/abs/2412.19819v2", "summary": "Recent advancements in large language models (LLMs) have expanded their\napplication across various domains, including chip design, where domain-adapted\nchip models like ChipNeMo have emerged. However, these models often struggle\nwith instruction alignment, a crucial capability for LLMs that involves\nfollowing explicit human directives. This limitation impedes the practical\napplication of chip LLMs, including serving as assistant chatbots for hardware\ndesign engineers. In this work, we introduce ChipAlign, a novel approach that\nutilizes a training-free model merging strategy, combining the strengths of a\ngeneral instruction-aligned LLM with a chip-specific LLM. By considering the\nunderlying manifold in the weight space, ChipAlign employs geodesic\ninterpolation to effectively fuse the weights of input LLMs, producing a merged\nmodel that inherits strong instruction alignment and chip expertise from the\nrespective instruction and chip LLMs. Our results demonstrate that ChipAlign\nsignificantly enhances instruction-following capabilities of existing chip\nLLMs, achieving up to a 26.6% improvement on the IFEval benchmark, while\nmaintaining comparable expertise in the chip domain. This improvement in\ninstruction alignment also translates to notable gains in instruction-involved\nQA tasks, delivering performance enhancements of 3.9% on the OpenROAD QA\nbenchmark and 8.25% on production-level chip QA benchmarks, surpassing\nstate-of-the-art baselines.", "comment": "Accepted to DAC 2025", "pdf_url": "http://arxiv.org/pdf/2412.19819v2", "cate": "cs.AR", "date": "2024-12-15", "updated": "2025-07-15"}
{"id": "2507.11683", "title": "PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training", "authors": ["Seth Ockerman", "Amal Gueroudji", "Tanwi Mallick", "Yixuan He", "Line Pouchard", "Robert Ross", "Shivaram Venkataraman"], "categories": ["cs.DC", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      To be published in the 2025 International Conference for High Performance Computing, Networking, Storage, and Analysis", "url": "http://arxiv.org/abs/2507.11683v1", "summary": "Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for\nmodeling spatial and temporal data dependencies. However, their applications\nhave been limited primarily to small-scale datasets because of memory\nconstraints. While distributed training offers a solution, current frameworks\nlack support for spatiotemporal models and overlook the properties of\nspatiotemporal data. Informed by a scaling study on a large-scale workload, we\npresent PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch\nGeometric Temporal that integrates distributed data parallel training and two\nnovel strategies: index-batching and distributed-index-batching. Our index\ntechniques exploit spatiotemporal structure to construct snapshots dynamically\nat runtime, significantly reducing memory overhead, while\ndistributed-index-batching extends this approach by enabling scalable\nprocessing across multiple GPUs. Our techniques enable the first-ever training\nof an ST-GNN on the entire PeMS dataset without graph partitioning, reducing\npeak memory usage by up to 89\\% and achieving up to a 13.1x speedup over\nstandard DDP with 128 GPUs.", "comment": "To be published in the 2025 International Conference for High\n  Performance Computing, Networking, Storage, and Analysis", "pdf_url": "http://arxiv.org/pdf/2507.11683v1", "cate": "cs.DC", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12414", "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models", "authors": ["Santosh Vasa", "Aditi Ramadwar", "Jnana Rama Krishna Darabattula", "Md Zafar Anwar", "Stanislaw Antol", "Andrei Vatavu", "Thomas Monninger", "Sihao Ding"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12414v1", "summary": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12414v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11661", "title": "Partitioner Guided Modal Learning Framework", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      acm multimedia 2025", "url": "http://arxiv.org/abs/2507.11661v1", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "comment": "acm multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.11661v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11807", "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels", "authors": ["Ruofan Hu", "Dongyu Zhang", "Huayi Zhang", "Elke Rundensteiner"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      KDD 2025, 12 pages, 7 figures", "url": "http://arxiv.org/abs/2507.11807v1", "summary": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.", "comment": "KDD 2025, 12 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.11807v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12356", "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "categories": ["cs.CL", "cs.HC", "cs.SD"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 5 figures, conference or other essential info", "url": "http://arxiv.org/abs/2507.12356v1", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "comment": "12 pages, 5 figures, conference or other essential info", "pdf_url": "http://arxiv.org/pdf/2507.12356v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11900", "title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos", "authors": ["Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      CompressedVQA-HDR won first place in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE ICME 2025", "url": "http://arxiv.org/abs/2507.11900v1", "summary": "Video compression is a standard procedure applied to all videos to minimize\nstorage and transmission demands while preserving visual quality as much as\npossible. Therefore, evaluating the visual quality of compressed videos is\ncrucial for guiding the practical usage and further development of video\ncompression algorithms. Although numerous compressed video quality assessment\n(VQA) methods have been proposed, they often lack the generalization capability\nneeded to handle the increasing diversity of video types, particularly high\ndynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an\neffective VQA framework designed to address the challenges of HDR video quality\nassessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the\nbackbone networks for the proposed full-reference (FR) and no-reference (NR)\nVQA models, respectively. For the FR model, we compute deep structural and\ntextural similarities between reference and distorted frames using\nintermediate-layer features extracted from the Swin Transformer as its\nquality-aware feature representation. For the NR model, we extract the global\nmean of the final-layer feature maps from SigLip 2 as its quality-aware\nrepresentation. To mitigate the issue of limited HDR training data, we\npre-train the FR model on a large-scale standard dynamic range (SDR) VQA\ndataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ\nan iterative mixed-dataset training strategy across multiple compressed VQA\ndatasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental\nresults show that our models achieve state-of-the-art performance compared to\nexisting FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place\nin the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand\nChallenge at IEEE ICME 2025. The code is available at\nhttps://github.com/sunwei925/CompressedVQA-HDR.", "comment": "CompressedVQA-HDR won first place in the FR track of the\n  Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE\n  ICME 2025", "pdf_url": "http://arxiv.org/pdf/2507.11900v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.08677", "title": "Qualitative Assessment of Low Power Wide Area Network Protocols and their Security Aspect", "authors": ["Wesley dos Reis Bezerra", "Lais Machado Bezerra", "Carlos Becker Westphall"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08677v2", "summary": "There are currently many communication options in the Internet of Things,\neven in particular areas such as constrained and battery-powered devices, such\nas Low Power Wide Area Networks. Understanding the differences and\ncharacteristics of each option is a challenge, even for professionals and\nresearchers in the field. To meet this need, this work analyses the qualitative\ncharacteristics of Low Power Wide Area Network protocols and the challenges and\nopportunities of using constrained devices for sparse networks based on\nlong-life batteries. For this study, a bibliographic survey of the literature\nwas carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and\nSigfox), and a detailing of the first one. As a result, there is a discussion\nabout the chosen network protocol and its use in IoT solutions with sparse\nsensors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08677v2", "cate": "cs.NI", "date": "2025-07-11", "updated": "2025-07-15"}
{"id": "2502.14198", "title": "Antenna Position and Beamforming Optimization for Movable Antenna Enabled ISAC: Optimal Solutions and Efficient Algorithms", "authors": ["Lebin Chen", "Ming-Min Zhao", "Min-Jian Zhao", "Rui Zhang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      16 pages, 8 figures", "url": "http://arxiv.org/abs/2502.14198v2", "summary": "In this paper, we propose an integrated sensing and communication (ISAC)\nsystem enabled by movable antennas (MAs), which can dynamically adjust antenna\npositions to enhance both sensing and communication performance for future\nwireless networks. To characterize the benefits of MA-enabled ISAC systems, we\nfirst derive the Cram\\'er-Rao bound (CRB) for angle estimation error, which is\nthen minimized for optimizing the antenna position vector (APV) and beamforming\ndesign, subject to a pre-defined signal-to-noise ratio (SNR) constraint to\nensure the communication performance. In particular, for the case with receive\nMAs only, we provide a closed-form optimal antenna position solution, and show\nthat employing MAs over conventional fixed-position antennas (FPAs) can achieve\na sensing performance gain upper-bounded by 4.77 dB. On the other hand, for the\ncase with transmit MAs only, we develop a boundary traversal breadth-first\nsearch (BT-BFS) algorithm to obtain the global optimal solution in the\nline-of-sight (LoS) channel scenario, along with a lower-complexity boundary\ntraversal depth-first search (BT-DFS) algorithm to find a local optimal\nsolution efficiently. While in the scenario with non-LoS (NLoS) channels, a\nmajorization-minimization (MM) based Rosen's gradient projection (RGP)\nalgorithm with an efficient initialization method is proposed to obtain\nstationary solutions for the considered problem, which can be extended to the\ngeneral case with both transmit and receive MAs. Extensive numerical results\nare presented to verify the effectiveness of the proposed algorithms, and\ndemonstrate the superiority of the considered MA-enabled ISAC system over\nconventional ISAC systems with FPAs in terms of sensing and communication\nperformance trade-off.", "comment": "16 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2502.14198v2", "cate": "cs.IT", "date": "2025-02-20", "updated": "2025-07-16"}
{"id": "2506.05566", "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted to MLCAD 2025", "url": "http://arxiv.org/abs/2506.05566v2", "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "comment": "Accepted to MLCAD 2025", "pdf_url": "http://arxiv.org/pdf/2506.05566v2", "cate": "cs.AR", "date": "2025-06-05", "updated": "2025-07-15"}
{"id": "2507.11830", "title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI", "authors": ["Samyam Rajbhandari", "Mert Hidayetoglu", "Aurick Qiao", "Ye Wang", "Juncheng Yang", "Jeff Rasley", "Michael Wyatt", "Yuxiong He"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11830v1", "summary": "Inference is now the dominant AI workload, yet existing systems force\ntrade-offs between latency, throughput, and cost. Arctic Inference, an\nopen-source vLLM plugin from Snowflake AI Research, introduces Shift\nParallelism, a dynamic parallelism strategy that adapts to real-world traffic\nwhile integrating speculative decoding, SwiftKV compute reduction, and\noptimized embedding inference. It achieves up to 3.4 times faster request\ncompletion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for\nembeddings, outperforming both latency- and throughput-optimized deployments.\nAlready powering Snowflake Cortex AI, Arctic Inference delivers\nstate-of-the-art, cost-effective inference for enterprise AI and is now\navailable to the community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11830v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2403.09567", "title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models", "authors": ["Laura Fernández-Becerra", "Miguel Ángel González-Santamarta", "Ángel Manuel Guerrero-Higueras", "Francisco Javier Rodríguez-Lera", "Vicente Matellán Olivera"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.09567v4", "summary": "The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.09567v4", "cate": "cs.RO", "date": "2024-03-14", "updated": "2025-07-15"}
{"id": "2507.11692", "title": "Galaxy image simplification using Generative AI", "authors": ["Sai Teja Erukude", "Lior Shamir"], "categories": ["astro-ph.GA", "astro-ph.IM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Astrophysics of Galaxies (astro-ph.GA)", "pdf_link": null, "comments": "Comments:      Astronomy and Computing, accepted", "url": "http://arxiv.org/abs/2507.11692v1", "summary": "Modern digital sky surveys have been acquiring images of billions of\ngalaxies. While these images often provide sufficient details to analyze the\nshape of the galaxies, accurate analysis of such high volumes of images\nrequires effective automation. Current solutions often rely on machine learning\nannotation of the galaxy images based on a set of pre-defined classes. Here we\nintroduce a new approach to galaxy image analysis that is based on generative\nAI. The method simplifies the galaxy images and automatically converts them\ninto a ``skeletonized\" form. The simplified images allow accurate measurements\nof the galaxy shapes and analysis that is not limited to a certain pre-defined\nset of classes. We demonstrate the method by applying it to galaxy images\nacquired by the DESI Legacy Survey. The code and data are publicly available.\nThe method was applied to 125,000 DESI Legacy Survey images, and the catalog of\nthe simplified images is publicly available.", "comment": "Astronomy and Computing, accepted", "pdf_url": "http://arxiv.org/pdf/2507.11692v1", "cate": "astro-ph.GA", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11818", "title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling", "authors": ["Andrei Rekesh", "Miruna Cretu", "Dmytro Shevchuk", "Vignesh Ram Somnath", "Pietro Liò", "Robert A. Batey", "Mike Tyers", "Michał Koziarski", "Cheng-Hao Liu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11818v1", "summary": "Ensuring synthesizability in generative small molecule design remains a major\nchallenge. While recent developments in synthesizable molecule generation have\ndemonstrated promising results, these efforts have been largely confined to 2D\nmolecular graph representations, limiting the ability to perform geometry-based\nconditional generation. In this work, we present SynCoGen (Synthesizable\nCo-Generation), a single framework that combines simultaneous masked graph\ndiffusion and flow matching for synthesizable 3D molecule generation. SynCoGen\nsamples from the joint distribution of molecular building blocks, chemical\nreactions, and atomic coordinates. To train the model, we curated SynSpace, a\ndataset containing over 600K synthesis-aware building block graphs and 3.3M\nconformers. SynCoGen achieves state-of-the-art performance in unconditional\nsmall molecule graph and conformer generation, and the model delivers\ncompetitive performance in zero-shot molecular linker design for protein ligand\ngeneration in drug discovery. Overall, this multimodal formulation represents a\nfoundation for future applications enabled by non-autoregressive molecular\ngeneration, including analog expansion, lead optimization, and direct structure\nconditioning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11818v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12370", "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "url": "http://arxiv.org/abs/2507.12370v1", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "pdf_url": "http://arxiv.org/pdf/2507.12370v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11910", "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the 28th IEEE International Conference on Intelligent Transportation Systems (ITSC 2025)", "url": "http://arxiv.org/abs/2507.11910v1", "summary": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.", "comment": "Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.11910v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2503.09174", "title": "Deterministic and Statistical Analysis of the DoF of Continuous Linear Arrays in the Near Field", "authors": ["Athanasios G. Kanatas", "Harris K. Armeniakos", "Harpreet S. Dhillon", "Marco Di Renzo"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      16 pages, Submitted to IEEE journal for possible publication", "url": "http://arxiv.org/abs/2503.09174v2", "summary": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF) in a wireless line-of-sight channel comprising a small\ncontinuous linear intelligent antenna array in the near field of a large one.\nThe framework allows for any orientations between the arrays and any positions\nin a two-dimensional space assuming that the transmitting array is placed at\nthe origin. Therefore, apart from the length of the two continuous arrays, four\nkey parameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveil key\nsystem-level insights. A comparison of the proposed method with the singular\nvalue decomposition-based method is illustrated to validate the model.", "comment": "16 pages, Submitted to IEEE journal for possible publication", "pdf_url": "http://arxiv.org/pdf/2503.09174v2", "cate": "cs.IT", "date": "2025-03-12", "updated": "2025-07-16"}
{"id": "2507.07044", "title": "Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics", "authors": ["Mehrdad Morsali", "Chengwei Zhou", "Deniz Najafi", "Sreetama Sarkar", "Pietro Mercati", "Navid Khoshavi", "Peter Beerel", "Mahdi Nikdast", "Gourav Datta", "Shaahin Angizi"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07044v2", "summary": "Vision Transformers (ViTs) have emerged as a powerful architecture for\ncomputer vision tasks due to their ability to model long-range dependencies and\nglobal contextual relationships. However, their substantial compute and memory\ndemands hinder efficient deployment in scenarios with strict energy and\nbandwidth limitations. In this work, we propose OptoViT, the first near-sensor,\nregion-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time\nand energy-efficient vision processing. Opto-ViT features a hybrid\nelectronic-photonic architecture, where the optical core handles\ncompute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting\nLasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and\nnormalization are executed electronically. To reduce redundant computation and\npatch processing, we introduce a lightweight Mask Generation Network (MGNet)\nthat identifies regions of interest in the current frame and prunes irrelevant\npatches before ViT encoding. We further co-optimize the ViT backbone using\nquantization-aware training and matrix decomposition tailored for photonic\nconstraints. Experiments across device fabrication, circuit and architecture\nco-design, to classification, detection, and video tasks demonstrate that\nOptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6%\naccuracy loss, while enabling scalable and efficient ViT deployment at the\nedge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07044v2", "cate": "cs.AR", "date": "2025-07-09", "updated": "2025-07-16"}
{"id": "2507.11899", "title": "Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst", "authors": ["Saeid Aghasoleymani Najafabadi"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11899v1", "summary": "Load balancing plays a pivotal role in cloud computing, ensuring that\nresources are optimally allocated to maintain high service quality and\noperational efficiency. As workloads in cloud environments become increasingly\ndynamic and unpredictable, load balancing strategies are evolving from\ntraditional static methods to more adaptive and intelligent approaches. In this\nstudy, the Cloud Analyst simulation tool was used to evaluate the performance\nof different load balancing algorithms under various scenarios, including both\ncentralized and distributed resource setups. The results highlight that while\nthe Round Robin algorithm yields slightly better processing times within a\nsingle data center, Equally Spread and Throttled techniques perform\ncompetitively, especially when network latency is considered. More importantly,\nwhen resources are distributed across multiple data centers, response times are\nsignificantly reduced, emphasizing the value of proximity and efficient load\ndistribution. In these distributed environments, Equally Spread and Throttled\nalgorithms not only maintain quick response times but also contribute to lower\noperational costs. These findings demonstrate the necessity of strategic\nresource placement and proactive infrastructure planning to balance performance\nand cost. Adopting intelligent, dynamic load balancing and resource management\npractices can help organizations meet evolving cloud demands, optimize costs,\nand maintain a competitive advantage. Continuous evaluation and integration of\nemerging technologies are crucial for sustaining effective and scalable cloud\noperations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11899v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2409.15590", "title": "MapEx: Indoor Structure Exploration with Probabilistic Information Gain from Global Map Predictions", "authors": ["Cherie Ho", "Seungchan Kim", "Brady Moon", "Aditya Parandekar", "Narek Harutyunyan", "Chen Wang", "Katia Sycara", "Graeme Best", "Sebastian Scherer"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2409.15590v3", "summary": "Exploration is a critical challenge in robotics, centered on understanding\nunknown environments. In this work, we focus on robots exploring structured\nindoor environments which are often predictable and composed of repeating\npatterns. Most existing approaches, such as conventional frontier approaches,\nhave difficulty leveraging the predictability and explore with simple\nheuristics such as `closest first'. Recent works use deep learning techniques\nto predict unknown regions of the map, using these predictions for information\ngain calculation. However, these approaches are often sensitive to the\npredicted map quality or do not reason over sensor coverage. To overcome these\nissues, our key insight is to jointly reason over what the robot can observe\nand its uncertainty to calculate probabilistic information gain. We introduce\nMapEx, a new exploration framework that uses predicted maps to form\nprobabilistic sensor model for information gain estimation. MapEx generates\nmultiple predicted maps based on observed information, and takes into\nconsideration both the computed variances of predicted maps and estimated\nvisible area to estimate the information gain of a given viewpoint. Experiments\non the real-world KTH dataset showed on average 12.4% improvement than\nrepresentative map-prediction based exploration and 25.4% improvement than\nnearest frontier approach. Website: mapex-explorer.github.io", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2409.15590v3", "cate": "cs.RO", "date": "2024-09-23", "updated": "2025-07-16"}
{"id": "2507.11694", "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Sáez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "Héctor Cerezo-Costas"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This work has been accepted for presentation at the 24nd Portuguese Conference on Artificial Intelligence (EPIA 2025) and will be published in the proceedings by Springer in the Lecture Notes in Computer Science (LNCS) series. Please cite the published version when available", "url": "http://arxiv.org/abs/2507.11694v1", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "pdf_url": "http://arxiv.org/pdf/2507.11694v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11836", "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "authors": ["Jian Gao", "Jianshe Wu", "JingYi Ding"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11836v1", "summary": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11836v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10024", "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles", "authors": ["Shaolun Ruan", "Rui Sheng", "Xiaolin Wen", "Jiachen Wang", "Tianyi Zhang", "Yong Wang", "Tim Dwyer", "Jiannan Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10024v2", "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10024v2", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.11931", "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "authors": ["Jingqian Wu", "Peiqi Duan", "Zongqiang Wang", "Changwei Wang", "Boxin Shi", "Edmund Y. Lam"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11931v1", "summary": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11931v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "1909.10455", "title": "Geometry, Computation, and Optimality in Stochastic Optimization", "authors": ["Chen Cheng", "Daniel Levy", "John C. Duchi"], "categories": ["math.OC", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      47 pages. An earlier version of this paper, entitled \"Necessary and Sufficient Geometries for Gradient Methods,\" appeared at NeurIPS 2019 ( arXiv:1909.10455v2 )", "url": "http://arxiv.org/abs/1909.10455v4", "summary": "We study computational and statistical consequences of problem geometry in\nstochastic and online optimization. By focusing on constraint set and gradient\ngeometry, we characterize the problem families for which stochastic- and\nadaptive-gradient methods are (minimax) optimal and, conversely, when nonlinear\nupdates -- such as those mirror descent employs -- are necessary for optimal\nconvergence. When the constraint set is quadratically convex, diagonally\npre-conditioned stochastic gradient methods are minimax optimal. We provide\nquantitative converses showing that the ``distance'' of the underlying\nconstraints from quadratic convexity determines the sub-optimality of\nsubgradient methods. These results apply, for example, to any $\\ell_p$-ball for\n$p < 2$, and the computation/accuracy tradeoffs they demonstrate exhibit a\nstriking analogy to those in Gaussian sequence models.", "comment": "47 pages. An earlier version of this paper, entitled \"Necessary and\n  Sufficient Geometries for Gradient Methods,\" appeared at NeurIPS 2019\n  (arXiv:1909.10455v2)", "pdf_url": "http://arxiv.org/pdf/1909.10455v4", "cate": "math.OC", "date": "2019-09-23", "updated": "2025-07-16"}
{"id": "2507.11331", "title": "SystolicAttention: Fusing FlashAttention within a Single Systolic Array", "authors": ["Jiawei Lin", "Guokai Chen", "Yuanlong Li", "Thomas Bourgeat"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11331v2", "summary": "Transformer models rely heavily on scaled dot-product attention (SDPA),\ntypically implemented using the FlashAttention algorithm. However, current\nsystolic-array-based accelerators face significant challenges when executing\nFlashAttention. Systolic arrays can only achieve high utilization for\nconsecutive and large matrix multiplications. In contrast, FlashAttention\nrequires frequently interleaved matrix multiplications and softmax operations.\n  The frequent data swaps between the systolic array and external vector units\nresult in low systolic array utilization. This is further exacerbated by the\nfact that softmax involves numerous non-matrix operations, which are not\nwell-suited for systolic arrays. Moreover, the concurrent execution of matrix\nmultiplication on systolic arrays and softmax on vector units leads to register\nfile and SRAM port contention, further degrading performance.\n  To overcome these limitations, we propose FSA, an enhanced systolic array\narchitecture that enables the entire FlashAttention algorithm to run entirely\nwithin a single systolic array, eliminating the need for external vector units.\nAt the core of FSA is SystolicAttention, a novel scheduling algorithm that maps\nFlashAttention operations onto systolic arrays with fine-grained, element-wise\noverlap. This significantly improves array utilization while preserving the\noriginal floating-point operation order to maintain numerical stability.\n  We implement FSA in synthesizable RTL and evaluate its performance against\nstate-of-the-art commercial accelerators. Our results show that FSA achieves\n1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS\nNeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area\noverhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11331v2", "cate": "cs.AR", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.11929", "title": "Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics", "authors": ["Minchen Yu", "Yinghao Ren", "Jiamu Zhao", "Jiaqi Li"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11929v1", "summary": "Serverless computing has attracted a broad range of applications due to its\nease of use and resource elasticity. However, developing serverless\napplications often poses a dilemma -- relying on general-purpose serverless\nplatforms can fall short of delivering satisfactory performance for complex\nworkloads, whereas building application-specific serverless systems undermines\nthe simplicity and generality. In this paper, we propose an extensible design\nprinciple for serverless computing. We argue that a platform should enable\ndevelopers to extend system behaviors for domain-specialized optimizations\nwhile retaining a shared, easy-to-use serverless environment. We take data\nanalytics as a representative serverless use case and realize this design\nprinciple in Proteus. Proteus introduces a novel abstraction of decision\nworkflows, allowing developers to customize control-plane behaviors for\nimproved application performance. Preliminary results show that Proteus's\nprototype effectively optimizes analytical query execution and supports\nfine-grained resource sharing across diverse applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11929v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11544", "title": "The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models", "authors": ["Ann-Kathrin Dombrowski", "Dillon Bowen", "Adam Gleave", "Chris Cundy"], "categories": ["cs.CY", "cs.LG", "68T07"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      9 pages plus appendix", "url": "http://arxiv.org/abs/2507.11544v1", "summary": "Open-weight large language models (LLMs) unlock huge benefits in innovation,\npersonalization, privacy, and democratization. However, their core advantage -\nmodifiability - opens the door to systemic risks: bad actors can trivially\nsubvert current safeguards, turning beneficial models into tools for harm. This\nleads to a 'safety gap': the difference in dangerous capabilities between a\nmodel with intact safeguards and one that has been stripped of those\nsafeguards. We open-source a toolkit to estimate the safety gap for\nstate-of-the-art open-weight models. As a case study, we evaluate biochemical\nand cyber capabilities, refusal rates, and generation quality of models from\ntwo families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to\n405B) using different safeguard removal techniques. Our experiments reveal that\nthe safety gap widens as model scale increases and effective dangerous\ncapabilities grow substantially when safeguards are removed. We hope that the\nSafety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve\nas an evaluation framework for common open-source models and as a motivation\nfor developing and testing tamper-resistant safeguards. We welcome\ncontributions to the toolkit from the community.", "comment": "9 pages plus appendix", "pdf_url": "http://arxiv.org/pdf/2507.11544v1", "cate": "cs.CY", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2410.15607", "title": "Reinforced Imitative Trajectory Planning for Urban Automated Driving", "authors": ["Di Zeng", "Ling Zheng", "Xiantong Yang", "Yinong Li"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      21 pages, 9 figures", "url": "http://arxiv.org/abs/2410.15607v2", "summary": "Reinforcement learning (RL) faces challenges in trajectory planning for urban\nautomated driving due to the poor convergence of RL and the difficulty in\ndesigning reward functions. Consequently, few RL-based trajectory planning\nmethods can achieve performance comparable to that of imitation learning-based\nmethods. The convergence problem is alleviated by combining RL with supervised\nlearning. However, most existing approaches only reason one step ahead and lack\nthe capability to plan for multiple future steps. Besides, although inverse\nreinforcement learning holds promise for solving the reward function design\nissue, existing methods for automated driving impose a linear structure\nassumption on reward functions, making them difficult to apply to urban\nautomated driving. In light of these challenges, this paper proposes a novel\nRL-based trajectory planning method that integrates RL with imitation learning\nto enable multi-step planning. Furthermore, a transformer-based Bayesian reward\nfunction is developed, providing effective reward signals for RL in urban\nscenarios. Moreover, a hybrid-driven trajectory planning framework is proposed\nto enhance safety and interpretability. The proposed methods were validated on\nthe large-scale real-world urban automated driving nuPlan dataset. Evaluated\nusing closed-loop metrics, the results demonstrated that the proposed method\nsignificantly outperformed the baseline employing the identical policy model\nstructure and achieved competitive performance compared to the state-of-the-art\nmethod. The code is available at https://github.com/Zigned/nuplan_zigned.", "comment": "21 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2410.15607v2", "cate": "cs.RO", "date": "2024-10-21", "updated": "2025-07-16"}
{"id": "2507.11742", "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Ludäscher"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint. Accepted to COLM 2025", "url": "http://arxiv.org/abs/2507.11742v1", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "comment": "Preprint. Accepted to COLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.11742v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11839", "title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM", "authors": ["Chengyue Gong", "Xinshi Chen", "Yuxuan Zhang", "Yuxuan Song", "Hao Zhou", "Wenzhi Xiao"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11839v1", "summary": "Lightweight inference is critical for biomolecular structure prediction and\nother downstream tasks, enabling efficient real-world deployment and\ninference-time scaling for large-scale applications. In this work, we address\nthe challenge of balancing model efficiency and prediction accuracy by making\nseveral key modifications, 1) Multi-step AF3 sampler is replaced by a few-step\nODE sampler, significantly reducing computational overhead for the diffusion\nmodule part during inference; 2) In the open-source Protenix framework, a\nsubset of pairformer or diffusion transformer blocks doesn't make contributions\nto the final structure prediction, presenting opportunities for architectural\npruning and lightweight redesign; 3) A model incorporating an ESM module is\ntrained to substitute the conventional MSA module, reducing MSA preprocessing\ntime. Building on these key insights, we present Protenix-Mini, a compact and\noptimized model designed for efficient protein structure prediction. This\nstreamlined version incorporates a more efficient architectural design with a\ntwo-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating\nredundant Transformer components and refining the sampling process,\nProtenix-Mini significantly reduces model complexity with slight accuracy drop.\nEvaluations on benchmark datasets demonstrate that it achieves high-fidelity\npredictions, with only a negligible 1 to 5 percent decrease in performance on\nbenchmark datasets compared to its full-scale counterpart. This makes\nProtenix-Mini an ideal choice for applications where computational resources\nare limited but accurate structure prediction remains crucial.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11839v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.02179", "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality", "authors": ["Yiqin Zhao", "Mallesham Dasari", "Tian Guo"], "categories": ["cs.CV", "cs.GR", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.02179v3", "summary": "High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.02179v3", "cate": "cs.CV", "date": "2024-11-04", "updated": "2025-07-16"}
{"id": "2507.11932", "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "authors": ["Mohammad Shahab Sepehri", "Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11932v1", "summary": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11932v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2408.10743", "title": "Fast Algorithms and Implementations for Computing the Minimum Distance of Quantum Codes", "authors": ["Fernando Hernando", "Gregorio Quintana-Ortí", "Markus Grassl"], "categories": ["quant-ph", "cs.CE", "cs.IT", "cs.MS", "math.IT", "81-04, 81-08, 94B05, 94B60, 94B99", "G.4"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures, 1 table", "url": "http://arxiv.org/abs/2408.10743v2", "summary": "The distance of a stabilizer quantum code is a very important feature since\nit determines the number of errors that can be detected and corrected. We\npresent three new fast algorithms and implementations for computing the\nsymplectic distance of the associated classical code. Our new algorithms are\nbased on the Brouwer-Zimmermann algorithm. Our experimental study shows that\nthese new implementations are much faster than current state-of-the-art\nlicensed implementations on single-core processors, multicore processors, and\nshared-memory multiprocessors. In the most computationally-demanding cases, the\nperformance gain in the computational time can be larger than one order of\nmagnitude. The experimental study also shows a good scalability on\nshared-memory parallel architectures.", "comment": "14 pages, 7 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2408.10743v2", "cate": "quant-ph", "date": "2024-08-20", "updated": "2025-07-16"}
{"id": "2507.11978", "title": "NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning", "authors": ["Jiacheng Huang", "Zimin Li", "Yinghui Li", "Haojie Wang"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11978v1", "summary": "The emergence of deep learning domain-specific languages (DSLs) has\nsubstantially reduced the obstacles in developing high-performance,\ncross-platform compute kernels. However, current DSLs, such as Triton, still\ndemand that developers possess expertise in parallel programming and expose\nthem to many low-level details. This requirement complicates the development\nprocess and adds to the difficulty of maintaining compute kernels.\nConsequently, developing a new programming model that supports serial\nprogramming for deep learning workloads is crucial.\n  This paper introduces NineToothed, a domain-specific language that offers\nserial semantics for machine learning programming. Through the automatic\ntransformation of serial code into parallel code, NineToothed significantly\nstreamlines the development process while causing minimal performance\ndegradation. NineToothed encompasses (1) a language with tensor-oriented\nmetaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the\nexpression of tiled computations without the need to manage low-level details\nand (2) a code generator for generating high-performance parallel code. Our\nevaluation results indicate that NineToothed can greatly simplify compute\nkernel development while maintaining performance comparable to that of Triton.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11978v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11546", "title": "AI Governance InternationaL Evaluation Index (AGILE Index) 2025", "authors": ["Yi Zeng", "Enmeng Lu", "Xiaoyang Guo", "Cunqing Huangfu", "Jiawei Xie", "Yu Chen", "Zhengqi Wang", "Dongqi Liang", "Gongce Cao", "Jin Wang", "Zizhe Ruan", "Xin Guan", "Ammar Younas"], "categories": ["cs.CY", "68T01", "A.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      81 pages, 29 figures, 7 tables", "url": "http://arxiv.org/abs/2507.11546v1", "summary": "The year 2024 witnessed accelerated global AI governance advancements, marked\nby strengthened multilateral frameworks and proliferating national regulatory\ninitiatives. This acceleration underscores an unprecedented need to\nsystematically track governance progress--an imperative that drove the launch\nof the AI Governance InternationaL Evaluation Index (AGILE Index) project since\n2023. The inaugural AGILE Index, released in February 2024 after assessing 14\ncountries, established an operational and comparable baseline framework.\nBuilding on pilot insights, AGILE Index 2025 incorporates systematic\nrefinements to better balance scientific rigor with practical adaptability. The\nupdated methodology expands data diversity while enhancing metric validity and\ncross-national comparability. Reflecting both research advancements and\npractical policy evolution, AGILE Index 2025 evaluates 40 countries across\nincome levels, regions, and technological development stages, with 4 Pillars,\n17 Dimensions and 43 Indicators. In compiling the data, the team integrates\nmulti-source evidence including policy documents, governance practices,\nresearch outputs, and risk exposure to construct a unified comparison\nframework. This approach maps global disparities while enabling countries to\nidentify governance strengths, gaps, and systemic constraints. Through ongoing\nrefinement and iterations, we hope the AGILE Index will fundamentally advance\ntransparency and measurability in global AI governance, delivering data-driven\nassessments that depict national AI governance capacity, assist governments in\nrecognizing their maturation stages and critical governance issues, and\nultimately provide actionable insights for enhancing AI governance systems\nnationally and globally.", "comment": "81 pages, 29 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.11546v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.03132", "title": "SPARK: A Modular Benchmark for Humanoid Robot Safety", "authors": ["Yifan Sun", "Rui Chen", "Kai S. Yun", "Yikuan Fang", "Sebin Jung", "Feihan Li", "Bowei Li", "Weiye Zhao", "Changliu Liu"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Presented at IFAC Symposium on Robotics", "url": "http://arxiv.org/abs/2502.03132v2", "summary": "This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a\ncomprehensive benchmark designed to ensure safety in humanoid autonomy and\nteleoperation. Humanoid robots pose significant safety risks due to their\nphysical capabilities of interacting with complex environments. The physical\nstructures of humanoid robots further add complexity to the design of general\nsafety solutions. To facilitate safe deployment of complex robot systems, SPARK\ncan be used as a toolbox that comes with state-of-the-art safe control\nalgorithms in a modular and composable robot control framework. Users can\neasily configure safety criteria and sensitivity levels to optimize the balance\nbetween safety and performance. To accelerate humanoid safety research and\ndevelopment, SPARK provides simulation benchmarks that compare safety\napproaches in a variety of environments, tasks, and robot models. Furthermore,\nSPARK allows quick deployment of synthesized safe controllers on real robots.\nFor hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion\nCapture System as external sensors, while offering interfaces for seamless\nintegration with alternative hardware setups at the same time. This paper\ndemonstrates SPARK's capability with both simulation experiments and case\nstudies with a Unitree G1 humanoid robot. Leveraging these advantages of SPARK,\nusers and researchers can significantly improve the safety of their humanoid\nsystems as well as accelerate relevant research. The open source code is\navailable at: https://github.com/intelligent-control-lab/spark.", "comment": "Presented at IFAC Symposium on Robotics", "pdf_url": "http://arxiv.org/pdf/2502.03132v2", "cate": "cs.RO", "date": "2025-02-05", "updated": "2025-07-16"}
{"id": "2507.11751", "title": "Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "categories": ["cs.NE", "cs.AI", "68-68W50"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      CSAIDE '25: Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy", "url": "http://arxiv.org/abs/2507.11751v1", "summary": "Identifying similar documents within extensive volumes of data poses a\nsignificant challenge. To tackle this issue, researchers have developed a\nvariety of effective distributed computing techniques. With the advancement of\ncomputing power and the rise of big data, deep neural networks and evolutionary\ncomputing algorithms such as genetic algorithms and differential evolution\nalgorithms have achieved greater success. This survey will explore the most\nrecent advancements in the search for documents based on their semantic text\nsimilarity, focusing on genetic and differential evolutionary computing\nalgorithms.", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "pdf_url": "http://arxiv.org/pdf/2507.11751v1", "cate": "cs.NE", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11847", "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update", "authors": ["Yu-Jie Zhang", "Sheng-An Xu", "Peng Zhao", "Masashi Sugiyama"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11847v1", "summary": "We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11847v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11330", "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Journal of the Association for Information Science and Technology, 2025", "url": "http://arxiv.org/abs/2507.11330v2", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.", "comment": "Journal of the Association for Information Science and Technology,\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.11330v2", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.11947", "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation", "authors": ["Geon Park", "Seon Bin Kim", "Gunho Jung", "Seong-Whan Lee"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 Pages", "url": "http://arxiv.org/abs/2507.11947v1", "summary": "With recent advancements in text-to-image (T2I) models, effectively\ngenerating multiple instances within a single image prompt has become a crucial\nchallenge. Existing methods, while successful in generating positions of\nindividual instances, often struggle to account for relationship discrepancy\nand multiple attributes leakage. To address these limitations, this paper\nproposes the relation-aware disentangled learning (RaDL) framework. RaDL\nenhances instance-specific attributes through learnable parameters and\ngenerates relation-aware image features via Relation Attention, utilizing\naction verbs extracted from the global prompt. Through extensive evaluations on\nbenchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that\nRaDL outperforms existing methods, showing significant improvements in\npositional accuracy, multiple attributes consideration, and the relationships\nbetween instances. Our results present RaDL as the solution for generating\nimages that consider both the relationships and multiple attributes of each\ninstance within the multi-instance image.", "comment": "6 Pages", "pdf_url": "http://arxiv.org/pdf/2507.11947v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2502.06399", "title": "A Linearly Convergent Algorithm for Computing the Petz-Augustin Mean", "authors": ["Chun-Neng Chu", "Wei-Fu Tseng", "Yen-Huan Li"], "categories": ["quant-ph", "cs.IT", "math.IT", "math.OC"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.06399v2", "summary": "We study the computation of the Petz-Augustin mean of order $\\alpha \\in (0,1)\n\\cup (1,\\infty)$, defined as the minimizer of a weighted sum of $n$\nPetz-R\\'enyi divergences of order $\\alpha$ over the set of $d$-by-$d$ quantum\nstates, where the Petz-R\\'enyi divergence is a quantum generalization of the\nclassical R\\'enyi divergence. We propose the first algorithm with a\nnon-asymptotic convergence guarantee for solving this optimization problem. The\niterates are guaranteed to converge to the Petz-Augustin mean at a linear rate\nof \\( O\\left( \\lvert 1 - 1/\\alpha \\rvert^T \\right) \\) with respect to the\nThompson metric for $\\alpha\\in(1/2,1)\\cup(1,\\infty)$, where \\( T \\) denotes the\nnumber of iterations. The algorithm has an initialization time complexity of\n$O\\left(nd^3\\right)$ and a per-iteration time complexity of $O\\left(nd^2 +\nd^3\\right)$.\n  Two applications follow. First, we propose the first iterative method with a\nnon-asymptotic convergence guarantee for computing the Petz capacity of order\n$\\alpha\\in(1/2,1)$, which generalizes the quantum channel capacity and\ncharacterizes the optimal error exponent in classical-quantum channel coding.\nSecond, we establish that the Petz-Augustin mean of order $\\alpha$, when all\nquantum states commute, is equivalent to the equilibrium prices in Fisher\nmarkets with constant elasticity of substitution (CES) utilities of common\nelasticity $\\rho=1-1/\\alpha$, and our proposed algorithm can be interpreted as\na t\\^{a}tonnement dynamic. We then extend the proposed algorithm to\ninhomogeneous Fisher markets, where buyers have different elasticities, and\nprove that it achieves a faster convergence rate compared to existing\nt\\^{a}tonnement-type algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.06399v2", "cate": "quant-ph", "date": "2025-02-10", "updated": "2025-07-16"}
{"id": "2507.12032", "title": "ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum", "authors": ["Brian-Frederik Jahnke", "René Brinkhege", "Jan Peter Meyer", "Daniel Tebernum", "Falk Howar"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12032v1", "summary": "Achieving sustainable, explainable, and maintainable automation for resource\noptimization is a core challenge across the edge-cloud continuum. Persistent\noverprovisioning and operational complexity often stem from heterogeneous\nplatforms and layered abstractions, while systems lacking explainability and\nmaintainability become fragile, impede safe recovery, and accumulate technical\ndebt. Existing solutions are frequently reactive, limited to single abstraction\nlayers, or require intrusive platform changes, leaving efficiency and\nmaintainability gains unrealized.\n  This paper addresses safe, transparent, and low-effort resource optimization\nin dynamic, multi-tenant edge-cloud systems, without disrupting operator\nworkflows or increasing technical debt. We introduce ARRC, a recommender system\nrooted in software engineering design principles, which delivers explainable,\ncross-layer resource recommendations directly into operator workflows (such as\ntickets and GitOps pull requests). ARRC encapsulates optimization logic in\nspecialized, auditable agents coordinated via a shared interface, supporting\nmaintainability and extensibility through transparency and the ability to\ninspect both recommendations and their rationale.\n  Empirical evaluation in a multi-region industrial deployment shows that ARRC\nreduces operator workload by over 50%, improves compute utilization by up to\n7.7x, and maintains error rates below 5%, with most benefits achieved through\nincremental, operator-approved changes. This demonstrates that explainable,\nrecommendation-based architectures can achieve sustainable efficiency and\nmaintainability improvements at production scale.\n  ARRC provides an empirically evaluated framework for integrating explainable,\nworkflow-driven automation into resource management, intended to advance best\npractices for robust, maintainable, and transparent edge-cloud continuum\nplatforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12032v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11552", "title": "The AI Ethical Resonance Hypothesis: The Possibility of Discovering Moral Meta-Patterns in AI Systems", "authors": ["Tomasz Zgliczyński-Cuber"], "categories": ["cs.CY", "I.2.0; K.4.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      69 pages", "url": "http://arxiv.org/abs/2507.11552v1", "summary": "This paper presents a theoretical framework for the AI ethical resonance\nhypothesis, which proposes that advanced AI systems with purposefully designed\ncognitive structures (\"ethical resonators\") may emerge with the ability to\nidentify subtle moral patterns that are invisible to the human mind. The paper\nexplores the possibility that by processing and synthesizing large amounts of\nethical contexts, AI systems may discover moral meta-patterns that transcend\ncultural, historical, and individual biases, potentially leading to a deeper\nunderstanding of universal ethical foundations. The paper also examines a\nparadoxical aspect of the hypothesis, in which AI systems could potentially\ndeepen our understanding of what we traditionally consider essentially human -\nour capacity for ethical reflection.", "comment": "69 pages", "pdf_url": "http://arxiv.org/pdf/2507.11552v1", "cate": "cs.CY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.11640", "title": "Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)", "authors": ["Veronika Trávníková", "Eric von Lieres", "Marek Behr"], "categories": ["cs.CE", "76-10, 68T07 (Primary) 76D05, 35Q68 (Secondary)"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      24 pages, 18 figures", "url": "http://arxiv.org/abs/2507.11640v1", "summary": "Stirred tanks are vital in chemical and biotechnological processes,\nparticularly as bioreactors. Although computational fluid dynamics (CFD) is\nwidely used to model the flow in stirred tanks, its high computational\ncost$-$especially in multi-query scenarios for process design and\noptimization$-$drives the need for efficient data-driven surrogate models.\nHowever, acquiring sufficiently large datasets can be costly. Physics-informed\nneural networks (PINNs) offer a promising solution to reduce data requirements\nwhile maintaining accuracy by embedding underlying physics into neural network\n(NN) training. This study quantifies the data requirements of vanilla PINNs for\ndeveloping surrogate models of a flow field in a 2D stirred tank. We compare\nthese requirements with classical supervised neural networks and\nboundary-informed neural networks (BINNs). Our findings demonstrate that\nsurrogate models can achieve prediction errors around 3% across Reynolds\nnumbers from 50 to 5000 using as few as six datapoints. Moreover, employing an\napproximation of the velocity profile in place of real data labels leads to\nprediction errors of around 2.5%. These results indicate that even with limited\nor approximate datasets, PINNs can be effectively trained to deliver high\naccuracy comparable to high-fidelity data.", "comment": "24 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2507.11640v1", "cate": "cs.CE", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2504.02477", "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision", "authors": ["Xiaofeng Han", "Shunpeng Chen", "Zenghuang Fu", "Zhe Feng", "Lue Fan", "Dong An", "Changwei Wang", "Li Guo", "Weiliang Meng", "Xiaopeng Zhang", "Rongtao Xu", "Shibiao Xu"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      27 pages, 11 figures, survey paper submitted to Information Fusion", "url": "http://arxiv.org/abs/2504.02477v2", "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.", "comment": "27 pages, 11 figures, survey paper submitted to Information Fusion", "pdf_url": "http://arxiv.org/pdf/2504.02477v2", "cate": "cs.RO", "date": "2025-04-03", "updated": "2025-07-16"}
{"id": "2507.11773", "title": "Small Data Explainer -- The impact of small data methods in everyday life", "authors": ["Maren Hackenberg", "Sophia G. Connor", "Fabian Kabus", "June Brawner", "Ella Markham", "Mahi Hardalupas", "Areeq Chowdhury", "Rolf Backofen", "Anna Köttgen", "Angelika Rohde", "Nadine Binder", "Harald Binder", "the Collaborative Research Center 1597 Small Data"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Written in collaboration with the Royal Society, contributing to the Disability Technology report ( this https URL )", "url": "http://arxiv.org/abs/2507.11773v1", "summary": "The emergence of breakthrough artificial intelligence (AI) techniques has led\nto a renewed focus on how small data settings, i.e., settings with limited\ninformation, can benefit from such developments. This includes societal issues\nsuch as how best to include under-represented groups in data-driven policy and\ndecision making, or the health benefits of assistive technologies such as\nwearables. We provide a conceptual overview, in particular contrasting small\ndata with big data, and identify common themes from exemplary case studies and\napplication areas. Potential solutions are described in a more detailed\ntechnical overview of current data analysis and modelling techniques,\nhighlighting contributions from different disciplines, such as knowledge-driven\nmodelling from statistics and data-driven modelling from computer science. By\nlinking application settings, conceptual contributions and specific techniques,\nwe highlight what is already feasible and suggest what an agenda for fully\nleveraging small data might look like.", "comment": "Written in collaboration with the Royal Society, contributing to the\n  Disability Technology report\n  (https://royalsociety.org/news-resources/projects/disability-data-assistive-technology/)", "pdf_url": "http://arxiv.org/pdf/2507.11773v1", "cate": "cs.CY", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11855", "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "authors": ["Davin Hill", "Brian L. Hill", "Aria Masoomi", "Vijay S. Nori", "Robert E. Tillman", "Jennifer Dy"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11855v1", "summary": "Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11855v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11955", "title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation", "authors": ["Yuhang Zhang", "Zhengyu Zhang", "Muxin Liao", "Shishun Tian", "Wenbin Zou", "Lu Zhang", "Chen Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper was accepted by IEEE Transactions on Intelligent Transportation Systems", "url": "http://arxiv.org/abs/2507.11955v1", "summary": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.", "comment": "This paper was accepted by IEEE Transactions on Intelligent\n  Transportation Systems", "pdf_url": "http://arxiv.org/pdf/2507.11955v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11534", "title": "Sharp Error-Rate Transitions in Quantum QC-LDPC Codes under Joint BP Decoding", "authors": ["Daiki Komoto", "Kenta Kasai"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11534v2", "summary": "In this study, we report that quantum quasi-cyclic low-density parity-check\ncodes decoded via joint belief propagation (BP) exhibit steep error-rate\ncurves, despite the presence of error floors. To the best of our knowledge,\nthis is the first observation of such threshold-like behavior for quantum LDPC\ncodes with non-vanishing coding rate, excluding those decoded with non-binary\nBP decoders. Moreover, we find that dominant error events contributing to the\nerror floor typically involve only a small number of bits. These findings\nsuggest that the error floor is caused by trapping sets--specific subgraph\nstructures in the Tanner graph--and indicate that identifying and avoiding such\nstructures may lead to further reduction of the error floor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11534v2", "cate": "quant-ph", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.12038", "title": "Distributed Algorithms for Potential Problems", "authors": ["Alkida Balliu", "Thomas Boudier", "Francesco d'Amore", "Dennis Olivetti", "Gustav Schmid", "Jukka Suomela"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      28 pages, 4 figures", "url": "http://arxiv.org/abs/2507.12038v1", "summary": "In this work we present a fast distributed algorithm for local potential\nproblems: these are graph problems where the task is to find a locally optimal\nsolution where no node can unilaterally improve the utility in its local\nneighborhood by changing its own label. A simple example of such a problem is\nthe task of finding a locally optimal cut, i.e., a cut where for each node at\nleast half of its incident edges are cut edges. The distributed round\ncomplexity of locally optimal cut has been wide open; the problem is known to\nrequire $\\Omega(\\log n)$ rounds in the deterministic LOCAL model and\n$\\Omega(\\log \\log n)$ rounds in the randomized LOCAL model, but the only known\nupper bound is the trivial brute-force solution of $O(n)$ rounds. Locally\noptimal cut in bounded-degree graphs is perhaps the simplest example of a\nlocally checkable labeling problem for which there is still such a large gap\nbetween current upper and lower bounds. We show that in bounded-degree graphs,\nall local potential problems, including locally optimal cut, can be solved in\n$\\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.\nIn particular, the deterministic round complexity of the locally optimal cut\nproblem is now settled to $\\log^{\\Theta(1)} n$.", "comment": "28 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.12038v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11567", "title": "Consumer Law for AI Agents", "authors": ["Christoph Busch"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11567v1", "summary": "Since the public release of ChatGPT in November 2022, the AI landscape is\nundergoing a rapid transformation. Currently, the use of AI chatbots by\nconsumers has largely been limited to image generation or question-answering\nlanguage models. The next generation of AI systems, AI agents that can plan and\nexecute complex tasks with only limited human involvement, will be capable of a\nmuch broader range of actions. In particular, consumers could soon be able to\ndelegate purchasing decisions to AI agents acting as Custobots. Against this\nbackground, the Article explores whether EU consumer law, as it currently\nstands, is ready for the rise of the Custobot Economy. In doing so, the Article\nmakes three contributions. First, it outlines how the advent of AI agents could\nchange the existing e-commerce landscape. Second, it explains how AI agents\nchallenge the premises of a human-centric consumer law which is based on the\nassumption that consumption decisions are made by humans. Third, the Article\npresents some initial considerations how a future consumer law could look like\nthat works for both humans and machines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11567v1", "cate": "cs.CY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.11870", "title": "MNO : A Multi-modal Neural Operator for Parametric Nonlinear BVPs", "authors": ["Vamshi C. Madala", "Nithin Govindarajan", "Shivkumar Chandrasekaran"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11870v1", "summary": "We introduce a novel Multimodal Neural Operator (MNO) architecture designed\nto learn solution operators for multi-parameter nonlinear boundary value\nproblems (BVPs). Traditional neural operators primarily map either the PDE\ncoefficients or source terms independently to the solution, limiting their\nflexibility and applicability. In contrast, our proposed MNO architecture\ngeneralizes these approaches by mapping multiple parameters including PDE\ncoefficients, source terms, and boundary conditions to the solution space in a\nunified manner. Our MNO is motivated by the hierarchical nested bases of the\nFast Multipole Method (FMM) and is constructed systematically through three key\ncomponents: a parameter efficient Generalized FMM (GFMM) block, a Unimodal\nNeural Operator (UNO) built upon GFMM blocks for single parameter mappings, and\nmost importantly, a multimodal fusion mechanism extending these components to\nlearn the joint map. We demonstrate the multimodal generalization capacity of\nour approach on both linear and nonlinear BVPs. Our experiments show that the\nnetwork effectively handles simultaneous variations in PDE coefficients and\nsource or boundary terms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11870v1", "cate": "cs.CE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.17080", "title": "Geometric Formulation of Unified Force-Impedance Control on SE(3) for Robotic Manipulators", "authors": ["Joohwan Seo", "Nikhil Potu Surya Prakash", "Soomi Lee", "Arvind Kruthiventy", "Megan Teng", "Jongeun Choi", "Roberto Horowitz"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.17080v2", "summary": "In this paper, we present an impedance control framework on the SE(3)\nmanifold, which enables force tracking while guaranteeing passivity. Building\nupon the unified force-impedance control (UFIC) and our previous work on\ngeometric impedance control (GIC), we develop the geometric unified force\nimpedance control (GUFIC) to account for the SE(3) manifold structure in the\ncontroller formulation using a differential geometric perspective. As in the\ncase of the UFIC, the GUFIC utilizes energy tank augmentation for both\nforce-tracking and impedance control to guarantee the manipulator's passivity\nrelative to external forces. This ensures that the end effector maintains safe\ncontact interaction with uncertain environments and tracks a desired\ninteraction force. Moreover, we resolve a non-causal implementation problem in\nthe UFIC formulation by introducing velocity and force fields. Due to its\nformulation on SE(3), the proposed GUFIC inherits the desirable SE(3)\ninvariance and equivariance properties of the GIC, which helps increase sample\nefficiency in machine learning applications where a learning algorithm is\nincorporated into the control law. The proposed control law is validated in a\nsimulation environment under scenarios requiring tracking an SE(3) trajectory,\nincorporating both position and orientation, while exerting a force on a\nsurface. The codes are available at\nhttps://github.com/Joohwan-Seo/GUFIC_mujoco.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.17080v2", "cate": "cs.RO", "date": "2025-04-23", "updated": "2025-07-16"}
{"id": "2507.11783", "title": "Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions", "authors": ["Gayal Kuruppu", "Neeraj Wagh", "Yogatheesan Varatharajah"], "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC", "A.1; I.2; I.5; J.3"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      20 pages, 5 figures, 2 tables", "url": "http://arxiv.org/abs/2507.11783v1", "summary": "Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.", "comment": "20 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.11783v1", "cate": "eess.SP", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11865", "title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers", "authors": ["Hanwen Dai", "Chang Gao", "Fang He", "Congyuan Ji", "Yanni Yang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11865v1", "summary": "The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11865v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11967", "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", "authors": ["Yuchi Ishikawa", "Shota Nakada", "Hokuto Munakata", "Kazuhiro Saito", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Interspeech 2025", "url": "http://arxiv.org/abs/2507.11967v1", "summary": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.", "comment": "Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.11967v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12106", "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "categories": ["cs.DC", "cs.CY"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      18 pages, 6 Figures", "url": "http://arxiv.org/abs/2507.12106v1", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life.", "comment": "18 pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2507.12106v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11802", "title": "FAIR-CS: Framework for Interdisciplinary Research Collaborations in Online Computing Programs", "authors": ["Breanna Shi", "Thomas Deatherage", "Jeanette Schofield", "Charles R. Clark", "Thomas Orth", "Nicholas Lytle"], "categories": ["cs.CY", "cs.GL"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11802v1", "summary": "Research experience is crucial for computing master's students pursuing\nacademic and scientific careers, yet online students have traditionally been\nexcluded from these opportunities due to the physical constraints of\ntraditional research environments. This paper presents the Framework for\nAccelerating Interdisciplinary Research in Computer Science (FAIR-CS), a method\nfor achieving research goals, developing research communities, and supporting\nhigh quality mentorship in an online research environment. This method advances\nvirtual research operations by orchestrating dynamic partnerships between\nmaster's level researchers and academic mentors, resulting in interdisciplinary\npublications. We then discuss the implementation of FAIR-CS in the\nHuman-Augmented Analytics Group (HAAG), with researchers from the Georgia\nTech's Online Master of Computer Science program. Through documented project\nrecords and experiences with 72 active users, we present our lessons learned\nand evaluate the evolution of FAIR-CS in HAAG. This paper serves as a\ncomprehensive resource for other institutions seeking to establish similar\nvirtual research initiatives, demonstrating how the traditional research lab\nenvironment can be effectively replicated in the virtual space while\nmaintaining robust collaborative relationships and supporting knowledge\ntransfer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11802v1", "cate": "cs.CY", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12233", "title": "Universal Fourier Neural Operators for Micromechanics", "authors": ["Binh Huy Nguyen", "Matti Schneider"], "categories": ["cs.CE", "cs.LG"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      48 pages, 13 figures", "url": "http://arxiv.org/abs/2507.12233v1", "summary": "\\noindent Solving cell problems in homogenization is hard, and available\ndeep-learning frameworks fail to match the speed and generality of traditional\ncomputational frameworks. More to the point, it is generally unclear what to\nexpect of machine-learning approaches, let alone single out which approaches\nare promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)\nfor micromechanics, empowering them by insights from computational\nmicromechanics methods based on the fast Fourier transform (FFT). We construct\nan FNO surrogate mimicking the basic scheme foundational for FFT-based methods\nand show that the resulting operator predicts solutions to cell problems with\n\\emph{arbitrary} stiffness distribution only subject to a material-contrast\nconstraint up to a desired accuracy. In particular, there are no restrictions\non the material symmetry like isotropy, on the number of phases and on the\ngeometry of the interfaces between materials. Also, the provided fidelity is\nsharp and uniform, providing explicit guarantees leveraging our physical\nempowerment of FNOs. To show the desired universal approximation property, we\nconstruct an FNO explicitly that requires no training to begin with. Still, the\nobtained neural operator complies with the same memory requirements as the\nbasic scheme and comes with runtimes proportional to classical FFT solvers. In\nparticular, large-scale problems with more than 100 million voxels are readily\nhandled. The goal of this work is to underline the potential of FNOs for\nsolving micromechanical problems, linking FFT-based methods to FNOs. This\nconnection is expected to provide a fruitful exchange between both worlds.", "comment": "48 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.12233v1", "cate": "cs.CE", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11873", "title": "Syntax Repair as Language Intersection", "authors": ["Breandan Considine"], "categories": ["cs.FL", "cs.PL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11873v1", "summary": "We introduce a new technique for repairing syntax errors in arbitrary\ncontext-free languages. This technique models syntax repair as a language\nintersection problem by defining a finite language that provably generates\nevery syntactically valid repair within a given edit distance. Leveraging a\ntheoretical connection between the Bar-Hillel construction from formal language\ntheory and CFL reachability from program analysis, we show that repairability\nin a finite number of typographic edits is polylogarithmic parallel time\ndecidable and provide an enumeration algorithm based on the Brzozowski\nderivative. Finally, we evaluate this algorithm and its implementation,\ndemonstrating state-of-the-art results on a Python syntax repair benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11873v1", "cate": "cs.FL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.00784", "title": "Reconfigurable legged metamachines that run on autonomous modular legs", "authors": ["Chen Yu", "David Matthews", "Jingxian Wang", "Jing Gu", "Douglas Blackiston", "Michael Rubenstein", "Sam Kriegman"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00784v2", "summary": "Legged machines are becoming increasingly agile and adaptive but they have so\nfar lacked the morphological diversity of legged animals, which have been\nrearranged and reshaped to fill millions of niches. Unlike their biological\ncounterparts, legged machines have largely converged over the past decade to\ncanonical quadrupedal and bipedal architectures that cannot be easily\nreconfigured to meet new tasks or recover from injury. Here we introduce\nautonomous modular legs: agile yet minimal, single-degree-of-freedom jointed\nlinks that can learn complex dynamic behaviors and may be freely attached to\nform legged metamachines at the meter scale. This enables rapid repair,\nredesign, and recombination of highly-dynamic modular agents that move quickly\nand acrobatically (non-quasistatically) through unstructured environments.\nBecause each module is itself a complete agent, legged metamachines are able to\nsustain deep structural damage that would completely disable other legged\nrobots. We also show how to encode the vast space of possible body\nconfigurations into a compact latent design genome that can be efficiently\nexplored, revealing a wide diversity of novel legged forms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00784v2", "cate": "cs.RO", "date": "2025-05-01", "updated": "2025-07-16"}
{"id": "2507.11799", "title": "Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network", "authors": ["Shin-ichi Ito"], "categories": ["physics.comp-ph", "cs.AI"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11799v1", "summary": "This paper presents a neural network (NN)-based solver for an\nintegro-differential equation that models shrinkage-induced fragmentation. The\nproposed method directly maps input parameters to the corresponding probability\ndensity function without numerically solving the governing equation, thereby\nsignificantly reducing computational costs. Specifically, it enables efficient\nevaluation of the density function in Monte Carlo simulations while maintaining\naccuracy comparable to or even exceeding that of conventional finite difference\nschemes. Validatation on synthetic data demonstrates both the method's\ncomputational efficiency and predictive reliability. This study establishes a\nfoundation for the data-driven inverse analysis of fragmentation and suggests\nthe potential for extending the framework beyond pre-specified model\nstructures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11799v1", "cate": "physics.comp-ph", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11901", "title": "Imbalanced Regression Pipeline Recommendation", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11901v1", "summary": "Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11901v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11968", "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation", "authors": ["Sahid Hossain Mustakim", "S M Jishanul Islam", "Ummay Maria Muna", "Montasir Chowdhury", "Mohammed Jawwadul Islam", "Sadia Ahmmed", "Tashfia Sikder", "Syed Tasdid Azam Dhrubo", "Swakkhar Shatabda"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted as long paper, SVU Workshop at ICCV 2025", "url": "http://arxiv.org/abs/2507.11968v1", "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.", "comment": "Accepted as long paper, SVU Workshop at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.11968v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12205", "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage", "authors": ["Junqing Lin", "Jingwei Sun", "Mingge Lu", "Guangzhong Sun"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.12205v1", "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.12205v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12162", "title": "A real-time metric of online engagement monitoring", "authors": ["Laura J. Johnston", "Jim E. Griffin", "Ioanna Manolopoulou", "Takoua Jendoubi"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      32 pages, 5 figures", "url": "http://arxiv.org/abs/2507.12162v1", "summary": "Measuring online behavioural student engagement often relies on simple count\nindicators or retrospective, predictive methods, which present challenges for\nreal-time application. To address these limitations, we reconceptualise an\nexisting course-wide engagement metric to create a chapter-based version that\naligns with the weekly structure of online courses. Derived directly from\nvirtual learning environment log data, the new metric allows for cumulative,\nreal-time tracking of student activity without requiring outcome data or model\ntraining. We evaluate the approach across three undergraduate statistics\nmodules over two academic years, comparing it to the course-wide formulation to\nassess how the reconceptualisation influences what is measured. Results\nindicate strong alignment from as early as week 3, along with comparable or\nimproved predictive validity for final grades in structured, lecture-based\ncontexts. By the course midpoint, the weekly metric identifies as many\nlow-performing students as are identifiable by the end of the course. While\nperformance varies across modules, the chapter-based formulation offers a\nscalable and interpretable method for early engagement monitoring and student\nsupport.", "comment": "32 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.12162v1", "cate": "cs.CY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12425", "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "authors": ["Chandana Cheerla"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12425v1", "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12425v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12102", "title": "Hyper pattern matching", "authors": ["Masaki Waga", "Étienne André"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      This is the author (and extended) version of the manuscript of the same name published in the proceedings of the 25th International Conference on Runtime Verification (RV 2025)", "url": "http://arxiv.org/abs/2507.12102v1", "summary": "In runtime verification, pattern matching, which searches for occurrences of\na specific pattern within a word, provides more information than a simple\nviolation detection of the monitored property, by locating concrete evidence of\nthe violation. However, witnessing violations of some properties, particularly\nhyperproperties, requires evidence across multiple input words or different\nparts of the same word, which goes beyond the scope of conventional pattern\nmatching. We propose here hyper pattern matching, a generalization of pattern\nmatching over a set of words. Properties of interest include robustness and\n(non-)interference. As a formalism for patterns, we use nondeterministic\nasynchronous finite automata (NAAs). We first provide a naive algorithm for\nhyper pattern matching and then devise several heuristics for better\nefficiency. Although we prove the NP-completeness of the problem, our\nimplementation HypPAu is able to address several case studies scalable in the\nlength, number of words (or logs) and number of dimensions, suggesting the\npractical relevance of our approach.", "comment": "This is the author (and extended) version of the manuscript of the\n  same name published in the proceedings of the 25th International Conference\n  on Runtime Verification (RV 2025)", "pdf_url": "http://arxiv.org/pdf/2507.12102v1", "cate": "cs.FL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.18212", "title": "Haptic-Informed ACT with a Soft Gripper and Recovery-Informed Training for Pseudo Oocyte Manipulation", "authors": ["Pedro Miguel Uriguen Eljuri", "Hironobu Shibata", "Maeyama Katsuyoshi", "Yuanyuan Jia", "Tadahiro Taniguchi"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2025) Project website this https URL", "url": "http://arxiv.org/abs/2506.18212v3", "summary": "In this paper, we introduce Haptic-Informed ACT, an advanced robotic system\nfor pseudo oocyte manipulation, integrating multimodal information and Action\nChunking with Transformers (ACT). Traditional automation methods for oocyte\ntransfer rely heavily on visual perception, often requiring human supervision\ndue to biological variability and environmental disturbances. Haptic-Informed\nACT enhances ACT by incorporating haptic feedback, enabling real-time grasp\nfailure detection and adaptive correction. Additionally, we introduce a\n3D-printed TPU soft gripper to facilitate delicate manipulations. Experimental\nresults demonstrate that Haptic-Informed ACT improves the task success rate,\nrobustness, and adaptability compared to conventional ACT, particularly in\ndynamic environments. These findings highlight the potential of multimodal\nlearning in robotics for biomedical automation.", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS2025) Project website\n  https://tanichu-laboratory.github.io/pedro_haptic_act_iros2025/", "pdf_url": "http://arxiv.org/pdf/2506.18212v3", "cate": "cs.RO", "date": "2025-06-23", "updated": "2025-07-16"}
{"id": "2507.11809", "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 Pages, 13 figures", "url": "http://arxiv.org/abs/2507.11809v1", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "comment": "18 Pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.11809v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11902", "title": "Resampling strategies for imbalanced regression: a survey and empirical analysis", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11902v1", "summary": "Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11902v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11969", "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models", "authors": ["Zhaohong Huang", "Yuxin Zhang", "Jingjing Xie", "Fei Chao", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11969v1", "summary": "Recent advances in test-time adaptation (TTA) for Vision-Language Models\n(VLMs) have garnered increasing attention, particularly through the use of\nmultiple augmented views of a single image to boost zero-shot generalization.\nUnfortunately, existing methods fail to strike a satisfactory balance between\nperformance and efficiency, either due to excessive overhead of tuning text\nprompts or unstable benefits from handcrafted, training-free visual feature\nenhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),\nan efficient and effective TTA paradigm that incorporates two learnable biases\nduring TTA, unfolded as the global bias and spatial bias. Particularly, the\nglobal bias captures the global semantic features of a test image by learning\nconsistency across augmented views, while spatial bias learns the semantic\ncoherence between regions in the image's spatial visual representation. It is\nworth highlighting that these two sets of biases are directly added to the\nlogits outputed by the pretrained VLMs, which circumvent the full\nbackpropagation through VLM that hinders the efficiency of existing TTA\nmethods. This endows GS-Bias with extremely high efficiency while achieving\nstate-of-the-art performance on 15 benchmark datasets. For example, it achieves\na 2.23% improvement over TPT in cross-dataset generalization and a 2.72%\nimprovement in domain generalization, while requiring only 6.5% of TPT's memory\nusage on ImageNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11969v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11941", "title": "BlockBPE: Parallel BPE Tokenization", "authors": ["Amos You"], "categories": ["cs.CL", "cs.DC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models (ICML 2025)", "url": "http://arxiv.org/abs/2507.11941v1", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.11941v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11568", "title": "La Última Frontera de La Filosofía: Hacia una Síntesis de La Ética del Futuro a Largo Plazo, el Riesgo Existencial y la Ontología Posthumana", "authors": ["Santos E. Moreta Reyes"], "categories": ["physics.soc-ph", "cs.CY", "physics.hist-ph"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      14 paginas, in Spanish language. sin figuras. Articulo en español con resumen y titulo en ingles. Este ensayo interdisciplinario sintetiza el largoplacismo (longtermism), el riesgo existencial y la filosofía posthumanista para articular una agenda de investigación para una filosofía prospectiva. Dirigido tanto a audiencias académicas como al publico general", "url": "http://arxiv.org/abs/2507.11568v1", "summary": "Humanity's unprecedented technological capacity and concurrent existential\nrisks reveal a critical lacuna in the philosophical tradition: the absence of a\nsystematic framework for the long-term future. This article argues that\nformulating such a framework is the central ethical imperative of our era. To\ndefend this thesis, it synthesizes the normative ethics of Hans Jonas and Derek\nParfit with the analytical framework of Nick Bostrom's work on existential risk\nand longtermism. The analysis further addresses the ontological challenge posed\nby posthumanism to the human 'subject' and explores the functional role of a\nsecular cosmic purpose in motivating long-term action. The paper's main\ncontribution is the articulation of a synthetic research agenda for a\nprospective philosophy, one that integrates axiology, risk management, and\nontology to guide humanity through its perilous technological adolescence.", "comment": "14 paginas, in Spanish language. sin figuras. Articulo en espa\\~nol\n  con resumen y titulo en ingles. Este ensayo interdisciplinario sintetiza el\n  largoplacismo (longtermism), el riesgo existencial y la filosof\\'ia\n  posthumanista para articular una agenda de investigaci\\'on para una\n  filosof\\'ia prospectiva. Dirigido tanto a audiencias acad\\'emicas como al\n  publico general", "pdf_url": "http://arxiv.org/pdf/2507.11568v1", "cate": "physics.soc-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.06444", "title": "Eyes on the Road, Mind Beyond Vision: Context-Aware Multi-modal Enhanced Risk Anticipation", "authors": ["Jiaxun Zhang", "Haicheng Liao", "Yumu Xie", "Chengyue Wang", "Yanchen Guan", "Bin Rao", "Zhenning Li"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      Accepted by ACMMM2025", "url": "http://arxiv.org/abs/2507.06444v2", "summary": "Accurate accident anticipation remains challenging when driver cognition and\ndynamic road conditions are underrepresented in predictive models. In this\npaper, we propose CAMERA (Context-Aware Multi-modal Enhanced Risk\nAnticipation), a multi-modal framework integrating dashcam video, textual\nannotations, and driver attention maps for robust accident anticipation. Unlike\nexisting methods that rely on static or environment-centric thresholds, CAMERA\nemploys an adaptive mechanism guided by scene complexity and gaze entropy,\nreducing false alarms while maintaining high recall in dynamic, multi-agent\ntraffic scenarios. A hierarchical fusion pipeline with Bi-GRU (Bidirectional\nGRU) captures spatio-temporal dependencies, while a Geo-Context Vision-Language\nmodule translates 3D spatial relationships into interpretable, human-centric\nalerts. Evaluations on the DADA-2000 and benchmarks show that CAMERA achieves\nstate-of-the-art performance, improving accuracy and lead time. These results\ndemonstrate the effectiveness of modeling driver attention, contextual\ndescription, and adaptive risk thresholds to enable more reliable accident\nanticipation.", "comment": "Accepted by ACMMM2025", "pdf_url": "http://arxiv.org/pdf/2507.06444v2", "cate": "cs.CE", "date": "2025-07-08", "updated": "2025-07-16"}
{"id": "2410.01590", "title": "Active Learning of Deterministic Transducers with Outputs in Arbitrary Monoids", "authors": ["Quentin Aristote"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      28 pages, 3 figures; preprint submitted to Logical Methods in Computer Science; an extended abstract of this work was presented to the 32nd EACSL Annual Conference on Computer Science Logic 2024", "url": "http://arxiv.org/abs/2410.01590v3", "summary": "We study monoidal transducers, transition systems arising as deterministic\nautomata whose transitions also produce outputs in an arbitrary monoid, for\ninstance allowing outputs to commute or to cancel out. We use the categorical\nframework for minimization and learning of Colcombet, Petri\\c{s}an and Stabile\nto recover the notion of minimal transducer recognizing a language, and give\nnecessary and sufficient conditions on the output monoid for this minimal\ntransducer to exist and be unique (up to isomorphism). The categorical\nframework then provides an abstract algorithm for learning it using membership\nand equivalence queries, and we discuss practical aspects of this algorithm's\nimplementation.", "comment": "28 pages, 3 figures; preprint submitted to Logical Methods in\n  Computer Science; an extended abstract of this work was presented to the 32nd\n  EACSL Annual Conference on Computer Science Logic 2024", "pdf_url": "http://arxiv.org/pdf/2410.01590v3", "cate": "cs.FL", "date": "2024-10-02", "updated": "2025-07-16"}
{"id": "2506.21030", "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner", "authors": ["Tianxing Zhou", "Zhirui Wang", "Haojia Ao", "Guangyan Chen", "Boyang Xing", "Jingwen Cheng", "Yi Yang", "Yufeng Yue"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21030v2", "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21030v2", "cate": "cs.RO", "date": "2025-06-26", "updated": "2025-07-16"}
{"id": "2507.11810", "title": "The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist", "authors": ["Haoxuan Zhang", "Ruochi Li", "Yang Zhang", "Ting Xiao", "Jiangping Chen", "Junhua Ding", "Haihua Chen"], "categories": ["cs.DL", "cs.AI"], "primary_category": "Subjects:       Digital Libraries (cs.DL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11810v1", "summary": "Scientific innovation is undergoing a paradigm shift driven by the rapid\nadvancement of Large Language Models (LLMs). As science faces mounting\nchallenges including information overload, disciplinary silos, and diminishing\nreturns on conventional research methods, LLMs are emerging as powerful agents\ncapable not only of enhancing scientific workflows but also of participating in\nand potentially leading the innovation process. Existing surveys mainly focus\non different perspectives, phrases, and tasks in scientific research and\ndiscovery, while they have limitations in understanding the transformative\npotential and role differentiation of LLM. This survey proposes a comprehensive\nframework to categorize the evolving roles of LLMs in scientific innovation\nacross three hierarchical levels: Evaluator, Collaborator, and Scientist. We\ndistinguish between LLMs' contributions to structured scientific research\nprocesses and open-ended scientific discovery, thereby offering a unified\ntaxonomy that clarifies capability boundaries, evaluation criteria, and\nhuman-AI interaction patterns at each level. Through an extensive analysis of\ncurrent methodologies, benchmarks, systems, and evaluation metrics, this survey\ndelivers an in-depth and systematic synthesis on LLM-driven scientific\ninnovation. We present LLMs not only as tools for automating existing\nprocesses, but also as catalysts capable of reshaping the epistemological\nfoundations of science itself. This survey offers conceptual clarity, practical\nguidance, and theoretical foundations for future research, while also\nhighlighting open challenges and ethical considerations in the pursuit of\nincreasingly autonomous AI-driven science. Resources related to this survey can\nbe accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11810v1", "cate": "cs.DL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11926", "title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning", "authors": ["Max Hopkins", "Sihan Liu", "Christopher Ye", "Yuichi Yoshida"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      67 pages", "url": "http://arxiv.org/abs/2507.11926v1", "summary": "The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.", "comment": "67 pages", "pdf_url": "http://arxiv.org/pdf/2507.11926v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11980", "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models", "authors": ["Jiajian Xie", "Shengyu Zhang", "Zhou Zhao", "Fan Wu", "Fei Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 8 figures", "url": "http://arxiv.org/abs/2507.11980v1", "summary": "Diffusion Models have shown remarkable proficiency in image and video\nsynthesis. As model size and latency increase limit user experience, hybrid\nedge-cloud collaborative framework was recently proposed to realize fast\ninference and high-quality generation, where the cloud model initiates\nhigh-quality semantic planning and the edge model expedites later-stage\nrefinement. However, excessive cloud denoising prolongs inference time, while\ninsufficient steps cause semantic ambiguity, leading to inconsistency in edge\nmodel output. To address these challenges, we propose EC-Diff that accelerates\ncloud inference through gradient-based noise estimation while identifying the\noptimal point for cloud-edge handoff to maintain generation quality.\nSpecifically, we design a K-step noise approximation strategy to reduce cloud\ninference frequency by using noise gradients between steps and applying cloud\ninference periodically to adjust errors. Then we design a two-stage greedy\nsearch algorithm to efficiently find the optimal parameters for noise\napproximation and edge model switching. Extensive experiments demonstrate that\nour method significantly enhances generation quality compared to edge\ninference, while achieving up to an average $2\\times$ speedup in inference\ncompared to cloud inference. Video samples and source code are available at\nhttps://ec-diff.github.io/.", "comment": "21 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.11980v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.03220", "title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning", "authors": ["Saransh Gupta", "Umesh Deshpande", "Travis Janssen", "Swami Sundararaman"], "categories": ["cs.DC", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03220v2", "summary": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the\ntask specific parameters into adapters, which are a fraction of the size of the\noriginal base model. Popularity of PEFT technique for fine-tuning has led to\ncreation of a large number of adapters for popular Large Language Models\n(LLMs). However, existing frameworks fall short in supporting inference or\nfine-tuning with multiple adapters in the following ways. 1) For fine-tuning,\neach job needs to deploy its dedicated base model instance, which results in\nexcessive GPU memory consumption and poor GPU utilization. 2) While popular\ninference platforms can serve multiple PEFT adapters, they do not allow\nindependent resource management or mixing of different PEFT methods. 3) They\ncannot share resources (such as base model instance) between inference and\nfine-tuning jobs. 4) They do not provide privacy to users who may not wish to\nexpose their fine-tuned parameters to service providers. In Symbiosis, we\naddress the above problems by enabling as-a-service deployment of base model.\nThe base model layers can be shared across multiple inference or fine-tuning\nprocesses. Our split-execution technique decouples the execution of\nclient-specific adapters and layers from the frozen base model layers offering\nthem flexibility to manage their resources, to select their fine-tuning method,\nto achieve their performance goals. Our approach is transparent to models and\nworks out-of-the-box for most models in the transformers library. Our\nevaluation on Llama2-13B shows the compared to baseline, Symbiosis can\nfine-tune 4X more adapters on the same set of GPUs in the same amount of time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03220v2", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-16"}
{"id": "2507.11966", "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11966v1", "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11966v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.08140", "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally", "authors": ["Tobias Schnabel", "Kiran Tomlinson", "Adith Swaminathan", "Jennifer Neville"], "categories": ["cs.AI", "cs.FL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      28 pages", "url": "http://arxiv.org/abs/2505.08140v3", "summary": "Despite their many successes, transformer-based large language models (LLMs)\ncontinue to struggle with tasks that require complex reasoning over large parts\nof their input. We argue that these failures arise due to capacity limits on\nthe accurate flow of information within LLMs. To formalize this issue, we\nintroduce the bounded attention prefix oracle (BAPO) model, a new computational\nframework that models bandwidth constraints on attention heads, the mechanism\nfor internal communication in LLMs. We show that several important reasoning\nproblems like graph reachability require high communication bandwidth for BAPOs\nto solve; we call these problems BAPO-hard. Our experiments corroborate our\ntheoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks\nand fail even on relatively small BAPO-hard tasks. BAPOs also reveal another\nbenefit of chain of thought (CoT): we prove that breaking down a task using CoT\ncan turn any BAPO-hard problem into a BAPO-easy one. Our results offer\nprincipled explanations for key LLM failures and suggest directions for\narchitectures and inference methods that mitigate bandwidth limits.", "comment": "28 pages", "pdf_url": "http://arxiv.org/pdf/2505.08140v3", "cate": "cs.AI", "date": "2025-05-13", "updated": "2025-07-15"}
{"id": "2507.09822", "title": "Active Probing with Multimodal Predictions for Motion Planning", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin D'sa"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      To appear at IROS '25. 8 pages. 3 tables. 6 figures", "url": "http://arxiv.org/abs/2507.09822v2", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09822v2", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-15"}
{"id": "2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.11936v1", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.11936v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11928", "title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning", "authors": ["Abhishek Sriram", "Neal Tuffy"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper is a pre-print version and has been submitted to the IEEE International Conference on Future Machine Learning and Data Science (FMLDS 2025)", "url": "http://arxiv.org/abs/2507.11928v1", "summary": "This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.", "comment": "This paper is a pre-print version and has been submitted to the IEEE\n  International Conference on Future Machine Learning and Data Science (FMLDS\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.11928v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11985", "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints", "authors": ["Jiahao Xia", "Yike Wu", "Wenjian Huang", "Jianguo Zhang", "Jian Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.11985v1", "summary": "Part-level features are crucial for image understanding, but few studies\nfocus on them because of the lack of fine-grained labels. Although unsupervised\npart discovery can eliminate the reliance on labels, most of them cannot\nmaintain robustness across various categories and scenarios, which restricts\ntheir application range. To overcome this limitation, we present a more\neffective paradigm for unsupervised part discovery, named Masked Part\nAutoencoder (MPAE). It first learns part descriptors as well as a feature map\nfrom the inputs and produces patch features from a masked version of the\noriginal images. Then, the masked regions are filled with the learned part\ndescriptors based on the similarity between the local features and descriptors.\nBy restoring these masked patches using the part descriptors, they become\nbetter aligned with their part shapes, guided by appearance features from\nunmasked patches. Finally, MPAE robustly discovers meaningful parts that\nclosely match the actual object shapes, even in complex scenarios. Moreover,\nseveral looser yet more effective constraints are proposed to enable MPAE to\nidentify the presence of parts across various scenarios and categories in an\nunsupervised manner. This provides the foundation for addressing challenges\nposed by occlusion and for exploring part similarity across multiple\ncategories. Extensive experiments demonstrate that our method robustly\ndiscovers meaningful parts across various categories and scenarios. The code is\navailable at the project https://github.com/Jiahao-UTS/MPAE.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.11985v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.06608", "title": "Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM Serving", "authors": ["Xiaoxiang Shi", "Colin Cai", "Junjia Du"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06608v4", "summary": "Monolithic serving with chunked prefill improves GPU utilization by batching\nprefill and decode together, but suffers from fine-grained phase interference.\nEngine-level prefill-decode (PD) disaggregation avoids interference but incurs\nhigher hardware and coordination overhead. Prior intra-GPU disaggregation\napproaches multiplex prefill and decode within a single GPU, using SLO-based\ntuning guided by heuristics from offline profiling or reactive feedback loops.\nHowever, these methods respond reactively to performance issues rather than\nanticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts\neffectively to dynamic workloads? The key challenge lies in managing the\nconflicting resource demands of prefill and decode under varying conditions. We\nfirst show that GPU resources exhibit diminishing returns -- beyond a\nsaturation point, more allocation yields minimal latency benefit. Second, we\nobserve that memory bandwidth contention becomes a critical bottleneck. These\ninsights motivate a design that dynamically partitions GPU resources across\nprefill and decode phases, while jointly considering compute capacity, memory\nfootprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms\nSGLang by up to 2x; and matches or exceeds disaggregated vLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06608v4", "cate": "cs.DC", "date": "2025-07-09", "updated": "2025-07-16"}
{"id": "2507.12103", "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7pages, 4 figures. Accepted to IJCAI 2025", "url": "http://arxiv.org/abs/2507.12103v1", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "comment": "7pages, 4 figures. Accepted to IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.12103v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11726", "title": "A Deep Reinforcement Learning Method for Multi-objective Transmission Switching", "authors": ["Ding Lin", "Jianhui Wang", "Tianqiao Zhao", "Meng Yue"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.11726v1", "summary": "Transmission switching is a well-established approach primarily applied to\nminimize operational costs through strategic network reconfiguration. However,\nexclusive focus on cost reduction can compromise system reliability. While\nmulti-objective transmission switching can balance cost savings with\nreliability improvements, feasible solutions become exceedingly difficult to\nobtain as system scale grows, due to the inherent nonlinearity and high\ncomputational demands involved. This paper proposes a deep reinforcement\nlearning (DRL) method for multi-objective transmission switching. The method\nincorporates a dueling-based actor-critic framework to evaluate the relative\nimpact of each line switching decision within the action space, which improves\ndecision quality and enhances both system reliability and cost efficiency.\nNumerical studies on the IEEE 118-bus system verify the effectiveness and\nefficiency of the proposed approach compared to two benchmark DRL algorithms.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.11726v1", "cate": "eess.SY", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11069", "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11069v2", "summary": "Understanding the 3D geometry of transparent objects from RGB images is\nchallenging due to their inherent physical properties, such as reflection and\nrefraction. To address these difficulties, especially in scenarios with sparse\nviews and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian\nSplatting-based depth reconstruction method for transparent objects. Our key\ninsight lies in separating transparent objects from the background, enabling\nfocused optimization of Gaussians corresponding to the object. We mitigate\nartifacts with an object-aware loss that places Gaussians in obscured regions,\nensuring coverage of invisible surfaces while reducing overfitting.\nFurthermore, we incorporate a physics-based simulation that refines the\nreconstruction in just a few seconds, effectively handling object removal and\nchain-reaction movement of remaining objects without the need for rescanning.\nTRAN-D is evaluated on both synthetic and real-world sequences, and it\nconsistently demonstrated robust improvements over existing GS-based\nstate-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean\nabsolute error by over 39% for the synthetic TRansPose sequences. Furthermore,\ndespite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm\naccuracy of 48.46%, over 1.5 times that of baselines, which uses six images.\nCode and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11069v2", "cate": "cs.RO", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.11939", "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in Progress", "url": "http://arxiv.org/abs/2507.11939v1", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2507.11939v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11997", "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection", "authors": ["Tairan Huang", "Yili Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11997v1", "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11997v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11986", "title": "Style Composition within Distinct LoRA modules for Traditional Art", "authors": ["Jaehyun Lee", "Wonhark Park", "Wonsik Shin", "Hyunho Lee", "Hyoung Min Na", "Nojun Kwak"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11986v1", "summary": "Diffusion-based text-to-image models have achieved remarkable results in\nsynthesizing diverse images from text prompts and can capture specific artistic\nstyles via style personalization. However, their entangled latent space and\nlack of smooth interpolation make it difficult to apply distinct painting\ntechniques in a controlled, regional manner, often causing one style to\ndominate. To overcome this, we propose a zero-shot diffusion pipeline that\nnaturally blends multiple styles by performing style composition on the\ndenoised latents predicted during the flow-matching denoising process of\nseparately trained, style-specialized models. We leverage the fact that\nlower-noise latents carry stronger stylistic information and fuse them across\nheterogeneous diffusion pipelines using spatial masks, enabling precise,\nregion-specific style control. This mechanism preserves the fidelity of each\nindividual style while allowing user-guided mixing. Furthermore, to ensure\nstructural coherence across different models, we incorporate depth-map\nconditioning via ControlNet into the diffusion framework. Qualitative and\nquantitative experiments demonstrate that our method successfully achieves\nregion-specific style mixing according to the given masks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11986v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.23210", "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "authors": ["Taehwan Yoon", "Bongjun Choi"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages,14 equation, 4 figure, 1table", "url": "http://arxiv.org/abs/2506.23210v2", "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and utilizes a reference model that\nincorporates previous model parameters. As a result, this method achieves both\nhigh model performance and clients' low computing cost.", "comment": "6 pages,14 equation, 4 figure, 1table", "pdf_url": "http://arxiv.org/pdf/2506.23210v2", "cate": "cs.LG", "date": "2025-06-29", "updated": "2025-07-16"}
{"id": "2505.09598", "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference", "authors": ["Nidhal Jegham", "Marwan Abdelatti", "Lassad Elmoubarki", "Abdeltawab Hendawi"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09598v3", "summary": "This paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.42 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nAlthough AI is becoming cheaper and faster, its global adoption drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09598v3", "cate": "cs.CY", "date": "2025-05-14", "updated": "2025-07-15"}
{"id": "2507.11749", "title": "Reconfigurable Battery Systems for Enhanced Fast Charging in Electric Vehicles", "authors": ["Jonathan Olivares", "Tyler Depe", "Rakeshkumar Mahto"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11749v1", "summary": "The adoption of electric vehicles (EVs) is rapidly growing as a key solution\nto reducing greenhouse gas emissions. However, prolonged charging times remain\na significant barrier to widespread EV usage, especially for individuals\nwithout access to fast charging infrastructure. This paper explores the\npotential of reconfigurable battery systems to reduce EV charging times without\ncompromising battery life. We propose innovative battery pack configurations\nthat dynamically adjust the arrangement of cells to optimize charging\nperformance. Simulations were conducted using MATLAB and Simulink to compare\nthe efficiency of various battery configurations, focusing on charging times,\nstate of charge (SOC), voltage, and current under different conditions. The\nresults demonstrate that connecting more batteries in series through\nreconfigurability in battery packs can significantly reduce charging times\nwhile maintaining operational safety. This study offers insights into how\nreconfigurable battery designs can provide a practical solution for faster,\nmore efficient home-based EV charging, making EV ownership more accessible and\nsustainable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11749v1", "cate": "eess.SY", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11498", "title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming", "authors": ["Asad Ali Shahid", "Francesco Braghin", "Loris Roveda"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11498v2", "summary": "Humanoid robots have seen remarkable advances in dexterity, balance, and\nlocomotion, yet their role in expressive domains such as music performance\nremains largely unexplored. Musical tasks, like drumming, present unique\nchallenges, including split-second timing, rapid contacts, and multi-limb\ncoordination over performances lasting minutes. In this paper, we introduce\nRobot Drummer, a humanoid capable of expressive, high-precision drumming across\na diverse repertoire of songs. We formulate humanoid drumming as sequential\nfulfillment of timed contacts and transform drum scores into a Rhythmic Contact\nChain. To handle the long-horizon nature of musical performance, we decompose\neach piece into fixed-length segments and train a single policy across all\nsegments in parallel using reinforcement learning. Through extensive\nexperiments on over thirty popular rock, metal, and jazz tracks, our results\ndemonstrate that Robot Drummer consistently achieves high F1 scores. The\nlearned behaviors exhibit emergent human-like drumming strategies, such as\ncross-arm strikes, and adaptive stick assignments, demonstrating the potential\nof reinforcement learning to bring humanoid robots into the domain of creative\nmusical performance. Project page: robotdrummer.github.io", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11498v2", "cate": "cs.RO", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.11959", "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ECAI 2025 (European Conference on Artificial Intelligence)", "url": "http://arxiv.org/abs/2507.11959v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "pdf_url": "http://arxiv.org/pdf/2507.11959v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12002", "title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing", "authors": ["Alice Zhang", "Callihan Bertley", "Dawei Liang", "Edison Thomaz"], "categories": ["cs.LG", "I.2.0; J.4"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12002v1", "summary": "Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12002v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11990", "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation", "authors": ["Hyun-Jun Jin", "Young-Eun Kim", "Seong-Whan Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11990v1", "summary": "Recently, personalized portrait generation with a text-to-image diffusion\nmodel has significantly advanced with Textual Inversion, emerging as a\npromising approach for creating high-fidelity personalized images. Despite its\npotential, current Textual Inversion methods struggle to maintain consistent\nfacial identity due to semantic misalignments between textual and visual\nembedding spaces regarding identity. We introduce ID-EA, a novel framework that\nguides text embeddings to align with visual identity embeddings, thereby\nimproving identity preservation in a personalized generation. ID-EA comprises\ntwo key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned\nAdapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings\nwith a textual ID anchor, refining visual identity embeddings derived from a\nface recognition model using representative text embeddings. Then, the\nID-Adapter leverages the identity-enhanced embedding to adapt the text\ncondition, ensuring identity preservation by adjusting the cross-attention\nmodule in the pre-trained UNet model. This process encourages the text features\nto find the most related visual clues across the foreground snippets. Extensive\nquantitative and qualitative evaluations demonstrate that ID-EA substantially\noutperforms state-of-the-art methods in identity preservation metrics while\nachieving remarkable computational efficiency, generating personalized\nportraits approximately 15 times faster than existing approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11990v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.00406", "title": "Partnering with AI: A Pedagogical Feedback System for LLM Integration into Programming Education", "authors": ["Niklas Scholz", "Manh Hung Nguyen", "Adish Singla", "Tomohiro Nagashima"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      This is an extended version of a poster paper accepted and published at ECTEL-2025", "url": "http://arxiv.org/abs/2507.00406v2", "summary": "Feedback is one of the most crucial components to facilitate effective\nlearning. With the rise of large language models (LLMs) in recent years,\nresearch in programming education has increasingly focused on automated\nfeedback generation to help teachers provide timely support to every student.\nHowever, prior studies often overlook key pedagogical principles, such as\nmastery and progress adaptation, that shape effective feedback strategies. This\npaper introduces a novel pedagogical framework for LLM-driven feedback\ngeneration derived from established feedback models and local insights from\nsecondary school teachers. To evaluate this framework, we implemented a\nweb-based application for Python programming with LLM-based feedback that\nfollows the framework and conducted a mixed-method evaluation with eight\nsecondary-school computer science teachers. Our findings suggest that teachers\nconsider that, when aligned with the framework, LLMs can effectively support\nstudents and even outperform human teachers in certain scenarios through\ninstant and precise feedback. However, we also found several limitations, such\nas its inability to adapt feedback to dynamic classroom contexts. Such a\nlimitation highlights the need to complement LLM-generated feedback with human\nexpertise to ensure effective student learning. This work demonstrates an\neffective way to use LLMs for feedback while adhering to pedagogical standards\nand highlights important considerations for future systems.", "comment": "This is an extended version of a poster paper accepted and published\n  at ECTEL-2025", "pdf_url": "http://arxiv.org/pdf/2507.00406v2", "cate": "cs.CY", "date": "2025-07-01", "updated": "2025-07-16"}
{"id": "2507.11849", "title": "Mobility Extraction and Analysis of GaN HEMTs for RF Applications Using TCAD and Experimental Data", "authors": ["Tanjim Rahman"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages, 7 figures", "url": "http://arxiv.org/abs/2507.11849v1", "summary": "This paper presents an analysis of GaN high-electron-mobility transistors\n(HEMTs) using both TCAD simulation and experimental characterization. The\nenergy band structure was studied using Nextnano simulation software to observe\ntwo-dimensional electron gas (2DEG) formation and carrier confinement under\nequilibrium conditions. Additionally, I-V and C-V data from fabricated\nresearch-grade GaN HEMTs were analyzed to extract key electrical parameters.\nThe device demonstrated an ON current of 1.9 mA and an OFF current of 0.01 mA,\nindicating a strong ON/OFF current ratio. A subthreshold swing of 80 mV/decade\nand a DIBL of 5 mV/V were observed, confirming good gate control and\nshort-channel suppression. The ON-resistance was 22.72 ohm per micron, with a\nsaturation voltage of 1 V . The peak transconductance was extracted as 0.18 mS\nin the linear region and 0.5 mS in saturation. Field-effect mobility was\ncalculated using the transconductance method, with a maximum value of\napproximately 1200 cm2/V.s at low drain bias. The combined simulation and\nexperimental approach provided comprehensive insight into GaN HEMT behavior,\nenabling a deeper understanding of structure-performance relationships critical\nto advanced transistor design.", "comment": "5 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.11849v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2312.07003", "title": "RACER: Rational Artificial Intelligence Car-following-model Enhanced by Reality", "authors": ["Tianyi Li", "Alexander Halatsis", "Raphael Stern"], "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.07003v2", "summary": "This paper introduces RACER, the Rational Artificial Intelligence\nCar-following model Enhanced by Reality, a cutting-edge deep learning\ncar-following model, that satisfies partial derivative constraints, designed to\npredict Adaptive Cruise Control (ACC) driving behavior while staying\ntheoretically feasible. Unlike conventional models, RACER effectively\nintegrates Rational Driving Constraints (RDCs), crucial tenets of actual\ndriving, resulting in strikingly accurate and realistic predictions. Against\nestablished models like the Optimal Velocity Relative Velocity (OVRV), a\ncar-following Neural Network (NN), and a car-following Physics-Informed Neural\nNetwork (PINN), RACER excels across key metrics, such as acceleration,\nvelocity, and spacing. Notably, it displays a perfect adherence to the RDCs,\nregistering zero violations, in stark contrast to other models. This study\nhighlights the immense value of incorporating physical constraints within AI\nmodels, especially for augmenting safety measures in transportation. It also\npaves the way for future research to test these models against human driving\ndata, with the potential to guide safer and more rational driving behavior. The\nversatility of the proposed model, including its potential to incorporate\nadditional derivative constraints and broader architectural applications,\nenhances its appeal and broadens its impact within the scientific community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.07003v2", "cate": "cs.AI", "date": "2023-12-12", "updated": "2025-07-16"}
{"id": "2507.11987", "title": "Formal Verification of Neural Certificates Done Dynamically", "authors": ["Thomas A. Henzinger", "Konstantin Kueffner", "Emily Yu"], "categories": ["cs.SC", "cs.AI"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "Comments:      Accepted at RV'25", "url": "http://arxiv.org/abs/2507.11987v1", "summary": "Neural certificates have emerged as a powerful tool in cyber-physical systems\ncontrol, providing witnesses of correctness. These certificates, such as\nbarrier functions, often learned alongside control policies, once verified,\nserve as mathematical proofs of system safety. However, traditional formal\nverification of their defining conditions typically faces scalability\nchallenges due to exhaustive state-space exploration. To address this\nchallenge, we propose a lightweight runtime monitoring framework that\nintegrates real-time verification and does not require access to the underlying\ncontrol policy. Our monitor observes the system during deployment and performs\non-the-fly verification of the certificate over a lookahead region to ensure\nsafety within a finite prediction horizon. We instantiate this framework for\nReLU-based control barrier functions and demonstrate its practical\neffectiveness in a case study. Our approach enables timely detection of safety\nviolations and incorrect certificates with minimal overhead, providing an\neffective but lightweight alternative to the static verification of the\ncertificates.", "comment": "Accepted at RV'25", "pdf_url": "http://arxiv.org/pdf/2507.11987v1", "cate": "cs.SC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12011", "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning", "authors": ["Yao Lu", "Hongyu Gao", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12011v1", "summary": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12011v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11994", "title": "SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation", "authors": ["Jun Yin", "Fei Wu", "Yupeng Ren", "Jisheng Huang", "Qiankun Li", "Heng jin", "Jianhai Fu", "Chanjie Cui"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IGARSS2025 accepted, Correspondence: fujianhai2024@gmail.com (J.F.), cuichj@mail2. this http URL (C.C.)", "url": "http://arxiv.org/abs/2507.11994v1", "summary": "Public remote sensing datasets often face limitations in universality due to\nresolution variability and inconsistent land cover category definitions. To\nharness the vast pool of unlabeled remote sensing data, we propose SAMST, a\nsemi-supervised semantic segmentation method. SAMST leverages the strengths of\nthe Segment Anything Model (SAM) in zero-shot generalization and boundary\ndetection. SAMST iteratively refines pseudo-labels through two main components:\nsupervised model self-training using both labeled and pseudo-labeled data, and\na SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three\nmodules: a Threshold Filter Module for preprocessing, a Prompt Generation\nModule for extracting connected regions and generating prompts for SAM, and a\nLabel Refinement Module for final label stitching. By integrating the\ngeneralization power of large models with the training efficiency of small\nmodels, SAMST improves pseudo-label accuracy, thereby enhancing overall model\nperformance. Experiments on the Potsdam dataset validate the effectiveness and\nfeasibility of SAMST, demonstrating its potential to address the challenges\nposed by limited labeled data in remote sensing semantic segmentation.", "comment": "IGARSS2025 accepted, Correspondence: fujianhai2024@gmail.com (J.F.),\n  cuichj@mail2.sysu.edu.cn (C.C.)", "pdf_url": "http://arxiv.org/pdf/2507.11994v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10559", "title": "NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research", "authors": ["Shomir Wilson"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.10559v2", "summary": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.10559v2", "cate": "cs.CY", "date": "2025-07-02", "updated": "2025-07-16"}
{"id": "2507.11872", "title": "Algorithm Design and Comparative Test of Natural Gradient Gaussian Approximation Filter", "authors": ["Wenhan Cao", "Tianyi Zhang", "Shengbo Eben Li"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11872v1", "summary": "Popular Bayes filters typically rely on linearization techniques such as\nTaylor series expansion and stochastic linear regression to use the structure\nof standard Kalman filter. These techniques may introduce large estimation\nerrors in nonlinear and non-Gaussian systems. This paper overviews a recent\nbreakthrough in filtering algorithm design called \\textit{N}atural\nGr\\textit{a}dient Gaussia\\textit{n} Appr\\textit{o}ximation (NANO) filter and\ncompare its performance over a large class of nonlinear filters. The NANO\nfilter interprets Bayesian filtering as solutions to two distinct optimization\nproblems, which allows to define optimal Gaussian approximation and derive its\ncorresponding extremum conditions. The algorithm design still follows the\ntwo-step structure of Bayes filters. In the prediction step, NANO filter\ncalculates the first two moments of the prior distribution, and this process is\nequivalent to a moment-matching filter. In the update step, natural gradient\ndescent is employed to directly minimize the objective of the update step,\nthereby avoiding errors caused by model linearization. Comparative tests are\nconducted on four classic systems, including the damped linear oscillator,\nsequence forecasting, modified growth model, and robot localization, under\nGaussian, Laplace, and Beta noise to evaluate the NANO filter's capability in\nhandling nonlinearity. Additionally, we validate the NANO filter's robustness\nto data outliers using a satellite attitude estimation example. It is observed\nthat the NANO filter outperforms popular Kalman filters family such as extended\nKalman filter (EKF), unscented Kalman filter (UKF), iterated extended Kalman\nfilter (IEKF) and posterior linearization filter (PLF), while having similar\ncomputational burden.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11872v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11846", "title": "Directional Measurements and Analysis for FR3 Low-Altitude Channels in a Campus Environment", "authors": ["Yulu Guo", "Tongjia Zhang", "Xiangwen Gu", "Shu Sun", "Meixia Tao", "Ruifeng Gao"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11846v1", "summary": "In this paper, we present detailed low-altitude channel measurements at the\nFR3 band in an outdoor campus environment. Using a time-domain channel sounder\nsystem, we conduct two types of measurements: path loss measurements by moving\nthe transmitter (Tx) at one-meter intervals along a 26-point rooftop path, and\ndirectional power angular spectrum measurements through antenna scanning at\nhalf-power beam width intervals. The path loss analysis across different Rx\nshows that the close-in model outperforms conventional 3GPP models and\nheight-corrected variants, with path loss exponents close to free space values\nindicating line-of-sight dominance. The power angular spectrum measurements\nshow that propagation behavior varies significantly with environmental\nconditions. Closer Rx exhibit stronger sensitivity to ground reflections during\ndownward Tx tilting, while obstructed links display uniform angular\ncharacteristics due to dominant scattering effects, and corridor environments\nproduce asymmetric power distributions. These results indicate that\nlow-altitude propagation is characterized by complex interactions between Tx\nheight and ground scattering mechanisms, providing fundamental insights for\nchannel modeling in emerging mid-band communication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11846v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2404.06050", "title": "Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes", "authors": ["Tianchen Deng", "Nailin Wang", "Chongdi Wang", "Shenghai Yuan", "Jingchuan Wang", "Hesheng Wang", "Danwei Wang", "Weidong Chen"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.06050v3", "summary": "Dense scene reconstruction for photo-realistic view synthesis has various\napplications, such as VR/AR, autonomous vehicles. However, most existing\nmethods have difficulties in large-scale scenes due to three core challenges:\n\\textit{(a) inaccurate depth input.} Accurate depth input is impossible to get\nin real-world large-scale scenes. \\textit{(b) inaccurate pose estimation.} Most\nexisting approaches rely on accurate pre-estimated camera poses. \\textit{(c)\ninsufficient scene representation capability.} A single global radiance field\nlacks the capacity to effectively scale to large-scale scenes. To this end, we\npropose an incremental joint learning framework, which can achieve accurate\ndepth, pose estimation, and large-scale scene reconstruction. A vision\ntransformer-based network is adopted as the backbone to enhance performance in\nscale information estimation. For pose estimation, a feature-metric bundle\nadjustment (FBA) method is designed for accurate and robust camera tracking in\nlarge-scale scenes. In terms of implicit scene representation, we propose an\nincremental scene representation method to construct the entire large-scale\nscene as multiple local radiance fields to enhance the scalability of 3D scene\nrepresentation. Extended experiments have been conducted to demonstrate the\neffectiveness and accuracy of our method in depth estimation, pose estimation,\nand large-scale scene reconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.06050v3", "cate": "cs.CV", "date": "2024-04-09", "updated": "2025-07-16"}
{"id": "2507.12006", "title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "authors": ["Linwei Chen", "Lin Gu", "Ying Fu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.12006v1", "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\n\\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12006v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12041", "title": "Granular feedback merits sophisticated aggregation", "authors": ["Anmol Kagrecha", "Henrik Marklund", "Potsawee Manakul", "Richard Zeckhauser", "Benjamin Van Roy"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages, 8 figures", "url": "http://arxiv.org/abs/2507.12041v1", "summary": "Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.", "comment": "31 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.12041v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12001", "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation", "authors": ["Hao Li", "Ju Dai", "Feng Zhou", "Kaida Ning", "Lei Li", "Junjun Pan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.12001v1", "summary": "While 3D facial animation has made impressive progress, challenges still\nexist in realizing fine-grained stylized 3D facial expression manipulation due\nto the lack of appropriate datasets. In this paper, we introduce the\nAUBlendSet, a 3D facial dataset based on AU-Blendshape representation for\nfine-grained facial expression manipulation across identities. AUBlendSet is a\nblendshape data collection based on 32 standard facial action units (AUs)\nacross 500 identities, along with an additional set of facial postures\nannotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to\nlearn AU-Blendshape basis vectors for different character styles. AUBlendNet\npredicts, in parallel, the AU-Blendshape basis vectors of the corresponding\nstyle for a given identity mesh, thereby achieving stylized 3D emotional facial\nmanipulation. We comprehensively validate the effectiveness of AUBlendSet and\nAUBlendNet through tasks such as stylized facial expression manipulation,\nspeech-driven emotional facial animation, and emotion recognition data\naugmentation. Through a series of qualitative and quantitative experiments, we\ndemonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D\nfacial animation tasks. To the best of our knowledge, AUBlendSet is the first\ndataset, and AUBlendNet is the first network for continuous 3D facial\nexpression manipulation for any identity through facial AUs. Our source code is\navailable at https://github.com/wslh852/AUBlendNet.git.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12001v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2403.08802", "title": "Governance of Generative Artificial Intelligence for Companies", "authors": ["Johannes Schneider", "Pauline Kuss", "Rene Abraham", "Christian Meske"], "categories": ["cs.AI", "cs.CY", "cs.LG", "I.2.m"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This paper is under submission", "url": "http://arxiv.org/abs/2403.08802v4", "summary": "Generative Artificial Intelligence (GenAI), specifically large language\nmodels(LLMs) like ChatGPT, has swiftly entered organizations without adequate\ngovernance, posing both opportunities and risks. Despite extensive debates on\nGenAI's transformative nature and regulatory measures, limited research\naddresses organizational governance, encompassing technical and business\nperspectives. Although numerous frameworks for governance of AI exist, it is\nnot clear to what extent they apply to GenAI. Our review paper fills this gap\nby surveying recent works with the purpose of better understanding fundamental\ncharacteristics of GenAI and adjusting prior frameworks specifically towards\nGenAI governance within companies. To do so, it extends Nickerson's framework\ndevelopment processes to include prior conceptualizations. Our framework\noutlines the scope, objectives, and governance mechanisms tailored to harness\nbusiness opportunities as well as mitigate risks associated with GenAI\nintegration. Our research contributes a focused approach to GenAI governance,\noffering practical insights for companies navigating the challenges of GenAI\nadoption and highlighting research gaps.", "comment": "This paper is under submission", "pdf_url": "http://arxiv.org/pdf/2403.08802v4", "cate": "cs.AI", "date": "2024-02-05", "updated": "2025-07-16"}
{"id": "2507.11924", "title": "Advantages of Feedback in Distributed Data-Gathering for Accurate and Power-Efficient State-Estimation", "authors": ["Hyeongmin Choe", "Soojean Han"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Archived version. Related work under further development", "url": "http://arxiv.org/abs/2507.11924v1", "summary": "In distributed target-tracking sensor networks, efficient data gathering\nmethods are necessary to save communication resources and assure information\naccuracy. This paper proposes a Feedback (FB) distributed data-gathering method\nwhich lets the central unit feed information back to the mobile sensors; each\nsensor then uses it to cancel redundant transmissions and reduce communication\ncongestion. We rigorously compare its performance, in terms of mean-squared\nerror (MSE) and cost of power per sensor, against more conventional\nNon-Feedback (NF) architectures by evaluating conditions of feasibility and\nadvantage under different architecture specifications (e.g., communication\ndelay rate, power cost rate, maximum back-off time, sampling period,\nobservation noise). Here, we defined the advantage as the performance gain\nachieved by FB over NF, while FB is said to be feasible if the advantage region\nis nonempty. Our theoretical analyses show that the feasibility of FB depends\nmore on the communication power cost, while the advantage depends on the\nsensors' propagation delay per transmission interval; we derive concrete\nconditions under which these outcomes hold. Using extensive numerical\nsimulations under a variety of settings, we confirm the accuracy of the derived\nconditions, and show that our theoretical results hold even for more complex\nscenarios where the simplifying assumptions no longer hold.", "comment": "Archived version. Related work under further development", "pdf_url": "http://arxiv.org/pdf/2507.11924v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11912", "title": "Joint UAV Placement and Transceiver Design in Multi-User Wireless Relay Networks", "authors": ["Tzu-Hsuan Chou", "Nicolo Michelusi", "David J. Love", "James V. Krogmeier"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This paper is accepted for publication in IEEE Transactions on Communications. 17 pages", "url": "http://arxiv.org/abs/2507.11912v1", "summary": "In this paper, a novel approach is proposed to improve the minimum\nsignal-to-interference-plus-noise-ratio (SINR) among users in non-orthogonal\nmulti-user wireless relay networks, by optimizing the placement of unmanned\naerial vehicle (UAV) relays, relay beamforming, and receive combining. The\ndesign is separated into two problems: beamforming-aware UAV placement\noptimization and transceiver design for minimum SINR maximization. A\nsignificant challenge in beamforming-aware UAV placement optimization is the\nlack of instantaneous channel state information (CSI) prior to deploying UAV\nrelays, making it difficult to derive the beamforming SINR in non-orthogonal\nmulti-user transmission. To address this issue, an approximation of the\nexpected beamforming SINR is derived using the narrow beam property of a\nmassive MIMO base station. Based on this, a UAV placement algorithm is proposed\nto provide UAV positions that improve the minimum expected beamforming SINR\namong users, using a difference-of-convex framework. Subsequently, after\ndeploying the UAV relays to the optimized positions, and with estimated CSI\navailable, a joint relay beamforming and receive combining (JRBC) algorithm is\nproposed to optimize the transceiver to improve the minimum beamforming SINR\namong users, using a block-coordinate descent approach. Numerical results show\nthat the UAV placement algorithm combined with the JRBC algorithm provides a\n4.6 dB SINR improvement over state-of-the-art schemes.", "comment": "This paper is accepted for publication in IEEE Transactions on\n  Communications. 17 pages", "pdf_url": "http://arxiv.org/pdf/2507.11912v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2409.15615", "title": "KISS-Matcher: Fast and Robust Point Cloud Registration Revisited", "authors": ["Hyungtae Lim", "Daebeom Kim", "Gunhee Shin", "Jingnan Shi", "Ignacio Vizzo", "Hyun Myung", "Jaesik Park", "Luca Carlone"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 9 figures", "url": "http://arxiv.org/abs/2409.15615v3", "summary": "While global point cloud registration systems have advanced significantly in\nall aspects, many studies have focused on specific components, such as feature\nextraction, graph-theoretic pruning, or pose solvers. In this paper, we take a\nholistic view on the registration problem and develop an open-source and\nversatile C++ library for point cloud registration, called KISS-Matcher.\nKISS-Matcher combines a novel feature detector, Faster-PFH, that improves over\nthe classical fast point feature histogram (FPFH). Moreover, it adopts a\n$k$-core-based graph-theoretic pruning to reduce the time complexity of\nrejecting outlier correspondences. Finally, it combines these modules in a\ncomplete, user-friendly, and ready-to-use pipeline. As verified by extensive\nexperiments, KISS-Matcher has superior scalability and broad applicability,\nachieving a substantial speed-up compared to state-of-the-art outlier-robust\nregistration pipelines while preserving accuracy. Our code will be available at\nhttps://github.com/MIT-SPARK/KISS-Matcher.", "comment": "9 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2409.15615v3", "cate": "cs.CV", "date": "2024-09-23", "updated": "2025-07-16"}
{"id": "2507.12008", "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation", "authors": ["Jiawen Wang", "Yinda Chen", "Xiaoyu Liu", "Che Liu", "Dong Liu", "Jianqing Gao", "Zhiwei Xiong"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICML 2025", "url": "http://arxiv.org/abs/2507.12008v1", "summary": "Recent works have correlated Masked Image Modeling (MIM) with consistency\nregularization in Unsupervised Domain Adaptation (UDA). However, they merely\ntreat masking as a special form of deformation on the input images and neglect\nthe theoretical analysis, which leads to a superficial understanding of masked\nreconstruction and insufficient exploitation of its potential in enhancing\nfeature extraction and representation learning. In this paper, we reframe\nmasked reconstruction as a sparse signal reconstruction problem and\ntheoretically prove that the dual form of complementary masks possesses\nsuperior capabilities in extracting domain-agnostic image features. Based on\nthis compelling insight, we propose MaskTwins, a simple yet effective UDA\nframework that integrates masked reconstruction directly into the main training\npipeline. MaskTwins uncovers intrinsic structural patterns that persist across\ndisparate domains by enforcing consistency between predictions of images masked\nin complementary ways, enabling domain generalization in an end-to-end manner.\nExtensive experiments verify the superiority of MaskTwins over baseline methods\nin natural and biological image segmentation. These results demonstrate the\nsignificant advantages of MaskTwins in extracting domain-invariant features\nwithout the need for separate pre-training, offering a new paradigm for\ndomain-adaptive segmentation.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.12008v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12043", "title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning", "authors": ["Wen Wen", "Tieliang Gong", "Yunjiao Zhang", "Zeyu Gao", "Weizhan Zhang", "Yong-Jin Liu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12043v1", "summary": "Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12043v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12012", "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease", "authors": ["Matthias Perkonigg", "Nina Bastati", "Ahmed Ba-Ssalamah", "Peter Mesenbrink", "Alexander Goehler", "Miljen Martic", "Xiaofei Zhou", "Michael Trauner", "Georg Langs"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12012v1", "summary": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12012v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2502.15873", "title": "Practical Principles for AI Cost and Compute Accounting", "authors": ["Stephen Casper", "Luke Bailey", "Tim Schreier"], "categories": ["cs.AI", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.15873v3", "summary": "Policymakers increasingly use development cost and compute as proxies for AI\ncapabilities and risks. Recent laws have introduced regulatory requirements\nthat are contingent on specific thresholds. However, technical ambiguities in\nhow to perform this accounting create loopholes that can undermine regulatory\neffectiveness. We propose seven principles for designing AI cost and compute\naccounting standards that (1) reduce opportunities for strategic gaming, (2)\navoid disincentivizing responsible risk mitigation, and (3) enable consistent\nimplementation across companies and jurisdictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.15873v3", "cate": "cs.AI", "date": "2025-02-21", "updated": "2025-07-16"}
{"id": "2507.12031", "title": "Towards Ultra-Reliable 6G in-X Subnetworks: Dynamic Link Adaptation by Deep Reinforcement Learning", "authors": ["Fateme Salehi", "Aamir Mahmood", "Sarder Fakhrul Abedin", "Kyi Thar", "Mikael Gidlund"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12031v1", "summary": "6G networks are composed of subnetworks expected to meet ultra-reliable\nlow-latency communication (URLLC) requirements for mission-critical\napplications such as industrial control and automation. An often-ignored aspect\nin URLLC is consecutive packet outages, which can destabilize control loops and\ncompromise safety in in-factory environments. Hence, the current work proposes\na link adaptation framework to support extreme reliability requirements using\nthe soft actor-critic (SAC)-based deep reinforcement learning (DRL) algorithm\nthat jointly optimizes energy efficiency (EE) and reliability under dynamic\nchannel and interference conditions. Unlike prior work focusing on average\nreliability, our method explicitly targets reducing burst/consecutive outages\nthrough adaptive control of transmit power and blocklength based solely on the\nobserved signal-to-interference-plus-noise ratio (SINR). The joint optimization\nproblem is formulated under finite blocklength and quality of service\nconstraints, balancing reliability and EE. Simulation results show that the\nproposed method significantly outperforms the baseline algorithms, reducing\noutage bursts while consuming only 18\\% of the transmission cost required by a\nfull/maximum resource allocation policy in the evaluated scenario. The\nframework also supports flexible trade-off tuning between EE and reliability by\nadjusting reward weights, making it adaptable to diverse industrial\nrequirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12031v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11913", "title": "Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission", "authors": ["Chen Zhu", "Siyun Liang", "Zhouxiang Zhao", "Jianrong Bao", "Zhaohui Yang", "Zhaoyang Zhang", "Dusit Niyato"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11913v1", "summary": "Semantic communication emphasizes the transmission of meaning rather than raw\nsymbols. It offers a promising solution to alleviate network congestion and\nimprove transmission efficiency. In this paper, we propose a wireless image\ncommunication framework that employs probability graphs as shared semantic\nknowledge base among distributed users. High-level image semantics are\nrepresented via scene graphs, and a two-stage compression algorithm is devised\nto remove predictable components based on learned conditional and co-occurrence\nprobabilities. At the transmitter, the algorithm filters redundant relations\nand entity pairs, while at the receiver, semantic recovery leverages the same\nprobability graphs to reconstruct omitted information. For further research, we\nalso put forward a multi-round semantic compression algorithm with its\ntheoretical performance analysis. Simulation results demonstrate that our\nsemantic-aware scheme achieves superior transmission throughput and satiable\nsemantic alignment, validating the efficacy of leveraging high-level semantics\nfor image communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11913v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2503.06138", "title": "System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems", "authors": ["Tadahiro Taniguchi", "Yasushi Hirai", "Masahiro Suzuki", "Shingo Murata", "Takato Horii", "Kazutoshi Tanaka"], "categories": ["cs.AI", "cs.RO", "q-bio.NC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2503.06138v3", "summary": "This paper introduces the System 0/1/2/3 framework as an extension of\ndual-process theory, employing a quad-process model of cognition. Expanding\nupon System 1 (fast, intuitive thinking) and System 2 (slow, deliberative\nthinking), we incorporate System 0, which represents pre-cognitive embodied\nprocesses, and System 3, which encompasses collective intelligence and symbol\nemergence. We contextualize this model within Bergson's philosophy by adopting\nmulti-scale time theory to unify the diverse temporal dynamics of cognition.\nSystem 0 emphasizes morphological computation and passive dynamics,\nillustrating how physical embodiment enables adaptive behavior without explicit\nneural processing. Systems 1 and 2 are explained from a constructive\nperspective, incorporating neurodynamical and AI viewpoints. In System 3, we\nintroduce collective predictive coding to explain how societal-level adaptation\nand symbol emergence operate over extended timescales. This comprehensive\nframework ranges from rapid embodied reactions to slow-evolving collective\nintelligence, offering a unified perspective on cognition across multiple\ntimescales, levels of abstraction, and forms of human intelligence. The System\n0/1/2/3 model provides a novel theoretical foundation for understanding the\ninterplay between adaptive and cognitive processes, thereby opening new avenues\nfor research in cognitive science, AI, robotics, and collective intelligence.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2503.06138v3", "cate": "cs.AI", "date": "2025-03-08", "updated": "2025-07-16"}
{"id": "2507.12017", "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection", "authors": ["Xiwei Zhang", "Chunjin Yang", "Yiming Xiao", "Runtong Zhang", "Fanman Meng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 main-pages, 3 reference-pages, 5 figures, 6 tables", "url": "http://arxiv.org/abs/2507.12017v1", "summary": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.", "comment": "8 main-pages, 3 reference-pages, 5 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.12017v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12053", "title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling", "authors": ["Seanglidet Yean", "Jiazu Zhou", "Bu-Sung Lee", "Markus Schläpfer"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      International Conference on Intelligent Digitization of Systems and Services, Valencia, Spain, 2025 (IDSS 2025)", "url": "http://arxiv.org/abs/2507.12053v1", "summary": "The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.", "comment": "International Conference on Intelligent Digitization of Systems and\n  Services, Valencia, Spain, 2025 (IDSS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.12053v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12022", "title": "Dataset Ownership Verification for Pre-trained Masked Models", "authors": ["Yuechen Xie", "Jie Song", "Yicheng Shan", "Xiaoyan Zhang", "Yuanyu Wan", "Shengxuming Zhang", "Jiarui Duan", "Mingli Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.12022v1", "summary": "High-quality open-source datasets have emerged as a pivotal catalyst driving\nthe swift advancement of deep learning, while facing the looming threat of\npotential exploitation. Protecting these datasets is of paramount importance\nfor the interests of their owners. The verification of dataset ownership has\nevolved into a crucial approach in this domain; however, existing verification\ntechniques are predominantly tailored to supervised models and contrastive\npre-trained models, rendering them ill-suited for direct application to the\nincreasingly prevalent masked models. In this work, we introduce the inaugural\nmethodology addressing this critical, yet unresolved challenge, termed Dataset\nOwnership Verification for Masked Modeling (DOV4MM). The central objective is\nto ascertain whether a suspicious black-box model has been pre-trained on a\nparticular unlabeled dataset, thereby assisting dataset owners in safeguarding\ntheir rights. DOV4MM is grounded in our empirical observation that when a model\nis pre-trained on the target dataset, the difficulty of reconstructing masked\ninformation within the embedding space exhibits a marked contrast to models not\npre-trained on that dataset. We validated the efficacy of DOV4MM through ten\nmasked image models on ImageNet-1K and four masked language models on\nWikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,\nwith a $p$-value considerably below 0.05, surpassing all prior approaches. Code\nis available at https://github.com/xieyc99/DOV4MM.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12022v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10577", "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "authors": ["Cécile Logé", "Rehan Ghori"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10577v2", "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10577v2", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-16"}
{"id": "2507.12052", "title": "Distributed Resilient State Estimation and Control with Strategically Implemented Security Measures", "authors": ["Takumi Shinohara", "Karl H. Johansson", "Henrik Sandberg"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12052v1", "summary": "This paper addresses the problem of distributed resilient state estimation\nand control for linear time-invariant systems in the presence of malicious\nfalse data injection sensor attacks and bounded noise. We consider a system\noperator (defender) capable of deploying cybersecurity measures to counteract\nthe sensor compromises. Although such measures enhance resilience against\nadversarial attacks, they may incur substantial costs; hence, it is crucial to\nselect countermeasures to balance resilience gains and cost efficiency\nstrategically. We first demonstrate that the system's resilience against\nattacks is maximized through the appropriate implementation of security\nmeasures, implying that no attacker can execute undetectable sensor attacks.\nBuilding on this analysis, we propose an algorithm that identifies the optimal\nsecurity measure. While determining this measure is NP-hard in general, we also\nderive sufficient conditions under which efficient computation is feasible.\nFurthermore, we develop a distributed resilient state estimation and control\nscheme informed by the optimal security measure and establish conditions that\nguarantee bounded estimation and control errors. Finally, we validate the\nefficacy of our approach via numerical simulations of a vehicle platooning\nscenario.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12052v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11919", "title": "STFT-based Time-Frequency Mode Decomposition: A Fast and Robust Method for Multicomponent Signal Analysis", "authors": ["Wei Zhou", "Wei-Jian Li", "Wei-Xin Ren"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11919v1", "summary": "The decomposition of complex, multicomponent, and non-stationary signals into\ntheir constituent modes is a fundamental yet significant challenge in science\nand engineering. Existing methods often struggle with a trade-off among\naccuracy, computational cost, and the need for prior information such as the\nnumber of modes. This paper introduces time-frequency mode decomposition\n(TFMD), a novel framework for the fast, robust, and adaptive decomposition of\nsuch signals. TFMD operates on the principle that modes form contiguous\nhigh-energy regions in the time-frequency domain. Its non-iterative pipeline\nreframes signal decomposition as an image segmentation task: a signal is\ntransformed into a spectrogram, which is then smoothed to enhance the\ncontinuity of these high-energy regions. A sequence of adaptive thresholding\nand connected-component labeling with size-based filtering is then employed to\nautomatically segment the spectrogram and generate a mask for each mode. The\nmodes are finally reconstructed via the inverse short-time Fourier transform.\nValidation on diverse synthetic signals demonstrates that TFMD accurately\ndetermines the number of modes and reconstructs them with high fidelity. Its\nperformance is particularly strong in high-noise conditions. A comparative\nanalysis confirms that TFMD provides robust, competitive performance across a\nwider variety of signal types, while a theoretical complexity analysis reveals\nits superior computational efficiency stemming from its non-iterative design.\nThe method's practical utility is further demonstrated by successfully\nextracting modal responses from a real-world footbridge vibration signal. TFMD\nprovides a computationally efficient and powerful paradigm for multicomponent\nsignal analysis, offering a compelling balance of accuracy, versatility, and\nefficiency for large-scale or time-sensitive applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11919v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.17791", "title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", "authors": ["Tetiana Martyniuk", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Raoul de Charette"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE IV 2025 (Oral); v2 - updated quantitative results based on the metrics (Voxel IoU) calculation code corrections", "url": "http://arxiv.org/abs/2504.17791v2", "summary": "Training diffusion models that work directly on lidar points at the scale of\noutdoor scenes is challenging due to the difficulty of generating fine-grained\ndetails from white noise over a broad field of view. The latest works\naddressing scene completion with diffusion models tackle this problem by\nreformulating the original DDPM as a local diffusion process. It contrasts with\nthe common practice of operating at the level of objects, where vanilla DDPMs\nare currently used. In this work, we close the gap between these two lines of\nwork. We identify approximations in the local diffusion formulation, show that\nthey are not required to operate at the scene level, and that a vanilla DDPM\nwith a well-chosen starting point is enough for completion. Finally, we\ndemonstrate that our method, LiDPM, leads to better results in scene completion\non SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .", "comment": "Accepted to IEEE IV 2025 (Oral); v2 - updated quantitative results\n  based on the metrics (Voxel IoU) calculation code corrections", "pdf_url": "http://arxiv.org/pdf/2504.17791v2", "cate": "cs.CV", "date": "2025-04-24", "updated": "2025-07-16"}
{"id": "2507.12029", "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "authors": ["Xinhang Wan", "Jiyuan Liu", "Qian Qu", "Suyuan Liu", "Chuyu Zhang", "Fangdi Wang", "Xinwang Liu", "En Zhu", "Kunlun He"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12029v1", "summary": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12029v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12070", "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "authors": ["George Bird"], "categories": ["cs.LG", "I.5.1; F.1.1; I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      36 pages, 31 figures", "url": "http://arxiv.org/abs/2507.12070v1", "summary": "This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.", "comment": "36 pages, 31 figures", "pdf_url": "http://arxiv.org/pdf/2507.12070v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12023", "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model", "authors": ["Xu Fan", "Zhihao Wang", "Yuetan Lin", "Yan Zhang", "Yang Xiang", "Hao Li"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12023v1", "summary": "Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12023v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12082", "title": "Inductance Estimation for High-Power Multilayer Rectangle Planar Windings", "authors": ["Theofilos Papadopoulos", "Antonios Antonopoulos"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      7 pages, 7 figures, IEEE Journal of Emerging and Selected Topics in Industrial Electronics, 2025", "url": "http://arxiv.org/abs/2507.12082v1", "summary": "This paper proposes a simple and accurate monomial-like equation for\nestimating the inductance of Multilayer Rectangle-shaped Planar Windings\n(MLRPWs) for high-frequency, high-power applications. The equation consists of\nthe power product of the geometrical dimensions, raised at individual power\ncoefficients. The coefficients are generated via Multiple Linear Regression\n(MLR), based on a large set of approximately 6,000 simulated windings, with an\n80/20 training/evaluation sample ratio. The resulting mean error value is 0%,\nwith a standard deviation below 1.8%. The accuracy of the inductance estimation\nis confirmed on several experimental samples, with dimensions both within and\noutside the initial training dataset.", "comment": "7 pages, 7 figures, IEEE Journal of Emerging and Selected Topics in\n  Industrial Electronics, 2025", "pdf_url": "http://arxiv.org/pdf/2507.12082v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12000", "title": "DSSD: Efficient Edge-Device Deployment and Collaborative Inference via Distributed Split Speculative Decoding", "authors": ["Jiahong Ning", "Ce Zheng", "Tingting Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2507.12000v1", "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.12000v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.00512", "title": "InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method", "authors": ["Nguyen Hoang Khoi Tran", "Julie Stephany Berrio", "Mao Shan", "Zhenxing Ming", "Stewart Worrall"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00512v3", "summary": "Online localization of road intersections is beneficial for autonomous\nvehicle localization, mapping and motion planning. Intersections offer strong\nlandmarks for correcting vehicle pose estimation, anchoring new sensor data in\nup-to-date maps, and guiding vehicle routing in road network graphs. Despite\nthis importance, intersection localization has not been widely studied, with\nexisting methods either ignoring the rich semantic information already computed\nonboard or relying on scarce, hand-labeled intersection datasets. To close this\ngap, we present a novel LiDAR-based method for online vehicle-centric\nintersection localization. We detect the intersection candidates in a bird's\neye view (BEV) representation formed by concatenating a sequence of semantic\nroad scans. We then refine these candidates by analyzing the intersecting road\nbranches and adjusting the intersection center point in a least-squares\nformulation. For evaluation, we introduce an automated pipeline that pairs\nlocalized intersection points with OpenStreetMap (OSM) intersection nodes using\nprecise GNSS/INS ground-truth poses. Experiments on the SemanticKITTI dataset\nshow that our method outperforms the latest learning-based baseline in accuracy\nand reliability. Sensitivity tests demonstrate the method's robustness to\nchallenging segmentation errors, highlighting its applicability in the real\nworld.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00512v3", "cate": "cs.CV", "date": "2025-05-01", "updated": "2025-07-16"}
{"id": "2507.12060", "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing", "authors": ["Kun-Hsiang Lin", "Yu-Wen Tseng", "Kang-Yang Huang", "Jhih-Ciang Wu", "Wen-Huang Cheng"], "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by MM'25", "url": "http://arxiv.org/abs/2507.12060v1", "summary": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.", "comment": "Accepted by MM'25", "pdf_url": "http://arxiv.org/pdf/2507.12060v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12094", "title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors", "authors": ["Yiding Feng", "Wei Tang"], "categories": ["cs.LG", "cs.GT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12094v1", "summary": "In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12094v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12026", "title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering", "authors": ["Rongtao Xu", "Han Gao", "Mingming Yu", "Dong An", "Shunpeng Chen", "Changwei Wang", "Li Guo", "Xiaodan Liang", "Shibiao Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.12026v1", "summary": "With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.12026v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12163", "title": "Integrated Switched Capacitor Array and Synchronous Charge Extraction with Adaptive Hybrid MPPT for Piezoelectric Harvesters", "authors": ["Pramit Karmakar", "Siddharth B", "Chinmay Murlidhar Kadnur Rao"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12163v1", "summary": "Energy Harvesting technologies will play a fundamental role in the\ndevelopment of the next generation of electronic systems as well as in\nadvancing the development of sustainable infrastructure. One of the critical\nchallenges in EH is utilizing ambient vibrations to harvest energy. Piezo\nEnergy Harvesting, which uses ambient vibrations, is a promising technology in\nenergy harvesting and a self-powered technology. However, it suffers from\nseveral practical challenges. Some of these challenges include narrow\nbandwidth, non-linearity, and impedance mismatch, among others. This paper\npresents a novel, simulated Piezo Energy Harvesting (PEH) framework that\naddresses some of these challenges. The proposed model is designed to be\nadaptive and effective against the inherent non-linearity of PEH. This detailed\nmodel covers a non-linear piezo, Synchronous Electric Charge Extraction (SECE),\nHybrid Maximum Power Point Tracking (MPPT) and a Switched Capacitor Array\n(SCA). The SECE extracts the maximum charge accumulated on the piezo every time\nthe piezo reaches the mechanical extremum. The Bouc-Wen model has been used to\nestablish nonlinearity in the system. The hybrid MPPT exhibits significant\nimprovement over conventional P&O, while the SCA-tuned system demonstrates\nresilience against variable frequency input.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12163v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12010", "title": "Enhancing Situational Awareness in ISAC Networks via Drone Swarms: A Real-World Channel Sounding Data Set", "authors": ["Julia Beuster", "Carsten Andrich", "Sebastian Giehl", "Marc Miranda", "Lorenz Mohr", "Dieter Novotny", "Tom Kaufmann", "Christian Schneider", "Reiner Thomä"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12010v1", "summary": "With the upcoming capabilities of integrated sensing and communication (ISAC)\nand the incorporation of user equipment (UE) like unmanned aerial vehicles\n(UAVs) in 6G mobile networks, there is a significant opportunity to enhance\nsituational awareness through multi-static radar sensing in meshed ISAC\nnetworks. This paper presents a real-world channel sounding data set acquired\nusing a testbed with synchronized, distributed ground-based sensor nodes and\nflying sensor nodes within a swarm of up to four drones. The conducted\nmeasurement campaign is designed to sense the bi-static reflectivity of objects\nsuch as parking cars, vertical take-off and landing (VTOL) aircraft, and small\ndrones in multi-path environments. We detail the rationale behind the selection\nof the included scenarios and the configuration of the participating nodesand\npresent exemplary results to demonstrate the potential of using collaborating\ndrone swarms for multi-static radar tracking and localization in air-to-air\n(A2A) and air-to-ground (A2G) scenarios. The data sets are publicly available\nto support the development and validation of future ISAC algorithms in\nreal-world environments rather than relying solely on simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12010v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11886", "title": "A Composite Alignment-Aware Framework for Myocardial Lesion Segmentation in Multi-sequence CMR Images", "authors": ["Yifan Gao", "Shaohao Rui", "Haoyang Su", "Jinyi Xiang", "Lianming Wu", "Xiaosong Wang"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025", "url": "http://arxiv.org/abs/2507.11886v1", "summary": "Accurate segmentation of myocardial lesions from multi-sequence cardiac\nmagnetic resonance imaging is essential for cardiac disease diagnosis and\ntreatment planning. However, achieving optimal feature correspondence is\nchallenging due to intensity variations across modalities and spatial\nmisalignment caused by inconsistent slice acquisition protocols. We propose\nCAA-Seg, a composite alignment-aware framework that addresses these challenges\nthrough a two-stage approach. First, we introduce a selective slice alignment\nmethod that dynamically identifies and aligns anatomically corresponding slice\npairs while excluding mismatched sections, ensuring reliable spatial\ncorrespondence between sequences. Second, we develop a hierarchical alignment\nnetwork that processes multi-sequence features at different semantic levels,\ni.e., local deformation correction modules address geometric variations in\nlow-level features, while global semantic fusion blocks enable semantic fusion\nat high levels where intensity discrepancies diminish. We validate our method\non a large-scale dataset comprising 397 patients. Experimental results show\nthat our proposed CAA-Seg achieves superior performance on most evaluation\nmetrics, with particularly strong results in myocardial infarction\nsegmentation, representing a substantial 5.54% improvement over\nstate-of-the-art approaches. The code is available at\nhttps://github.com/yifangao112/CAA-Seg.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.11886v1", "cate": "eess.IV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.09092", "title": "OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions", "authors": ["Yuhang Wang", "Abdulaziz Alhuraish", "Shengming Yuan", "Hao Zhou"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09092v2", "summary": "Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its\nreal-world performance remains underexplored due to proprietary systems and\nlimited data access. This paper presents OpenLKA, the first open, large-scale\ndataset for LKA evaluation and improvement. It includes 400 hours of driving\ndata from 62 production vehicle models, collected through extensive road\ntesting in Tampa, Florida and global contributions from the Comma.ai driving\ncommunity. The dataset spans a wide range of challenging scenarios, including\ncomplex road geometries, degraded lane markings, adverse weather, lighting\nconditions and surrounding traffic. The dataset is multimodal, comprising: i)\nfull CAN bus streams, decoded using custom reverse-engineered DBC files to\nextract key LKA events (e.g., system disengagements, lane detection failures);\nii) synchronized high-resolution dash-cam video; iii) real-time outputs from\nOpenpilot, providing accurate estimates of road curvature and lane positioning;\niv) enhanced scene annotations generated by Vision Language Models, describing\nlane visibility, pavement quality, weather, lighting, and traffic conditions.\nBy integrating vehicle-internal signals with high-fidelity perception and rich\nsemantic context, OpenLKA provides a comprehensive platform for benchmarking\nthe real-world performance of production LKA systems, identifying\nsafety-critical operational scenarios, and assessing the readiness of current\nroad infrastructure for autonomous driving. The dataset is publicly available\nat: https://github.com/OpenLKA/OpenLKA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09092v2", "cate": "cs.CV", "date": "2025-05-14", "updated": "2025-07-16"}
{"id": "2507.12064", "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12064v1", "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12064v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12127", "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "authors": ["Ngoc Duy Pham", "Thusitha Dayaratne", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12127v1", "summary": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12127v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12049", "title": "MoViAD: Modular Visual Anomaly Detection", "authors": ["Manuel Barusco", "Francesco Borsatti", "Arianna Stropeni", "Davide Dalle Pezze", "Gian Antonio Susto"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12049v1", "summary": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12049v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12187", "title": "Learning, fast and slow: a two-fold algorithm for data-based model adaptation", "authors": ["Laura Boca de Giuli", "Alessio La Bella", "Riccardo Scattolini"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12187v1", "summary": "This article addresses the challenge of adapting data-based models over time.\nWe propose a novel two-fold modelling architecture designed to correct\nplant-model mismatch caused by two types of uncertainty. Out-of-domain\nuncertainty arises when the system operates under conditions not represented in\nthe initial training dataset, while in-domain uncertainty results from\nreal-world variability and flaws in the model structure or training process. To\nhandle out-of-domain uncertainty, a slow learning component, inspired by the\nhuman brain's slow thinking process, learns system dynamics under unexplored\noperating conditions, and it is activated only when a monitoring strategy deems\nit necessary. This component consists of an ensemble of models, featuring (i) a\ncombination rule that weights individual models based on the statistical\nproximity between their training data and the current operating condition, and\n(ii) a monitoring algorithm based on statistical control charts that supervises\nthe ensemble's reliability and triggers the offline training and integration of\na new model when a new operating condition is detected. To address in-domain\nuncertainty, a fast learning component, inspired by the human brain's fast\nthinking process, continuously compensates in real time for the mismatch of the\nslow learning model. This component is implemented as a Gaussian process (GP)\nmodel, trained online at each iteration using recent data while discarding\nolder samples. The proposed methodology is tested on a benchmark energy system\nreferenced in the literature, demonstrating that the combined use of slow and\nfast learning components improves model accuracy compared to standard\nadaptation approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12187v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12132", "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "categories": ["eess.SP", "cs.CV"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12132v1", "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12132v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12237", "title": "Constructed Realities? Technical and Contextual Anomalies in a High-Profile Image", "authors": ["Matthias Wjst"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      32 pages, 8 figures, 36 references", "url": "http://arxiv.org/abs/2507.12237v1", "summary": "This study offers a forensic assessment of a widely circulated photograph\nfeaturing Prince Andrew, Virginia Giuffre, and Ghislaine Maxwell - an image\nthat has played a pivotal role in public discourse and legal narratives.\nThrough analysis of multiple published versions, several inconsistencies are\nidentified, including irregularities in lighting, posture, and physical\ninteraction, which are more consistent with digital compositing than with an\nunaltered snapshot. While the absence of the original negative and a verifiable\naudit trail precludes definitive conclusions, the technical and contextual\nanomalies suggest that the image may have been deliberately constructed.\nNevertheless, without additional evidence, the photograph remains an unresolved\nbut symbolically charged fragment within a complex story of abuse, memory, and\ncontested truth.", "comment": "32 pages, 8 figures, 36 references", "pdf_url": "http://arxiv.org/pdf/2507.12237v1", "cate": "eess.IV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10158", "title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "authors": ["Obaidullah Zaland", "Erik Elmroth", "Monowar Bhuyan"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The work is accepted for presentation at IEEE SMC 2025", "url": "http://arxiv.org/abs/2507.10158v2", "summary": "Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.", "comment": "The work is accepted for presentation at IEEE SMC 2025", "pdf_url": "http://arxiv.org/pdf/2507.10158v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.12075", "title": "BOOKCOREF: Coreference Resolution at Book Scale", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Lluís Huguet Cabot", "Roberto Navigli"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025 Main Conference. 19 pages", "url": "http://arxiv.org/abs/2507.12075v1", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "pdf_url": "http://arxiv.org/pdf/2507.12075v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12133", "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD", "authors": ["Hanwen Liu", "Yuhe Huang", "Yifeng Gong", "Yanjie Zhai", "Jiaxuan Lu"], "categories": ["cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12133v1", "summary": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12133v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12062", "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning", "authors": ["Hongxu Ma", "Guanshuo Wang", "Fufu Yu", "Qiong Jia", "Shouhong Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM'25", "url": "http://arxiv.org/abs/2507.12062v1", "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint\nspecific moments and assess clip-wise relevance based on the text query. While\nDETR-based joint frameworks have made significant strides, there remains\nuntapped potential in harnessing the intricate relationships between temporal\nmotion and spatial semantics within video content. In this paper, we propose\nthe Motion-Semantics DETR (MS-DETR), a framework that captures rich\nmotion-semantics features through unified learning for MR/HD tasks. The encoder\nfirst explicitly models disentangled intra-modal correlations within motion and\nsemantics dimensions, guided by the given text queries. Subsequently, the\ndecoder utilizes the task-wise correlation across temporal motion and spatial\nsemantics dimensions to enable precise query-guided localization for MR and\nrefined highlight boundary delineation for HD. Furthermore, we observe the\ninherent sparsity dilemma within the motion and semantics dimensions of MR/HD\ndatasets. To address this issue, we enrich the corpus from both dimensions by\ngeneration strategies and propose contrastive denoising learning to ensure the\nabove components learn robustly and effectively. Extensive experiments on four\nMR/HD benchmarks demonstrate that our method outperforms existing\nstate-of-the-art models by a margin. Our code is available at\nhttps://github.com/snailma0229/MS-DETR.git.", "comment": "Accepted by ACM MM'25", "pdf_url": "http://arxiv.org/pdf/2507.12062v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12259", "title": "Neural Co-state Regulator: A Data-Driven Paradigm for Real-time Optimal Control with Input Constraints", "authors": ["Lihan Lian", "Yuxin Tong", "Uduak Inyang-Udoh"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12259v1", "summary": "We propose a novel unsupervised learning framework for solving nonlinear\noptimal control problems (OCPs) with input constraints in real-time. In this\nframework, a neural network (NN) learns to predict the optimal co-state\ntrajectory that minimizes the control Hamiltonian for a given system, at any\nsystem's state, based on the Pontryagin's Minimum Principle (PMP).\nSpecifically, the NN is trained to find the norm-optimal co-state solution that\nsimultaneously satisfies the nonlinear system dynamics and minimizes a\nquadratic regulation cost. The control input is then extracted from the\npredicted optimal co-state trajectory by solving a quadratic program (QP) to\nsatisfy input constraints and optimality conditions. We coin the term neural\nco-state regulator (NCR) to describe the combination of the co-state NN and\ncontrol input QP solver. To demonstrate the effectiveness of the NCR, we\ncompare its feedback control performance with that of an expert nonlinear model\npredictive control (MPC) solver on a unicycle model. Because the NCR's training\ndoes not rely on expert nonlinear control solvers which are often suboptimal,\nthe NCR is able to produce solutions that outperform the nonlinear MPC solver\nin terms of convergence error and input trajectory smoothness even for system\nconditions that are outside its original training domain. At the same time, the\nNCR offers two orders of magnitude less computational time than the nonlinear\nMPC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12259v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12146", "title": "A Practical Analysis: Understanding Phase Noise Modelling in Time and Frequency Domain for Phase-Locked Loops", "authors": ["Carl Collmann", "Bitan Banerjee", "Ahmad Nimr", "Gerhard Fettweis"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      14 Pages", "url": "http://arxiv.org/abs/2507.12146v1", "summary": "In MIMO systems, the presence of phase noise is a significant factor that can\ndegrade performance. For MIMO testbeds build from SDR devices, phase noise\ncannot be ignored, particular in applications that require phase\nsynchronization. This is especially relevant in MIMO systems that employ\ndigital beamforming, where precise phase alignment is crucial. Accordingly,\naccurate phase noise modelling of SDR devices is essential. However, the\ninformation provided in data sheets for different SDR models varies widely and\nis often insufficient for comprehensive characterization of their phase noise\nperformance. While numerical simulations of PLL phase noise behavior are\ndocumented in the literature, there is a lack of extensive measurements\nsupported by appropriate system modelling. In this work, we present a practical\nphase noise modeling methodology applied to an SDR from the USRP X310 series.\nBased on measurement data, we derive estimates of key PLL performance\nindicators such as cycle-to-cycle jitter, oscillator constants, and PLL\nbandwidth. Furthermore, we propose a parametric model for the phase noise PSD\nof the PLL circuit and provide corresponding parameter estimates. This model\ncan be used for further investigation into the impact of phase noise on MIMO\nsystem performance implemented by similar SDR devices.", "comment": "14 Pages", "pdf_url": "http://arxiv.org/pdf/2507.12146v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12427", "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation", "authors": ["Ashkan Shakarami", "Azade Farshad", "Yousef Yeganeh", "Lorenzo Nicole", "Peter Schuffler", "Stefano Ghidoni", "Nassir Navab"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      12 pages, 6 figures", "url": "http://arxiv.org/abs/2507.12427v1", "summary": "We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.12427v1", "cate": "eess.IV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12117", "title": "Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations", "authors": ["Timothy Heightman", "Edward Jiang", "Ruth Mora-Soto", "Maciej Lewenstein", "Marcin Płodzień"], "categories": ["quant-ph", "cs.AI", "math-ph", "math.MP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12117v1", "summary": "Quantum machine learning (QML) seeks to exploit the intrinsic properties of\nquantum mechanical systems, including superposition, coherence, and quantum\nentanglement for classical data processing. However, due to the exponential\ngrowth of the Hilbert space, QML faces practical limits in classical\nsimulations with the state-vector representation of quantum system. On the\nother hand, phase-space methods offer an alternative by encoding quantum states\nas quasi-probability functions. Building on prior work in qubit phase-space and\nthe Stratonovich-Weyl (SW) correspondence, we construct a closed, composable\ndynamical formalism for one- and many-qubit systems in phase-space. This\nformalism replaces the operator algebra of the Pauli group with function\ndynamics on symplectic manifolds, and recasts the curse of dimensionality in\nterms of harmonic support on a domain that scales linearly with the number of\nqubits. It opens a new route for QML based on variational modelling over\nphase-space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12117v1", "cate": "quant-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12142", "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12142v1", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12142v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12087", "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association", "authors": ["Xiang Yu", "Xinyao Liu", "Guang Liang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12087v1", "summary": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12087v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12327", "title": "Mixed-integer Second-Order Cone Programming for Multi-period Scheduling of Flexible AC Transmission System Devices", "authors": ["Mohamad Charara", "Martin De Montigny", "Nivine Abou Daher", "Hanane Dagdougui", "Antoine Lesage-Landry"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure, submitted to CIGRÉ 2025 International Symposium, Paper 10998, PS1: System Enhancement, Markets and Regulation", "url": "http://arxiv.org/abs/2507.12327v1", "summary": "With the increasing energy demand and the growing integration of renewable\nsources of energy, power systems face operational challenges such as overloads,\nlosses, and stability concerns, particularly as networks operate near their\ncapacity limits. Flexible alternating current transmission system (FACTS)\ndevices are essential to ensure reliable grid operations and enable the\nefficient integration of renewable energy. This work introduces a mixed-integer\nsecond-order cone programming (MISOCP) model for the multi-period scheduling of\nkey FACTS devices in electric transmission systems. The proposed model\nintegrates four key control mechanisms: (i) on-load tap changers (OLTCs) for\nvoltage regulation via discrete taps; (ii) static synchronous compensators\n(STATCOMs) and (iii) shunt reactors for reactive power compensation; and (iv)\nthyristor-controlled series capacitors (TCSCs) for adjustable impedance and\nflow control. The objective is to minimize active power losses using a limited\nnumber of control actions while meeting physical and operational constraints at\nall times throughout the defined time horizon. To ensure tractability, the\nmodel employs a second-order cone relaxation of the power flow. Device-specific\nconstraints are handled via binary expansion and linearization: OLTCs and shunt\nreactors are modelled with discrete variables, STATCOMs through reactive power\nbounds, and TCSCs using a reformulation-linearization technique (RLT). A\nmulti-period formulation captures the sequential nature of decision making,\nensuring consistency across time steps. The model is evaluated on the IEEE\n9-bus, 30-bus, and RTS96 test systems, demonstrating its ability to reduce\nlosses, with potential applicability to larger-scale grids.", "comment": "10 pages, 1 figure, submitted to CIGR\\'E 2025 International\n  Symposium, Paper 10998, PS1: System Enhancement, Markets and Regulation", "pdf_url": "http://arxiv.org/pdf/2507.12327v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12210", "title": "PAPR of DFT-s-OTFS with Pulse Shaping", "authors": ["Jialiang Zhu", "Sanoopkumar P. S.", "Arman Farhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12210v1", "summary": "Orthogonal Time Frequency Space (OTFS) suffers from high peak-to-average\npower ratio (PAPR) when the number of Doppler bins is large. To address this\nissue, a discrete Fourier transform spread OTFS (DFT-s-OTFS) scheme is employed\nby applying DFT spreading across the Doppler dimension. This paper presents a\nthorough PAPR analysis of DFT-s-OTFS in the uplink scenario using different\npulse shaping filters and resource allocation strategies. Specifically, we\nderive a PAPR upper bound of DFT-s-OTFS with interleaved and block Doppler\nresource allocation schemes. Our analysis reveals that DFT-s-OTFS with\ninterleaved allocation yields a lower PAPR than that of block allocation.\nFurthermore, we show that interleaved allocation produces a periodic\ntime-domain signal composed of repeated quadrature amplitude modulated (QAM)\nsymbols which simplifies the transmitter design. Based on our analytical\nresults, the root raised cosine (RRC) pulse generally results in a higher\nmaximum PAPR compared to the rectangular pulse. Simulation results confirm the\nvalidity of the derived PAPR upper bounds. Furthermore, we also demonstrate\nthrough BER simulation analysis that the DFT-s-OTFS gives the same performance\nas OTFS without DFT spreading.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12210v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12432", "title": "Energy-based models for inverse imaging problems", "authors": ["Andreas Habring", "Martin Holler", "Thomas Pock", "Martin Zach"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12432v1", "summary": "In this chapter we provide a thorough overview of the use of energy-based\nmodels (EBMs) in the context of inverse imaging problems. EBMs are probability\ndistributions modeled via Gibbs densities $p(x) \\propto \\exp{-E(x)}$ with an\nappropriate energy functional $E$. Within this chapter we present a rigorous\ntheoretical introduction to Bayesian inverse problems that includes results on\nwell-posedness and stability in the finite-dimensional and infinite-dimensional\nsetting. Afterwards we discuss the use of EBMs for Bayesian inverse problems\nand explain the most relevant techniques for learning EBMs from data. As a\ncrucial part of Bayesian inverse problems, we cover several popular algorithms\nfor sampling from EBMs, namely the Metropolis-Hastings algorithm, Gibbs\nsampling, Langevin Monte Carlo, and Hamiltonian Monte Carlo. Moreover, we\npresent numerical results for the resolution of several inverse imaging\nproblems obtained by leveraging an EBM that allows for the explicit\nverification of those properties that are needed for valid energy-based\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12432v1", "cate": "eess.IV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12145", "title": "PRISM: Distributed Inference for Foundation Models at Edge", "authors": ["Muhammad Azlan Qazi", "Alexandros Iosifidis", "Qi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12145v1", "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12145v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12144", "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale", "authors": ["Boris Bonev", "Thorsten Kurth", "Ankur Mahesh", "Mauro Bisson", "Jean Kossaifi", "Karthik Kashinath", "Anima Anandkumar", "William D. Collins", "Michael S. Pritchard", "Alexander Keller"], "categories": ["cs.LG", "physics.ao-ph", "86-10, 68T07", "I.2.1; I.6.5; G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12144v1", "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12144v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12092", "title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis", "authors": ["Nataliia Molchanova", "Alessandro Cagol", "Mario Ocampo-Pineda", "Po-Jui Lu", "Matthias Weigel", "Xinjie Chen", "Erin Beck", "Charidimos Tsagkas", "Daniel Reich", "Colin Vanden Bulcke", "Anna Stolting", "Serena Borrelli", "Pietro Maggi", "Adrien Depeursinge", "Cristina Granziera", "Henning Mueller", "Pedro M. Gordaliza", "Meritxell Bach Cuadra"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12092v1", "summary": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple\nsclerosis (MS), offering high diagnostic specificity and prognostic relevance.\nHowever, their routine clinical integration remains limited due to subtle\nmagnetic resonance imaging (MRI) appearance, challenges in expert annotation,\nand a lack of standardized automated methods. We propose a comprehensive\nmulti-centric benchmark of CL detection and segmentation in MRI. A total of 656\nMRI scans, including clinical trial and research data from four institutions,\nwere acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with\nexpert-consensus annotations. We rely on the self-configuring nnU-Net\nframework, designed for medical imaging segmentation, and propose adaptations\ntailored to the improved CL detection. We evaluated model generalization\nthrough out-of-distribution testing, demonstrating strong lesion detection\ncapabilities with an F1-score of 0.64 and 0.5 in and out of the domain,\nrespectively. We also analyze internal model features and model errors for a\nbetter understanding of AI decision-making. Our study examines how data\nvariability, lesion ambiguity, and protocol differences impact model\nperformance, offering future recommendations to address these barriers to\nclinical adoption. To reinforce the reproducibility, the implementation and\nmodels will be publicly accessible and ready to use at\nhttps://github.com/Medical-Image-Analysis-Laboratory/ and\nhttps://doi.org/10.5281/zenodo.15911797.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12092v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12339", "title": "Symbolic Control: Unveiling Free Robustness Margins", "authors": ["Youssef Ait Si", "Antoine Girard", "Adnane Saoud"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      9", "url": "http://arxiv.org/abs/2507.12339v1", "summary": "This paper addresses the challenge of ensuring robustness in the presence of\nsystem perturbations for symbolic control techniques. Given a discrete-time\ncontrol system that is related to its symbolic model by an alternating\nsimulation relation. In this paper, we focus on computing the maximum\nrobustness margin under which the symbolic model remains valid for a\nperturbed-version of the discrete-time control system. We first show that\nsymbolic models are inherently equipped with a certain free robustness margins.\nWe then provide constructive procedures to compute uniform and non-uniform\n(state and input dependent) robustness margins. We also show that the tightness\nof the robustness margin depends on the tightness of the reachability technique\nused to compute the symbolic model. We then explain how the computed robustness\nmargin can be used for the sake of controller synthesis. Finally, we present\ntwo illustrative examples to demonstrate the effectiveness of our approach.", "comment": "9", "pdf_url": "http://arxiv.org/pdf/2507.12339v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12211", "title": "Cell Sensing: Traffic detection", "authors": ["Saúl Fenollosa"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      39 pages, 16 figures. Student project report at the Telecommunications Circuits Laboratory (TCL), EPFL. Supervised by Prof. Andreas Burg and Sitian Li", "url": "http://arxiv.org/abs/2507.12211v1", "summary": "This work presents a passive sensing system for traffic monitoring using\nambient Long Term Evolution (LTE) signals as a non-intrusive and scalable\nalternative to traditional surveillance methods. The approach employs a\ndual-receiver architecture analyzing Channel State Information (CSI) to isolate\ndifferential Doppler shifts induced by moving targets, effectively mitigating\nhardware-induced phase impairments. Implemented with a Software Defined Radio\n(SDR) platform and srsRAN software, the system demonstrated over 90% detection\naccuracy for speeds above 6000 mm/min in controlled indoor tests, and provided\nreliable speed estimations for pedestrians and vehicles in outdoor evaluations.\nDespite challenges at low speeds, directional ambiguity, and multipath fading\nin urban settings, the results validate LTE-based passive sensing as a feasible\ntraffic monitoring method, identifying critical areas for future research such\nas angle-of-arrival (AoA) integration, machine learning, and real-time embedded\nsystem development.", "comment": "39 pages, 16 figures. Student project report at the\n  Telecommunications Circuits Laboratory (TCL), EPFL. Supervised by Prof.\n  Andreas Burg and Sitian Li", "pdf_url": "http://arxiv.org/pdf/2507.12211v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12042", "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification", "authors": ["Kazuki Shimada", "Archontis Politis", "Iran R. Roman", "Parthasaarathy Sudarsanam", "David Diaz-Guerra", "Ruchi Pandey", "Kengo Uchida", "Yuichiro Koyama", "Naoya Takahashi", "Takashi Shibuya", "Shusuke Takahashi", "Tuomas Virtanen", "Yuki Mitsufuji"], "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures", "url": "http://arxiv.org/abs/2507.12042v1", "summary": "This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.", "comment": "5 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.12042v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12045", "title": "Self-Boosted Weight-Constrained FxLMS: A Robustness Distributed Active Noise Control Algorithm Without Internode Communication", "authors": ["Junwei Ji", "Dongyuan Shi", "Zhengding Luo", "Boxiang Wang", "Woon-Seng Gan"], "categories": ["eess.AS", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12045v1", "summary": "Compared to the conventional centralized multichannel active noise control\n(MCANC) algorithm, which requires substantial computational resources,\ndecentralized approaches exhibit higher computational efficiency but typically\nresult in inferior noise reduction performance. To enhance performance,\ndistributed ANC methods have been introduced, enabling information exchange\namong ANC nodes; however, the resulting communication latency often compromises\nsystem stability. To overcome these limitations, we propose a self-boosted\nweight-constrained filtered-reference least mean square (SB-WCFxLMS) algorithm\nfor the distributed MCANC system without internode communication. The WCFxLMS\nalgorithm is specifically designed to mitigate divergence issues caused by the\ninternode cross-talk effect. The self-boosted strategy lets each ANC node\nindependently adapt its constraint parameters based on its local noise\nreduction performance, thus ensuring effective noise cancellation without the\nneed for inter-node communication. With the assistance of this mechanism, this\napproach significantly reduces both computational complexity and communication\noverhead. Numerical simulations employing real acoustic paths and compressor\nnoise validate the effectiveness and robustness of the proposed system. The\nresults demonstrate that our proposed method achieves satisfactory noise\ncancellation performance with minimal resource requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12045v1", "cate": "eess.AS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12188", "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement", "authors": ["Shuangli Du", "Siming Yan", "Zhenghao Shi", "Zhenzhen You", "Lu Sun"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12188v1", "summary": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12188v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12165", "title": "Multi-Component VAE with Gaussian Markov Random Field", "authors": ["Fouad Oubari", "Mohamed El-Baha", "Raphael Meunier", "Rodrigue Décatoire", "Mathilde Mougeot"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12165v1", "summary": "Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12165v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12095", "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images", "authors": ["Davide Di Nucci", "Matteo Tomei", "Guido Borghi", "Luca Ciuffreda", "Roberto Vezzani", "Rita Cucchiara"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12095v1", "summary": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12095v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12444", "title": "BitWave: Exploiting Column-Based Bit-Level Sparsity for Deep Learning Acceleration", "authors": ["Man Shi", "Vikram Jain", "Antony Joseph", "Maurice Meijer", "Marian Verhelst"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      15 pages, 18 figures, 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "url": "http://arxiv.org/abs/2507.12444v1", "summary": "Bit-serial computation facilitates bit-wise sequential data processing,\noffering numerous benefits, such as a reduced area footprint and\ndynamically-adaptive computational precision. It has emerged as a prominent\napproach, particularly in leveraging bit-level sparsity in Deep Neural Networks\n(DNNs). However, existing bit-serial accelerators exploit bit-level sparsity to\nreduce computations by skipping zero bits, but they suffer from inefficient\nmemory accesses due to the irregular indices of the non-zero bits.\n  As memory accesses typically are the dominant contributor to DNN accelerator\nperformance, this paper introduces a novel computing approach called\n\"bit-column-serial\" and a compatible architecture design named \"BitWave.\"\nBitWave harnesses the advantages of the \"bit-column-serial\" approach,\nleveraging structured bit-level sparsity in combination with dynamic dataflow\ntechniques. This achieves a reduction in computations and memory footprints\nthrough redundant computation skipping and weight compression. BitWave is able\nto mitigate the performance drop or the need for retraining that is typically\nassociated with sparsity-enhancing techniques using a post-training\noptimization involving selected weight bit-flips. Empirical studies conducted\non four deep-learning benchmarks demonstrate the achievements of BitWave: (1)\nMaximally realize 13.25x higher speedup, 7.71x efficiency compared to\nstate-of-the-art sparsity-aware accelerators. (2) Occupying 1.138 mm2 area and\nconsuming 17.56 mW power in 16nm FinFet process node.", "comment": "15 pages, 18 figures, 2024 IEEE International Symposium on\n  High-Performance Computer Architecture (HPCA)", "pdf_url": "http://arxiv.org/pdf/2507.12444v1", "cate": "eess.SY", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12221", "title": "Novel Approach to Dual-Channel Estimation in Integrated Sensing and Communications for 6G", "authors": ["Alejandro Castilla", "Saúl Fenollosa", "Monika Drozdowska", "Alejandro Lopez-Escudero", "Sergio Micò-Rosa", "Narcis Cardona"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 13 figures. Accepted for publication at the 2024 IEEE 35th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)", "url": "http://arxiv.org/abs/2507.12221v1", "summary": "Integrated Sensing and Communication (ISAC) design is crucial for 6G and\nharmonizes environmental data sensing with communication, emphasizing the need\nto understand and model these elements. This paper delves into dual-channel\nmodels for ISAC, employing channel extraction techniques to validate and\nenhance accuracy. Focusing on millimeter wave (mmWave) radars, it explores the\nextraction of the bistatic sensing channel from monostatic measurements and\nsubsequent communication channel estimation. The proposed methods involve\ninterference extraction, module and phase correlation analyses, chirp\nclustering, and auto-clutter reduction. A comprehensive set-up in an anechoic\nchamber with controlled scenarios evaluates the proposed techniques,\ndemonstrating successful channel extraction and validation through Root Mean\nSquare Delay Spread (RMS DS), Power Delay Profile (PDP), and Angle of Arrival\n(AoA) analysis. Comparison with Ray-Tracing (RT) simulations confirms the\neffectiveness of the proposed approach, presenting an innovative stride towards\nfully integrated sensing and communication in future networks.", "comment": "6 pages, 13 figures. Accepted for publication at the 2024 IEEE 35th\n  International Symposium on Personal, Indoor and Mobile Radio Communications\n  (PIMRC)", "pdf_url": "http://arxiv.org/pdf/2507.12221v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2211.05622", "title": "InstantGroup: Instant Template Generation for Scalable Group of Brain MRI Registration", "authors": ["Ziyi He", "Albert C. S. Chung"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      14 pages, 10 figures, and 6 tables. Accepted by IEEE Transactions on Image Processing", "url": "http://arxiv.org/abs/2211.05622v3", "summary": "Template generation is a critical step in groupwise image registration, which\ninvolves aligning a group of subjects into a common space. While existing\nmethods can generate high-quality template images, they often incur substantial\ntime costs or are limited by fixed group scales. In this paper, we present\nInstantGroup, an efficient groupwise template generation framework based on\nvariational autoencoder (VAE) models that leverage latent representations'\narithmetic properties, enabling scalability to groups of any size. InstantGroup\nfeatures a Dual VAE backbone with shared-weight twin networks to handle pairs\nof inputs and incorporates a Displacement Inversion Module (DIM) to maintain\ntemplate unbiasedness and a Subject-Template Alignment Module (STAM) to improve\ntemplate quality and registration accuracy. Experiments on 3D brain MRI scans\nfrom the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces\nruntime, generating templates within seconds for various group sizes while\nmaintaining superior performance compared to state-of-the-art baselines on\nquantitative metrics, including unbiasedness and registration accuracy.", "comment": "14 pages, 10 figures, and 6 tables. Accepted by IEEE Transactions on\n  Image Processing", "pdf_url": "http://arxiv.org/pdf/2211.05622v3", "cate": "eess.IV", "date": "2022-11-10", "updated": "2025-07-16"}
{"id": "2507.12081", "title": "VoxATtack: A Multimodal Attack on Voice Anonymization Systems", "authors": ["Ahmad Aloradi", "Ünal Ege Gaznepoglu", "Emanuël A. P. Habets", "Daniel Tenbrinck"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures, 3 tables, accepted at WASPAA 2025", "url": "http://arxiv.org/abs/2507.12081v1", "summary": "Voice anonymization systems aim to protect speaker privacy by obscuring vocal\ntraits while preserving the linguistic content relevant for downstream\napplications. However, because these linguistic cues remain intact, they can be\nexploited to identify semantic speech patterns associated with specific\nspeakers. In this work, we present VoxATtack, a novel multimodal\nde-anonymization model that incorporates both acoustic and textual information\nto attack anonymization systems. While previous research has focused on\nrefining speaker representations extracted from speech, we show that\nincorporating textual information with a standard ECAPA-TDNN improves the\nattacker's performance. Our proposed VoxATtack model employs a dual-branch\narchitecture, with an ECAPA-TDNN processing anonymized speech and a pretrained\nBERT encoding the transcriptions. Both outputs are projected into embeddings of\nequal dimensionality and then fused based on confidence weights computed on a\nper-utterance basis. When evaluating our approach on the VoicePrivacy Attacker\nChallenge (VPAC) dataset, it outperforms the top-ranking attackers on five out\nof seven benchmarks, namely B3, B4, B5, T8-5, and T12-5. To further boost\nperformance, we leverage anonymized speech and SpecAugment as augmentation\ntechniques. This enhancement enables VoxATtack to achieve state-of-the-art on\nall VPAC benchmarks, after scoring 20.6% and 27.2% average equal error rate on\nT10-2 and T25-1, respectively. Our results demonstrate that incorporating\ntextual information and selective data augmentation reveals critical\nvulnerabilities in current voice anonymization methods and exposes potential\nweaknesses in the datasets used to evaluate them.", "comment": "5 pages, 3 figures, 3 tables, accepted at WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.12081v1", "cate": "eess.AS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12189", "title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search", "authors": ["Azhar Ikhtiarudin", "Aditi Das", "Param Thakkar", "Akash Kundu"], "categories": ["quant-ph", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Comprehensive RL agent benchmark for QAS. Contributions are welcomed here: this https URL", "url": "http://arxiv.org/abs/2507.12189v1", "summary": "We introduce BenchRL-QAS, a unified benchmarking framework for systematically\nevaluating reinforcement learning (RL) algorithms in quantum architecture\nsearch (QAS) across diverse variational quantum algorithm tasks and system\nsizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including\nboth value-based and policy-gradient methods on representative quantum problems\nsuch as variational quantum eigensolver, variational quantum state\ndiagonalization, quantum classification, and state preparation, spanning both\nnoiseless and realistic noisy regimes. We propose a weighted ranking metric\nthat balances accuracy, circuit depth, gate count, and computational\nefficiency, enabling fair and comprehensive comparison. Our results first\nreveal that RL-based quantum classifier outperforms baseline variational\nclassifiers. Then we conclude that no single RL algorithm is universally\noptimal when considering a set of QAS tasks; algorithmic performance is highly\ncontext-dependent, varying with task structure, qubit count, and noise. This\nempirical finding provides strong evidence for the \"no free lunch\" principle in\nRL-based quantum circuit design and highlights the necessity of tailored\nalgorithm selection and systematic benchmarking for advancing quantum circuit\nsynthesis. This work represents the most comprehensive RL-QAS benchmarking\neffort to date, and BenchRL-QAS along with all experimental data are made\npublicly available to support reproducibility and future research\nhttps://github.com/azhar-ikhtiarudin/bench-rlqas.", "comment": "Comprehensive RL agent benchmark for QAS. Contributions are welcomed\n  here: https://github.com/azhar-ikhtiarudin/bench-rlqas", "pdf_url": "http://arxiv.org/pdf/2507.12189v1", "cate": "quant-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12166", "title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication", "authors": ["Xiucheng Wang", "Qiming Zhang", "Nan Cheng", "Junting Chen", "Zezhong Zhang", "Zan Li", "Shuguang Cui", "Xuemin Shen"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12166v1", "summary": "Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12166v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12105", "title": "Out-of-distribution data supervision towards biomedical semantic segmentation", "authors": ["Yiquan Gao", "Duohui Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper was published in Proceedings of SPIE Volume 13442 and is reprinted with permission. The official version is available at this https URL . One personal copy is allowed. Reproduction, distribution, or commercial use is prohibited", "url": "http://arxiv.org/abs/2507.12105v1", "summary": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.", "comment": "This paper was published in Proceedings of SPIE Volume 13442 and is\n  reprinted with permission. The official version is available at\n  https://doi.org/10.1117/12.3052988. One personal copy is allowed.\n  Reproduction, distribution, or commercial use is prohibited", "pdf_url": "http://arxiv.org/pdf/2507.12105v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12122", "title": "Soft-Constrained Spatially Selective Active Noise Control for Open-fitting Hearables", "authors": ["Tong Xiao", "Reinhild Roden", "Matthias Blau", "Simon Doclo"], "categories": ["eess.AS", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "url": "http://arxiv.org/abs/2507.12122v1", "summary": "Recent advances in spatially selective active noise control (SSANC) using\nmultiple microphones have enabled hearables to suppress undesired noise while\npreserving desired speech from a specific direction. Aiming to achieve minimal\nspeech distortion, a hard constraint has been used in previous work in the\noptimization problem to compute the control filter. In this work, we propose a\nsoft-constrained SSANC system that uses a frequency-independent parameter to\ntrade off between speech distortion and noise reduction. We derive both time-\nand frequency-domain formulations, and show that conventional active noise\ncontrol and hard-constrained SSANC represent two limiting cases of the proposed\ndesign. We evaluate the system through simulations using a pair of open-fitting\nhearables in an anechoic environment with one speech source and two noise\nsources. The simulation results validate the theoretical derivations and\ndemonstrate that for a broad range of the trade-off parameter, the\nsignal-to-noise ratio and the speech quality and intelligibility in terms of\nPESQ and ESTOI can be substantially improved compared to the hard-constrained\ndesign.", "comment": "Accepted at IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "pdf_url": "http://arxiv.org/pdf/2507.12122v1", "cate": "eess.AS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12235", "title": "Frequency-responsive RCS characteristics and scaling implications for ISAC development", "authors": ["Saúl Fenollosa", "Monika Drozdowska", "Wenfei Yang", "Sergio Micó-Rosa", "Alejandro Castilla", "Alejandro Lopez-Escudero", "Jian Li", "Narcis Cardona"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 12 figures, 3 tables. Accepted for publication at the 2024 IEEE Global Communications Conference (GLOBECOM), WS-02: Workshop on Propagation Channel Models and Evaluation Methodologies for 6G", "url": "http://arxiv.org/abs/2507.12235v1", "summary": "This paper presents an investigation on the Radar Cross-Section (RCS) of\nvarious targets, with the objective of analysing how RCS properties vary with\nfrequency. Targets such as an Automated Guided Vehicle (AGV), a pedestrian, and\na full-scale car were measured in the frequency bands referred to in industry\nstandards as FR2 and FR3. Measurements were taken in diverse environments,\nindoors and outdoors, to ensure comprehensive scenario coverage. The\nmethodology employed in RCS extraction performs background subtraction,\nfollowed by time-domain gating to isolate the influence of the target. This\nanalysis compares the RCS values and how the points of greatest contribution\nare distributed across different bands based on the range response of the RCS.\nAnalysis of the results demonstrated how RCS values change with frequency and\ntarget shape, providing insights into the electromagnetic behaviour of these\ntargets. Key findings highlight how much scaling RCS values based on frequency\nand geometry is complex and varies among different types of materials and\nshapes. These insights are instrumental for advancing sensing systems and\nenhancing 3GPP channel models, particularly for Integrated Sensing and\nCommunications (ISAC) techniques proposed for 6G standards.", "comment": "6 pages, 12 figures, 3 tables. Accepted for publication at the 2024\n  IEEE Global Communications Conference (GLOBECOM), WS-02: Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "pdf_url": "http://arxiv.org/pdf/2507.12235v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2308.09730", "title": "The Utility of the Virtual Imaging Trials Methodology for Objective Characterization of AI Systems and Training Data", "authors": ["Fakrul Islam Tushar", "Lavsen Dahal", "Saman Sotoudeh-Paima", "Ehsan Abadi", "W. Paul Segars", "Ehsan Samei", "Joseph Y. Lo"], "categories": ["eess.IV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      8 figures, 4 Tables", "url": "http://arxiv.org/abs/2308.09730v5", "summary": "Purpose: The credibility of Artificial Intelligence (AI) models for medical\nimaging continues to be a challenge, affected by the diversity of models, the\ndata used to train the models, and applicability of their combination to\nproduce reproducible results for new data. Approach: In this work we aimed to\nexplore if the emerging Virtual Imaging Trials (VIT) methodologies can provide\nan objective resource to approach this challenge. The study was conducted for\nthe case example of COVID-19 diagnosis using clinical and virtual computed\ntomography (CT) and chest radiography (CXR) processed with convolutional neural\nnetworks. Multiple AI models were developed and tested using 3D ResNet-like and\n2D EfficientNetv2 architectures across diverse datasets. Results: The\nperformance differences were evaluated in terms of the area under the curve\n(AUC) and the DeLong method for AUC confidence intervals. The models trained on\nthe most diverse datasets showed the highest external testing performance, with\nAUC values ranging from 0.73-0.76 for CT and 0.70-0.73 for CXR. Internal\ntesting yielded higher AUC values (0.77 -0.85 for CT and 0.77-1.0 for CXR),\nhighlighting a substantial drop in performance during external validation,\nwhich underscores the importance of diverse and comprehensive training and\ntesting data. Most notably, VIT approach provided objective assessment of the\nutility of diverse models and datasets while further providing insight into the\ninfluence of dataset characteristics, patient factors, and imaging physics on\nAI efficacy. Conclusions: The VIT approach can be used to enhance model\ntransparency and reliability, offering nuanced insights into the factors\ndriving AI performance and bridging the gap between experimental and clinical\nsettings.", "comment": "8 figures, 4 Tables", "pdf_url": "http://arxiv.org/pdf/2308.09730v5", "cate": "eess.IV", "date": "2023-08-17", "updated": "2025-07-16"}
{"id": "2507.11777", "title": "Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake Detection", "authors": ["Ivan Viakhirev", "Daniil Sirota", "Aleksandr Smirnov", "Kirill Borodin"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11777v1", "summary": "Advances in voice conversion and text-to-speech synthesis have made automatic\nspeaker verification (ASV) systems more susceptible to spoofing attacks. This\nwork explores modest refinements to the AASIST anti-spoofing architecture. It\nincorporates a frozen Wav2Vec 2.0 encoder to retain self-supervised speech\nrepresentations in limited-data settings, substitutes the original graph\nattention block with a standardized multi-head attention module using\nheterogeneous query projections, and replaces heuristic frame-segment fusion\nwith a trainable, context-aware integration layer. When evaluated on the\nASVspoof 5 corpus, the proposed system reaches a 7.6\\% equal error rate (EER),\nimproving on a re-implemented AASIST baseline under the same training\nconditions. Ablation experiments suggest that each architectural change\ncontributes to the overall performance, indicating that targeted adjustments to\nestablished models may help strengthen speech deepfake detection in practical\nscenarios. The code is publicly available at\nhttps://github.com/KORALLLL/AASIST_SCALING.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11777v1", "cate": "cs.SD", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12195", "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "authors": ["Arkaprabha Basu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12195v1", "summary": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12195v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12192", "title": "Explainable Evidential Clustering", "authors": ["Victor F. Lopes de Souza", "Karima Bakhti", "Sofiane Ramdani", "Denis Mottet", "Abdelhak Imoussaten"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12192v1", "summary": "Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12192v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12114", "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance", "authors": ["Yuzhou Ji", "Ke Ma", "Hong Cai", "Anchun Zhang", "Lizhuang Ma", "Xin Tan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12114v1", "summary": "Dynamic driving scene reconstruction is of great importance in fields like\ndigital twin system and autonomous driving simulation. However, unacceptable\ndegradation occurs when the view deviates from the input trajectory, leading to\ncorrupted background and vehicle models. To improve reconstruction quality on\nnovel trajectory, existing methods are subject to various limitations including\ninconsistency, deformation, and time consumption. This paper proposes\nLidarPainter, a one-step diffusion model that recovers consistent driving views\nfrom sparse LiDAR condition and artifact-corrupted renderings in real-time,\nenabling high-fidelity lane shifts in driving scene reconstruction. Extensive\nexperiments show that LidarPainter outperforms state-of-the-art methods in\nspeed, quality and resource efficiency, specifically 7 x faster than\nStreetCrafter with only one fifth of GPU memory required. LidarPainter also\nsupports stylized generation using text prompts such as \"foggy\" and \"night\",\nallowing for a diverse expansion of the existing asset library.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12114v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12196", "title": "Selective Quantization Tuning for ONNX Models", "authors": ["Nikolaos Louloudakis", "Ajitha Rajan"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures, 2 tables", "url": "http://arxiv.org/abs/2507.12196v1", "summary": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.", "comment": "5 pages, 3 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.12196v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12317", "title": "Road Roughness Estimation via Fusion of Standard Onboard Automotive Sensors", "authors": ["Martin Agebjär", "Gustav Zetterqvist", "Fredrik Gustafsson", "Johan Wahlström", "Gustaf Hendeby"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted for publication in FUSION 2025 - 28th International Conference on Information Fusion (FUSION), IEEE (2025)", "url": "http://arxiv.org/abs/2507.12317v1", "summary": "Road roughness significantly affects vehicle vibrations and ride quality. We\nintroduce a Kalman filter (KF)-based method for estimating road roughness in\nterms of the international roughness index (IRI) by fusing inertial and speed\nmeasurements, offering a cost-effective solution for pavement monitoring. The\nmethod involves system identification on a physical vehicle to estimate\nrealistic model parameters, followed by KF-based reconstruction of the\nlongitudinal road profile to compute IRI values. It explores IRI estimation\nusing vertical and lateral vibrations, the latter more common in modern\nvehicles. Validation on 230 km of real-world data shows promising results, with\nIRI estimation errors ranging from 1% to 10% of the reference values. However,\naccuracy deteriorates significantly when using only lateral vibrations,\nhighlighting their limitations. These findings demonstrate the potential of\nKF-based estimation for efficient road roughness monitoring.", "comment": "Accepted for publication in FUSION 2025 - 28th International\n  Conference on Information Fusion (FUSION), IEEE (2025)", "pdf_url": "http://arxiv.org/pdf/2507.12317v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2404.05102", "title": "LHU-Net: a Lean Hybrid U-Net for Cost-efficient, High-performance Volumetric Segmentation", "authors": ["Yousef Sadegheih", "Afshin Bozorgpour", "Pratibha Kumari", "Reza Azad", "Dorit Merhof"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI-2025", "url": "http://arxiv.org/abs/2404.05102v3", "summary": "The rise of Transformer architectures has advanced medical image\nsegmentation, leading to hybrid models that combine Convolutional Neural\nNetworks (CNNs) and Transformers. However, these models often suffer from\nexcessive complexity and fail to effectively integrate spatial and channel\nfeatures, crucial for precise segmentation. To address this, we propose\nLHU-Net, a Lean Hybrid U-Net for volumetric medical image segmentation. LHU-Net\nprioritizes spatial feature extraction before refining channel features,\noptimizing both efficiency and accuracy. Evaluated on four benchmark datasets\n(Synapse, Left Atrial, BraTS-Decathlon, and Lung-Decathlon), LHU-Net\nconsistently outperforms existing models across diverse modalities (CT/MRI) and\noutput configurations. It achieves state-of-the-art Dice scores while using\nfour times fewer parameters and 20% fewer FLOPs than competing models, without\nthe need for pre-training, additional data, or model ensembles. With an average\nof 11 million parameters, LHU-Net sets a new benchmark for computational\nefficiency and segmentation accuracy. Our implementation is available on\nGitHub: https://github.com/xmindflow/LHUNet", "comment": "Accepted at MICCAI-2025", "pdf_url": "http://arxiv.org/pdf/2404.05102v3", "cate": "eess.IV", "date": "2024-04-07", "updated": "2025-07-16"}
{"id": "2507.11812", "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction", "authors": ["Wei Huang", "Yuqiang Huang", "Yanan Wu", "Tianhe Xu", "Junting Wang", "Hao Zhang"], "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11812v1", "summary": "Sound speed profiles (SSPs) are essential parameters underwater that affects\nthe propagation mode of underwater signals and has a critical impact on the\nenergy efficiency of underwater acoustic communication and accuracy of\nunderwater acoustic positioning. Traditionally, SSPs can be obtained by\nmatching field processing (MFP), compressive sensing (CS), and deep learning\n(DL) methods. However, existing methods mainly rely on on-site underwater sonar\nobservation data, which put forward strict requirements on the deployment of\nsonar observation systems. To achieve high-precision estimation of sound\nvelocity distribution in a given sea area without on-site underwater data\nmeasurement, we propose a multi-modal data-fusion generative adversarial\nnetwork model with residual attention block (MDF-RAGAN) for SSP construction.\nTo improve the model's ability for capturing global spatial feature\ncorrelations, we embedded the attention mechanisms, and use residual modules\nfor deeply capturing small disturbances in the deep ocean sound velocity\ndistribution caused by changes of SST. Experimental results on real open\ndataset show that the proposed model outperforms other state-of-the-art\nmethods, which achieves an accuracy with an error of less than 0.3m/s.\nSpecifically, MDF-RAGAN not only outperforms convolutional neural network (CNN)\nand spatial interpolation (SITP) by nearly a factor of two, but also achieves\nabout 65.8\\% root mean square error (RMSE) reduction compared to mean profile,\nwhich fully reflects the enhancement of overall profile matching by\nmulti-source fusion and cross-modal attention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11812v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12197", "title": "Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations", "authors": ["Yichen Han", "Xiaoyang Hao", "Keming Chen", "Weibo Xiong", "Jun He", "Ruonan Zhang", "Junjie Cao", "Yue Liu", "Bowen Li", "Dongrui Zhang", "Hui Xia", "Huilei Fu", "Kai Jia", "Kaixuan Guo", "Mingli Jin", "Qingyun Meng", "Ruidong Ma", "Ruiqian Fang", "Shaotong Guo", "Xuhui Li", "Yang Xiang", "Ying Zhang", "Yulong Liu", "Yunfeng Li", "Yuyi Zhang", "Yuze Zhou", "Zhen Wang", "Zhaowen Chen"], "categories": ["cs.SD", "cs.AI"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12197v1", "summary": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12197v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12218", "title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "authors": ["Tomohisa Okazaki"], "categories": ["cs.LG", "physics.geo-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12218v1", "summary": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12218v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12123", "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph", "authors": ["Sergey Linok", "Gleb Naumov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 5 figures, 2 tables", "url": "http://arxiv.org/abs/2507.12123v1", "summary": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.", "comment": "13 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.12123v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12433", "title": "Traffic-Aware Pedestrian Intention Prediction", "authors": ["Fahimeh Orvati Nia", "Hai Lin"], "categories": ["cs.CV", "cs.SY", "eess.SY", "I.2.10; I.5.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures. Accepted to the American Control Conference (ACC) 2025", "url": "http://arxiv.org/abs/2507.12433v1", "summary": "Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.", "comment": "6 pages, 4 figures. Accepted to the American Control Conference (ACC)\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.12433v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12417", "title": "Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI", "authors": ["Weichen Dai", "Yuxuan Huang", "Li Zhu", "Dongjun Liu", "Yu Zhang", "Qibin Zhao", "Andrzej Cichocki", "Fabio Babiloni", "Ke Li", "Jianyu Qiu", "Gangyong Jia", "Wanzeng Kong", "Qing Wu"], "categories": ["q-bio.NC", "cs.CV", "eess.SP"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12417v1", "summary": "Humans possess a remarkable capacity for spatial cognition, allowing for\nself-localization even in novel or unfamiliar environments. While hippocampal\nneurons encoding position and orientation are well documented, the large-scale\nneural dynamics supporting spatial representation, particularly during\nnaturalistic, passive experience, remain poorly understood. Here, we\ndemonstrate for the first time that non-invasive brain-computer interfaces\n(BCIs) based on electroencephalography (EEG) can decode spontaneous,\nfine-grained egocentric 6D pose, comprising three-dimensional position and\norientation, during passive viewing of egocentric video. Despite EEG's limited\nspatial resolution and high signal noise, we find that spatially coherent\nvisual input (i.e., continuous and structured motion) reliably evokes decodable\nspatial representations, aligning with participants' subjective sense of\nspatial engagement. Decoding performance further improves when visual input is\npresented at a frame rate of 100 ms per image, suggesting alignment with\nintrinsic neural temporal dynamics. Using gradient-based backpropagation\nthrough a neural decoding model, we identify distinct EEG channels contributing\nto position -- and orientation specific -- components, revealing a distributed\nyet complementary neural encoding scheme. These findings indicate that the\nbrain's spatial systems operate spontaneously and continuously, even under\npassive conditions, challenging traditional distinctions between active and\npassive spatial cognition. Our results offer a non-invasive window into the\nautomatic construction of egocentric spatial maps and advance our understanding\nof how the human mind transforms everyday sensory experience into structured\ninternal representations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12417v1", "cate": "q-bio.NC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.15513", "title": "SPA: Efficient User-Preference Alignment against Uncertainty in Medical Image Segmentation", "authors": ["Jiayuan Zhu", "Junde Wu", "Cheng Ouyang", "Konstantinos Kamnitsas", "J. Alison Noble"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.15513v2", "summary": "Medical image segmentation data inherently contain uncertainty. This can stem\nfrom both imperfect image quality and variability in labeling preferences on\nambiguous pixels, which depend on annotator expertise and the clinical context\nof the annotations. For instance, a boundary pixel might be labeled as tumor in\ndiagnosis to avoid under-estimation of severity, but as normal tissue in\nradiotherapy to prevent damage to sensitive structures. As segmentation\npreferences vary across downstream applications, it is often desirable for an\nimage segmentation model to offer user-adaptable predictions rather than a\nfixed output. While prior uncertainty-aware and interactive methods offer\nadaptability, they are inefficient at test time: uncertainty-aware models\nrequire users to choose from numerous similar outputs, while interactive models\ndemand significant user input through click or box prompts to refine\nsegmentation. To address these challenges, we propose \\textbf{SPA}, a new\n\\textbf{S}egmentation \\textbf{P}reference \\textbf{A}lignment framework that\nefficiently adapts to diverse test-time preferences with minimal human\ninteraction. By presenting users with a select few, distinct segmentation\ncandidates that best capture uncertainties, it reduces the user workload to\nreach the preferred segmentation. To accommodate user preference, we introduce\na probabilistic mechanism that leverages user feedback to adapt a model's\nsegmentation preference. The proposed framework is evaluated on several medical\nimage segmentation tasks: color fundus images, lung lesion and kidney CT scans,\nMRI scans of brain and prostate. SPA shows 1) a significant reduction in user\ntime and effort compared to existing interactive segmentation approaches, 2)\nstrong adaptability based on human feedback, and 3) state-of-the-art image\nsegmentation performance across different imaging modalities and semantic\nlabels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.15513v2", "cate": "eess.IV", "date": "2024-11-23", "updated": "2025-07-16"}
{"id": "2507.11925", "title": "Schrödinger Bridge Consistency Trajectory Models for Speech Enhancement", "authors": ["Shuichiro Nishigori", "Koichi Saito", "Naoki Murata", "Masato Hirano", "Shusuke Takahashi", "Yuki Mitsufuji"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11925v1", "summary": "Speech enhancement (SE) utilizing diffusion models is a promising technology\nthat improves speech quality in noisy speech data. Furthermore, the\nSchr\\\"odinger bridge (SB) has recently been used in diffusion-based SE to\nimprove speech quality by resolving a mismatch between the endpoint of the\nforward process and the starting point of the reverse process. However, the SB\nstill exhibits slow inference owing to the necessity of a large number of\nfunction evaluations (NFE) for inference to obtain high-quality results. While\nConsistency Models (CMs) address this issue by employing consistency training\nthat uses distillation from pretrained models in the field of image generation,\nit does not improve generation quality when the number of steps increases. As a\nsolution to this problem, Consistency Trajectory Models (CTMs) not only\naccelerate inference speed but also maintain a favorable trade-off between\nquality and speed. Furthermore, SoundCTM demonstrates the applicability of CTM\ntechniques to the field of sound generation. In this paper, we present\nSchr\\\"odinger bridge Consistency Trajectory Models (SBCTM) by applying the\nCTM's technique to the Schr\\\"odinger bridge for SE. Additionally, we introduce\na novel auxiliary loss, including a perceptual loss, into the original CTM's\ntraining framework. As a result, SBCTM achieves an approximately 16x\nimprovement in the real-time factor (RTF) compared to the conventional\nSchr\\\"odinger bridge for SE. Furthermore, the favorable trade-off between\nquality and speed in SBCTM allows for time-efficient inference by limiting\nmulti-step refinement to cases where 1-step inference is insufficient. Our\ncode, pretrained models, and audio samples are available at\nhttps://github.com/sony/sbctm/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11925v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12202", "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control", "authors": ["Anton Klenitskiy", "Konstantin Polev", "Daria Denisova", "Alexey Vasilev", "Dmitry Simakov", "Gleb Gusev"], "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12202v1", "summary": "Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12202v1", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12224", "title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This", "authors": ["Razvan Pascanu", "Clare Lyle", "Ionut-Vlad Modoranu", "Naima Elosegui Borras", "Dan Alistarh", "Petar Velickovic", "Sarath Chandar", "Soham De", "James Martens"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12224v1", "summary": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12224v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12125", "title": "Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers", "authors": ["Yi-Kuan Hsieh", "Jun-Wei Hsieh", "Xin Li", "Yu-Ming Chang", "Yu-Chee Tseng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12125v1", "summary": "Vision Transformer (ViT) has achieved impressive results across various\nvision tasks, yet its high computational cost limits practical applications.\nRecent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning\nunimportant tokens. However, these techniques often sacrifice accuracy by\nindependently pruning query (Q) and key (K) tokens, leading to performance\ndegradation due to overlooked token interactions. To address this limitation,\nwe introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for\nefficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.\nUnlike previous methods that consider only a single direction, our approach\nevaluates each token and its neighbors to decide which tokens to retain by\ntaking token interaction into account. The retained tokens are compressed\nthrough a similarity fusion step, preserving key information while reducing\ncomputational costs. The shared weights of Q/K tokens create a symmetric\nattention matrix, allowing pruning only the upper triangular part for speed up.\nBSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning\nlevels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%\non DeiT-S, while reducing computational overhead by 50%. It achieves 40%\nspeedup with improved accuracy across various ViTs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12125v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12452", "title": "A single chip 1.024 Tb/s silicon photonics PAM4 receiver", "authors": ["Ali Pirmoradi", "Han Hao", "Kaisarbek Omirzakhov", "Alexander J. Geers", "Firooz Aflatouni"], "categories": ["physics.optics", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      29 pages, 5 main figures, 4 extended data figures, 1 extended data table", "url": "http://arxiv.org/abs/2507.12452v1", "summary": "Energy-efficient high-bandwidth interconnects play a key role in computing\nsystems. Advances in silicon photonic electro-optic modulators and wavelength\nselective components have enabled the utilization of\nwavelength-division-multiplexing (WDM) in integrated optical transceivers,\noffering a high data-rate operation while achieving enhanced energy efficiency,\nbandwidth density, scalability, and the reach required for data-centers. Here,\nwe report the demonstration of a single chip optical WDM PAM4 receiver, where\nby co-integration of a 32-channel optical demultiplexer (O-DeMux) with\nautonomous wavelength tuning and locking at a near-zero power consumption and a\n32-channel ultra-low power concurrent electrical detection system, a record\nchip energy efficiency of under 0.38 pJ/bit is measured. The implemented 32\nchannel monolithic WDM optical receiver chip achieves an end-to-end latency of\nunder 100 ps and a bit-error-rate of less than 10-12 with no equalization,\npre-distortion, or digital-signal-processing, while operating at 1.024 Tb/s\naggregate data-rate on a single input fiber, the largest reported data-rate for\na WDM PAM4 receiver chip to date. The receiver bandwidth density of more than\n3.55 Tb/s/mm2 corresponds to more than an order-of-magnitude larger bandwidth\ndensity-energy efficiency product compared to the state-of-the-art optical PAM4\nreceivers for beyond 100Gb/s links. The chip, integrated using GlobalFoundries\n45CLO CMOS-photonic process, can be used for implementation of energy-efficient\nhigh data-rate optical links for AI applications.", "comment": "29 pages, 5 main figures, 4 extended data figures, 1 extended data\n  table", "pdf_url": "http://arxiv.org/pdf/2507.12452v1", "cate": "physics.optics", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.04074", "title": "Space Surveillance with High-Frequency Radar", "authors": ["Brendan Hennessy", "Heath Yardley", "Rob Debnam", "Tristan A. Camilleri", "Nicholas K. Spencer", "David A. Holdsworth", "Goeff Warne", "Brian Cheung", "Sergey Kharabash"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04074v2", "summary": "High-Frequency (HF) radar is well suited to the surveillance of\nlow-Earth-orbit space. For large targets, a small deployable HF radar is able\nto match the detection performance of much larger space surveillance radar\nsystems operating at higher frequencies. However, there are some unique\nchallenges associated with the use of HF, including the range--Doppler coupling\nbias, coarse detection-level localisation, and the presence of meteor returns\nand other unwanted signals. This paper details the use of HF radar for space\nsurveillance, including signal processing and radar product formation,\ntracking, ionospheric correction, and orbit determination. It is shown that by\nfusing measurements from multiple passes, accurate orbital estimates can be\nobtained. Included are results from recent SpaceFest trials of the Defence\nScience and Technology Group's HF space surveillance radar, achieving real-time\nwide-area surveillance in tracking, orbit determination, and cueing of other\nspace surveillance sensors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04074v2", "cate": "eess.SP", "date": "2025-04-05", "updated": "2025-07-16"}
{"id": "2412.16425", "title": "Patherea: Cell Detection and Classification for the 2020s", "authors": ["Dejan Štepec", "Maja Jerše", "Snežana Đokić", "Jera Jeruc", "Nina Zidar", "Danijel Skočaj"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to Medical Image Analysis", "url": "http://arxiv.org/abs/2412.16425v2", "summary": "We present Patherea, a unified framework for point-based cell detection and\nclassification that enables the development and fair evaluation of\nstate-of-the-art methods. To support this, we introduce a large-scale dataset\nthat replicates the clinical workflow for Ki-67 proliferation index estimation.\nOur method directly predicts cell locations and classes without relying on\nintermediate representations. It incorporates a hybrid Hungarian matching\nstrategy for accurate point assignment and supports flexible backbones and\ntraining regimes, including recent pathology foundation models. Patherea\nachieves state-of-the-art performance on public datasets - Lizard, BRCA-M2C,\nand BCData - while highlighting performance saturation on these benchmarks. In\ncontrast, our newly proposed Patherea dataset presents a significantly more\nchallenging benchmark. Additionally, we identify and correct common errors in\ncurrent evaluation protocols and provide an updated benchmarking utility for\nstandardized assessment. The Patherea dataset and code are publicly available\nto facilitate further research and fair comparisons.", "comment": "Submitted to Medical Image Analysis", "pdf_url": "http://arxiv.org/pdf/2412.16425v2", "cate": "eess.IV", "date": "2024-12-21", "updated": "2025-07-16"}
{"id": "2507.12090", "title": "MambaRate: Speech Quality Assessment Across Different Sampling Rates", "authors": ["Panos Kakoulidis", "Iakovi Alexiou", "Junkwang Oh", "Gunu Jho", "Inchul Hwang", "Pirros Tsiakoulis", "Aimilios Chalamandaris"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Submitted to ASRU 2025 (AudioMOS Challenge 2025 Track 3)", "url": "http://arxiv.org/abs/2507.12090v1", "summary": "We propose MambaRate, which predicts Mean Opinion Scores (MOS) with limited\nbias regarding the sampling rate of the waveform under evaluation. It is\ndesigned for Track 3 of the AudioMOS Challenge 2025, which focuses on\npredicting MOS for speech in high sampling frequencies. Our model leverages\nself-supervised embeddings and selective state space modeling. The target\nratings are encoded in a continuous representation via Gaussian radial basis\nfunctions (RBF). The results of the challenge were based on the system-level\nSpearman's Rank Correllation Coefficient (SRCC) metric. An initial MambaRate\nversion (T16 system) outperformed the pre-trained baseline (B03) by ~14% in a\nfew-shot setting without pre-training. T16 ranked fourth out of five in the\nchallenge, differing by ~6% from the winning system. We present additional\nresults on the BVCC dataset as well as ablations with different representations\nas input, which outperform the initial T16 version.", "comment": "Submitted to ASRU 2025 (AudioMOS Challenge 2025 Track 3)", "pdf_url": "http://arxiv.org/pdf/2507.12090v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12242", "title": "Looking for Fairness in Recommender Systems", "authors": ["Cécile Logé"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12242v1", "summary": "Recommender systems can be found everywhere today, shaping our everyday\nexperience whenever we're consuming content, ordering food, buying groceries\nonline, or even just reading the news. Let's imagine we're in the process of\nbuilding a recommender system to make content suggestions to users on social\nmedia. When thinking about fairness, it becomes clear there are several\nperspectives to consider: the users asking for tailored suggestions, the\ncontent creators hoping for some limelight, and society at large, navigating\nthe repercussions of algorithmic recommendations. A shared fairness concern\nacross all three is the emergence of filter bubbles, a side-effect that takes\nplace when recommender systems are almost \"too good\", making recommendations so\ntailored that users become inadvertently confined to a narrow set of\nopinions/themes and isolated from alternative ideas. From the user's\nperspective, this is akin to manipulation. From the small content creator's\nperspective, this is an obstacle preventing them access to a whole range of\npotential fans. From society's perspective, the potential consequences are\nfar-reaching, influencing collective opinions, social behavior and political\ndecisions. How can our recommender system be fine-tuned to avoid the creation\nof filter bubbles, and ensure a more inclusive and diverse content landscape?\nApproaching this problem involves defining one (or more) performance metric to\nrepresent diversity, and tweaking our recommender system's performance through\nthe lens of fairness. By incorporating this metric into our evaluation\nframework, we aim to strike a balance between personalized recommendations and\nthe broader societal goal of fostering rich and varied cultures and points of\nview.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12242v1", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12257", "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws", "authors": ["Matteo Tusoni", "Giuseppe Masi", "Andrea Coletta", "Aldo Glielmo", "Viviana Arrigoni", "Novella Bartolini"], "categories": ["cs.LG", "physics.data-an", "stat.ML", "stat.OT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12257v1", "summary": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12257v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12135", "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement", "authors": ["Junyu Lou", "Xiaorui Zhao", "Kexuan Shi", "Shuhang Gu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.12135v1", "summary": "Deep learning-based bilateral grid processing has emerged as a promising\nsolution for image enhancement, inherently encoding spatial and intensity\ninformation while enabling efficient full-resolution processing through slicing\noperations. However, existing approaches are limited to linear affine\ntransformations, hindering their ability to model complex color relationships.\nMeanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,\ntraditional MLP-based methods employ globally shared parameters, which is hard\nto deal with localized variations. To overcome these dual challenges, we\npropose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)\nframework. Our approach synergizes the spatial modeling of bilateral grids with\nthe non-linear capabilities of MLPs. Specifically, we generate bilateral grids\ncontaining MLP parameters, where each pixel dynamically retrieves its unique\ntransformation parameters and obtain a distinct MLP for color mapping based on\nspatial coordinates and intensity values. In addition, we propose a novel grid\ndecomposition strategy that categorizes MLP parameters into distinct types\nstored in separate subgrids. Multi-channel guidance maps are used to extract\ncategory-specific parameters from corresponding subgrids, ensuring effective\nutilization of color information during slicing while guiding precise parameter\ngeneration. Extensive experiments on public datasets demonstrate that our\nmethod outperforms state-of-the-art methods in performance while maintaining\nreal-time processing capabilities.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12135v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2407.20814", "title": "Embracing Fairness in Consumer Electricity Markets using an Automatic Market Maker", "authors": ["Shaun Sweeney", "Chris King", "Mark O'Malley", "Robert Shorten"], "categories": ["eess.SY", "cs.GT", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Invalid technical approach - this could mislead future researchers, will resubmit a new version when fixed", "url": "http://arxiv.org/abs/2407.20814v2", "summary": "As consumer flexibility becomes expected, it is important that the market\nmechanisms which attain that flexibility are perceived as fair. We set out\nfairness issues in energy markets today, and propose a market design to address\nthem. Consumption is categorised as either essential or flexible with different\nprices and reliability levels for each. Prices are generated by an Automatic\nMarket Maker (AMM) based on instantaneous scarcity and resource is allocated\nusing a novel Fair Play algorithm. We empirically show the performance of the\nsystem over 1 year for 101 UK households and benchmark its performance against\nmore classical approaches.", "comment": "Invalid technical approach - this could mislead future researchers,\n  will resubmit a new version when fixed", "pdf_url": "http://arxiv.org/pdf/2407.20814v2", "cate": "eess.SY", "date": "2024-07-30", "updated": "2025-07-16"}
{"id": "2507.09458", "title": "An Energy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation", "authors": ["Ning Wang", "Chenyu Zhang", "Yanshi Sun", "Minghui Min", "Yuanwei Liu", "Shiyin Li"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is Paper-TW-Jul-25-1790", "url": "http://arxiv.org/abs/2507.09458v2", "summary": "Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which\neffectively utilizes both NOMA and orthogonal multiple access (OMA)\ntechnologies through flexible resource allocation in a single transmission, has\ndemonstrated immense potential for enhancing the performance of wireless\ncommunication systems. To further release the potential of HNOMA, this paper\nproposes a novel design of H-NOMA which jointly incorporates hybrid successive\ninterference cancellation (HSIC) and power adaptation (PA) in the NOMA\ntransmission phase. To reveal the potential of the proposed HSIC-PA aided\nH-NOMA scheme, closed-form expression for the probability of the event that\nH-NOMA can achieve a higher data rate than pure OMA by consuming less energy is\nrigorously derived. Furthermore, the asymptotic analysis demonstrates that the\nprobability of the proposed H-NOMA scheme approaches 1 in the high\nsignal-to-noise ratio (SNR) regime without any constraints on either users'\ntarget rates or transmit power ratios. This represents a significant\nimprovement over conventional H-NOMA schemes, which require specific\nrestrictive conditions to achieve probability 1 at high SNRs as shown in\nexisting work. The above observation indicates that with less energy\nconsumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate\nthan pure OMA with probability 1 at high SNRs, and hence a higher energy\nefficiency. Finally, numerical results are provided to verify the accuracy of\nthe analysis and also demonstrate the superior performance of the proposed\nH-NOMA scheme.", "comment": "13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is\n  Paper-TW-Jul-25-1790", "pdf_url": "http://arxiv.org/pdf/2507.09458v2", "cate": "eess.SP", "date": "2025-07-13", "updated": "2025-07-16"}
{"id": "2502.15186", "title": "LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement", "authors": ["Namrah Siddiqua", "Kim Suneung", "Seong-Whan Lee"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures. Accepted to the 2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Vienna, Austria", "url": "http://arxiv.org/abs/2502.15186v2", "summary": "Low-light image enhancement (LLIE) is a crucial task in computer vision aimed\nat enhancing the visual fidelity of images captured under low-illumination\nconditions. Conventional methods frequently struggle with noise, overexposure,\nand color distortion, leading to significant image quality degradation. To\naddress these challenges, we propose LUMINA-Net, an unsupervised deep learning\nframework that learns adaptive priors from low-light image pairs by integrating\nmulti-stage illumination and reflectance modules. To assist the Retinex\ndecomposition, inappropriate features in the raw image can be removed using a\nsimple self-supervised mechanism. First, the illumination module intelligently\nadjusts brightness and contrast while preserving intricate textural details.\nSecond, the reflectance module incorporates a noise reduction mechanism that\nleverages spatial attention and channel-wise feature refinement to mitigate\nnoise contamination. Through extensive experiments on LOL and SICE datasets,\nevaluated using PSNR, SSIM, and LPIPS metrics, LUMINA-Net surpasses\nstate-of-the-art methods, demonstrating its efficacy in low-light image\nenhancement.", "comment": "6 pages, 4 figures. Accepted to the 2025 IEEE International\n  Conference on Systems, Man, and Cybernetics (SMC), Vienna, Austria", "pdf_url": "http://arxiv.org/pdf/2502.15186v2", "cate": "eess.IV", "date": "2025-02-21", "updated": "2025-07-16"}
{"id": "2507.12136", "title": "Room Impulse Response Generation Conditioned on Acoustic Parameters", "authors": ["Silvia Arellano", "Chunghsin Yeh", "Gautam Bhattacharya", "Daniel Arteaga"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      4+1 pages, 2 figures; accepted in IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2025)", "url": "http://arxiv.org/abs/2507.12136v1", "summary": "The generation of room impulse responses (RIRs) using deep neural networks\nhas attracted growing research interest due to its applications in virtual and\naugmented reality, audio postproduction, and related fields. Most existing\napproaches condition generative models on physical descriptions of a room, such\nas its size, shape, and surface materials. However, this reliance on geometric\ninformation limits their usability in scenarios where the room layout is\nunknown or when perceptual realism (how a space sounds to a listener) is more\nimportant than strict physical accuracy. In this study, we propose an\nalternative strategy: conditioning RIR generation directly on a set of RIR\nacoustic parameters. These parameters include various measures of reverberation\ntime and direct sound to reverberation ratio, both broadband and bandwise. By\nspecifying how the space should sound instead of how it should look, our method\nenables more flexible and perceptually driven RIR generation. We explore both\nautoregressive and non-autoregressive generative models operating in the\nDescript Audio Codec domain, using either discrete token sequences or\ncontinuous embeddings. Specifically, we have selected four models to evaluate:\nan autoregressive transformer, the MaskGIT model, a flow matching model, and a\nclassifier-based approach. Objective and subjective evaluations are performed\nto compare these methods with state-of-the-art alternatives. Results show that\nthe proposed models match or outperform state-of-the-art alternatives, with the\nMaskGIT model achieving the best performance.", "comment": "4+1 pages, 2 figures; accepted in IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA 2025)", "pdf_url": "http://arxiv.org/pdf/2507.12136v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12252", "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "authors": ["Shilin Zhou", "Zhenghua Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12252v1", "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12252v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12262", "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters", "authors": ["Zachary James", "Joseph Guinness"], "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12262v1", "summary": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12262v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12137", "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving", "authors": ["Jiawei Xu", "Kai Deng", "Zexin Fan", "Shenlong Wang", "Jin Xie", "Jian Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.12137v1", "summary": "Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12137v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.04202", "title": "Observability and Generalized Sensor Placement for Nonlinear Quality Models in Drinking Water Networks", "authors": ["Mohamad H. Kazma", "Salma M. Elsherif", "Ahmad F. Taha"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.04202v4", "summary": "This paper studies the problem of optimal placement of water quality (WQ)\nsensors in water distribution networks (WDNs), with a focus on chlorine\ntransport, decay, and reaction models. Such models are traditionally used as\nsuitable proxies for WQ. The literature on this topic is inveterate, but has a\nkey limitation: it utilizes simplified single-species decay and reaction models\nthat do not capture WQ transients for nonlinear, multi-species interactions.\nThis results in sensor placements (SP) that do not account for nonlinear WQ\ndynamics. Furthermore, as WQ simulations are parameterized by hydraulic\nprofiles and demand patterns, the placement of sensors are often\nhydraulics-dependent. This study produces a greedy algorithm that addresses the\ntwo aforementioned limitations. The algorithm is grounded in nonlinear dynamic\nsystems and observability theory, and yields SPs that are submodular and robust\nto hydraulic changes. Case studies on benchmark water networks are provided.\nThe key findings provide practical recommendations for WDN operators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.04202v4", "cate": "eess.SY", "date": "2024-11-06", "updated": "2025-07-15"}
{"id": "2505.13017", "title": "Optimal Scalogram for Computational Complexity Reduction in Acoustic Recognition Using Deep Learning", "authors": ["Dang Thoai Phan", "Tuan Anh Huynh", "Van Tuan Pham", "Cao Minh Tran", "Van Thuan Mai", "Ngoc Quy Tran"], "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13017v4", "summary": "The Continuous Wavelet Transform (CWT) is an effective tool for feature\nextraction in acoustic recognition using Convolutional Neural Networks (CNNs),\nparticularly when applied to non-stationary audio. However, its high\ncomputational cost poses a significant challenge, often leading researchers to\nprefer alternative methods such as the Short-Time Fourier Transform (STFT). To\naddress this issue, this paper proposes a method to reduce the computational\ncomplexity of CWT by optimizing the length of the wavelet kernel and the hop\nsize of the output scalogram. Experimental results demonstrate that the\nproposed approach significantly reduces computational cost while maintaining\nthe robust performance of the trained model in acoustic recognition tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13017v4", "cate": "eess.AS", "date": "2025-05-19", "updated": "2025-07-16"}
{"id": "2503.05063", "title": "Lightweight Hypercomplex MRI Reconstruction: A Generalized Kronecker-Parameterized Approach", "authors": ["Haosen Zhang", "Jiahao Huang", "Yinzhe Wu", "Congren Dai", "Fanwen Wang", "Zhenxuan Zhang", "Guang Yang"], "categories": ["eess.IV", "cs.CV", "I.2.6; I.4.5"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures. Submitted for publication", "url": "http://arxiv.org/abs/2503.05063v3", "summary": "Magnetic Resonance Imaging (MRI) is crucial for clinical diagnostics but is\nhindered by prolonged scan times. Current deep learning models enhance MRI\nreconstruction but are often memory-intensive and unsuitable for\nresource-limited systems. This paper introduces a lightweight MRI\nreconstruction model leveraging Kronecker-Parameterized Hypercomplex Neural\nNetworks to achieve high performance with reduced parameters. By integrating\nKronecker-based modules, including Kronecker MLP, Kronecker Window Attention,\nand Kronecker Convolution, the proposed model efficiently extracts spatial\nfeatures while preserving representational power. We introduce Kronecker U-Net\nand Kronecker SwinMR, which maintain high reconstruction quality with\napproximately 50% fewer parameters compared to existing models. Experimental\nevaluation on the FastMRI dataset demonstrates competitive PSNR, SSIM, and\nLPIPS metrics, even at high acceleration factors (8x and 16x), with no\nsignificant performance drop. Additionally, Kronecker variants exhibit superior\ngeneralization and reduced overfitting on limited datasets, facilitating\nefficient MRI reconstruction on hardware-constrained systems. This approach\nsets a new benchmark for parameter-efficient medical imaging models.", "comment": "11 pages, 3 figures. Submitted for publication", "pdf_url": "http://arxiv.org/pdf/2503.05063v3", "cate": "eess.IV", "date": "2025-03-07", "updated": "2025-07-15"}
{"id": "2507.12175", "title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection", "authors": ["Sungkyun Chang", "Simon Dixon", "Emmanouil Benetos"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to WASPAA 2025", "url": "http://arxiv.org/abs/2507.12175v1", "summary": "This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.", "comment": "Accepted to WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.12175v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12261", "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Submitted to EMNLP 2025 System Demonstrations | Code: this https URL | Video: this https URL | Demo: this https URL | HuggingFace Spaces: this https URL", "url": "http://arxiv.org/abs/2507.12261v1", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "pdf_url": "http://arxiv.org/pdf/2507.12261v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12297", "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12297v1", "summary": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12297v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12138", "title": "Neural Human Pose Prior", "authors": ["Michal Heker", "Sefy Kararlitsky", "David Tolpin"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.12138v1", "summary": "We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.12138v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.13140", "title": "Robust Convergency Indicator using MIMO-PI Controller in the presence of disturbances", "authors": ["Zimao Sheng", "Hongan Yang", "Jiakang Wang", "Tong Zhang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      15 pages, 10 figures", "url": "http://arxiv.org/abs/2411.13140v2", "summary": "The PID controller remains the most widely adopted control architecture, with\ngroundbreaking success across extensive implications. However, optimal\nparameter tuning for PID controller remains a critical challenge. Existing\ntheories predominantly focus on linear time-invariant systems and Single-Input\nSingle-Output (SISO) scenarios, leaving a research gap in addressing complex\nPID control problems for Multi-Input Multi-Output (MIMO) nonlinear systems with\ndisturbances. This study enhances controller robustness by leveraging insights\ninto the velocity form of nonlinear systems. It establishes a quantitative\nmetric to evaluate the robustness of MIMO-PI controller, clarifies key theories\non how robustness influences exponential error stabilization. Guided by these\ntheories, an optimal robust MIMO-PI controller is developed without\noversimplifying assumptions. Experimental results demonstrate that the\ncontroller achieves effective exponential stabilization and exhibits\nexceptional robustness under the guidance of the proposed robust indicator.\nNotably, the robust convergence indicator can also effectively assess\ncomprehensive performance.", "comment": "15 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2411.13140v2", "cate": "eess.SY", "date": "2024-11-20", "updated": "2025-07-16"}
{"id": "2506.01588", "title": "Learning Perceptually Relevant Temporal Envelope Morphing", "authors": ["Satvik Dixit", "Sungjoon Park", "Chris Donahue", "Laurie M. Heller"], "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at WASPAA 2025", "url": "http://arxiv.org/abs/2506.01588v2", "summary": "Temporal envelope morphing, the process of interpolating between the\namplitude dynamics of two audio signals, is an emerging problem in generative\naudio systems that lacks sufficient perceptual grounding. Morphing of temporal\nenvelopes in a perceptually intuitive manner should enable new methods for\nsound blending in creative media and for probing perceptual organization in\npsychoacoustics. However, existing audio morphing techniques often fail to\nproduce intermediate temporal envelopes when input sounds have distinct\ntemporal structures; many morphers effectively overlay both temporal\nstructures, leading to perceptually unnatural results. In this paper, we\nintroduce a novel workflow for learning envelope morphing with perceptual\nguidance: we first derive perceptually grounded morphing principles through\nhuman listening studies, then synthesize large-scale datasets encoding these\nprinciples, and finally train machine learning models to create perceptually\nintermediate morphs. Specifically, we present: (1) perceptual principles that\nguide envelope morphing, derived from our listening studies, (2) a supervised\nframework to learn these principles, (3) an autoencoder that learns to compress\ntemporal envelope structures into latent representations, and (4) benchmarks\nfor evaluating audio envelope morphs, using both synthetic and naturalistic\ndata, and show that our approach outperforms existing methods in producing\ntemporally intermediate morphs. All code, models, and datasets will be made\npublicly available upon publication.", "comment": "Accepted at WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2506.01588v2", "cate": "cs.SD", "date": "2025-06-02", "updated": "2025-07-16"}
{"id": "2506.08677", "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "authors": ["Milica Škipina", "Nikola Jovišić", "Nicola Dall'Asen", "Vanja Švenda", "Anil Osman Tur", "Slobodan Ilić", "Elisa Ricci", "Dubravko Ćulibrk"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      21 pages, 14 figures, 7 tables This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2506.08677v2", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final model,\nsignificantly aiding the noise removal process. This design enables MAMBO to\ngenerate highly realistic mammograms of up to 3840x3840 pixels. Importantly,\nthis approach can be used to enhance the training of classification models and\nextended to anomaly segmentation. Experiments, both numerical and radiologist\nvalidation, assess MAMBO's capabilities in image generation, super-resolution,\nand anomaly segmentation, highlighting its potential to enhance mammography\nanalysis for more accurate diagnoses and earlier lesion detection. The source\ncode used in this study is publicly available at:\nhttps://github.com/iai-rs/mambo.", "comment": "21 pages, 14 figures, 7 tables This work has been submitted to the\n  IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.08677v2", "cate": "eess.IV", "date": "2025-06-10", "updated": "2025-07-16"}
{"id": "2507.12217", "title": "Towards few-shot isolated word reading assessment", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "categories": ["cs.CL", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to SLaTE 2025", "url": "http://arxiv.org/abs/2507.12217v1", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "comment": "Accepted to SLaTE 2025", "pdf_url": "http://arxiv.org/pdf/2507.12217v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11681", "title": "Finite Pinwheel Scheduling: the k-Visits Problem", "authors": ["Sotiris Kanellopoulos", "Christos Pergaminelis", "Maria Kokkou", "Euripides Markou", "Aris Pagourtzis"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11681v1", "summary": "Pinwheel Scheduling is a fundamental scheduling problem, in which each task\n$i$ is associated with a positive integer $d_i$, and the objective is to\nschedule one task per time slot, ensuring each task perpetually appears at\nleast once in every $d_i$ time slots. Although conjectured to be\nPSPACE-complete, it remains open whether Pinwheel Scheduling is NP-hard (unless\na compact input encoding is used) or even contained in NP.\n  We introduce k-Visits, a finite version of Pinwheel Scheduling, where given n\ndeadlines, the goal is to schedule each task exactly k times. While we observe\nthat the 1-Visit problem is trivial, we prove that 2-Visits is strongly\nNP-complete through a surprising reduction from Numerical 3-Dimensional\nMatching (N3DM). As intermediate steps in the reduction, we define NP-complete\nvariants of N3DM which may be of independent interest. We further extend our\nstrong NP-hardness result to a generalization of k-Visits $k\\geq 2$ in which\nthe deadline of each task may vary throughout the schedule, as well as to a\nsimilar generalization of Pinwheel Scheduling, thus making progress towards\nsettling the complexity of Pinwheel Scheduling.\n  Additionally, we prove that 2-Visits can be solved in linear time if all\ndeadlines are distinct, rendering it one of the rare natural problems which\nexhibit the interesting dichotomy of being in P if their input is a set and\nNP-complete if the input is a multiset. We achieve this through a Turing\nreduction from 2-Visits to a variation of N3DM, which we call Position\nMatching. Based on this reduction, we also show an FPT algorithm for 2-Visits\nparameterized by a value related to how close the input deadlines are to each\nother, as well as a linear-time algorithm for instances with up to two distinct\ndeadlines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11681v1", "cate": "cs.DS", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12269", "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "authors": ["Sybelle Goedicke-Fritz", "Michelle Bous", "Annika Engel", "Matthias Flotho", "Pascal Hirsch", "Hannah Wittig", "Dino Milanovic", "Dominik Mohr", "Mathias Kaspar", "Sogand Nemat", "Dorothea Kerner", "Arno Bücker", "Andreas Keller", "Sascha Meyer", "Michael Zemlin", "Philipp Flotho"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      S.G.-F., M.B., and A.E. contributed equally to this work and share first authorship. M.Z. and P.F. contributed equally to this work and share senior authorship", "url": "http://arxiv.org/abs/2507.12269v1", "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship", "pdf_url": "http://arxiv.org/pdf/2507.12269v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12305", "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.12305v1", "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12305v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12157", "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation", "authors": ["Edwin Arkel Rios", "Fernando Mikael", "Oswin Gosal", "Femiloye Oyerinde", "Hao-Chun Liang", "Bo-Cheng Lai", "Min-Chun Hu"], "categories": ["cs.CV", "I.2; I.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Main: 10 pages, 2 figures, 4 tables", "url": "http://arxiv.org/abs/2507.12157v1", "summary": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.", "comment": "Main: 10 pages, 2 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.12157v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2503.17483", "title": "Sharp Hybrid Zonotopes: Set Operations and the Reformulation-linearization Technique", "authors": ["Jonah J. Glunt", "Joshua A. Robbins", "Jacob A. Siefert", "Daniel Silvestre", "Herschel C. Pangborn"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17483v2", "summary": "Mixed integer set representations, and specifically hybrid zonotopes, have\nenabled new techniques for reachability and verification of nonlinear and\nhybrid systems. Mixed-integer sets which have the property that their convex\nrelaxation is equal to their convex hull are said to be sharp. This property\nallows the convex hull to be computed with minimal overhead, and is known to be\nimportant for improving the convergence rates of mixed-integer optimization\nalgorithms that rely on convex relaxations. This paper examines methods for\nformulating sharp hybrid zonotopes and provides sharpness-preserving methods\nfor performing several key set operations. The paper then shows how the\nreformulation-linearization technique can be applied to create a sharp\nrealization of a hybrid zonotope that is initially not sharp. A numerical\nexample applies this technique to find the convex hull of a level set of a\nfeedforward ReLU neural network.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17483v2", "cate": "eess.SY", "date": "2025-03-21", "updated": "2025-07-16"}
{"id": "2506.22790", "title": "ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge", "authors": ["Yixu Chen", "Bowen Chen", "Hai Wei", "Alan C. Bovik", "Baojun Li", "Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Dounia Hammou", "Fei Yin", "Rafal Mantiuk", "Amritha Premkumar", "Prajit T Rajendran", "Vignesh V Menon"], "categories": ["eess.IV", "cs.CV", "cs.MM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      ICME 2025 Grand Challenges", "url": "http://arxiv.org/abs/2506.22790v2", "summary": "This paper reports IEEE International Conference on Multimedia \\& Expo (ICME)\n2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.\nWith the rapid development of video technology, especially High Dynamic Range\n(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and\ngeneralizable Video Quality Assessment (VQA) methods has become increasingly\ndemanded. Existing VQA models often struggle to deliver consistent performance\nacross varying dynamic ranges, distortion types, and diverse content. This\nchallenge was established to benchmark and promote VQA approaches capable of\njointly handling HDR and SDR content. In the final evaluation phase, five teams\nsubmitted seven models along with technical reports to the Full Reference (FR)\nand No Reference (NR) tracks. Among them, four methods outperformed VMAF\nbaseline, while the top-performing model achieved state-of-the-art performance,\nsetting a new benchmark for generalizable video quality assessment.", "comment": "ICME 2025 Grand Challenges", "pdf_url": "http://arxiv.org/pdf/2506.22790v2", "cate": "eess.IV", "date": "2025-06-28", "updated": "2025-07-15"}
{"id": "2505.07615", "title": "Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models", "authors": ["Riccardo Passoni", "Francesca Ronchini", "Luca Comanducci", "Romain Serizel", "Fabio Antonacci"], "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted at WASPAA 2025", "url": "http://arxiv.org/abs/2505.07615v2", "summary": "Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models.", "comment": "Accepted at WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2505.07615v2", "cate": "eess.AS", "date": "2025-05-12", "updated": "2025-07-16"}
{"id": "2507.11724", "title": "Approaching Optimality for Solving Dense Linear Systems with Low-Rank Structure", "authors": ["Michał Dereziński", "Aaron Sidford"], "categories": ["cs.DS", "cs.NA", "math.NA", "math.OC", "stat.ML"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11724v1", "summary": "We provide new high-accuracy randomized algorithms for solving linear systems\nand regression problems that are well-conditioned except for $k$ large singular\nvalues. For solving such $d \\times d$ positive definite system our algorithms\nsucceed whp. and run in time $\\tilde O(d^2 + k^\\omega)$. For solving such\nregression problems in a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$ our\nmethods succeed whp. and run in time $\\tilde O(\\mathrm{nnz}(\\mathbf{A}) + d^2 +\nk^\\omega)$ where $\\omega$ is the matrix multiplication exponent and\n$\\mathrm{nnz}(\\mathbf{A})$ is the number of non-zeros in $\\mathbf{A}$. Our\nmethods nearly-match a natural complexity limit under dense inputs for these\nproblems and improve upon a trade-off in prior approaches that obtain running\ntimes of either $\\tilde O(d^{2.065}+k^\\omega)$ or $\\tilde O(d^2 +\ndk^{\\omega-1})$ for $d\\times d$ systems. Moreover, we show how to obtain these\nrunning times even under the weaker assumption that all but $k$ of the singular\nvalues have a suitably bounded generalized mean. Consequently, we give the\nfirst nearly-linear time algorithm for computing a multiplicative approximation\nto the nuclear norm of an arbitrary dense matrix. Our algorithms are built on\nthree general recursive preconditioning frameworks, where matrix sketching and\nlow-rank update formulas are carefully tailored to the problems' structure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11724v1", "cate": "cs.DS", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12295", "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "authors": ["Feng Xiao", "Jicong Fan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12295v1", "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12295v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12341", "title": "Nonlinear Concept Erasure: a Density Matching Approach", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      17 pages, 10 figures, accepted for publication in ECAI 2025 (28th European Conference on Artificial Intelligence)", "url": "http://arxiv.org/abs/2507.12341v1", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "pdf_url": "http://arxiv.org/pdf/2507.12341v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12177", "title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification", "authors": ["Zahid Ullah", "Dragan Pamucar", "Jihie Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12177v1", "summary": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable\ntool for detecting tumors due to its capability to produce detailed images that\nreveal their presence. However, the accuracy of diagnosis can be compromised\nwhen human specialists evaluate these images. Factors such as fatigue, limited\nexpertise, and insufficient image detail can lead to errors. For example, small\ntumors might go unnoticed, or overlap with healthy brain regions could result\nin misidentification. To address these challenges and enhance diagnostic\nprecision, this study proposes a novel double ensembling framework, consisting\nof ensembled pre-trained deep learning (DL) models for feature extraction and\nensembled fine-tuned hyperparameter machine learning (ML) models to efficiently\nclassify brain tumors. Specifically, our method includes extensive\npreprocessing and augmentation, transfer learning concepts by utilizing various\npre-trained deep convolutional neural networks and vision transformer networks\nto extract deep features from brain MRI, and fine-tune hyperparameters of ML\nclassifiers. Our experiments utilized three different publicly available Kaggle\nMRI brain tumor datasets to evaluate the pre-trained DL feature extractor\nmodels, ML classifiers, and the effectiveness of an ensemble of deep features\nalong with an ensemble of ML classifiers for brain tumor classification. Our\nresults indicate that the proposed feature fusion and classifier fusion improve\nupon the state of the art, with hyperparameter fine-tuning providing a\nsignificant enhancement over the ensemble method. Additionally, we present an\nablation study to illustrate how each component contributes to accurate brain\ntumor classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12177v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.15026", "title": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles: Minimizing Lane Changes and Merging", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15026v2", "summary": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced\nsafety and operational efficiency. However, frequent lane changes and merging\nmaneuvers continue to pose significant safety risks and disrupt traffic flow.\nThis paper introduces the Minimizing Lane Change Algorithm (MLCA), a\nstate-machine-based approach designed to reduce unnecessary lane changes,\nthereby enhancing both traffic safety and efficiency. The MLCA algorithm\nprioritizes maintaining lane stability unless safety-critical conditions\nnecessitate a lane change. The algorithm's effectiveness was evaluated through\nsimulations conducted on the SUMO platform, comparing its performance against\nestablished models, including LC2017 and MOBIL. Results demonstrate substantial\nreductions in lane changes and collisions, leading to smoother traffic flow and\nimproved safety metrics. Additionally, the study highlights the MLCA's\nadaptability to various traffic densities and roadway configurations,\nshowcasing its potential for wide-scale deployment in real-world AV systems.\nFuture work aims to validate these findings in more complex scenarios using the\nCARLA simulator, which will enable the testing of the algorithm under more\ndynamic and high-fidelity conditions, such as urban traffic environments with\ndiverse road users. Moreover, the integration of cybersecurity measures for\nvehicle-to-vehicle (V2V) communication will be explored to ensure robust and\nsecure data exchange, further enhancing the reliability and safety of AV\noperations. This research contributes to the broader goal of developing\nintelligent traffic systems that optimize both individual vehicle performance\nand overall traffic network efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15026v2", "cate": "eess.SY", "date": "2025-06-17", "updated": "2025-07-16"}
{"id": "2507.01881", "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs", "authors": ["Niccolò McConnell", "Pardeep Vasudev", "Daisuke Yamada", "Daryl Cheng", "Mehran Azimbagirad", "John McCabe", "Shahab Aslani", "Ahmed H. Shahin", "Yukun Zhou", "The SUMMIT Consortium", "Andre Altmann", "Yipeng Hu", "Paul Taylor", "Sam M. Janes", "Daniel C. Alexander", "Joseph Jacob"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01881v2", "summary": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening\n(LCS) programs is increasing in uptake worldwide. LCS programs herald a\ngenerational opportunity to simultaneously detect cancer and non-cancer-related\nearly-stage lung disease. Yet these efforts are hampered by a shortage of\nradiologists to interpret scans at scale. Here, we present TANGERINE, a\ncomputationally frugal, open-source vision foundation model for volumetric LDCT\nanalysis. Designed for broad accessibility and rapid adaptation, TANGERINE can\nbe fine-tuned off the shelf for a wide range of disease-specific tasks with\nlimited computational resources and training data. Relative to models trained\nfrom scratch, TANGERINE demonstrates fast convergence during fine-tuning,\nthereby requiring significantly fewer GPU hours, and displays strong label\nefficiency, achieving comparable or superior performance with a fraction of\nfine-tuning data. Pretrained using self-supervised learning on over 98,000\nthoracic LDCTs, including the UK's largest LCS initiative to date and 27 public\ndatasets, TANGERINE achieves state-of-the-art performance across 14 disease\nclassification tasks, including lung cancer and multiple respiratory diseases,\nwhile generalising robustly across diverse clinical centres. By extending a\nmasked autoencoder framework to 3D imaging, TANGERINE offers a scalable\nsolution for LDCT analysis, departing from recent closed, resource-intensive\nmodels by combining architectural simplicity, public availability, and modest\ncomputational requirements. Its accessible, open-source lightweight design lays\nthe foundation for rapid integration into next-generation medical imaging tools\nthat could transform LCS initiatives, allowing them to pivot from a singular\nfocus on lung cancer detection to comprehensive respiratory disease management\nin high-risk populations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01881v2", "cate": "eess.IV", "date": "2025-07-02", "updated": "2025-07-15"}
{"id": "2302.00646", "title": "Epic-Sounds: A Large-scale Dataset of Actions That Sound", "authors": ["Jaesung Huh", "Jacob Chalk", "Evangelos Kazakos", "Dima Damen", "Andrew Zisserman"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at TPAMI", "url": "http://arxiv.org/abs/2302.00646v3", "summary": "We introduce EPIC-SOUNDS, a large-scale dataset of audio annotations\ncapturing temporal extents and class labels within the audio stream of the\negocentric videos. We propose an annotation pipeline where annotators\ntemporally label distinguishable audio segments and describe the action that\ncould have caused this sound. We identify actions that can be discriminated\npurely from audio, through grouping these free-form descriptions of audio into\nclasses. For actions that involve objects colliding, we collect human\nannotations of the materials of these objects (e.g. a glass object being placed\non a wooden surface), which we verify from video, discarding ambiguities.\nOverall, EPIC-SOUNDS includes 78.4k categorised segments of audible events and\nactions, distributed across 44 classes as well as 39.2k non-categorised\nsegments. We train and evaluate state-of-the-art audio recognition and\ndetection models on our dataset, for both audio-only and audio-visual methods.\nWe also conduct analysis on: the temporal overlap between audio events, the\ntemporal and label correlations between audio and visual modalities, the\nambiguities in annotating materials from audio-only input, the importance of\naudio-only labels and the limitations of current models to understand actions\nthat sound.", "comment": "Accepted at TPAMI", "pdf_url": "http://arxiv.org/pdf/2302.00646v3", "cate": "cs.SD", "date": "2023-02-01", "updated": "2025-07-16"}
{"id": "2507.12047", "title": "Pathfinding in Self-Deleting Graphs", "authors": ["Michal Dvořák", "Dušan Knop", "Michal Opler", "Jan Pokorný", "Ondřej Suchý", "Krisztina Szilágyi"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12047v1", "summary": "In this paper, we study the problem of pathfinding on traversal-dependent\ngraphs, i.e., graphs whose edges change depending on the previously visited\nvertices. In particular, we study \\emph{self-deleting graphs}, introduced by\nCarmesin et al. (Sarah Carmesin, David Woller, David Parker, Miroslav Kulich,\nand Masoumeh Mansouri. The Hamiltonian cycle and travelling salesperson\nproblems with traversal-dependent edge deletion. J. Comput. Sci.), which\nconsist of a graph $G=(V, E)$ and a function $f\\colon V\\rightarrow 2^E$, where\n$f(v)$ is the set of edges that will be deleted after visiting the vertex $v$.\nIn the \\textsc{(Shortest) Self-Deleting $s$-$t$-path} problem we are given a\nself-deleting graph and its vertices $s$ and $t$, and we are asked to find a\n(shortest) path from $s$ to $t$, such that it does not traverse an edge in\n$f(v)$ after visiting $v$ for any vertex $v$.\n  We prove that \\textsc{Self-Deleting $s$-$t$-path} is NP-hard even if the\ngiven graph is outerplanar, bipartite, has maximum degree $3$, bandwidth $2$\nand $|f(v)|\\leq 1$ for each vertex $v$. We show that \\textsc{Shortest\nSelf-Deleting $s$-$t$-path} is W[1]-complete parameterized by the length of the\nsought path and that \\textsc{Self-Deleting $s$-$t$-path} is \\W{1}-complete\nparameterized by the vertex cover number, feedback vertex set number and\ntreedepth. We also show that the problem becomes FPT when we parameterize by\nthe maximum size of $f(v)$ and several structural parameters. Lastly, we show\nthat the problem does not admit a polynomial kernel even for parameterization\nby the vertex cover number and the maximum size of $f(v)$ combined already on\n2-outerplanar graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12047v1", "cate": "cs.DS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12318", "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "authors": ["Samuel Lavoie", "Michael Noukhovitch", "Aaron Courville"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      In submission, 22 pages, 7 tables, 12 figures", "url": "http://arxiv.org/abs/2507.12318v1", "summary": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.", "comment": "In submission, 22 pages, 7 tables, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.12318v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12380", "title": "Heat Kernel Goes Topological", "authors": ["Maximilian Krahn", "Vikas Garg"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12380v1", "summary": "Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12380v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12201", "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models", "authors": ["Yiqi Tian", "Pengfei Jin", "Mingze Yuan", "Na Li", "Bo Zeng", "Quanzheng Li"], "categories": ["cs.CV", "math.OC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12201v1", "summary": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12201v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.03205", "title": "BondMatcher: H-Bond Stability Analysis in Molecular Systems", "authors": ["Thomas Daniel", "Malgorzata Olejniczak", "Julien Tierny"], "categories": ["cs.LG", "eess.IV", "physics.chem-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in IEEE Transactions on Visualization and Computer Graphics (Proc. of IEEE VIS 2025)", "url": "http://arxiv.org/abs/2504.03205v2", "summary": "This application paper investigates the stability of hydrogen bonds\n(H-bonds), as characterized by the Quantum Theory of Atoms in Molecules\n(QTAIM). First, we contribute a database of 4544 electron densities associated\nto four isomers of water hexamers (the so-called Ring, Book, Cage and Prism),\ngenerated by distorting their equilibrium geometry under various structural\nperturbations, modeling the natural dynamic behavior of molecular systems.\nSecond, we present a new stability measure, called bond occurrence rate,\nassociating each bond path present at equilibrium with its rate of occurrence\nwithin the input ensemble. We also provide an algorithm, called BondMatcher,\nfor its automatic computation, based on a tailored, geometry-aware partial\nisomorphism estimation between the extremum graphs of the considered electron\ndensities. Our new stability measure allows for the automatic identification of\ndensities lacking H-bond paths, enabling further visual inspections.\nSpecifically, the topological analysis enabled by our framework corroborates\nexperimental observations and provides refined geometrical criteria for\ncharacterizing the disappearance of H-bond paths. Our electron density database\nand our C++ implementation are available at this address:\nhttps://github.com/thom-dani/BondMatcher.", "comment": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  (Proc. of IEEE VIS 2025)", "pdf_url": "http://arxiv.org/pdf/2504.03205v2", "cate": "cs.LG", "date": "2025-04-04", "updated": "2025-07-16"}
{"id": "2505.04457", "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE WASPAA2025", "url": "http://arxiv.org/abs/2505.04457v3", "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaveFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators.", "comment": "Accepted to IEEE WASPAA2025", "pdf_url": "http://arxiv.org/pdf/2505.04457v3", "cate": "cs.SD", "date": "2025-05-07", "updated": "2025-07-16"}
{"id": "2507.12130", "title": "Weighted $k$-Server Admits an Exponentially Competitive Algorithm", "authors": ["Adithya Bijoy", "Ankit Mondal", "Ashish Chiplunkar"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12130v1", "summary": "The weighted $k$-server is a variant of the $k$-server problem, where the\ncost of moving a server is the server's weight times the distance through which\nit moves. The problem is famous for its intriguing properties and for evading\nstandard techniques for designing and analyzing online algorithms. Even on\nuniform metric spaces with sufficiently many points, the deterministic\ncompetitive ratio of weighted $k$-server is known to increase doubly\nexponentially with respect to $k$, while the behavior of its randomized\ncompetitive ratio is not fully understood. Specifically, no upper bound better\nthan doubly exponential is known, while the best known lower bound is singly\nexponential in $k$. In this paper, we close the exponential gap between these\nbounds by giving an $\\exp(O(k^2))$-competitive randomized online algorithm for\nthe weighted $k$-server problem on uniform metrics, thus breaking the doubly\nexponential barrier for deterministic algorithms for the first time. This is\nachieved by a recursively defined notion of a phase which, on the one hand,\nforces a lower bound on the cost of any offline solution, while, on the other\nhand, also admits a randomized online algorithm with bounded expected cost. The\nalgorithm is also recursive; it involves running several algorithms virtually\nand in parallel and following the decisions of one of them in a random order.\nWe also show that our techniques can be lifted to construct an\n$\\exp(O(k^2))$-competitive randomized online algorithm for the generalized\n$k$-server problem on weighted uniform metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12130v1", "cate": "cs.DS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11794", "title": "Real-Time Cloth Simulation Using WebGPU: Evaluating Limits of High-Resolution", "authors": ["Nak-Jun Sung", "Jun Ma", "TaeHeon Kim", "Yoo-joo Choi", "Min-Hyung Choi", "Min Hong"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11794v1", "summary": "This study explores the capabilities of WebGPU, an emerging web graphics\nparadigm, for real-time cloth simulation. Traditional WebGL-based methods have\nbeen in handling complex physical simulations due to their emphasis on graphics\nrendering rather than general-purpose GPU (GPGPU) operations. WebGPU, designed\nto provide modern 3D graphics and computational capabilities, offers\nsignificant improvements through parallel processing and support for\ncomputational shaders. In this work, we implemented a cloth simulation system\nusing the Mass-Spring Method within the WebGPU framework, integrating collision\ndetection and response handling with the 3D surface model. First, comparative\nperformance evaluations demonstrate that WebGPU substantially outperforms\nWebGL, particularly in high-resolution simulations, maintaining 60 frames per\nsecond (fps) even with up to 640K nodes. The second experiment aimed to\ndetermine the real-time limitations of WebGPU and confirmed that WebGPU can\nhandle real-time collisions between 4K and 100k cloth node models and a 100K\ntriangle surface model in real-time. These experiments also highlight the\nimportance of balancing real-time performance with realistic rendering when\nhandling collisions between cloth models and complex 3D objects. Our source\ncode is available at https://github.com/nakjun/Cloth-Simulation-WebGPU", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11794v1", "cate": "cs.GR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12359", "title": "Cluster Contrast for Unsupervised Visual Representation Learning", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICIP 2025", "url": "http://arxiv.org/abs/2507.12359v1", "summary": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.", "comment": "ICIP 2025", "pdf_url": "http://arxiv.org/pdf/2507.12359v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12383", "title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation", "authors": ["Mohit Prashant", "Arvind Easwaran"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.12383v1", "summary": "In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.12383v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12232", "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM", "authors": ["Tao Chen", "Jingyi Zhang", "Decheng Liu", "Chunlei Peng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12232v1", "summary": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12232v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.15670", "title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model", "authors": ["Ke Hu", "Ehsan Hosseini-Asl", "Chen Chen", "Edresson Casanova", "Subhankar Ghosh", "Piotr Żelasko", "Zhehuai Chen", "Jason Li", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2505.15670v3", "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2505.15670v3", "cate": "cs.CL", "date": "2025-05-21", "updated": "2025-07-15"}
{"id": "2507.12304", "title": "A near-complete resolution of the exponential-time complexity of k-opt for the traveling salesman problem", "authors": ["Sophia Heimann", "Hung P. Hoang", "Stefan Hougardy"], "categories": ["cs.DS", "cs.DM", "68W25, 68W40, 68Q25, 90C27", "F.2.2; G.2.1; G.2.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      41 pages, 19 figures", "url": "http://arxiv.org/abs/2507.12304v1", "summary": "The $k$-opt algorithm is one of the simplest and most widely used heuristics\nfor solving the traveling salesman problem. Starting from an arbitrary tour,\nthe $k$-opt algorithm improves the current tour in each iteration by exchanging\nup to $k$ edges. The algorithm continues until no further improvement of this\nkind is possible. For a long time, it remained an open question how many\niterations the $k$-opt algorithm might require for small values of $k$,\nassuming the use of an optimal pivot rule. In this paper, we resolve this\nquestion for the cases $k = 3$ and $k = 4$ by proving that in both these cases\nan exponential number of iterations may be needed even if an optimal pivot rule\nis used. Combined with a recent result from Heimann, Hoang, and Hougardy (ICALP\n2024), this provides a complete answer for all $k \\geq 3$ regarding the number\nof iterations the $k$-opt algorithm may require under an optimal pivot rule. In\naddition we establish an analogous exponential lower bound for the 2.5-opt\nalgorithm, a variant that generalizes 2-opt and is a restricted version of\n3-opt. All our results hold for both the general and the metric traveling\nsalesman problem.", "comment": "41 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2507.12304v1", "cate": "cs.DS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11971", "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "authors": ["Tielong Wang", "Yuxuan Xiong", "Jinfan Liu", "Zhifan Zhang", "Ye Chen", "Yue Shi", "Bingbing Ni"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11971v1", "summary": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11971v1", "cate": "cs.GR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12366", "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization", "authors": ["Yifei Zhou", "Xuchu Huang", "Chenyu Ni", "Min Zhou", "Zheyu Yan", "Xunzhao Yin", "Cheng Zhuo"], "categories": ["cs.SC", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures, 2 tables, to be published in the 62nd DAC (Design Automation Conference) proceedings", "url": "http://arxiv.org/abs/2507.12366v1", "summary": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical\nanalysis and reasoning. Hyperdimensional Computing (HDC), a promising\nbrain-inspired computational model, is integral to neuro-symbolic AI. Various\nHDC models have been proposed to represent class-instance and class-class\nrelations, but when representing the more complex class-subclass relation,\nwhere multiple objects associate different levels of classes and subclasses,\nthey face challenges for factorization, a crucial task for neuro-symbolic AI\nsystems. In this article, we propose FactorHD, a novel HDC model capable of\nrepresenting and factorizing the complex class-subclass relation efficiently.\nFactorHD features a symbolic encoding method that embeds an extra memorization\nclause, preserving more information for multiple objects. In addition, it\nemploys an efficient factorization algorithm that selectively eliminates\nredundant classes by identifying the memorization clause of the target class.\nSuch model significantly enhances computing efficiency and accuracy in\nrepresenting and factorizing multiple objects with class-subclass relation,\novercoming limitations of existing HDC models such as \"superposition\ncatastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves\napproximately 5667x speedup at a representation size of 10^9 compared to\nexisting HDC models. When integrated with the ResNet-18 neural network,\nFactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.", "comment": "7 pages, 5 figures, 2 tables, to be published in the 62nd DAC (Design\n  Automation Conference) proceedings", "pdf_url": "http://arxiv.org/pdf/2507.12366v1", "cate": "cs.SC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12399", "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "authors": ["Florian E. Dorner", "Yatong Chen", "André F. Cruz", "Fanny Yang"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      35 pages, 9 Figures", "url": "http://arxiv.org/abs/2507.12399v1", "summary": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.", "comment": "35 pages, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2507.12399v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12236", "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models", "authors": ["Felix Nützel", "Mischa Dombrowski", "Bernhard Kainz"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      20 pages, 6 figures. To appear in Proc. MIDL 2025 (PMLR)", "url": "http://arxiv.org/abs/2507.12236v1", "summary": "Phrase grounding, i.e., mapping natural language phrases to specific image\nregions, holds significant potential for disease localization in medical\nimaging through clinical reports. While current state-of-the-art methods rely\non discriminative, self-supervised contrastive models, we demonstrate that\ngenerative text-to-image diffusion models, leveraging cross-attention maps, can\nachieve superior zero-shot phrase grounding performance. Contrary to prior\nassumptions, we show that fine-tuning diffusion models with a frozen,\ndomain-specific language model, such as CXR-BERT, substantially outperforms\ndomain-agnostic counterparts. This setup achieves remarkable improvements, with\nmIoU scores doubling those of current discriminative methods. These findings\nhighlight the underexplored potential of generative models for phrase grounding\ntasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),\na novel post-processing technique that aligns text and image biases to identify\nregions of high certainty. BBM refines cross-attention maps, achieving even\ngreater localization accuracy. Our results establish generative approaches as a\nmore effective paradigm for phrase grounding in the medical imaging domain,\npaving the way for more robust and interpretable applications in clinical\npractice. The source code and model weights are available at\nhttps://github.com/Felix-012/generate_to_ground.", "comment": "20 pages, 6 figures. To appear in Proc. MIDL 2025 (PMLR)", "pdf_url": "http://arxiv.org/pdf/2507.12236v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.18296", "title": "JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking Styles", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted on Interspeech 2025", "url": "http://arxiv.org/abs/2506.18296v2", "summary": "We construct Japanese Idol Speech Corpus (JIS) to advance research in speech\ngeneration AI, including text-to-speech synthesis (TTS) and voice conversion\n(VC). JIS will facilitate more rigorous evaluations of speaker similarity in\nTTS and VC systems since all speakers in JIS belong to a highly specific\ncategory: \"young female live idols\" in Japan, and each speaker is identified by\na stage name, enabling researchers to recruit listeners familiar with these\nidols for listening experiments. With its unique speaker attributes, JIS will\nfoster compelling research, including generating voices tailored to listener\npreferences-an area not yet widely studied. JIS will be distributed free of\ncharge to promote research in speech generation AI, with usage restricted to\nnon-commercial, basic research. We describe the construction of JIS, provide an\noverview of Japanese live idol culture to support effective and ethical use of\nJIS, and offer a basic analysis to guide application of JIS.", "comment": "Accepted on Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.18296v2", "cate": "cs.SD", "date": "2025-06-23", "updated": "2025-07-15"}
{"id": "2507.12357", "title": "Online Block Packing", "authors": ["Ariel Ben Eliezer", "Noam Nisan"], "categories": ["cs.DS", "cs.GT"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12357v1", "summary": "We consider the algorithmic challenge that is faced by blockchains that have\nmultidimensional block constraints and serve quasi-patient bidders. We provide\nonline approximation algorithms for this problem, thus solving open problems\nleft by [Babaioff and Nisan, EC 2025].", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12357v1", "cate": "cs.DS", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12156", "title": "SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models", "authors": ["Chen Li", "Shanshan Dong", "Sheng Qiu", "Jianmin Han", "Zan Gao", "Kemeng Huang", "Taku Komura"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12156v1", "summary": "Reconstructing dynamic fluids from sparse views is a long-standing and\nchallenging problem, due to the severe lack of 3D information from insufficient\nview coverage. While several pioneering approaches have attempted to address\nthis issue using differentiable rendering or novel view synthesis, they are\noften limited by time-consuming optimization and refinement processes under\nill-posed conditions. To tackle above challenges, we propose SmokeSVD, an\nefficient and effective framework to progressively generate and reconstruct\ndynamic smoke from a single video by integrating both the powerful generative\ncapabilities from diffusion models and physically guided consistency\noptimization towards realistic appearance and dynamic evolution. Specifically,\nwe first propose a physically guided side-view synthesizer based on diffusion\nmodels, which explicitly incorporates divergence and gradient guidance of\nvelocity fields to generate visually realistic and spatio-temporally consistent\nside-view images frame by frame, significantly alleviating the ill-posedness of\nsingle-view reconstruction without imposing additional constraints.\nSubsequently, we determine a rough estimation of density field from the pair of\nfront-view input and side-view synthetic image, and further refine 2D blurry\nnovel-view images and 3D coarse-grained density field through an iterative\nprocess that progressively renders and enhances the images from increasing\nnovel viewing angles, generating high-quality multi-view image sequences.\nFinally, we reconstruct and estimate the fine-grained density field, velocity\nfield, and smoke source via differentiable advection by leveraging the\nNavier-Stokes equations. Extensive quantitative and qualitative experiments\nshow that our approach achieves high-quality reconstruction and outperforms\nprevious state-of-the-art techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12156v1", "cate": "cs.GR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12379", "title": "Probing for Arithmetic Errors in Language Models", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12379v1", "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12379v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12412", "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data", "authors": ["Dzung Dinh", "Boqi Chen", "Marc Niethammer", "Junier Oliva"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12412v1", "summary": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12412v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12245", "title": "Calisthenics Skills Temporal Video Segmentation", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures, In Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 2", "url": "http://arxiv.org/abs/2507.12245v1", "summary": "Calisthenics is a fast-growing bodyweight discipline that consists of\ndifferent categories, one of which is focused on skills. Skills in calisthenics\nencompass both static and dynamic elements performed by athletes. The\nevaluation of static skills is based on their difficulty level and the duration\nof the hold. Automated tools able to recognize isometric skills from a video by\nsegmenting them to estimate their duration would be desirable to assist\nathletes in their training and judges during competitions. Although the video\nunderstanding literature on action recognition through body pose analysis is\nrich, no previous work has specifically addressed the problem of calisthenics\nskill temporal video segmentation. This study aims to provide an initial step\ntowards the implementation of automated tools within the field of Calisthenics.\nTo advance knowledge in this context, we propose a dataset of video footage of\nstatic calisthenics skills performed by athletes. Each video is annotated with\na temporal segmentation which determines the extent of each skill. We hence\nreport the results of a baseline approach to address the problem of skill\ntemporal segmentation on the proposed dataset. The results highlight the\nfeasibility of the proposed problem, while there is still room for improvement.", "comment": "9 pages, 6 figures, In Proceedings of the 19th International Joint\n  Conference on Computer Vision, Imaging and Computer Graphics Theory and\n  Applications - Volume 2", "pdf_url": "http://arxiv.org/pdf/2507.12245v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12005", "title": "Kernelization for list $H$-coloring for graphs with small vertex cover", "authors": ["Marta Piecyk", "Astrid Pieterse", "Paweł Rzążewski", "Magnus Wahlström"], "categories": ["math.CO", "cs.DS"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12005v1", "summary": "For a fixed graph $H$, in the List $H$-Coloring problem, we are given a graph\n$G$ along with list $L(v) \\subseteq V(H)$ for every $v \\in V(G)$, and we have\nto determine if there exists a list homomorphism $\\varphi$ from $(G,L)$ to $H$,\ni.e., an edge preserving mapping $\\varphi: V(G)\\to V(H)$ that satisfies\n$\\varphi(v)\\in L(v)$ for every $v\\in V(G)$. Note that if $H$ is the complete\ngraph on $q$ vertices, the problem is equivalent to List $q$-Coloring. We\ninvestigate the kernelization properties of List $H$-Coloring parameterized by\nthe vertex cover number of $G$: given an instance $(G,L)$ and a vertex cover of\n$G$ of size $k$, can we reduce $(G,L)$ to an equivalent instance $(G',L')$ of\nList $H$-Coloring where the size of $G'$ is bounded by a low-degree polynomial\n$p(k)$ in $k$? This question has been investigated previously by Jansen and\nPieterse [Algorithmica 2019], who provided an upper bound, which turns out to\nbe optimal if $H$ is a complete graph, i.e., for List $q$-Coloring. This result\nwas one of the first applications of the method of kernelization via\nbounded-degree polynomials. We define two new integral graph invariants,\n$c^*(H)$ and $d^*(H)$, with $d^*(H) \\leq c^*(H) \\leq d^*(H)+1$, and show that\nfor every graph $H$, List $H$-Coloring\n  -- has a kernel with $\\mathcal{O}(k^{c^*(H)})$ vertices,\n  -- admits no kernel of size $\\mathcal{O}(k^{d^*(H)-\\varepsilon})$ for any\n$\\varepsilon > 0$, unless the polynomial hierarchy collapses.\n  -- Furthermore, if $c^*(H) > d^*(H)$, then there is a kernel with\n$\\mathcal{O}(k^{c^*(H)-\\varepsilon})$ vertices where $\\varepsilon \\geq\n2^{1-c^*(H)}$.\n  Additionally, we show that for some classes of graphs, including powers of\ncycles and graphs $H$ where $\\Delta(H) \\leq c^*(H)$ (which in particular\nincludes cliques), the bound $d^*(H)$ is tight, using the polynomial method. We\nconjecture that this holds in general.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12005v1", "cate": "math.CO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12168", "title": "Shape Adaptation for 3D Hairstyle Retargeting", "authors": ["Lu Yu", "Zhong Ren", "Youyi Zheng", "Xiang Chen", "Kun Zhou"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12168v1", "summary": "It is demanding to author an existing hairstyle for novel characters in games\nand VR applications. However, it is a non-trivial task for artists due to the\ncomplicated hair geometries and spatial interactions to preserve. In this\npaper, we present an automatic shape adaptation method to retarget 3D\nhairstyles. We formulate the adaptation process as a constrained optimization\nproblem, where all the shape properties and spatial relationships are converted\ninto individual objectives and constraints. To make such an optimization on\nhigh-resolution hairstyles tractable, we adopt a multi-scale strategy to\ncompute the target positions of the hair strands in a coarse-to-fine manner.\nThe global solving for the inter-strands coupling is restricted to the coarse\nlevel, and the solving for fine details is made local and parallel. In\naddition, we present a novel hairline edit tool to allow for user customization\nduring retargeting. We achieve it by solving physics-based deformations of an\nembedded membrane to redistribute the hair roots with minimal distortion. We\ndemonstrate the efficacy of our method through quantitative and qualitative\nexperiments on various hairstyles and characters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12168v1", "cate": "cs.GR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12416", "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "authors": ["Jaehyun Kwak", "Ramahdani Muhammad Izaaz Inhar", "Se-Young Yun", "Sung-Ju Lee"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025", "url": "http://arxiv.org/abs/2507.12416v1", "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.12416v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12419", "title": "Mixture of Raytraced Experts", "authors": ["Andrea Perin", "Giacomo Lagomarsini", "Claudio Gallicchio", "Giuseppe Nuti"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preliminary version (pre-submission)", "url": "http://arxiv.org/abs/2507.12419v1", "summary": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing", "comment": "Preliminary version (pre-submission)", "pdf_url": "http://arxiv.org/pdf/2507.12419v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12248", "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST", "authors": ["Anida Nezović", "Jalal Romano", "Nada Marić", "Medina Kapo", "Amila Akagić"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12248v1", "summary": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12248v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2111.09290", "title": "Matroid-Based TSP Rounding for Half-Integral Solutions", "authors": ["Anupam Gupta", "Euiwoong Lee", "Jason Li", "Marcin Mucha", "Heather Newman", "Sherry Sarkar"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2111.09290v2", "summary": "We show how to round any half-integral solution to the subtour-elimination\nrelaxation for the TSP, while losing a less-than-1.5 factor. Such a rounding\nalgorithm was recently given by Karlin, Klein, and Oveis Gharan based on\nsampling from max-entropy distributions. We build on an approach of Haddadan\nand Newman to show how sampling from the matroid intersection polytope, and a\nnew use of max-entropy sampling, can give better guarantees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2111.09290v2", "cate": "cs.DS", "date": "2021-11-17", "updated": "2025-07-15"}
{"id": "2506.05935", "title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction", "authors": ["Yuchao Zheng", "Jianing Zhang", "Guochen Ning", "Hongen Liao"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05935v2", "summary": "Intraoperative navigation relies heavily on precise 3D reconstruction to\nensure accuracy and safety during surgical procedures. However, endoscopic\nscenarios present unique challenges, including sparse features and inconsistent\nlighting, which render many existing Structure-from-Motion (SfM)-based methods\ninadequate and prone to reconstruction failure. To mitigate these constraints,\nwe propose SurGSplat, a novel paradigm designed to progressively refine 3D\nGaussian Splatting (3DGS) through the integration of geometric constraints. By\nenabling the detailed reconstruction of vascular structures and other critical\nfeatures, SurGSplat provides surgeons with enhanced visual clarity,\nfacilitating precise intraoperative decision-making. Experimental evaluations\ndemonstrate that SurGSplat achieves superior performance in both novel view\nsynthesis (NVS) and pose estimation accuracy, establishing it as a\nhigh-fidelity and efficient solution for surgical scene reconstruction. More\ninformation and results can be found on the page https://surgsplat.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05935v2", "cate": "cs.GR", "date": "2025-06-06", "updated": "2025-07-16"}
{"id": "2507.12428", "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12428v1", "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12428v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12435", "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks", "authors": ["Yi Li", "David Mccoy", "Nolan Gunter", "Kaitlyn Lee", "Alejandro Schuler", "Mark van der Laan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12435v1", "summary": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12435v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12283", "title": "FADE: Adversarial Concept Erasure in Flow Models", "authors": ["Zixuan Fu", "Yan Ren", "Finn Carter", "Chenyue Wang", "Ze Niu", "Dacheng Yu", "Emily Davis", "Bo Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Camera Ready", "url": "http://arxiv.org/abs/2507.12283v1", "summary": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch.", "comment": "Camera Ready", "pdf_url": "http://arxiv.org/pdf/2507.12283v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2402.13357", "title": "Minimizing Tardy Processing Time on a Single Machine in Near-Linear Time", "authors": ["Nick Fischer", "Leo Wennmann"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      17 pages. This is the TheoretiCS journal version", "url": "http://arxiv.org/abs/2402.13357v3", "summary": "In this work we revisit the elementary scheduling problem $1||\\sum p_j U_j$.\nThe goal is to select, among $n$ jobs with processing times and due dates, a\nsubset of jobs with maximum total processing time that can be scheduled in\nsequence without violating their due dates. This problem is NP-hard, but a\nclassical algorithm by Lawler and Moore from the 60s solves this problem in\npseudo-polynomial time $O(nP)$, where $P$ is the total processing time of all\njobs. With the aim to develop best-possible pseudo-polynomial-time algorithms,\na recent wave of results has improved Lawler and Moore's algorithm for $1||\\sum\np_j U_j$: First to time $\\tilde O(P^{7/4})$ [Bringmann, Fischer, Hermelin,\nShabtay, Wellnitz; ICALP'20], then to time $\\tilde O(P^{5/3})$ [Klein, Polak,\nRohwedder; SODA'23], and finally to time $\\tilde O(P^{7/5})$ [Schieber,\nSitaraman; WADS'23]. It remained an exciting open question whether these works\ncan be improved further.\n  In this work we develop an algorithm in near-linear time $\\tilde O(P)$ for\nthe $1||\\sum p_j U_j$ problem. This running time not only significantly\nimproves upon the previous results, but also matches conditional lower bounds\nbased on the Strong Exponential Time Hypothesis or the Set Cover Hypothesis and\nis therefore likely optimal (up to subpolynomial factors). Our new algorithm\nalso extends to the case of $m$ machines in time $\\tilde O(P^m)$. In contrast\nto the previous improvements, we take a different, more direct approach\ninspired by the recent reductions from Modular Subset Sum to dynamic string\nproblems. We thereby arrive at a satisfyingly simple algorithm.", "comment": "17 pages. This is the TheoretiCS journal version", "pdf_url": "http://arxiv.org/pdf/2402.13357v3", "cate": "cs.DS", "date": "2024-02-20", "updated": "2025-07-16"}
{"id": "2506.13212", "title": "Volumetric Functional Maps", "authors": ["Filippo Maggioli", "Simone Melzi", "Marco Livesu"], "categories": ["cs.GR", "cs.CG", "68U05", "I.3"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.13212v2", "summary": "The computation of volumetric correspondences between 3D shapes is a\nprominent tool for medical and industrial applications. In this work, we pave\nthe way for spectral volume mapping, extending for the first time the\nfunctional maps framework from the surface to the volumetric setting. We show\nthat the eigenfunctions of the volumetric Laplace operator define a functional\nspace that is suitable for high-quality signal transfer. We also experiment\nwith various techniques that edit this functional space, porting them to volume\ndomains. We validate our method on novel volumetric datasets and on\ntetrahedralizations of well established surface datasets, also showcasing\npractical applications involving both discrete and continuous signal mapping,\nfor segmentation transfer, mesh connectivity transfer and solid texturing. Last\nbut not least, we show that considering the volumetric spectrum greatly\nimproves the accuracy for classical shape matching tasks among surfaces,\nconsistently outperforming existing surface-only spectral methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.13212v2", "cate": "cs.GR", "date": "2025-06-16", "updated": "2025-07-15"}
{"id": "2507.12451", "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted as a long paper for ACL 2025 main conference", "url": "http://arxiv.org/abs/2507.12451v1", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "comment": "Accepted as a long paper for ACL 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2507.12451v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12453", "title": "Cost-aware Stopping for Bayesian Optimization", "authors": ["Qian Xie", "Linda Cai", "Alexander Terenin", "Peter I. Frazier", "Ziv Scully"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12453v1", "summary": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12453v1", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12292", "title": "Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures, In International Conference on Image Analysis and Processing", "url": "http://arxiv.org/abs/2507.12292v1", "summary": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.", "comment": "13 pages, 4 figures, In International Conference on Image Analysis\n  and Processing", "pdf_url": "http://arxiv.org/pdf/2507.12292v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11115", "title": "Finding Order-Preserving Subgraphs", "authors": ["Haruya Imamura", "Yasuaki Kobayashi", "Yota Otachi", "Toshiki Saitoh", "Keita Sato", "Asahi Takaoka", "Ryo Yoshinaka", "Tom C. van der Zanden"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11115v2", "summary": "(Induced) Subgraph Isomorphism and Maximum Common (Induced) Subgraph are\nfundamental problems in graph pattern matching and similarity computation. In\ngraphs derived from time-series data or protein structures, a natural total\nordering of vertices often arises from their underlying structure, such as\ntemporal sequences or amino acid sequences. This motivates the study of problem\nvariants that respect this inherent ordering. This paper addresses Ordered\n(Induced) Subgraph Isomorphism (O(I)SI) and its generalization, Maximum Common\nOrdered (Induced) Subgraph (MCO(I)S), which seek to find subgraph isomorphisms\nthat preserve the vertex orderings of two given ordered graphs. Our main\ncontributions are threefold: (1) We prove that these problems remain\nNP-complete even when restricted to small graph classes, such as trees of depth\n2 and threshold graphs. (2) We establish a gap in computational complexity\nbetween OSI and OISI on certain graph classes. For instance, OSI is\npolynomial-time solvable for interval graphs with their interval orderings,\nwhereas OISI remains NP-complete under the same setting. (3) We demonstrate\nthat the tractability of these problems can depend on the vertex ordering. For\nexample, while OISI is NP-complete on threshold graphs, its generalization,\nMCOIS, can be solved in polynomial time if the specific vertex orderings that\ncharacterize the threshold graphs are provided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11115v2", "cate": "cs.DS", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.12461", "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis", "authors": ["Trong-Thang Pham", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM MM 2025", "url": "http://arxiv.org/abs/2507.12461v1", "summary": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.", "comment": "ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.12461v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11768", "title": "LLMs are Bayesian, in Expectation, not in Realization", "authors": ["Leon Chlon", "Sarah Rashidi", "Zein Khamis", "MarcAntonio M. Awada"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11768v1", "summary": "Large language models demonstrate remarkable in-context learning\ncapabilities, adapting to new tasks without parameter updates. While this\nphenomenon has been successfully modeled as implicit Bayesian inference, recent\nempirical findings reveal a fundamental contradiction: transformers\nsystematically violate the martingale property, a cornerstone requirement of\nBayesian updating on exchangeable data. This violation challenges the\ntheoretical foundations underlying uncertainty quantification in critical\napplications.\n  Our theoretical analysis establishes four key results: (1) positional\nencodings induce martingale violations of order $\\Theta(\\log n / n)$; (2)\ntransformers achieve information-theoretic optimality with excess risk\n$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior\nrepresentation converges to the true Bayesian posterior in the space of\nsufficient statistics; and (4) we derive the optimal chain-of-thought length as\n$k^* = \\Theta(\\sqrt{n}\\log(1/\\varepsilon))$ with explicit constants, providing\na principled approach to reduce inference costs while maintaining performance.\nEmpirical validation on GPT-3 confirms predictions (1)-(3), with transformers\nreaching 99\\% of theoretical entropy limits within 20 examples. Our framework\nprovides practical methods for extracting calibrated uncertainty estimates from\nposition-aware architectures and optimizing computational efficiency in\ndeployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11768v1", "cate": "stat.ML", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12336", "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors", "authors": ["Subin Jeon", "In Cho", "Junyoung Hong", "Seon Joo Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12336v1", "summary": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12336v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11256", "title": "Fully Dynamic Euclidean k-Means", "authors": ["Sayan Bhattacharya", "Martín Costa", "Ermiya Farokhnejad", "Shaofeng H. -C. Jiang", "Yaonan Jin", "Jianing Lou"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11256v2", "summary": "We consider the fundamental Euclidean $k$-means clustering problem in a\ndynamic setting, where the input $X \\subseteq \\mathbb{R}^d$ evolves over time\nvia a sequence of point insertions/deletions. We have to explicitly maintain a\nsolution (a set of $k$ centers) $S \\subseteq \\mathbb{R}^d$ throughout these\nupdates, while minimizing the approximation ratio, the update time (time taken\nto handle a point insertion/deletion) and the recourse (number of changes made\nto the solution $S$) of the algorithm.\n  We present a dynamic algorithm for this problem with\n$\\text{poly}(1/\\epsilon)$-approximation ratio, $\\tilde{O}(k^{\\epsilon})$ update\ntime and $\\tilde{O}(1)$ recourse. In the general regime, where the dimension\n$d$ cannot be assumed to be a fixed constant, our algorithm has almost optimal\nguarantees across all these three parameters. Indeed, improving our update time\nor approximation ratio would imply beating the state-of-the-art static\nalgorithm for this problem (which is widely believed to be the best possible),\nand the recourse of any dynamic algorithm must be $\\Omega(1)$.\n  We obtain our result by building on top of the recent work of [Bhattacharya,\nCosta, Farokhnejad; STOC'25], which gave a near-optimal dynamic algorithm for\n$k$-means in general metric spaces (as opposed to in the Euclidean setting).\nAlong the way, we design several novel geometric data structures that are of\nindependent interest. Specifically, one of our main contributions is designing\nthe first consistent hashing scheme [Czumaj, Jiang, Krauthgamer, Vesel\\'y,\nYang; FOCS'22] that achieves $\\tilde O(n^\\epsilon)$ running time per point\nevaluation with competitive parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11256v2", "cate": "cs.DS", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2011.10672", "title": "Artificial Intelligence Governance for Businesses", "authors": ["Johannes Schneider", "Rene Abraham", "Christian Meske", "Jan vom Brocke"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2011.10672v3", "summary": "Artificial Intelligence (AI) governance regulates the exercise of authority\nand control over the management of AI. It aims at leveraging AI through\neffective use of data and minimization of AI-related cost and risk. While\ntopics such as AI governance and AI ethics are thoroughly discussed on a\ntheoretical, philosophical, societal and regulatory level, there is limited\nwork on AI governance targeted to companies and corporations. This work views\nAI products as systems, where key functionality is delivered by machine\nlearning (ML) models leveraging (training) data. We derive a conceptual\nframework by synthesizing literature on AI and related fields such as ML. Our\nframework decomposes AI governance into governance of data, (ML) models and\n(AI) systems along four dimensions. It relates to existing IT and data\ngovernance frameworks and practices. It can be adopted by practitioners and\nacademics alike. For practitioners the synthesis of mainly research papers, but\nalso practitioner publications and publications of regulatory bodies provides a\nvaluable starting point to implement AI governance, while for academics the\npaper highlights a number of areas of AI governance that deserve more\nattention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2011.10672v3", "cate": "cs.AI", "date": "2020-11-20", "updated": "2025-07-16"}
{"id": "2507.11780", "title": "Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing", "authors": ["Justin Whitehouse", "Morgane Austern", "Vasilis Syrgkanis"], "categories": ["econ.EM", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Econometrics (econ.EM)", "pdf_link": null, "comments": "Comments:      40 pages, 2 figures", "url": "http://arxiv.org/abs/2507.11780v1", "summary": "Constructing confidence intervals for the value of an optimal treatment\npolicy is an important problem in causal inference. Insight into the optimal\npolicy value can guide the development of reward-maximizing, individualized\ntreatment regimes. However, because the functional that defines the optimal\nvalue is non-differentiable, standard semi-parametric approaches for performing\ninference fail to be directly applicable. Existing approaches for handling this\nnon-differentiability fall roughly into two camps. In one camp are estimators\nbased on constructing smooth approximations of the optimal value. These\napproaches are computationally lightweight, but typically place unrealistic\nparametric assumptions on outcome regressions. In another camp are approaches\nthat directly de-bias the non-smooth objective. These approaches don't place\nparametric assumptions on nuisance functions, but they either require the\ncomputation of intractably-many nuisance estimates, assume unrealistic\n$L^\\infty$ nuisance convergence rates, or make strong margin assumptions that\nprohibit non-response to a treatment. In this paper, we revisit the problem of\nconstructing smooth approximations of non-differentiable functionals. By\ncarefully controlling first-order bias and second-order remainders, we show\nthat a softmax smoothing-based estimator can be used to estimate parameters\nthat are specified as a maximum of scores involving nuisance components. In\nparticular, this includes the value of the optimal treatment policy as a\nspecial case. Our estimator obtains $\\sqrt{n}$ convergence rates, avoids\nparametric restrictions/unrealistic margin assumptions, and is often\nstatistically efficient.", "comment": "40 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.11780v1", "cate": "econ.EM", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.12344", "title": "Improving Lightweight Weed Detection via Knowledge Distillation", "authors": ["Ahmet Oğuz Saltık", "Max Voigt", "Sourav Modak", "Mike Beckworth", "Anthony Stein"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12344v1", "summary": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12344v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11484", "title": "Multipass Linear Sketches for Geometric LP-Type Problems", "authors": ["N. Efe Çekirge", "William Gay", "David P. Woodruff"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      To Appear in APPROX 2025, 45 pages; Updated author information in v2", "url": "http://arxiv.org/abs/2507.11484v2", "summary": "LP-type problems such as the Minimum Enclosing Ball (MEB), Linear Support\nVector Machine (SVM), Linear Programming (LP), and Semidefinite Programming\n(SDP) are fundamental combinatorial optimization problems, with many important\napplications in machine learning applications such as classification,\nbioinformatics, and noisy learning. We study LP-type problems in several\nstreaming and distributed big data models, giving $\\varepsilon$-approximation\nlinear sketching algorithms with a focus on the high accuracy regime with low\ndimensionality $d$, that is, when ${d < (1/\\varepsilon)^{0.999}}$. Our main\nresult is an $O(ds)$ pass algorithm with $O(s( \\sqrt{d}/\\varepsilon)^{3d/s})\n\\cdot \\mathrm{poly}(d, \\log (1/\\varepsilon))$ space complexity in words, for\nany parameter $s \\in [1, d \\log (1/\\varepsilon)]$, to solve\n$\\varepsilon$-approximate LP-type problems of $O(d)$ combinatorial and VC\ndimension. Notably, by taking $s = d \\log (1/\\varepsilon)$, we achieve space\ncomplexity polynomial in $d$ and polylogarithmic in $1/\\varepsilon$, presenting\nexponential improvements in $1/\\varepsilon$ over current algorithms. We\ncomplement our results by showing lower bounds of $(1/\\varepsilon)^{\\Omega(d)}$\nfor any $1$-pass algorithm solving the $(1 + \\varepsilon)$-approximation MEB\nand linear SVM problems, further motivating our multi-pass approach.", "comment": "To Appear in APPROX 2025, 45 pages; Updated author information in v2", "pdf_url": "http://arxiv.org/pdf/2507.11484v2", "cate": "cs.DS", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2410.01349", "title": "Life, uh, Finds a Way: Hyperadaptability by Behavioral Search", "authors": ["Alex Baranski", "Jun Tani"], "categories": ["cs.AI", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      39 pages, 9 figures", "url": "http://arxiv.org/abs/2410.01349v2", "summary": "Living beings are able to solve a wide variety of problems that they\nencounter rarely or only once. Without the benefit of extensive and repeated\nexperience with these problems, they can solve them in an ad-hoc manner. We\ncall this capacity to always find a solution to a physically solvable problem\n$hyperadaptability$. To explain how hyperadaptability can be achieved, we\npropose a theory that frames behavior as the physical manifestation of a\nself-modifying search procedure. Rather than exploring randomly, our system\nachieves robust problem-solving by dynamically ordering an infinite set of\ncontinuous behaviors according to simplicity and effectiveness. Behaviors are\nsampled from paths over cognitive graphs, their order determined by a tight\nbehavior-execution/graph-modification feedback loop. We implement cognitive\ngraphs using Hebbian-learning and a novel harmonic neural representation\nsupporting flexible information storage. We validate our approach through\nsimulation experiments showing rapid achievement of highly-robust navigation\nability in complex mazes, as well as high reward on difficult extensions of\nclassic reinforcement learning problems. This framework offers a new\ntheoretical model for developmental learning and paves the way for robots that\ncan autonomously master complex skills and handle exceptional circumstances.", "comment": "39 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2410.01349v2", "cate": "cs.AI", "date": "2024-10-02", "updated": "2025-07-16"}
{"id": "2507.11806", "title": "MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling", "authors": ["Hendrik Kraß", "Ju Huang", "Seyed Mohamad Moosavi"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11806v1", "summary": "Universal machine learning interatomic potentials (uMLIPs) have emerged as\npowerful tools for accelerating atomistic simulations, offering scalable and\nefficient modeling with accuracy close to quantum calculations. However, their\nreliability and effectiveness in practical, real-world applications remain an\nopen question. Metal-organic frameworks (MOFs) and related nanoporous materials\nare highly porous crystals with critical relevance in carbon capture, energy\nstorage, and catalysis applications. Modeling nanoporous materials presents\ndistinct challenges for uMLIPs due to their diverse chemistry, structural\ncomplexity, including porosity and coordination bonds, and the absence from\nexisting training datasets. Here, we introduce MOFSimBench, a benchmark to\nevaluate uMLIPs on key materials modeling tasks for nanoporous materials,\nincluding structural optimization, molecular dynamics (MD) stability, the\nprediction of bulk properties, such as bulk modulus and heat capacity, and\nguest-host interactions. Evaluating over 20 models from various architectures\non a chemically and structurally diverse materials set, we find that\ntop-performing uMLIPs consistently outperform classical force fields and\nfine-tuned machine learning potentials across all tasks, demonstrating their\nreadiness for deployment in nanoporous materials modeling. Our analysis\nhighlights that data quality, particularly the diversity of training sets and\ninclusion of out-of-equilibrium conformations, plays a more critical role than\nmodel architecture in determining performance across all evaluated uMLIPs. We\nrelease our modular and extendable benchmarking framework at\nhttps://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide\nthe adoption for nanoporous materials modeling and further development of\nuMLIPs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11806v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12382", "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "authors": ["Kaiwen Huang", "Yi Zhou", "Huazhu Fu", "Yizhe Zhang", "Chen Gong", "Tao Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages; 2 figures; Have been accepted by MICCAI 2025", "url": "http://arxiv.org/abs/2507.12382v1", "summary": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "comment": "10 pages; 2 figures; Have been accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.12382v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2412.18375", "title": "Many Objective Problems Where Crossover is Provably Essential", "authors": ["Andre Opris"], "categories": ["cs.NE", "cs.AI", "cs.DS", "68Q25, 68Q87, 68T20, 68W20, 68W40, 68W50", "F.2.2; G.3; I.2.8"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Conference version appeared at AAAI 2025, submitted to artificial intelligence", "url": "http://arxiv.org/abs/2412.18375v2", "summary": "This article addresses theory in evolutionary many-objective optimization and\nfocuses on the role of crossover operators. The advantages of using crossover\nare hardly understood and rigorous runtime analyses with crossover are lagging\nfar behind its use in practice, specifically in the case of more than two\nobjectives. We present two many-objective problems $RR_{\\text{RO}}$ and\n$uRR_{\\text{RO}}$ together with a theoretical runtime analysis of the GSEMO and\nthe widely used NSGA-III algorithm to demonstrate that one point crossover on\n$RR_{\\text{RO}}$, as well as uniform crossover on $uRR_{\\text{RO}}$, can yield\nan exponential speedup in the runtime. In particular, when the number of\nobjectives is constant, this algorithms can find the Pareto set of both\nproblems in expected polynomial time when using crossover while without\ncrossover they require exponential time to even find a single Pareto-optimal\npoint. For both problems, we also demonstrate a significant performance gap in\ncertain superconstant parameter regimes for the number of objectives. To the\nbest of our knowledge, this is one of the first rigorous runtime analysis in\nmany-objective optimization which demonstrates an exponential performance gap\nwhen using crossover for more than two objectives. Additionally, it is the\nfirst runtime analysis involving crossover in many-objective optimization where\nthe number of objectives is not necessarily constant.", "comment": "Conference version appeared at AAAI 2025, submitted to artificial\n  intelligence", "pdf_url": "http://arxiv.org/pdf/2412.18375v2", "cate": "cs.NE", "date": "2024-12-24", "updated": "2025-07-15"}
{"id": "2410.15460", "title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training", "authors": ["Shahrad Mohammadzadeh", "Juan David Guerra", "Marco Bonizzato", "Reihaneh Rabbany", "Golnoosh Farnadi"], "categories": ["cs.AI", "cs.CL", "math.SP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025, accepted to Safe Generative AI Workshop @ NeurIPS 2024. Camera-ready version for ACL 2025 (to appear). Submitted July 2025", "url": "http://arxiv.org/abs/2410.15460v4", "summary": "As large language models (LLMs) become increasingly prevalent, concerns about\ntheir reliability, particularly due to hallucinations - factually inaccurate or\nirrelevant outputs - have grown. Our research investigates the relationship\nbetween the uncertainty in training dynamics and the emergence of\nhallucinations. Using models from the Pythia suite and several hallucination\ndetection metrics, we analyze hallucination trends and identify significant\nvariance during training. To address this, we propose \\textbf{Sensitivity\nDropout (SenD)}, a novel training protocol designed to reduce hallucination\nvariance during training by deterministically dropping embedding indices with\nsignificant variability. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This metric is integrated into our training\nprotocol, allowing SenD to be both computationally scalable and effective at\nreducing hallucination variance. SenD improves test-time reliability of Pythia\nand Meta's Llama models by up to 17\\% and enhances factual accuracy in\nWikipedia, Medical, Legal, and Coding domains without affecting downstream task\nperformance.", "comment": "Accepted to ACL 2025, accepted to Safe Generative AI Workshop @\n  NeurIPS 2024. Camera-ready version for ACL 2025 (to appear). Submitted July\n  2025", "pdf_url": "http://arxiv.org/pdf/2410.15460v4", "cate": "cs.AI", "date": "2024-10-20", "updated": "2025-07-16"}
{"id": "2507.11842", "title": "CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching", "authors": ["Sidharth Kannan", "Tian Qiu", "Carolina Cuesta-Lazaro", "Haewon Jeong"], "categories": ["astro-ph.CO", "cs.LG"], "primary_category": "Subjects:       Cosmology and Nongalactic Astrophysics (astro-ph.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11842v1", "summary": "Generative machine learning models have been demonstrated to be able to learn\nlow dimensional representations of data that preserve information required for\ndownstream tasks. In this work, we demonstrate that flow matching based\ngenerative models can learn compact, semantically rich latent representations\nof field level cold dark matter (CDM) simulation data without supervision. Our\nmodel, CosmoFlow, learns representations 32x smaller than the raw field data,\nusable for field level reconstruction, synthetic data generation, and parameter\ninference. Our model also learns interpretable representations, in which\ndifferent latent channels correspond to features at different cosmological\nscales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11842v1", "cate": "astro-ph.CO", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12396", "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments", "authors": ["Hayat Ullah", "Abbas Khan", "Arslan Munir", "Hari Kalva"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.12396v1", "summary": "Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.12396v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.14995", "title": "Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution", "authors": ["Jonas Gösgens", "Niklas Jansen", "Hector Geffner"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      accepted at ICAPS 2025", "url": "http://arxiv.org/abs/2411.14995v3", "summary": "Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.", "comment": "accepted at ICAPS 2025", "pdf_url": "http://arxiv.org/pdf/2411.14995v3", "cate": "cs.AI", "date": "2024-11-22", "updated": "2025-07-16"}
{"id": "2507.11851", "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11851v1", "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11851v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12420", "title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization", "authors": ["Haoyuan Liu", "Hiroshi Watanabe"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12420v1", "summary": "Bounding box regression (BBR) is fundamental to object detection, where the\nregression loss is crucial for accurate localization. Existing IoU-based losses\noften incorporate handcrafted geometric penalties to address IoU's\nnon-differentiability in non-overlapping cases and enhance BBR performance.\nHowever, these penalties are sensitive to box shape, size, and distribution,\noften leading to suboptimal optimization for small objects and undesired\nbehaviors such as bounding box enlargement due to misalignment with the IoU\nobjective. To address these limitations, we propose InterpIoU, a novel loss\nfunction that replaces handcrafted geometric penalties with a term based on the\nIoU between interpolated boxes and the target. By using interpolated boxes to\nbridge the gap between predictions and ground truth, InterpIoU provides\nmeaningful gradients in non-overlapping cases and inherently avoids the box\nenlargement issue caused by misaligned penalties. Simulation results further\nshow that IoU itself serves as an ideal regression target, while existing\ngeometric penalties are both unnecessary and suboptimal. Building on InterpIoU,\nwe introduce Dynamic InterpIoU, which dynamically adjusts interpolation\ncoefficients based on IoU values, enhancing adaptability to scenarios with\ndiverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC\nshow that our methods consistently outperform state-of-the-art IoU-based losses\nacross various detection frameworks, with particularly notable improvements in\nsmall object detection, confirming their effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12420v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2412.06771", "title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty", "authors": ["Meera Hahn", "Wenjun Zeng", "Nithish Kannen", "Rich Galt", "Kartikeya Badola", "Been Kim", "Zi Wang"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.06771v2", "summary": "User prompts for generative AI models are often underspecified, leading to a\nmisalignment between the user intent and models' understanding. As a result,\nusers commonly have to painstakingly refine their prompts. We study this\nalignment problem in text-to-image (T2I) generation and propose a prototype for\nproactive T2I agents equipped with an interface to (1) actively ask\nclarification questions when uncertain, and (2) present their uncertainty about\nuser intent as an understandable and editable belief graph. We build simple\nprototypes for such agents and propose a new scalable and automated evaluation\napproach using two agents, one with a ground truth intent (an image) while the\nother tries to ask as few questions as possible to align with the ground truth.\nWe experiment over three image-text datasets: ImageInWords (Garg et al., 2024),\nCOCO (Lin et al., 2014) and DesignBench, a benchmark we curated with strong\nartistic and design elements. Experiments over the three datasets demonstrate\nthe proposed T2I agents' ability to ask informative questions and elicit\ncrucial information to achieve successful alignment with at least 2 times\nhigher VQAScore (Lin et al., 2024) than the standard T2I generation. Moreover,\nwe conducted human studies and observed that at least 90% of human subjects\nfound these agents and their belief graphs helpful for their T2I workflow,\nhighlighting the effectiveness of our approach. Code and DesignBench can be\nfound at https://github.com/google-deepmind/proactive_t2i_agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.06771v2", "cate": "cs.AI", "date": "2024-12-09", "updated": "2025-07-16"}
{"id": "2507.11891", "title": "Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?", "authors": ["Shuangning Li", "Chonghuan Wang", "Jingyan Wang"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11891v1", "summary": "We study A/B experiments that are designed to compare the performance of two\nrecommendation algorithms. Prior work has shown that the standard\ndifference-in-means estimator is biased in estimating the global treatment\neffect (GTE) due to a particular form of interference between experimental\nunits. Specifically, units under the treatment and control algorithms\ncontribute to a shared pool of data that subsequently train both algorithms,\nresulting in interference between the two groups. The bias arising from this\ntype of data sharing is known as \"symbiosis bias\". In this paper, we highlight\nthat, for decision-making purposes, the sign of the GTE often matters more than\nits precise magnitude when selecting the better algorithm. We formalize this\ninsight under a multi-armed bandit framework and theoretically characterize\nwhen the sign of the expected GTE estimate under data sharing aligns with or\ncontradicts the sign of the true GTE. Our analysis identifies the level of\nexploration versus exploitation as a key determinant of how symbiosis bias\nimpacts algorithm selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11891v1", "cate": "stat.ML", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12426", "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition", "authors": ["Hayat Ullah", "Muhammad Ali Shafique", "Abbas Khan", "Arslan Munir"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2507.12426v1", "summary": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2507.12426v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2501.00226", "title": "Generative Emergent Communication: Large Language Model is a Collective World Model", "authors": ["Tadahiro Taniguchi", "Ryo Ueda", "Tomoaki Nakamura", "Masahiro Suzuki", "Akira Taniguchi"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.00226v2", "summary": "Large Language Models (LLMs) have demonstrated a remarkable ability to\ncapture extensive world knowledge, yet how this is achieved without direct\nsensorimotor experience remains a fundamental puzzle. This study proposes a\nnovel theoretical solution by introducing the Collective World Model\nhypothesis. We argue that an LLM does not learn a world model from scratch;\ninstead, it learns a statistical approximation of a collective world model that\nis already implicitly encoded in human language through a society-wide process\nof embodied, interactive sense-making. To formalize this process, we introduce\ngenerative emergent communication (Generative EmCom), a framework built on the\nCollective Predictive Coding (CPC). This framework models the emergence of\nlanguage as a process of decentralized Bayesian inference over the internal\nstates of multiple agents. We argue that this process effectively creates an\nencoder-decoder structure at a societal scale: human society collectively\nencodes its grounded, internal representations into language, and an LLM\nsubsequently decodes these symbols to reconstruct a latent space that mirrors\nthe structure of the original collective representations. This perspective\nprovides a principled, mathematical explanation for how LLMs acquire their\ncapabilities. The main contributions of this paper are: 1) the formalization of\nthe Generative EmCom framework, clarifying its connection to world models and\nmulti-agent reinforcement learning, and 2) its application to interpret LLMs,\nexplaining phenomena such as distributional semantics as a natural consequence\nof representation reconstruction. This work provides a unified theory that\nbridges individual cognitive development, collective language evolution, and\nthe foundations of large-scale AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.00226v2", "cate": "cs.AI", "date": "2024-12-31", "updated": "2025-07-16"}
{"id": "2507.11895", "title": "Newfluence: Boosting Model interpretability and Understanding in High Dimensions", "authors": ["Haolin Zou", "Arnab Auddy", "Yongchan Kwon", "Kamiar Rahnama Rad", "Arian Maleki"], "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11895v1", "summary": "The increasing complexity of machine learning (ML) and artificial\nintelligence (AI) models has created a pressing need for tools that help\nscientists, engineers, and policymakers interpret and refine model decisions\nand predictions. Influence functions, originating from robust statistics, have\nemerged as a popular approach for this purpose.\n  However, the heuristic foundations of influence functions rely on\nlow-dimensional assumptions where the number of parameters $p$ is much smaller\nthan the number of observations $n$. In contrast, modern AI models often\noperate in high-dimensional regimes with large $p$, challenging these\nassumptions.\n  In this paper, we examine the accuracy of influence functions in\nhigh-dimensional settings. Our theoretical and empirical analyses reveal that\ninfluence functions cannot reliably fulfill their intended purpose. We then\nintroduce an alternative approximation, called Newfluence, that maintains\nsimilar computational efficiency while offering significantly improved\naccuracy.\n  Newfluence is expected to provide more accurate insights than many existing\nmethods for interpreting complex AI models and diagnosing their issues.\nMoreover, the high-dimensional framework we develop in this paper can also be\napplied to analyze other popular techniques, such as Shapley values.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11895v1", "cate": "stat.ML", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12441", "title": "Describe Anything Model for Visual Question Answering on Text-rich Images", "authors": ["Yen-Linh Vu", "Dinh-Thang Duong", "Truong-Binh Duong", "Anh-Khoi Nguyen", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Jianhua Xing", "Xingjian Li", "Tianyang Wang", "Ulas Bagci", "Min Xu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025", "url": "http://arxiv.org/abs/2507.12441v1", "summary": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.", "comment": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12441v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.09037", "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      72 pages, 6 figures", "url": "http://arxiv.org/abs/2504.09037v2", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...", "comment": "72 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2504.09037v2", "cate": "cs.AI", "date": "2025-04-12", "updated": "2025-07-16"}
{"id": "2507.11950", "title": "RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery", "authors": ["Lauren Lui", "Torben Nielsen"], "categories": ["q-bio.GN", "cs.LG"], "primary_category": "Subjects:       Genomics (q-bio.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11950v1", "summary": "Functional annotation of microbial genomes is often biased toward\nprotein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs\n(ncRNAs) that are critical for regulating bacterial and archaeal physiology,\nstress response and metabolism. Identifying ncRNAs directly from genomic\nsequence is a paramount challenge in bioinformatics and biology, essential for\nunderstanding the complete regulatory potential of an organism. This paper\npresents RNAMunin, a machine learning (ML) model that is capable of finding\nncRNAs using genomic sequence alone. It is also computationally viable for\nlarge sequence datasets such as long read metagenomic assemblies with contigs\ntotaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from\napproximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary\nsamples. We know of no other model that can detect ncRNAs based solely on\ngenomic sequence at this scale. Since RNAMunin only requires genomic sequence\nas input, we do not need for an ncRNA to be transcribed to find it, i.e., we do\nnot need transcriptomics data. We wrote this manuscript in a narrative style in\norder to best convey how RNAMunin was developed and how it works in detail.\nUnlike almost all current ML models, at approximately 1M parameters, RNAMunin\nis very small and very fast.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11950v1", "cate": "q-bio.GN", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12449", "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios", "authors": ["Van-Hoang-Anh Phan", "Chi-Tam Nguyen", "Doan-Trung Au", "Thanh-Danh Phan", "Minh-Thien Duong", "My-Ha Le"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, 4 tables, HSI 2025", "url": "http://arxiv.org/abs/2507.12449v1", "summary": "Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8", "comment": "7 pages, 6 figures, 4 tables, HSI 2025", "pdf_url": "http://arxiv.org/pdf/2507.12449v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.00785", "title": "GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning", "authors": ["Sahiti Yerramilli", "Nilay Pande", "Rynaa Grover", "Jayant Sravan Tamarapalli"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00785v2", "summary": "This paper introduces GeoChain, a large-scale benchmark for evaluating\nstep-by-step geographic reasoning in multimodal large language models (MLLMs).\nLeveraging 1.46 million Mapillary street-level images, GeoChain pairs each\nimage with a 21-step chain-of-thought (CoT) question sequence (over 30 million\nQ&A pairs). These sequences guide models from coarse attributes to fine-grained\nlocalization across four reasoning categories - visual, spatial, cultural, and\nprecise geolocation - annotated by difficulty. Images are also enriched with\nsemantic segmentation (150 classes) and a visual locatability score. Our\nbenchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5\nvariants) on a diverse 2,088-image subset reveals consistent challenges: models\nfrequently exhibit weaknesses in visual grounding, display erratic reasoning,\nand struggle to achieve accurate localization, especially as the reasoning\ncomplexity escalates. GeoChain offers a robust diagnostic methodology, critical\nfor fostering significant advancements in complex geographic reasoning within\nMLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00785v2", "cate": "cs.AI", "date": "2025-06-01", "updated": "2025-07-16"}
{"id": "2507.11953", "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025", "url": "http://arxiv.org/abs/2507.11953v1", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.11953v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12455", "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention", "authors": ["Shangpin Peng", "Senqiao Yang", "Li Jiang", "Zhuotao Tian"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12455v1", "summary": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12455v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.04135", "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents", "authors": ["Pei Yang", "Hai Ci", "Mike Zheng Shou"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.04135v3", "summary": "Graphical User Interface (GUI) agents show promising capabilities for\nautomating computer-use tasks and facilitating accessibility, but existing\ninteractive benchmarks are mostly English-only, covering web-use or Windows,\nLinux, and Android environments, but not macOS. macOS is a major OS with\ndistinctive GUI patterns and exclusive applications. To bridge the gaps, we\npresent macOSWorld, the first comprehensive benchmark for evaluating GUI agents\non macOS. macOSWorld features 202 multilingual interactive tasks across 30\napplications (28 macOS-exclusive), with task instructions and OS interfaces\noffered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As\nGUI agents are shown to be vulnerable to deception attacks, macOSWorld also\nincludes a dedicated safety benchmarking subset. Our evaluation on six GUI\nagents reveals a dramatic gap: proprietary computer-use agents lead at above\n30% success rate, while open-source lightweight research models lag at below\n5%, highlighting the need for macOS domain adaptation. Multilingual benchmarks\nalso expose common weaknesses, especially in Arabic, with a 28.8% average\ndegradation compared to English. Results from safety benchmarking also\nhighlight that deception attacks are more general and demand immediate\nattention. macOSWorld is available at https://github.com/showlab/macosworld.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.04135v3", "cate": "cs.AI", "date": "2025-06-04", "updated": "2025-07-16"}
{"id": "2507.11954", "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures, 7 tables", "url": "http://arxiv.org/abs/2507.11954v1", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "comment": "15 pages, 3 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.11954v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12462", "title": "SpatialTrackerV2: 3D Point Tracking Made Easy", "authors": ["Yuxi Xiao", "Jianyuan Wang", "Nan Xue", "Nikita Karaev", "Yuri Makarov", "Bingyi Kang", "Xing Zhu", "Hujun Bao", "Yujun Shen", "Xiaowei Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Computer Vision, ICCV 2025. Huggingface Demo: this https URL , Code: this https URL", "url": "http://arxiv.org/abs/2507.12462v1", "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.", "comment": "International Conference on Computer Vision, ICCV 2025. Huggingface\n  Demo: https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2, Code:\n  https://github.com/henry123-boy/SpaTrackerV2", "pdf_url": "http://arxiv.org/pdf/2507.12462v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.04632", "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?", "authors": ["Yun Qu", "Qi Cheems Wang", "Yixiu Mao", "Vincent Tao Hu", "Xiangyang Ji"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04632v2", "summary": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04632v2", "cate": "cs.AI", "date": "2025-07-07", "updated": "2025-07-16"}
{"id": "2507.11977", "title": "Recent results on searches with boosted Higgs bosons at CMS", "authors": ["Farouk Mokhtar"], "categories": ["hep-ex", "cs.LG"], "primary_category": "Subjects:       High Energy Physics - Experiment (hep-ex)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, The Thirteenth Annual Large Hadron Collider Physics (LHCP2025)", "url": "http://arxiv.org/abs/2507.11977v1", "summary": "The study of boosted Higgs bosons at the LHC provides a unique window to\nprobe Higgs boson couplings at high energy scales and search for signs of\nphysics beyond the standard model. In these proceedings, we present recent\nresults on boosted Higgs boson searches at the CMS experiment, highlighting\ninnovative reconstruction and tagging techniques that enhance sensitivity in\nthis challenging regime.", "comment": "6 pages, 3 figures, The Thirteenth Annual Large Hadron Collider\n  Physics (LHCP2025)", "pdf_url": "http://arxiv.org/pdf/2507.11977v1", "cate": "hep-ex", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12463", "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding", "authors": ["Renjie Li", "Ruijie Ye", "Mingyang Wu", "Hao Frank Yang", "Zhiwen Fan", "Hezhen Hu", "Zhengzhong Tu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12463v1", "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12463v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.05297", "title": "Continuous Classification Aggregation", "authors": ["Zijun Meng"], "categories": ["cs.AI", "econ.TH", "math.CO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages; 2 figures", "url": "http://arxiv.org/abs/2507.05297v5", "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean. We also provide a characterization for the case when $m=p=2$.", "comment": "9 pages; 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.05297v5", "cate": "cs.AI", "date": "2025-07-06", "updated": "2025-07-16"}
{"id": "2507.12021", "title": "Incorporating Fairness Constraints into Archetypal Analysis", "authors": ["Aleix Alcacer", "Irene Epifanio"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12021v1", "summary": "Archetypal Analysis (AA) is an unsupervised learning method that represents\ndata as convex combinations of extreme patterns called archetypes. While AA\nprovides interpretable and low-dimensional representations, it can\ninadvertently encode sensitive attributes, leading to fairness concerns. In\nthis work, we propose Fair Archetypal Analysis (FairAA), a modified formulation\nthat explicitly reduces the influence of sensitive group information in the\nlearned projections. We also introduce FairKernelAA, a nonlinear extension that\naddresses fairness in more complex data distributions. Our approach\nincorporates a fairness regularization term while preserving the structure and\ninterpretability of the archetypes. We evaluate FairAA and FairKernelAA on\nsynthetic datasets, including linear, nonlinear, and multi-group scenarios,\ndemonstrating their ability to reduce group separability -- as measured by mean\nmaximum discrepancy and linear separability -- without substantially\ncompromising explained variance. We further validate our methods on the\nreal-world ANSUR I dataset, confirming their robustness and practical utility.\nThe results show that FairAA achieves a favorable trade-off between utility and\nfairness, making it a promising tool for responsible representation learning in\nsensitive applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12021v1", "cate": "stat.ML", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12464", "title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "authors": ["Muhammed Furkan Dasdelen", "Hyesu Lim", "Michele Buck", "Katharina S. Götze", "Carsten Marr", "Steffen Schneider"], "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures", "url": "http://arxiv.org/abs/2507.12464v1", "summary": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.12464v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.09850", "title": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at the Second AI for Math Workshop at the 42nd International Conference on Machine Learning (ICML 2025)", "url": "http://arxiv.org/abs/2507.09850v3", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.09850v3", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.12091", "title": "Improved Analysis for Sign-based Methods with Momentum Updates", "authors": ["Wei Jiang", "Dingzhi Yu", "Sifan Yang", "Wenhao Yang", "Lijun Zhang"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12091v1", "summary": "In this paper, we present enhanced analysis for sign-based optimization\nalgorithms with momentum updates. Traditional sign-based methods, under the\nseparable smoothness assumption, guarantee a convergence rate of\n$\\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume\nunimodal symmetric stochastic noise. To address these limitations, we\ndemonstrate that signSGD with momentum can achieve the same convergence rate\nusing constant batch sizes without additional assumptions. Our analysis, under\nthe standard $l_2$-smoothness condition, improves upon the result of the prior\nmomentum-based signSGD method by a factor of $\\mathcal{O}(d^{1/2})$, where $d$\nis the problem dimension. Furthermore, we explore sign-based methods with\nmajority vote in distributed settings and show that the proposed momentum-based\nmethod yields convergence rates of $\\mathcal{O}\\left( d^{1/2}T^{-1/2} +\ndn^{-1/2} \\right)$ and $\\mathcal{O}\\left( \\max \\{ d^{1/4}T^{-1/4},\nd^{1/10}T^{-1/5} \\} \\right)$, which outperform the previous results of\n$\\mathcal{O}\\left( dT^{-1/4} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left(\nd^{3/8}T^{-1/8} \\right)$, respectively. Numerical experiments further validate\nthe effectiveness of the proposed methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12091v1", "cate": "math.OC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12465", "title": "PhysX: Physical-Grounded 3D Asset Generation", "authors": ["Ziang Cao", "Zhaoxi Chen", "Linag Pan", "Ziwei Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.12465v1", "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.", "comment": "Project page: https://physx-3d.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.12465v1", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10076", "title": "On Gradual Semantics for Assumption-Based Argumentation", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10076v2", "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10076v2", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.12126", "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12126v1", "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12126v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11711", "title": "Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer", "authors": ["Daniel Moreno-Cartagena", "Guillermo Cabrera-Vives", "Alejandra M. Muñoz Arancibia", "Pavlos Protopapas", "Francisco Förster", "Márcio Catelan", "A. Bayo", "Pablo A. Estévez", "P. Sánchez-Sáez", "Franz E. Bauer", "M. Pavez-Herrera", "L. Hernández-García", "Gonzalo Rojas"], "categories": ["astro-ph.IM", "cs.CV"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      Accepted at the 2025 Workshop on Machine Learning for Astrophysics at the International Conference on Machine Learning (ICML)", "url": "http://arxiv.org/abs/2507.11711v1", "summary": "We explore the use of Swin Transformer V2, a pre-trained vision Transformer,\nfor photometric classification in a multi-survey setting by leveraging light\ncurves from the Zwicky Transient Facility (ZTF) and the Asteroid\nTerrestrial-impact Last Alert System (ATLAS). We evaluate different strategies\nfor integrating data from these surveys and find that a multi-survey\narchitecture which processes them jointly achieves the best performance. These\nresults highlight the importance of modeling survey-specific characteristics\nand cross-survey interactions, and provide guidance for building scalable\nclassifiers for future time-domain astronomy.", "comment": "Accepted at the 2025 Workshop on Machine Learning for Astrophysics at\n  the International Conference on Machine Learning (ICML)", "pdf_url": "http://arxiv.org/pdf/2507.11711v1", "cate": "astro-ph.IM", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2310.20360", "title": "Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory", "authors": ["Arnulf Jentzen", "Benno Kuckuck", "Philippe von Wurstemberger"], "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "math.PR", "stat.ML", "68T07"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      737 pages, 33 figures, 45 source codes, 87 exercises. In v3, Chapters 5, 6, and 7 in Part III (Optimization) have been expanded", "url": "http://arxiv.org/abs/2310.20360v3", "summary": "This book aims to provide an introduction to the topic of deep learning\nalgorithms. We review essential components of deep learning algorithms in full\nmathematical detail including different artificial neural network (ANN)\narchitectures (such as fully-connected feedforward ANNs, convolutional ANNs,\nrecurrent ANNs, residual ANNs, and ANNs with batch normalization) and different\noptimization algorithms (such as the basic stochastic gradient descent (SGD)\nmethod, accelerated methods, and adaptive methods). We also cover several\ntheoretical aspects of deep learning algorithms such as approximation\ncapacities of ANNs (including a calculus for ANNs), optimization theory\n(including Kurdyka-{\\L}ojasiewicz inequalities), and generalization errors. In\nthe last part of the book some deep learning approximation methods for PDEs are\nreviewed including physics-informed neural networks (PINNs) and deep Galerkin\nmethods. We hope that this book will be useful for students and scientists who\ndo not yet have any background in deep learning at all and would like to gain a\nsolid foundation as well as for practitioners who would like to obtain a firmer\nmathematical understanding of the objects and methods considered in deep\nlearning.", "comment": "737 pages, 33 figures, 45 source codes, 87 exercises. In v3, Chapters\n  5, 6, and 7 in Part III (Optimization) have been expanded", "pdf_url": "http://arxiv.org/pdf/2310.20360v3", "cate": "cs.LG", "date": "2023-10-31", "updated": "2025-07-15"}
{"id": "2507.12256", "title": "Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator", "authors": ["Monica Lăcătuş", "Matthias Möller"], "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      31 pages, 14 figures", "url": "http://arxiv.org/abs/2507.12256v1", "summary": "Direct numerical simulation of turbulent flows at high Reynolds numbers\nremains a major challenge for traditional computational fluid dynamics (CFD)\ntools running on classical computer hardware. This has motivated growing\ninterest in quantum algorithms for CFD to enable flow simulations on quantum\ncomputers. The reason being that these computers are expected to deliver\npotential speed-ups for certain problems. One promising quantum CFD approach is\na fully quantum implementation of the lattice Boltzmann method called QLBM.\nAlthough efficient quantum routines are now available for the streaming step,\nimplementing the nonlinear, irreversible collision step with a low depth\ncircuit that avoids additional ancilla qubits, probabilistic post-selection and\nrepeated executions remains a significant challenge. In this study, we address\nthis challenge by introducing a framework for learning a surrogate quantum\ncircuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision\noperator for the D2Q9 lattice. The four qubit circuit is trained to respect the\nphysical properties of the BGK collision operator, including mass and momentum\nconservation, D8 equivariance and scale equivariance. When compiled to the gate\nset used by IBM Heron processor under the assumption of full qubit\nconnectivity, the 15 block SQC requires only 2,430 native gates and uses\nneither ancilla qubits nor post-selection or repeated executions. Moreover, its\ndepth is independent of the grid resolution, as collision is a local operation\nthat can exploit quantum parallelism to its full extent. We validate the SQC on\ntwo benchmark flows, the Taylor Green vortex decay and the lid driven cavity,\ndemonstrating that it accurately captures vortex dissipation and flow\nrecirculation.", "comment": "31 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.12256v1", "cate": "quant-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11853", "title": "A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy", "authors": ["J. Senthilnath", "Jayasanker Jayabalan", "Zhuoyi Lin", "Aye Phyu Phyu Aung", "Chen Hao", "Kaixin Xu", "Yeow Kheng Lim", "F. C. Wellstood"], "categories": ["physics.ins-det", "cs.CV"], "primary_category": "Subjects:       Instrumentation and Detectors (physics.ins-det)", "pdf_link": null, "comments": "Comments:      copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "url": "http://arxiv.org/abs/2507.11853v1", "summary": "The development of advanced packaging is essential in the semiconductor\nmanufacturing industry. However, non-destructive testing (NDT) of advanced\npackaging becomes increasingly challenging due to the depth and complexity of\nthe layers involved. In such a scenario, Magnetic field imaging (MFI) enables\nthe imaging of magnetic fields generated by currents. For MFI to be effective\nin NDT, the magnetic fields must be converted into current density. This\nconversion has typically relied solely on a Fast Fourier Transform (FFT) for\nmagnetic field inversion; however, the existing approach does not consider eddy\ncurrent effects or image misalignment in the test setup. In this paper, we\npresent a spatial-physics informed model (SPIM) designed for a 3D spiral sample\nscanned using Superconducting QUantum Interference Device (SQUID) microscopy.\nThe SPIM encompasses three key components: i) magnetic image enhancement by\naligning all the \"sharp\" wire field signals to mitigate the eddy current effect\nusing both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)\nmagnetic image alignment that addresses skew effects caused by any misalignment\nof the scanning SQUID microscope relative to the wire segments; and (iii) an\ninversion method for converting magnetic fields to magnetic currents by\nintegrating the Biot-Savart Law with FFT. The results show that the SPIM\nimproves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.\nAlso, we were able to remove rotational and skew misalignments of 0.30 in a\nreal image. Overall, SPIM highlights the potential of combining spatial\nanalysis with physics-driven models in practical applications.", "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "pdf_url": "http://arxiv.org/pdf/2507.11853v1", "cate": "physics.ins-det", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2312.14628", "title": "Holistic analysis on the sustainability of Federated Learning across AI product lifecycle", "authors": ["Hongliu Cao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in ECAI 2025 (PAIS track)", "url": "http://arxiv.org/abs/2312.14628v3", "summary": "In light of emerging legal requirements and policies focused on privacy\nprotection, there is a growing trend of companies across various industries\nadopting Federated Learning (FL). This decentralized approach involves multiple\nclients or silos, collaboratively training a global model under the\ncoordination of a central server while utilizing their private local data.\nUnlike traditional methods that necessitate data sharing and transmission,\nCross-Silo FL allows clients to share model updates rather than raw data,\nthereby enhancing privacy. Despite its growing adoption, the carbon impact\nassociated with Cross-Silo FL remains poorly understood due to the limited\nresearch in this area. This study seeks to bridge this gap by evaluating the\nsustainability of Cross-Silo FL throughout the entire AI product lifecycle,\nextending the analysis beyond the model training phase alone. We systematically\ncompare this decentralized method with traditional centralized approaches and\npresent a robust quantitative framework for assessing the costs and CO2\nemissions in real-world Cross-Silo FL environments. Our findings indicate that\nthe energy consumption and costs of model training are comparable between\nCross-Silo Federated Learning and Centralized Learning. However, the additional\ndata transfer and storage requirements inherent in Centralized Learning can\nresult in significant, often overlooked CO2 emissions. Moreover, we introduce\nan innovative data and application management system that integrates Cross-Silo\nFL and analytics, aiming at improving the sustainability and economic\nefficiency of IT enterprises.", "comment": "Accepted in ECAI 2025 (PAIS track)", "pdf_url": "http://arxiv.org/pdf/2312.14628v3", "cate": "cs.LG", "date": "2023-12-22", "updated": "2025-07-16"}
{"id": "2507.12404", "title": "Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts", "authors": ["Yeming Xian", "Xiaoming Wang", "Yanfa Yan"], "categories": ["physics.data-an", "cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Data Analysis, Statistics and Probability (physics.data-an)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2507.12404v1", "summary": "Understanding and predicting the activity of oxide perovskite catalysts for\nthe oxygen evolution reaction (OER) requires descriptors that are both accurate\nand physically interpretable. While symbolic regression (SR) offers a path to\ndiscover such formulas, its performance degrades with high-dimensional inputs\nand small datasets. We present a two-phase framework that combines neural\nnetworks (NN), feature importance analysis, and symbolic regression (SR) to\ndiscover interpretable descriptors for OER activity in oxide perovskites. In\nPhase I, using a small dataset and seven structural features, we reproduce and\nimprove the known {\\mu}/t descriptor by engineering composite features and\napplying symbolic regression, achieving training and validation MAEs of 22.8\nand 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce\ndimensionality, and identify LUMO energy as a key electronic descriptor. A\nfinal formula using {\\mu}/t, {\\mu}/RA, and LUMO energy achieves improved\naccuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong\nphysical interpretability. Our results demonstrate that NN-guided symbolic\nregression enables accurate, interpretable, and physically meaningful\ndescriptor discovery in data-scarce regimes, indicating interpretability need\nnot sacrifice accuracy for materials informatics.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2507.12404v1", "cate": "physics.data-an", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2309.10527", "title": "SPOT: Scalable 3D Pre-training via Occupancy Prediction for Learning Transferable 3D Representations", "authors": ["Xiangchao Yan", "Runjian Chen", "Bo Zhang", "Hancheng Ye", "Renqiu Xia", "Jiakang Yuan", "Hongbin Zhou", "Xinyu Cai", "Botian Shi", "Wenqi Shao", "Ping Luo", "Yu Qiao", "Tao Chen", "Junchi Yan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      SPOT has been accepted as a Regular Paper in T-PAMI, and code is available at this https URL", "url": "http://arxiv.org/abs/2309.10527v4", "summary": "Annotating 3D LiDAR point clouds for perception tasks is fundamental for many\napplications e.g., autonomous driving, yet it still remains notoriously\nlabor-intensive. Pretraining-finetuning approach can alleviate the labeling\nburden by fine-tuning a pre-trained backbone across various downstream datasets\nas well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training\nvia Occupancy prediction for learning Transferable 3D representations under\nsuch a label-efficient fine-tuning paradigm. SPOT achieves effectiveness on\nvarious public datasets with different downstream tasks, showcasing its general\nrepresentation power, cross-domain robustness and data scalability which are\nthree key factors for real-world application. Specifically, we both\ntheoretically and empirically show, for the first time, that general\nrepresentations learning can be achieved through the task of occupancy\nprediction. Then, to address the domain gap caused by different LiDAR sensors\nand annotation methods, we develop a beam re-sampling technique for point cloud\naugmentation combined with class-balancing strategy. Furthermore, scalable\npre-training is observed, that is, the downstream performance across all the\nexperiments gets better with more pre-training data. Additionally, such\npre-training strategy also remains compatible with unlabeled data. The hope is\nthat our findings will facilitate the understanding of LiDAR points and pave\nthe way for future advancements in LiDAR pre-training.", "comment": "SPOT has been accepted as a Regular Paper in T-PAMI, and code is\n  available at https://github.com/PJLab-ADG/3DTrans", "pdf_url": "http://arxiv.org/pdf/2309.10527v4", "cate": "cs.CV", "date": "2023-09-19", "updated": "2025-07-16"}
{"id": "2401.15801", "title": "On the Statistical Properties of Generative Adversarial Models for Low Intrinsic Data Dimension", "authors": ["Saptarshi Chakraborty", "Peter L. Bartlett"], "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Journal of Machine Learning Research (2025), volume 26", "url": "http://arxiv.org/abs/2401.15801v2", "summary": "Despite the remarkable empirical successes of Generative Adversarial Networks\n(GANs), the theoretical guarantees for their statistical accuracy remain rather\npessimistic. In particular, the data distributions on which GANs are applied,\nsuch as natural images, are often hypothesized to have an intrinsic\nlow-dimensional structure in a typically high-dimensional feature space, but\nthis is often not reflected in the derived rates in the state-of-the-art\nanalyses. In this paper, we attempt to bridge the gap between the theory and\npractice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),\nby deriving statistical guarantees on the estimated densities in terms of the\nintrinsic dimension of the data and the latent space. We analytically show that\nif one has access to $n$ samples from the unknown target distribution and the\nnetwork architectures are properly chosen, the expected Wasserstein-1 distance\nof the estimates from the target scales as $O\\left( n^{-1/d_\\mu } \\right)$ for\nGANs and $\\tilde{O}\\left( n^{-1/(d_\\mu+\\ell)} \\right)$ for BiGANs, where\n$d_\\mu$ and $\\ell$ are the upper Wasserstein-1 dimension of the\ndata-distribution and latent-space dimension, respectively. The theoretical\nanalyses not only suggest that these methods successfully avoid the curse of\ndimensionality, in the sense that the exponent of $n$ in the error rates does\nnot depend on the data dimension but also serve to bridge the gap between the\ntheoretical analyses of GANs and the known sharp rates from optimal transport\nliterature. Additionally, we demonstrate that GANs can effectively achieve the\nminimax optimal rate even for non-smooth underlying distributions, with the use\nof interpolating generator networks.", "comment": "Journal of Machine Learning Research (2025), volume 26", "pdf_url": "http://arxiv.org/pdf/2401.15801v2", "cate": "stat.ML", "date": "2024-01-28", "updated": "2025-07-16"}
{"id": "2507.12466", "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      44 pages, 25 figures, 13 tables", "url": "http://arxiv.org/abs/2507.12466v1", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "comment": "44 pages, 25 figures, 13 tables", "pdf_url": "http://arxiv.org/pdf/2507.12466v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2312.05968", "title": "Jumpstarting Surgical Computer Vision", "authors": ["Deepak Alapatt", "Aditya Murali", "Vinkle Srivastav", "Pietro Mascagni", "AI4SafeChole Consortium", "Nicolas Padoy"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2312.05968v2", "summary": "Consensus amongst researchers and industry points to a lack of large,\nrepresentative annotated datasets as the biggest obstacle to progress in the\nfield of surgical data science. Advances in Self-Supervised Learning (SSL)\nrepresent a solution, reducing the dependence on large labeled datasets by\nproviding task-agnostic initializations. However, the robustness of current\nself-supervised learning methods to domain shifts remains unclear, limiting our\nunderstanding of its utility for leveraging diverse sources of surgical data.\nShifting the focus from methods to data, we demonstrate that the downstream\nvalue of SSL-based initializations is intricately intertwined with the\ncomposition of pre-training datasets. These results underscore an important gap\nthat needs to be filled as we scale self-supervised approaches toward building\ngeneral-purpose \"foundation models\" that enable diverse use-cases within the\nsurgical domain. Through several stages of controlled experimentation, we\ndevelop recommendations for pretraining dataset composition evidenced through\nover 300 experiments spanning 20 pre-training datasets, 9 surgical procedures,\n7 centers (hospitals), 3 labeled-data settings, 3 downstream tasks, and\nmultiple runs. Using the approaches here described, we outperform\nstate-of-the-art pre-trainings on two public benchmarks for phase recognition:\nup to 2.2% on Cholec80 and 5.1% on AutoLaparo.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2312.05968v2", "cate": "cs.CV", "date": "2023-12-10", "updated": "2025-07-16"}
{"id": "2404.09158", "title": "StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging", "authors": ["Xuelong Li", "Hongjun An", "Haofei Zhao", "Guangying Li", "Bo Liu", "Xing Wang", "Guanghua Cheng", "Guojun Wu", "Zhe Sun"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Transactions on Image Processing (T-IP)", "url": "http://arxiv.org/abs/2404.09158v4", "summary": "In this paper, we introduce StreakNet-Arch, a real-time, end-to-end\nbinary-classification framework based on our self-developed Underwater Carrier\nLiDAR-Radar (UCLR) that embeds Self-Attention and our novel Double Branch Cross\nAttention (DBC-Attention) to enhance scatter suppression. Under controlled\nwater tank validation conditions, StreakNet-Arch with Self-Attention or\nDBC-Attention outperforms traditional bandpass filtering and achieves higher\n$F_1$ scores than learning-based MP networks and CNNs at comparable model size\nand complexity. Real-time benchmarks on an NVIDIA RTX 3060 show a constant\nAverage Imaging Time (54 to 84 ms) regardless of frame count, versus a linear\nincrease (58 to 1,257 ms) for conventional methods. To facilitate further\nresearch, we contribute a publicly available streak-tube camera image dataset\ncontains 2,695,168 real-world underwater 3D point cloud data. More importantly,\nwe validate our UCLR system in a South China Sea trial, reaching an error of\n46mm for 3D target at 1,000 m depth and 20 m range. Source code and data are\navailable at https://github.com/BestAnHongjun/StreakNet .", "comment": "Accepted by IEEE Transactions on Image Processing (T-IP)", "pdf_url": "http://arxiv.org/pdf/2404.09158v4", "cate": "cs.CV", "date": "2024-04-14", "updated": "2025-07-16"}
{"id": "2306.06974", "title": "A Computational Theory and Semi-Supervised Algorithm for Clustering", "authors": ["Nassir Mohammad"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.06974v2", "summary": "A computational theory for clustering and a semi-supervised clustering\nalgorithm is presented. Clustering is defined to be the obtainment of groupings\nof data such that each group contains no anomalies with respect to a chosen\ngrouping principle and measure; all other examples are considered to be fringe\npoints, isolated anomalies, anomalous clusters or unknown clusters. More\nprecisely, after appropriate modelling under the assumption of uniform random\ndistribution, any example whose expectation of occurrence is <1 with respect to\na group is considered an anomaly; otherwise it is assigned a membership of that\ngroup. Thus, clustering is conceived as the dual of anomaly detection. The\nrepresentation of data is taken to be the Euclidean distance of a point to a\ncluster median. This is due to the robustness properties of the median to\noutliers, its approximate location of centrality and so that decision\nboundaries are general purpose. The kernel of the clustering method is the\nperception anomaly detection algorithm, resulting in a parameter-free, fast,\nand efficient clustering algorithm. Acknowledging that clustering is an\ninteractive and iterative process, the algorithm relies on a small fraction of\nknown relationships between examples. These relationships serve as seeds to\ndefine the user's objectives and guide the clustering process. The method then\nexpands the clusters accordingly, leaving the remaining examples for\nexploration and subsequent iterations. Results are presented on synthetic and\nrealworld data sets, demonstrating the advantages over the most popular\nunsupervised and semi-supervised clustering methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.06974v2", "cate": "cs.LG", "date": "2023-06-12", "updated": "2025-07-16"}
{"id": "2403.06403", "title": "PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models", "authors": ["Qingdong He", "Jinlong Peng", "Zhengkai Jiang", "Xiaobin Hu", "Jiangning Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 E2E3D Workshop", "url": "http://arxiv.org/abs/2403.06403v5", "summary": "Recent success of vision foundation models have shown promising performance\nfor the 2D perception tasks. However, it is difficult to train a 3D foundation\nnetwork directly due to the limited dataset and it remains under explored\nwhether existing foundation models can be lifted to 3D space seamlessly. In\nthis paper, we present PointSeg, a novel training-free paradigm that leverages\noff-the-shelf vision foundation models to address 3D scene perception tasks.\nPointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to\nalign their corresponding pixels across frames. Concretely, we design a\ntwo-branch prompts learning structure to construct the 3D point-box prompts\npairs, combining with the bidirectional matching strategy for accurate point\nand proposal prompts generation. Then, we perform the iterative post-refinement\nadaptively when cooperated with different vision foundation models. Moreover,\nwe design a affinity-aware merging algorithm to improve the final ensemble\nmasks. PointSeg demonstrates impressive segmentation performance across various\ndatasets, all without training. Specifically, our approach significantly\nsurpasses the state-of-the-art specialist training-free model by 14.1$\\%$,\n12.3$\\%$, and 12.6$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets,\nrespectively. On top of that, PointSeg can incorporate with various foundation\nmodels and even surpasses the specialist training-based methods by\n3.4$\\%$-5.4$\\%$ mAP across various datasets, serving as an effective generalist\nmodel.", "comment": "Accepted by ICCV 2025 E2E3D Workshop", "pdf_url": "http://arxiv.org/pdf/2403.06403v5", "cate": "cs.CV", "date": "2024-03-11", "updated": "2025-07-16"}
{"id": "2406.08788", "title": "Towards Understanding Link Predictor Generalizability Under Distribution Shifts", "authors": ["Jay Revolinsky", "Harry Shomer", "Jiliang Tang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      KDD 25' Datasets & Benchmarks Track, 14 pages, 8 figures, 18 tables", "url": "http://arxiv.org/abs/2406.08788v3", "summary": "State-of-the-art link prediction (LP) models demonstrate impressive benchmark\nresults. However, popular benchmark datasets often assume that training,\nvalidation, and testing samples are representative of the overall dataset\ndistribution. In real-world situations, this assumption is often incorrect;\nuncontrolled factors lead new dataset samples to come from a different\ndistribution than training samples. Additionally, the majority of recent work\nwith graph dataset shift focuses on node- and graph-level tasks, largely\nignoring link-level tasks. To bridge this gap, we introduce a novel splitting\nstrategy, known as LPShift, which utilizes structural properties to induce a\ncontrolled distribution shift. We verify LPShift's effect through empirical\nevaluation of SOTA LP models on 16 LPShift variants of original dataset splits,\nwith results indicating drastic changes to model performance. Additional\nexperiments demonstrate graph structure has a strong influence on the success\nof current generalization methods. Source Code Available Here:\nhttps://github.com/revolins/LPShift", "comment": "KDD 25' Datasets & Benchmarks Track, 14 pages, 8 figures, 18 tables", "pdf_url": "http://arxiv.org/pdf/2406.08788v3", "cate": "cs.LG", "date": "2024-06-13", "updated": "2025-07-16"}
{"id": "2310.02718", "title": "Understanding Pan-Sharpening via Generalized Inverse", "authors": ["Shiqi Liu", "Yihua Tan", "Yutong Bai", "Alan Yuille"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.02718v3", "summary": "Pan-sharpening algorithms utilize a panchromatic image and a multispectral\nimage to generate a high spatial and high spectral image. However, the\noptimizations of the algorithms are designed with different standards. We\nemploy a simple matrix equation to describe the Pan-sharpening problem. The\nconditions for the existence of a solution and the acquisition of spectral and\nspatial resolution are discussed. A down-sampling enhancement method is\nintroduced to improve the estimation of spatial and spectral down-sample\nmatrices.\n  Using generalized inverse theory, we discovered two kinds of solution spaces\nof generalized inverse matrix formulations, which correspond to the two\nprominent classes of Pan-sharpening methods: component substitution and\nmulti-resolution analysis. Specifically, the Gram-Schmidt adaptive method is\ndemonstrated to align with the generalized inverse matrix formulation of\ncomponent substitution. A model prior of the generalized inverse matrix of the\nspectral function is rendered. Theoretical errors are analyzed. The diffusion\nprior is naturally embedded with the help of general solution spaces of the\ngeneralized inverse form, enabling the acquisition of refined Pan-sharpening\nresults.\n  Extensive experiments, including comparative, synthetic, real-data ablation\nand diffusion-related tests are conducted. The proposed methods produce\nqualitatively sharper and superior results in both synthetic and real\nexperiments. The down-sampling enhancement method demonstrates quantitatively\nand qualitatively better outcomes in real-data experiments. The diffusion prior\ncan significantly improve the performance of our methods across almost all\nevaluation measures.\n  The generalized inverse matrix theory helps deepen the understanding of\nPan-sharpening mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.02718v3", "cate": "cs.LG", "date": "2023-10-04", "updated": "2025-07-16"}
{"id": "2403.14559", "title": "VAPO: Visibility-Aware Keypoint Localization for Efficient 6DoF Object Pose Estimation", "authors": ["Ruyi Lian", "Yuewei Lin", "Longin Jan Latecki", "Haibin Ling"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted for publication in the Proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) as oral presentation", "url": "http://arxiv.org/abs/2403.14559v4", "summary": "Localizing predefined 3D keypoints in a 2D image is an effective way to\nestablish 3D-2D correspondences for instance-level 6DoF object pose estimation.\nHowever, unreliable localization results of invisible keypoints degrade the\nquality of correspondences. In this paper, we address this issue by localizing\nthe important keypoints in terms of visibility. Since keypoint visibility\ninformation is currently missing in the dataset collection process, we propose\nan efficient way to generate binary visibility labels from available\nobject-level annotations, for keypoints of both asymmetric objects and\nsymmetric objects. We further derive real-valued visibility-aware importance\nfrom binary labels based on the PageRank algorithm. Taking advantage of the\nflexibility of our visibility-aware importance, we construct VAPO\n(Visibility-Aware POse estimator) by integrating the visibility-aware\nimportance with a state-of-the-art pose estimation algorithm, along with\nadditional positional encoding. VAPO can work in both CAD-based and CAD-free\nsettings. Extensive experiments are conducted on popular pose estimation\nbenchmarks including Linemod, Linemod-Occlusion, and YCB-V, demonstrating that\nVAPO clearly achieves state-of-the-art performances. Project page:\nhttps://github.com/RuyiLian/VAPO.", "comment": "accepted for publication in the Proceedings of the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025) as\n  oral presentation", "pdf_url": "http://arxiv.org/pdf/2403.14559v4", "cate": "cs.CV", "date": "2024-03-21", "updated": "2025-07-15"}
{"id": "2406.14335", "title": "Linearly-Interpretable Concept Embedding Models for Text Analysis", "authors": ["Francesco De Santis", "Philippe Bich", "Gabriele Ciravegna", "Pietro Barbiero", "Danilo Giordano", "Tania Cerquitelli"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.14335v2", "summary": "Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.14335v2", "cate": "cs.CL", "date": "2024-06-20", "updated": "2025-07-16"}
{"id": "2311.05128", "title": "Exploring and Analyzing Wildland Fire Data Via Machine Learning Techniques", "authors": ["Dipak Dulal", "Joseph J. Charney", "Michael Gallagher", "Carmeliza Navasca", "Nicholas Skowronski"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This version has been significantly updated and superseded by a more complete and revised version under the new title \"Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire\", available at arXiv ID: arXiv:2507.11012", "url": "http://arxiv.org/abs/2311.05128v2", "summary": "This research project investigated the correlation between a 10 Hz time\nseries of thermocouple temperatures and turbulent kinetic energy (TKE) computed\nfrom wind speeds collected from a small experimental prescribed burn at the\nSilas Little Experimental Forest in New Jersey, USA. The primary objective of\nthis project was to explore the potential for using thermocouple temperatures\nas predictors for estimating the TKE produced by a wildland fire. Machine\nlearning models, including Deep Neural Networks, Random Forest Regressor,\nGradient Boosting, and Gaussian Process Regressor, are employed to assess the\npotential for thermocouple temperature perturbations to predict TKE values.\nData visualization and correlation analyses reveal patterns and relationships\nbetween thermocouple temperatures and TKE, providing insight into the\nunderlying dynamics. The project achieves high accuracy in predicting TKE by\nemploying various machine learning models despite a weak correlation between\nthe predictors and the target variable. The results demonstrate significant\nsuccess, particularly from regression models, in accurately estimating the TKE.\nThe research findings contribute to fire behavior and smoke modeling science,\nemphasizing the importance of incorporating machine learning approaches and\nidentifying complex relationships between fine-scale fire behavior and\nturbulence. Accurate TKE estimation using thermocouple temperatures allows for\nthe refinement of models that can inform decision-making in fire management\nstrategies, facilitate effective risk mitigation, and optimize fire management\nefforts. This project highlights the valuable role of machine learning\ntechniques in analyzing wildland fire data, showcasing their potential to\nadvance fire research and management practices.", "comment": "This version has been significantly updated and superseded by a more\n  complete and revised version under the new title \"Leveraging Advanced Machine\n  Learning to Predict Turbulence Dynamics from Temperature Observations at an\n  Experimental Prescribed Fire\", available at arXiv ID: arXiv:2507.11012", "pdf_url": "http://arxiv.org/pdf/2311.05128v2", "cate": "cs.LG", "date": "2023-11-09", "updated": "2025-07-16"}
{"id": "2406.08379", "title": "Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities", "authors": ["Michele Mazzamuto", "Antonino Furnari", "Yoichi Sato", "Giovanni Maria Farinella"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.08379v5", "summary": "We address the challenge of unsupervised mistake detection in egocentric\nvideo of skilled human activities through the analysis of gaze signals. While\ntraditional methods rely on manually labeled mistakes, our approach does not\nrequire mistake annotations, hence overcoming the need of domain-specific\nlabeled data. Based on the observation that eye movements closely follow object\nmanipulation activities, we assess to what extent eye-gaze signals can support\nmistake detection, proposing to identify deviations in attention patterns\nmeasured through a gaze tracker with respect to those estimated by a gaze\nprediction model. Since predicting gaze in video is characterized by high\nuncertainty, we propose a novel gaze completion task, where eye fixations are\npredicted from visual observations and partial gaze trajectories, and\ncontribute a novel gaze completion approach which explicitly models\ncorrelations between gaze information and local visual tokens. Inconsistencies\nbetween predicted and observed gaze trajectories act as an indicator to\nidentify mistakes. Experiments highlight the effectiveness of the proposed\napproach in different settings, with relative gains up to +14%, +11%, and +5%\nin EPIC-Tent, HoloAssist and IndustReal respectively, remarkably matching\nresults of supervised approaches without seeing any labels. We further show\nthat gaze-based analysis is particularly useful in the presence of skilled\nactions, low action execution confidence, and actions requiring hand-eye\ncoordination and object manipulation skills. Our method is ranked first on the\nHoloAssist Mistake Detection challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.08379v5", "cate": "cs.CV", "date": "2024-06-12", "updated": "2025-07-16"}
{"id": "2410.06405", "title": "Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects", "authors": ["Wenhao Li", "Yudong Xu", "Scott Sanner", "Elias Boutros Khalil"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.06405v2", "summary": "The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on\nvisual reasoning in the evaluation of Artificial Intelligence systems. In its\noriginal framing, an ARC task requires solving a program synthesis problem over\nsmall 2D images using a few input-output training pairs. In this work, we adopt\nthe recently popular data-driven approach to the ARC and ask whether a Vision\nTransformer (ViT) can learn the implicit mapping, from input image to output\nimage, that underlies the task. We show that a ViT -- otherwise a\nstate-of-the-art model for images -- fails dramatically on most ARC tasks even\nwhen trained on one million examples per task. This points to an inherent\nrepresentational deficiency of the ViT architecture that makes it incapable of\nuncovering the simple structured mappings underlying the ARC tasks. Building on\nthese insights, we propose ViTARC, a ViT-style architecture that unlocks some\nof the visual reasoning capabilities required by the ARC. Specifically, we use\na pixel-level input representation, design a spatially-aware tokenization\nscheme, and introduce a novel object-based positional encoding that leverages\nautomatic segmentation, among other enhancements. Our task-specific ViTARC\nmodels achieve a test solve rate close to 100% on more than half of the 400\npublic ARC tasks strictly through supervised learning from input-output grids.\nThis calls attention to the importance of imbuing the powerful (Vision)\nTransformer with the correct inductive biases for abstract visual reasoning\nthat are critical even when the training data is plentiful and the mapping is\nnoise-free. Hence, ViTARC provides a strong foundation for future research in\nvisual reasoning using transformer-based architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.06405v2", "cate": "cs.CV", "date": "2024-10-08", "updated": "2025-07-15"}
{"id": "2403.01234", "title": "Active Deep Kernel Learning of Molecular Properties: Realizing Dynamic Structural Embeddings", "authors": ["Ayana Ghosh", "Maxim Ziatdinov", "Sergei V. Kalinin"], "categories": ["cs.LG", "physics.chem-ph", "physics.comp-ph", "physics.data-an"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.01234v2", "summary": "As vast databases of chemical identities become increasingly available, the\nchallenge shifts to how we effectively explore and leverage these resources to\nstudy molecular properties. This paper presents an active learning approach for\nmolecular discovery using Deep Kernel Learning (DKL), demonstrated on the QM9\ndataset. DKL links structural embeddings directly to properties, creating\norganized latent spaces that prioritize relevant property information. By\niteratively recalculating embedding vectors in alignment with target\nproperties, DKL uncovers concentrated maxima representing key molecular\nproperties and reveals unexplored regions with potential for innovation. This\napproach underscores DKL's potential in advancing molecular research and\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.01234v2", "cate": "cs.LG", "date": "2024-03-02", "updated": "2025-07-16"}
{"id": "2408.02426", "title": "Boosting Memory Efficiency in Transfer Learning for High-Resolution Medical Image Classification", "authors": ["Yijin Huang", "Pujin Cheng", "Roger Tam", "Xiaoying Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.02426v3", "summary": "The success of large-scale pre-trained models has established fine-tuning as\na standard method for achieving significant improvements in downstream tasks.\nHowever, fine-tuning the entire parameter set of a pre-trained model is costly.\nParameter-efficient transfer learning (PETL) has recently emerged as a\ncost-effective alternative for adapting pre-trained models to downstream tasks.\nDespite its advantages, the increasing model size and input resolution present\nchallenges for PETL, as the training memory consumption is not reduced as\neffectively as the parameter usage. In this paper, we introduce Fine-grained\nPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medical\nimage classification, which significantly reduces the training memory\nconsumption compared to other PETL methods. FPT+ performs transfer learning by\ntraining a lightweight side network and accessing pre-trained knowledge from a\nlarge pre-trained model (LPM) through fine-grained prompts and fusion modules.\nSpecifically, we freeze the LPM of interest and construct a learnable\nlightweight side network. The frozen LPM processes high-resolution images to\nextract fine-grained features, while the side network employs corresponding\ndown-sampled low-resolution images to minimize the memory usage. To enable the\nside network to leverage pre-trained knowledge, we propose fine-grained prompts\nand fusion modules, which collaborate to summarize information through the\nLPM's intermediate activations. We evaluate FPT+ on eight medical image\ndatasets of varying sizes, modalities, and complexities. Experimental results\ndemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the\nlearnable parameters and 3.18% of the memory required for fine-tuning an entire\nViT-B model. Our code is available on https://github.com/YijinHuang/FPT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.02426v3", "cate": "cs.CV", "date": "2024-08-05", "updated": "2025-07-16"}
{"id": "2410.09474", "title": "Distilling Invariant Representations with Dual Augmentation", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07", "I.4; I.2"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      After further review, we determined that the submission does not meet the quality standards we intended", "url": "http://arxiv.org/abs/2410.09474v4", "summary": "Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.", "comment": "After further review, we determined that the submission does not meet\n  the quality standards we intended", "pdf_url": "http://arxiv.org/pdf/2410.09474v4", "cate": "cs.CV", "date": "2024-10-12", "updated": "2025-07-16"}
{"id": "2403.02004", "title": "Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities", "authors": ["Rocco Caprio", "Juan Kuntz", "Samuel Power", "Adam M. Johansen"], "categories": ["cs.LG", "math.FA", "math.OC", "stat.CO", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.02004v3", "summary": "We prove non-asymptotic error bounds for particle gradient descent (PGD,\nKuntz et al., 2023), a recently introduced algorithm for maximum likelihood\nestimation of large latent variable models obtained by discretizing a gradient\nflow of the free energy. We begin by showing that the flow converges\nexponentially fast to the free energy's minimizers for models satisfying a\ncondition that generalizes both the log-Sobolev and the Polyak--{\\L}ojasiewicz\ninequalities (LSI and P{\\L}I, respectively). We achieve this by extending a\nresult well-known in the optimal transport literature (that the LSI implies the\nTalagrand inequality) and its counterpart in the optimization literature (that\nthe P{\\L}I implies the so-called quadratic growth condition), and applying the\nextension to our new setting. We also generalize the Bakry--\\'Emery Theorem and\nshow that the LSI/P{\\L}I extension holds for models with strongly concave\nlog-likelihoods. For such models, we further control PGD's discretization error\nand obtain the non-asymptotic error bounds. While we are motivated by the study\nof PGD, we believe that the inequalities and results we extend may be of\nindependent interest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.02004v3", "cate": "cs.LG", "date": "2024-03-04", "updated": "2025-07-16"}
{"id": "2410.14987", "title": "SeaS: Few-shot Industrial Anomaly Image Generation with Separation and Sharing Fine-tuning", "authors": ["Zhewei Dai", "Shilei Zeng", "Haotian Liu", "Xurui Li", "Feng Xue", "Yu Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.14987v2", "summary": "We introduce SeaS, a unified industrial generative model for automatically\ncreating diverse anomalies, authentic normal products, and precise anomaly\nmasks. While extensive research exists, most efforts either focus on specific\ntasks, i.e., anomalies or normal products only, or require separate models for\neach anomaly type. Consequently, prior methods either offer limited generative\ncapability or depend on a vast array of anomaly-specific models. We demonstrate\nthat U-Net's differentiated learning ability captures the distinct visual\ntraits of slightly-varied normal products and diverse anomalies, enabling us to\nconstruct a unified model for all tasks. Specifically, we first introduce an\nUnbalanced Abnormal (UA) Text Prompt, comprising one normal token and multiple\nanomaly tokens. More importantly, our Decoupled Anomaly Alignment (DA) loss\ndecouples anomaly attributes and binds them to distinct anomaly tokens of UA,\nenabling SeaS to create unseen anomalies by recombining these attributes.\nFurthermore, our Normal-image Alignment (NA) loss aligns the normal token to\nnormal patterns, making generated normal products globally consistent and\nlocally varied. Finally, SeaS produces accurate anomaly masks by fusing\ndiscriminative U-Net features with high-resolution VAE features. SeaS sets a\nnew benchmark for industrial generation, significantly enhancing downstream\napplications, with average improvements of $+8.66\\%$ pixel-level AP for\nsynthesis-based AD approaches, $+1.10\\%$ image-level AP for unsupervised AD\nmethods, and $+12.79\\%$ IoU for supervised segmentation models. Code is\navailable at\n\\href{https://github.com/HUST-SLOW/SeaS}{https://github.com/HUST-SLOW/SeaS}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.14987v2", "cate": "cs.CV", "date": "2024-10-19", "updated": "2025-07-16"}
{"id": "2410.19704", "title": "Multi-view biomedical foundation models for molecule-target and property prediction", "authors": ["Parthasarathy Suryanarayanan", "Yunguang Qiu", "Shreyans Sethi", "Diwakar Mahajan", "Hongyang Li", "Yuxin Yang", "Elif Eyigoz", "Aldo Guzman Saenz", "Daniel E. Platt", "Timothy H. Rumbell", "Kenney Ng", "Sanjoy Dey", "Myson Burch", "Bum Chul Kwon", "Pablo Meyer", "Feixiong Cheng", "Jianying Hu", "Joseph A. Morrone"], "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "Comments:      40 pages including supplement. 10 figures, 8 tables", "url": "http://arxiv.org/abs/2410.19704v4", "summary": "Quality molecular representations are key to foundation model development in\nbio-medical research. Previous efforts have typically focused on a single\nrepresentation or molecular view, which may have strengths or weaknesses on a\ngiven task. We develop Multi-view Molecular Embedding with Late Fusion\n(MMELON), an approach that integrates graph, image and text views in a\nfoundation model setting and may be readily extended to additional\nrepresentations. Single-view foundation models are each pre-trained on a\ndataset of up to 200M molecules. The multi-view model performs robustly,\nmatching the performance of the highest-ranked single-view. It is validated on\nover 120 tasks, including molecular solubility, ADME properties, and activity\nagainst G Protein-Coupled receptors (GPCRs). We identify 33 GPCRs that are\nrelated to Alzheimer's disease and employ the multi-view model to select strong\nbinders from a compound screen. Predictions are validated through\nstructure-based modeling and identification of key binding motifs.", "comment": "40 pages including supplement. 10 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2410.19704v4", "cate": "q-bio.BM", "date": "2024-10-25", "updated": "2025-07-15"}
{"id": "2404.17789", "title": "BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part I: PDE-Constrained Optimization", "authors": ["Ray Zirui Zhang", "Christopher E. Miles", "Xiaohui Xie", "John S. Lowengrub"], "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "65M32", "I.2.6; G.1.8"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.17789v5", "summary": "We propose a new neural network based method for solving inverse problems for\npartial differential equations (PDEs) by formulating the PDE inverse problem as\na bilevel optimization problem. At the upper level, we minimize the data loss\nwith respect to the PDE parameters. At the lower level, we train a neural\nnetwork to locally approximate the PDE solution operator in the neighborhood of\na given set of PDE parameters, which enables an accurate approximation of the\ndescent direction for the upper level optimization problem. The lower level\nloss function includes the L2 norms of both the residual and its derivative\nwith respect to the PDE parameters. We apply gradient descent simultaneously on\nboth the upper and lower level optimization problems, leading to an effective\nand fast algorithm. The method, which we refer to as BiLO (Bilevel Local\nOperator learning), is also able to efficiently infer unknown functions in the\nPDEs through the introduction of an auxiliary variable. We provide a\ntheoretical analysis that justifies our approach. Through extensive experiments\nover multiple PDE systems, we demonstrate that our method enforces strong PDE\nconstraints, is robust to sparse and noisy data, and eliminates the need to\nbalance the residual and the data loss, which is inherent to the soft PDE\nconstraints in many existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.17789v5", "cate": "cs.LG", "date": "2024-04-27", "updated": "2025-07-16"}
{"id": "2411.00355", "title": "TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images", "authors": ["Mengcheng Li", "Fei Chao"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.00355v3", "summary": "In this paper, we propose TextDestroyer, the first training- and\nannotation-free method for scene text destruction using a pre-trained diffusion\nmodel. Existing scene text removal models require complex annotation and\nretraining, and may leave faint yet recognizable text information, compromising\nprivacy protection and content concealment. TextDestroyer addresses these\nissues by employing a three-stage hierarchical process to obtain accurate text\nmasks. Our method scrambles text areas in the latent start code using a\nGaussian distribution before reconstruction. During the diffusion denoising\nprocess, self-attention key and value are referenced from the original latent\nto restore the compromised background. Latent codes saved at each inversion\nstep are used for replacement during reconstruction, ensuring perfect\nbackground restoration. The advantages of TextDestroyer include: (1) it\neliminates labor-intensive data annotation and resource-intensive training; (2)\nit achieves more thorough text destruction, preventing recognizable traces; and\n(3) it demonstrates better generalization capabilities, performing well on both\nreal-world scenes and generated images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.00355v3", "cate": "cs.CV", "date": "2024-11-01", "updated": "2025-07-16"}
{"id": "2411.00265", "title": "Quantifying calibration error in modern neural networks through evidence based theory", "authors": ["Koffi Ismael Ouattara"], "categories": ["cs.LG", "cs.AI", "math.LO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at FUSION 2025 Conference", "url": "http://arxiv.org/abs/2411.00265v2", "summary": "Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.", "comment": "Accepted at FUSION 2025 Conference", "pdf_url": "http://arxiv.org/pdf/2411.00265v2", "cate": "cs.LG", "date": "2024-10-31", "updated": "2025-07-16"}
{"id": "2407.00765", "title": "Structured and Balanced Multi-Component and Multi-Layer Neural Networks", "authors": ["Shijun Zhang", "Hongkai Zhao", "Yimin Zhong", "Haomin Zhou"], "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Our codes and implementation details are available at this https URL", "url": "http://arxiv.org/abs/2407.00765v3", "summary": "In this work, we propose a balanced multi-component and multi-layer neural\nnetwork (MMNN) structure to accurately and efficiently approximate functions\nwith complex features, in terms of both degrees of freedom and computational\ncost. The main idea is inspired by a multi-component approach, in which each\ncomponent can be effectively approximated by a single-layer network, combined\nwith a multi-layer decomposition strategy to capture the complexity of the\ntarget function. Although MMNNs can be viewed as a simple modification of fully\nconnected neural networks (FCNNs) or multi-layer perceptrons (MLPs) by\nintroducing balanced multi-component structures, they achieve a significant\nreduction in training parameters, a much more efficient training process, and\nimproved accuracy compared to FCNNs or MLPs. Extensive numerical experiments\ndemonstrate the effectiveness of MMNNs in approximating highly oscillatory\nfunctions and their ability to automatically adapt to localized features.", "comment": "Our codes and implementation details are available at\n  https://github.com/ShijunZhangMath/MMNN", "pdf_url": "http://arxiv.org/pdf/2407.00765v3", "cate": "cs.LG", "date": "2024-06-30", "updated": "2025-07-16"}
{"id": "2411.10745", "title": "Bridging the Skeleton-Text Modality Gap: Diffusion-Powered Modality Alignment for Zero-shot Skeleton-based Action Recognition", "authors": ["Jeonghyeok Do", "Munchurl Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (camera-ready version). Please visit our project page at this https URL", "url": "http://arxiv.org/abs/2411.10745v4", "summary": "In zero-shot skeleton-based action recognition (ZSAR), aligning skeleton\nfeatures with the text features of action labels is essential for accurately\npredicting unseen actions. ZSAR faces a fundamental challenge in bridging the\nmodality gap between the two-kind features, which severely limits\ngeneralization to unseen actions. Previous methods focus on direct alignment\nbetween skeleton and text latent spaces, but the modality gaps between these\nspaces hinder robust generalization learning. Motivated by the success of\ndiffusion models in multi-modal alignment (e.g., text-to-image, text-to-video),\nwe firstly present a diffusion-based skeleton-text alignment framework for\nZSAR. Our approach, Triplet Diffusion for Skeleton-Text Matching (TDSM),\nfocuses on cross-alignment power of diffusion models rather than their\ngenerative capability. Specifically, TDSM aligns skeleton features with text\nprompts by incorporating text features into the reverse diffusion process,\nwhere skeleton features are denoised under text guidance, forming a unified\nskeleton-text latent space for robust matching. To enhance discriminative\npower, we introduce a triplet diffusion (TD) loss that encourages our TDSM to\ncorrect skeleton-text matches while pushing them apart for different action\nclasses. Our TDSM significantly outperforms very recent state-of-the-art\nmethods with significantly large margins of 2.36%-point to 13.05%-point,\ndemonstrating superior accuracy and scalability in zero-shot settings through\neffective skeleton-text matching.", "comment": "ICCV 2025 (camera-ready version). Please visit our project page at\n  https://kaist-viclab.github.io/TDSM_site/", "pdf_url": "http://arxiv.org/pdf/2411.10745v4", "cate": "cs.CV", "date": "2024-11-16", "updated": "2025-07-16"}
{"id": "2411.02572", "title": "ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy", "authors": ["Kian Kenyon-Dean", "Zitong Jerry Wang", "John Urbanik", "Konstantin Donhauser", "Jason Hartford", "Saber Saberian", "Nil Sahin", "Ihab Bendidi", "Safiye Celik", "Marta Fay", "Juan Sebastian Rodriguez Vera", "Imran S Haque", "Oren Kraus"], "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07", "I.2; I.4"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 main-track paper (42nd International Conference on Machine Learning). Formerly appeared as best paper runner-up at NeurIPS 2024 Foundation Models for Science Workshop (38th Conference on Neural Information Processing Systems). 18 pages, 7 figures", "url": "http://arxiv.org/abs/2411.02572v2", "summary": "Large-scale cell microscopy screens are used in drug discovery and molecular\nbiology research to study the effects of millions of chemical and genetic\nperturbations on cells. To use these images in downstream analysis, we need\nmodels that can map each image into a feature space that represents diverse\nbiological phenotypes consistently, in the sense that perturbations with\nsimilar biological effects have similar representations. In this work, we\npresent the largest foundation model for cell microscopy data to date, a new\n1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image\ncrops. Compared to a previous published ViT-L/8 MAE, our new model achieves a\n60% improvement in linear separability of genetic perturbations and obtains the\nbest overall performance on whole-genome biological relationship recall and\nreplicate consistency benchmarks. Beyond scaling, we developed two key methods\nthat improve performance: (1) training on a curated and diverse dataset; and,\n(2) using biologically motivated linear probing tasks to search across each\ntransformer block for the best candidate representation of whole-genome\nscreens. We find that many self-supervised vision transformers, pretrained on\neither natural or microscopy images, yield significantly more biologically\nmeaningful representations of microscopy images in their intermediate blocks\nthan in their typically used final blocks. More broadly, our approach and\nresults provide insights toward a general strategy for successfully building\nfoundation models for large-scale biological data.", "comment": "ICML 2025 main-track paper (42nd International Conference on Machine\n  Learning). Formerly appeared as best paper runner-up at NeurIPS 2024\n  Foundation Models for Science Workshop (38th Conference on Neural Information\n  Processing Systems). 18 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2411.02572v2", "cate": "cs.LG", "date": "2024-11-04", "updated": "2025-07-16"}
{"id": "2407.09357", "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees", "authors": ["Alexia Jolicoeur-Martineau", "Aristide Baratin", "Kisoo Kwon", "Boris Knyazev", "Yan Zhang"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code: this https URL", "url": "http://arxiv.org/abs/2407.09357v3", "summary": "Generating novel molecules is challenging, with most representations leading\nto generative models producing many invalid molecules. Spanning Tree-based\nGraph Generation (STGG) is a promising approach to ensure the generation of\nvalid molecules, outperforming state-of-the-art SMILES and graph diffusion\nmodels for unconditional generation. In the real world, we want to be able to\ngenerate molecules conditional on one or multiple desired properties rather\nthan unconditionally. Thus, in this work, we extend STGG to\nmulti-property-conditional generation. Our approach, STGG+, incorporates a\nmodern Transformer architecture, random masking of properties during training\n(enabling conditioning on any subset of properties and classifier-free\nguidance), an auxiliary property-prediction loss (allowing the model to\nself-criticize molecules and select the best ones), and other improvements. We\nshow that STGG+ achieves state-of-the-art performance on in-distribution and\nout-of-distribution conditional generation, and reward maximization.", "comment": "Code: https://github.com/SamsungSAILMontreal/AnyMolGenCritic", "pdf_url": "http://arxiv.org/pdf/2407.09357v3", "cate": "cs.LG", "date": "2024-07-12", "updated": "2025-07-15"}
{"id": "2411.17240", "title": "Boost 3D Reconstruction using Diffusion-based Monocular Camera Calibration", "authors": ["Junyuan Deng", "Wei Yin", "Xiaoyang Guo", "Qian Zhang", "Xiaotao Hu", "Weiqiang Ren", "Xiaoxiao-Long", "Ping Tan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.17240v2", "summary": "In this paper, we present DM-Calib, a diffusion-based approach for estimating\npinhole camera intrinsic parameters from a single input image. Monocular camera\ncalibration is essential for many 3D vision tasks. However, most existing\nmethods depend on handcrafted assumptions or are constrained by limited\ntraining data, resulting in poor generalization across diverse real-world\nimages. Recent advancements in stable diffusion models, trained on massive\ndata, have shown the ability to generate high-quality images with varied\ncharacteristics. Emerging evidence indicates that these models implicitly\ncapture the relationship between camera focal length and image content.\nBuilding on this insight, we explore how to leverage the powerful priors of\ndiffusion models for monocular pinhole camera calibration. Specifically, we\nintroduce a new image-based representation, termed Camera Image, which\nlosslessly encodes the numerical camera intrinsics and integrates seamlessly\nwith the diffusion framework. Using this representation, we reformulate the\nproblem of estimating camera intrinsics as the generation of a dense Camera\nImage conditioned on an input image. By fine-tuning a stable diffusion model to\ngenerate a Camera Image from a single RGB input, we can extract camera\nintrinsics via a RANSAC operation. We further demonstrate that our monocular\ncalibration method enhances performance across various 3D tasks, including\nzero-shot metric depth estimation, 3D metrology, pose estimation and\nsparse-view reconstruction. Extensive experiments on multiple public datasets\nshow that our approach significantly outperforms baselines and provides broad\nbenefits to 3D vision tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.17240v2", "cate": "cs.CV", "date": "2024-11-26", "updated": "2025-07-16"}
{"id": "2501.13959", "title": "Learning an Effective Premise Retrieval Model for Efficient Mathematical Formalization", "authors": ["Yicheng Tao", "Haotian Liu", "Shanwen Wang", "Hongteng Xu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.13959v3", "summary": "Formalized mathematics has recently garnered significant attention for its\nability to assist mathematicians across various fields. Premise retrieval, as a\ncommon step in mathematical formalization, has been a challenge, particularly\nfor inexperienced users. Existing retrieval methods that facilitate natural\nlanguage queries require a certain level of mathematical expertise from users,\nwhile approaches based on formal languages (e.g., Lean) typically struggle with\nthe scarcity of training data, hindering the training of effective and\ngeneralizable retrieval models. In this work, we introduce a novel method that\nleverages data extracted from Mathlib to train a lightweight and effective\npremise retrieval model. In particular, the proposed model embeds queries\n(i.e., proof state provided by Lean) and premises in a latent space, featuring\na tokenizer specifically trained on formal corpora. The model is learned in a\ncontrastive learning framework, in which a fine-grained similarity calculation\nmethod and a re-ranking module are applied to enhance the retrieval\nperformance. Experimental results demonstrate that our model outperforms\nexisting baselines, achieving higher accuracy while maintaining a lower\ncomputational load. We have released an open-source search engine based on our\nretrieval model at https://premise-search.com/. The source code and the trained\nmodel can be found at https://github.com/ruc-ai4math/Premise-Retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.13959v3", "cate": "cs.CL", "date": "2025-01-21", "updated": "2025-07-16"}
{"id": "2409.00979", "title": "Regret Analysis for Randomized Gaussian Process Upper Confidence Bound", "authors": ["Shion Takeno", "Yu Inatsu", "Masayuki Karasuyama"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      37 pages, 4 figures. Accepted to Journal of Artificial Intelligence Research as an extended paper from arXiv:2302.01511", "url": "http://arxiv.org/abs/2409.00979v3", "summary": "Gaussian process upper confidence bound (GP-UCB) is a theoretically\nestablished algorithm for Bayesian optimization (BO), where we assume the\nobjective function $f$ follows a GP. One notable drawback of GP-UCB is that the\ntheoretical confidence parameter $\\beta$ increases along with the iterations\nand is too large. To alleviate this drawback, this paper analyzes the\nrandomized variant of GP-UCB called improved randomized GP-UCB (IRGP-UCB),\nwhich uses the confidence parameter generated from the shifted exponential\ndistribution. We analyze the expected regret and conditional expected regret,\nwhere the expectation and the probability are taken respectively with $f$ and\nnoise and with the randomness of the BO algorithm. In both regret analyses,\nIRGP-UCB achieves a sub-linear regret upper bound without increasing the\nconfidence parameter if the input domain is finite. Furthermore, we show that\nrandomization plays a key role in avoiding an increase in confidence parameter\nby showing that GP-UCB using a constant confidence parameter can incur linearly\ngrowing expected cumulative regret. Finally, we show numerical experiments\nusing synthetic and benchmark functions and real-world emulators.", "comment": "37 pages, 4 figures. Accepted to Journal of Artificial Intelligence\n  Research as an extended paper from arXiv:2302.01511", "pdf_url": "http://arxiv.org/pdf/2409.00979v3", "cate": "cs.LG", "date": "2024-09-02", "updated": "2025-07-16"}
{"id": "2501.03629", "title": "CFFormer: Cross CNN-Transformer Channel Attention and Spatial Feature Fusion for Improved Segmentation of Heterogeneous Medical Images", "authors": ["Jiaxuan Li", "Qing Xu", "Xiangjian He", "Ziyu Liu", "Daokun Zhang", "Ruili Wang", "Rong Qu", "Guoping Qiu"], "categories": ["cs.CV", "I.2; I.4; I.5"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.03629v2", "summary": "Medical image segmentation plays an important role in computer-aided\ndiagnosis. Existing methods mainly utilize spatial attention to highlight the\nregion of interest. However, due to limitations of medical imaging devices,\nmedical images exhibit significant heterogeneity, posing challenges for\nsegmentation. Ultrasound images, for instance, often suffer from speckle noise,\nlow resolution, and poor contrast between target tissues and background, which\nmay lead to inaccurate boundary delineation. To address these challenges caused\nby heterogeneous image quality, we propose a hybrid CNN-Transformer\nmodel,called CFFormer, which leverages effective channel feature extraction to\nenhance the model' s ability to accurately identify tissue regions by capturing\nrich contextual information. The proposed architecture contains two key\ncomponents: the Cross Feature Channel Attention (CFCA) module and the X-Spatial\nFeature Fusion (XFF) module. The model incorporates dual encoders, with the CNN\nencoder focusing on capturing local features and the Transformer encoder\nmodeling global features. The CFCA module filters and facilitates interactions\nbetween the channel features from the two encoders, while the XFF module\neffectively reduces the significant semantic information differences in spatial\nfeatures, enabling a smooth and cohesive spatial feature fusion. We evaluate\nour model across eight datasets covering five modalities to test its\ngeneralization capability. Experimental results demonstrate that our model\noutperforms current state-of-the-art methods and maintains accurate tissue\nregion segmentation across heterogeneous medical image datasets. The code is\navailable at https://github.com/JiaxuanFelix/CFFormer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.03629v2", "cate": "cs.CV", "date": "2025-01-07", "updated": "2025-07-16"}
{"id": "2501.16605", "title": "The Impact of Modern AI in Metadata Management", "authors": ["Wenli Yang", "Rui Fu", "Muhammad Bilal Amin", "Byeong Kang"], "categories": ["cs.DB", "cs.AI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.16605v2", "summary": "Metadata management plays a critical role in data governance, resource\ndiscovery, and decision-making in the data-driven era. While traditional\nmetadata approaches have primarily focused on organization, classification, and\nresource reuse, the integration of modern artificial intelligence (AI)\ntechnologies has significantly transformed these processes. This paper\ninvestigates both traditional and AI-driven metadata approaches by examining\nopen-source solutions, commercial tools, and research initiatives. A\ncomparative analysis of traditional and AI-driven metadata management methods\nis provided, highlighting existing challenges and their impact on\nnext-generation datasets. The paper also presents an innovative AI-assisted\nmetadata management framework designed to address these challenges. This\nframework leverages more advanced modern AI technologies to automate metadata\ngeneration, enhance governance, and improve the accessibility and usability of\nmodern datasets. Finally, the paper outlines future directions for research and\ndevelopment, proposing opportunities to further advance metadata management in\nthe context of AI-driven innovation and complex datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.16605v2", "cate": "cs.DB", "date": "2025-01-28", "updated": "2025-07-16"}
{"id": "2410.08355", "title": "Metalic: Meta-Learning In-Context with Protein Language Models", "authors": ["Jacob Beck", "Shikha Surana", "Manus McAuliffe", "Oliver Bent", "Thomas D. Barrett", "Juan Jose Garau Luis", "Paul Duckworth"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at The Thirteenth International Conference on Learning Representations (ICLR 2025). Code is provided at this https URL . Also relevant to searches for \"metallic\", \"meta-learning in-context\", \"LLM\", and \"protein language model\"", "url": "http://arxiv.org/abs/2410.08355v3", "summary": "Predicting the biophysical and functional properties of proteins is essential\nfor in silico protein design. Machine learning has emerged as a promising\ntechnique for such prediction tasks. However, the relative scarcity of in vitro\nannotations means that these models often have little, or no, specific data on\nthe desired fitness prediction task. As a result of limited data, protein\nlanguage models (PLMs) are typically trained on general protein sequence\nmodeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness\nprediction. When no task data is available, the models make strong assumptions\nabout the correlation between the protein sequence likelihood and fitness\nscores. In contrast, we propose meta-learning over a distribution of standard\nfitness prediction tasks, and demonstrate positive transfer to unseen fitness\nprediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses\nin-context learning and fine-tuning, when data is available, to adapt to new\ntasks. Crucially, fine-tuning enables considerable generalization, even though\nit is not accounted for during meta-training. Our fine-tuned models achieve\nstrong results with 18 times fewer parameters than state-of-the-art models.\nMoreover, our method sets a new state-of-the-art in low-data settings on\nProteinGym, an established fitness-prediction benchmark. Due to data scarcity,\nwe believe meta-learning will play a pivotal role in advancing protein\nengineering.", "comment": "Published at The Thirteenth International Conference on Learning\n  Representations (ICLR 2025). Code is provided at\n  https://github.com/instadeepai/metalic. Also relevant to searches for\n  \"metallic\", \"meta-learning in-context\", \"LLM\", and \"protein language model\"", "pdf_url": "http://arxiv.org/pdf/2410.08355v3", "cate": "cs.LG", "date": "2024-10-10", "updated": "2025-07-15"}
{"id": "2501.15151", "title": "SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neuron Networks", "authors": ["Yimeng Fan", "Changsong Liu", "Mingyang Li", "Dongze Liu", "Yanyan Liu", "Wei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.15151v3", "summary": "Spiking Neural Networks (SNNs) are the third generation of neural networks.\nThey have gained widespread attention in object detection due to their low\npower consumption and biological interpretability. However, existing SNN-based\nobject detection methods suffer from local firing saturation, where neurons in\ninformation-concentrated regions fire continuously throughout all time steps.\nThis abnormal neuron firing pattern reduces the feature discrimination\ncapability and detection accuracy, while also increasing the firing rates that\nprevent SNNs from achieving their potential energy efficiency. To address this\nproblem, we propose SpikeDet, a novel spiking object detector that optimizes\nfiring patterns for accurate and energy-efficient detection. Specifically, we\ndesign a spiking backbone network, MDSNet, which effectively adjusts the\nmembrane synaptic input distribution at each layer, achieving better neuron\nfiring patterns during spiking feature extraction. Additionally, to better\nutilize and preserve these high-quality backbone features, we introduce the\nSpiking Multi-direction Fusion Module (SMFM), which realizes multi-direction\nfusion of spiking features, enhancing the multi-scale detection capability of\nthe model. Experimental results demonstrate that SpikeDet achieves superior\nperformance. On the COCO 2017 dataset, it achieves 51.4% AP, outperforming\nprevious SNN-based methods by 2.5% AP while requiring only half the power\nconsumption. On object detection sub-tasks, including the GEN1 event-based\ndataset and the URPC 2019 underwater dataset, SpikeDet also achieves the best\nperformance. Notably, on GEN1, our method achieves 47.6% AP, outperforming\nprevious SNN-based methods by 7.2% AP with better energy efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.15151v3", "cate": "cs.CV", "date": "2025-01-25", "updated": "2025-07-16"}
{"id": "2502.01912", "title": "PATCH: a deep learning method to assess heterogeneity of artistic practice in historical paintings", "authors": ["Andrew Van Horn", "Lauryn Smith", "Mahamad Mahmoud", "Michael McMaster", "Clara Pinchbeck", "Ina Martin", "Andrew Lininger", "Anthony Ingrisano", "Adam Lowe", "Carlos Bayod", "Elizabeth Bolman", "Kenneth Singer", "Michael Hinczewski"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      main text: 15 pages, 5 figures; SI: 10 pages, 4 figures; v2: minor typo corrections, higher resolution figures; v3: additional comparisons with alternative methods", "url": "http://arxiv.org/abs/2502.01912v3", "summary": "The history of art has seen significant shifts in the manner in which\nartworks are created, making understanding of creative processes a central\nquestion in technical art history. In the Renaissance and Early Modern period,\npaintings were largely produced by master painters directing workshops of\napprentices who often contributed to projects. The masters varied significantly\nin artistic and managerial styles, meaning different combinations of artists\nand implements might be seen both between masters and within workshops or even\nindividual canvases. Information on how different workshops were managed and\nthe processes by which artworks were created remains elusive. Machine learning\nmethods have potential to unearth new information about artists' creative\nprocesses by extending the analysis of brushwork to a microscopic scale.\nAnalysis of workshop paintings, however, presents a challenge in that\ndocumentation of the artists and materials involved is sparse, meaning external\nexamples are not available to train networks to recognize their contributions.\nHere we present a novel machine learning approach we call pairwise assignment\ntraining for classifying heterogeneity (PATCH) that is capable of identifying\nindividual artistic practice regimes with no external training data, or \"ground\ntruth.\" The method achieves unsupervised results by supervised means, and\noutperforms both simple statistical procedures and unsupervised machine\nlearning methods. We apply this method to two historical paintings by the\nSpanish Renaissance master, El Greco: The Baptism of Christ and Christ on the\nCross with Landscape, and our findings regarding the former potentially\nchallenge previous work that has assigned the painting to workshop members.\nFurther, the results of our analyses create a measure of heterogeneity of\nartistic practice that can be used to characterize artworks across time and\nspace.", "comment": "main text: 15 pages, 5 figures; SI: 10 pages, 4 figures; v2: minor\n  typo corrections, higher resolution figures; v3: additional comparisons with\n  alternative methods", "pdf_url": "http://arxiv.org/pdf/2502.01912v3", "cate": "cs.CV", "date": "2025-02-04", "updated": "2025-07-16"}
{"id": "2411.02813", "title": "Sparse Orthogonal Parameters Tuning for Continual Learning", "authors": ["Hai-Jian Ke", "Kun-Peng Ning", "Yu-Yang Liu", "Jia-Yu Yao", "Yong-Hong Tian", "Li Yuan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.02813v2", "summary": "Continual learning methods based on pre-trained models (PTM) have recently\ngained attention which adapt to successive downstream tasks without\ncatastrophic forgetting. These methods typically refrain from updating the\npre-trained parameters and instead employ additional adapters, prompts, and\nclassifiers. In this paper, we from a novel perspective investigate the benefit\nof sparse orthogonal parameters for continual learning. We found that merging\nsparse orthogonality of models learned from multiple streaming tasks has great\npotential in addressing catastrophic forgetting. Leveraging this insight, we\npropose a novel yet effective method called SoTU (Sparse Orthogonal Parameters\nTUning). We hypothesize that the effectiveness of SoTU lies in the\ntransformation of knowledge learned from multiple domains into the fusion of\northogonal delta parameters. Experimental evaluations on diverse CL benchmarks\ndemonstrate the effectiveness of the proposed approach. Notably, SoTU achieves\noptimal feature representation for streaming data without necessitating complex\nclassifier designs, making it a Plug-and-Play solution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.02813v2", "cate": "cs.LG", "date": "2024-11-05", "updated": "2025-07-16"}
{"id": "2501.19034", "title": "XRF V2: A Dataset for Action Summarization with Wi-Fi Signals, and IMUs in Phones, Watches, Earbuds, and Glasses", "authors": ["Bo Lan", "Pei Li", "Jiaxi Yin", "Yunpeng Song", "Ge Wang", "Han Ding", "Jinsong Han", "Fei Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ACM IMWUT/UBICOMP 2025", "url": "http://arxiv.org/abs/2501.19034v2", "summary": "Human Action Recognition (HAR) plays a crucial role in applications such as\nhealth monitoring, smart home automation, and human-computer interaction. While\nHAR has been extensively studied, action summarization using Wi-Fi and IMU\nsignals in smart-home environments , which involves identifying and summarizing\ncontinuous actions, remains an emerging task. This paper introduces the novel\nXRF V2 dataset, designed for indoor daily activity Temporal Action Localization\n(TAL) and action summarization. XRF V2 integrates multimodal data from Wi-Fi\nsignals, IMU sensors (smartphones, smartwatches, headphones, and smart\nglasses), and synchronized video recordings, offering a diverse collection of\nindoor activities from 16 volunteers across three distinct environments. To\ntackle TAL and action summarization, we propose the XRFMamba neural network,\nwhich excels at capturing long-term dependencies in untrimmed sensory sequences\nand achieves the best performance with an average mAP of 78.74, outperforming\nthe recent WiFiTAD by 5.49 points in mAP@avg while using 35% fewer parameters.\nIn action summarization, we introduce a new metric, Response Meaning\nConsistency (RMC), to evaluate action summarization performance. And it\nachieves an average Response Meaning Consistency (mRMC) of 0.802. We envision\nXRF V2 as a valuable resource for advancing research in human action\nlocalization, action forecasting, pose estimation, multimodal foundation models\npre-training, synthetic data generation, and more. The data and code are\navailable at https://github.com/aiotgroup/XRFV2.", "comment": "accepted by ACM IMWUT/UBICOMP 2025", "pdf_url": "http://arxiv.org/pdf/2501.19034v2", "cate": "cs.CV", "date": "2025-01-31", "updated": "2025-07-16"}
{"id": "2507.11582", "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "authors": ["Kazuyoshi Otsuka"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      38 pages. Manuscript submitted for review to the Journal of Computational Literary Studies (JCLS)", "url": "http://arxiv.org/abs/2507.11582v1", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "pdf_url": "http://arxiv.org/pdf/2507.11582v1", "cate": "cs.CL", "date": "2025-06-07", "updated": "2025-06-07"}
{"id": "2502.05111", "title": "Flexible and Efficient Grammar-Constrained Decoding", "authors": ["Kanghee Park", "Timothy Zhou", "Loris D'Antoni"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05111v2", "summary": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05111v2", "cate": "cs.CL", "date": "2025-02-07", "updated": "2025-07-15"}
{"id": "2411.05813", "title": "AI for Explosive Ordnance Detection in Clearance Operations: The State of Research", "authors": ["Björn Kischelewski", "Gregory Cathcart", "David Wahl", "Benjamin Guedj"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The research paper was accepted for publication in The Journal of Conventional Weapons Destruction", "url": "http://arxiv.org/abs/2411.05813v2", "summary": "The detection and clearance of explosive ordnance (EO) continues to be a\npredominantly manual and high-risk process that can benefit from advances in\ntechnology to improve its efficiency and effectiveness. Research on artificial\nintelligence (AI) for EO detection in clearance operations has grown\nsignificantly in recent years. However, this research spans a wide range of\nfields, making it difficult to gain a comprehensive understanding of current\ntrends and developments. Therefore, this article provides a literature review\nof academic research on AI for EO detection in clearance operations. It finds\nthat research can be grouped into two main streams: AI for EO object detection\nand AI for EO risk prediction, with the latter being much less studied than the\nformer. From the literature review, we develop three opportunities for future\nresearch. These include a call for renewed efforts in the use of AI for EO risk\nprediction, the combination of different AI systems and data sources, and novel\napproaches to improve EO risk prediction performance, such as pattern-based\npredictions. Finally, we provide a perspective on the future of AI for EO\ndetection in clearance operations. We emphasize the role of traditional machine\nlearning (ML) for this task, the need to dynamically incorporate expert\nknowledge into the models, and the importance of effectively integrating AI\nsystems with real-world operations.", "comment": "The research paper was accepted for publication in The Journal of\n  Conventional Weapons Destruction", "pdf_url": "http://arxiv.org/pdf/2411.05813v2", "cate": "cs.LG", "date": "2024-10-31", "updated": "2025-07-15"}
{"id": "2502.05843", "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object Detectors via LLM-guided Symbolic Reasoning", "authors": ["Yuhui Zeng", "Haoxiang Wu", "Wenjie Nie", "Xiawu Zheng", "Guangyao Chen", "Yunhang Shen", "Jun Peng", "Yonghong Tian", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures", "url": "http://arxiv.org/abs/2502.05843v4", "summary": "Current object detectors excel at entity localization and classification, yet\nexhibit inherent limitations in event recognition capabilities. This deficiency\narises from their architecture's emphasis on discrete object identification\nrather than modeling the compositional reasoning, inter-object correlations,\nand contextual semantics essential for comprehensive event understanding. To\naddress this challenge, we present a novel framework that expands the\ncapability of standard object detectors beyond mere object recognition to\ncomplex event understanding through LLM-guided symbolic reasoning. Our key\ninnovation lies in bridging the semantic gap between object detection and event\nunderstanding without requiring expensive task-specific training. The proposed\nplug-and-play framework interfaces with any open-vocabulary detector while\nextending their inherent capabilities across architectures. At its core, our\napproach combines (i) a symbolic regression mechanism exploring relationship\npatterns among detected entities and (ii) a LLM-guided strategically guiding\nthe search toward meaningful expressions. These discovered symbolic rules\ntransform low-level visual perception into interpretable event understanding,\nproviding a transparent reasoning path from objects to events with strong\ntransferability across domains.We compared our training-free framework against\nspecialized event recognition systems across diverse application domains.\nExperiments demonstrate that our framework enhances multiple object detector\narchitectures to recognize complex events such as illegal fishing activities\n(75% AUROC, +8.36% improvement), construction safety violations (+15.77%), and\nabnormal crowd behaviors (+23.16%). Code is available at\n\\href{https://github.com/MAC-AutoML/SymbolicDet}{here}.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2502.05843v4", "cate": "cs.CV", "date": "2025-02-09", "updated": "2025-07-16"}
{"id": "2507.11764", "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "url": "http://arxiv.org/abs/2507.11764v1", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "pdf_url": "http://arxiv.org/pdf/2507.11764v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.11866", "title": "Similarity-Guided Diffusion for Contrastive Sequential Recommendation", "authors": ["Jinkyeong Choi", "Yejin Noh", "Donghyeon Park"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      14 pages, 5 figures", "url": "http://arxiv.org/abs/2507.11866v1", "summary": "In sequential recommendation systems, data augmentation and contrastive\nlearning techniques have recently been introduced using diffusion models to\nachieve robust representation learning. However, most of the existing\napproaches use random augmentation, which risk damaging the contextual\ninformation of the original sequence. Accordingly, we propose a\nSimilarity-Guided Diffusion for Contrastive Sequential Recommendation. Our\nmethod leverages the similarity between item embedding vectors to generate\nsemantically consistent noise. Moreover, we utilize high confidence score in\nthe denoising process to select our augmentation positions. This approach more\neffectively reflects contextual and structural information compared to\naugmentation at random positions. From a contrastive learning perspective, the\nproposed augmentation technique provides more discriminative positive and\nnegative samples, simultaneously improving training efficiency and\nrecommendation performance. Experimental results on five benchmark datasets\nshow that SimDiffRec outperforms the existing baseline models.", "comment": "14 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.11866v1", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2502.12272", "title": "Learning to Reason at the Frontier of Learnability", "authors": ["Thomas Foster", "Jakob Foerster"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.12272v4", "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.12272v4", "cate": "cs.LG", "date": "2025-02-17", "updated": "2025-07-16"}
{"id": "2411.09127", "title": "Complexity-Aware Training of Deep Neural Networks for Optimal Structure Discovery", "authors": ["Valentin Frank Ingmar Guenter", "Athanasios Sideris"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages, 6 figures, 6 tables. Restructured Sections 1 and 3, added simulation data in Section 5 and added appendices", "url": "http://arxiv.org/abs/2411.09127v2", "summary": "We propose a novel algorithm for combined unit and layer pruning of deep\nneural networks that functions during training and without requiring a\npre-trained network to apply. Our algorithm optimally trades-off learning\naccuracy and pruning levels while balancing layer vs. unit pruning and\ncomputational vs. parameter complexity using only three user-defined\nparameters, which are easy to interpret and tune. We formulate a stochastic\noptimization problem over the network weights and the parameters of variational\nBernoulli distributions for binary Random Variables taking values either 0 or 1\nand scaling the units and layers of the network. Optimal network structures are\nfound as the solution to this optimization problem. Pruning occurs when a\nvariational parameter converges to 0 rendering the corresponding structure\npermanently inactive, thus saving computations both during training and\nprediction. A key contribution of our approach is to define a cost function\nthat combines the objectives of prediction accuracy and network pruning in a\ncomputational/parameter complexity-aware manner and the automatic selection of\nthe many regularization parameters. We show that the proposed algorithm\nconverges to solutions of the optimization problem corresponding to\ndeterministic networks. We analyze the ODE system that underlies our stochastic\noptimization algorithm and establish domains of attraction for the dynamics of\nthe network parameters. These theoretical results lead to practical pruning\nconditions avoiding the premature pruning of units and layers during training.\nWe evaluate our method on the CIFAR-10/100 and ImageNet datasets using ResNet\narchitectures and demonstrate that it gives improved results with respect to\npruning ratios and test accuracy over layer-only or unit-only pruning and\nfavorably competes with combined unit and layer pruning algorithms requiring\npre-trained networks.", "comment": "31 pages, 6 figures, 6 tables. Restructured Sections 1 and 3, added\n  simulation data in Section 5 and added appendices", "pdf_url": "http://arxiv.org/pdf/2411.09127v2", "cate": "cs.LG", "date": "2024-11-14", "updated": "2025-07-16"}
{"id": "2502.12567", "title": "DeltaDiff: Reality-Driven Diffusion with AnchorResiduals for Faithful SR", "authors": ["Chao Yang", "Yong Fan", "Qichao Zhang", "Cheng Lu", "Zhijing Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.12567v2", "summary": "Recently, the transfer application of diffusion models in super-resolu-tion\ntasks has faced the problem ofdecreased fidelity. Due to the inherent\nrandomsampling characteristics ofdiffusion models, direct application in\nsuper-resolu-tion tasks can result in generated details deviating from the true\ndistribution ofhigh-resolution images. To address this, we propose DeltaDiff, a\nnovel frame.work that constrains the difusion process, its essence is to\nestablish a determin-istic mapping path between HR and LR, rather than the\nrandom noise disturbanceprocess oftraditional difusion models. Theoretical\nanalysis demonstrates a 25%reduction in diffusion entropy in the residual space\ncompared to pixel-space diffiusion, effectively suppressing irrelevant noise\ninterference. The experimentalresults show that our method surpasses\nstate-of-the-art models and generates re-sults with better fidelity. This work\nestablishes a new low-rank constrained par-adigm for applying diffusion models\nto image reconstruction tasks, balancingstochastic generation with structural\nfidelity. Our code and model are publiclyavailable at\nhttps://github.com/continueyang/DeltaDiff .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.12567v2", "cate": "cs.CV", "date": "2025-02-18", "updated": "2025-07-16"}
{"id": "2507.11832", "title": "ILID: Native Script Language Identification for Indian Languages", "authors": ["Yash Ingle", "Pruthwik Mishra"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "url": "http://arxiv.org/abs/2507.11832v1", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11832v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12311", "title": "An Ecosystem for Ontology Interoperability", "authors": ["Zhangcheng Qiang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      4 pages, 8 figures", "url": "http://arxiv.org/abs/2507.12311v1", "summary": "Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nin real-world applications. A case study in the building domain validates the\nusefulness of the proposed ecosystem.", "comment": "4 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.12311v1", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2502.13497", "title": "Towards Geo-Culturally Grounded LLM Generations", "authors": ["Piyawat Lertvittayakumjorn", "David Kinney", "Vinodkumar Prabhakaran", "Donald Martin Jr.", "Sunipa Dev"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 (main conference)", "url": "http://arxiv.org/abs/2502.13497v4", "summary": "Generative large language models (LLMs) have demonstrated gaps in diverse\ncultural awareness across the globe. We investigate the effect of retrieval\naugmented generation and search-grounding techniques on LLMs' ability to\ndisplay familiarity with various national cultures. Specifically, we compare\nthe performance of standard LLMs, LLMs augmented with retrievals from a bespoke\nknowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a\nweb search (i.e., search grounding) on multiple cultural awareness benchmarks.\nWe find that search grounding significantly improves the LLM performance on\nmultiple-choice benchmarks that test propositional knowledge (e.g., cultural\nnorms, artifacts, and institutions), while KB grounding's effectiveness is\nlimited by inadequate knowledge base coverage and a suboptimal retriever.\nHowever, search grounding also increases the risk of stereotypical judgments by\nlanguage models and fails to improve evaluators' judgments of cultural\nfamiliarity in a human evaluation with adequate statistical power. These\nresults highlight the distinction between propositional cultural knowledge and\nopen-ended cultural fluency when it comes to evaluating LLMs' cultural\nawareness.", "comment": "ACL 2025 (main conference)", "pdf_url": "http://arxiv.org/pdf/2502.13497v4", "cate": "cs.CL", "date": "2025-02-19", "updated": "2025-07-16"}
{"id": "2411.10438", "title": "MARS: Unleashing the Power of Variance Reduction for Training Large Models", "authors": ["Huizhuo Yuan", "Yifeng Liu", "Shuang Wu", "Xun Zhou", "Quanquan Gu"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      35 pages, 19 figures, 12 tables", "url": "http://arxiv.org/abs/2411.10438v3", "summary": "Training deep neural networks--and more recently, large models demands\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\nAdamW, and their variants have been central to this task. Despite the\ndevelopment of numerous variance reduction algorithms in the past decade aimed\nat accelerating stochastic optimization in both convex and nonconvex settings,\nvariance reduction has not found widespread success in training deep neural\nnetworks or large language models. Consequently, it has remained a less favored\napproach in modern AI. In this paper, to unleash the power of variance\nreduction for efficient training of large models, we propose a unified\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\npreconditioned gradient methods with variance reduction via a scaled stochastic\nrecursive momentum technique. Within our framework, we introduce three\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\nLion, and Shampoo, respectively. We also draw a connection between our\nalgorithms and existing optimizers. Experimental results on training GPT-2\nmodels indicate that MARS consistently outperforms AdamW by a large margin. The\nimplementation of MARS is available at https://github.com/AGI-Arena/MARS.", "comment": "35 pages, 19 figures, 12 tables", "pdf_url": "http://arxiv.org/pdf/2411.10438v3", "cate": "cs.LG", "date": "2024-11-15", "updated": "2025-07-16"}
{"id": "2502.15203", "title": "FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image Generation", "authors": ["Young Beom Woo", "Sun Eung Kim", "Seong-Whan Lee"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE SMC 2025", "url": "http://arxiv.org/abs/2502.15203v2", "summary": "Integrating multiple personalized concepts into a single image has recently\ngained attention in text-to-image (T2I) generation. However, existing methods\noften suffer from performance degradation in complex scenes due to distortions\nin non-personalized regions and the need for additional fine-tuning, limiting\ntheir practicality. To address this issue, we propose FlipConcept, a novel\napproach that seamlessly integrates multiple personalized concepts into a\nsingle image without requiring additional tuning. We introduce guided\nappearance attention to enhance the visual fidelity of personalized concepts.\nAdditionally, we introduce mask-guided noise mixing to protect non-personalized\nregions during concept integration. Lastly, we apply background dilution to\nminimize concept leakage, i.e., the undesired blending of personalized concepts\nwith other objects in the image. In our experiments, we demonstrate that the\nproposed method, despite not requiring tuning, outperforms existing models in\nboth single and multiple personalized concept inference. These results\ndemonstrate the effectiveness and practicality of our approach for scalable,\nhigh-quality multi-concept personalization.", "comment": "Accepted by IEEE SMC 2025", "pdf_url": "http://arxiv.org/pdf/2502.15203v2", "cate": "cs.CV", "date": "2025-02-21", "updated": "2025-07-16"}
{"id": "2507.11862", "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to CLNLP 2025", "url": "http://arxiv.org/abs/2507.11862v1", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "comment": "Accepted to CLNLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11862v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12378", "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker", "authors": ["Rachna Saxena", "Abhijeet Kumar", "Suresh Shanmugam"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Presented at NLP@IR workshop at SIGIR conference", "url": "http://arxiv.org/abs/2507.12378v1", "summary": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.", "comment": "Presented at NLP@IR workshop at SIGIR conference", "pdf_url": "http://arxiv.org/pdf/2507.12378v1", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2502.16994", "title": "FADE: Why Bad Descriptions Happen to Good Features", "authors": ["Bruno Puri", "Aakriti Jain", "Elena Golimblevskaia", "Patrick Kahardipraja", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16994v2", "summary": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16994v2", "cate": "cs.LG", "date": "2025-02-24", "updated": "2025-07-16"}
{"id": "2412.10543", "title": "METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation", "authors": ["Siddhant Ray", "Rui Pan", "Zhuohan Gu", "Kuntai Du", "Shaoting Feng", "Ganesh Ananthanarayanan", "Ravi Netravali", "Junchen Jiang"], "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      17 pages, 18 figures", "url": "http://arxiv.org/abs/2412.10543v2", "summary": "RAG (Retrieval Augmented Generation) allows LLMs (large language models) to\ngenerate better responses with external knowledge, but using more external\nknowledge often improves generation quality at the expense of response delay.\nPrior work either reduces the response delay (through better scheduling of RAG\nqueries) or strives to maximize quality (which involves tuning the RAG\nworkflow), but they fall short in optimizing the tradeoff between the delay and\nquality of RAG responses. This paper presents METIS, the first RAG system that\njointly schedules queries and adapts the key RAG configurations of each query,\nsuch as the number of retrieved text chunks and synthesis methods, in order to\nbalance quality optimization and response delay reduction. Using 4 popular\nRAG-QA datasets, we show that compared with the state-of-the-art RAG\noptimization schemes, METIS reduces the generation latency by $1.64-2.54\\times$\nwithout sacrificing generation quality.", "comment": "17 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2412.10543v2", "cate": "cs.LG", "date": "2024-12-13", "updated": "2025-07-16"}
{"id": "2502.17066", "title": "DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications", "authors": ["Ibrahim Fayad", "Max Zimmer", "Martin Schwartz", "Fabian Gieseke", "Philippe Ciais", "Gabriel Belouze", "Sarah Brood", "Aurelien De Truchis", "Alexandre d'Aspremont"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      26 pages, 8 figures", "url": "http://arxiv.org/abs/2502.17066v2", "summary": "Significant efforts have been directed towards adapting self-supervised\nmultimodal learning for Earth observation applications. However, most current\nmethods produce coarse patch-sized embeddings, limiting their effectiveness and\nintegration with other modalities like LiDAR. To close this gap, we present\nDUNIA, an approach to learn pixel-sized embeddings through cross-modal\nalignment between images and full-waveform LiDAR data. As the model is trained\nin a contrastive manner, the embeddings can be directly leveraged in the\ncontext of a variety of environmental monitoring tasks in a zero-shot setting.\nIn our experiments, we demonstrate the effectiveness of the embeddings for\nseven such tasks: canopy height mapping, fractional canopy cover, land cover\nmapping, tree species identification, plant area index, crop type\nclassification, and per-pixel waveform-based vertical structure mapping. The\nresults show that the embeddings, along with zero-shot classifiers, often\noutperform specialized supervised models, even in low-data regimes. In the\nfine-tuning setting, we show strong performances near or better than the\nstate-of-the-art on five out of six tasks.", "comment": "26 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2502.17066v2", "cate": "cs.CV", "date": "2025-02-24", "updated": "2025-07-16"}
{"id": "2507.11867", "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "authors": ["Xiangyu Yang", "Xinying Qiu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to CLNLP 2025", "url": "http://arxiv.org/abs/2507.11867v1", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "comment": "Accepted to CLNLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11867v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11907", "title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes", "authors": ["Zhaoheng Li", "Silu Huang", "Wei Ding", "Yongjoo Park", "Jianjun Chen"], "categories": ["cs.DB", "cs.IR"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11907v1", "summary": "Many real-world tasks such as recommending videos with the kids tag can be\nreduced to finding most similar vectors associated with hard predicates. This\ntask, filtered vector search, is challenging as prior state-of-the-art\ngraph-based (unfiltered) similarity search techniques quickly degenerate when\nhard constraints are considered. That is, effective graph-based filtered\nsimilarity search relies on sufficient connectivity for reaching the most\nsimilar items within just a few hops. To consider predicates, recent works\npropose modifying graph traversal to visit only the items that may satisfy\npredicates. However, they fail to offer the just-a-few-hops property for a wide\nrange of predicates: they must restrict predicates significantly or lose\nefficiency if only a small fraction of items satisfy predicates.\n  We propose an opposite approach: instead of constraining traversal, we build\nmany indexes each serving different predicate forms. For effective\nconstruction, we devise a three-dimensional analytical model capturing\nrelationships among index size, search time, and recall, with which we follow a\nworkload-aware approach to pack as many useful indexes as possible into a\ncollection. At query time, the analytical model is employed yet again to\ndiscern the one that offers the fastest search at a given recall. We show\nsuperior performance and support on datasets with varying selectivities and\nforms: our approach achieves up to 8.06x speedup while having as low as 1%\nbuild time versus other indexes, with less than 2.15x memory of a standard HNSW\ngraph and modest knowledge of past workloads.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11907v1", "cate": "cs.DB", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2503.11167", "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction", "authors": ["Haonan Wang", "Qixiang Zhang", "Lehan Wang", "Xuanqi Huang", "Xiaomeng Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025, camera ready version", "url": "http://arxiv.org/abs/2503.11167v3", "summary": "Decoding visual stimuli from neural activity is essential for understanding\nthe human brain. While fMRI methods have successfully reconstructed static\nimages, fMRI-to-video reconstruction faces challenges due to the need for\ncapturing spatiotemporal dynamics like motion and scene transitions. Recent\napproaches have improved semantic and perceptual alignment but struggle to\nintegrate coarse fMRI data with detailed visual features. Inspired by the\nhierarchical organization of the visual system, we propose NEURONS, a novel\nframework that decouples learning into four correlated sub-tasks: key object\nsegmentation, concept recognition, scene description, and blurry video\nreconstruction. This approach simulates the visual cortex's functional\nspecialization, allowing the model to capture diverse video content. In the\ninference stage, NEURONS generates robust conditioning signals for a\npre-trained text-to-video diffusion model to reconstruct the videos. Extensive\nexperiments demonstrate that NEURONS outperforms state-of-the-art baselines,\nachieving solid improvements in video consistency (26.6%) and semantic-level\naccuracy (19.1%). Notably, NEURONS shows a strong functional correlation with\nthe visual cortex, highlighting its potential for brain-computer interfaces and\nclinical applications. Code and model weights are available at:\nhttps://github.com/xmed-lab/NEURONS.", "comment": "Accepted by ICCV 2025, camera ready version", "pdf_url": "http://arxiv.org/pdf/2503.11167v3", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-07-16"}
{"id": "2501.13916", "title": "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy", "authors": ["Linh Tran", "Timothy Castiglia", "Stacy Patterson", "Ana Milanova"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.13916v3", "summary": "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.13916v3", "cate": "cs.LG", "date": "2025-01-23", "updated": "2025-07-16"}
{"id": "2503.08384", "title": "Prototype-Based Multiple Instance Learning for Gigapixel Whole Slide Image Classification", "authors": ["Susu Sun", "Dominique van Midden", "Geert Litjens", "Christian F. Baumgartner"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2503.08384v2", "summary": "Multiple Instance Learning (MIL) methods have succeeded remarkably in\nhistopathology whole slide image (WSI) analysis. However, most MIL models only\noffer attention-based explanations that do not faithfully capture the model's\ndecision mechanism and do not allow human-model interaction. To address these\nlimitations, we introduce ProtoMIL, an inherently interpretable MIL model for\nWSI analysis that offers user-friendly explanations and supports human\nintervention. Our approach employs a sparse autoencoder to discover\nhuman-interpretable concepts from the image feature space, which are then used\nto train ProtoMIL. The model represents predictions as linear combinations of\nconcepts, making the decision process transparent. Furthermore, ProtoMIL allows\nusers to perform model interventions by altering the input concepts.\nExperiments on two widely used pathology datasets demonstrate that ProtoMIL\nachieves a classification performance comparable to state-of-the-art MIL models\nwhile offering intuitively understandable explanations. Moreover, we\ndemonstrate that our method can eliminate reliance on diagnostically irrelevant\ninformation via human intervention, guiding the model toward being right for\nthe right reason. Code will be publicly available at\nhttps://github.com/ss-sun/ProtoMIL.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2503.08384v2", "cate": "cs.CV", "date": "2025-03-11", "updated": "2025-07-16"}
{"id": "2507.11875", "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to CCL 2025", "url": "http://arxiv.org/abs/2507.11875v1", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "comment": "Accepted to CCL 2025", "pdf_url": "http://arxiv.org/pdf/2507.11875v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.13789", "title": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display Advertisement Recommender System", "authors": ["Fengxin Li", "Yi Li", "Yue Liu", "Chao Zhou", "Yuan Wang", "Xiaoxiang Deng", "Wei Xue", "Dapeng Liu", "Lei Xiao", "Haijie Gu", "Jie Jiang", "Hongyan Liu", "Biao Qin", "Jun He"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by VLDB 2025 Industrial Track", "url": "http://arxiv.org/abs/2411.13789v3", "summary": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.", "comment": "Accepted by VLDB 2025 Industrial Track", "pdf_url": "http://arxiv.org/pdf/2411.13789v3", "cate": "cs.IR", "date": "2024-11-21", "updated": "2025-07-16"}
{"id": "2507.11788", "title": "Simulated Language Acquisition in a Biologically Realistic Model of the Brain", "authors": ["Daniel Mitropolsky", "Christos Papadimitriou"], "categories": ["cs.NE", "cs.CL"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.11788v1", "summary": "Despite tremendous progress in neuroscience, we do not have a compelling\nnarrative for the precise way whereby the spiking of neurons in our brain\nresults in high-level cognitive phenomena such as planning and language. We\nintroduce a simple mathematical formulation of six basic and broadly accepted\nprinciples of neuroscience: excitatory neurons, brain areas, random synapses,\nHebbian plasticity, local inhibition, and inter-area inhibition. We implement a\nsimulated neuromorphic system based on this formalism, which is capable of\nbasic language acquisition: Starting from a tabula rasa, the system learns, in\nany language, the semantics of words, their syntactic role (verb versus noun),\nand the word order of the language, including the ability to generate novel\nsentences, through the exposure to a modest number of grounded sentences in the\nsame language. We discuss several possible extensions and implications of this\nresult.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.11788v1", "cate": "cs.NE", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2503.15426", "title": "Visual Position Prompt for MLLM based Visual Grounding", "authors": ["Wei Tang", "Yanpeng Sun", "Qinying Gu", "Zechao Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.15426v4", "summary": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address these\nissues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms: the global VPP overlays a learnable, axis-like tensor\nonto the input image to provide structured spatial cues, while the local VPP\nincorporates position-aware queries to support fine-grained localization.To\neffectively train our model with spatial guidance, we further introduce\nVPP-SFT, a curated dataset of 0.6M high-quality visual grounding samples.\nDesigned in a compact format, it enables efficient training and is\nsignificantly smaller than datasets used by other MLLMs (e.g., ~21M samples in\nMiniGPT-v2), yet still provides a strong performance boost. The resulting\nmodel, VPP-LLaVA, not only achieves state-of-the-art results on standard visual\ngrounding benchmarks but also demonstrates strong zero-shot generalization to\nchallenging unseen datasets. The code and dataset are available at\nhttps://github.com/WayneTomas/VPP-LLaVA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.15426v4", "cate": "cs.CV", "date": "2025-03-19", "updated": "2025-07-16"}
{"id": "2501.17965", "title": "Variational Combinatorial Sequential Monte Carlo for Bayesian Phylogenetics in Hyperbolic Space", "authors": ["Alex Chen", "Philipe Chlenski", "Kenneth Munyuza", "Antonio Khalil Moretti", "Christian A. Naesseth", "Itsik Pe'er"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 10 figures", "url": "http://arxiv.org/abs/2501.17965v2", "summary": "Hyperbolic space naturally encodes hierarchical structures such as\nphylogenies (binary trees), where inward-bending geodesics reflect paths\nthrough least common ancestors, and the exponential growth of neighborhoods\nmirrors the super-exponential scaling of topologies. This scaling challenge\nlimits the efficiency of Euclidean-based approximate inference methods.\nMotivated by the geometric connections between trees and hyperbolic space, we\ndevelop novel hyperbolic extensions of two sequential search algorithms:\nCombinatorial and Nested Combinatorial Sequential Monte Carlo (\\textsc{Csmc}\nand \\textsc{Ncsmc}). Our approach introduces consistent and unbiased\nestimators, along with variational inference methods (\\textsc{H-Vcsmc} and\n\\textsc{H-Vncsmc}), which outperform their Euclidean counterparts. Empirical\nresults demonstrate improved speed, scalability and performance in\nhigh-dimensional phylogenetic inference tasks.", "comment": "24 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2501.17965v2", "cate": "cs.LG", "date": "2025-01-29", "updated": "2025-07-15"}
{"id": "2503.11579", "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers", "authors": ["Weiming Ren", "Wentao Ma", "Huan Yang", "Cong Wei", "Ge Zhang", "Wenhu Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Camera Ready Version. Project Page: this https URL", "url": "http://arxiv.org/abs/2503.11579v2", "summary": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640$\\times$360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks.", "comment": "ICCV 2025 Camera Ready Version. Project Page:\n  https://tiger-ai-lab.github.io/Vamba/", "pdf_url": "http://arxiv.org/pdf/2503.11579v2", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-07-16"}
{"id": "2507.11878", "title": "LLMs Encode Harmfulness and Refusal Separately", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11878v1", "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11878v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.03556", "title": "A Multistakeholder Approach to Value-Driven Co-Design of Recommender System Evaluation Metrics in Digital Archives", "authors": ["Florian Atzenhofer-Baumgartner", "Georg Vogeler", "Dominik Kowald"], "categories": ["cs.IR", "cs.DL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at RecSys 2025", "url": "http://arxiv.org/abs/2507.03556v2", "summary": "This paper presents the first multistakeholder approach for translating\ndiverse stakeholder values into an evaluation metric setup for Recommender\nSystems (RecSys) in digital archives. While commercial platforms mainly rely on\nengagement metrics, cultural heritage domains require frameworks that balance\ncompeting priorities among archivists, platform owners, researchers, and other\nstakeholders. To address this challenge, we conducted high-profile focus groups\n(5 groups x 5 persons) with upstream, provider, system, consumer, and\ndownstream stakeholders, identifying value priorities across critical\ndimensions: visibility/representation, expertise adaptation, and\ntransparency/trust. Our analysis shows that stakeholder concerns naturally\nalign with four sequential research funnel stages: discovery, interaction,\nintegration, and impact. The resulting evaluation setup addresses\ndomain-specific challenges including collection representation imbalances,\nnon-linear research patterns, and tensions between specialized expertise and\nbroader accessibility. We propose directions for tailored metrics in each stage\nof this research journey, such as research path quality for discovery,\ncontextual appropriateness for interaction, metadata-weighted relevance for\nintegration, and cross-stakeholder value alignment for impact assessment. Our\ncontributions extend beyond digital archives to the broader RecSys community,\noffering transferable evaluation approaches for domains where value emerges\nthrough sustained engagement rather than immediate consumption.", "comment": "Accepted at RecSys 2025", "pdf_url": "http://arxiv.org/pdf/2507.03556v2", "cate": "cs.IR", "date": "2025-07-04", "updated": "2025-07-16"}
{"id": "2507.12306", "title": "MaCE: General Mass Conserving Dynamics for Cellular Automata", "authors": ["Vassilis Papadopoulos", "Etienne Guichard"], "categories": ["nlin.CG", "cs.NE", "nlin.AO"], "primary_category": "Subjects:       Cellular Automata and Lattice Gases (nlin.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12306v1", "summary": "We present Mass-Conserving Evolution (MaCE), a general method for\nimplementing mass conservation in Cellular Automata (CA). MaCE is a simple\nevolution rule that can be easily 'attached' to existing CAs to make them\nmass-conserving, which tends to produce interesting behaviours more often, as\npatterns can no longer explode or die out. We first show that MaCE is\nnumerically stable and admits a simple continuous limit. We then test MaCE on\nLenia, and through several experiments, we demonstrate that it produces a wide\nvariety of interesting behaviours, starting from the variety and abundance of\nsolitons up to hints of intrinsic evolution in resource-constrained\nenvironments. Finally, we showcase the versatility of MaCE by applying it to\nNeural-CAs and discrete CAs, and discuss promising research directions opened\nup by this scheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12306v1", "cate": "nlin.CG", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2503.17070", "title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning", "authors": ["Daniel M. Jimenez-Gutierrez", "Mehrdad Hassanzadeh", "Aris Anagnostopoulos", "Ioannis Chatzigiannakis", "Andrea Vitaletti"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17070v2", "summary": "Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17070v2", "cate": "cs.LG", "date": "2025-03-21", "updated": "2025-07-16"}
{"id": "2502.05668", "title": "The late-stage training dynamics of (stochastic) subgradient descent on homogeneous neural networks", "authors": ["Sholom Schechtman", "Nicolas Schreuder"], "categories": ["cs.LG", "cs.NE", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted/presented at the 38th Annual Conference on Learning Theory (COLT 2025)", "url": "http://arxiv.org/abs/2502.05668v2", "summary": "We analyze the implicit bias of constant step stochastic subgradient descent\n(SGD). We consider the setting of binary classification with homogeneous neural\nnetworks - a large class of deep neural networks with ReLU-type activation\nfunctions such as MLPs and CNNs without biases. We interpret the dynamics of\nnormalized SGD iterates as an Euler-like discretization of a conservative field\nflow that is naturally associated to the normalized classification margin.\nOwing to this interpretation, we show that normalized SGD iterates converge to\nthe set of critical points of the normalized margin at late-stage training\n(i.e., assuming that the data is correctly classified with positive normalized\nmargin). Up to our knowledge, this is the first extension of the analysis of\nLyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth\nand stochastic setting. Our main result applies to binary classification with\nexponential or logistic losses. We additionally discuss extensions to more\ngeneral settings.", "comment": "Accepted/presented at the 38th Annual Conference on Learning Theory\n  (COLT 2025)", "pdf_url": "http://arxiv.org/pdf/2502.05668v2", "cate": "cs.LG", "date": "2025-02-08", "updated": "2025-07-16"}
{"id": "2503.12335", "title": "GS-I$^{3}$: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images", "authors": ["Tengfei Wang", "Xin Wang", "Yongmao Hou", "Zhaoning Zhang", "Yiwei Xu", "Zongqian Zhan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12335v3", "summary": "Accurate geometric surface reconstruction, providing essential environmental\ninformation for navigation and manipulation tasks, is critical for enabling\nrobotic self-exploration and interaction. Recently, 3D Gaussian Splatting\n(3DGS) has gained significant attention in the field of surface reconstruction\ndue to its impressive geometric quality and computational efficiency. While\nrecent relevant advancements in novel view synthesis under inconsistent\nillumination using 3DGS have shown promise, the challenge of robust surface\nreconstruction under such conditions is still being explored. To address this\nchallenge, we propose a method called GS-3I. Specifically, to mitigate 3D\nGaussian optimization bias caused by underexposed regions in single-view\nimages, based on Convolutional Neural Network (CNN), a tone mapping correction\nframework is introduced. Furthermore, inconsistent lighting across multi-view\nimages, resulting from variations in camera settings and complex scene\nillumination, often leads to geometric constraint mismatches and deviations in\nthe reconstructed surface. To overcome this, we propose a normal compensation\nmechanism that integrates reference normals extracted from single-view image\nwith normals computed from multi-view observations to effectively constrain\ngeometric inconsistencies. Extensive experimental evaluations demonstrate that\nGS-3I can achieve robust and accurate surface reconstruction across complex\nillumination scenarios, highlighting its effectiveness and versatility in this\ncritical challenge. https://github.com/TFwang-9527/GS-3I", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12335v3", "cate": "cs.CV", "date": "2025-03-16", "updated": "2025-07-16"}
{"id": "2507.11882", "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Main Conference paper", "url": "http://arxiv.org/abs/2507.11882v1", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "comment": "ACL 2025 Main Conference paper", "pdf_url": "http://arxiv.org/pdf/2507.11882v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2402.11662", "title": "TDE-3: An improved prior for optical flow computation in spiking neural networks", "authors": ["Matthew Yedutenko", "Federico Paredes-Valles", "Lyes Khacef", "Guido C. H. E. De Croon"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.11662v2", "summary": "Motion detection is a primary task required for robotic systems to perceive\nand navigate in their environment. Proposed in the literature bioinspired\nneuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and\nprocessors with spiking neural networks to provide real-time and\nenergy-efficient motion detection through extracting temporal correlations\nbetween two points in space. However, on the algorithmic level, this design\nleads to loss of direction-selectivity of individual TDEs in textured\nenvironments. Here we propose an augmented 3-point TDE (TDE-3) with additional\ninhibitory input that makes TDE-3 direction-selectivity robust in textured\nenvironments. We developed a procedure to train the new TDE-3 using\nbackpropagation through time and surrogate gradients to linearly map input\nvelocities into an output spike count or an Inter-Spike Interval (ISI). Our\nwork is the first instance of training a spiking neuron to have a specific ISI.\nUsing synthetic data we compared training and inference with spike count and\nISI with respect to changes in stimuli dynamic range, spatial frequency, and\nlevel of noise. ISI turns out to be more robust towards variation in spatial\nfrequency, whereas the spike count is a more reliable training signal in the\npresence of noise. We performed the first in-depth quantitative investigation\nof optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of\nenergy-efficiency and coding precision. Results show that on the network level\nboth detectors show similar precision (20 degree angular error, 88% correlation\nwith ground truth). Yet, due to the more robust direction-selectivity of\nindividual TDEs, TDE-3 based network spike less and hence is more\nenergy-efficient. Reported precision is on par with model-based methods but the\nspike-based processing of the TDEs provides allows more energy-efficient\ninference with neuromorphic hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.11662v2", "cate": "cs.NE", "date": "2024-02-18", "updated": "2025-07-16"}
{"id": "2503.22526", "title": "AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with Fine-Grained Categorization", "authors": ["Martin Kišš", "Michal Hradiš", "Martina Dvořáková", "Václav Jiroušek", "Filip Kersch"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 2 tables, 7 figures; Accepted to GREC Workshop at ICDAR2025", "url": "http://arxiv.org/abs/2503.22526v3", "summary": "We introduce the AnnoPage Dataset, a novel collection of 7,550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.", "comment": "17 pages, 2 tables, 7 figures; Accepted to GREC Workshop at ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2503.22526v3", "cate": "cs.CV", "date": "2025-03-28", "updated": "2025-07-16"}
{"id": "2502.09724", "title": "Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning", "authors": ["Cheol Woo Kim", "Jai Moondra", "Shresth Verma", "Madeleine Pollack", "Lingkai Kong", "Milind Tambe", "Swati Gupta"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.09724v2", "summary": "In many real-world applications of reinforcement learning (RL), deployed\npolicies have varied impacts on different stakeholders, creating challenges in\nreaching consensus on how to effectively aggregate their preferences.\nGeneralized $p$-means form a widely used class of social welfare functions for\nthis purpose, with broad applications in fair resource allocation, AI\nalignment, and decision-making. This class includes well-known welfare\nfunctions such as Egalitarian, Nash, and Utilitarian welfare. However,\nselecting the appropriate social welfare function is challenging for\ndecision-makers, as the structure and outcomes of optimal policies can be\nhighly sensitive to the choice of $p$. To address this challenge, we study the\nconcept of an $\\alpha$-approximate portfolio in RL, a set of policies that are\napproximately optimal across the family of generalized $p$-means for all $p \\in\n[-\\infty, 1]$. We propose algorithms to compute such portfolios and provide\ntheoretical guarantees on the trade-offs among approximation factor, portfolio\nsize, and computational efficiency. Experimental results on synthetic and\nreal-world datasets demonstrate the effectiveness of our approach in\nsummarizing the policy space induced by varying $p$ values, empowering\ndecision-makers to navigate this landscape more effectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.09724v2", "cate": "cs.LG", "date": "2025-02-13", "updated": "2025-07-16"}
{"id": "2503.12955", "title": "HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding", "authors": ["Jiahe Zhao", "Ruibing Hou", "Zejie Tian", "Hong Chang", "Shiguang Shan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2503.12955v2", "summary": "We propose a new task to benchmark human-in-scene understanding for embodied\nagents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within\na 3D scene, HIS-QA requires the agent to comprehend human states and behaviors,\nreason about its surrounding environment, and answer human-related questions\nwithin the scene. To support this new task, we present HIS-Bench, a multimodal\nbenchmark that systematically evaluates HIS understanding across a broad\nspectrum, from basic perception to commonsense reasoning and planning. Our\nevaluation of various vision-language models on HIS-Bench reveals significant\nlimitations in their ability to handle HIS-QA tasks. To this end, we propose\nHIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates\n3D scene context and human motion dynamics into large language models while\nincorporating specialized mechanisms to capture human-scene interactions.\nExtensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on\nHIS-QA tasks. We hope this work inspires future research on human behavior\nanalysis in 3D scenes, advancing embodied AI and world models. The codes and\ndata: https://github.com/ZJHTerry18/HumanInScene.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.12955v2", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-16"}
{"id": "2507.11942", "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025", "url": "http://arxiv.org/abs/2507.11942v1", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.11942v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2504.15110", "title": "Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives", "authors": ["Anastasis Kratsios", "Bum Jun Kim", "Takashi Furuya"], "categories": ["cs.LG", "cs.NA", "cs.NE", "math.FA", "math.NA", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.15110v2", "summary": "Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold\nNetworks (KANs) have recently emerged as an improved backbone for most deep\nlearning frameworks, promising more adaptivity than their multilayer perception\n(MLP) predecessor by allowing for trainable spline-based activation functions.\nIn this paper, we probe the theoretical foundations of the KAN architecture by\nshowing that it can optimally approximate any Besov function in\n$B^{s}_{p,q}(\\mathcal{X})$ on a bounded open, or even fractal, domain\n$\\mathcal{X}$ in $\\mathbb{R}^d$ at the optimal approximation rate with respect\nto any weaker Besov norm $B^{\\alpha}_{p,q}(\\mathcal{X})$; where $\\alpha < s$.\nWe complement our approximation guarantee with a dimension-free estimate on the\nsample complexity of a residual KAN model when learning a function of Besov\nregularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates\ncontemporary deep learning wisdom by leveraging residual/skip connections\nbetween layers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15110v2", "cate": "cs.LG", "date": "2025-04-21", "updated": "2025-07-15"}
{"id": "2504.00584", "title": "Semantic Adapter for Universal Text Embeddings: Diagnosing and Mitigating Negation Blindness to Enhance Universality", "authors": ["Hongliu Cao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted in ECAI 2025 main track", "url": "http://arxiv.org/abs/2504.00584v2", "summary": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings.", "comment": "Accepted in ECAI 2025 main track", "pdf_url": "http://arxiv.org/pdf/2504.00584v2", "cate": "cs.CL", "date": "2025-04-01", "updated": "2025-07-16"}
{"id": "2502.12937", "title": "Tuning Algorithmic and Architectural Hyperparameters in Graph-Based Semi-Supervised Learning with Provable Guarantees", "authors": ["Ally Yalei Du", "Eric Huang", "Dravyansh Sharma"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages (12 pages main body), 2 figures. UAI 2025", "url": "http://arxiv.org/abs/2502.12937v2", "summary": "Graph-based semi-supervised learning is a powerful paradigm in machine\nlearning for modeling and exploiting the underlying graph structure that\ncaptures the relationship between labeled and unlabeled data. A large number of\nclassical as well as modern deep learning based algorithms have been proposed\nfor this problem, often having tunable hyperparameters. We initiate a formal\nstudy of tuning algorithm hyperparameters from parameterized algorithm families\nfor this problem. We obtain novel $O(\\log n)$ pseudo-dimension upper bounds for\nhyperparameter selection in three classical label propagation-based algorithm\nfamilies, where $n$ is the number of nodes, implying bounds on the amount of\ndata needed for learning provably good parameters. We further provide matching\n$\\Omega(\\log n)$ pseudo-dimension lower bounds, thus asymptotically\ncharacterizing the learning-theoretic complexity of the parameter tuning\nproblem. We extend our study to selecting architectural hyperparameters in\nmodern graph neural networks. We bound the Rademacher complexity for tuning the\nself-loop weighting in recently proposed Simplified Graph Convolution (SGC)\nnetworks. We further propose a tunable architecture that interpolates graph\nconvolutional neural networks (GCN) and graph attention networks (GAT) in every\nlayer, and provide Rademacher complexity bounds for tuning the interpolation\ncoefficient.", "comment": "31 pages (12 pages main body), 2 figures. UAI 2025", "pdf_url": "http://arxiv.org/pdf/2502.12937v2", "cate": "cs.LG", "date": "2025-02-18", "updated": "2025-07-16"}
{"id": "2503.13026", "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model", "authors": ["Tao Wang", "Changxu Cheng", "Lingfeng Wang", "Senda Chen", "Wuyue Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025; the code is at this https URL", "url": "http://arxiv.org/abs/2503.13026v2", "summary": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.", "comment": "Accepted by ICCV 2025; the code is at\n  https://github.com/yayafengzi/LMM-HiMTok", "pdf_url": "http://arxiv.org/pdf/2503.13026v2", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-16"}
{"id": "2507.11972", "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "categories": ["cs.CL", "q-bio.NC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11972v1", "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11972v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10678", "title": "A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks", "authors": ["Cutter Dawes", "Simon Segert", "Kamesh Krishnamurthy", "Jonathan D. Cohen"], "categories": ["cs.LG", "cs.AI", "cs.NE", "q-bio.NC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 6 figures; typos corrected", "url": "http://arxiv.org/abs/2507.10678v2", "summary": "A major challenge in the use of neural networks both for modeling human\ncognitive function and for artificial intelligence is the design of systems\nwith the capacity to efficiently learn functions that support radical\ngeneralization. At the roots of this is the capacity to discover and implement\nsymmetry functions. In this paper, we investigate a paradigmatic example of\nradical generalization through the use of symmetry: base addition. We present a\ngroup theoretic analysis of base addition, a fundamental and defining\ncharacteristic of which is the carry function -- the transfer of the remainder,\nwhen a sum exceeds the base modulus, to the next significant place. Our\nanalysis exposes a range of alternative carry functions for a given base, and\nwe introduce quantitative measures to characterize these. We then exploit\ndifferences in carry functions to probe the inductive biases of neural networks\nin symmetry learning, by training neural networks to carry out base addition\nusing different carries, and comparing efficacy and rate of learning as a\nfunction of their structure. We find that even simple neural networks can\nachieve radical generalization with the right input format and carry function,\nand that learnability is closely correlated with carry function structure. We\nthen discuss the relevance this has for cognitive science and machine learning.", "comment": "22 pages, 6 figures; typos corrected", "pdf_url": "http://arxiv.org/pdf/2507.10678v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2504.19982", "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.19982v2", "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.19982v2", "cate": "cs.CL", "date": "2025-04-28", "updated": "2025-07-16"}
{"id": "2502.16075", "title": "Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks", "authors": ["Yuhang Cai", "Kangjie Zhou", "Jingfeng Wu", "Song Mei", "Michael Lindsey", "Peter L. Bartlett"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      79 pages, appeared in Proceedings of the 42nd International Conference on Machine Learning, Vancouver, Canada", "url": "http://arxiv.org/abs/2502.16075v2", "summary": "We establish the asymptotic implicit bias of gradient descent (GD) for\ngeneric non-homogeneous deep networks under exponential loss. Specifically, we\ncharacterize three key properties of GD iterates starting from a sufficiently\nsmall empirical risk, where the threshold is determined by a measure of the\nnetwork's non-homogeneity. First, we show that a normalized margin induced by\nthe GD iterates increases nearly monotonically. Second, we prove that while the\nnorm of the GD iterates diverges to infinity, the iterates themselves converge\nin direction. Finally, we establish that this directional limit satisfies the\nKarush-Kuhn-Tucker (KKT) conditions of a margin maximization problem. Prior\nworks on implicit bias have focused exclusively on homogeneous networks; in\ncontrast, our results apply to a broad class of non-homogeneous networks\nsatisfying a mild near-homogeneity condition. In particular, our results apply\nto networks with residual connections and non-homogeneous activation functions,\nthereby resolving an open problem posed by Ji and Telgarsky (2020).", "comment": "79 pages, appeared in Proceedings of the 42nd International\n  Conference on Machine Learning, Vancouver, Canada", "pdf_url": "http://arxiv.org/pdf/2502.16075v2", "cate": "cs.LG", "date": "2025-02-22", "updated": "2025-07-15"}
{"id": "2505.05470", "title": "Flow-GRPO: Training Flow Matching Models via Online RL", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Yangguang Li", "Jiaheng Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Wanli Ouyang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code: this https URL", "url": "http://arxiv.org/abs/2505.05470v4", "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation.", "comment": "Code: https://github.com/yifan123/flow_grpo", "pdf_url": "http://arxiv.org/pdf/2505.05470v4", "cate": "cs.CV", "date": "2025-05-08", "updated": "2025-07-16"}
{"id": "2507.11981", "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "authors": ["Lukas Ellinger", "Miriam Anschütz", "Georg Groh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by RANLP 2025", "url": "http://arxiv.org/abs/2507.11981v1", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "comment": "Accepted by RANLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11981v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12015", "title": "EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis", "authors": ["Haoxun Li", "Leyuan Qu", "Jiaxi Hu", "Taihao Li"], "categories": ["cs.SD"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by INTERSPEECH 2025", "url": "http://arxiv.org/abs/2507.12015v1", "summary": "In recent years, emotional Text-to-Speech (TTS) synthesis and\nemphasis-controllable speech synthesis have advanced significantly. However,\ntheir interaction remains underexplored. We propose Emphasis Meets Emotion TTS\n(EME-TTS), a novel framework designed to address two key research questions:\n(1) how to effectively utilize emphasis to enhance the expressiveness of\nemotional speech, and (2) how to maintain the perceptual clarity and stability\nof target emphasis across different emotions. EME-TTS employs weakly supervised\nlearning with emphasis pseudo-labels and variance-based emphasis features.\nAdditionally, the proposed Emphasis Perception Enhancement (EPE) block enhances\nthe interaction between emotional signals and emphasis positions. Experimental\nresults show that EME-TTS, when combined with large language models for\nemphasis position prediction, enables more natural emotional speech synthesis\nwhile preserving stable and distinguishable target emphasis across emotions.\nSynthesized samples are available on-line.", "comment": "Accepted by INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.12015v1", "cate": "cs.SD", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2505.23836", "title": "Large Language Models Often Know When They Are Being Evaluated", "authors": ["Joe Needham", "Giles Edkins", "Govind Pimpale", "Henning Bartsch", "Marius Hobbhahn"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.23836v3", "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.23836v3", "cate": "cs.CL", "date": "2025-05-28", "updated": "2025-07-16"}
{"id": "2504.07618", "title": "CTSR: Cartesian tensor-based sparse regression for data-driven discovery of high-dimensional invariant governing equations", "authors": ["Boqian Zhang", "Juanmian Lei", "Guoyou Sun", "Shuaibing Ding", "Jian Guo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.07618v2", "summary": "Accurate and concise governing equations are crucial for understanding system\ndynamics. Recently, data-driven methods such as sparse regression have been\nemployed to automatically uncover governing equations from data, representing a\nsignificant shift from traditional first-principles modeling. However, most\nexisting methods focus on scalar equations, limiting their applicability to\nsimple, low-dimensional scenarios, and failing to ensure rotation and\nreflection invariance without incurring significant computational cost or\nrequiring additional prior knowledge. This paper proposes a Cartesian\ntensor-based sparse regression (CTSR) technique to accurately and efficiently\nuncover complex, high-dimensional governing equations while ensuring\ninvariance. Evaluations on two two-dimensional (2D) and two three-dimensional\n(3D) test cases demonstrate that the proposed method achieves superior accuracy\nand efficiency compared to the conventional technique.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.07618v2", "cate": "cs.LG", "date": "2025-04-10", "updated": "2025-07-16"}
{"id": "2505.07530", "title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images", "authors": ["Raul Ismayilov", "Dzemila Sero", "Luuk Spreeuwers"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.07530v3", "summary": "Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nalong with a dataset of 14,889 synthetic identities. We generate synthetic\nfaces with user-defined identity attribute distributions, offering both\ndocument-style and trusted live capture images. The dataset generated using the\nFLUXSynID framework shows improved alignment with real-world identity\ndistributions and greater inter-class diversity compared to prior work. Our\nwork is publicly released to support biometric research, including face\nrecognition and morphing attack detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.07530v3", "cate": "cs.CV", "date": "2025-05-12", "updated": "2025-07-16"}
{"id": "2507.12004", "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "authors": ["Josip Jukić"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12004v1", "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12004v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.12299", "title": "Modal Analysis of Multimode Waveguides Based on Large Step Size AdaMax from Far-Field Amplitudes", "authors": ["Jingtong Li", "Dongting Huang", "Minhui Xiong", "Mingzhi Li"], "categories": ["physics.comp-ph", "cs.SD", "physics.optics"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12299v1", "summary": "Optimizing multimode waveguide performance depends on modal analysis;\nhowever, current approaches focus predominantly on modal power distribution\nand, limited by experimental hardware and conditions, exhibit low accuracy,\npoor adaptability, and high computational cost. In this work, under a\npower-normalization constraint, we employ the AdaMax optimizer with a\nlarge-step-size strategy to perform modal analysis of multimode waveguides from\nfar-field amplitude measurements. Our method retrieves both the modal power\ndistribution and the modal relative-phase distribution, and we elucidate how\ntwin-image ambiguity limits the capability to analyze modal relative-phase\ndistributions. Experimental results demonstrate that the proposed method\nperforms well for both rectangular and circular waveguides, maintaining high\naccuracy and robustness under noise with signal-to-noise ratios (SNRs) ranging\nfrom 20 to 120 dB, and achieving substantial improvements in accuracy and\ncomputational cost over comparable methods. This method provides a novel\nsolution for modal analysis with broad application potential.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12299v1", "cate": "physics.comp-ph", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.00713", "title": "AKReF: An argumentative knowledge representation framework for structured argumentation", "authors": ["Debarati Bhattacharjee", "Ashish Anand"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures, 2 tables", "url": "http://arxiv.org/abs/2506.00713v3", "summary": "This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). The proposed argumentative knowledge representation\nframework (AKReF) extends the theoretical foundation and enables the AKG to\nprovide a graphical view of the argumentative structure that is easier to\nunderstand. Starting with basic annotations of argumentative components (ACs)\nand argumentative relations (ARs), we enrich the information by constructing a\nknowledge base (KB) graph with metadata attributes for nodes. Next, we apply\nmodus ponens on premises and inference rules from the KB to form arguments.\nFrom these arguments, we create an AKG. The nodes and edges of the AKG have\nattributes capturing key argumentative features such as the type of premise\n(e.g., axiom, ordinary premise, assumption), the type of inference rule (e.g.,\nstrict, defeasible), preference order over defeasible rules, markers (e.g.,\n\"therefore\", \"however\"), and the type of attack (e.g., undercut, rebuttal,\nundermining). We identify inference rules by locating a specific set of\nmarkers, called inference markers (IM). This, in turn, makes it possible to\nidentify undercut attacks previously undetectable in existing datasets. AKG\nprepares the ground for reasoning tasks, including checking the coherence of\narguments and identifying opportunities for revision. For this, it is essential\nto find indirect relations, many of which are implicit. Our proposed AKG\nformat, with annotated inference rules and modus ponens, helps reasoning models\nlearn the implicit, indirect relations that require inference over arguments\nand their interconnections. We use an essay from the AAEC dataset to illustrate\nthe framework. We further show its application in complex analyses such as\nextracting a conflict-free set and a maximal set of admissible arguments.", "comment": "20 pages, 7 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.00713v3", "cate": "cs.CL", "date": "2025-05-31", "updated": "2025-07-15"}
{"id": "2505.14635", "title": "Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning", "authors": ["Benjamin Prada", "Shion Matsumoto", "Abdul Malik Zekri", "Ankur Mali"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 2 figures", "url": "http://arxiv.org/abs/2505.14635v2", "summary": "We present the first theoretical framework that connects predictive coding\n(PC), a biologically inspired local learning rule, with the minimum description\nlength (MDL) principle in deep networks. We prove that layerwise PC performs\nblock-coordinate descent on the MDL two-part code objective, thereby jointly\nminimizing empirical risk and model complexity. Using Hoeffding's inequality\nand a prefix-code prior, we derive a novel generalization bound of the form\n$R(\\theta) \\le \\hat{R}(\\theta) + \\frac{L(\\theta)}{N}$, capturing the tradeoff\nbetween fit and compression. We further prove that each PC sweep monotonically\ndecreases the empirical two-part codelength, yielding tighter high-probability\nrisk bounds than unconstrained gradient descent. Finally, we show that repeated\nPC updates converge to a block-coordinate stationary point, providing an\napproximate MDL-optimal solution. To our knowledge, this is the first result\noffering formal generalization and convergence guarantees for PC-trained deep\nmodels, positioning PC as a theoretically grounded and biologically plausible\nalternative to backpropagation.", "comment": "24 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2505.14635v2", "cate": "cs.LG", "date": "2025-05-20", "updated": "2025-07-16"}
{"id": "2505.11493", "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing", "authors": ["Yusu Qian", "Jiasen Lu", "Tsu-Jui Fu", "Xinze Wang", "Chen Chen", "Yinfei Yang", "Wenze Hu", "Zhe Gan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2505.11493v2", "summary": "Editing images using natural language instructions has become a natural and\nexpressive way to modify visual content; yet, evaluating the performance of\nsuch models remains challenging. Existing evaluation approaches often rely on\nimage-text similarity metrics like CLIP, which lack precision. In this work, we\nintroduce a new benchmark designed to evaluate text-guided image editing models\nin a more grounded manner, along two critical dimensions: (i) functional\ncorrectness, assessed via automatically generated multiple-choice questions\nthat verify whether the intended change was successfully applied; and (ii)\nimage content preservation, which ensures that non-targeted regions of the\nimage remain visually consistent using an object-aware masking technique and\npreservation scoring. The benchmark includes over 1000 high-quality editing\nexamples across 20 diverse content categories, each annotated with detailed\nediting instructions, evaluation questions, and spatial object masks. We\nconduct a large-scale study comparing GPT-Image-1, the latest flagship in the\ntext-guided image editing space, against several state-of-the-art editing\nmodels, and validate our automatic metrics against human ratings. Results show\nthat GPT-Image-1 leads in instruction-following accuracy, but often\nover-modifies irrelevant image regions, highlighting a key trade-off in the\ncurrent model behavior. GIE-Bench provides a scalable, reproducible framework\nfor advancing more accurate evaluation of text-guided image editing.", "comment": "Project page: https://sueqian6.github.io/GIE-Bench-web/", "pdf_url": "http://arxiv.org/pdf/2505.11493v2", "cate": "cs.CV", "date": "2025-05-16", "updated": "2025-07-16"}
{"id": "2507.12039", "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at KES 2025. To appear in Procedia Computer Science (Elsevier)", "url": "http://arxiv.org/abs/2507.12039v1", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "pdf_url": "http://arxiv.org/pdf/2507.12039v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11646", "title": "State-based approach to the numerical solution of Dirichlet boundary optimal control problems for the Laplace equation", "authors": ["Ulrich Langer", "Richard Löscher", "Olaf Steinbach", "Huidong Yang"], "categories": ["math.NA", "cs.NA", "math.OC", "49J20, 49K20, 65K10, 65N30, 65N22"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11646v1", "summary": "We investigate the Dirichlet boundary control of the Laplace equation,\nconsidering the control in $H^{1/2}(\\partial \\Omega)$, which is the natural\nspace for Dirichlet data when the state belongs to $H^1(\\Omega)$. The cost of\nthe control is measured in the $H^{1/2}(\\partial \\Omega)$ norm that also plays\nthe role of the regularization term. We discuss regularization and finite\nelement error estimates enabling us to derive an optimal relation between the\nfinite element mesh size $h$ and the regularization parameter $\\varrho$,\nbalancing the energy cost for the control and the accuracy of the approximation\nof the desired state. This relationship is also crucial in designing efficient\nsolvers. We also discuss additional box constraints imposed on the control and\nthe state. Our theoretical findings are complemented by numerical examples,\nincluding one example with box constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11646v1", "cate": "math.NA", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2506.03194", "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs", "authors": ["Rynaa Grover", "Jayant Sravan Tamarapalli", "Sahiti Yerramilli", "Nilay Pande"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03194v2", "summary": "Multimodal Large Language Models (MLLMs) excel at high-level visual\nreasoning, but their performance on nuanced perceptual tasks remains\nsurprisingly limited. We present HueManity, a benchmark designed to assess\nvisual perception in MLLMs. The dataset comprises 83,850 images featuring\ntwo-character alphanumeric strings embedded in Ishihara test style dot\npatterns, challenging models on precise pattern recognition. Our evaluation of\nnine state-of-the-art MLLMs on HueManity demonstrates a significant performance\ndeficit compared to human and traditional computer vision baselines. The\nbest-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a\nstriking 3% on the alphanumeric `hard' task. In contrast, human participants\nachieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model\nreached accuracies of 96.5% and 94.5%. These results highlight a critical gap\nin the visual capabilities of current MLLMs. Our analysis further explores\npotential architectural and training-paradigm factors contributing to this\nperceptual gap in MLLMs. We open-source HueManity dataset and code to foster\nfurther research in improving perceptual robustness of MLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03194v2", "cate": "cs.CV", "date": "2025-05-31", "updated": "2025-07-16"}
{"id": "2506.10972", "title": "Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large Language Models", "authors": ["Houyi Li", "Wenzhen Zheng", "Qiufeng Wang", "Zhenyu Ding", "Haoying Wang", "Zili Wang", "Shijie Xuyang", "Ning Ding", "Shuigeng Zhou", "Xiangyu Zhang", "Daxin Jiang"], "categories": ["cs.LG", "cs.AI", "I.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34", "url": "http://arxiv.org/abs/2506.10972v3", "summary": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research.", "comment": "34", "pdf_url": "http://arxiv.org/pdf/2506.10972v3", "cate": "cs.LG", "date": "2025-06-12", "updated": "2025-07-16"}
{"id": "2506.06852", "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation", "authors": ["John Waithaka", "Moise Busogi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.06852v2", "summary": "Semantic segmentation of satellite imagery is crucial for Earth observation\napplications, but remains constrained by limited labelled training data. While\nself-supervised pretraining methods like Masked Autoencoders (MAE) have shown\npromise, they focus on reconstruction rather than localisation-a fundamental\naspect of segmentation tasks. We propose adapting LOCA (Location-aware), a\nposition prediction self-supervised learning method, for multimodal satellite\nimagery semantic segmentation. Our approach addresses the unique challenges of\nsatellite data by extending SatMAE's channel grouping from multispectral to\nmultimodal data, enabling effective handling of multiple modalities, and\nintroducing same-group attention masking to encourage cross-modal interaction\nduring pretraining. The method uses relative patch position prediction,\nencouraging spatial reasoning for localisation rather than reconstruction. We\nevaluate our approach on the Sen1Floods11 flood mapping dataset, where it\nsignificantly outperforms existing reconstruction-based self-supervised\nlearning methods for satellite imagery. Our results demonstrate that position\nprediction tasks, when properly adapted for multimodal satellite imagery, learn\nrepresentations more effective for satellite image semantic segmentation than\nreconstruction-based approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.06852v2", "cate": "cs.CV", "date": "2025-06-07", "updated": "2025-07-16"}
{"id": "2507.12059", "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop on Qualitative Reasoning at IJCAI", "url": "http://arxiv.org/abs/2507.12059v1", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "pdf_url": "http://arxiv.org/pdf/2507.12059v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11695", "title": "Discontinuous Galerkin approximation for a Stokes-Brinkman-type formulation for the eigenvalue problem in porous media", "authors": ["Felipe Lepe", "Gonzalo Rivera", "Jesus Vellojin"], "categories": ["math.NA", "cs.NA", "35Q35, 65N15, 65N25, 65N30, 65N50"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11695v1", "summary": "We introduce a family of discontinuous Galerkin methods to approximate the\neigenvalues and eigenfunctions of a Stokes-Brinkman type of problem based in\nthe interior penalty strategy. Under the standard assumptions on the meshes and\na suitable norm, we prove the stability of the discrete scheme. Due to the\nnon-conforming nature of the method, we use the well-known non-compact\noperators theory to derive convergence and error estimates for the method. We\npresent an exhaustive computational analysis where we compute the spectrum with\ndifferent stabilization parameters with the aim of study its influence when the\nspectrum is approximated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11695v1", "cate": "math.NA", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.06273", "title": "Magneto-radiative modelling and artificial neural network optimization of biofluid flow in a stenosed arterial domain", "authors": ["S P Shivakumar", "Gunisetty Ramasekhar", "P Nimmy", "Sujesh Areekara", "L Thanuja", "T V Smitha", "S Devanathan", "Ganesh R Naik", "K V Nagaraja"], "categories": ["physics.med-ph", "cs.AI", "cs.NA", "math.NA", "physics.bio-ph"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06273v2", "summary": "The increasing complexity of cardiovascular diseases and limitations in\ntraditional healing methods mandate the invention of new drug delivery systems\nthat assure targeted, effective, and regulated treatments, contributing\ndirectly to UN SDGs 3 and 9, thereby encouraging the utilization of sustainable\nmedical technologies in healthcare. This study investigates the flow of a\nCasson-Maxwell nanofluid through a stenosed arterial domain. The quantities,\nsuch as skin friction and heat transfer rate, are analysed in detail. The\nCasson-Maxwell fluid shows a lower velocity profile than the Casson fluids,\nwhich indicates the improved residence time for efficient drug delivery. The\nheat transfer rate shows an increase with higher volume fractions of copper and\naluminium oxide nanoparticles and a decrease with higher volume fractions of\nsilver nanoparticles. The skin friction coefficient decreases by 219% with a\nunit increase in the Maxwell parameter, whereas it increases by 66.1% with a\nunit rise in the Casson parameter. This work supports SDGs 4 and 17 by\nfostering interdisciplinary learning and collaboration in fluid dynamics and\nhealthcare innovation. Additionally, the rate of heat flow was forecasted (with\nan overall R-value of 0.99457) using the Levenberg-Marquardt backpropagation\ntraining scheme under the influence of magneto-radiative, linear heat source\nand Casson-Maxwell parameters along with the tri-metallic nanoparticle volume\nfractions. It is also observed that the drag coefficient is most sensitive to\nthe changes in the Maxwell parameter.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06273v2", "cate": "physics.med-ph", "date": "2025-07-08", "updated": "2025-07-16"}
{"id": "2507.03560", "title": "Simplifying Graph Kernels for Efficient", "authors": ["Lin Wang", "Shijie Wang", "Sirui Huang", "Qing Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03560v2", "summary": "While kernel methods and Graph Neural Networks offer complementary strengths,\nintegrating the two has posed challenges in efficiency and scalability. The\nGraph Neural Tangent Kernel provides a theoretical bridge by interpreting GNNs\nthrough the lens of neural tangent kernels. However, its reliance on deep,\nstacked layers introduces repeated computations that hinder performance. In\nthis work, we introduce a new perspective by designing the simplified graph\nkernel, which replaces deep layer stacking with a streamlined $K$-step message\naggregation process. This formulation avoids iterative layer-wise propagation\naltogether, leading to a more concise and computationally efficient framework\nwithout sacrificing the expressive power needed for graph tasks. Beyond this\nsimplification, we propose another Simplified Graph Kernel, which draws from\nGaussian Process theory to model infinite-width GNNs. Rather than simulating\nnetwork depth, this kernel analytically computes kernel values based on the\nstatistical behavior of nonlinear activations in the infinite limit. This\neliminates the need for explicit architecture simulation, further reducing\ncomplexity. Our experiments on standard graph and node classification\nbenchmarks show that our methods achieve competitive accuracy while reducing\nruntime. This makes them practical alternatives for learning on graphs at\nscale. Full implementation and reproducibility materials are provided at:\nhttps://anonymous.4open.science/r/SGNK-1CE4/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03560v2", "cate": "cs.LG", "date": "2025-07-04", "updated": "2025-07-16"}
{"id": "2506.21316", "title": "DRISHTIKON: Visual Grounding at Multiple Granularities in Documents", "authors": ["Badri Vishal Kasuba", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Work in Progress", "url": "http://arxiv.org/abs/2506.21316v2", "summary": "Visual grounding in text-rich document images is a critical yet underexplored\nchallenge for Document Intelligence and Visual Question Answering (VQA)\nsystems. We present DRISHTIKON, a multi-granular and multi-block visual\ngrounding framework designed to enhance interpretability and trust in VQA for\ncomplex, multilingual documents. Our approach integrates multilingual OCR,\nlarge language models, and a novel region matching algorithm to localize answer\nspans at the block, line, word, and point levels. We introduce the\nMulti-Granular Visual Grounding (MGVG) benchmark, a curated test set of diverse\ncircular notifications from various sectors, each manually annotated with\nfine-grained, human-verified labels across multiple granularities. Extensive\nexperiments show that our method achieves state-of-the-art grounding accuracy,\nwith line-level granularity providing the best balance between precision and\nrecall. Ablation studies further highlight the benefits of multi-block and\nmulti-line reasoning. Comparative evaluations reveal that leading\nvision-language models struggle with precise localization, underscoring the\neffectiveness of our structured, alignment-based approach. Our findings pave\nthe way for more robust and interpretable document understanding systems in\nreal-world, text-centric scenarios with multi-granular grounding support. Code\nand dataset are made available for future research.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2506.21316v2", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-16"}
{"id": "2507.12079", "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This paper was accepted for the special issue AI for Education by the IEEE Signal Processing Magazine journal", "url": "http://arxiv.org/abs/2507.12079v1", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "pdf_url": "http://arxiv.org/pdf/2507.12079v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11700", "title": "Norm-Stabilized Imaginary-Time Evolution via Feedback Control", "authors": ["Stylianos Savva"], "categories": ["math.NA", "cs.NA", "nlin.PS", "physics.comp-ph", "65M06, 35Q55", "G.1.7; G.1.8"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures. Code and reproducibility: this https URL", "url": "http://arxiv.org/abs/2507.11700v1", "summary": "We present a norm-stabilized imaginary-time evolution (ITE) scheme for the\none-dimensional nonlinear Schrodinger equation (NLSE). Traditional ITE solvers\noften require explicit renormalization of the wavefunction after each step to\npreserve norm, which can be disruptive and algorithmically inflexible. We\npropose an alternative approach in which the evolution is continuously\nstabilized using an adaptive feedback term mu(tau), proportional to the time\nderivative of the wavefunction norm. This results in a self-regulating flow\nthat requires no external normalization while preserving convergence toward\nsoliton solutions. We demonstrate the method's effectiveness by comparing the\nfinal wavefunction profiles and L2 errors against analytical solutions and\nbaseline methods without feedback. Although this work focuses on the 1D case,\nthe framework is designed to extend naturally to higher dimensions. Future work\nwill explore the behavior of the feedback mechanism in 2D and 3D systems,\nmulti-soliton scenarios, and external potentials.", "comment": "9 pages, 5 figures. Code and reproducibility:\n  https://github.com/rrumabo/Stabilised-ITE-Solver", "pdf_url": "http://arxiv.org/pdf/2507.11700v1", "cate": "math.NA", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.09477", "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      submitted to ARR May", "url": "http://arxiv.org/abs/2507.09477v2", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "comment": "submitted to ARR May", "pdf_url": "http://arxiv.org/pdf/2507.09477v2", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-16"}
{"id": "2507.03833", "title": "MatRL: Provably Generalizable Iterative Algorithm Discovery via Monte-Carlo Tree Search", "authors": ["Sungyoon Kim", "Rajat Vadiraj Dwaraknath", "Longling geng", "Mert Pilanci"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03833v2", "summary": "Iterative methods for computing matrix functions have been extensively\nstudied and their convergence speed can be significantly improved with the\nright tuning of parameters and by mixing different iteration types. Handtuning\nthe design options for optimal performance can be cumbersome, especially in\nmodern computing environments: numerous different classical iterations and\ntheir variants exist, each with non-trivial per-step cost and tuning\nparameters. To this end, we propose MatRL -- a reinforcement learning based\nframework that automatically discovers iterative algorithms for computing\nmatrix functions. The key idea is to treat algorithm design as a sequential\ndecision-making process. Monte-Carlo tree search is then used to plan a hybrid\nsequence of matrix iterations and step sizes, tailored to a specific input\nmatrix distribution and computing environment. Moreover, we also show that the\nlearned algorithms provably generalize to sufficiently large matrices drawn\nfrom the same distribution. Finally, we corroborate our theoretical results\nwith numerical experiments demonstrating that MatRL produces algorithms that\noutperform various baselines in the literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03833v2", "cate": "cs.LG", "date": "2025-07-04", "updated": "2025-07-16"}
{"id": "2506.21444", "title": "Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation", "authors": ["Sweta Banerjee", "Viktoria Weiss", "Taryn A. Donovan", "Rutger H. J. Fick", "Thomas Conrad", "Jonas Ammeling", "Nils Porsche", "Robert Klopfleisch", "Christopher Kaltenecker", "Katharina Breininger", "Marc Aubreville", "Christof A. Bertram"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21444v2", "summary": "Atypical mitosis marks a deviation in the cell division process that has been\nshown be an independent prognostic marker for tumor malignancy. However,\natypical mitosis classification remains challenging due to low prevalence, at\ntimes subtle morphological differences from normal mitotic figures, low\ninter-rater agreement among pathologists, and class imbalance in datasets.\nBuilding on the Atypical Mitosis dataset for Breast Cancer (AMi-Br), this study\npresents a comprehensive benchmark comparing deep learning approaches for\nautomated atypical mitotic figure (AMF) classification, including end-to-end\ntrained deep learning models, foundation models with linear probing, and\nfoundation models fine-tuned with low-rank adaptation (LoRA). For rigorous\nevaluation, we further introduce two new held-out AMF datasets - AtNorM-Br, a\ndataset of mitotic figures from the TCGA breast cancer cohort, and AtNorM-MD, a\nmulti-domain dataset of mitotic figures from a subset of the MIDOG++ training\nset. We found average balanced accuracy values of up to 0.8135, 0.7788, and\n0.7723 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and AtNorM-MD\ndatasets, respectively. Our work shows that atypical mitotic figure\nclassification, while being a challenging problem, can be effectively addressed\nthrough the use of recent advances in transfer learning and model fine-tuning\ntechniques. We make all code and data used in this paper available in this\ngithub repository: https://github.com/DeepMicroscopy/AMi-Br_Benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21444v2", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-16"}
{"id": "2507.12143", "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "authors": ["Pavel Šindelář", "Ondřej Bojar"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation Forum", "url": "http://arxiv.org/abs/2507.12143v1", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "pdf_url": "http://arxiv.org/pdf/2507.12143v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11746", "title": "Acceleration methods for fixed point iterations", "authors": ["Yousef Saad"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      80 pp", "url": "http://arxiv.org/abs/2507.11746v1", "summary": "A pervasive approach in scientific computing is to express the solution to a\ngiven problem as the limit of a sequence of vectors or other mathematical\nobjects. In many situations these sequences are generated by slowly converging\niterative procedures and this led practitioners to seek faster alternatives to\nreach the limit. ``Acceleration techniques'' comprise a broad array of methods\nspecifically designed with this goal in mind. They started as a means of\nimproving the convergence of general scalar sequences by various forms of\n``extrapolation to the limit'', i.e., by extrapolating the most recent iterates\nto the limit via linear combinations. Extrapolation methods of this type, the\nbest known example of which is Aitken's Delta-squared process, require only the\nsequence of vectors as input. However, limiting methods to only use the\niterates is too restrictive. Accelerating sequences generated by fixed-point\niterations by utilizing both the iterates and the fixed-point mapping itself\nhas proven highly successful across various areas of physics. A notable example\nof these Fixed-Point accelerators (FP-Accelerators) is a method developed by D.\nAnderson in 1965 and now widely known as Anderson Acceleration (AA).\nFurthermore, Quasi-Newton and Inexact Newton methods can also be placed in this\ncategory as well. This paper presents an overview of these methods -- with an\nemphasis on those, such as AA, that are geared toward accelerating fixed point\niterations.", "comment": "80 pp", "pdf_url": "http://arxiv.org/pdf/2507.11746v1", "cate": "math.NA", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.09592", "title": "THOR: Transformer Heuristics for On-Demand Retrieval", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "categories": ["cs.DB", "cs.AI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09592v2", "summary": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09592v2", "cate": "cs.DB", "date": "2025-07-13", "updated": "2025-07-16"}
{"id": "2507.07511", "title": "Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs. Deep Learning", "authors": ["Joris Suurmeijer", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures; fixed typos", "url": "http://arxiv.org/abs/2507.07511v2", "summary": "Brain-computer interfaces (BCIs) turn brain signals into functionally useful\noutput, but they are not always accurate. A good Machine Learning classifier\nshould be able to indicate how confident it is about a given classification, by\ngiving a probability for its classification. Standard classifiers for Motor\nImagery BCIs do give such probabilities, but research on uncertainty\nquantification has been limited to Deep Learning. We compare the uncertainty\nquantification ability of established BCI classifiers using Common Spatial\nPatterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in\nDeep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as\nstandard Convolutional Neural Networks (CNNs).\n  We found that the overconfidence typically seen in Deep Learning is not a\nproblem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we\nsolved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best\nuncertainty estimates, but Deep Ensembles and standard CNNs give the best\nclassifications. We show that all models are able to separate between easy and\ndifficult estimates, so that we can increase the accuracy of a Motor Imagery\nBCI by rejecting samples that are ambiguous.", "comment": "6 pages, 3 figures; fixed typos", "pdf_url": "http://arxiv.org/pdf/2507.07511v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-16"}
{"id": "2506.22027", "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "authors": ["Han Wang", "Shengyang Li", "Jian Yang", "Yuxuan Liu", "Yixuan Lv", "Zhuang Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2506.22027v3", "summary": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22027v3", "cate": "cs.CV", "date": "2025-06-27", "updated": "2025-07-16"}
{"id": "2507.12208", "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12208v1", "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12208v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11819", "title": "A quasi-interpolation operator yielding fully computable error bounds", "authors": ["T. Chaumont-Frelet", "M. Vohralik"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11819v1", "summary": "We design a quasi-interpolation operator from the Sobolev space\n$H^1_0(\\Omega)$ to its finite-dimensional finite element subspace formed by\npiecewise polynomials on a simplicial mesh with a computable approximation\nconstant. The operator 1) is defined on the entire $H^1_0(\\Omega)$, no\nadditional regularity is needed; 2) allows for an arbitrary polynomial degree;\n3) works in any space dimension; 4) is defined locally, in vertex patches of\nmesh elements; 5) yields optimal estimates for both the $H^1$ seminorm and the\n$L^2$ norm error; 6) gives a computable constant for both the $H^1$ seminorm\nand the $L^2$ norm error; 7) leads to the equivalence of global-best and\nlocal-best errors; 8) possesses the projection property. Its construction\nfollows the so-called potential reconstruction from a posteriori error\nanalysis. Numerical experiments illustrate that our quasi-interpolation\noperator systematically gives the correct convergence rates in both the $H^1$\nseminorm and the $L^2$ norm and its certified overestimation factor is rather\nsharp and stable in all tested situations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11819v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.09888", "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting", "authors": ["Huibo Xu", "Likang Wu", "Xianquan Wang", "Haoning Dang", "Chun-Wun Cheng", "Angelica I Aviles-Rivero", "Qi Liu"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09888v2", "summary": "Time series forecasting is a fundamental task with broad applications, yet\nconventional methods often treat data as discrete sequences, overlooking their\norigin as noisy samples of continuous processes. Crucially, discrete noisy\nobservations cannot uniquely determine a continuous function; instead, they\ncorrespond to a family of plausible functions. Mathematically, time series can\nbe viewed as noisy observations of a continuous function family governed by a\nshared probability measure. Thus, the forecasting task can be framed as\nlearning the transition from the historical function family to the future\nfunction family. This reframing introduces two key challenges: (1) How can we\nleverage discrete historical and future observations to learn the relationships\nbetween their underlying continuous functions? (2) How can we model the\ntransition path in function space from the historical function family to the\nfuture function family? To address these challenges, we propose NeuTSFlow, a\nnovel framework that leverages Neural Operators to facilitate flow matching for\nlearning path of measure between historical and future function families. By\nparameterizing the velocity field of the flow in infinite-dimensional function\nspaces, NeuTSFlow moves beyond traditional methods that focus on dependencies\nat discrete points, directly modeling function-level features instead.\nExperiments on diverse forecasting tasks demonstrate NeuTSFlow's superior\naccuracy and robustness, validating the effectiveness of the function-family\nperspective.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09888v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.07883", "title": "SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation", "authors": ["Hao Ban", "Gokul Ram Subramani", "Kaiyi Ji"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.07883v3", "summary": "Multi-task learning (MTL) enables a joint model to capture commonalities\nacross multiple tasks, reducing computation costs and improving data\nefficiency. However, a major challenge in MTL optimization is task conflicts,\nwhere the task gradients differ in direction or magnitude, limiting model\nperformance compared to single-task counterparts. Sharpness-aware minimization\n(SAM) minimizes task loss while simultaneously reducing the sharpness of the\nloss landscape. Our empirical observations show that SAM effectively mitigates\ntask conflicts in MTL. Motivated by these findings, we explore integrating SAM\ninto MTL but face two key challenges. While both the average loss gradient and\nindividual task gradients-referred to as global and local\ninformation-contribute to SAM, how to combine them remains unclear. Moreover,\ndirectly computing each task gradient introduces significant computational and\nmemory overheads. To address these challenges, we propose SAMO, a lightweight\n\\textbf{S}harpness-\\textbf{A}ware \\textbf{M}ulti-task \\textbf{O}ptimization\napproach, that leverages a joint global-local perturbation. The local\nperturbations are approximated using only forward passes and are layerwise\nnormalized to improve efficiency. Extensive experiments on a suite of\nmulti-task benchmarks demonstrate both the effectiveness and efficiency of our\nmethod. Code is available at https://github.com/OptMN-Lab/SAMO.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07883v3", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-15"}
{"id": "2507.03564", "title": "2.5D Object Detection for Intelligent Roadside Infrastructure", "authors": ["Nikolai Polley", "Yacin Boualili", "Ferdinand Mütsch", "Maximilian Zipfl", "Tobias Fleck", "J. Marius Zöllner"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at 2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC)", "url": "http://arxiv.org/abs/2507.03564v2", "summary": "On-board sensors of autonomous vehicles can be obstructed, occluded, or\nlimited by restricted fields of view, complicating downstream driving\ndecisions. Intelligent roadside infrastructure perception systems, installed at\nelevated vantage points, can provide wide, unobstructed intersection coverage,\nsupplying a complementary information stream to autonomous vehicles via\nvehicle-to-everything (V2X) communication. However, conventional 3D\nobject-detection algorithms struggle to generalize under the domain shift\nintroduced by top-down perspectives and steep camera angles. We introduce a\n2.5D object detection framework, tailored specifically for infrastructure\nroadside-mounted cameras. Unlike conventional 2D or 3D object detection, we\nemploy a prediction approach to detect ground planes of vehicles as\nparallelograms in the image frame. The parallelogram preserves the planar\nposition, size, and orientation of objects while omitting their height, which\nis unnecessary for most downstream applications. For training, a mix of\nreal-world and synthetically generated scenes is leveraged. We evaluate\ngeneralizability on a held-out camera viewpoint and in adverse-weather\nscenarios absent from the training set. Our results show high detection\naccuracy, strong cross-viewpoint generalization, and robustness to diverse\nlighting and weather conditions. Model weights and inference code are provided\nat: https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection", "comment": "Accepted at 2025 IEEE 28th International Conference on Intelligent\n  Transportation Systems (ITSC)", "pdf_url": "http://arxiv.org/pdf/2507.03564v2", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-07-16"}
{"id": "2507.12260", "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12260v1", "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12260v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11822", "title": "Analysis of a fast fully discrete finite element method for fractional viscoelastic wave propagation", "authors": ["Hao Yuan", "Xiaoping Xie"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11822v1", "summary": "This paper is devoted to a numerical analysis of a fractional viscoelastic\nwave propagation model that generalizes the fractional Maxwell model and the\nfractional Zener model. First, we convert the model problem into a velocity\ntype integro-differential equation and establish existence, uniqueness and\nregularity of its solution. Then we consider a conforming\nlinear/bilinear/trilinear finite element semi-discrete scheme and a fast scheme\nof backward Euler full discretization with a sum-of-exponentials (SOE)\napproximation for the convolution integral, and derive error estimates for the\nsemi-discrete and fully discrete schemes. Finally, we provide several numerical\nexamples to verify the theoretical results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11822v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10136", "title": "A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run\" Therapeutic Strategy in Melanoma", "authors": ["Zhonglin Liu"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures. Submitted to the IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2025. Code is available at this https URL", "url": "http://arxiv.org/abs/2507.10136v3", "summary": "Innate resistance to anti-PD-1 immunotherapy remains a major clinical\nchallenge in metastatic melanoma, with the underlying molecular networks being\npoorly understood. To address this, we constructed a dynamic Probabilistic\nBoolean Network model using transcriptomic data from patient tumor biopsies to\nelucidate the regulatory logic governing therapy response. We then employed a\nreinforcement learning agent to systematically discover optimal, multi-step\ntherapeutic interventions and used explainable artificial intelligence to\nmechanistically interpret the agent's control policy. The analysis revealed\nthat a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2\nprotein (LOXL2) was the most effective strategy. Our explainable analysis\nshowed that this ''hit-and-run\" intervention is sufficient to erase the\nmolecular signature driving resistance, allowing the network to self-correct\nwithout requiring sustained intervention. This study presents a novel,\ntime-dependent therapeutic hypothesis for overcoming immunotherapy resistance\nand provides a powerful computational framework for identifying non-obvious\nintervention protocols in complex biological systems.", "comment": "9 pages, 5 figures. Submitted to the IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM) 2025. Code is available at\n  https://github.com/Liu-Zhonglin/pbn-melanoma-project", "pdf_url": "http://arxiv.org/pdf/2507.10136v3", "cate": "q-bio.QM", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.09016", "title": "Enhancing RLHF with Human Gaze Modeling", "authors": ["Karim Galliamov", "Ivan Titov", "Ilya Pershin"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09016v2", "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models with\nhuman preferences but is computationally expensive. We explore two approaches\nthat leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models\nand (2) gaze-based distribution of sparse rewards at token level. Our\nexperiments demonstate that gaze-informed RLHF achieves faster convergence\nwhile maintaining or slightly improving performance, thus, reducing\ncomputational costs during policy optimization. These results show that human\ngaze provides a valuable and underused signal for policy optimization, pointing\nto a promising direction for improving RLHF efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09016v2", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-16"}
{"id": "2507.03976", "title": "Robust Low-light Scene Restoration via Illumination Transition", "authors": ["Ze Li", "Feng Zhang", "Xiatian Zhu", "Meng Zhang", "Yanghong Zhou", "P. Y. Mok"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.03976v2", "summary": "Synthesizing normal-light novel views from low-light multiview images is an\nimportant yet challenging task, given the low visibility and high ISO noise\npresent in the input images. Existing low-light enhancement methods often\nstruggle to effectively preprocess such low-light inputs, as they fail to\nconsider correlations among multiple views. Although other state-of-the-art\nmethods have introduced illumination-related components offering alternative\nsolutions to the problem, they often result in drawbacks such as color\ndistortions and artifacts, and they provide limited denoising effectiveness. In\nthis paper, we propose a novel Robust Low-light Scene Restoration framework\n(RoSe), which enables effective synthesis of novel views in normal lighting\nconditions from low-light multiview image inputs, by formulating the task as an\nilluminance transition estimation problem in 3D space, conceptualizing it as a\nspecialized rendering task. This multiview-consistent illuminance transition\nfield establishes a robust connection between low-light and normal-light\nconditions. By further exploiting the inherent low-rank property of\nillumination to constrain the transition representation, we achieve more\neffective denoising without complex 2D techniques or explicit noise modeling.\nTo implement RoSe, we design a concise dual-branch architecture and introduce a\nlow-rank denoising module. Experiments demonstrate that RoSe significantly\noutperforms state-of-the-art models in both rendering quality and multiview\nconsistency on standard benchmarks. The codes and data are available at\nhttps://pegasus2004.github.io/RoSe.", "comment": "10 pages, 5 figures, Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.03976v2", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-07-16"}
{"id": "2507.12372", "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12372v1", "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12372v1", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.11944", "title": "Automatic reproducing kernel and regularization for learning convolution kernels", "authors": ["Haibo Li", "Fei Lu"], "categories": ["math.NA", "cs.NA", "47A52, 65F22, 65J20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11944v1", "summary": "Learning convolution kernels in operators from data arises in numerous\napplications and represents an ill-posed inverse problem of broad interest.\nWith scant prior information, kernel methods offer a natural nonparametric\napproach with regularization. However, a major challenge is to select a proper\nreproducing kernel, especially as operators and data vary. We show that the\ninput data and convolution operator themselves induce an automatic,\ndata-adaptive RKHS (DA-RKHS), obviating manual kernel selection. In particular,\nwhen the observation data is discrete and finite, there is a finite set of\nautomatic basis functions sufficient to represent the estimators in the\nDA-RKHS, including the minimal-norm least-squares, Tikhonov, and\nconjugate-gradient estimators. We develop both Tikhonov and scalable iterative\nand hybrid algorithms using the automatic basis functions. Numerical\nexperiments on integral, nonlocal, and aggregation operators confirm that our\nautomatic RKHS regularization consistently outperforms standard ridge\nregression and Gaussian process methods with preselected kernels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11944v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10502", "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop", "authors": ["Elizabeth Fahsbender", "Alma Andersson", "Jeremy Ash", "Polina Binder", "Daniel Burkhardt", "Benjamin Chang", "Georg K. Gerber", "Anthony Gitter", "Patrick Godau", "Ankit Gupta", "Genevieve Haliburton", "Siyu He", "Trey Ideker", "Ivana Jelic", "Aly Khan", "Yang-Joon Kim", "Aditi Krishnapriyan", "Jon M. Laurent", "Tianyu Liu 28", "Emma Lundberg", "Shalin B. Mehta", "Rob Moccia", "Angela Oliveira Pisco", "Katherine S. Pollard", "Suresh Ramani", "Julio Saez-Rodriguez", "Yasin Senbabaoglu", "Elana Simon", "Srinivasan Sivanandan", "Gustavo Stolovitzky", "Marc Valer", "Bo Wang", "Xikun Zhang", "James Zou", "Katrina Kalantar"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10502v1", "summary": "Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10502v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10452", "title": "Some remarks on gradient dominance and LQR policy optimization", "authors": ["Eduardo D. Sontag"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This is a short paper summarizing the first part of the slides presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has been added", "url": "http://arxiv.org/abs/2507.10452v1", "summary": "Solutions of optimization problems, including policy optimization in\nreinforcement learning, typically rely upon some variant of gradient descent.\nThere has been much recent work in the machine learning, control, and\noptimization communities applying the Polyak-{\\L}ojasiewicz Inequality (PLI) to\nsuch problems in order to establish an exponential rate of convergence (a.k.a.\n``linear convergence'' in the local-iteration language of numerical analysis)\nof loss functions to their minima under the gradient flow. Often, as is the\ncase of policy iteration for the continuous-time LQR problem, this rate\nvanishes for large initial conditions, resulting in a mixed globally linear /\nlocally exponential behavior. This is in sharp contrast with the discrete-time\nLQR problem, where there is global exponential convergence. That gap between CT\nand DT behaviors motivates the search for various generalized PLI-like\nconditions, and this talk will address that topic. Moreover, these\ngeneralizations are key to understanding the transient and asymptotic effects\nof errors in the estimation of the gradient, errors which might arise from\nadversarial attacks, wrong evaluation by an oracle, early stopping of a\nsimulation, inaccurate and very approximate digital twins, stochastic\ncomputations (algorithm ``reproducibility''), or learning by sampling from\nlimited data. We describe an ``input to state stability'' (ISS) analysis of\nthis issue. The lecture also discussed convergence and PLI-like properties of\n``linear feedforward neural networks'' in feedback control, but this arXiv\nskips that part (to be updated). Much of the work described here was done in\ncollaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping\nJiang, and Milad Siami.", "comment": "This is a short paper summarizing the first part of the slides\n  presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control\n  Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has\n  been added. A second part on neural net feedback controllers is to be added", "pdf_url": "http://arxiv.org/pdf/2507.10452v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.06210", "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions", "authors": ["Yuchen Huang", "Zhiyuan Fan", "Zhitao He", "Sandeep Polisetty", "Wenyan Li", "Yi R. Fung"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      25 pages, COLM 2025", "url": "http://arxiv.org/abs/2507.06210v2", "summary": "Pretrained vision-language models (VLMs) such as CLIP excel in general\nmultimodal comprehension but often struggle to capture nuanced,\ncontext-dependent visual cues. This makes it difficult to distinguish between\nsimilar-looking concepts with potentially different cultural meanings. Such\ndeficiencies are mainly due to a limited amount of high-quality cultural data,\ncontextual information, and the lack of negative examples that highlight subtle\ndifferences. To mitigate this, we design a data curation pipeline leveraging\nopen-sourced VLMs and text-to-image models to construct CulTwin, a synthetic\ncultural dataset. This dataset consists of paired concept-caption-image\ntriplets, where concepts visually resemble each other but are culturally\ndifferent. Then, we fine-tune CLIP on CulTwin to develop CultureCLIP, which\naligns cultural concepts with contextually enhanced captions and synthetic\nimages through tailored contrastive learning. Experiments on culture-specific\nbenchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a\nnotable 5.49% improvement in fine-grained concept recognition on certain tasks\nwhile preserving CLIP's original generalization ability, validating the\neffectiveness of our data synthesis and VLM backbone training paradigm in\ncapturing subtle cultural distinctions.", "comment": "25 pages, COLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.06210v2", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-16"}
{"id": "2403.09040", "title": "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems", "authors": ["Jennifer Hsia", "Afreen Shaikh", "Zhiruo Wang", "Graham Neubig"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2403.09040v3", "summary": "Retrieval-augmented generation (RAG) enhances language models by integrating\nexternal knowledge, but its effectiveness is highly dependent on system\nconfiguration. Improper retrieval settings can degrade performance, making RAG\nless reliable than closed-book generation. In this work, we introduce RAGGED, a\nframework for systematically evaluating RAG systems across diverse\nretriever-reader configurations, retrieval depths, and datasets. Our analysis\nreveals that reader robustness to noise is the key determinant of RAG stability\nand scalability. Some readers benefit from increased retrieval depth, while\nothers degrade due to their sensitivity to distracting content. Through\nlarge-scale experiments on open-domain, multi-hop, and specialized-domain\ndatasets, we show that retrievers, rerankers, and prompts influence performance\nbut do not fundamentally alter these reader-driven trends. By providing a\nprincipled framework and new metrics to assess RAG stability and scalability,\nRAGGED enables systematic evaluation of retrieval-augmented generation systems,\nguiding future research on optimizing retrieval depth and model robustness.", "comment": "Project page: https://github.com/neulab/ragged", "pdf_url": "http://arxiv.org/pdf/2403.09040v3", "cate": "cs.CL", "date": "2024-03-14", "updated": "2025-07-16"}
{"id": "2507.11962", "title": "Structured First-Layer Initialization Pre-Training Techniques to Accelerate Training Process Based on $\\varepsilon$-Rank", "authors": ["Tao Tang", "Jiang Yang", "Yuxiang Zhao", "Quanhui Zhu"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11962v1", "summary": "Training deep neural networks for scientific computing remains\ncomputationally expensive due to the slow formation of diverse feature\nrepresentations in early training stages. Recent studies identify a staircase\nphenomenon in training dynamics, where loss decreases are closely correlated\nwith increases in $\\varepsilon$-rank, reflecting the effective number of\nlinearly independent neuron functions. Motivated by this observation, this work\nproposes a structured first-layer initialization (SFLI) pre-training method to\nenhance the diversity of neural features at initialization by constructing\n$\\varepsilon$-linearly independent neurons in the input layer. We present\nsystematic initialization schemes compatible with various activation functions\nand integrate the strategy into multiple neural architectures, including\nmodified multi-layer perceptrons and physics-informed residual adaptive\nnetworks. Extensive numerical experiments on function approximation and PDE\nbenchmarks, demonstrate that SFLI significantly improves the initial\n$\\varepsilon$-rank, accelerates convergence, mitigates spectral bias, and\nenhances prediction accuracy. With the help of SILP, we only need to add one\nline of code to conventional existing algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11962v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10530", "title": "Accurate generation of chemical reaction transition states by conditional flow matching", "authors": ["Ping Tuo", "Jiale Chen", "Ju Li"], "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10530v2", "summary": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10530v2", "cate": "physics.chem-ph", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.10594", "title": "Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features", "authors": ["Shengda Zhuo", "Di Wu", "Yi He", "Shuqiang Huang", "Xindong Wu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10594v2", "summary": "Online learning, where feature spaces can change over time, offers a flexible\nlearning paradigm that has attracted considerable attention. However, it still\nfaces three significant challenges. First, the heterogeneity of real-world data\nstreams with mixed feature types presents challenges for traditional parametric\nmodeling. Second, data stream distributions can shift over time, causing an\nabrupt and substantial decline in model performance. Additionally, the time and\ncost constraints make it infeasible to label every data instance in a\nsupervised setting. To overcome these challenges, we propose a new algorithm\nOnline Learning from Mix-typed, Drifted, and Incomplete Streaming Features\n(OL-MDISF), which aims to relax restrictions on both feature types, data\ndistribution, and supervision information. Our approach involves utilizing\ncopula models to create a comprehensive latent space, employing an adaptive\nsliding window for detecting drift points to ensure model stability, and\nestablishing label proximity information based on geometric structural\nrelationships. To demonstrate the model's efficiency and effectiveness, we\nprovide theoretical analysis and comprehensive experimental results.\n  This extension serves as a standalone technical reference to the original\nOL-MDISF method. It provides (i) a contextual analysis of OL-MDISF within the\nbroader landscape of online learning, covering recent advances in mixed-type\nfeature modeling, concept drift adaptation, and weak supervision, and (ii) a\ncomprehensive set of experiments across 14 real-world datasets under two types\nof drift scenarios. These include full CER trends, ablation studies,\nsensitivity analyses, and temporal ensemble dynamics. We hope this document can\nserve as a reproducible benchmark and technical resource for researchers\nworking on nonstationary, heterogeneous, and weakly supervised data streams.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10594v2", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-16"}
{"id": "2507.08492", "title": "Dual Dimensions Geometric Representation Learning Based Document Dewarping", "authors": ["Heng Li", "Qingcai Chen", "Xiangping Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08492v2", "summary": "Document image dewarping remains a challenging task in the deep learning era.\nWhile existing methods have improved by leveraging text line awareness, they\ntypically focus only on a single horizontal dimension. In this paper, we\npropose a fine-grained deformation perception model that focuses on Dual\nDimensions of document horizontal-vertical-lines to improve document Dewarping\ncalled D2Dewarp. It can perceive distortion trends in different directions\nacross document details. To combine the horizontal and vertical granularity\nfeatures, an effective fusion module based on X and Y coordinate is designed to\nfacilitate interaction and constraint between the two dimensions for feature\ncomplementarity. Due to the lack of annotated line features in current public\ndewarping datasets, we also propose an automatic fine-grained annotation method\nusing public document texture images and an automatic rendering engine to build\na new large-scale distortion training dataset. The code and dataset will be\npublicly released. On public Chinese and English benchmarks, both quantitative\nand qualitative results show that our method achieves better rectification\nresults compared with the state-of-the-art methods. The dataset will be\npublicly available at https://github.com/xiaomore/DocDewarpHV", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08492v2", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-16"}
{"id": "2406.17241", "title": "Understanding Language Model Circuits through Knowledge Editing", "authors": ["Huaizhi Ge", "Frank Rudzicz", "Zining Zhu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      A previous version of this document contained a hidden prompt entered by Z Zhu without knowledge of -- or consent by -- his co-authors. This version does not contain the prompt", "url": "http://arxiv.org/abs/2406.17241v4", "summary": "Recent advances in language model interpretability have identified circuits,\ncritical subnetworks that replicate model behaviors, yet how knowledge is\nstructured within these crucial subnetworks remains opaque. To gain an\nunderstanding toward the knowledge in the circuits, we conduct systematic\nknowledge editing experiments on the circuits of the GPT-2 language model. Our\nanalysis reveals intriguing patterns in how circuits respond to editing\nattempts, the extent of knowledge distribution across network components, and\nthe architectural composition of knowledge-bearing circuits. These findings\noffer insights into the complex relationship between model circuits and\nknowledge representation, deepening the understanding of how information is\norganized within language models. Our findings offer novel insights into the\n``meanings'' of the circuits, and introduce directions for further\ninterpretability and safety research of language models.", "comment": "A previous version of this document contained a hidden prompt entered\n  by Z Zhu without knowledge of -- or consent by -- his co-authors. This\n  version does not contain the prompt", "pdf_url": "http://arxiv.org/pdf/2406.17241v4", "cate": "cs.CL", "date": "2024-06-25", "updated": "2025-07-15"}
{"id": "2507.12036", "title": "The Arrow-Hurwicz iteration for virtual element discretizations of the incompressible Navier-Stokes equations", "authors": ["Binbin Du", "Shenxiang Cheng", "Yue Yu", "Chuanjun Chen"], "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      32 pages, 6 figures", "url": "http://arxiv.org/abs/2507.12036v1", "summary": "This article presents a detailed analysis of the Arrow-Hurwicz iteration\napplied to the solution of the incompressible Navier-Stokes equations,\ndiscretized by a divergence-free mixed virtual element method. Under a set of\nappropriate assumptions, it is rigorously demonstrated that the method exhibits\ngeometric convergence, with a contraction factor that remains independent of\nthe mesh sizes. A series of numerical experiments are conducted to validate the\ntheoretical findings and to assess the computational performance of the\nproposed method.", "comment": "32 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.12036v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.10628", "title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning", "authors": ["Ziru Liu", "Cheng Gong", "Xinyu Fu", "Yaofang Liu", "Ran Chen", "Shoubo Hu", "Suiyun Zhang", "Rui Liu", "Qingfu Zhang", "Dandan Tu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code avaiable at this https URL", "url": "http://arxiv.org/abs/2507.10628v2", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels.", "comment": "Code avaiable at https://github.com/hkgc-1/GHPO", "pdf_url": "http://arxiv.org/pdf/2507.10628v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2507.11439", "title": "Data Augmentation in Time Series Forecasting through Inverted Framework", "authors": ["Hongming Tan", "Ting Chen", "Ruochong Jin", "Wai Kin Chan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The paper is under consideration at Pattern Recognition Letters", "url": "http://arxiv.org/abs/2507.11439v2", "summary": "Currently, iTransformer is one of the most popular and effective models for\nmultivariate time series (MTS) forecasting. Thanks to its inverted framework,\niTransformer effectively captures multivariate correlation. However, the\ninverted framework still has some limitations. It diminishes temporal\ninterdependency information, and introduces noise in cases of nonsignificant\nvariable correlation. To address these limitations, we introduce a novel data\naugmentation method on inverted framework, called DAIF. Unlike previous data\naugmentation methods, DAIF stands out as the first real-time augmentation\nspecifically designed for the inverted framework in MTS forecasting. We first\ndefine the structure of the inverted sequence-to-sequence framework, then\npropose two different DAIF strategies, Frequency Filtering and Cross-variation\nPatching to address the existing challenges of the inverted framework.\nExperiments across multiple datasets and inverted models have demonstrated the\neffectiveness of our DAIF.", "comment": "The paper is under consideration at Pattern Recognition Letters", "pdf_url": "http://arxiv.org/pdf/2507.11439v2", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2507.09953", "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The authors have decided to withdraw this submission due to incorrect affiliation information and certain logical inconsistencies in the manuscript that require revision", "url": "http://arxiv.org/abs/2507.09953v2", "summary": "While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.", "comment": "The authors have decided to withdraw this submission due to incorrect\n  affiliation information and certain logical inconsistencies in the manuscript\n  that require revision", "pdf_url": "http://arxiv.org/pdf/2507.09953v2", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2406.17253", "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?", "authors": ["Huaizhi Ge", "Frank Rudzicz", "Zining Zhu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      A previous version of this document contained a hidden prompt entered by Z Zhu without knowledge of -- or consent by -- his co-authors. This version does not contain the prompt", "url": "http://arxiv.org/abs/2406.17253v3", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nupdating their knowledge post-training remains a critical challenge. While\nrecent model editing techniques like Rank-One Model Editing (ROME) show\npromise, their effectiveness may vary based on the nature of the knowledge\nbeing edited. We introduce the concept of ``perplexingness'': the degree to\nwhich new knowledge conflicts with an LLM's learned conceptual hierarchies and\ncategorical relationships. For instance, editing ``British Shorthair is a kind\nof cat'' to ``British Shorthair is a kind of dog'' represents a\nlow-perplexingness edit within the same taxonomic level, while editing ``A cat\nis a kind of animal'' to ``A cat is a kind of plant'' represents a\nhigh-perplexingness edit that violates fundamental categorical boundaries. To\nsystematically investigate this phenomenon, we introduce HierarchyData, a\ncarefully curated dataset of 99 hyponym-hypernym pairs across diverse\ncategories. Through controlled experiments across three models and four editing\nmethods, we demonstrate a strong negative correlation between the\nperplexingness of new knowledge and the effectiveness of knowledge editing. Our\nanalysis reveals that edits involving more abstract concepts (hypernyms)\ngenerally exhibit higher perplexingness and are more resistant to modification\nthan their specific counterparts (hyponyms). These findings highlight a\nfundamental challenge in LLM knowledge editing: the more a new fact contradicts\nan LLM's learned conceptual hierarchies, the harder it becomes to reliably\nencode that knowledge.", "comment": "A previous version of this document contained a hidden prompt entered\n  by Z Zhu without knowledge of -- or consent by -- his co-authors. This\n  version does not contain the prompt", "pdf_url": "http://arxiv.org/pdf/2406.17253v3", "cate": "cs.CL", "date": "2024-06-25", "updated": "2025-07-15"}
{"id": "2507.12140", "title": "A Hybrid High-Order method for the power-law Brinkman problem robust in all regimes", "authors": ["Daniel Castañón Quiroz", "Daniele A. Di Pietro", "Jérôme Droniou", "Marwa Salah"], "categories": ["math.NA", "cs.NA", "65N30, 65N08, 76S05, 76D07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12140v1", "summary": "In this work we propose and analyze a new Hybrid High-Order method for the\nBrinkman problem for fluids with power-law viscosity. The proposed method\nsupports general meshes and arbitrary approximation orders and is robust in all\nregimes, from pure (power-law) Stokes to pure Darcy. Robustness is reflected by\nerror estimates that distinguish the contributions from Stokes- and\nDarcy-dominated elements as identified by an appropriate dimensionless number,\nand that additionally account for pre-asymptotic orders of convergence.\nTheoretical results are illustrated by a complete panel of numerical\nexperiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12140v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2309.10301", "title": "Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms", "authors": ["Keru Wu", "Yuansi Chen", "Wooseok Ha", "Bin Yu"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.10301v3", "summary": "Domain adaptation (DA) is a statistical learning problem that arises when the\ndistribution of the source data used to train a model differs from that of the\ntarget data used to evaluate the model. While many DA algorithms have\ndemonstrated considerable empirical success, blindly applying these algorithms\ncan often lead to worse performance on new datasets. To address this, it is\ncrucial to clarify the assumptions under which a DA algorithm has good target\nperformance. In this work, we focus on the assumption of the presence of\nconditionally invariant components (CICs), which are relevant for prediction\nand remain conditionally invariant across the source and target data. We\ndemonstrate that CICs, which can be estimated through conditional invariant\npenalty (CIP), play three prominent roles in providing target risk guarantees\nin DA. First, we propose a new algorithm based on CICs, importance-weighted\nconditional invariant penalty (IW-CIP), which has target risk guarantees beyond\nsimple settings such as covariate shift and label shift. Second, we show that\nCICs help identify large discrepancies between source and target risks of other\nDA algorithms. Finally, we demonstrate that incorporating CICs into the domain\ninvariant projection (DIP) algorithm can address its failure scenario caused by\nlabel-flipping features. We support our new algorithms and theoretical findings\nvia numerical experiments on synthetic data, MNIST, CelebA, Camelyon17, and\nDomainNet datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.10301v3", "cate": "stat.ML", "date": "2023-09-19", "updated": "2025-07-16"}
{"id": "2507.10432", "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "authors": ["Qiang Li", "Qingsen Yan", "Haojian Huang", "Peng Wu", "Haokui Zhang", "Yanning Zhang"], "categories": ["cs.CV", "I.4.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures, Accepted at ACMMM 2025", "url": "http://arxiv.org/abs/2507.10432v3", "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.", "comment": "9 pages, 5 figures, Accepted at ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.10432v3", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-16"}
{"id": "2410.11647", "title": "Measuring Spiritual Values and Bias of Large Language Models", "authors": ["Songyuan Liu", "Ziyang Zhang", "Runze Yan", "Wei Wu", "Carl Yang", "Jiaying Lu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages including appendix; 5 figures; 5 tables", "url": "http://arxiv.org/abs/2410.11647v2", "summary": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.", "comment": "9 pages including appendix; 5 figures; 5 tables", "pdf_url": "http://arxiv.org/pdf/2410.11647v2", "cate": "cs.CL", "date": "2024-10-15", "updated": "2025-07-16"}
{"id": "2507.12226", "title": "Optimal Spectral Approximation in the Overlaps for Generalized Finite Element Methods", "authors": ["Christian Alber", "Peter Bastian", "Moritz Hauck", "Robert Scheichl"], "categories": ["math.NA", "cs.NA", "65F10, 65N15, 65N30, 65N55"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      22 pages, 7 figures", "url": "http://arxiv.org/abs/2507.12226v1", "summary": "In this paper, we study a generalized finite element method for solving\nsecond-order elliptic partial differential equations with rough coefficients.\nThe method uses local approximation spaces computed by solving eigenvalue\nproblems on rings around the boundary of local subdomains. Compared to the\ncorresponding method that solves eigenvalue problems on the whole subdomains,\nthe problem size and the bandwidth of the resulting system matrices are\nsubstantially reduced, resulting in faster spectral computations. We prove a\nnearly exponential a priori decay result for the local approximation errors of\nthe proposed method, which implies the nearly exponential decay of the overall\napproximation error of the method. The proposed method can also be used as a\npreconditioner, and only a slight adaptation of our theory is necessary to\nprove the optimal convergence of the preconditioned iteration. Numerical\nexperiments are presented to support the effectiveness of the proposed method\nand to investigate its coefficient robustness.", "comment": "22 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.12226v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2411.12898", "title": "Problem-dependent convergence bounds for randomized linear gradient compression", "authors": ["Thomas Flynn", "Patrick Johnstone", "Shinjae Yoo"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.12898v3", "summary": "In distributed optimization, the communication of model updates can be a\nperformance bottleneck. Consequently, gradient compression has been proposed as\na means of increasing optimization throughput. In general, due to information\nloss, compression introduces a penalty on the number of iterations needed to\nreach a solution. In this work, we investigate how the iteration penalty\ndepends on the interaction between compression and problem structure, in the\ncontext of non-convex stochastic optimization. We focus on linear schemes,\nwhere compression and decompression can be modeled as multiplication with a\nrandom matrix. We consider several distributions of matrices, among them\nHaar-distributed orthogonal matrices and matrices with random Gaussian entries.\nWe find that the impact of compression on convergence can be quantified in\nterms of a smoothness matrix associated with the objective function, using a\nnorm defined by the compression scheme. The analysis reveals that in certain\ncases, compression performance is related to low-rank structure or other\nspectral properties of the problem and our bounds predict that the penalty\nintroduced by compression is significantly reduced compared to worst-case\nbounds that only consider the compression level, ignoring problem data. We\nverify the theoretical findings experimentally, including fine-tuning an image\nclassification model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.12898v3", "cate": "math.OC", "date": "2024-11-19", "updated": "2025-07-15"}
{"id": "2507.11055", "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Jinman Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.11055v2", "summary": "Medical language-guided segmentation, integrating textual clinical reports as\nauxiliary guidance to enhance image segmentation, has demonstrated significant\nimprovements over unimodal approaches. However, its inherent reliance on paired\nimage-text input, which we refer to as ``textual reliance\", presents two\nfundamental limitations: 1) many medical segmentation datasets lack paired\nreports, leaving a substantial portion of image-only data underutilized for\ntraining; and 2) inference is limited to retrospective analysis of cases with\npaired reports, limiting its applicability in most clinical scenarios where\nsegmentation typically precedes reporting. To address these limitations, we\npropose ProLearn, the first Prototype-driven Learning framework for\nlanguage-guided segmentation that fundamentally alleviates textual reliance. At\nits core, we introduce a novel Prototype-driven Semantic Approximation (PSA)\nmodule to enable approximation of semantic guidance from textual input. PSA\ninitializes a discrete and compact prototype space by distilling\nsegmentation-relevant semantics from textual reports. Once initialized, it\nsupports a query-and-respond mechanism which approximates semantic guidance for\nimages without textual input, thereby alleviating textual reliance. Extensive\nexperiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn\noutperforms state-of-the-art language-guided methods when limited text is\navailable.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.11055v2", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2410.16069", "title": "Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context", "authors": ["Maggie Mi", "Aline Villavicencio", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025", "url": "http://arxiv.org/abs/2410.16069v2", "summary": "Human processing of idioms relies on understanding the contextual sentences\nin which idioms occur, as well as language-intrinsic features such as frequency\nand speaker-intrinsic factors like familiarity. While LLMs have shown high\nperformance on idiomaticity detection tasks, this success may be attributed to\nreasoning shortcuts in existing datasets. To this end, we construct a novel,\ncontrolled contrastive dataset designed to test whether LLMs can effectively\nuse context to disambiguate idiomatic meaning. Additionally, we explore how\ncollocational frequency and sentence probability influence model performance.\nOur findings reveal that LLMs often fail to resolve idiomaticity when it is\nrequired to attend to the surrounding context, and that models perform better\non sentences that have higher likelihood. The collocational frequency of\nexpressions also impacts performance. We make our code and dataset publicly\navailable.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2410.16069v2", "cate": "cs.CL", "date": "2024-10-21", "updated": "2025-07-15"}
{"id": "2507.12307", "title": "The iterated Golub-Kahan-Tikhonov method", "authors": ["Davide Bianchi", "Marco Donatelli", "Davide Furchì", "Lothar Reichel"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12307v1", "summary": "The Golub-Kahan-Tikhonov method is a popular solution technique for large\nlinear discrete ill-posed problems. This method first applies partial\nGolub-Kahan bidiagonalization to reduce the size of the given problem and then\nuses Tikhonov regularization to compute a meaningful approximate solution of\nthe reduced problem. It is well known that iterated variants of this method\noften yield approximate solutions of higher quality than the standard\nnon-iterated method. Moreover, it produces more accurate computed solutions\nthan the Arnoldi method when the matrix that defines the linear discrete\nill-posed problem is far from symmetric.\n  This paper starts with an ill-posed operator equation in infinite-dimensional\nHilbert space, discretizes the equation, and then applies the iterated\nGolub-Kahan-Tikhonov method to the solution of the latter problem. An error\nanalysis that addresses all discretization and approximation errors is\nprovided. Additionally, a new approach for choosing the regularization\nparameter is described. This solution scheme produces more accurate approximate\nsolutions than the standard (non-iterated) Golub-Kahan-Tikhonov method and the\niterated Arnoldi-Tikhonov method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12307v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2501.00691", "title": "Labels Generated by Large Language Models Help Measure People's Empathy in Vitro", "authors": ["Md Rakibul Hasan", "Yue Yao", "Md Zakir Hossain", "Aneesh Krishna", "Imre Rudas", "Shafin Rahman", "Tom Gedeon"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2501.00691v2", "summary": "Large language models (LLMs) have revolutionised many fields, with\nLLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions\nwithout costly task-specific training. In contrast to the widely studied prompt\nengineering for directly solving tasks (in vivo), this paper explores LLMs'\npotential for in-vitro applications: using LLM-generated labels to improve\nsupervised training of mainstream models. We examine two strategies - (1) noisy\nlabel correction and (2) training data augmentation - in empathy computing, an\nemerging task to predict psychology-based questionnaire outcomes from inputs\nlike textual narratives. Crowdsourced datasets in this domain often suffer from\nnoisy labels that misrepresent underlying empathy. We show that replacing or\nsupplementing these crowdsourced labels with LLM-generated labels, developed\nusing psychology-based scale-aware prompts, achieves statistically significant\naccuracy improvements. Notably, the RoBERTa pre-trained language model (PLM)\ntrained with noise-reduced labels yields a state-of-the-art Pearson correlation\ncoefficient of 0.648 on the public NewsEmp benchmarks. This paper further\nanalyses evaluation metric selection and demographic biases to help guide the\nfuture development of more equitable empathy computing models. Code and\nLLM-generated labels are available at\nhttps://github.com/hasan-rakibul/LLMPathy.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2501.00691v2", "cate": "cs.CL", "date": "2025-01-01", "updated": "2025-07-16"}
{"id": "2412.07682", "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation", "authors": ["Alfredo Garrachón Ruiz", "Tomás de la Rosa", "Daniel Borrajo"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages, 12 tables, 7 figures", "url": "http://arxiv.org/abs/2412.07682v4", "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.", "comment": "13 pages, 12 tables, 7 figures", "pdf_url": "http://arxiv.org/pdf/2412.07682v4", "cate": "cs.CL", "date": "2024-12-10", "updated": "2025-07-16"}
{"id": "2507.12338", "title": "A bound-preserving and conservative enriched Galerkin method for elliptic problems", "authors": ["Gabriel R. Barrenechea", "Philip L. Lederer", "Andreas Rupp"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12338v1", "summary": "We propose a locally conservative enriched Galerkin scheme that respects the\ndiscrete maximum principle of an elliptic problem. To this end, we use a\nsubstantial over-penalization of the discrete solution's jumps to obtain\noptimal convergence. To avoid the ill-conditioning issues that arise in\nover-penalized schemes, we introduce an involved splitting approach that\nseparates the system of equations for the discontinuous solution part from the\nsystem of equations for the continuous solution part, yielding well-behaved\nsubproblems. We prove the existence of discrete solutions and optimal error\nestimates, which are validated numerically.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12338v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2501.12189", "title": "MirrorCBO: A consensus-based optimization method in the spirit of mirror descent", "authors": ["Leon Bungert", "Franca Hoffmann", "Dohyeon Kim", "Tim Roith"], "categories": ["math.OC", "cs.LG", "35B40, 35Q84, 35Q89, 35Q90, 65K10, 90C26, 90C56"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      66 pages, 18 figures, 19 tables", "url": "http://arxiv.org/abs/2501.12189v2", "summary": "In this work we propose MirrorCBO, a consensus-based optimization (CBO)\nmethod which generalizes standard CBO in the same way that mirror descent\ngeneralizes gradient descent. For this we apply the CBO methodology to a swarm\nof dual particles and retain the primal particle positions by applying the\ninverse of the mirror map, which we parametrize as the subdifferential of a\nstrongly convex function $\\phi$. In this way, we combine the advantages of a\nderivative-free non-convex optimization algorithm with those of mirror descent.\nAs a special case, the method extends CBO to optimization problems with convex\nconstraints. Assuming bounds on the Bregman distance associated to $\\phi$, we\nprovide asymptotic convergence results for MirrorCBO with explicit exponential\nrate. Another key contribution is an exploratory numerical study of this new\nalgorithm across different application settings, focusing on (i)\nsparsity-inducing optimization, and (ii) constrained optimization,\ndemonstrating the competitive performance of MirrorCBO. We observe empirically\nthat the method can also be used for optimization on (non-convex) submanifolds\nof Euclidean space, can be adapted to mirrored versions of other recent CBO\nvariants, and that it inherits from mirror descent the capability to select\ndesirable minimizers, like sparse ones. We also include an overview of recent\nCBO approaches for constrained optimization and compare their performance to\nMirrorCBO.", "comment": "66 pages, 18 figures, 19 tables", "pdf_url": "http://arxiv.org/pdf/2501.12189v2", "cate": "math.OC", "date": "2025-01-21", "updated": "2025-07-16"}
{"id": "2502.10341", "title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation", "authors": ["Alexander Wettig", "Kyle Lo", "Sewon Min", "Hannaneh Hajishirzi", "Danqi Chen", "Luca Soldaini"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2502.10341v3", "summary": "Modern language models are trained on large, unstructured datasets consisting\nof trillions of tokens and obtained by crawling the web. The unstructured\nnature makes it difficult to reason about their contents and develop systematic\napproaches to data curation. In this paper, we unpack monolithic web corpora by\ndeveloping taxonomies of their contents and organizing them into domains. We\nintroduce WebOrganizer, a framework for organizing web pages in terms of both\ntheir topic and format. Using these two complementary notions of domains, we\nautomatically annotate pre-training data by distilling annotations from a large\nlanguage model into efficient classifiers. This allows us to study how data\nfrom different domains should be mixed to improve models on downstream tasks,\nand we show that we can combine insights about effective topics and formats to\nfurther boost performance. We demonstrate that our domain mixing also improves\nexisting methods that select data based on quality. Furthermore, we study and\ncompare how quality-based methods will implicitly change the domain mixture.\nOverall, our work demonstrates that constructing and mixing domains provides a\nvaluable complement to quality-based data curation methods, opening new avenues\nfor effective and insightful pre-training data curation.", "comment": "Accepted at ICML 2025. Project page: https://weborganizer.allen.ai", "pdf_url": "http://arxiv.org/pdf/2502.10341v3", "cate": "cs.CL", "date": "2025-02-14", "updated": "2025-07-16"}
{"id": "2507.12406", "title": "Refinement of the theory and convergence of the Sinc convolution", "authors": ["Tomoaki Okayama"], "categories": ["math.NA", "cs.NA", "65D15, 65D30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12406v1", "summary": "The Sinc convolution is an approximate formula for indefinite convolutions\nproposed by F. Stenger. The formula was derived based on the Sinc indefinite\nintegration formula combined with the single-exponential transformation.\nAlthough its efficiency has been confirmed in variety of areas, there remain\nsome open problems in its theory. The first contribution of this study is to\nresolve those problems by refinement of the theory of the Sinc convolution.\nThis contribution includes a partial resolution of Stenger's conjecture. The\nsecond contribution of this study is to improve the convergence rate by\nreplacement of the single-exponential transformation with the\ndouble-exponential transformation. In both theoretical and numerical ways, this\nstudy also shows that the convergence rate of the new formula is improved\ncompared to Stenger's formula.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12406v1", "cate": "math.NA", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2502.05676", "title": "Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction", "authors": ["Lars van der Laan", "Ahmed Alaa"], "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05676v3", "summary": "Ensuring model calibration is critical for reliable prediction, yet popular\ndistribution-free methods such as histogram binning and isotonic regression\noffer only asymptotic guarantees. We introduce a unified framework for Venn and\nVenn-Abers calibration that extends Vovk's approach beyond binary\nclassification to a broad class of prediction problems defined by generic loss\nfunctions. Our method transforms any perfectly in-sample calibrated predictor\ninto a set-valued predictor that, in finite samples, outputs at least one\nmarginally calibrated point prediction. These set predictions shrink\nasymptotically and converge to a single conditionally calibrated prediction,\ncapturing epistemic uncertainty. We further propose Venn multicalibration, a\nnew approach for achieving finite-sample calibration across subpopulations. For\nquantile loss, our framework recovers group-conditional and multicalibrated\nconformal prediction as special cases and yields novel prediction intervals\nwith quantile-conditional coverage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05676v3", "cate": "stat.ML", "date": "2025-02-08", "updated": "2025-07-15"}
{"id": "2502.11078", "title": "DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling", "authors": ["Aili Chen", "Chengyu Du", "Jiangjie Chen", "Jinghan Xu", "Yikai Zhang", "Siyu Yuan", "Zulong Chen", "Liangyue Li", "Yanghua Xiao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11078v2", "summary": "To advance personalized applications such as recommendation systems and user\nbehavior prediction, recent research increasingly adopts large language models\n(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,\neffective persona modeling necessitates leveraging streaming behavior data to\ncontinually optimize user personas. However, existing methods -whether\nregenerating personas or incrementally extending them with new behaviors -often\nfail to achieve sustained improvements in persona quality or future behavior\nprediction accuracy. To address this, we propose DEEPER, a novel approach for\ndynamic persona modeling that enables continual persona optimization.\nSpecifically, we enhance the model's direction -search capability through an\niterative reinforcement learning framework, allowing it to automatically\nidentify effective update directions and optimize personas using discrepancies\nbetween user behaviors and model predictions. Extensive experiments on dynamic\npersona modeling involving 4800 users across 10 domains highlight the superior\npersona optimization capabilities of DEEPER, delivering an impressive 32.2%\naverage reduction in user behavior prediction error over four update rounds\n-outperforming the best baseline by a remarkable 22.92%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11078v2", "cate": "cs.CL", "date": "2025-02-16", "updated": "2025-07-16"}
{"id": "2507.12040", "title": "An augmented Lagrangian method for strongly regular minimizers in a class of convex composite optimization problems", "authors": ["Chengjing Wang", "Peipei Tang"], "categories": ["math.OC", "cs.NA", "math.NA", "49J52, 49J53, 90C31, 90C22"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2507.12040v1", "summary": "In this paper, we study a class of convex composite optimization problems. We\nbegin by characterizing the equivalence between the primal/dual strong\nsecond-order sufficient condition and the dual/primal nondegeneracy condition.\nBuilding on this foundation, we derive a specific set of equivalent conditions\nfor the perturbation analysis of the problem. Furthermore, we employ the\naugmented Lagrangian method (ALM) to solve the problem and provide theoretical\nguarantees for its performance. Specifically, we establish the equivalence\nbetween the primal/dual second-order sufficient condition and the dual/primal\nstrict Robinson constraint qualification, as well as the equivalence between\nthe dual nondegeneracy condition and the nonsingularity of Clarke's generalized\nJacobian for the ALM subproblem. These theoretical results form a solid\nfoundation for designing efficient algorithms. Finally, we apply the ALM to the\nvon Neumann entropy optimization problem and present numerical experiments to\ndemonstrate the algorithm's effectiveness.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2507.12040v1", "cate": "math.OC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.04602", "title": "MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable Player in Basketball", "authors": ["Haifeng Sun", "Yu Xiong", "Runze Wu", "Kai Wang", "Lan Zhang", "Changjie Fan", "Shaojie Tang", "Xiang-Yang Li"], "categories": ["cs.GT", "cs.LG"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.04602v2", "summary": "The burgeoning growth of the esports and multiplayer online gaming community\nhas highlighted the critical importance of evaluating the Most Valuable Player\n(MVP). The establishment of an explainable and practical MVP evaluation method\nis very challenging. In our study, we specifically focus on play-by-play data,\nwhich records related events during the game, such as assists and points. We\naim to address the challenges by introducing a new MVP evaluation framework,\ndenoted as \\oursys, which leverages Shapley values. This approach encompasses\nfeature processing, win-loss model training, Shapley value allocation, and MVP\nranking determination based on players' contributions. Additionally, we\noptimize our algorithm to align with expert voting results from the perspective\nof causality. Finally, we substantiated the efficacy of our method through\nvalidation using the NBA dataset and the Dunk City Dynasty dataset and\nimplemented online deployment in the industry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.04602v2", "cate": "cs.GT", "date": "2025-06-05", "updated": "2025-07-16"}
{"id": "2503.08506", "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews", "authors": ["Xian Gao", "Jiacheng Ruan", "Zongyun Zhang", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2503.08506v3", "summary": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2503.08506v3", "cate": "cs.CL", "date": "2025-03-11", "updated": "2025-07-16"}
{"id": "2507.12411", "title": "Linearization-Based Feedback Stabilization of McKean-Vlasov PDEs", "authors": ["Dante Kalise", "Lucas M. Moschen", "Grigorios A. Pavliotis"], "categories": ["math.OC", "cs.NA", "math-ph", "math.MP", "math.NA", "49M41, 35Q84, 65M70", "G.1.6; G.1.8"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      24 pages, 8 figures", "url": "http://arxiv.org/abs/2507.12411v1", "summary": "We study the feedback stabilization of the McKean-Vlasov PDE on the torus.\nOur goal is to steer the dynamics toward a prescribed stationary distribution\nor accelerate convergence to it using a time-dependent control potential. We\nreformulate the controlled PDE in a weighted-projected space and apply the\nground-state transform to obtain a Schrodinger-type operator. The resulting\noperator framework enables spectral analysis, verification of the\ninfinite-dimensional Hautus test, and the construction of Riccati-based\nfeedback laws. We rigorously prove local exponential stabilization via maximal\nregularity arguments and nonlinear estimates. Numerical experiments on\nwell-studied models (the noisy Kuramoto model for synchronization, the O(2)\nspin model in a magnetic field, and the Gaussian/von Mises attractive\ninteraction potential) showcase the effectiveness of our control strategy,\ndemonstrating convergence speed-ups and stabilization of otherwise unstable\nequilibria.", "comment": "24 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.12411v1", "cate": "math.OC", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2506.13575", "title": "Machine Learning-Driven Compensation for Non-Ideal Channels in AWG-Based FBG Interrogator", "authors": ["Ivan A. Kazakov", "Iana V. Kulichenko", "Egor E. Kovalev", "Angelina A. Treskova", "Daria D. Barma", "Kirill M. Malakhov", "Ivan V. Oseledets", "Arkady V. Shipulin"], "categories": ["physics.optics", "cs.LG"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      The manuscript has been accepted and is now available in early access in IEEE Sensors Letters. This revision includes the addition of a co-author, and updates the style of Figure 4 and the formatting of Table 1", "url": "http://arxiv.org/abs/2506.13575v2", "summary": "We present an experimental study of a fiber Bragg grating (FBG) interrogator\nbased on a silicon oxynitride (SiON) photonic integrated arrayed waveguide\ngrating (AWG). While AWG-based interrogators are compact and scalable, their\npractical performance is limited by non-ideal spectral responses. To address\nthis, two calibration strategies within a 2.4 nm spectral region were compared:\n(1) a segmented analytical model based on a sigmoid fitting function, and (2) a\nmachine learning (ML)-based regression model. The analytical method achieves a\nroot mean square error (RMSE) of 7.11 pm within the calibrated range, while the\nML approach based on exponential regression achieves 3.17 pm. Moreover, the ML\nmodel demonstrates generalization across an extended 2.9 nm wavelength span,\nmaintaining sub-5 pm accuracy without re-fitting. Residual and error\ndistribution analyses further illustrate the trade-offs between the two\napproaches. ML-based calibration provides a robust, data-driven alternative to\nanalytical methods, delivering enhanced accuracy for non-ideal channel\nresponses, reduced manual calibration effort, and improved scalability across\ndiverse FBG sensor configurations.", "comment": "The manuscript has been accepted and is now available in early access\n  in IEEE Sensors Letters. This revision includes the addition of a co-author,\n  and updates the style of Figure 4 and the formatting of Table 1", "pdf_url": "http://arxiv.org/pdf/2506.13575v2", "cate": "physics.optics", "date": "2025-06-16", "updated": "2025-07-15"}
{"id": "2503.22913", "title": "Resona: Improving Context Copying in Linear Recurrence Models with Retrieval", "authors": ["Xinyu Wang", "Linrui Ma", "Jerry Huang", "Peng Lu", "Prasanna Parthasarathi", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Comments: Accepted at COLM 2025 (Conference on Learning with Machines)", "url": "http://arxiv.org/abs/2503.22913v2", "summary": "Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce Resona, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. Resona augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that Resona-augmented models observe significant performance gains\non a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs.", "comment": "Comments: Accepted at COLM 2025 (Conference on Learning with\n  Machines)", "pdf_url": "http://arxiv.org/pdf/2503.22913v2", "cate": "cs.CL", "date": "2025-03-28", "updated": "2025-07-16"}
{"id": "2306.04473", "title": "Fast adaptive high-order integral equation methods for electromagnetic scattering from smooth perfect electric conductors", "authors": ["Felipe Vico", "Leslie Greengard", "Michael O'Neil", "Manas Rachh"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.04473v2", "summary": "Many integral equation-based methods are available for problems of\ntime-harmonic electromagnetic scattering from perfect electric conductors.\nAmong the many challenges that arise in such calculations are the avoidance of\nspurious resonances, robustness of the method to scatterers of non-trivial\ntopology or multiscale features, stability under mesh refinement, ease of\nimplementation with high-order basis functions, and behavior in the static\nlimit. Since three-dimensional scattering is a challenging, large-scale\nproblem, many of these issues have been historically difficult to investigate.\nIt is only with the advent of fast algorithms for matrix-vector multiplies\ncoupled with modern iterative methods that a careful study of these issues can\nbe carried out effectively. Our focus here is on comparing the behavior of\nseveral integral equation formulations with regard to the issues noted above,\nnamely: the well-known, standard electric, magnetic, and combined field\nintegral equations with standard RWG basis functions, and the more modern\nnon-resonant charge-current and decoupled potential integral equation.\nNumerical results are provided to demonstrate the behavior of each of these\nschemes. Furthermore, we provide some analytical properties and comparisons\nwith the electric charge-current integral equation and the augmented\nregularized combined source integral equation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.04473v2", "cate": "math.NA", "date": "2023-06-07", "updated": "2025-07-16"}
{"id": "2507.06607", "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation", "authors": ["Liliang Ren", "Congcong Chen", "Haoran Xu", "Young Jin Kim", "Adam Atkinson", "Zheng Zhan", "Jiankai Sun", "Baolin Peng", "Liyuan Liu", "Shuohang Wang", "Hao Cheng", "Jianfeng Gao", "Weizhu Chen", "Yelong Shen"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06607v2", "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06607v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-16"}
{"id": "2505.10389", "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "authors": ["Benjamin White", "Anastasia Shimorina"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10389v2", "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. We investigate whether a single fine-tuned model can effectively\nhandle multiple domain-specific taxonomies simultaneously. We demonstrate that\na combined multi-domain model achieves performance comparable to specialized\nsingle-domain models while reducing operational complexity. We also share\nlessons learned for handling non-extractive predictions and evaluating various\nfailure modes when developing LLM-based systems for structured prediction\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10389v2", "cate": "cs.CL", "date": "2025-05-15", "updated": "2025-07-15"}
{"id": "2403.08269", "title": "A posteriori error estimates for the Generalized Burgers-Huxley equation with weakly singular kernels", "authors": ["Sumit Mahajan", "Arbaz Khan"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.08269v2", "summary": "This paper explores the residual based a posteriori error estimations for the\ngeneralized Burgers-Huxley equation (GBHE) featuring weakly singular kernels.\nInitially, we present a reliable and efficient error estimator for both the\nstationary GBHE and the semi-discrete GBHE with memory, utilizing the\ndiscontinuous Galerkin finite element method (DGFEM) in spatial dimensions.\nAdditionally, employing backward Euler and Crank Nicolson discretization in the\ntemporal domain and DGFEM in spatial dimensions, we introduce an estimator for\nthe fully discrete GBHE, taking into account the influence of past history. The\npaper also establishes optimal $L^2$ error estimates for both the stationary\nGBHE and GBHE. Ultimately, we validate the effectiveness of the proposed error\nestimator through numerical results, demonstrating its efficacy in an adaptive\nrefinement strategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.08269v2", "cate": "math.NA", "date": "2024-03-13", "updated": "2025-07-16"}
{"id": "2507.08218", "title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning", "authors": ["Atticus Wang", "Joshua Engels", "Oliver Clive-Griffin", "Senthooran Rajamanoharan", "Neel Nanda"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ICML 2025 Workshop R2-FM", "url": "http://arxiv.org/abs/2507.08218v2", "summary": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.", "comment": "ICML 2025 Workshop R2-FM", "pdf_url": "http://arxiv.org/pdf/2507.08218v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-16"}
{"id": "2507.11049", "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint. 24 pages", "url": "http://arxiv.org/abs/2507.11049v2", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.", "comment": "Preprint. 24 pages", "pdf_url": "http://arxiv.org/pdf/2507.11049v2", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2503.04695", "title": "A linearly-implicit energy-momentum preserving scheme for geometrically nonlinear mechanics based on non-canonical Hamiltonian formulations", "authors": ["Andrea Brugnoli", "Denis Matignon", "Joseph Morlier"], "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      34 pages, 22 figures", "url": "http://arxiv.org/abs/2503.04695v3", "summary": "This work presents a novel formulation and numerical strategy for the\nsimulation of geometrically nonlinear structures. First, a non-canonical\nHamiltonian (Poisson) formulation is introduced by including the dynamics of\nthe stress tensor. This framework is developed for von-K\\'arm\\'an\nnonlinearities in beams and plates, as well as geometrically nonlinear\nelasticity with Saint-Venant material behavior. In the case of plates, both\nnegligible and non-negligible membrane inertia are considered. For the former\ncase the two-dimensional elasticity complex is leveraged to express the\ndynamics in terms of the Airy stress function. The finite element\ndiscretization employs a mixed approach, combining a conforming approximation\nfor displacement and velocity fields with a discontinuous stress tensor\nrepresentation. A staggered, linear implicit time integration scheme is\nproposed, establishing connections with existing explicit-implicit\nenergy-preserving methods. The stress degrees of freedom are statically\ncondensed, reducing the computational complexity to solving a system with a\npositive definite matrix. The integration strategy preserves energy and angular\nmomentum exactly. The methodology is validated through numerical experiments on\nthe Duffing oscillator, a von-K\\'arm\\'an beam, and a column undergoing finite\ndeformations. Comparisons with fully implicit energy-preserving method and the\nleapfrog scheme demonstrate that the proposed approach achieves superior\naccuracy while maintaining energy stability. Additionally, it enables larger\ntime steps compared to explicit schemes and exhibits computational efficiency\ncomparable to the leapfrog method.", "comment": "34 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2503.04695v3", "cate": "math.NA", "date": "2025-03-06", "updated": "2025-07-16"}
{"id": "2507.09828", "title": "Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization", "authors": ["Shion Takeno", "Yu Inatsu", "Masayuki Karasuyama", "Ichiro Takeuchi"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      35pages, 5 figures, fix trivial errors", "url": "http://arxiv.org/abs/2507.09828v2", "summary": "Bayesian optimization is a powerful tool for optimizing an\nexpensive-to-evaluate black-box function. In particular, the effectiveness of\nexpected improvement (EI) has been demonstrated in a wide range of\napplications. However, theoretical analyses of EI are limited compared with\nother theoretically established algorithms. This paper analyzes a randomized\nvariant of EI, which evaluates the EI from the maximum of the posterior sample\npath. We show that this posterior sampling-based random EI achieves the\nsublinear Bayesian cumulative regret bounds under the assumption that the\nblack-box function follows a Gaussian process. Finally, we demonstrate the\neffectiveness of the proposed method through numerical experiments.", "comment": "35pages, 5 figures, fix trivial errors", "pdf_url": "http://arxiv.org/pdf/2507.09828v2", "cate": "stat.ML", "date": "2025-07-13", "updated": "2025-07-16"}
{"id": "2507.11423", "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "authors": ["Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Thierry Charnois"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11423v2", "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11423v2", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-16"}
{"id": "2501.14930", "title": "Moving-Boundary Port-Hamiltonian Systems", "authors": ["T. J. Meijer", "A. Das", "S. Weiland"], "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.14930v2", "summary": "In this paper, we consider linear boundary port-Hamiltonian distributed\nparameter systems on a time-varying spatial domain. We derive the specific\ntime-varying Dirac structure that these systems give rise to and use it to\nformally establish a new class of moving-boundary port-Hamiltonian systems by\nshowing that these distributed parameter systems on a time-varying spatial\ndomain admit a port-Hamiltonian representation. We demonstrate that our results\ncan be leveraged to develop a spatial discretization scheme with dynamic\nmeshing for approximating the telegrapher's equations on a time-varying spatial\ndomain, which we subsequently verify numerically.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.14930v2", "cate": "math.OC", "date": "2025-01-24", "updated": "2025-07-15"}
{"id": "2507.11381", "title": "From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies", "authors": ["Rom Gutman", "Shimon Sheiba", "Omer Noy Klein", "Naama Dekel Bird", "Amit Gruber", "Doron Aronson", "Oren Caspi", "Uri Shalit"], "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11381v2", "summary": "We propose a framework for building patient-specific treatment recommendation\nmodels, building on the large recent literature on learning patient-level\ncausal models and inspired by the target trial paradigm of Hernan and Robins.\nWe focus on safety and validity, including the crucial issue of causal\nidentification when using observational data. We do not provide a specific\nmodel, but rather a way to integrate existing methods and know-how into a\npractical pipeline. We further provide a real world use-case of treatment\noptimization for patients with heart failure who develop acute kidney injury\nduring hospitalization. The results suggest our pipeline can improve patient\noutcomes over the current treatment regime.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11381v2", "cate": "stat.ML", "date": "2025-07-15", "updated": "2025-07-16"}
