{"id": "2507.21060", "title": "Privacy-Preserving AI for Encrypted Medical Imaging: A Framework for Secure Diagnosis and Learning", "authors": ["Abdullah Al Siam", "Sadequzzaman Shohan"], "categories": ["cs.CR", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21060v1", "summary": "The rapid integration of Artificial Intelligence (AI) into medical\ndiagnostics has raised pressing concerns about patient privacy, especially when\nsensitive imaging data must be transferred, stored, or processed. In this\npaper, we propose a novel framework for privacy-preserving diagnostic inference\non encrypted medical images using a modified convolutional neural network\n(Masked-CNN) capable of operating on transformed or ciphered image formats. Our\napproach leverages AES-CBC encryption coupled with JPEG2000 compression to\nprotect medical images while maintaining their suitability for AI inference. We\nevaluate the system using public DICOM datasets (NIH ChestX-ray14 and\nLIDC-IDRI), focusing on diagnostic accuracy, inference latency, storage\nefficiency, and privacy leakage resistance. Experimental results show that the\nencrypted inference model achieves performance comparable to its unencrypted\ncounterpart, with only marginal trade-offs in accuracy and latency. The\nproposed framework bridges the gap between data privacy and clinical utility,\noffering a practical, scalable solution for secure AI-driven diagnostics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21060v1", "cate": "cs.CR", "date": "2025-05-17", "updated": "2025-05-17"}
{"id": "2507.21061", "title": "Security practices in AI development", "authors": ["Petr Spelda", "Vit Stritecky"], "categories": ["cs.CR", "cs.CY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.21061v1", "summary": "What makes safety claims about general purpose AI systems such as large\nlanguage models trustworthy? We show that rather than the capabilities of\nsecurity tools such as alignment and red teaming procedures, it is security\npractices based on these tools that contributed to reconfiguring the image of\nAI safety and made the claims acceptable. After showing what causes the gap\nbetween the capabilities of security tools and the desired safety guarantees,\nwe critically investigate how AI security practices attempt to fill the gap and\nidentify several shortcomings in diversity and participation. We found that\nthese security practices are part of securitization processes aiming to support\n(commercial) development of general purpose AI systems whose trustworthiness\ncan only be imperfectly tested instead of guaranteed. We conclude by offering\nseveral improvements to the current AI security practices.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.21061v1", "cate": "cs.CR", "date": "2025-05-17", "updated": "2025-05-17"}
{"id": "2507.21068", "title": "Private key and password protection by steganographic image encryption", "authors": ["Debesh Choudhury", "Sujoy Chakraborty"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures, Applications of Digital Image Processing XLV, SPIE Optical Engineering + Applications 2022, Proc. SPIE 12226,", "url": "http://arxiv.org/abs/2507.21068v1", "summary": "We propose a technique to protect and preserve a private key or a passcode in\nan encrypted two-dimensional graphical image. The plaintext private key or the\npasscode is converted into an encrypted QR code and embedded into a real-life\ncolor image with a steganographic scheme. The private key or the passcode is\nrecovered from the stego color image by first extracting the encrypted QR code\nfrom the color image, followed by decryption of the QR code. The cryptographic\nkey for encryption of the QR code is generated from the output of a Linear\nFeedback Shift Register (LFSR), initialized by a seed image chosen by the user.\nThe user can store the seed image securely, without the knowledge of an\nattacker. Even if an active attacker modifies the seed image (without knowledge\nof the fact that it is the seed image), the user can easily restore it if\nhe/she keeps multiple copies of it, so that the encryption key can be\nregenerated easily. Our experiments prove the feasibility of the technique\nusing sample private key data and real-life color images.", "comment": "5 pages, 3 figures, Applications of Digital Image Processing XLV,\n  SPIE Optical Engineering + Applications 2022, Proc. SPIE 12226,", "pdf_url": "http://arxiv.org/pdf/2507.21068v1", "cate": "cs.CR", "date": "2025-06-05", "updated": "2025-06-05"}
{"id": "2507.21085", "title": "Applications Of Zero-Knowledge Proofs On Bitcoin", "authors": ["Yusuf Ozmiş"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21085v1", "summary": "This paper explores how zero-knowledge proofs can enhance Bitcoin's\nfunctionality and privacy. First, we consider Proof-of-Reserve schemes: by\nusing zk-STARKs, a custodian can prove its Bitcoin holdings are more than a\npredefined threshold X, without revealing addresses or actual balances. We\noutline a STARK-based protocol for Bitcoin UTXOs and discuss its efficiency.\nSecond, we examine ZK Light Clients, where a mobile or lightweight device\nverifies Bitcoin's proof-of-work chain using succinct proofs. We propose a\nprotocol for generating and verifying a STARK-based proof of a chain of block\nheaders, enabling trust-minimized client operation. Third, we explore\nPrivacy-Preserving Rollups via BitVM: leveraging BitVM, we design a conceptual\nrollup that keeps transaction data confidential using zero-knowledge proofs. In\neach case, we analyze security, compare with existing approaches, and discuss\nimplementation considerations. Our contributions include the design of concrete\nprotocols adapted to Bitcoin's UTXO model and an assessment of their\npracticality. The results suggest that while ZK proofs can bring powerful\nfeatures (e.g., on-chain reserve audits, trustless light clients, and private\nlayer-2 execution) to Bitcoin, each application requires careful trade-offs in\nefficiency and trust assumptions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21085v1", "cate": "cs.CR", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.21087", "title": "Intelligent ARP Spoofing Detection using Multi-layered Machine Learning (ML) Techniques for IoT Networks", "authors": ["Anas Ali", "Mubashar Husain", "Peter Hans"], "categories": ["cs.CR", "cs.CY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21087v1", "summary": "Address Resolution Protocol (ARP) spoofing remains a critical threat to IoT\nnetworks, enabling attackers to intercept, modify, or disrupt data transmission\nby exploiting ARP's lack of authentication. The decentralized and\nresource-constrained nature of IoT environments amplifies this vulnerability,\nmaking conventional detection mechanisms ineffective at scale. This paper\nintroduces an intelligent, multi-layered machine learning framework designed to\ndetect ARP spoofing in real-time IoT deployments. Our approach combines feature\nengineering based on ARP header behavior, traffic flow analysis, and temporal\npacket anomalies with a hybrid detection pipeline incorporating decision trees,\nensemble models, and deep learning classifiers. We propose a hierarchical\narchitecture to prioritize lightweight models at edge gateways and deeper\nmodels at centralized nodes to balance detection accuracy and computational\nefficiency. The system is validated on both simulated IoT traffic and the\nCICIDS2017 dataset, achieving over 97% detection accuracy with low false\npositive rates. Comparative evaluations with signature-based and rule-based\nsystems demonstrate the robustness and generalizability of our approach. Our\nresults show that intelligent machine learning integration enables proactive\nARP spoofing detection tailored for IoT scenarios, laying the groundwork for\nscalable and autonomous network security solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21087v1", "cate": "cs.CR", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.21092", "title": "The Discovery, Disclosure, and Investigation of CVE-2024-25825", "authors": ["Hunter Chasens"], "categories": ["cs.CR", "D.4.6; K.6.5"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      97 pages, BSc thesis", "url": "http://arxiv.org/abs/2507.21092v1", "summary": "CVE-2024-25825 is a vulnerability found in FydeOS. This thesis describes its\ndiscovery, disclosure, and its further investigation in connection to a nation\nstate actor. The vulnerability is CWE-1392: Use of Default Credentials,\nCWE-1393: Use of Default Password, and CWE-258: Empty Password in Configuration\nFile found in the /etc/shadow configuration file. The root users entry in the\n/etc/shadow file contains a wildcard allowing entry with any, or no, password.\nFollowing responsable disclosure, Fyde, CISA, and Mitre were informed. Fyde was\nalready aware of the vulnerability. There was concern that this vulnerability\nmight have been purposefully placed, perhaps by a nation state actor. After\nfurther investigation, it appears that this is unlikely to be the case. In\ncases in which poisoned code is suspected it might be prudent to contact the\nappropriate CERT, rather than the parent company. This, however, clashes with\nthe typical teaching of responsable disclosure.", "comment": "97 pages, BSc thesis", "pdf_url": "http://arxiv.org/pdf/2507.21092v1", "cate": "cs.CR", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.21094", "title": "SkyEye: When Your Vision Reaches Beyond IAM Boundary Scope in AWS Cloud", "authors": ["Minh Hoang Nguyen", "Anh Minh Ho", "Bao Son To"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21094v1", "summary": "In recent years, cloud security has emerged as a primary concern for\nenterprises due to the increasing trend of migrating internal infrastructure\nand applications to cloud environments. This shift is driven by the desire to\nreduce the high costs and maintenance fees associated with traditional\non-premise infrastructure. By leveraging cloud capacities such as high\navailability and scalability, companies can achieve greater operational\nefficiency and flexibility. However, this migration also introduces new\nsecurity challenges. Ensuring the protection of sensitive data, maintaining\ncompliance with regulatory requirements, and mitigating the risks of cyber\nthreats are critical issues that must be addressed. Identity and Access\nManagement (IAM) constitutes the critical security backbone of most cloud\ndeployments, particularly within AWS environments. As organizations adopt AWS\nto scale applications and store data, the need for a thorough, methodical, and\nprecise enumeration of IAM configurations grows exponentially. Enumeration\nrefers to the systematic mapping and interrogation of identities, permissions,\nand resource authorizations with the objective of gaining situational\nawareness. By understanding the interplay between users, groups, and their\nmyriads of policies, whether inline or attached managed policies, security\nprofessionals need to enumerate and identify misconfigurations, reduce the risk\nof unauthorized privilege escalation, and maintain robust compliance postures.\nThis paper will present SkyEye, a cooperative multi-principal IAM enumeration\nframework, which comprises cutting-edge enumeration models in supporting\ncomplete situational awareness regarding the IAMs of provided AWS credentials,\ncrossing the boundary of principal-specific IAM entitlement vision to reveal\nthe complete visionary while insufficient authorization is the main challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21094v1", "cate": "cs.CR", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.21096", "title": "HexaMorphHash HMH- Homomorphic Hashing for Secure and Efficient Cryptographic Operations in Data Integrity Verification", "authors": ["Krishnendu Das"], "categories": ["cs.CR", "cs.DB"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21096v1", "summary": "In the realm of big data and cloud computing, distributed systems are tasked\nwith proficiently managing, storing, and validating extensive datasets across\nnumerous nodes, all while maintaining robust data integrity. Conventional\nhashing methods, though straightforward, encounter substan tial difficulties in\ndynamic settings due to the necessity for thorough rehashing when nodes are\naltered. Consistent hashing mitigates some of these challenges by reducing data\nredistribution; however, it still contends with limitations in load balancing\nand scalability under intensive update conditions. This paper introduces an\ninnovative approach using a lattice based homomorphic hash function\nHexaMorphHash that facilitates constant time, incremental updates while\npreserving a constant digest size. By utilizing the complexity of the Short\nInteger Solutions SIS problem, our method secures strong protective measures,\neven against quantum threats. We further com pare our method with existing ones\nsuch as direct signatures for each update, comprehensive database signing,\nMerkle tree based techniques, AdHash, MuHash, ECMH, and homomorphic sig nature\nschemes highlighting notable advancements in computational efficiency, memory\nusage, and scalability. Our contributions present a viable solution for\nfrequent update dissemination in expansive distributed systems, safeguarding\nboth data integrity and system performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21096v1", "cate": "cs.CR", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.21097", "title": "Singularity Cipher: A Topology-Driven Cryptographic Scheme Based on Visual Paradox and Klein Bottle Illusions", "authors": ["Abraham Itzhak Weinberg"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21097v1", "summary": "This paper presents the Singularity Cipher, a novel\ncryptographic-steganographic framework that integrates topological\ntransformations and visual paradoxes to achieve multidimensional security.\nInspired by the non-orientable properties of the Klein bottle -- constructed\nfrom two Mobius strips -- the cipher applies symbolic twist functions to\nsimulate topological traversal, producing high confusion and diffusion in the\nciphertext. The resulting binary data is then encoded using perceptual\nillusions, such as the missing square paradox, to visually obscure the presence\nof encrypted content. Unlike conventional ciphers that rely solely on algebraic\ncomplexity, the Singularity Cipher introduces a dual-layer approach: symbolic\nencryption rooted in topology and visual steganography designed for human\ncognitive ambiguity. This combination enhances both cryptographic strength and\ndetection resistance, making it well-suited for secure communication,\nwatermarking, and plausible deniability in adversarial environments. The paper\nformalizes the architecture, provides encryption and decryption algorithms,\nevaluates security properties, and compares the method against classical,\npost-quantum, and steganographic approaches. Potential applications and future\nresearch directions are also discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21097v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2507.21101", "title": "SoK: A Systematic Review of Context- and Behavior-Aware Adaptive Authentication in Mobile Environments", "authors": ["Vyoma Harshitha Podapati", "Divyansh Nigam", "Sanchari Das"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21101v1", "summary": "As mobile computing becomes central to digital interaction, researchers have\nturned their attention to adaptive authentication for its real-time, context-\nand behavior-aware verification capabilities. However, many implementations\nremain fragmented, inconsistently apply intelligent techniques, and fall short\nof user expectations. In this Systematization of Knowledge (SoK), we analyze 41\npeer-reviewed studies since 2011 that focus on adaptive authentication in\nmobile environments. Our analysis spans seven dimensions: privacy and security\nmodels, interaction modalities, user behavior, risk perception, implementation\nchallenges, usability needs, and machine learning frameworks. Our findings\nreveal a strong reliance on machine learning (64.3%), especially for continuous\nauthentication (61.9%) and unauthorized access prevention (54.8%). AI-driven\napproaches such as anomaly detection (57.1%) and spatio-temporal analysis\n(52.4%) increasingly shape the interaction landscape, alongside growing use of\nsensor-based and location-aware models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21101v1", "cate": "cs.CR", "date": "2025-07-06", "updated": "2025-07-06"}
{"id": "2507.21111", "title": "A Formal Rebuttal of \"The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability\"", "authors": ["Craig Wright"], "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.GT", "cs.SE", "68M14, 68P25, 68T30, 94A60, 18C10", "D.4.6; K.6.5; C.2.1; H.3.5; D.2.11"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      79 pages; A response and rebuttal of [Mssassi, Souhail, and Anas Abou El Kalam. \"The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability.\" Applied Sciences 15, no. 1 (2024): 19. this https URL .]", "url": "http://arxiv.org/abs/2507.21111v1", "summary": "This paper presents a comprehensive refutation of the so-called \"blockchain\ntrilemma,\" a widely cited but formally ungrounded claim asserting an inherent\ntrade-off between decentralisation, security, and scalability in blockchain\nprotocols. Through formal analysis, empirical evidence, and detailed critique\nof both methodology and terminology, we demonstrate that the trilemma rests on\nsemantic equivocation, misuse of distributed systems theory, and a failure to\ndefine operational metrics. Particular focus is placed on the conflation of\ntopological network analogies with protocol-level architecture, the\nmischaracterisation of Bitcoin's design--including the role of miners, SPV\nclients, and header-based verification--and the failure to ground claims in\ncomplexity-theoretic or adversarial models. By reconstructing Bitcoin as a\ndeterministic, stateless distribution protocol governed by evidentiary trust,\nwe show that scalability is not a trade-off but an engineering outcome. The\npaper concludes by identifying systemic issues in academic discourse and peer\nreview that have allowed such fallacies to persist, and offers formal criteria\nfor evaluating future claims in blockchain research.", "comment": "79 pages; A response and rebuttal of [Mssassi, Souhail, and Anas Abou\n  El Kalam. \"The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs\n  Among Decentralization, Security, and Scalability.\" Applied Sciences 15, no.\n  1 (2024): 19. https://doi.org/10.3390/app15010019.]", "pdf_url": "http://arxiv.org/pdf/2507.21111v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.21113", "title": "Vulnerability Mitigation System (VMS): LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Farzana Abdulzada"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21113v1", "summary": "As the frequency of cyber threats increases, conventional penetration testing\nis failing to capture the entirety of todays complex environments. To solve\nthis problem, we propose the Vulnerability Mitigation System (VMS), a novel\nagent based on a Large Language Model (LLM) capable of performing penetration\ntesting without human intervention. The VMS has a two-part architecture for\nplanning and a Summarizer, which enable it to generate commands and process\nfeedback. To standardize testing, we designed two new Capture the Flag (CTF)\nbenchmarks based on the PicoCTF and OverTheWire platforms with 200 challenges.\nThese benchmarks allow us to evaluate how effectively the system functions. We\nperformed a number of experiments using various LLMs while tuning the\ntemperature and top-p parameters and found that GPT-4o performed best,\nsometimes even better than expected. The results indicate that LLMs can be\neffectively applied to many cybersecurity tasks; however, there are risks. To\nensure safe operation, we used a containerized environment. Both the VMS and\nthe benchmarks are publicly available, advancing the creation of secure,\nautonomous cybersecurity tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21113v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.21122", "title": "Kintsugi: Decentralized E2EE Key Recovery", "authors": ["Emilie Ma", "Martin Kleppmann"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      15 pages with 4 additional pages of workshop discussion transcript. To be published in the proceedings of the Twenty-ninth International Workshop on Security Protocols", "url": "http://arxiv.org/abs/2507.21122v1", "summary": "Kintsugi is a protocol for key recovery, allowing a user to regain access to\nend-to-end encrypted data after they have lost their device, but still have\ntheir (potentially low-entropy) password. Existing E2EE key recovery methods,\nsuch as those deployed by Signal and WhatsApp, centralize trust by relying on\nservers administered by a single provider. Kintsugi is decentralized,\ndistributing trust over multiple recovery nodes, which could be servers run by\nindependent parties, or end user devices in a peer-to-peer setting. To recover\na user's keys, a threshold $t+1$ of recovery nodes must assist the user in\ndecrypting a shared backup. Kintsugi is password-authenticated and protects\nagainst offline brute-force password guessing without requiring any specialized\nsecure hardware. Kintsugi can tolerate up to $t$ honest-but-curious colluding\nrecovery nodes, as well as $n - t - 1$ offline nodes, and operates safely in an\nasynchronous network model where messages can be arbitrarily delayed.", "comment": "15 pages with 4 additional pages of workshop discussion transcript.\n  To be published in the proceedings of the Twenty-ninth International Workshop\n  on Security Protocols", "pdf_url": "http://arxiv.org/pdf/2507.21122v1", "cate": "cs.CR", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.21128", "title": "Security study based on the Chatgptplugin system: ldentifying Security Vulnerabilities", "authors": ["Ruomai Ren"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Master's thesis", "url": "http://arxiv.org/abs/2507.21128v1", "summary": "Plugin systems are a class of external programmes that provide users with a\nwide range of functionality, and while they enhance the user experience, their\nsecurity is always a challenge. Especially due to the diversity and complexity\nof developers, many plugin systems lack adequate regulation. As ChatGPT has\nbecome a popular large-scale language modelling platform, its plugin system is\nalso gradually developing, and the open platform provides creators with the\nopportunity to upload plugins covering a wide range of application scenarios.\nHowever, current research and discussions mostly focus on the security issues\nof the ChatGPT model itself, while ignoring the possible security risks posed\nby the plugin system. This study aims to analyse the security of plugins in the\nChatGPT plugin shop, reveal its major security vulnerabilities, and propose\ncorresponding improvements.", "comment": "Master's thesis", "pdf_url": "http://arxiv.org/pdf/2507.21128v1", "cate": "cs.CR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.21341", "title": "Replicating the behaviour of electric vehicle drivers using an agent-based reinforcement learning model", "authors": ["Zixin Feng", "Qunshan Zhao", "Alison Heppenstall"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21341v1", "summary": "Despite the rapid expansion of electric vehicle (EV) charging networks,\nquestions remain about their efficiency in meeting the growing needs of EV\ndrivers. Previous simulation-based approaches, which rely on static behavioural\nrules, have struggled to capture the adaptive behaviours of human drivers.\nAlthough reinforcement learning has been introduced in EV simulation studies,\nits application has primarily focused on optimising fleet operations rather\nthan modelling private drivers who make independent charging decisions.\nAdditionally, long-distance travel remains a primary concern for EV drivers.\nHowever, existing simulation studies rarely explore charging behaviour over\nlarge geographical scales. To address these gaps, we propose a multi-stage\nreinforcement learning framework that simulates EV charging demand across large\ngeographical areas. We validate the model against real-world data, and identify\nthe training stage that most closely reflects actual driver behaviour, which\ncaptures both the adaptive behaviours and bounded rationality of private\ndrivers. Based on the simulation results, we also identify critical 'charging\ndeserts' where EV drivers consistently have low state of charge. Our findings\nalso highlight recent policy shifts toward expanding rapid charging hubs along\nmotorway corridors and city boundaries to meet the demand from long-distance\ntrips.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21341v1", "cate": "cs.MA", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21133", "title": "Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities", "authors": ["Atil Samancioglu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21133v1", "summary": "Large Language Models (LLMs) demonstrate complex responses to threat-based\nmanipulations, revealing both vulnerabilities and unexpected performance\nenhancement opportunities. This study presents a comprehensive analysis of\n3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini)\nacross 10 task domains under 6 threat conditions. We introduce a novel threat\ntaxonomy and multi-metric evaluation framework to quantify both negative\nmanipulation effects and positive performance improvements. Results reveal\nsystematic vulnerabilities, with policy evaluation showing the highest metric\nsignificance rates under role-based threats, alongside substantial performance\nenhancements in numerous cases with effect sizes up to +1336%. Statistical\nanalysis indicates systematic certainty manipulation (pFDR < 0.0001) and\nsignificant improvements in analytical depth and response quality. These\nfindings have dual implications for AI safety and practical prompt engineering\nin high-stakes applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21133v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21724", "title": "Agent-Based Exploration of Recommendation Systems in Misinformation Propagation", "authors": ["Lise Jakobsen", "Anna Johanne Holden", "Önder Gürcan", "Özlem Özgöbek"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      14 pages, 3 figures, Social Simulation Conference 2025 (SSC'2025)", "url": "http://arxiv.org/abs/2507.21724v1", "summary": "This study uses agent-based modeling to examine the impact of various\nrecommendation algorithms on the propagation of misinformation on online social\nnetworks. We simulate a synthetic environment consisting of heterogeneous\nagents, including regular users, bots, and influencers, interacting through a\nsocial network with recommendation systems. We evaluate four recommendation\nstrategies: popularity-based, collaborative filtering, and content-based\nfiltering, along with a random baseline. Our results show that\npopularity-driven algorithms significantly amplify misinformation, while\nitem-based collaborative filtering and content-based approaches are more\neffective in limiting exposure to fake content. Item-based collaborative\nfiltering was found to perform better than previously reported in related\nliterature. These findings highlight the role of algorithm design in shaping\nonline information exposure and show that agent-based modeling can be used to\ngain realistic insight into how misinformation spreads.", "comment": "14 pages, 3 figures, Social Simulation Conference 2025 (SSC'2025)", "pdf_url": "http://arxiv.org/pdf/2507.21724v1", "cate": "cs.MA", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21139", "title": "Learning-based Privacy-Preserving Graph Publishing Against Sensitive Link Inference Attacks", "authors": ["Yucheng Wu", "Yuncong Yang", "Xiao Han", "Leye Wang", "Junjie Wu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21139v1", "summary": "Publishing graph data is widely desired to enable a variety of structural\nanalyses and downstream tasks. However, it also potentially poses severe\nprivacy leakage, as attackers may leverage the released graph data to launch\nattacks and precisely infer private information such as the existence of hidden\nsensitive links in the graph. Prior studies on privacy-preserving graph data\npublishing relied on heuristic graph modification strategies and it is\ndifficult to determine the graph with the optimal privacy--utility trade-off\nfor publishing. In contrast, we propose the first privacy-preserving graph\nstructure learning framework against sensitive link inference attacks, named\nPPGSL, which can automatically learn a graph with the optimal privacy--utility\ntrade-off. The PPGSL operates by first simulating a powerful surrogate attacker\nconducting sensitive link attacks on a given graph. It then trains a\nparameterized graph to defend against the simulated adversarial attacks while\nmaintaining the favorable utility of the original graph. To learn the\nparameters of both parts of the PPGSL, we introduce a secure iterative training\nprotocol. It can enhance privacy preservation and ensure stable convergence\nduring the training process, as supported by the theoretical proof.\nAdditionally, we incorporate multiple acceleration techniques to improve the\nefficiency of the PPGSL in handling large-scale graphs. The experimental\nresults confirm that the PPGSL achieves state-of-the-art privacy--utility\ntrade-off performance and effectively thwarts various sensitive link inference\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21139v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21969", "title": "Towards Cognitive Synergy in LLM-Based Multi-Agent Systems: Integrating Theory of Mind and Critical Evaluation", "authors": ["Adam Kostka", "Jarosław A. Chudziak"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      Accepted at CogSci 2025", "url": "http://arxiv.org/abs/2507.21969v1", "summary": "Recently, the field of Multi-Agent Systems (MAS) has gained popularity as\nresearchers are trying to develop artificial intelligence capable of efficient\ncollective reasoning. Agents based on Large Language Models (LLMs) perform well\nin isolated tasks, yet struggle with higher-order cognition required for\nadaptive collaboration. Human teams achieve synergy not only through knowledge\nsharing, but also through recursive reasoning, structured critique, and the\nability to infer others' mental states. Current artificial systems lack these\nessential mechanisms, limiting their ability to engage in sophisticated\ncollective reasoning. This work explores cognitive processes that enable\neffective collaboration, focusing on adaptive theory of mind (ToM) and\nsystematic critical evaluation. We investigate three key questions. First, how\ndoes the ability to model others' perspectives enhance coordination and reduce\nredundant reasoning? Second, to what extent does structured critique improve\nreasoning quality by identifying logical gaps and mitigating biases? Third, the\ninterplay of these mechanisms can lead to emergent cognitive synergy, where the\ncollective intelligence of the system exceeds the sum of its parts. Through an\nempirical case study on complex decision making, we show that the integration\nof these cognitive mechanisms leads to more coherent, adaptive, and rigorous\nagent interactions. This article contributes to the field of cognitive science\nand AI research by presenting a structured framework that emulates human-like\ncollaborative reasoning MAS. It highlights the significance of dynamic ToM and\ncritical evaluation in advancing multi-agent systems' ability to tackle\ncomplex, real-world challenges.", "comment": "Accepted at CogSci 2025", "pdf_url": "http://arxiv.org/pdf/2507.21969v1", "cate": "cs.MA", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21142", "title": "Privacy Artifact ConnecTor (PACT): Embedding Enterprise Artifacts for Compliance AI Agents", "authors": ["Chenhao Fang", "Yanqing Peng", "Rajeev Rao", "Matt Sarmiento", "Wendy Summer", "Arya Pudota", "Alex Goncalves", "Jordi Mola", "Hervé Robert"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21142v1", "summary": "Enterprise environments contain a heterogeneous, rapidly growing collection\nof internal artifacts related to code, data, and many different tools. Critical\ninformation for assessing privacy risk and ensuring regulatory compliance is\noften embedded across these varied resources, each with their own arcane\ndiscovery and extraction techniques. Therefore, large-scale privacy compliance\nin adherence to governmental regulations requires systems to discern the\ninterconnected nature of diverse artifacts in a common, shared universe.\n  We present Privacy Artifact ConnecT or (PACT), an embeddings-driven graph\nthat links millions of artifacts spanning multiple artifact types generated by\na variety of teams and projects. Powered by the state-of-the-art DRAGON\nembedding model, PACT uses a contrastive learning objective with light\nfine-tuning to link artifacts via their textual components such as raw\nmetadata, ownership specifics, and compliance context. Experimental results\nshow that PACT's fine-tuned model improves recall@1 from 18% to 53%, the query\nmatch rate from 9.6% to 69.7% when paired with a baseline AI agent, and the\nhitrate@1 from 25.7% to 44.9% for candidate selection in a standard recommender\nsystem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21142v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.22049", "title": "Validating Generative Agent-Based Models of Social Norm Enforcement: From Replication to Novel Predictions", "authors": ["Logan Cross", "Nick Haber", "Daniel L. K. Yamins"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22049v1", "summary": "As large language models (LLMs) advance, there is growing interest in using\nthem to simulate human social behavior through generative agent-based modeling\n(GABM). However, validating these models remains a key challenge. We present a\nsystematic two-stage validation approach using social dilemma paradigms from\npsychological literature, first identifying the cognitive components necessary\nfor LLM agents to reproduce known human behaviors in mixed-motive settings from\ntwo landmark papers, then using the validated architecture to simulate novel\nconditions. Our model comparison of different cognitive architectures shows\nthat both persona-based individual differences and theory of mind capabilities\nare essential for replicating third-party punishment (TPP) as a costly signal\nof trustworthiness. For the second study on public goods games, this\narchitecture is able to replicate an increase in cooperation from the spread of\nreputational information through gossip. However, an additional strategic\ncomponent is necessary to replicate the additional boost in cooperation rates\nin the condition that allows both ostracism and gossip. We then test novel\npredictions for each paper with our validated generative agents. We find that\nTPP rates significantly drop in settings where punishment is anonymous, yet a\nsubstantial amount of TPP persists, suggesting that both reputational and\nintrinsic moral motivations play a role in this behavior. For the second paper,\nwe introduce a novel intervention and see that open discussion periods before\nrounds of the public goods game further increase contributions, allowing groups\nto develop social norms for cooperation. This work provides a framework for\nvalidating generative agent models while demonstrating their potential to\ngenerate novel and testable insights into human social behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22049v1", "cate": "cs.MA", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21225", "title": "Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors", "authors": ["Annan Zhang", "Miguel Flores-Acton", "Andy Yu", "Anshul Gupta", "Maggie Yao", "Daniela Rus"], "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the proceedings of the 2025 International Symposium on Experimental Robotics (ISER)", "url": "http://arxiv.org/abs/2507.21225v1", "summary": "Tactile sensing plays a fundamental role in enabling robots to navigate\ndynamic and unstructured environments, particularly in applications such as\ndelicate object manipulation, surface exploration, and human-robot interaction.\nIn this paper, we introduce a passive soft robotic fingertip with integrated\ntactile sensing, fabricated using a 3D-printed elastomer lattice with embedded\nair channels. This sensorization approach, termed fluidic innervation,\ntransforms the lattice into a tactile sensor by detecting pressure changes\nwithin sealed air channels, providing a simple yet robust solution to tactile\nsensing in robotics. Unlike conventional methods that rely on complex materials\nor designs, fluidic innervation offers a simple, scalable, single-material\nfabrication process. We characterize the sensors' response, develop a geometric\nmodel to estimate tip displacement, and train a neural network to accurately\npredict contact location and contact force. Additionally, we integrate the\nfingertip with an admittance controller to emulate spring-like behavior,\ndemonstrate its capability for environment exploration through tactile\nfeedback, and validate its durability under high impact and cyclic loading\nconditions. This tactile sensing technique offers advantages in terms of\nsimplicity, adaptability, and durability and opens up new opportunities for\nversatile robotic manipulation.", "comment": "Accepted for publication in the proceedings of the 2025 International\n  Symposium on Experimental Robotics (ISER)", "pdf_url": "http://arxiv.org/pdf/2507.21225v1", "cate": "cs.RO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21145", "title": "Leveraging Trustworthy AI for Automotive Security in Multi-Domain Operations: Towards a Responsive Human-AI Multi-Domain Task Force for Cyber Social Security", "authors": ["Vita Santa Barletta", "Danilo Caivano", "Gabriel Cellammare", "Samuele del Vescovo", "Annita Larissa Sciacovelli"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures, 1 table", "url": "http://arxiv.org/abs/2507.21145v1", "summary": "Multi-Domain Operations (MDOs) emphasize cross-domain defense against complex\nand synergistic threats, with civilian infrastructures like smart cities and\nConnected Autonomous Vehicles (CAVs) emerging as primary targets. As dual-use\nassets, CAVs are vulnerable to Multi-Surface Threats (MSTs), particularly from\nAdversarial Machine Learning (AML) which can simultaneously compromise multiple\nin-vehicle ML systems (e.g., Intrusion Detection Systems, Traffic Sign\nRecognition Systems). Therefore, this study investigates how key\nhyperparameters in Decision Tree-based ensemble models-Random Forest (RF),\nGradient Boosting (GB), and Extreme Gradient Boosting (XGB)-affect the time\nrequired for a Black-Box AML attack i.e. Zeroth Order Optimization (ZOO).\nFindings show that parameters like the number of trees or boosting rounds\nsignificantly influence attack execution time, with RF and GB being more\nsensitive than XGB. Adversarial Training (AT) time is also analyzed to assess\nthe attacker's window of opportunity. By optimizing hyperparameters, this\nresearch supports Defensive Trustworthy AI (D-TAI) practices within MST\nscenarios and contributes to the development of resilient ML systems for\ncivilian and military domains, aligned with Cyber Social Security framework in\nMDOs and Human-AI Multi-Domain Task Forces.", "comment": "13 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.21145v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21075", "title": "Can LLMs Reason About Trust?: A Pilot Study", "authors": ["Anushka Debnath", "Stephen Cranefield", "Emiliano Lorini", "Bastin Tony Roy Savarimuthu"], "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      17 pages, 5 figures, 3 tables Accepted for presentation as a full paper at the COINE 2025 workshop at AAMAS 2025 see this https URL", "url": "http://arxiv.org/abs/2507.21075v1", "summary": "In human society, trust is an essential component of social attitude that\nhelps build and maintain long-term, healthy relationships which creates a\nstrong foundation for cooperation, enabling individuals to work together\neffectively and achieve shared goals. As many human interactions occur through\nelectronic means such as using mobile apps, the potential arises for AI systems\nto assist users in understanding the social state of their relationships. In\nthis paper we investigate the ability of Large Language Models (LLMs) to reason\nabout trust between two individuals in an environment which requires fostering\ntrust relationships. We also assess whether LLMs are capable of inducing trust\nby role-playing one party in a trust based interaction and planning actions\nwhich can instil trust.", "comment": "17 pages, 5 figures, 3 tables Accepted for presentation as a full\n  paper at the COINE 2025 workshop at AAMAS 2025 see\n  https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html", "pdf_url": "http://arxiv.org/pdf/2507.21075v1", "cate": "cs.HC", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.21245", "title": "Diffusion Denoiser-Aided Gyrocompassing", "authors": ["Gershy Ben-Arie", "Daniel Engelsman", "Rotem Dror", "Itzik Klein"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures", "url": "http://arxiv.org/abs/2507.21245v1", "summary": "An accurate initial heading angle is essential for efficient and safe\nnavigation across diverse domains. Unlike magnetometers, gyroscopes can provide\naccurate heading reference independent of the magnetic disturbances in a\nprocess known as gyrocompassing. Yet, accurate and timely gyrocompassing, using\nlow-cost gyroscopes, remains a significant challenge in scenarios where\nexternal navigation aids are unavailable. Such challenges are commonly\naddressed in real-world applications such as autonomous vehicles, where size,\nweight, and power limitations restrict sensor quality, and noisy measurements\nseverely degrade gyrocompassing performance. To cope with this challenge, we\npropose a novel diffusion denoiser-aided gyrocompass approach. It integrates a\ndiffusion-based denoising framework with an enhanced learning-based heading\nestimation model. The diffusion denoiser processes raw inertial sensor signals\nbefore input to the deep learning model, resulting in accurate gyrocompassing.\nExperiments using both simulated and real sensor data demonstrate that our\nproposed approach improves gyrocompassing accuracy by 26% compared to\nmodel-based gyrocompassing and by 15% compared to other learning-driven\napproaches. This advancement holds particular significance for ensuring\naccurate and robust navigation in autonomous platforms that incorporate\nlow-cost gyroscopes within their navigation systems.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.21245v1", "cate": "cs.RO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21146", "title": "Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems", "authors": ["Gauri Sharma", "Vidhi Kulkarni", "Miles King", "Ken Huang"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.21146v1", "summary": "Evolving AI systems increasingly deploy multi-agent architectures where\nautonomous agents collaborate, share information, and delegate tasks through\ndeveloping protocols. This connectivity, while powerful, introduces novel\nsecurity risks. One such risk is a cascading risk: a breach in one agent can\ncascade through the system, compromising others by exploiting inter-agent\ntrust. In tandem with OWASP's initiative for an Agentic AI Vulnerability\nScoring System we define an attack vector, Agent Cascading Injection, analogous\nto Agent Impact Chain and Blast Radius, operating across networks of agents. In\nan ACI attack, a malicious input or tool exploit injected at one agent leads to\ncascading compromises and amplified downstream effects across agents that trust\nits outputs. We formalize this attack with an adversarial goal equation and key\nvariables (compromised agent, injected exploit, polluted observations, etc.),\ncapturing how a localized vulnerability can escalate into system-wide failure.\nWe then analyze ACI's properties -- propagation chains, amplification factors,\nand inter-agent compound effects -- and map these to OWASP's emerging Agentic\nAI risk categories (e.g. Impact Chain and Orchestration Exploits). Finally, we\nargue that ACI highlights a critical need for quantitative benchmarking\nframeworks to evaluate the security of agent-to-agent communication protocols.\nWe outline a methodology for stress-testing multi-agent systems (using\narchitectures such as Google's A2A and Anthropic's MCP) against cascading trust\nfailures, developing upon groundwork for measurable, standardized\nagent-to-agent security evaluation. Our work provides the necessary apparatus\nfor engineers to benchmark system resilience, make data-driven architectural\ntrade-offs, and develop robust defenses against a new generation of agentic\nthreats.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.21146v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21159", "title": "Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity", "authors": ["Zhihao Peng", "Liuxin Bao", "Shengyuan Liu", "Yixuan Yuan"], "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21159v1", "summary": "The collaborativeness of large language models (LLMs) has proven effective in\nnatural language processing systems, holding considerable promise for\nhealthcare development. However, it lacks explicit component selection rules,\nnecessitating human intervention or clinical-specific validation. Moreover,\nexisting architectures heavily rely on a predefined LLM cluster, where partial\nLLMs underperform in medical decision support scenarios, invalidating the\ncollaborativeness of LLMs. To this end, we propose an adaptive cluster\ncollaborativeness methodology involving self-diversity and cross-consistency\nmaximization mechanisms to boost LLMs medical decision support capacity. For\nthe self-diversity, we calculate the fuzzy matching value of pairwise outputs\nwithin an LLM as its self-diversity value, subsequently prioritizing LLMs with\nhigh self-diversity values as cluster components in a training-free manner. For\nthe cross-consistency, we first measure cross-consistency values between the\nLLM with the highest self-diversity value and others, and then gradually mask\nout the LLM having the lowest cross-consistency value to eliminate the\npotential inconsistent output during the collaborative propagation. Extensive\nexperiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health,\ndemonstrate the effectiveness of our method across physician-oriented\nspecialties. For example, on NEJMQA, our method achieves the accuracy rate up\nto the publicly official passing score across all disciplines, especially\nachieving ACC of 65.47\\% compared to the 56.12\\% achieved by GPT-4 on the\nObstetrics and Gynecology discipline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21159v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21259", "title": "NMPCM: Nonlinear Model Predictive Control on Resource-Constrained Microcontrollers", "authors": ["Van Chung Nguyen", "Pratik Walunj", "Chuong Le", "An Duy Nguyen", "Hung Manh La"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21259v1", "summary": "Nonlinear Model Predictive Control (NMPC) is a powerful approach for\ncontrolling highly dynamic robotic systems, as it accounts for system dynamics\nand optimizes control inputs at each step. However, its high computational\ncomplexity makes implementation on resource-constrained microcontrollers\nimpractical. While recent studies have demonstrated the feasibility of Model\nPredictive Control (MPC) with linearized dynamics on microcontrollers, applying\nfull NMPC remains a significant challenge. This work presents an efficient\nsolution for generating and deploying NMPC on microcontrollers (NMPCM) to\ncontrol quadrotor UAVs. The proposed method optimizes computational efficiency\nwhile maintaining high control accuracy. Simulations in Gazebo/ROS and\nreal-world experiments validate the effectiveness of the approach,\ndemonstrating its capability to achieve high-frequency NMPC execution in\nreal-time systems. The code is available at:\nhttps://github.com/aralab-unr/NMPCM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21259v1", "cate": "cs.RO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21150", "title": "WaveVerify: A Novel Audio Watermarking Framework for Media Authentication and Combatting Deepfakes", "authors": ["Aditya Pujari", "Ajita Rattani"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to IJCB 2025 (IEEE/IAPR International Joint Conference on Biometrics). Code available at: (1) Official Lab Repo: this https URL (2) Original Author Repo: this https URL", "url": "http://arxiv.org/abs/2507.21150v1", "summary": "The rapid advancement of voice generation technologies has enabled the\nsynthesis of speech that is perceptually indistinguishable from genuine human\nvoices. While these innovations facilitate beneficial applications such as\npersonalized text-to-speech systems and voice preservation, they have also\nintroduced significant risks, including deepfake impersonation scams and\nsynthetic media-driven disinformation campaigns. Recent reports indicate that\nin 2024, deepfake fraud attempts surged by over 1,300% compared to 2023,\nunderscoring the urgent need for robust audio content authentication. The\nfinancial sector has been particularly impacted, with a loss of over 10 million\nUSD to voice scams and individual victims reporting losses exceeding $6,000\nfrom AI-generated deepfake calls. In response, regulators and governments\nworldwide are enacting measures to improve AI content transparency and\ntraceability, emphasizing the development of forensic tools and watermarking\ntechniques as essential strategies to uphold media integrity.", "comment": "Accepted to IJCB 2025 (IEEE/IAPR International Joint Conference on\n  Biometrics). Code available at: (1) Official Lab Repo:\n  https://github.com/vcbsl/WaveVerify (2) Original Author Repo:\n  https://github.com/pujariaditya/WaveVerify", "pdf_url": "http://arxiv.org/pdf/2507.21150v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21354", "title": "Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems", "authors": ["Monika Zamojska", "Jarosław A. Chudziak"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci 2025), this https URL", "url": "http://arxiv.org/abs/2507.21354v1", "summary": "Multi-Agent Systems (MAS) are increasingly used to simulate social\ninteractions, but most of the frameworks miss the underlying cognitive\ncomplexity of human behavior. In this paper, we introduce Trans-ACT\n(Transactional Analysis Cognitive Toolkit), an approach embedding Transactional\nAnalysis (TA) principles into MAS to generate agents with realistic\npsychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego\nstates into an agent's cognitive architecture. Each ego state retrieves\ncontext-specific memories and uses them to shape response to new situations.\nThe final answer is chosen according to the underlying life script of the\nagent. Our experimental simulation, which reproduces the Stupid game scenario,\ndemonstrates that agents grounded in cognitive and TA principles produce deeper\nand context-aware interactions. Looking ahead, our research opens a new way for\na variety of applications, including conflict resolution, educational support,\nand advanced social psychology studies.", "comment": "Proceedings of the Annual Meeting of the Cognitive Science Society\n  (CogSci 2025), https://escholarship.org/uc/item/7gg6j165", "pdf_url": "http://arxiv.org/pdf/2507.21354v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21338", "title": "Autonomous Exploration with Terrestrial-Aerial Bimodal Vehicles", "authors": ["Yuman Gao", "Ruibin Zhang", "Tiancheng Lai", "Yanjun Cao", "Chao Xu", "Fei Gao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21338v1", "summary": "Terrestrial-aerial bimodal vehicles, which integrate the high mobility of\naerial robots with the long endurance of ground robots, offer significant\npotential for autonomous exploration. Given the inherent energy and time\nconstraints in practical exploration tasks, we present a hierarchical framework\nfor the bimodal vehicle to utilize its flexible locomotion modalities for\nexploration. Beginning with extracting environmental information to identify\ninformative regions, we generate a set of potential bimodal viewpoints. To\nadaptively manage energy and time constraints, we introduce an extended Monte\nCarlo Tree Search approach that strategically optimizes both modality selection\nand viewpoint sequencing. Combined with an improved bimodal vehicle motion\nplanner, we present a complete bimodal energy- and time-aware exploration\nsystem. Extensive simulations and deployment on a customized real-world\nplatform demonstrate the effectiveness of our system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21338v1", "cate": "cs.RO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21151", "title": "NIST Post-Quantum Cryptography Standard Algorithms Based on Quantum Random Number Generators", "authors": ["Abel C. H. Chen"], "categories": ["cs.CR", "cs.PF", "quant-ph", "stat.AP"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      in Chinese language", "url": "http://arxiv.org/abs/2507.21151v1", "summary": "In recent years, the advancement of quantum computing technology has posed\npotential security threats to RSA cryptography and elliptic curve cryptography.\nIn response, the National Institute of Standards and Technology (NIST)\npublished several Federal Information Processing Standards (FIPS) of\npost-quantum cryptography (PQC) in August 2024, including the\nModule-Lattice-Based Key-Encapsulation Mechanism (ML-KEM), Module-Lattice-Based\nDigital Signature Algorithm (ML-DSA), and Stateless Hash-Based Digital\nSignature Algorithm (SLH-DSA). Although these PQC algorithms are designed to\nresist quantum computing attacks, they may not provide adequate security in\ncertain specialized application scenarios. To address this issue, this study\nproposes quantum random number generator (QRNG)-based PQC algorithms. These\nalgorithms leverage quantum computing to generate random numbers, which serve\nas the foundation for key pair generation, key encapsulation, and digital\nsignature generation. A generalized architecture of QRNG is proposed, along\nwith the design of six QRNGs. Each generator is evaluated according to the\nstatistical validation procedures outlined in NIST SP 800-90B, including tests\nfor verification of entropy sources and independent and identically distributed\n(IID) outputs. Experimental results assess the computation time of the six\nQRNGs, as well as the performance of QRNG-based ML-KEM, QRNG-based ML-DSA, and\nQRNG-based SLH-DSA. These findings provide valuable reference data for future\ndeployment of PQC systems.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2507.21151v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.21631", "title": "\"Teammates, Am I Clear?\": Analysing Legible Behaviours in Teams", "authors": ["Miguel Faria", "Francisco S. Melo", "Ana Paiva"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21631v1", "summary": "In this paper we investigate the notion of legibility in sequential\ndecision-making in the context of teams and teamwork. There have been works\nthat extend the notion of legibility to sequential decision making, for\ndeterministic and for stochastic scenarios. However, these works focus on one\nagent interacting with one human, foregoing the benefits of having legible\ndecision making in teams of agents or in team configurations with humans. In\nthis work we propose an extension of legible decision-making to multi-agent\nsettings that improves the performance of agents working in collaboration. We\nshowcase the performance of legible decision making in team scenarios using our\nproposed extension in multi-agent benchmark scenarios. We show that a team with\na legible agent is able to outperform a team composed solely of agents with\nstandard optimal behaviour.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21631v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21384", "title": "Projecting the New Body: How Body Image Evolves During Learning to Walk with a Wearable Robot", "authors": ["I-Chieh Lee", "He Huang"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21384v1", "summary": "Advances in wearable robotics challenge the traditional definition of human\nmotor systems, as wearable robots redefine body structure, movement capability,\nand perception of their own bodies. We measured gait performance and perceived\nbody images via Selected Coefficient of Perceived Motion, SCoMo, after each\ntraining session. Based on human motor learning theory extended to wearer-robot\nsystems, we hypothesized that learning the perceived body image when walking\nwith a robotic leg co-evolves with the actual gait improvement and becomes more\ncertain and more accurate to the actual motion. Our result confirmed that motor\nlearning improved both physical and perceived gait pattern towards normal,\nindicating that via practice the wearers incorporated the robotic leg into\ntheir sensorimotor systems to enable wearer-robot movement coordination.\nHowever, a persistent discrepancy between perceived and actual motion remained,\nlikely due to the absence of direct sensation and control of the prosthesis\nfrom wearers. Additionally, the perceptual overestimation at the later training\nsessions might limit further motor improvement. These findings suggest that\nenhancing the human sense of wearable robots and frequent calibrating\nperception of body image are essential for effective training with lower limb\nwearable robots and for developing more embodied assistive technologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21384v1", "cate": "cs.RO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21154", "title": "Assessment of Quantitative Cyber-Physical Reliability of SCADA Systems in Autonomous Vehicle to Grid (V2G) Capable Smart Grids", "authors": ["Md Abdul Gaffar"], "categories": ["cs.CR", "cs.SY", "eess.SY", "math.OC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      5 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21154v1", "summary": "The integration of electric vehicles (EVs) into power grids via\nVehicle-to-Grid (V2G) system technology is increasing day by day, but these\nphenomena present both advantages and disadvantages. V2G can increase grid\nreliability by providing distributed energy storage and ancillary services.\nHowever, on the other hand, it has a scope that encompasses the cyber-physical\nattack surface of the national power grid, introducing new vulnerabilities in\nmonitoring and supervisory control and data acquisition (SCADA) systems. This\npaper investigates the maliciousness caused by Autonomous Vehicle to Grid\n(AV2G) communication infrastructures and assesses their impacts on SCADA system\nreliability. This paper presents a quantitative reliability assessment using\nBayesian attack graph combined with probabilistic capacity outage modeling\nbased on IEEE RTS-79 system data. This work presents how AV2G-based attacks\ndegrade system performance by using Monte Carlo simulations method,\nhighlighting the need for cybersecurity-hardening strategies in smart grid\ndesign.", "comment": "5 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21154v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2405.14078", "title": "A finite time analysis of distributed Q-learning", "authors": ["Han-Dong Lim", "Donghwan Lee"], "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Published at RLC2025", "url": "http://arxiv.org/abs/2405.14078v2", "summary": "Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in\ninterest, fueled by the empirical success achieved in applications of\nsingle-agent reinforcement learning (RL). In this study, we consider a\ndistributed Q-learning scenario, wherein a number of agents cooperatively solve\na sequential decision making problem without access to the central reward\nfunction which is an average of the local rewards. In particular, we study\nfinite-time analysis of a distributed Q-learning algorithm, and provide a new\nsample complexity result of $\\tilde{\\mathcal{O}}\\left(\n\\min\\left\\{\\frac{1}{\\epsilon^2}\\frac{t_{\\text{mix}}}{(1-\\gamma)^6 d_{\\min}^4 }\n,\\frac{1}{\\epsilon}\\frac{\\sqrt{|\\gS||\\gA|}}{(1-\\sigma_2(\\boldsymbol{W}))(1-\\gamma)^4\nd_{\\min}^3} \\right\\}\\right)$ under tabular lookup", "comment": "Published at RLC2025", "pdf_url": "http://arxiv.org/pdf/2405.14078v2", "cate": "cs.AI", "date": "2024-05-23", "updated": "2025-07-29"}
{"id": "2507.21431", "title": "Sound Source Localization for Human-Robot Interaction in Outdoor Environments", "authors": ["Victor Liu", "Timothy Du", "Jordy Sehn", "Jack Collier", "François Grondin"], "categories": ["cs.RO", "cs.HC", "eess.AS"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21431v1", "summary": "This paper presents a sound source localization strategy that relies on a\nmicrophone array embedded in an unmanned ground vehicle and an asynchronous\nclose-talking microphone near the operator. A signal coarse alignment strategy\nis combined with a time-domain acoustic echo cancellation algorithm to estimate\na time-frequency ideal ratio mask to isolate the target speech from\ninterferences and environmental noise. This allows selective sound source\nlocalization, and provides the robot with the direction of arrival of sound\nfrom the active operator, which enables rich interaction in noisy scenarios.\nResults demonstrate an average angle error of 4 degrees and an accuracy within\n5 degrees of 95\\% at a signal-to-noise ratio of 1dB, which is significantly\nsuperior to the state-of-the-art localization methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21431v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21067", "title": "SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration", "authors": ["Jan Kapusta"], "categories": ["cs.AI", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0 protocol specification, and formal BNF grammar", "url": "http://arxiv.org/abs/2507.21067v1", "summary": "Current AI systems rely on opaque reasoning processes that hinder human\noversight and collaborative potential. Conventional explainable AI approaches\noffer post-hoc justifications and often fail to establish genuine symbiotic\ncollaboration. In this paper, the Symbiotic Epistemology is presented as a\nphilosophical foundation for human-AI cognitive partnerships. Unlike frameworks\nthat treat AI as a mere tool or replacement, symbiotic epistemology positions\nAI as a reasoning partner, fostering calibrated trust by aligning human\nconfidence with AI reliability through explicit reasoning patterns and\nconfidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as\na formal protocol for transparent human-AI collaboration. The framework is\nempirically validated through actual human-AI dialogues demonstrating AI's\nadaptation to structured reasoning protocols and successful metacognitive\nintervention. The protocol defines two complementary mechanisms: TRACE for\nhigh-level reasoning patterns and TRACE_FE for detailed factor explanations. It\nalso integrates confidence quantification, declarative control over AI\nbehavior, and context inheritance for multi-agent coordination. By structuring\ncommunication and embedding confidence-calibrated transparency, SynLang,\ntogether with symbiotic epistemology, enables AI systems that enhance human\nintelligence, preserve human agency, and uphold ethical accountability in\ncollaborative decision-making. Through dual-level transparency, beginning with\nhigh-level reasoning patterns and progressing to granular explanations, the\nprotocol facilitates rapid comprehension and supports thorough verification of\nAI decision-making.", "comment": "32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0\n  protocol specification, and formal BNF grammar", "pdf_url": "http://arxiv.org/pdf/2507.21067v1", "cate": "cs.AI", "date": "2025-06-03", "updated": "2025-06-03"}
{"id": "2507.21157", "title": "Unmasking Synthetic Realities in Generative AI: A Comprehensive Review of Adversarially Robust Deepfake Detection Systems", "authors": ["Naseem Khan", "Tuan Nguyen", "Amine Bermak", "Issa Khalil"], "categories": ["cs.CR", "cs.CV", "F.2.2; I.2.7"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      27 pages, 4 Tables, 3 Figures", "url": "http://arxiv.org/abs/2507.21157v1", "summary": "The rapid advancement of Generative Artificial Intelligence has fueled\ndeepfake proliferation-synthetic media encompassing fully generated content and\nsubtly edited authentic material-posing challenges to digital security,\nmisinformation mitigation, and identity preservation. This systematic review\nevaluates state-of-the-art deepfake detection methodologies, emphasizing\nreproducible implementations for transparency and validation. We delineate two\ncore paradigms: (1) detection of fully synthetic media leveraging statistical\nanomalies and hierarchical feature extraction, and (2) localization of\nmanipulated regions within authentic content employing multi-modal cues such as\nvisual artifacts and temporal inconsistencies. These approaches, spanning\nuni-modal and multi-modal frameworks, demonstrate notable precision and\nadaptability in controlled settings, effectively identifying manipulations\nthrough advanced learning techniques and cross-modal fusion. However,\ncomprehensive assessment reveals insufficient evaluation of adversarial\nrobustness across both paradigms. Current methods exhibit vulnerability to\nadversarial perturbations-subtle alterations designed to evade\ndetection-undermining reliability in real-world adversarial contexts. This gap\nhighlights critical disconnect between methodological development and evolving\nthreat landscapes. To address this, we contribute a curated GitHub repository\naggregating open-source implementations, enabling replication and testing. Our\nfindings emphasize urgent need for future work prioritizing adversarial\nresilience, advocating scalable, modality-agnostic architectures capable of\nwithstanding sophisticated manipulations. This review synthesizes strengths and\nshortcomings of contemporary deepfake detection while charting paths toward\nrobust trustworthy systems.", "comment": "27 pages, 4 Tables, 3 Figures", "pdf_url": "http://arxiv.org/pdf/2507.21157v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2502.05934", "title": "Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis", "authors": ["Aran Nayebi"], "categories": ["cs.AI", "cs.CC", "cs.GT", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      20 pages, improved lower bounds and added clarifications", "url": "http://arxiv.org/abs/2502.05934v2", "summary": "We formalize AI alignment as a multi-objective optimization problem called\n$\\langle M,N,\\varepsilon,\\delta\\rangle$-agreement that generalizes prior\napproaches with fewer assumptions, in which a set of $N$ agents (including\nhumans) must reach approximate ($\\varepsilon$) agreement across $M$ candidate\nobjectives with probability at least $1-\\delta$. Using communication\ncomplexity, we prove an information-theoretic lower bound demonstrating that\nonce either $M$ or $N$ is large enough, no interaction or rationality can avoid\nintrinsic alignment overheads. This barrier establishes rigorous intrinsic\nlimits to alignment \\emph{itself}, not merely to specific methods, clarifying a\ncrucial ``no free lunch'' principle: encoding ``all human values'' inevitably\nleads to misalignment, requiring future methods to explicitly manage complexity\nthrough consensus-driven reduction or prioritization of objectives.\nComplementing this impossibility result, we provide explicit algorithms\nachieving alignment under both computationally unbounded and bounded\nrationality with noisy messages. Even in these best-case scenarios where\nalignment to arbitrary precision is theoretically guaranteed, our analysis\nidentifies three critical scalability barriers: the number of tasks ($M$),\nagents ($N$), and task state space size ($D$); thereby highlighting fundamental\ncomplexity-theoretic constraints and providing guidelines for safer, scalable\nhuman-AI collaboration.", "comment": "20 pages, improved lower bounds and added clarifications", "pdf_url": "http://arxiv.org/pdf/2502.05934v2", "cate": "cs.AI", "date": "2025-02-09", "updated": "2025-07-29"}
{"id": "2507.21496", "title": "Multifunctional physical reservoir computing in soft tensegrity robots", "authors": ["Ryo Terajima", "Katsuma Inoue", "Kohei Nakajima", "Yasuo Kuniyoshi"], "categories": ["cs.RO", "cs.LG", "nlin.CD"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      25 pages, 12 figures. The following article has been accepted by Chaos: An Interdisciplinary Journal of Nonlinear Science", "url": "http://arxiv.org/abs/2507.21496v1", "summary": "Recent studies have demonstrated that the dynamics of physical systems can be\nutilized for the desired information processing under the framework of physical\nreservoir computing (PRC). Robots with soft bodies are examples of such\nphysical systems, and their nonlinear body-environment dynamics can be used to\ncompute and generate the motor signals necessary for the control of their own\nbehavior. In this simulation study, we extend this approach to control and\nembed not only one but also multiple behaviors into a type of soft robot called\na tensegrity robot. The resulting system, consisting of the robot and the\nenvironment, is a multistable dynamical system that converges to different\nattractors from varying initial conditions. Furthermore, attractor analysis\nreveals that there exist \"untrained attractors\" in the state space of the\nsystem outside the training data. These untrained attractors reflect the\nintrinsic properties and structures of the tensegrity robot and its\ninteractions with the environment. The impacts of these recent findings in PRC\nremain unexplored in embodied AI research. We here illustrate their potential\nto understand various features of embodied cognition that have not been fully\naddressed to date.", "comment": "25 pages, 12 figures. The following article has been accepted by\n  Chaos: An Interdisciplinary Journal of Nonlinear Science", "pdf_url": "http://arxiv.org/pdf/2507.21496v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21098", "title": "Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism", "authors": ["Marta Sidorkiewicz", "Karolina Królikowska", "Berenika Dyczek", "Edyta Pijet-Migon", "Anna Dubel"], "categories": ["cs.AI", "cs.CY", "I.2.6; I.2.1; H.4.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures. Accepted for presentation at the 27th European Conference on Artificial Intelligence (ECAI 2025), October 19-24, 2025, Bologna, Italy", "url": "http://arxiv.org/abs/2507.21098v1", "summary": "This study examines the role of Artificial Intelligence (AI) in enhancing\nsustainability and efficiency within the wine industry. It focuses on AI-driven\nintelligent management in viticulture, wine production, and enotourism. As the\nwine industry faces environmental and economic challenges, AI offers innovative\nsolutions to optimize resource use, reduce environmental impact, and improve\ncustomer engagement. Understanding AI's potential in sustainable winemaking is\ncrucial for fostering responsible and efficient industry practices. The\nresearch is based on a questionnaire survey conducted among Polish winemakers,\ncombined with a comprehensive analysis of AI methods applicable to viticulture,\nproduction, and tourism. Key AI technologies, including predictive analytics,\nmachine learning, and computer vision, are explored. The findings indicate that\nAI enhances vineyard monitoring, optimizes irrigation, and streamlines\nproduction processes, contributing to sustainable resource management. In\nenotourism, AI-powered chatbots, recommendation systems, and virtual tastings\npersonalize consumer experiences. The study highlights AI's impact on economic,\nenvironmental, and social sustainability, supporting local wine enterprises and\ncultural heritage. Keywords: Artificial Intelligence, Sustainable Development,\nAI-Driven Management, Viticulture, Wine Production, Enotourism, Wine\nEnterprises, Local Communities", "comment": "6 pages, 4 figures. Accepted for presentation at the 27th European\n  Conference on Artificial Intelligence (ECAI 2025), October 19-24, 2025,\n  Bologna, Italy", "pdf_url": "http://arxiv.org/pdf/2507.21098v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2507.21109", "title": "Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students", "authors": ["Prital Bamnodkar"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21109v1", "summary": "Deep Neural Networks often suffer from a critical limitation known as\nCatastrophic Forgetting, where performance on past tasks degrades after\nlearning new ones. This paper introduces a novel continual learning approach\ninspired by human learning strategies like Active Recall, Deliberate Practice\nand Spaced Repetition, named Task Focused Consolidation with Spaced Recall\n(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we\ntermed the Active Recall Probe. It is a periodic, task-aware evaluation of the\nmodel's memory that stabilizes the representations of past knowledge. We test\nTFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading\nregularization-based and replay-based baselines. Our results show that TFC-SR\nperforms significantly better than these methods. For instance, on the Split\nCIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's\n7.40%. We demonstrate that this advantage comes from the stabilizing effect of\nthe probe itself, and not from the difference in replay volume. Additionally,\nwe analyze the trade-off between memory size and performance and show that\nwhile TFC-SR performs better in memory-constrained environments, higher replay\nvolume is still more effective when available memory is abundant. We conclude\nthat TFC-SR is a robust and efficient approach, highlighting the importance of\nintegrating active memory retrieval mechanisms into continual learning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21109v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.21163", "title": "Generating Adversarial Point Clouds Using Diffusion Model", "authors": ["Ruiyang Zhao", "Bingbing Zhu", "Chuxuan Tong", "Xiaoyi Zhou", "Xi Zheng"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21163v1", "summary": "Adversarial attack methods for 3D point cloud classification reveal the\nvulnerabilities of point cloud recognition models. This vulnerability could\nlead to safety risks in critical applications that use deep learning models,\nsuch as autonomous vehicles. To uncover the deficiencies of these models,\nresearchers can evaluate their security through adversarial attacks. However,\nmost existing adversarial attack methods are based on white-box attacks. While\nthese methods achieve high attack success rates and imperceptibility, their\napplicability in real-world scenarios is limited. Black-box attacks, which are\nmore meaningful in real-world scenarios, often yield poor results. This paper\nproposes a novel black-box adversarial example generation method that utilizes\na diffusion model to improve the attack success rate and imperceptibility in\nthe black-box setting, without relying on the internal information of the point\ncloud classification model to generate adversarial samples. We use a 3D\ndiffusion model to use the compressed features of the point cloud as prior\nknowledge to guide the reverse diffusion process to add adversarial points to\nclean examples. Subsequently, its reverse process is employed to transform the\ndistribution of other categories into adversarial points, which are then added\nto the point cloud.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21163v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.20230", "title": "A Multi-Agent System Enables Versatile Information Extraction from the Chemical Literature", "authors": ["Yufan Chen", "Ching Ting Leung", "Bowen Yu", "Jianwei Sun", "Yong Huang", "Linyan Li", "Hao Chen", "Hanyu Gao"], "categories": ["cs.AI", "cs.CV", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20230v2", "summary": "To fully expedite AI-powered chemical research, high-quality chemical\ndatabases are the cornerstone. Automatic extraction of chemical information\nfrom the literature is essential for constructing reaction databases, but it is\ncurrently limited by the multimodality and style variability of chemical\ninformation. In this work, we developed a multimodal large language model\n(MLLM)-based multi-agent system for robust and automated chemical information\nextraction. It utilizes the MLLM's strong reasoning capability to understand\nthe structure of diverse chemical graphics, decompose the extraction task into\nsub-tasks, and coordinate a set of specialized agents, each combining the\ncapabilities of the MLLM with the precise, domain-specific strengths of\ndedicated tools, to solve them accurately and integrate the results into a\nunified output. Our system achieved an F1 score of 80.8% on a benchmark dataset\nof sophisticated multimodal chemical reaction graphics from the literature,\nsurpassing the previous state-of-the-art model (F1 score of 35.6%) by a\nsignificant margin. Additionally, it demonstrated consistent improvements in\nkey sub-tasks, including molecular image recognition, reaction image parsing,\nnamed entity recognition and text-based reaction extraction. This work is a\ncritical step toward automated chemical information extraction into structured\ndatasets, which will be a strong promoter of AI-driven chemical research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20230v2", "cate": "cs.AI", "date": "2025-07-27", "updated": "2025-07-29"}
{"id": "2507.21506", "title": "Decision Transformer-Based Drone Trajectory Planning with Dynamic Safety-Efficiency Trade-Offs", "authors": ["Chang-Hun Ji", "SiWoon Song", "Youn-Hee Han", "SungTae Moon"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025. \\c{opyright} 2025 IEEE. Personal use of this material is permitted. \\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "url": "http://arxiv.org/abs/2507.21506v1", "summary": "A drone trajectory planner should be able to dynamically adjust the\nsafety-efficiency trade-off according to varying mission requirements in\nunknown environments. Although traditional polynomial-based planners offer\ncomputational efficiency and smooth trajectory generation, they require expert\nknowledge to tune multiple parameters to adjust this trade-off. Moreover, even\nwith careful tuning, the resulting adjustment may fail to achieve the desired\ntrade-off. Similarly, although reinforcement learning-based planners are\nadaptable in unknown environments, they do not explicitly address the\nsafety-efficiency trade-off. To overcome this limitation, we introduce a\nDecision Transformer-based trajectory planner that leverages a single\nparameter, Return-to-Go (RTG), as a \\emph{temperature parameter} to dynamically\nadjust the safety-efficiency trade-off. In our framework, since RTG intuitively\nmeasures the safety and efficiency of a trajectory, RTG tuning does not require\nexpert knowledge. We validate our approach using Gazebo simulations in both\nstructured grid and unstructured random environments. The experimental results\ndemonstrate that our planner can dynamically adjust the safety-efficiency\ntrade-off by simply tuning the RTG parameter. Furthermore, our planner\noutperforms existing baseline methods across various RTG settings, generating\nsafer trajectories when tuned for safety and more efficient trajectories when\ntuned for efficiency. Real-world experiments further confirm the reliability\nand practicality of our proposed planner.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. Permission from IEEE must be obtained for all other uses", "pdf_url": "http://arxiv.org/pdf/2507.21506v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21123", "title": "Leveraging Generative AI to Enhance Synthea Module Development", "authors": ["Mark A. Kramer", "Aanchal Mathur", "Caroline E. Adams", "Jason A. Walonoski"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Title: Leveraging Generative AI to Enhance Synthea Module Development Word Count: [Approximately 12,000 words] Figures: 3 Tables: 3 Supplementary Material: Extensive appendices with prompts and disease profiles", "url": "http://arxiv.org/abs/2507.21123v1", "summary": "This paper explores the use of large language models (LLMs) to assist in the\ndevelopment of new disease modules for Synthea, an open-source synthetic health\ndata generator. Incorporating LLMs into the module development process has the\npotential to reduce development time, reduce required expertise, expand model\ndiversity, and improve the overall quality of synthetic patient data. We\ndemonstrate four ways that LLMs can support Synthea module creation: generating\na disease profile, generating a disease module from a disease profile,\nevaluating an existing Synthea module, and refining an existing module. We\nintroduce the concept of progressive refinement, which involves iteratively\nevaluating the LLM-generated module by checking its syntactic correctness and\nclinical accuracy, and then using that information to modify the module. While\nthe use of LLMs in this context shows promise, we also acknowledge the\nchallenges and limitations, such as the need for human oversight, the\nimportance of rigorous testing and validation, and the potential for\ninaccuracies in LLM-generated content. The paper concludes with recommendations\nfor future research and development to fully realize the potential of LLM-aided\nsynthetic data creation.", "comment": "Title: Leveraging Generative AI to Enhance Synthea Module Development\n  Word Count: [Approximately 12,000 words] Figures: 3 Tables: 3 Supplementary\n  Material: Extensive appendices with prompts and disease profiles", "pdf_url": "http://arxiv.org/pdf/2507.21123v1", "cate": "cs.AI", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.21119", "title": "Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks", "authors": ["Yousuf Moiz Ali", "Jaroslaw E. Prilepsky", "Nicola Sambo", "João Pedro", "Mohammad M. Hosseini", "Antonio Napoli", "Sergei K. Turitsyn", "Pedro Freire"], "categories": ["cs.LG", "eess.SP", "physics.optics"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      3 pages + 1 page for acknowledgement and references", "url": "http://arxiv.org/abs/2507.21119v1", "summary": "We compare pre-, in-, and post-processing techniques for class imbalance\nmitigation in optical network failure detection. Threshold Adjustment achieves\nthe highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the\nfastest inference, highlighting a key performance-complexity trade-off.", "comment": "3 pages + 1 page for acknowledgement and references", "pdf_url": "http://arxiv.org/pdf/2507.21119v1", "cate": "cs.LG", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.21170", "title": "OneShield -- the Next Generation of LLM Guardrails", "authors": ["Chad DeLuca", "Anna Lisa Gentile", "Shubhi Asthana", "Bing Zhang", "Pawan Chowdhary", "Kellen Cheng", "Basel Shbita", "Pengyuan Li", "Guang-Jie Ren", "Sandeep Gopisetty"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21170v1", "summary": "The rise of Large Language Models has created a general excitement about the\ngreat potential for a myriad of applications. While LLMs offer many\npossibilities, questions about safety, privacy, and ethics have emerged, and\nall the key actors are working to address these issues with protective measures\nfor their own models and standalone solutions. The constantly evolving nature\nof LLMs makes the task of universally shielding users against their potential\nrisks extremely challenging, and one-size-fits-all solutions unfeasible. In\nthis work, we propose OneShield, our stand-alone, model-agnostic and\ncustomizable solution to safeguard LLMs. OneShield aims to provide facilities\nfor defining risk factors, expressing and declaring contextual safety and\ncompliance policies, and mitigating LLM risks, with a focus on each specific\ncustomer. We describe the implementation of the framework, the scalability\nconsiderations and provide usage statistics of OneShield since its first\ndeployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21170v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21517", "title": "LITE: A Learning-Integrated Topological Explorer for Multi-Floor Indoor Environments", "authors": ["Junhao Chen", "Zhen Zhang", "Chengrui Zhu", "Xiaojun Hou", "Tianyang Hu", "Huifeng Wu", "Yong Liu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IROS2025", "url": "http://arxiv.org/abs/2507.21517v1", "summary": "This work focuses on multi-floor indoor exploration, which remains an open\narea of research. Compared to traditional methods, recent learning-based\nexplorers have demonstrated significant potential due to their robust\nenvironmental learning and modeling capabilities, but most are restricted to 2D\nenvironments. In this paper, we proposed a learning-integrated topological\nexplorer, LITE, for multi-floor indoor environments. LITE decomposes the\nenvironment into a floor-stair topology, enabling seamless integration of\nlearning or non-learning-based 2D exploration methods for 3D exploration. As we\nincrementally build floor-stair topology in exploration using YOLO11-based\ninstance segmentation model, the agent can transition between floors through a\nfinite state machine. Additionally, we implement an attention-based 2D\nexploration policy that utilizes an attention mechanism to capture spatial\ndependencies between different regions, thereby determining the next global\ngoal for more efficient exploration. Extensive comparison and ablation studies\nconducted on the HM3D and MP3D datasets demonstrate that our proposed 2D\nexploration policy significantly outperforms all baseline explorers in terms of\nexploration efficiency. Furthermore, experiments in several 3D multi-floor\nenvironments indicate that our framework is compatible with various 2D\nexploration methods, facilitating effective multi-floor indoor exploration.\nFinally, we validate our method in the real world with a quadruped robot,\nhighlighting its strong generalization capabilities.", "comment": "IROS2025", "pdf_url": "http://arxiv.org/pdf/2507.21517v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21129", "title": "Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics", "authors": ["Jae Wan Shim"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21129v1", "summary": "The remarkable capabilities of Large Language Models (LLMs) are now\nextensively documented on task-specific benchmarks, yet the internal mechanisms\nthat produce these results are the subject of intense scientific inquiry. This\npaper contributes to this inquiry by moving beyond metrics that measure\n\\textit{what} models can do, to a methodology that characterizes \\textit{how}\nthey process information. We introduce a novel, task-agnostic approach to probe\nthese dynamics by creating a quantitative ``Cognitive Profile\" for any given\nmodel. This profile is centered on the \\textbf{Entropy Decay Curve}, a\nvisualization that traces how a model's normalized predictive uncertainty\nchanges as a function of context length. Applying this methodology to several\nstate-of-the-art LLMs across diverse texts, we uncover unique and consistent\ncognitive profiles that are sensitive to both model scale and text complexity.\nWe also introduce the Information Gain Span (IGS) index to summarize the\ndesirability of the decay trajectory. This work thus provides a new, principled\nlens for analyzing and comparing the intrinsic operational dynamics of\nartificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21129v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.21135", "title": "Quantum Geometry of Data", "authors": ["Alexander G. Abanov", "Luca Candelori", "Harold C. Steinacker", "Martin T. Wells", "Jerome R. Busemeyer", "Cameron J. Hogan", "Vahagn Kirakosyan", "Nicola Marzari", "Sunil Pinnamaneni", "Dario Villani", "Mengjia Xu", "Kharen Musaelian"], "categories": ["cs.LG", "quant-ph", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      27 pages, 14 figures, 1 table", "url": "http://arxiv.org/abs/2507.21135v1", "summary": "We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as\nquantum geometry. In QCML, features of the data are represented by learned\nHermitian matrices, and data points are mapped to states in Hilbert space. The\nquantum geometry description endows the dataset with rich geometric and\ntopological structure - including intrinsic dimension, quantum metric, and\nBerry curvature - derived directly from the data. QCML captures global\nproperties of data, while avoiding the curse of dimensionality inherent in\nlocal methods. We illustrate this on a number of synthetic and real-world\nexamples. Quantum geometric representation of QCML could advance our\nunderstanding of cognitive phenomena within the framework of quantum cognition.", "comment": "27 pages, 14 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.21135v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21177", "title": "FedBAP: Backdoor Defense via Benign Adversarial Perturbation in Federated Learning", "authors": ["Xinhai Yan", "Libing Wu", "Zhuangzhuang Zhang", "Bingyi Liu", "Lijuan Huo", "Jing Wang"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.21177v1", "summary": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy, but it is highly vulnerable to backdoor attacks. Most existing\ndefense methods in FL have limited effectiveness due to their neglect of the\nmodel's over-reliance on backdoor triggers, particularly as the proportion of\nmalicious clients increases. In this paper, we propose FedBAP, a novel defense\nframework for mitigating backdoor attacks in FL by reducing the model's\nreliance on backdoor triggers. Specifically, first, we propose a perturbed\ntrigger generation mechanism that creates perturbation triggers precisely\nmatching backdoor triggers in location and size, ensuring strong influence on\nmodel outputs. Second, we utilize these perturbation triggers to generate\nbenign adversarial perturbations that disrupt the model's dependence on\nbackdoor triggers while forcing it to learn more robust decision boundaries.\nFinally, we design an adaptive scaling mechanism to dynamically adjust\nperturbation intensity, effectively balancing defense strength and model\nperformance. The experimental results demonstrate that FedBAP reduces the\nattack success rates by 0.22%-5.34%, 0.48%-6.34%, and 97.22%-97.6% under three\ntypes of backdoor attacks, respectively. In particular, FedBAP demonstrates\noutstanding performance against novel backdoor attacks.", "comment": "Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.21177v1", "cate": "cs.CR", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21533", "title": "Model Predictive Adversarial Imitation Learning for Planning from Observation", "authors": ["Tyler Han", "Yanda Bao", "Bhaumik Mehta", "Gabriel Guo", "Anubhav Vishwakarma", "Emily Kang", "Sanghun Jung", "Rosario Scalise", "Jason Zhou", "Bryan Xu", "Byron Boots"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Open-source code in process of being cleaned and documented for release. Please contact directly in the meantime for code. Under Review", "url": "http://arxiv.org/abs/2507.21533v1", "summary": "Human demonstration data is often ambiguous and incomplete, motivating\nimitation learning approaches that also exhibit reliable planning behavior. A\ncommon paradigm to perform planning-from-demonstration involves learning a\nreward function via Inverse Reinforcement Learning (IRL) then deploying this\nreward via Model Predictive Control (MPC). Towards unifying these methods, we\nderive a replacement of the policy in IRL with a planning-based agent. With\nconnections to Adversarial Imitation Learning, this formulation enables\nend-to-end interactive learning of planners from observation-only\ndemonstrations. In addition to benefits in interpretability, complexity, and\nsafety, we study and observe significant improvements on sample efficiency,\nout-of-distribution generalization, and robustness. The study includes\nevaluations in both simulated control benchmarks and real-world navigation\nexperiments using few-to-single observation-only demonstrations.", "comment": "Open-source code in process of being cleaned and documented for\n  release. Please contact directly in the meantime for code. Under Review", "pdf_url": "http://arxiv.org/pdf/2507.21533v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21130", "title": "INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems", "authors": ["Bintao Tang", "Xin Yang", "Yuhao Wang", "Zixuan Qiu", "Zimo Ji", "Wenyuan Jiang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19 pages, 5 figures", "url": "http://arxiv.org/abs/2507.21130v1", "summary": "We present INTEGRALBENCH, a focused benchmark designed to evaluate Large\nLanguage Model (LLM) performance on definite integral problems. INTEGRALBENCH\nprovides both symbolic and numerical ground truth solutions with manual\ndifficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals\nsignificant performance gaps and strong correlations between problem difficulty\nand model accuracy, establishing baseline metrics for this challenging domain.\nINTEGRALBENCH aims to advance automated mathematical reasoning by providing a\nrigorous evaluation framework specifically tailored for definite integral\ncomputation.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.21130v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21136", "title": "A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning", "authors": ["Mojtaba Moattari"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21136v1", "summary": "Unsupervised and supervised learning methods conventionally use kernels to\ncapture nonlinearities inherent in data structure. However experts have to\nensure their proposed nonlinearity maximizes variability and capture inherent\ndiversity of data. We reviewed all independence criteria to design unsupervised\nlearners. Then we proposed 3 independence criteria and used them to design\nunsupervised and supervised dimensionality reduction methods. We evaluated\ncontrast, accuracy and interpretability of these methods in both linear and\nneural nonlinear settings. The results show that the methods have outperformed\nthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and\nlayer sharing) and opened a new line of interpretable machine learning (ML) for\nthe researchers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21136v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21070", "title": "Enhancing Manufacturing Training Through VR Simulations", "authors": ["Vladislav Li", "Ilias Siniosoglou", "Panagiotis Sarigiannidis", "Vasileios Argyriou"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21070v1", "summary": "In contemporary training for industrial manufacturing, reconciling\ntheoretical knowledge with practical experience continues to be a significant\ndifficulty. As companies transition to more intricate and technology-oriented\nsettings, conventional training methods frequently inadequately equip workers\nwith essential practical skills while maintaining safety and efficiency.\nVirtual Reality has emerged as a transformational instrument to tackle this\nissue by providing immersive, interactive, and risk-free teaching experiences.\nThrough the simulation of authentic industrial environments, virtual reality\nfacilitates the acquisition of vital skills for trainees within a regulated and\nstimulating context, therefore mitigating the hazards linked to experiential\nlearning in the workplace. This paper presents a sophisticated VR-based\nindustrial training architecture aimed at improving learning efficacy via\nhigh-fidelity simulations, dynamic and context-sensitive scenarios, and\nadaptive feedback systems. The suggested system incorporates intuitive\ngesture-based controls, reducing the learning curve for users across all skill\nlevels. A new scoring metric, namely, VR Training Scenario Score (VRTSS), is\nused to assess trainee performance dynamically, guaranteeing ongoing engagement\nand incentive. The experimental assessment of the system reveals promising\noutcomes, with significant enhancements in information retention, task\nexecution precision, and overall training efficacy. The results highlight the\ncapability of VR as a crucial instrument in industrial training, providing a\nscalable, interactive, and efficient substitute for conventional learning\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21070v1", "cate": "cs.HC", "date": "2025-06-06", "updated": "2025-06-06"}
{"id": "2507.21178", "title": "SHoM: A Mental-Synthesis Trust Management Model for Mitigating Botnet-Driven DDoS Attacks in the Internet of Things", "authors": ["Masoud Hayeri Khyavi"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      22 Pages, 15 figure, 9 tables", "url": "http://arxiv.org/abs/2507.21178v2", "summary": "The advantages of IoT in strengthening commercial, industrial, and social\necosystems have led to its widespread expansion. Nevertheless, because endpoint\ndevices have limited computation, storage, and communication capabilities, the\nIoT infrastructure is vulnerable to several cyber threats. As a result, DDoS\nattacks pose a severe risk to the security of IoT. By taking advantage of these\nweaknesses, attackers may quickly employ IoT devices as a component of botnets\nto execute DDoS attacks. The most critical development is how more armies of\nrobots are being constructed from IoT devices. We offer a Model for dealing\nwith DDOS attacks on botnets in the Internet of Things via trust management. In\nthis Model, an attempt has been made to consider all aspects of security\nconcerning trust factors to design a reliable and flexible model against DDoS\nattacks against the Internet of Things. In the initial studies, about 40-50\nsecurity models related to the subject have been studied by using review\narticles", "comment": "22 Pages, 15 figure, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.21178v2", "cate": "cs.CR", "date": "2025-07-26", "updated": "2025-07-30"}
{"id": "2507.21545", "title": "Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning", "authors": ["Haoming Ye", "Yunxiao Xiao", "Cewu Lu", "Panpan Cai"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Preprint. Under review", "url": "http://arxiv.org/abs/2507.21545v1", "summary": "Robotic task planning in real-world environments requires reasoning over\nimplicit constraints from language and vision. While LLMs and VLMs offer strong\npriors, they struggle with long-horizon structure and symbolic grounding.\nExisting methods that combine LLMs with symbolic planning often rely on\nhandcrafted or narrow domains, limiting generalization. We propose UniDomain, a\nframework that pre-trains a PDDL domain from robot manipulation demonstrations\nand applies it for online robotic task planning. It extracts atomic domains\nfrom 12,393 manipulation videos to form a unified domain with 3137 operators,\n2875 predicates, and 16481 causal edges. Given a target class of tasks, it\nretrieves relevant atomics from the unified domain and systematically fuses\nthem into high-quality meta-domains to support compositional generalization in\nplanning. Experiments on diverse real-world tasks show that UniDomain solves\ncomplex, unseen tasks in a zero-shot manner, achieving up to 58% higher task\nsuccess and 160% improvement in plan optimality over state-of-the-art LLM and\nLLM-PDDL baselines.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2507.21545v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21131", "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback", "authors": ["Madhava Gaikwad", "Ashwini Ramchandra Doke"], "categories": ["cs.AI", "68T05", "H.5.1; I.2.6; C.4"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.21131v1", "summary": "We present NPO, an alignment-aware learning framework that operationalizes\nfeedback-driven adaptation in human-in-the-loop decision systems. Unlike prior\napproaches that treat alignment as a static or post-hoc property, NPO\nintroduces a formalization of alignment loss that is measurable, supervisable,\nand reducible under structured feedback. In parallel, we propose meta-alignment\nas the fidelity of the monitoring process that governs retraining or override\ntriggers, and show that it is formally reducible to primary alignment via\nthreshold fidelity. Our implementation spans a scalable operational loop\ninvolving scenario scoring, threshold tuning, policy validation, and structured\nfeedback ingestion, including \"likes\", overrides, and abstentions. We provide\nformal convergence results under stochastic feedback and show that both\nalignment loss and monitoring fidelity converge additively. Empirically, NPO\ndemonstrates measurable value in hyperscale deployment settings. A\nsimulation-based artifact and ablation studies further illustrate the\ntheoretical principles in action. Together, NPO offers a compact, inspectable\narchitecture for continual alignment monitoring, helping bridge theoretical\nalignment guarantees with practical reliability in dynamic environments.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.21131v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21147", "title": "Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning", "authors": ["Fabrizio Lo Scudo", "Alessio De Rango", "Luca Furnari", "Alfonso Senatore", "Donato D'Ambrosio", "Giuseppe Mendicino", "Gianluigi Greco"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in the Proceedings of ECAI 2025", "url": "http://arxiv.org/abs/2507.21147v1", "summary": "Wildfires significantly impact natural ecosystems and human health, leading\nto biodiversity loss, increased hydrogeological risks, and elevated emissions\nof toxic substances. Climate change exacerbates these effects, particularly in\nregions with rising temperatures and prolonged dry periods, such as the\nMediterranean. This requires the development of advanced risk management\nstrategies that utilize state-of-the-art technologies. However, in this\ncontext, the data show a bias toward an imbalanced setting, where the incidence\nof wildfire events is significantly lower than typical situations. This\nimbalance, coupled with the inherent complexity of high-dimensional\nspatio-temporal data, poses significant challenges for training deep learning\narchitectures. Moreover, since precise wildfire predictions depend mainly on\nweather data, finding a way to reduce computational costs to enable more\nfrequent updates using the latest weather forecasts would be beneficial. This\npaper investigates how adopting a contrastive framework can address these\nchallenges through enhanced latent representations for the patch's dynamic\nfeatures. We thus introduce a new morphology-based curriculum contrastive\nlearning that mitigates issues associated with diverse regional characteristics\nand enables the use of smaller patch sizes without compromising performance. An\nexperimental analysis is performed to validate the effectiveness of the\nproposed modeling strategies.", "comment": "To appear in the Proceedings of ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.21147v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21071", "title": "FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents", "authors": ["Qinglong Yang", "Haoming Li", "Haotian Zhao", "Xiaokai Yan", "Jingtao Ding", "Fengli Xu", "Yong Li"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21071v1", "summary": "Mobile GUI agents are becoming critical tools for enhancing human-device\ninteraction efficiency, with multimodal large language models (MLLMs) emerging\nas dominant paradigms in this domain. Current agents, however, are limited to\nfollowing explicit human instructions, resulting in insufficient capability for\nproactive intent anticipation. Additionally, these agents fail to leverage the\ncontextual information associated with users during task execution, thereby\nneglecting potentially vast differences in user preferences. To address these\nchallenges, we introduce the FingerTip benchmark. It contains two new tracks:\nproactive task suggestions by analyzing environment observation and users'\nprevious intents, and personalized task execution by catering to users' action\npreferences. We collected unique human demonstrations of multi-step Android\ndevice interactions across a variety of everyday apps. These demonstrations are\nnot isolated but are continuously acquired from the users' long-term usage in\ntheir real lives, and encompass essential user-related contextual information.\nOur experiments reveal challenges of the tasks we propose. The model fine-tuned\nwith the data we collected effectively utilized user information and achieved\ngood results, highlighting the potential of our approach in building more\nuser-oriented mobile GUI agents. Our code is open-source at\nhttps://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21071v1", "cate": "cs.HC", "date": "2025-06-09", "updated": "2025-06-09"}
{"id": "2507.21200", "title": "PanoGAN A Deep Generative Model for Panoramic Dental Radiographs", "authors": ["Soren Pedersen", "Sanyam Jain", "Mikkel Chavez", "Viktor Ladehoff", "Bruna Neves de Freitas", "Ruben Pauwels"], "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21200v1", "summary": "This paper presents the development of a generative adversarial network (GAN)\nfor synthesizing dental panoramic radiographs. Although exploratory in nature,\nthe study aims to address the scarcity of data in dental research and\neducation. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss\nwith gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying\nquality. The focus was on the dentoalveolar regions, other anatomical\nstructures were cropped out. Extensive preprocessing and data cleaning were\nperformed to standardize the inputs while preserving anatomical variability. We\nexplored four candidate models by varying critic iterations, feature depth, and\nthe use of denoising prior to training. A clinical expert evaluated the\ngenerated radiographs based on anatomical visibility and realism, using a\n5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical\ndepiction, although some were degraded by artifacts. A trade-off was observed\nthe model trained on non-denoised data yielded finer details especially in\nstructures like the mandibular canal and trabecular bone, while a model trained\non denoised data offered superior overall image clarity and sharpness. These\nfindings provide a foundation for future work on GAN-based methods in dental\nimaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21200v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21181", "title": "Mitigation of Social Media Platforms Impact on the Users", "authors": ["Smita Khapre", "Sudhanshu Semwal"], "categories": ["cs.CR", "cs.CY", "cs.GR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      WSCG 2025 33. International Conference on Computer Graphics, Visualization and Computer Vision 2025", "url": "http://arxiv.org/abs/2507.21181v1", "summary": "Social media platforms offer numerous benefits and allow people to come\ntogether for various causes. Many communities, academia, government agencies,\ninstitutions, healthcare, entertainment, and businesses are on social media\nplatforms. They are intuitive and free for users. It has become unimaginable to\nlive without social media. Their architecture and data handling are geared\ntowards scalability, uninterrupted availability, and both personal and\ncollaborative revenue generation. Primarily, artificial intelligence algorithms\nare employed on stored user data for optimization and feeds. This has the\npotential to impact user safety, privacy, and security, even when metadata is\nused. A new decentralized data arrangement framework based on the Fractal-tree\nand L-Systems algorithm is proposed to mitigate some of the impacts of social\nmedia platforms.\n  Future work will focus on demonstrating the effectiveness of the new\ndecentralized framework by comparing its results against state-of-the-art\nsecurity methods currently used in databases. A cryptographic algorithm could\nalso be implemented for the framework, employing a new key generation for each\nbranch. This will strengthen database security; for example, if a user key is\nleaked, regenerating the key for each branch will keep the data secure by\napplying defense mechanisms in the proposed L-System-based tree framework.", "comment": "WSCG 2025 33. International Conference on Computer Graphics,\n  Visualization and Computer Vision 2025", "pdf_url": "http://arxiv.org/pdf/2507.21181v1", "cate": "cs.CR", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21553", "title": "Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments", "authors": ["Federica Di Lauro", "Domenico G. Sorrenti", "Miguel Angel Sotelo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures", "url": "http://arxiv.org/abs/2507.21553v2", "summary": "Multi-robot SLAM aims at localizing and building a map with multiple robots,\ninteracting with each other. In the work described in this article, we analyze\nthe pipeline of a decentralized LiDAR SLAM system to study the current\nlimitations of the state of the art, and we discover a significant source of\nfailures, i.e., that the loop detection is the source of too many false\npositives. We therefore develop and propose a new heuristic to overcome these\nlimitations. The environment taken as reference in this work is the highly\nchallenging case of underground tunnels. We also highlight potential new\nresearch areas still under-explored.", "comment": "14 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.21553v2", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21132", "title": "Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses", "authors": ["Joshua Adrian Cahyono", "Saran Subramanian"], "categories": ["cs.AI", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21132v1", "summary": "Large Language Models (LLMs) are increasingly consulted for high-stakes life\nadvice, yet they lack standard safeguards against providing confident but\nmisguided responses. This creates risks of sycophancy and over-confidence. This\npaper investigates these failure modes through three experiments: (1) a\nmultiple-choice evaluation to measure model stability against user pressure;\n(2) a free-response analysis using a novel safety typology and an LLM Judge;\nand (3) a mechanistic interpretability experiment to steer model behavior by\nmanipulating a \"high-stakes\" activation vector. Our results show that while\nsome models exhibit sycophancy, others like o4-mini remain robust.\nTop-performing models achieve high safety scores by frequently asking\nclarifying questions, a key feature of a safe, inquisitive approach, rather\nthan issuing prescriptive advice. Furthermore, we demonstrate that a model's\ncautiousness can be directly controlled via activation steering, suggesting a\nnew path for safety alignment. These findings underscore the need for nuanced,\nmulti-faceted benchmarks to ensure LLMs can be trusted with life-changing\ndecisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21132v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21152", "title": "Deep Unfolding for MIMO Signal Detection", "authors": ["Hangli Ge", "Noboru Koshizuka"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21152v1", "summary": "In this paper, we propose a deep unfolding neural network-based MIMO detector\nthat incorporates complex-valued computations using Wirtinger calculus. The\nmethod, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables\nefficient, interpretable, and low-complexity MIMO signal detection. Unlike\nprior approaches that rely on real-valued approximations, our method operates\nnatively in the complex domain, aligning with the fundamental nature of signal\nprocessing tasks. The proposed algorithm requires only a small number of\ntrainable parameters, allowing for simplified training. Numerical results\ndemonstrate that the proposed method achieves superior detection performance\nwith fewer iterations and lower computational complexity, making it a practical\nsolution for next-generation massive MIMO systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21152v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.21072", "title": "Snap, Segment, Deploy: A Visual Data and Detection Pipeline for Wearable Industrial Assistants", "authors": ["Di Wen", "Junwei Zheng", "Ruiping Liu", "Yi Xu", "Kunyu Peng", "Rainer Stiefelhagen"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21072v1", "summary": "Industrial assembly tasks increasingly demand rapid adaptation to complex\nprocedures and varied components, yet are often conducted in environments with\nlimited computing, connectivity, and strict privacy requirements. These\nconstraints make conventional cloud-based or fully autonomous solutions\nimpractical for factory deployment. This paper introduces a mobile-device-based\nassistant system for industrial training and operational support, enabling\nreal-time, semi-hands-free interaction through on-device perception and voice\ninterfaces. The system integrates lightweight object detection, speech\nrecognition, and Retrieval-Augmented Generation (RAG) into a modular on-device\npipeline that operates entirely on-device, enabling intuitive support for part\nhandling and procedure understanding without relying on manual supervision or\ncloud services. To enable scalable training, we adopt an automated data\nconstruction pipeline and introduce a two-stage refinement strategy to improve\nvisual robustness under domain shift. Experiments on our generated dataset,\ni.e., Gear8, demonstrate improved robustness to domain shift and common visual\ncorruptions. A structured user study further confirms its practical viability,\nwith positive user feedback on the clarity of the guidance and the quality of\nthe interaction. These results indicate that our framework offers a deployable\nsolution for real-time, privacy-preserving smart assistance in industrial\nenvironments. We will release the Gear8 dataset and source code upon\nacceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21072v1", "cate": "cs.HC", "date": "2025-06-09", "updated": "2025-06-09"}
{"id": "2507.21303", "title": "Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems", "authors": ["Viktoria Marcus", "Griffin Pitts", "Sanaz Motamedi"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted and to be presented at ASPIRE 2025 - the 69th International Annual Meeting of HFES", "url": "http://arxiv.org/abs/2507.21303v1", "summary": "Each year, over half of global traffic fatalities involve vulnerable road\nusers (e.g. pedestrians), often due to human error. Level-5 automated driving\nsystems (ADSs) could reduce driver errors contributing to pedestrian accidents,\nthough effectiveness depends on clarity and understandability for other road\nusers. External human-machine interfaces (eHMIs) have been proposed to\nfacilitate pedestrian-ADS communication, though consensus on optimal eHMI\nfeatures remains unclear. In an online survey, 153 participants responded to\nroad-crossing scenarios involving level-5 ADSs, with and without eHMIs. With\neHMIs, pedestrians crossed earlier and more confidently, and reported\nsignificantly increased perceptions of safety, trust, and understanding when\ninteracting with level-5 ADSs. Visual eHMI features (including a text display\nand external speedometer) were ranked more necessary than auditory ones, though\nauditory cues received positive feedback. This study demonstrates that eHMIs\ncan significantly improve pedestrians' understanding of level-5 ADS intent and\nenhance perceived safety and trust, facilitating more intuitive pedestrian-ADS\ninteractions.", "comment": "Accepted and to be presented at ASPIRE 2025 - the 69th International\n  Annual Meeting of HFES", "pdf_url": "http://arxiv.org/pdf/2507.21303v1", "cate": "cs.HC", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21182", "title": "SDD: Self-Degraded Defense against Malicious Fine-tuning", "authors": ["Zixuan Chen", "Weikai Lu", "Xin Lin", "Ziqian Zeng"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by ACL2025", "url": "http://arxiv.org/abs/2507.21182v1", "summary": "Open-source Large Language Models (LLMs) often employ safety alignment\nmethods to resist harmful instructions. However, recent research shows that\nmaliciously fine-tuning these LLMs on harmful data can easily bypass these\nsafeguards. To counter this, we theoretically uncover why malicious fine-tuning\nsucceeds and identify potential defense strategies. Building on the theoretical\nanalysis, we introduce the Self-Degraded Defense (SDD) framework. SDD\nencourages LLMs to produce high-quality but irrelevant responses to harmful\nprompts. When attackers attempt malicious fine-tuning, the general capability\nof the LLM aligned by SDD will significantly decrease, rendering it incapable\nof following harmful instructions. Our experimental results confirm SDD's\neffectiveness against such attacks.", "comment": "Accepted by ACL2025", "pdf_url": "http://arxiv.org/pdf/2507.21182v1", "cate": "cs.CR", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21610", "title": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "authors": ["Ruiyang Hao", "Haibao Yu", "Jiaru Zhong", "Chuanye Wang", "Jiahao Wang", "Yiming Kan", "Wenxian Yang", "Siqi Fan", "Huilin Yin", "Jianing Qiu", "Yao Mu", "Jiankai Sun", "Li Chen", "Walter Zimmer", "Dandan Zhang", "Shanghang Zhang", "Mac Schwager", "Wei Huang", "Xiaobo Zhang", "Ping Luo", "Zaiqing Nie"], "categories": ["cs.RO", "cs.CV", "I.4.9"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, accepted by ICCVW", "url": "http://arxiv.org/abs/2507.21610v1", "summary": "With the rapid advancement of autonomous driving technology,\nvehicle-to-everything (V2X) communication has emerged as a key enabler for\nextending perception range and enhancing driving safety by providing visibility\nbeyond the line of sight. However, integrating multi-source sensor data from\nboth ego-vehicles and infrastructure under real-world constraints, such as\nlimited communication bandwidth and dynamic environments, presents significant\ntechnical challenges. To facilitate research in this area, we organized the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge, which features\ntwo tracks: cooperative temporal perception and cooperative end-to-end\nplanning. Built on the UniV2X framework and the V2X-Seq-SPD dataset, the\nchallenge attracted participation from over 30 teams worldwide and established\na unified benchmark for evaluating cooperative driving systems. This paper\ndescribes the design and outcomes of the challenge, highlights key research\nproblems including bandwidth-aware fusion, robust multi-agent planning, and\nheterogeneous sensor integration, and analyzes emerging technical trends among\ntop-performing solutions. By addressing practical constraints in communication\nand data fusion, the challenge contributes to the development of scalable and\nreliable V2X-cooperative autonomous driving systems.", "comment": "10 pages, 4 figures, accepted by ICCVW", "pdf_url": "http://arxiv.org/pdf/2507.21610v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21137", "title": "Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?", "authors": ["Arman Eisenkolb-Vaithyanathan"], "categories": ["cs.AI", "I.2.8"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      24 pages, 8 Figures", "url": "http://arxiv.org/abs/2507.21137v1", "summary": "In this paper we try to answer the question \"What constitutes Sudoku\ndifficulty rating across different Sudoku websites?\" Using two distinct methods\nthat can both solve every Sudoku puzzle, I propose two new metrics to\ncharacterize Sudoku difficulty. The first method is based on converting a\nSudoku puzzle into its corresponding Satisfiability (SAT) problem. The first\nproposed metric is derived from SAT Clause Length Distribution which captures\nthe structural complexity of a Sudoku puzzle including the number of given\ndigits and the cells they are in. The second method simulates human Sudoku\nsolvers by intertwining four popular Sudoku strategies within a backtracking\nalgorithm called Nishio. The second metric is computed by counting the number\nof times Sudoku strategies are applied within the backtracking iterations of a\nrandomized Nishio. Using these two metrics, I analyze more than a thousand\nSudoku puzzles across five popular websites to characterize every difficulty\nlevel in each website. I evaluate the relationship between the proposed metrics\nand website-labeled difficulty levels using Spearman's rank correlation\ncoefficient, finding strong correlations for 4 out of 5 websites. I construct a\nuniversal rating system using a simple, unsupervised classifier based on the\ntwo proposed metrics. This rating system is capable of classifying both\nindividual puzzles and entire difficulty levels from the different Sudoku\nwebsites into three categories - Universal Easy, Universal Medium, and\nUniversal Hard - thereby enabling consistent difficulty mapping across Sudoku\nwebsites. The experimental results show that for 4 out of 5 Sudoku websites,\nthe universal classification aligns well with website-labeled difficulty\nlevels. Finally, I present an algorithm that can be used by early Sudoku\npractitioners to solve Sudoku puzzles.", "comment": "24 pages, 8 Figures", "pdf_url": "http://arxiv.org/pdf/2507.21137v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21153", "title": "Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers", "authors": ["Abderaouf Bahi", "Amel Ourici"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21153v1", "summary": "This paper explores the implementation of a Deep Reinforcement Learning\n(DRL)-optimized energy management system for e-commerce data centers, aimed at\nenhancing energy efficiency, cost-effectiveness, and environmental\nsustainability. The proposed system leverages DRL algorithms to dynamically\nmanage the integration of renewable energy sources, energy storage, and grid\npower, adapting to fluctuating energy availability in real time. The study\ndemonstrates that the DRL-optimized system achieves a 38\\% reduction in energy\ncosts, significantly outperforming traditional Reinforcement Learning (RL)\nmethods (28\\%) and heuristic approaches (22\\%). Additionally, it maintains a\nlow SLA violation rate of 1.5\\%, compared to 3.0\\% for RL and 4.8\\% for\nheuristic methods. The DRL-optimized approach also results in an 82\\%\nimprovement in energy efficiency, surpassing other methods, and a 45\\%\nreduction in carbon emissions, making it the most environmentally friendly\nsolution. The system's cumulative reward of 950 reflects its superior\nperformance in balancing multiple objectives. Through rigorous testing and\nablation studies, the paper validates the effectiveness of the DRL model's\narchitecture and parameters, offering a robust solution for energy management\nin data centers. The findings highlight the potential of DRL in advancing\nenergy optimization strategies and addressing sustainability challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21153v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.21074", "title": "Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education", "authors": ["Qian Huang", "Thijs Willems"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.21074v1", "summary": "As generative AI (Gen-AI) tools become more prevalent in education, there is\na growing need to understand how educators, not just students, can actively\nshape their design and use. This study investigates how two instructors\nintegrated four custom GPT tools into a Masters-level Qualitative Research\nMethods course for Urban Planning Policy students. Addressing two key gaps: the\ndominant framing of students as passive AI users, and the limited use of AI in\nqualitative methods education. The study explores how Gen-AI can support\ndisciplinary learning when aligned with pedagogical intent. Drawing on the\nTechnological Pedagogical Content Knowledge (TPACK) framework and action\nresearch methodology, the instructors designed GPTs to scaffold tasks such as\nresearch question formulation, interview practice, fieldnote analysis, and\ndesign thinking. Thematic analysis of student reflections, AI chat logs, and\nfinal assignments revealed that the tools enhanced student reflexivity,\nimproved interview techniques, and supported structured analytic thinking.\nHowever, students also expressed concerns about cognitive overload, reduced\nimmersion in data, and the formulaic nature of AI responses. The study offers\nthree key insights: AI can be a powerful scaffold for active learning when\npaired with human facilitation; custom GPTs can serve as cognitive partners in\niterative research practice; and educator-led design is critical to\npedagogically meaningful AI integration. This research contributes to emerging\nscholarship on AI in higher education by demonstrating how empowering educators\nto design custom tools can promote more reflective, responsible, and\ncollaborative learning with AI.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.21074v1", "cate": "cs.HC", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.21447", "title": "LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests", "authors": ["Zachariah Sollenberger", "Rahul Patel", "Saieda Ali Zada", "Sunita Chandrasekaran"], "categories": ["cs.SE", "cs.ET"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21447v1", "summary": "The usage of Large Language Models (LLMs) for software and test development\nhas continued to increase since LLMs were first introduced, but only recently\nhave the expectations of LLMs become more realistic. Verifying the correctness\nof code generated by LLMs is key to improving their usefulness, but there have\nbeen no comprehensive and fully autonomous solutions developed yet.\nHallucinations are a major concern when LLMs are applied blindly to problems\nwithout taking the time and effort to verify their outputs, and an inability to\nexplain the logical reasoning of LLMs leads to issues with trusting their\nresults. To address these challenges while also aiming to effectively apply\nLLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a\ndiscriminative LLM) and experiments with the usage of LLMs for the generation\nof a large volume of compiler tests. We experimented with a number of LLMs\npossessing varying parameter counts and presented results using ten\ncarefully-chosen metrics that we describe in detail in our narrative. Through\nour findings, it is evident that LLMs possess the promising potential to\ngenerate quality compiler tests and verify them automatically.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21447v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21193", "title": "Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs", "authors": ["Sotiris Chatzimiltis", "Mohammad Shojafar", "Mahdi Boloursaz Mashhadi", "Rahim Tafazolli"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21193v1", "summary": "Next generation Radio Access Networks (RANs) introduce programmability,\nintelligence, and near real-time control through intelligent controllers,\nenabling enhanced security within the RAN and across broader 5G/6G\ninfrastructures. This paper presents a comprehensive survey highlighting\nopportunities, challenges, and research gaps for Large Language Models\n(LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future\nRAN environments. Motivated by this, we propose an LLM interpretable\nanomaly-based detection system for distributed denial-of-service (DDoS) attacks\nusing multivariate time series key performance measures (KPMs), extracted from\nE2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC).\nAn LSTM-based model is trained to identify malicious User Equipment (UE)\nbehavior based on these KPMs. To enhance transparency, we apply post-hoc local\nexplainability methods such as LIME and SHAP to interpret individual\npredictions. Furthermore, LLMs are employed to convert technical explanations\ninto natural-language insights accessible to non-expert users. Experimental\nresults on real 5G network KPMs demonstrate that our framework achieves high\ndetection accuracy (F1-score > 0.96) while delivering actionable and\ninterpretable outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21193v1", "cate": "cs.CR", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21709", "title": "Adaptive Prior Scene-Object SLAM for Dynamic Environments", "authors": ["Haolan Zhang", "Thanh Nguyen Canh", "Chenghao Li", "Nak Young Chong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE The 2025 IEEE International Conference on Real-time Computing and Robotics", "url": "http://arxiv.org/abs/2507.21709v1", "summary": "Visual Simultaneous Localization and Mapping (SLAM) plays a vital role in\nreal-time localization for autonomous systems. However, traditional SLAM\nmethods, which assume a static environment, often suffer from significant\nlocalization drift in dynamic scenarios. While recent advancements have\nimproved SLAM performance in such environments, these systems still struggle\nwith localization drift, particularly due to abrupt viewpoint changes and\npoorly characterized moving objects. In this paper, we propose a novel\nscene-object-based reliability assessment framework that comprehensively\nevaluates SLAM stability through both current frame quality metrics and scene\nchanges relative to reliable reference frames. Furthermore, to tackle the lack\nof error correction mechanisms in existing systems when pose estimation becomes\nunreliable, we employ a pose refinement strategy that leverages information\nfrom reliable frames to optimize camera pose estimation, effectively mitigating\nthe adverse effects of dynamic interference. Extensive experiments on the TUM\nRGB-D datasets demonstrate that our approach achieves substantial improvements\nin localization accuracy and system robustness under challenging dynamic\nscenarios.", "comment": "Accepted by IEEE The 2025 IEEE International Conference on Real-time\n  Computing and Robotics", "pdf_url": "http://arxiv.org/pdf/2507.21709v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21141", "title": "The Geometry of Harmfulness in LLMs through Subconcept Probing", "authors": ["McNair Shah", "Saleena Angeline", "Adhitya Rajendra Kumar", "Naitik Chheda", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien", "Will Cai"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21141v1", "summary": "Recent advances in large language models (LLMs) have intensified the need to\nunderstand and reliably curb their harmful behaviours. We introduce a\nmultidimensional framework for probing and steering harmful content in model\ninternals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate,\nemployment scams, weapons), we learn a linear probe, yielding 55 interpretable\ndirections in activation space. Collectively, these directions span a\nharmfulness subspace that we show is strikingly low-rank. We then test ablation\nof the entire subspace from model internals, as well as steering and ablation\nin the subspace's dominant direction. We find that dominant direction steering\nallows for near elimination of harmfulness with a low decrease in utility. Our\nfindings advance the emerging view that concept subspaces provide a scalable\nlens on LLM behaviour and offer practical tools for the community to audit and\nharden future generations of language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21141v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21155", "title": "SPADE-S: A Sparsity-Robust Foundational Forecaster", "authors": ["Malcolm Wolff", "Matthew Li", "Ravi Kiran Selvam", "Hanjing Zhu", "Kin G. Olivares", "Ruijun Ma", "Abhinav Katoch", "Shankar Ramasubramanian", "Mengfei Cao", "Roberto Bandarra", "Rahul Gopalsamy", "Stefania La Vattiata", "Sitan Yang", "Michael M. Mahoney"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21155v1", "summary": "Despite significant advancements in time series forecasting, accurate\nmodeling of time series with strong heterogeneity in magnitude and/or sparsity\npatterns remains challenging for state-of-the-art deep learning architectures.\nWe identify several factors that lead existing models to systematically\nunderperform on low-magnitude and sparse time series, including loss functions\nwith implicit biases toward high-magnitude series, training-time sampling\nmethods, and limitations of time series encoding methods.\n  SPADE-S is a robust forecasting architecture that significantly reduces\nmagnitude- and sparsity-based systematic biases and improves overall prediction\naccuracy. Empirical results demonstrate that SPADE-S outperforms existing\nstate-of-the-art approaches across a diverse set of use cases in demand\nforecasting. In particular, we show that, depending on the quantile forecast\nand magnitude of the series, SPADE-S can improve forecast accuracy by up to\n15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and\n4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%,\nrespectively, for each of three distinct datasets, ranging from 3 million to\n700 million series, from a large online retailer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21155v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.21077", "title": "Data-Driven and Participatory Approaches toward Neuro-Inclusive AI", "authors": ["Naba Rizvi"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      PhD Dissertation at UC San Diego (June 2025)", "url": "http://arxiv.org/abs/2507.21077v1", "summary": "Biased data representation in AI marginalizes up to 75 million autistic\npeople worldwide through medical applications viewing autism as a deficit of\nneurotypical social skills rather than an aspect of human diversity, and this\nperspective is grounded in research questioning the humanity of autistic\npeople. Turing defined artificial intelligence as the ability to mimic human\ncommunication, and as AI development increasingly focuses on human-like agents,\nthis benchmark remains popular. In contrast, we define Neuro-Inclusive AI as\ndatasets and systems that move away from mimicking humanness as a benchmark for\nmachine intelligence. Then, we explore the origins, prevalence, and impact of\nanti-autistic biases in current research. Our work finds that 90% of human-like\nAI agents exclude autistic perspectives, and AI creators continue to believe\nethical considerations are beyond the scope of their work. To improve the\nautistic representation in data, we conduct empirical experiments with\nannotators and LLMs, finding that binary labeling schemes sufficiently capture\nthe nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can\nbe used to evaluate or fine-tune models, and was developed to serve as a\nfoundation for more neuro-inclusive future work.", "comment": "PhD Dissertation at UC San Diego (June 2025)", "pdf_url": "http://arxiv.org/pdf/2507.21077v1", "cate": "cs.HC", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2507.21448", "title": "Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations", "authors": ["Teng", "Ma", "Sile Yin", "Li-Chia Yang", "Shuo Zhang"], "categories": ["eess.AS", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted into Interspeech 2025", "url": "http://arxiv.org/abs/2507.21448v1", "summary": "Speech enhancement in audio-only settings remains challenging, particularly\nin the presence of interfering speakers. This paper presents a simple yet\neffective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which\nisolates and enhances the on-screen target speaker while suppressing\ninterfering speakers and background noise. We investigate how visual embeddings\nlearned from audio-visual speech recognition (AVSR) and active speaker\ndetection (ASD) contribute to AVSE across different SNR conditions and numbers\nof interfering speakers. Our results show concatenating embeddings from AVSR\nand ASD models provides the greatest improvement in low-SNR, multi-speaker\nenvironments, while AVSR embeddings alone perform best in noise-only scenarios.\nIn addition, we develop a real-time streaming system that operates on a\ncomputer CPU and we provide a video demonstration and code repository. To our\nknowledge, this is the first open-source implementation of a real-time AVSE\nsystem.", "comment": "Accepted into Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.21448v1", "cate": "eess.AS", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21069", "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21069v1", "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21069v1", "cate": "cs.CV", "date": "2025-06-06", "updated": "2025-06-06"}
{"id": "2507.21195", "title": "MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models", "authors": ["Po-Yuan Mao", "Cheng-Chang Tsai", "Chun-Shien Lu"], "categories": ["cs.CR", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21195v1", "summary": "The great success of the diffusion model in image synthesis led to the\nrelease of gigantic commercial models, raising the issue of copyright\nprotection and inappropriate content generation. Training-free diffusion\nwatermarking provides a low-cost solution for these issues. However, the prior\nworks remain vulnerable to rotation, scaling, and translation (RST) attacks.\nAlthough some methods employ meticulously designed patterns to mitigate this\nissue, they often reduce watermark capacity, which can result in identity (ID)\ncollusion. To address these problems, we propose MaXsive, a training-free\ndiffusion model generative watermarking technique that has high capacity and\nrobustness. MaXsive best utilizes the initial noise to watermark the diffusion\nmodel. Moreover, instead of using a meticulously repetitive ring pattern, we\npropose injecting the X-shape template to recover the RST distortions. This\ndesign significantly increases robustness without losing any capacity, making\nID collusion less likely to happen. The effectiveness of MaXsive has been\nverified on two well-known watermarking benchmarks under the scenarios of\nverification and identification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21195v1", "cate": "cs.CR", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21772", "title": "Multi-UAV Deployment in Obstacle-Cluttered Environments with LOS Connectivity", "authors": ["Yuda Chen", "Shuaikang Wang", "Jie Li", "Meng Guo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      iros2025", "url": "http://arxiv.org/abs/2507.21772v1", "summary": "A reliable communication network is essential for multiple UAVs operating\nwithin obstacle-cluttered environments, where limited communication due to\nobstructions often occurs. A common solution is to deploy intermediate UAVs to\nrelay information via a multi-hop network, which introduces two challenges: (i)\nhow to design the structure of multihop networks; and (ii) how to maintain\nconnectivity during collaborative motion. To this end, this work first proposes\nan efficient constrained search method based on the minimumedge RRT? algorithm,\nto find a spanning-tree topology that requires a less number of UAVs for the\ndeployment task. Then, to achieve this deployment, a distributed model\npredictive control strategy is proposed for the online motion coordination. It\nexplicitly incorporates not only the inter-UAV and UAVobstacle distance\nconstraints, but also the line-of-sight (LOS) connectivity constraint. These\nconstraints are well-known to be nonlinear and often tackled by various\napproximations. In contrast, this work provides a theoretical guarantee that\nall agent trajectories are ensured to be collision-free with a teamwise LOS\nconnectivity at all time. Numerous simulations are performed in 3D valley-like\nenvironments, while hardware experiments validate its dynamic adaptation when\nthe deployment position changes online.", "comment": "iros2025", "pdf_url": "http://arxiv.org/pdf/2507.21772v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21158", "title": "Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams", "authors": ["Nishani Fernando", "Bahareh Nakisa", "Adnan Ahmad", "Mohammad Naim Rastgoo"], "categories": ["cs.AI", "cs.HC", "H.1.2; I.2.6; I.2.4"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      15 pages, 1 figure, Accepted to MAI-XAI@ECAI2025", "url": "http://arxiv.org/abs/2507.21158v1", "summary": "Effective human-AI teaming heavily depends on swift trust, particularly in\nhigh-stakes scenarios such as emergency response, where timely and accurate\ndecision-making is critical. In these time-sensitive and cognitively demanding\nsettings, adaptive explainability is essential for fostering trust between\nhuman operators and AI systems. However, existing explainable AI (XAI)\napproaches typically offer uniform explanations and rely heavily on explicit\nfeedback mechanisms, which are often impractical in such high-pressure\nscenarios. To address this gap, we propose a conceptual framework for adaptive\nXAI that operates non-intrusively by responding to users' real-time cognitive\nand emotional states through implicit feedback, thereby enhancing swift trust\nin high-stakes environments. The proposed adaptive explainability trust\nframework (AXTF) leverages physiological and behavioral signals, such as EEG,\nECG, and eye tracking, to infer user states and support explanation adaptation.\nAt its core is a multi-objective, personalized trust estimation model that maps\nworkload, stress, and emotion to dynamic trust estimates. These estimates guide\nthe modulation of explanation features enabling responsive and personalized\nsupport that promotes swift trust in human-AI collaboration. This conceptual\nframework establishes a foundation for developing adaptive, non-intrusive XAI\nsystems tailored to the rigorous demands of high-pressure, time-sensitive\nenvironments.", "comment": "15 pages, 1 figure, Accepted to MAI-XAI@ECAI2025", "pdf_url": "http://arxiv.org/pdf/2507.21158v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21160", "title": "Handling Out-of-Distribution Data: A Survey", "authors": ["Lakpa Tamang", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "categories": ["cs.LG", "cs.AI", "68T07 (Primary), 68T45, 68T10 (Secondary)", "I.5.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages, 6 figures, 6 tables. Accepted at IEEE Transactions on Knowledge and Data Engineering", "url": "http://arxiv.org/abs/2507.21160v1", "summary": "In the field of Machine Learning (ML) and data-driven applications, one of\nthe significant challenge is the change in data distribution between the\ntraining and deployment stages, commonly known as distribution shift. This\npaper outlines different mechanisms for handling two main types of distribution\nshifts: (i) Covariate shift: where the value of features or covariates change\nbetween train and test data, and (ii) Concept/Semantic-shift: where model\nexperiences shift in the concept learned during training due to emergence of\nnovel classes in the test phase. We sum up our contributions in three folds.\nFirst, we formalize distribution shifts, recite on how the conventional method\nfails to handle them adequately and urge for a model that can simultaneously\nperform better in all types of distribution shifts. Second, we discuss why\nhandling distribution shifts is important and provide an extensive review of\nthe methods and techniques that have been developed to detect, measure, and\nmitigate the effects of these shifts. Third, we discuss the current state of\ndistribution shift handling mechanisms and propose future research directions\nin this area. Overall, we provide a retrospective synopsis of the literature in\nthe distribution shift, focusing on OOD data that had been overlooked in the\nexisting surveys.", "comment": "20 pages, 6 figures, 6 tables. Accepted at IEEE Transactions on\n  Knowledge and Data Engineering", "pdf_url": "http://arxiv.org/pdf/2507.21160v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21078", "title": "What Makes a Level Hard in Super Mario Maker 2?", "authors": ["Carlo A. Furia", "Andrea Mocci"], "categories": ["cs.HC", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21078v1", "summary": "Games like Super Mario Maker 2 (SMM2) lower the barrier for casual users to\nbecome level designers. In this paper, we set out to analyze a vast amount of\ndata about SMM2 user-written levels, in order to understand what factors affect\na level's difficulty as experienced by other users. To this end, we perform two\nkinds of analyses: one based on regression models and one using natural\nlanguage processing techniques. The main results shed light on which level\ncharacteristics (e.g., its style, popularity, timing) and which topics and\nsentiments have a consistent association with easier or harder levels. While\nnone of our findings are startling, they help distill some key differences\nbetween easy and hard SMM2 levels, which, in turn, can pave the way for a\nbetter understanding of end-user level design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21078v1", "cate": "cs.HC", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.21937", "title": "A Grover-Based Quantum Algorithm for Solving Perfect Mazes via Fitness-Guided Search", "authors": ["Michelle L. Wu"], "categories": ["quant-ph", "cs.ET", "math.QA", "81P68, 68Q12", "F.1.2; F.2.2; I.2.8"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      11 pages. Submitted under MIT-LL & MITRE Corporation 2025. Theoretical proposal; future work will extend to implementation and benchmarking", "url": "http://arxiv.org/abs/2507.21937v2", "summary": "We present a quantum algorithm for solving perfect mazes by casting the\npathfinding task as a structured search problem. Building on Grover's amplitude\namplification, the algorithm encodes all candidate paths in superposition and\nevaluates their proximity to the goal using a reversible fitness operator based\non quantum arithmetic. A Grover-compatible oracle marks high-fitness states,\nand an adaptive cutoff strategy refines the search iteratively. We provide\nformal definitions, unitary constructions, and convergence guarantees, along\nwith a resource analysis showing efficient scaling with maze size and path\nlength. The framework serves as a foundation for quantum-hybrid pathfinding and\nplanning. The full algorithmic pipeline is specified from encoding to\namplification, including oracle design and fitness evaluation. The approach is\nreadily extensible to other search domains, including navigation over tree-like\nor acyclic graphs.", "comment": "This revision addresses privacy concerns, intellectual property\n  rights, and removes personally identifiable information along with\n  institutionally affiliated acknowledgements. No changes were made to the\n  scientific and mathematical contents", "pdf_url": "http://arxiv.org/pdf/2507.21937v2", "cate": "quant-ph", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21161", "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE 3rd International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "url": "http://arxiv.org/abs/2507.21161v1", "summary": "Pedestrian intention prediction is essential for autonomous driving in\ncomplex urban environments. Conventional approaches depend on supervised\nlearning over frame sequences and require extensive retraining to adapt to new\nscenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention\nPrediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing\nintentions directly from short, continuous video clips enriched with structured\nJAAD metadata. In contrast to GPT-4V based methods that operate on discrete\nframes, BF-PIP processes uninterrupted temporal clips. It also incorporates\nbounding-box annotations and ego-vehicle speed via specialized multimodal\nprompts. Without any additional training, BF-PIP achieves 73% prediction\naccuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate\nthat combining temporal video inputs with contextual cues enhances\nspatiotemporal perception and improves intent inference under ambiguous\nconditions. This approach paves the way for agile, retraining-free perception\nmodule in intelligent transportation system.", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "pdf_url": "http://arxiv.org/pdf/2507.21161v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21271", "title": "Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement", "authors": ["Zhaorui Yang", "Yuxin Qiu", "Haichao Zhu", "Qian Zhang"], "categories": ["cs.SE", "K.6.3"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      ICSME 2025 Registered Reports", "url": "http://arxiv.org/abs/2507.21271v1", "summary": "[Context] Modern AI applications increasingly process highly structured data,\nsuch as 3D meshes and point clouds, where test input generation must preserve\nboth structural and semantic validity. However, existing fuzzing tools and\ninput generators are typically handcrafted for specific input types and often\ngenerate invalid inputs that are subsequently discarded, leading to\ninefficiency and poor generalizability. [Objective] This study investigates\nwhether test inputs for structured domains can be unified through a graph-based\nrepresentation, enabling general, reusable mutation strategies while enforcing\nstructural constraints. We will evaluate the effectiveness of this approach in\nenhancing input validity and semantic preservation across eight AI systems.\n[Method] We develop and evaluate GRAphRef, a graph-based test input generation\nframework that supports constraint-based mutation and refinement. GRAphRef maps\nstructured inputs to graphs, applies neighbor-similarity-guided mutations, and\nuses a constraint-refinement phase to repair invalid inputs. We will conduct a\nconfirmatory study across eight real-world mesh-processing AI systems,\ncomparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.\nEvaluation metrics include structural validity, semantic preservation (via\nprediction consistency), and performance overhead. Experimental data is derived\nfrom ShapeNetCore mesh seeds and model outputs from systems like MeshCNN and\nHodgeNet. Statistical analysis and component latency breakdowns will be used to\nassess each hypothesis.", "comment": "ICSME 2025 Registered Reports", "pdf_url": "http://arxiv.org/pdf/2507.21271v1", "cate": "cs.SE", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21258", "title": "Verification Cost Asymmetry in Cognitive Warfare: A Complexity-Theoretic Framework", "authors": ["Joshua Luberisse"], "categories": ["cs.CR", "cs.CC", "cs.CY", "cs.GT", "F.0; H.0"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21258v1", "summary": "Human verification under adversarial information flow operates as a\ncost-bounded decision procedure constrained by working memory limits and\ncognitive biases. We introduce the Verification Cost Asymmetry (VCA)\ncoefficient, formalizing it as the ratio of expected verification work between\npopulations under identical claim distributions. Drawing on probabilistically\ncheckable proofs (PCP) and parameterized complexity theory, we construct\ndissemination protocols that reduce verification for trusted audiences to\nconstant human effort while imposing superlinear costs on adversarial\npopulations lacking cryptographic infrastructure. We prove theoretical\nguarantees for this asymmetry, validate the framework through controlled user\nstudies measuring verification effort with and without spot-checkable\nprovenance, and demonstrate practical encoding of real-world information\ncampaigns. The results establish complexity-theoretic foundations for\nengineering democratic advantage in cognitive warfare, with immediate\napplications to content authentication, platform governance, and information\noperations doctrine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21258v1", "cate": "cs.CR", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21796", "title": "MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects", "authors": ["Yuying Zhang", "Kevin Sebastian Luck", "Francesco Verdoja", "Ville Kyrki", "Joni Pajarinen"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21796v1", "summary": "Mobile manipulation is a critical capability for robots operating in diverse,\nreal-world environments. However, manipulating deformable objects and materials\nremains a major challenge for existing robot learning algorithms. While various\nbenchmarks have been proposed to evaluate manipulation strategies with rigid\nobjects, there is still a notable lack of standardized benchmarks that address\nmobile manipulation tasks involving deformable objects.\n  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation\nDeformable Object task suite, designed specifically for robot learning.\nMoDeSuite consists of eight distinct mobile manipulation tasks covering both\nelastic objects and deformable objects, each presenting a unique challenge\ninspired by real-world robot applications. Success in these tasks requires\neffective collaboration between the robot's base and manipulator, as well as\nthe ability to exploit the deformability of the objects. To evaluate and\ndemonstrate the use of the proposed benchmark, we train two state-of-the-art\nreinforcement learning algorithms and two imitation learning algorithms,\nhighlighting the difficulties encountered and showing their performance in\nsimulation. Furthermore, we demonstrate the practical relevance of the suite by\ndeploying the trained policies directly into the real world with the Spot\nrobot, showcasing the potential for sim-to-real transfer. We expect that\nMoDeSuite will open a novel research domain in mobile manipulation involving\ndeformable objects. Find more details, code, and videos at\nhttps://sites.google.com/view/modesuite/home.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21796v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21162", "title": "Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems", "authors": ["Xu Yang", "Chenhui Lin", "Yue Yang", "Qi Wang", "Haotian Liu", "Haizhou Hua", "Wenchuan Wu"], "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21162v1", "summary": "The increasing penetration of distributed energy resources into active\ndistribution networks (ADNs) has made effective ADN dispatch imperative.\nHowever, the numerous newly-integrated ADN operators, such as distribution\nsystem aggregators, virtual power plant managers, and end prosumers, often lack\nspecialized expertise in power system operation, modeling, optimization, and\nprogramming. This knowledge gap renders reliance on human experts both costly\nand time-intensive. To address this challenge and enable intelligent, flexible\nADN dispatch, this paper proposes a large language model (LLM) powered\nautomated modeling and optimization approach. First, the ADN dispatch problems\nare decomposed into sequential stages, and a multi-LLM coordination\narchitecture is designed. This framework comprises an Information Extractor, a\nProblem Formulator, and a Code Programmer, tasked with information retrieval,\noptimization problem formulation, and code implementation, respectively.\nAfterwards, tailored refinement techniques are developed for each LLM agent,\ngreatly improving the accuracy and reliability of generated content. The\nproposed approach features a user-centric interface that enables ADN operators\nto derive dispatch strategies via simple natural language queries, eliminating\ntechnical barriers and increasing efficiency. Comprehensive comparisons and\nend-to-end demonstrations on various test cases validate the effectiveness of\nthe proposed architecture and methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21162v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21164", "title": "OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection", "authors": ["Nicolas Pinon", "Carole Lartizien"], "categories": ["cs.LG", "cs.AI", "eess.IV", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21164v1", "summary": "Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled\ndata, a necessity in many machine learning applications where anomalous samples\nare rare or not available. Most state-of-the-art methods fall into two\ncategories: reconstruction-based approaches, which often reconstruct anomalies\ntoo well, and decoupled representation learning with density estimators, which\ncan suffer from suboptimal feature spaces. While some recent methods attempt to\ncouple feature learning and anomaly detection, they often rely on surrogate\nobjectives, restrict kernel choices, or introduce approximations that limit\ntheir expressiveness and robustness. To address this challenge, we propose a\nnovel method that tightly couples representation learning with an analytically\nsolvable one-class SVM (OCSVM), through a custom loss formulation that directly\naligns latent features with the OCSVM decision boundary. The model is evaluated\non two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI\nsubtle lesion detection task. Unlike most methods that focus on large,\nhyperintense lesions at the image level, our approach succeeds to target small,\nnon-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a\nmore clinically relevant scenario. Both experiments evaluate a form of\nrobustness to domain shifts, including corruption types in MNIST-C and\nscanner/age variations in MRI. Results demonstrate performance and robustness\nof our proposed mode,highlighting its potential for general UAD and real-world\nmedical imaging applications. The source code is available at\nhttps://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21164v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21079", "title": "Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention", "authors": ["Joe Hasei", "Yosuke Matsumoto", "Hiroki Kawai", "Yuko Okahisa", "Manabu Takaki", "Toshifumi Ozaki"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21079v1", "summary": "This study assessed metaverse-based support groups designed to reduce social\nisolation and suicide risk among LGBTQ+ youths. Using the Cluster platform,\nenhanced anonymity, avatar-based self-expression, and accessibility were\nprovided. Key findings showed that 79.2% chose avatars matching their gender\nidentity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean:\n1.79/5). Social confidence significantly improved in virtual spaces compared to\nreal-world interactions (p<0.001), particularly among participants with\ninitially low confidence, averaging an increase of 2.08 points. About half of\nthe first-time participants were 16 or younger, highlighting potential for\nearly intervention. The metaverse scored higher than real-world environments\nfor safety/privacy (3.94/5), self-expression (4.02/5), and accessibility\n(4.21/5). Additionally, 73.6% reported feeling more accepted virtually.\nHowever, some highly confident individuals offline experienced mild adaptation\nchallenges, averaging a confidence decrease of 0.58 points, indicating virtual\nsupport complements rather than replaces in-person services. These findings\nsuggest metaverse-based support effectively lowers psychological barriers and\nprovides affirming spaces, potentially reducing severe outcomes such as\nsuicidal ideation. Future studies should focus on integrating virtual support\nwith existing community and clinical frameworks to enhance long-term impacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21079v1", "cate": "cs.HC", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2507.21956", "title": "A Novel Framework for Near-Field Covert Communications with RIS and RSMA", "authors": ["Atiquzzaman Mondal", "Amira Bendaimi", "Huseyin Arslan"], "categories": ["eess.SP", "cs.ET"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12 pages, 7 figures, IEEE Transactions on Communications", "url": "http://arxiv.org/abs/2507.21956v1", "summary": "This paper explores the near field (NF) covert communication with the aid of\nrate-splitting multiple access (RSMA) and reconfigurable intelligent surfaces\n(RIS). In particular, the RIS operates in the NF of both the legitimate user\nand the passive adversary, enhancing the legitimate users received signal while\nsuppressing the adversarys detection capability. Whereas, the base station (BS)\napplies RSMA to increase the covert communication rate composed of a private\nand a shared rate component. To characterize system covertness, we derive\nclosed form expressions for the detection error probability (DEP), outage\nprobability (OP), and optimal detection threshold for the adversary. We\nformulate a non-convex joint beamforming optimization problem at the BS and RIS\nunder unit-modulus constraints to maximize the covert rate. To tackle this, we\npropose an alternating optimization (AO) algorithm, where the BS beamformer is\ndesigned using a two-stage iterative method based on successive convex\napproximation (SCA). Additionally, two low-complexity techniques are introduced\nto further reduce the adversarys received power. Simulation results demonstrate\nthat the proposed algorithm effectively improves the covert communication rate,\nhighlighting the potential of near field RSMA-RIS integration in covert\ncommunication.", "comment": "12 pages, 7 figures, IEEE Transactions on Communications", "pdf_url": "http://arxiv.org/pdf/2507.21956v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21167", "title": "ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions", "authors": ["Donglu Yang", "Liang Zhang", "Zihao Yue", "Liangyu Chen", "Yichen Xu", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21167v2", "summary": "Charts are a fundamental visualization format widely used in data analysis\nacross research and industry. While enabling users to edit charts based on\nhigh-level intentions is of great practical value, existing methods primarily\nrely on natural language instructions, which are often too ambiguous to support\nfine-grained editing. In this work, we introduce a novel paradigm for\nmultimodal chart editing, where user intent is expressed through a combination\nof natural language and visual indicators that explicitly highlight the\nelements to be modified. To support this paradigm, we present\nChart$\\text{M}^3$, a new benchmark for Multimodal chart editing with\nMulti-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$\ncontains 1,000 samples spanning four levels of editing difficulty. Each sample\nincludes triplets in the form of (chart, code, multimodal instructions). To\ncomprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides\nmetrics that assess both visual appearance and code correctness. Our benchmark\nreveals significant limitations in current multimodal large language models\n(MLLMs), including GPT-4o, particularly in their ability to interpret and act\non visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a\nlarge-scale training set with 24,000 multimodal chart editing samples.\nFine-tuning MLLMs on this dataset leads to substantial improvements,\ndemonstrating the importance of multimodal supervision in building practical\nchart editing systems. Our datasets, codes, and evaluation tools are available\nat https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our\ndatasets, codes, and evaluation tools are available at\nhttps://github.com/yaolinli/VCE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21167v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-30"}
{"id": "2507.21280", "title": "\"Maybe We Need Some More Examples:\" Individual and Team Drivers of Developer GenAI Tool Use", "authors": ["Courtney Miller", "Rudrajit Choudhuri", "Mara Ulloa", "Sankeerti Haniyur", "Robert DeLine", "Margaret-Anne Storey", "Emerson Murphy-Hill", "Christian Bird", "Jenna L. Butler"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21280v1", "summary": "Despite the widespread availability of generative AI tools in software\nengineering, developer adoption remains uneven. This unevenness is problematic\nbecause it hampers productivity efforts, frustrates management's expectations,\nand creates uncertainty around the future roles of developers. Through paired\ninterviews with 54 developers across 27 teams -- one frequent and one\ninfrequent user per team -- we demonstrate that differences in usage result\nprimarily from how developers perceive the tool (as a collaborator vs.\nfeature), their engagement approach (experimental vs. conservative), and how\nthey respond when encountering challenges (with adaptive persistence vs. quick\nabandonment). Our findings imply that widespread organizational expectations\nfor rapid productivity gains without sufficient investment in learning support\ncreates a \"Productivity Pressure Paradox,\" undermining the very productivity\nbenefits that motivate adoption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21280v1", "cate": "cs.SE", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21387", "title": "Radio Adversarial Attacks on EMG-based Gesture Recognition Networks", "authors": ["Hongyi Xie"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21387v1", "summary": "Surface electromyography (EMG) enables non-invasive human-computer\ninteraction in rehabilitation, prosthetics, and virtual reality. While deep\nlearning models achieve over 97% classification accuracy, their vulnerability\nto adversarial attacks remains largely unexplored in the physical domain. We\npresent ERa Attack, the first radio frequency (RF) adversarial method targeting\nEMG devices through intentional electromagnetic interference (IEMI). Using\nlow-power software-defined radio transmitters, attackers inject optimized RF\nperturbations to mislead downstream models. Our approach bridges digital and\nphysical domains: we generate adversarial perturbations using Projected\nGradient Descent, extract 50-150 Hz components via inverse STFT, and employ\nsynchronization-free strategies (constant spectrum noise or narrowband\nmodulation). Perturbations, constrained to 1-10% of signal amplitude, are\namplitude-modulated onto 433 MHz carriers. Experiments on the Myo Dataset (7\ngestures, 350 samples) demonstrate significant impact: at 1 meter and 0 dBm\ntransmission power, classification accuracy drops from 97.8% to 58.3%, with\n41.7% misclassification rate and 25.6% targeted attack success rate. Attack\neffectiveness decreases exponentially with distance, recovering to 85% accuracy\nat 3 meters. Increasing power to 10 dBm reduces accuracy by an additional 15%\nat 1 meter. This work pioneers RF-based adversarial attacks on EMG recognition\nsystems, revealing critical vulnerabilities in safety-critical applications. We\nquantify attack effectiveness across different perturbation modes and\ndistances, and propose defenses including hardware shielding, spectrum\nmonitoring, and adversarial training. Our findings inform the design of robust\nEMG systems against electromagnetic threats.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21387v1", "cate": "cs.CR", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21814", "title": "Interactive Adversarial Testing of Autonomous Vehicles with Adjustable Confrontation Intensity", "authors": ["Yicheng Guo", "Chengkai Xu", "Jiaqi Liu", "Hao Zhang", "Peng Hang", "Jian Sun"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21814v1", "summary": "Scientific testing techniques are essential for ensuring the safe operation\nof autonomous vehicles (AVs), with high-risk, highly interactive scenarios\nbeing a primary focus. To address the limitations of existing testing methods,\nsuch as their heavy reliance on high-quality test data, weak interaction\ncapabilities, and low adversarial robustness, this paper proposes ExamPPO, an\ninteractive adversarial testing framework that enables scenario-adaptive and\nintensity-controllable evaluation of autonomous vehicles. The framework models\nthe Surrounding Vehicle (SV) as an intelligent examiner, equipped with a\nmulti-head attention-enhanced policy network, enabling context-sensitive and\nsustained behavioral interventions. A scalar confrontation factor is introduced\nto modulate the intensity of adversarial behaviors, allowing continuous,\nfine-grained adjustment of test difficulty. Coupled with structured evaluation\nmetrics, ExamPPO systematically probes AV's robustness across diverse scenarios\nand strategies. Extensive experiments across multiple scenarios and AV\nstrategies demonstrate that ExamPPO can effectively modulate adversarial\nbehavior, expose decision-making weaknesses in tested AVs, and generalize\nacross heterogeneous environments, thereby offering a unified and reproducible\nsolution for evaluating the safety and intelligence of autonomous\ndecision-making systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21814v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21171", "title": "An ontological analysis of risk in Basic Formal Ontology", "authors": ["Federico Donato", "Adrien Barton"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      7 pages. 2 figures. Conference: Semantic Technology for Intelligence, Defense, and Security (STIDS 2024)", "url": "http://arxiv.org/abs/2507.21171v1", "summary": "The paper explores the nature of risk, providing a characterization using the\ncategories of the Basic Formal Ontology (BFO). It argues that the category Risk\nis a subclass of BFO:Role, contrasting it with a similar view classifying Risk\nas a subclass of BFO:Disposition. This modeling choice is applied on one\nexample of risk, which represents objects, processes (both physical and mental)\nand their interrelations, then generalizing from the instances in the example\nto obtain an overall analysis of risk, making explicit what are the sufficient\nconditions for being a risk. Plausible necessary conditions are also mentioned\nfor future work. Index Terms: ontology, risk, BFO, role, disposition", "comment": "7 pages. 2 figures. Conference: Semantic Technology for Intelligence,\n  Defense, and Security (STIDS 2024)", "pdf_url": "http://arxiv.org/pdf/2507.21171v1", "cate": "cs.AI", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21166", "title": "AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation", "authors": ["Ren Zhuang", "Ben Wang", "Shuifa Sun"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21166v1", "summary": "Progress in complex reasoning is constrained by the static nature of the\ncurrent training datasets. We propose structured interaction as a new scaling\naxis, moving beyond the prevailing paradigm of increasing model parameters. Our\nself-evolving framework, AGORA, enables a collaborative ensemble to achieve\nreasoning performance exceeding state-of-the-art monolithic systems by up to\n4.45 percentage points on challenging mathematical benchmarks. This gain stems\nfrom group emergent ability-the synthesis of collective capabilities\nunattainable by isolated models, validating interaction as a scalable driver of\nintelligence. Our results position the engineering of collaborative ecosystems\nas a vital frontier for capability emergence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21166v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21081", "title": "Empathy in Explanation", "authors": ["Katherine M. Collins", "Kartik Chandra", "Adrian Weller", "Jonathan Ragan-Kelley", "Joshua B. Tenenbaum"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      CogSci non-archival conference paper", "url": "http://arxiv.org/abs/2507.21081v1", "summary": "Why do we give the explanations we do? Recent work has suggested that we\nshould think of explanation as a kind of cooperative social interaction,\nbetween a why-question-asker and an explainer. Here, we apply this perspective\nto consider the role that emotion plays in this social interaction. We develop\na computational framework for modeling explainers who consider the emotional\nimpact an explanation might have on a listener. We test our framework by using\nit to model human intuitions about how a doctor might explain to a patient why\nthey have a disease, taking into account the patient's propensity for regret.\nOur model predicts human intuitions well, better than emotion-agnostic\nablations, suggesting that people do indeed reason about emotion when giving\nexplanations.", "comment": "CogSci non-archival conference paper", "pdf_url": "http://arxiv.org/pdf/2507.21081v1", "cate": "cs.HC", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.21984", "title": "Higher-Order Kuramoto Oscillator Network for Dense Associative Memory", "authors": ["Jona Nagerl", "Natalia G. Berloff"], "categories": ["nlin.AO", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Adaptation and Self-Organizing Systems (nlin.AO)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures", "url": "http://arxiv.org/abs/2507.21984v1", "summary": "Networks of phase oscillators can serve as dense associative memories if they\nincorporate higher-order coupling beyond the classical Kuramoto model's\npairwise interactions. Here we introduce a generalized Kuramoto model with\ncombined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling,\ninspired by dense Hopfield memory theory. Using mean-field theory and its\ndynamical approximation, we obtain a phase diagram for dense associative memory\nmodel that exhibits a tricritical point at which the continuous onset of memory\nretrieval is supplanted by a discontinuous, hysteretic transition. In the\nquartic-dominated regime, the system supports bistable phase-locked states\ncorresponding to stored memory patterns, with a sizable energy barrier between\nmemory and incoherent states. We analytically determine this bistable region\nand show that the escape time from a memory state (due to noise) grows\nexponentially with network size, indicating robust storage. Extending the\ntheory to finite memory load, we show that higher-order couplings achieve\nsuperlinear scaling of memory capacity with system size, far exceeding the\nlimit of pairwise-only oscillators. Large-scale simulations of the oscillator\nnetwork confirm our theoretical predictions, demonstrating rapid pattern\nretrieval and robust storage of many phase patterns. These results bridge the\nKuramoto synchronization with modern Hopfield memories, pointing toward\nexperimental realization of high-capacity, analog associative memory in\noscillator systems.", "comment": "13 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.21984v1", "cate": "nlin.AO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21246", "title": "On Explaining Visual Captioning with Hybrid Markov Logic Networks", "authors": ["Monika Shah", "Somdeb Sarkhel", "Deepak Venugopal"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21246v1", "summary": "Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks\nsuch as image captioning. However, explaining/interpreting how these models\nintegrate visual information, language information and knowledge representation\nto generate meaningful captions remains a challenging problem. Standard metrics\nto measure performance typically rely on comparing generated captions with\nhuman-written ones that may not provide a user with a deep insights into this\nintegration. In this work, we develop a novel explanation framework that is\neasily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language\nthat can combine symbolic rules with real-valued functions - where we\nhypothesize how relevant examples from the training data could have influenced\nthe generation of the observed caption. To do this, we learn a HMLN\ndistribution over the training instances and infer the shift in distributions\nover these instances when we condition on the generated sample which allows us\nto quantify which examples may have been a source of richer information to\ngenerate the observed caption. Our experiments on captions generated for\nseveral state-of-the-art captioning models using Amazon Mechanical Turk\nillustrate the interpretability of our explanations, and allow us to compare\nthese models along the dimension of explainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21246v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21318", "title": "Black-Box Bug-Amplification for Multithreaded Software", "authors": ["Yeshayahu Weiss", "Gal Amram", "Achiya Elyasaf", "Eitan Farchi", "Oded Margalit", "Gera Weiss"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      35 pages, 5 figurs, 4 listing and 3 tables", "url": "http://arxiv.org/abs/2507.21318v1", "summary": "Bugs, especially those in concurrent systems, are often hard to reproduce\nbecause they manifest only under rare conditions. Testers frequently encounter\nfailures that occur only under specific inputs, even when occurring with low\nprobability. We propose an approach to systematically amplify the occurrence of\nsuch elusive bugs. We treat the system under test as a black-box and use\nrepeated trial executions to train a predictive model that estimates the\nprobability of a given input configuration triggering a bug. We evaluate this\napproach on a dataset of 17 representative concurrency bugs spanning diverse\ncategories. Several model-based search techniques are compared against a\nbrute-force random sampling baseline. Our results show that an ensemble of\nregression models can significantly increase bug occurrence rates across nearly\nall scenarios, often achieving an order-of-magnitude improvement over random\nsampling. The contributions of this work include: (i) a novel formulation of\nbug-amplification as a rare-event regression problem; (ii) an empirical\nevaluation of multiple techniques for amplifying bug occurrence, demonstrating\nthe effectiveness of model-guided search; and (iii) a practical, non-invasive\ntesting framework that helps practitioners expose hidden concurrency faults\nwithout altering the internal system architecture.", "comment": "35 pages, 5 figurs, 4 listing and 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.21318v1", "cate": "cs.SE", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21187", "title": "Half-life of Youtube News Videos: Diffusion Dynamics and Predictive Factors", "authors": ["Anahit Sargsyan", "Hridoy Sankar Dutta", "Juergen Pfeffer"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      To be published in International Conference on Machine Learning, Optimization, and Data Science (LOD 2025)", "url": "http://arxiv.org/abs/2507.21187v1", "summary": "Consumption of YouTube news videos significantly shapes public opinion and\npolitical narratives. While prior works have studied the longitudinal\ndissemination dynamics of YouTube News videos across extended periods, limited\nattention has been paid to the short-term trends. In this paper, we investigate\nthe early-stage diffusion patterns and dispersion rate of news videos on\nYouTube, focusing on the first 24 hours. To this end, we introduce and analyze\na rich dataset of over 50,000 videos across 75 countries and six continents. We\nprovide the first quantitative evaluation of the 24-hour half-life of YouTube\nnews videos as well as identify their distinct diffusion patterns. According to\nthe findings, the average 24-hour half-life is approximately 7 hours, with\nsubstantial variance both within and across countries, ranging from as short as\n2 hours to as long as 15 hours. Additionally, we explore the problem of\npredicting the latency of news videos' 24-hour half-lives. Leveraging the\npresented datasets, we train and contrast the performance of 6 different models\nbased on statistical as well as Deep Learning techniques. The difference in\nprediction results across the models is traced and analyzed. Lastly, we\ninvestigate the importance of video- and channel-related predictors through\nExplainable AI (XAI) techniques. The dataset, analysis codebase and the trained\nmodels are released at http://bit.ly/3ILvTLU to facilitate further research in\nthis area.", "comment": "To be published in International Conference on Machine Learning,\n  Optimization, and Data Science (LOD 2025)", "pdf_url": "http://arxiv.org/pdf/2507.21187v1", "cate": "cs.SI", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21398", "title": "Digital identity management system with blockchain:An implementation with Ethereum and Ganache", "authors": ["André Davi Lopes", "Tais Mello", "Wesley dos Reis Bezerra"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21398v1", "summary": "This paper presents the development of a distributed digital identity system\nutilizing modern technologies, including FastAPI, MongoDB, gRPC, Docker, and\nblockchain simulation with Ganache and Ethereum. The objective is to\ndemonstrate the benefits of distributed systems and blockchain for the\nsecurity, traceability, and decentralization of digital identities. The\nmethodology included the development of a microservices architecture with JWT\nauthentication, data persistence in MongoDB, simulation of blockchain\noperations using Ganache, and containerization with Docker. The results\ndemonstrate the feasibility of the proposed approach, with a functional web\ninterface, complete audit logs, and blockchain simulation with Ethereum. The\ntheoretical foundations, technical implementation, results obtained, and\nprospects for integration with real blockchain networks are discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21398v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21859", "title": "Evaluating Interactions between Automated Vehicles and Cyclists using a coupled In-the-Loop Test Environment", "authors": ["Michael Kaiser", "Clemens Groß", "Lisa Marie Otto", "Steffen Müller"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21859v1", "summary": "Testing and evaluating automated driving systems (ADS) in interactions with\nvulnerable road users (VRUs), such as cyclists, are essential for improving the\nsafety of VRUs, but often lack realism. This paper presents and validates a\ncoupled in-the-loop test environment that integrates a Cyclist-in-the Loop test\nbench with a Vehicle-in-the-Loop test bench via a virtual environment (VE)\ndeveloped in Unreal Engine 5. The setup enables closed-loop, bidirectional\ninteraction between a real human cyclist and a real automated vehicle under\nsafe and controllable conditions. The automated vehicle reacts to cyclist\ngestures via stimulated camera input, while the cyclist, riding a stationary\nbicycle, perceives and reacts to the vehicle in the VE in real time. Validation\nexperiments are conducted using a real automated shuttle bus with a\ntrack-and-follow function, performing three test maneuvers - straight-line\ndriving with stop, circular track driving, and double lane change - on a\nproving ground and in the coupled in-the-loop test environment. The performance\nis evaluated by comparing the resulting vehicle trajectories in both\nenvironments. Additionally, the introduced latencies of individual components\nin the test setup are measured. The results demonstrate the feasibility of the\napproach and highlight its strengths and limitations for realistic ADS\nevaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21859v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21172", "title": "Ontological Foundations of State Sovereignty", "authors": ["John Beverley", "Danielle Limbaugh"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      6 pages. 0 figures. Conference: Semantic Technology for Intelligence, Defense, and Security (STIDS 2024)", "url": "http://arxiv.org/abs/2507.21172v1", "summary": "This short paper is a primer on the nature of state sovereignty and the\nimportance of claims about it. It also aims to reveal (merely reveal) a\nstrategy for working with vague or contradictory data about which states, in\nfact, are sovereign. These goals together are intended to set the stage for\napplied work in ontology about international affairs.", "comment": "6 pages. 0 figures. Conference: Semantic Technology for Intelligence,\n  Defense, and Security (STIDS 2024)", "pdf_url": "http://arxiv.org/pdf/2507.21172v1", "cate": "cs.AI", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21179", "title": "LLM-Adapted Interpretation Framework for Machine Learning Models", "authors": ["Yuqi Jin", "Zihan Hu", "Weiteng Zhang", "Weihao Xie", "Jianwei Shuai", "Xian Shen", "Zhen Feng"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures, 2 tables", "url": "http://arxiv.org/abs/2507.21179v1", "summary": "Background & Aims: High-performance machine learning models like XGBoost are\noften \"black boxes,\" limiting their clinical adoption due to a lack of\ninterpretability. This study aims to bridge the gap between predictive accuracy\nand narrative transparency for sarcopenia risk assessment. Methods: We propose\nthe LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge\ndistillation architecture. LAI-ML transforms feature attributions from a\ntrained XGBoost model into a probabilistic format using specialized techniques\n(HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement\nlearning loop and case-based retrieval, then generates data-faithful diagnostic\nnarratives. Results: The LAI-ML framework achieved 83% prediction accuracy,\nsignificantly outperforming the baseline XGBoost model, 13% higher. Notably,\nthe LLM not only replicated the teacher model's logic but also corrected its\npredictions in 21.7% of discordant cases, demonstrating enhanced reasoning.\nConclusion: LAI-ML effectively translates opaque model predictions into\ntrustworthy and interpretable clinical insights, offering a deployable solution\nto the \"black-box\" problem in medical AI.", "comment": "11 pages, 8 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.21179v1", "cate": "cs.LG", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21088", "title": "Eliciting User Requirements for AI-Enhanced Learning Environments using a Participatory Approach", "authors": ["Bibeg Limbu", "Irene-Angelica Chounta", "Vilma Sukacke", "Andromachi Filippidi", "Chara Spyropoulou", "Marianna Anagnostopoulou", "Eleftheria Tsourlidaki", "Nikos Karacapilidis"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures, 15th International Conference on Methodologies and Intelligent Systems for Technology Enhanced Learning (mis4tel), Workshop Track: Workshop on Integration of Emerging Technologies into Education and Training (ETELT) this https URL , accepted", "url": "http://arxiv.org/abs/2507.21088v2", "summary": "This paper explores the needs and expectations of educational stakeholders\nfor AI (Artificial Intelligence)-enhanced learning environments. Data was\ncollected following two-phased participatory workshops. The first workshop\noutlined stakeholders' profiles in terms of technical and pedagogical\ncharacteristics. The qualitative data collected was analysed using deductive\nthematic analysis with Activity Theory, explicating the user needs. The second\nworkshop articulated expectations related to the integration of AI in\neducation. Inductive thematic analysis of the second workshop led to the\nelicitation of users' expectations. We cross-examined the needs and\nexpectations, identifying contradictions, to generate user requirements for\nemerging technologies. The paper provides suggestions for future design\ninitiatives that incorporate AI in learning environments.", "comment": "12 pages, 2 figures, 15th International Conference on Methodologies\n  and Intelligent Systems for Technology Enhanced Learning (mis4tel), Workshop\n  Track: Workshop on Integration of Emerging Technologies into Education and\n  Training (ETELT) https://mis4tel-conference.net/tracks/workshops/etelt,\n  accepted", "pdf_url": "http://arxiv.org/pdf/2507.21088v2", "cate": "cs.HC", "date": "2025-06-24", "updated": "2025-07-30"}
{"id": "2502.03086", "title": "Implementing Large Quantum Boltzmann Machines as Generative AI Models for Dataset Balancing", "authors": ["Salvatore Sinno", "Markus Bertl", "Arati Sahoo", "Bhavika Bhalgamiya", "Thomas Groß", "Nicholas Chancellor"], "categories": ["cs.ET", "cs.AI", "cs.LG", "cs.NE", "quant-ph"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "Comments:      S. Sinno, M. Bertl, A. Sahoo, B. Bhalgamiya, T. Groß and N. Chancellor, \"Implementing Large Quantum Boltzmann Machines as Generative AI Models for Dataset Balancing,\" 2025 International Conference on Next Generation Information System Engineering (NGISE), Ghaziabad, Delhi (NCR), India, 2025, pp. 1-8, doi: https://doi.org/10.1109/NGISE64126.2025.11085158", "url": "http://arxiv.org/abs/2502.03086v2", "summary": "This study explores the implementation of large Quantum Restricted Boltzmann\nMachines (QRBMs), a key advancement in Quantum Machine Learning (QML), as\ngenerative models on D-Wave's Pegasus quantum hardware to address dataset\nimbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's\nenhanced connectivity and computational capabilities, a QRBM with 120 visible\nand 120 hidden units was successfully embedded, surpassing the limitations of\ndefault embedding tools. The QRBM synthesized over 1.6 million attack samples,\nachieving a balanced dataset of over 4.2 million records. Comparative\nevaluations with traditional balancing methods, such as SMOTE and\nRandomOversampler, revealed that QRBMs produced higher-quality synthetic\nsamples, significantly improving detection rates, precision, recall, and F1\nscore across diverse classifiers. The study underscores the scalability and\nefficiency of QRBMs, completing balancing tasks in milliseconds. These findings\nhighlight the transformative potential of QML and QRBMs as next-generation\ntools in data preprocessing, offering robust solutions for complex\ncomputational challenges in modern information systems.", "comment": "S. Sinno, M. Bertl, A. Sahoo, B. Bhalgamiya, T. Gro{\\ss} and N.\n  Chancellor, \"Implementing Large Quantum Boltzmann Machines as Generative AI\n  Models for Dataset Balancing,\" 2025 International Conference on Next\n  Generation Information System Engineering (NGISE), Ghaziabad, Delhi (NCR),\n  India, 2025, pp. 1-8, doi: 10.1109/NGISE64126.2025.11085158", "pdf_url": "http://arxiv.org/pdf/2502.03086v2", "cate": "cs.ET", "date": "2025-02-05", "updated": "2025-07-29"}
{"id": "2507.21247", "title": "Dual Guidance Semi-Supervised Action Detection", "authors": ["Ankit Singh", "Efstratios Gavves", "Cees G. M. Snoek", "Hilde Kuehne"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21247v1", "summary": "Semi-Supervised Learning (SSL) has shown tremendous potential to improve the\npredictive performance of deep learning models when annotations are hard to\nobtain. However, the application of SSL has so far been mainly studied in the\ncontext of image classification. In this work, we present a semi-supervised\napproach for spatial-temporal action localization. We introduce a dual guidance\nnetwork to select better pseudo-bounding boxes. It combines a frame-level\nclassification with a bounding-box prediction to enforce action class\nconsistency across frames and boxes. Our evaluation across well-known\nspatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and\nAVA shows that the proposed module considerably enhances the model's\nperformance in limited labeled data settings. Our framework achieves superior\nresults compared to extended image-based semi-supervised baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21247v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21329", "title": "Does Editing Improve Answer Quality on Stack Overflow? A Data-Driven Investigation", "authors": ["Saikat Mondal", "Chanchal K. Roy"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted in the 41st International Conference on Software Maintenance and Evolution (ICSME 2025 - Research Track)", "url": "http://arxiv.org/abs/2507.21329v1", "summary": "High-quality answers in technical Q&A platforms like Stack Overflow (SO) are\ncrucial as they directly influence software development practices. Poor-quality\nanswers can introduce inefficiencies, bugs, and security vulnerabilities, and\nthus increase maintenance costs and technical debt in production software. To\nimprove content quality, SO allows collaborative editing, where users revise\nanswers to enhance clarity, correctness, and formatting. Several studies have\nexamined rejected edits and identified the causes of rejection. However, prior\nresearch has not systematically assessed whether accepted edits enhance key\nquality dimensions. While one study investigated the impact of edits on C/C++\nvulnerabilities, broader quality aspects remain unexplored. In this study, we\nanalyze 94,994 Python-related answers that have at least one accepted edit to\ndetermine whether edits improve (1) semantic relevance, (2) code usability, (3)\ncode complexity, (4) security vulnerabilities, (5) code optimization, and (6)\nreadability. Our findings show both positive and negative effects of edits.\nWhile 53.3% of edits improve how well answers match questions, 38.1% make them\nless relevant. Some previously broken code (9%) becomes executable, yet working\ncode (14.7%) turns non-parsable after edits. Many edits increase complexity\n(32.3%), making code harder to maintain. Instead of fixing security issues,\n20.5% of edits introduce additional issues. Even though 51.0% of edits optimize\nperformance, execution time still increases overall. Readability also suffers,\nas 49.7% of edits make code harder to read. This study highlights the\ninconsistencies in editing outcomes and provides insights into how edits impact\nsoftware maintainability, security, and efficiency that might caution users and\nmoderators and help future improvements in collaborative editing systems.", "comment": "Accepted in the 41st International Conference on Software Maintenance\n  and Evolution (ICSME 2025 - Research Track)", "pdf_url": "http://arxiv.org/pdf/2507.21329v1", "cate": "cs.SE", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21418", "title": "How Growing Toxicity Manifests: A Topic Trajectory Analysis of U.S. Immigration Discourse on Social Media", "authors": ["Una Joh", "Yiqi Li", "Jeff Hemsley"], "categories": ["cs.SI", "J.4; K.4.2; I.2.7"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      This is the preprint of a paper accepted at ICWSM 2026", "url": "http://arxiv.org/abs/2507.21418v1", "summary": "In the online public sphere, discussions about immigration often become\nincreasingly fractious, marked by toxic language and polarization. Drawing on 4\nmillion X posts over six months, we combine a user- and topic-centric approach\nto study how shifts in toxicity manifest as topical shifts. Our topic discovery\nmethod, which leverages instruction-based embeddings and recursive HDBSCAN,\nuncovers 157 fine-grained subtopics within the U.S. immigration discourse. We\nfocus on users in four groups: (1) those with increasing toxicity, (2) those\nwith decreasing toxicity, and two reference groups with no significant toxicity\ntrend but matched toxicity levels. Treating each posting history as a\ntrajectory through a five-dimensional topic space, we compare average group\ntrajectories using permutational MANOVA. Our findings show that users with\nincreasing toxicity drift toward alarmist, fear-based frames, whereas those\nwith decreasing toxicity pivot toward legal and policy-focused themes. Both\npatterns diverge statistically significantly from their reference groups. This\npipeline, which combines hierarchical topic discovery with trajectory analysis,\noffers a replicable method for studying dynamic conversations around social\nissues at scale.", "comment": "This is the preprint of a paper accepted at ICWSM 2026", "pdf_url": "http://arxiv.org/pdf/2507.21418v1", "cate": "cs.SI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21412", "title": "Cascading and Proxy Membership Inference Attacks", "authors": ["Yuntao Du", "Jiacheng Li", "Yuetian Chen", "Kaiyuan Zhang", "Zhizhen Yuan", "Hanshen Xiao", "Bruno Ribeiro", "Ninghui Li"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Our code is available at: this https URL", "url": "http://arxiv.org/abs/2507.21412v1", "summary": "A Membership Inference Attack (MIA) assesses how much a trained machine\nlearning model reveals about its training data by determining whether specific\nquery instances were included in the dataset. We classify existing MIAs into\nadaptive or non-adaptive, depending on whether the adversary is allowed to\ntrain shadow models on membership queries. In the adaptive setting, where the\nadversary can train shadow models after accessing query instances, we highlight\nthe importance of exploiting membership dependencies between instances and\npropose an attack-agnostic framework called Cascading Membership Inference\nAttack (CMIA), which incorporates membership dependencies via conditional\nshadow training to boost membership inference performance.\n  In the non-adaptive setting, where the adversary is restricted to training\nshadow models before obtaining membership queries, we introduce Proxy\nMembership Inference Attack (PMIA). PMIA employs a proxy selection strategy\nthat identifies samples with similar behaviors to the query instance and uses\ntheir behaviors in shadow models to perform a membership posterior odds test\nfor membership inference. We provide theoretical analyses for both attacks, and\nextensive experimental results demonstrate that CMIA and PMIA substantially\noutperform existing MIAs in both settings, particularly in the low\nfalse-positive regime, which is crucial for evaluating privacy risks.", "comment": "Our code is available at: https://github.com/zealscott/MIA", "pdf_url": "http://arxiv.org/pdf/2507.21412v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21896", "title": "A Systematic Robot Design Optimization Methodology with Application to Redundant Dual-Arm Manipulators", "authors": ["Dominic Guri", "George Kantor"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2507.21896v1", "summary": "One major recurring challenge in deploying manipulation robots is determining\nthe optimal placement of manipulators to maximize performance. This challenge\nis exacerbated in complex, cluttered agricultural environments of high-value\ncrops, such as flowers, fruits, and vegetables, that could greatly benefit from\nrobotic systems tailored to their specific requirements. However, the design of\nsuch systems remains a challenging, intuition-driven process, limiting the\naffordability and adoption of robotics-based automation by domain experts like\nfarmers. To address this challenge, we propose a four-part design optimization\nmethodology for automating the development of task-specific robotic systems.\nThis framework includes (a) a robot design model, (b) task and environment\nrepresentations for simulation, (c) task-specific performance metrics, and (d)\noptimization algorithms for refining configurations. We demonstrate our\nframework by optimizing a dual-arm robotic system for pepper harvesting using\ntwo off-the-shelf redundant manipulators. To enhance performance, we introduce\nnovel task metrics that leverage self-motion manifolds to characterize\nmanipulator redundancy comprehensively. Our results show that our framework\nachieves simultaneous improvements in reachability success rates and\nimprovements in dexterity. Specifically, our approach improves reachability\nsuccess by at least 14\\% over baseline methods and achieves over 30\\%\nimprovement in dexterity based on our task-specific metric.", "comment": "8 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.21896v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21176", "title": "Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs", "authors": ["Farzana Islam Adiba", "Rahmatollah Beheshti"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21176v1", "summary": "Large language models (LLMs) that are used in medical applications are known\nto show biased and unfair patterns. Prior to adopting these in clinical\ndecision-making applications, it is crucial to identify these bias patterns to\nenable effective mitigation of their impact. In this study, we present a novel\nframework combining knowledge graphs (KGs) with auxiliary LLMs to\nsystematically reveal complex bias patterns in medical LLMs. Specifically, the\nproposed approach integrates adversarial perturbation techniques to identify\nsubtle bias patterns. The approach adopts a customized multi-hop\ncharacterization of KGs to enhance the systematic evaluation of arbitrary LLMs.\nThrough a series of comprehensive experiments (on three datasets, six LLMs, and\nfive bias types), we show that our proposed framework has noticeably greater\nability and scalability to reveal complex biased patterns of LLMs compared to\nother baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21176v1", "cate": "cs.AI", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21183", "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge", "authors": ["Guangchen Lan", "Sipeng Zhang", "Tianle Wang", "Yuwei Zhang", "Daoan Zhang", "Xinpeng Wei", "Xiaoman Pan", "Hongming Zhang", "Dong-Jun Han", "Christopher G. Brinton"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21183v1", "summary": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21183v1", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21089", "title": "Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations", "authors": ["Xiaotian Su", "Naim Zierau", "Soomin Kim", "April Yi Wang", "Thiemo Wambsganss"], "categories": ["cs.HC", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21089v1", "summary": "Social media platforms increasingly employ proactive moderation techniques,\nsuch as detecting and curbing toxic and uncivil comments, to prevent the spread\nof harmful content. Despite these efforts, such approaches are often criticized\nfor creating a climate of censorship and failing to address the underlying\ncauses of uncivil behavior. Our work makes both theoretical and practical\ncontributions by proposing and evaluating two types of emotion monitoring\ndashboards to users' emotional awareness and mitigate hate speech. In a study\ninvolving 211 participants, we evaluate the effects of the two mechanisms on\nuser commenting behavior and emotional experiences. The results reveal that\nthese interventions effectively increase users' awareness of their emotional\nstates and reduce hate speech. However, our findings also indicate potential\nunintended effects, including increased expression of negative emotions (Angry,\nFear, and Sad) when discussing sensitive issues. These insights provide a basis\nfor further research on integrating proactive emotion regulation tools into\nsocial media platforms to foster healthier digital interactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21089v1", "cate": "cs.HC", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2411.01297", "title": "Receding Hamiltonian-Informed Optimal Neural Control and State Estimation for Closed-Loop Dynamical Systems", "authors": ["Josue N. Rivera", "Dengfeng Sun"], "categories": ["eess.SY", "cs.AI", "cs.ET", "cs.LG", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      27 pages. Source code: this https URL", "url": "http://arxiv.org/abs/2411.01297v3", "summary": "This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers,\na novel class of neural network-based controllers for dynamical systems and\nexplicit non-linear model-predictive control. Hion controllers estimate future\nstates and develop an optimal control strategy using Pontryagin's Maximum\nPrinciple. The proposed framework, along with our Taylored Multi-Faceted\nApproach for Neural ODE and Optimal Control (T-mano) architecture, allows for\ncustom transient behavior, predictive control, and closed-loop feedback,\naddressing limitations of existing methods. Comparative analyses with\nestablished model-predictive controllers revealed Hion controllers' superior\noptimality and tracking capabilities. Optimal control strategies are also\ndemonstrated for both linear and non-linear dynamical systems.", "comment": "27 pages. Source code: https://github.com/wzjoriv/Hion", "pdf_url": "http://arxiv.org/pdf/2411.01297v3", "cate": "eess.SY", "date": "2024-11-02", "updated": "2025-07-29"}
{"id": "2507.21256", "title": "Tracking Moose using Aerial Object Detection", "authors": ["Christopher Indris", "Raiyan Rahman", "Goetz Bramesfeld", "Guanghui Wang"], "categories": ["cs.CV", "I.4.8"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 6 figures, 8 tables", "url": "http://arxiv.org/abs/2507.21256v1", "summary": "Aerial wildlife tracking is critical for conservation efforts and relies on\ndetecting small objects on the ground below the aircraft. It presents technical\nchallenges: crewed aircraft are expensive, risky and disruptive; autonomous\ndrones have limited computational capacity for onboard AI systems. Since the\nobjects of interest may appear only a few pixels wide, small object detection\nis an inherently challenging computer vision subfield compounded by\ncomputational efficiency needs. This paper applies a patching augmentation to\ndatasets to study model performance under various settings. A comparative study\nof three common yet architecturally diverse object detectors is conducted using\nthe data, varying the patching method's hyperparameters against detection\naccuracy. Each model achieved at least 93\\% mAP@IoU=0.5 on at least one\npatching configuration. Statistical analyses provide an in-depth commentary on\nthe effects of various factors. Analysis also shows that faster, simpler models\nare about as effective as models that require more computational power for this\ntask and perform well given limited patch scales, encouraging UAV deployment.\nDatasets and models will be made available via\nhttps://github.com/chrisindris/Moose.", "comment": "18 pages, 6 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2507.21256v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21382", "title": "MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration", "authors": ["Ruiyin Li", "Yiran Zhang", "Xiyu Zhou", "Peng Liang", "Weisong Sun", "Jifeng Xuan", "Zhi Jin", "Yang Liu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      23 pages, 8 images, 1 table, Manuscript submitted to a journal (2025)", "url": "http://arxiv.org/abs/2507.21382v1", "summary": "Software architecture design is a critical, yet inherently complex and\nknowledge-intensive phase of software development. It requires deep domain\nexpertise, development experience, architectural knowledge, careful trade-offs\namong competing quality attributes, and the ability to adapt to evolving\nrequirements. Traditionally, this process is time-consuming and\nlabor-intensive, and relies heavily on architects, often resulting in limited\ndesign alternatives, especially under the pressures of agile development. While\nLarge Language Model (LLM)-based agents have shown promising performance across\nvarious SE tasks, their application to architecture design remains relatively\nscarce and requires more exploration, particularly in light of diverse domain\nknowledge and complex decision-making. To address the challenges, we proposed\nMAAD (Multi-Agent Architecture Design), an automated framework that employs a\nknowledge-driven Multi-Agent System (MAS) for architecture design. MAAD\norchestrates four specialized agents (i.e., Analyst, Modeler, Designer and\nEvaluator) to collaboratively interpret requirements specifications and produce\narchitectural blueprints enriched with quality attributes-based evaluation\nreports. We then evaluated MAAD through a case study and comparative\nexperiments against MetaGPT, a state-of-the-art MAS baseline. Our results show\nthat MAAD's superiority lies in generating comprehensive architectural\ncomponents and delivering insightful and structured architecture evaluation\nreports. Feedback from industrial architects across 11 requirements\nspecifications further reinforces MAAD's practical usability. We finally\nexplored the performance of the MAAD framework with three LLMs (GPT-4o,\nDeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance\nin producing architecture design, emphasizing the importance of LLM selection\nin MAS-driven architecture design.", "comment": "23 pages, 8 images, 1 table, Manuscript submitted to a journal (2025)", "pdf_url": "http://arxiv.org/pdf/2507.21382v1", "cate": "cs.SE", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21903", "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation", "authors": ["Tiviatis Sim", "Kaiwen Yang", "Shen Xin", "Kenji Kawaguchi"], "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21903v1", "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21903v1", "cate": "cs.SI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21372", "title": "Load Balancing for AI Training Workloads", "authors": ["Sarah McClure", "Sylvia Ratnasamy", "Scott Shenker"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21372v1", "summary": "We investigate the performance of various load balancing algorithms for\nlarge-scale AI training workloads that are running on dedicated infrastructure.\nThe performance of load balancing depends on both the congestion control and\nloss recovery algorithms, so our evaluation also sheds light on the appropriate\nchoices for those designs as well.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21372v1", "cate": "cs.NI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21483", "title": "NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples", "authors": ["Pu Shi"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21483v1", "summary": "Neural networks have received a lot of attention recently, and related\nsecurity issues have come with it. Many studies have shown that neural networks\nare vulnerable to adversarial examples that have been artificially perturbed\nwith modification, which is too small to be distinguishable by human\nperception. Different attacks and defenses have been proposed to solve these\nproblems, but there is little research on evaluating the robustness of neural\nnetworks and their inputs. In this work, we propose a metric called the neuron\ncover change rate (NCCR) to measure the ability of deep learning models to\nresist attacks and the stability of adversarial examples. NCCR monitors\nalterations in the output of specifically chosen neurons when the input is\nperturbed, and networks with a smaller degree of variation are considered to be\nmore robust. The results of the experiment on image recognition and the speaker\nrecognition model show that our metrics can provide a good assessment of the\nrobustness of neural networks or their inputs. It can also be used to detect\nwhether an input is adversarial or not, as adversarial examples are always less\nrobust.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21483v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21957", "title": "ODE Methods for Computing One-Dimensional Self-Motion Manifolds", "authors": ["Dominic Guri", "George Kantor"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21957v1", "summary": "Redundant manipulators are well understood to offer infinite joint\nconfigurations for achieving a desired end-effector pose. The multiplicity of\ninverse kinematics (IK) solutions allows for the simultaneous solving of\nauxiliary tasks like avoiding joint limits or obstacles. However, the most\nwidely used IK solvers are numerical gradient-based iterative methods that\ninherently return a locally optimal solution. In this work, we explore the\ncomputation of self-motion manifolds (SMMs), which represent the set of all\njoint configurations that solve the inverse kinematics problem for redundant\nmanipulators. Thus, SMMs are global IK solutions for redundant manipulators. We\nfocus on task redundancies of dimensionality 1, introducing a novel ODE\nformulation for computing SMMs using standard explicit fixed-step ODE\nintegrators. We also address the challenge of ``inducing'' redundancy in\notherwise non-redundant manipulators assigned to tasks naturally described by\none degree of freedom less than the non-redundant manipulator. Furthermore,\nrecognizing that SMMs can consist of multiple disconnected components, we\npropose methods for searching for these separate SMM components. Our\nformulations and algorithms compute accurate SMM solutions without requiring\nadditional IK refinement, and we extend our methods to prismatic joint systems\n-- an area not covered in current SMM literature. This manuscript presents the\nderivation of these methods and several examples that show how the methods work\nand their limitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21957v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21206", "title": "Agentic Web: Weaving the Next Web with AI Agents", "authors": ["Yingxuan Yang", "Mulei Ma", "Yuxuan Huang", "Huacan Chai", "Chenyu Gong", "Haoran Geng", "Yuanjian Zhou", "Ying Wen", "Meng Fang", "Muhao Chen", "Shangding Gu", "Ming Jin", "Costas Spanos", "Yang Yang", "Pieter Abbeel", "Dawn Song", "Weinan Zhang", "Jun Wang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21206v1", "summary": "The emergence of AI agents powered by large language models (LLMs) marks a\npivotal shift toward the Agentic Web, a new phase of the internet defined by\nautonomous, goal-driven interactions. In this paradigm, agents interact\ndirectly with one another to plan, coordinate, and execute complex tasks on\nbehalf of users. This transition from human-driven to machine-to-machine\ninteraction allows intent to be delegated, relieving users from routine digital\noperations and enabling a more interactive, automated web experience. In this\npaper, we present a structured framework for understanding and building the\nAgentic Web. We trace its evolution from the PC and Mobile Web eras and\nidentify the core technological foundations that support this shift. Central to\nour framework is a conceptual model consisting of three key dimensions:\nintelligence, interaction, and economics. These dimensions collectively enable\nthe capabilities of AI agents, such as retrieval, recommendation, planning, and\ncollaboration. We analyze the architectural and infrastructural challenges\ninvolved in creating scalable agentic systems, including communication\nprotocols, orchestration strategies, and emerging paradigms such as the Agent\nAttention Economy. We conclude by discussing the potential applications,\nsocietal risks, and governance issues posed by agentic systems, and outline\nresearch directions for developing open, secure, and intelligent ecosystems\nshaped by both human intent and autonomous agent behavior. A continuously\nupdated collection of relevant studies for agentic web is available at:\nhttps://github.com/SafeRL-Lab/agentic-web.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21206v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21184", "title": "EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models", "authors": ["Haowei Lin", "Xiangyu Wang", "Jianzhu Ma", "Yitao Liang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21184v1", "summary": "Scaling laws are fundamental mathematical relationships that predict how\nneural network performance evolves with changes in variables such as model\nsize, dataset size, and computational resources. Traditionally, discovering\nthese laws requires extensive human expertise and manual experimentation. We\nintroduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that\nleverages evolutionary algorithms guided by Large Language Models (LLMs) to\nco-evolve symbolic expressions and their optimization routines. Formulated to\nhandle scaling variables, control variables, and response metrics across\ndiverse experimental settings, EvoSLD searches for parsimonious, universal\nfunctional forms that minimize fitting errors on grouped data subsets.\nEvaluated on five real-world scenarios from recent literature, EvoSLD\nrediscovers exact human-derived laws in two cases and surpasses them in others,\nachieving up to orders-of-magnitude reductions in normalized mean squared error\non held-out test sets. Compared to baselines like symbolic regression and\nablated variants, EvoSLD demonstrates superior accuracy, interpretability, and\nefficiency, highlighting its potential to accelerate AI research. Code is\navailable at https://github.com/linhaowei1/SLD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21184v1", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21090", "title": "Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?", "authors": ["Yiling Zhao", "Audrey Michal", "Nithum Thain", "Hari Subramonyam"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21090v1", "summary": "As AI systems shape individual and societal decisions, fostering critical AI\nliteracy is essential. Traditional approaches, such as blog articles, static\nlessons, and social media discussions, often fail to support deep conceptual\nunderstanding and critical engagement. This study examines whether interactive\nsimulations can help learners think like a scientist by engaging them in\nhypothesis testing, experimentation, and direct observation of AI behavior. In\na controlled study with 605 participants, we assess how interactive AI\ntutorials impact learning of key concepts such as fairness, dataset\nrepresentativeness, and bias in language models. Results show that interactive\nsimulations effectively enhance AI literacy across topics, supporting greater\nknowledge transfer and self-reported confidence, though engagement alone does\nnot predict learning. This work contributes to the growing field of AI literacy\neducation, highlighting how interactive, inquiry-driven methodologies can\nbetter equip individuals to critically engage with AI in their daily lives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21090v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.14116", "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification", "authors": ["Daniëlle Schuman", "Mark V. Seebode", "Tobias Rohe", "Maximilian Balthasar Mansky", "Michael Schroedl-Baumann", "Jonas Stein", "Claudia Linnhoff-Popien", "Florian Krellner"], "categories": ["quant-ph", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 5 figures (10 if counting subfigures), 2 tables. To be published in the proceedings of the 2025 IEEE International Conference on Quantum Computing and Engineering (QCE)", "url": "http://arxiv.org/abs/2507.14116v2", "summary": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.", "comment": "12 pages, 5 figures (10 if counting subfigures), 2 tables. To be\n  published in the proceedings of the 2025 IEEE International Conference on\n  Quantum Computing and Engineering (QCE)", "pdf_url": "http://arxiv.org/pdf/2507.14116v2", "cate": "quant-ph", "date": "2025-07-18", "updated": "2025-07-29"}
{"id": "2507.21261", "title": "HDR Environment Map Estimation with Latent Diffusion Models", "authors": ["Jack Hilliard", "Adrian Hilton", "Jean-Yves Guillemaut"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21261v1", "summary": "We advance the field of HDR environment map estimation from a single-view\nimage by establishing a novel approach leveraging the Latent Diffusion Model\n(LDM) to produce high-quality environment maps that can plausibly light\nmirror-reflective surfaces. A common issue when using the ERP representation,\nthe format used by the vast majority of approaches, is distortions at the poles\nand a seam at the sides of the environment map. We remove the border seam\nartefact by proposing an ERP convolutional padding in the latent autoencoder.\nAdditionally, we investigate whether adapting the diffusion network\narchitecture to the ERP format can improve the quality and accuracy of the\nestimated environment map by proposing a panoramically-adapted Diffusion\nTransformer architecture. Our proposed PanoDiT network reduces ERP distortions\nand artefacts, but at the cost of image quality and plausibility. We evaluate\nwith standard benchmarks to demonstrate that our models estimate high-quality\nenvironment maps that perform competitively with state-of-the-art approaches in\nboth image quality and lighting accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21261v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21485", "title": "HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions", "authors": ["Jing Wang", "Shang Liu", "Yao Lu", "Zhiyao Xie"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This work has been accepted at ICCAD 2025 (International Conference on Computer-Aided Design)", "url": "http://arxiv.org/abs/2507.21485v1", "summary": "High-level synthesis (HLS) accelerates hardware design by enabling the\nautomatic translation of high-level descriptions into efficient hardware\nimplementations. However, debugging HLS code is a challenging and\nlabor-intensive task, especially for novice circuit designers or software\nengineers without sufficient hardware domain knowledge. The recent emergence of\nLarge Language Models (LLMs) is promising in automating the HLS debugging\nprocess. Despite the great potential, three key challenges persist when\napplying LLMs to HLS logic debugging: 1) High-quality circuit data for training\nLLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in\nhardware is inherently more complex than identifying software bugs with\nexisting golden test cases. 3) The absence of reliable test cases requires\nmulti-tasking solutions, performing both bug identification and correction.\ncomplicates the multi-tasking required for effective HLS debugging. In this\nwork, we propose a customized solution named HLSDebugger to address the\nchallenges. HLSDebugger first generates and releases a large labeled dataset\nwith 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts\nan encoder-decoder structure, performing bug location identification, bug type\nprediction, and bug correction with the same model. HLSDebugger significantly\noutperforms advanced LLMs like GPT-4 in bug identification and by more than 3x\nin bug correction. It makes a substantial advancement in the exploration of\nautomated debugging of HLS code.", "comment": "This work has been accepted at ICCAD 2025 (International Conference\n  on Computer-Aided Design)", "pdf_url": "http://arxiv.org/pdf/2507.21485v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21055", "title": "Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models", "authors": ["Leyi Ouyang"], "categories": ["cs.CY", "cs.AI", "cs.SI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, 5 tables", "url": "http://arxiv.org/abs/2507.21055v1", "summary": "In the interconnected world, news media are critical in conveying information\nto public across diverse domains including technology, finance, and\nagriculture. Journalists make efforts to present accurate information, however,\nthe interpretation of news often varies significantly among different audiences\ndue to their specific expertise and age. In this work, we investigate how to\nidentify these comprehension gaps and provide solutions to improve audiences\nunderstanding of news content, particular to the aspects of articles outside\ntheir primary domains of knowledge. We propose a agent-based framework using\nlarge language models (LLMs) to simulate society communication behaviors, where\nseveral agents can discuss news. These agents can be designed to be experts\nfrom various occupation, or from different age group. Our results indicate that\nthis framework can identify confusions or even misunderstanding of news for the\nagent through the iterative discussion process. Based on these accurate\nidentification, the framework can design a supplement material specific to\nthese agents on the news. Our results show that agents exhibit significantly\nimproved news understanding after receiving this material. These findings\nhighlight our framework's utility and efficiency in enhancing news\ncomprehension for diverse audiences by directly addressing their understanding\ngap.", "comment": "9 pages, 3 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.21055v1", "cate": "cs.CY", "date": "2025-04-30", "updated": "2025-04-30"}
{"id": "2507.21385", "title": "Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving", "authors": ["Wei Mao", "Lili Wei", "Omid Semiari", "Shu-ping Yeh", "Hosein Nikopour"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 7 figures", "url": "http://arxiv.org/abs/2507.21385v1", "summary": "3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX)\nis an important new network energy saving feature for 5G. As a time-domain\ntechnique, it periodically aggregates the user data transmissions in a given\nduration of time when the traffic load is not heavy, so that the remaining time\ncan be kept silent and advanced sleep modes (ASM) can be enabled to shut down\nmore radio components and save more energy for the cell. However, inevitably\nthe packet delay is increased, as during the silent period no transmission is\nallowed. In this paper we study how to configure cell DTX/DRX to optimally\nbalance energy saving and packet delay, so that for delay-sensitive traffic\nmaximum energy saving can be achieved while the degradation of quality of\nservice (QoS) is minimized. As the optimal configuration can be different for\ndifferent network and traffic conditions, the problem is complex and we resort\nto deep reinforcement learning (DRL) framework to train an AI agent to solve\nit. Through careful design of 1) the learning algorithm, which implements a\ndeep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward\nfunction, which utilizes a smooth approximation of a theoretically optimal but\ndiscontinuous reward function, we are able to train an AI agent that always\ntries to select the best possible Cell DTX/DRX configuration under any network\nand traffic conditions. Simulation results show that compared to the case when\ncell DTX/DRX is not used, our agent can achieve up to ~45% energy saving\ndepending on the traffic load scenario, while always maintaining no more than\n~1% QoS degradation.", "comment": "7 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.21385v1", "cate": "cs.NI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21538", "title": "Can We End the Cat-and-Mouse Game? Simulating Self-Evolving Phishing Attacks with LLMs and Genetic Algorithms", "authors": ["Seiji Sato", "Tetsushi Ohki", "Masakatsu Nishigaki"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21538v1", "summary": "Anticipating emerging attack methodologies is crucial for proactive\ncybersecurity. Recent advances in Large Language Models (LLMs) have enabled the\nautomated generation of phishing messages and accelerated research into\npotential attack techniques. However, predicting future threats remains\nchallenging due to reliance on existing training data. To address this\nlimitation, we propose a novel framework that integrates LLM-based phishing\nattack simulations with a genetic algorithm in a psychological context,\nenabling phishing strategies to evolve dynamically through adversarial\ninteractions with simulated victims. Through simulations using Llama 3.1, we\ndemonstrate that (1) self-evolving phishing strategies employ increasingly\nsophisticated psychological manipulation techniques, surpassing naive\nLLM-generated attacks, (2) variations in a victim's prior knowledge\nsignificantly influence the evolution of attack strategies, and (3) adversarial\ninteractions between evolving attacks and adaptive defenses create a\ncat-and-mouse dynamic, revealing an inherent asymmetry in cybersecurity --\nattackers continuously refine their methods, whereas defenders struggle to\ncomprehensively counter all evolving threats. Our approach provides a scalable,\ncost-effective method for analyzing the evolution of phishing strategies and\ndefenses, offering insights into future social engineering threats and\nunderscoring the necessity of proactive cybersecurity measures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21538v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21965", "title": "A Deep Learning-Driven Autonomous System for Retinal Vein Cannulation: Validation Using a Chicken Embryo Model", "authors": ["Yi Wang", "Peiyao Zhang", "Mojtaba Esfandiari", "Peter Gehlbach", "Iulian I. Iordachita"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21965v1", "summary": "Retinal vein cannulation (RVC) is a minimally invasive microsurgical\nprocedure for treating retinal vein occlusion (RVO), a leading cause of vision\nimpairment. However, the small size and fragility of retinal veins, coupled\nwith the need for high-precision, tremor-free needle manipulation, create\nsignificant technical challenges. These limitations highlight the need for\nrobotic assistance to improve accuracy and stability. This study presents an\nautomated robotic system with a top-down microscope and B-scan optical\ncoherence tomography (OCT) imaging for precise depth sensing. Deep\nlearning-based models enable real-time needle navigation, contact detection,\nand vein puncture recognition, using a chicken embryo model as a surrogate for\nhuman retinal veins. The system autonomously detects needle position and\npuncture events with 85% accuracy. The experiments demonstrate notable\nreductions in navigation and puncture times compared to manual methods. Our\nresults demonstrate the potential of integrating advanced imaging and deep\nlearning to automate microsurgical tasks, providing a pathway for safer and\nmore reliable RVC procedures with enhanced precision and reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21965v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21257", "title": "CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting", "authors": ["David Maria Schmidt", "Raoul Schubert", "Philipp Cimiano"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Research Track, 24th International Semantic Web Conference (ISWC 2025), November 2-6, 2025, Nara, Japan", "url": "http://arxiv.org/abs/2507.21257v1", "summary": "Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.", "comment": "Research Track, 24th International Semantic Web Conference (ISWC\n  2025), November 2-6, 2025, Nara, Japan", "pdf_url": "http://arxiv.org/pdf/2507.21257v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21188", "title": "Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs", "authors": ["Raj Krishnan Vijayaraj"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21188v1", "summary": "LLMs for clinical decision support often fail under small but clinically\nmeaningful input shifts such as masking a symptom or negating a finding,\ndespite high performance on static benchmarks. These reasoning failures\nfrequently go undetected by standard NLP metrics, which are insensitive to\nlatent representation shifts that drive diagnosis instability. We propose a\ngeometry-aware evaluation framework, LAPD (Latent Agentic Perturbation\nDiagnostics), which systematically probes the latent robustness of clinical\nLLMs under structured adversarial edits. Within this framework, we introduce\nLatent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that\ncaptures representational instability when embeddings cross decision boundaries\nin PCA-reduced latent space. Clinical notes are generated using a structured\nprompting pipeline grounded in diagnostic reasoning, then perturbed along four\naxes: masking, negation, synonym replacement, and numeric variation to simulate\ncommon ambiguities and omissions. We compute LDFR across both foundation and\nclinical LLMs, finding that latent fragility emerges even under minimal\nsurface-level changes. Finally, we validate our findings on 90 real clinical\nnotes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of\nLDFR beyond synthetic settings. Our results reveal a persistent gap between\nsurface robustness and semantic stability, underscoring the importance of\ngeometry-aware auditing in safety-critical clinical AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21188v1", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21124", "title": "VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization", "authors": ["Ayan Biswas", "Terece L. Turton", "Nishath Rajiv Ranasinghe", "Shawn Jones", "Bradley Love", "William Jones", "Aric Hagberg", "Han-Wei Shen", "Nathan DeBardeleben", "Earl Lawrence"], "categories": ["cs.HC", "cs.AI", "cs.GR", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21124v1", "summary": "We present VizGenie, a self-improving, agentic framework that advances\nscientific visualization through large language model (LLM) by orchestrating of\na collection of domain-specific and dynamically generated modules. Users\ninitially access core functionalities--such as threshold-based filtering, slice\nextraction, and statistical analysis--through pre-existing tools. For tasks\nbeyond this baseline, VizGenie autonomously employs LLMs to generate new\nvisualization scripts (e.g., VTK Python code), expanding its capabilities\non-demand. Each generated script undergoes automated backend validation and is\nseamlessly integrated upon successful testing, continuously enhancing the\nsystem's adaptability and robustness. A distinctive feature of VizGenie is its\nintuitive natural language interface, allowing users to issue high-level\nfeature-based queries (e.g., ``visualize the skull\"). The system leverages\nimage-based analysis and visual question answering (VQA) via fine-tuned vision\nmodels to interpret these queries precisely, bridging domain expertise and\ntechnical implementation. Additionally, users can interactively query generated\nvisualizations through VQA, facilitating deeper exploration. Reliability and\nreproducibility are further strengthened by Retrieval-Augmented Generation\n(RAG), providing context-driven responses while maintaining comprehensive\nprovenance records. Evaluations on complex volumetric datasets demonstrate\nsignificant reductions in cognitive overhead for iterative visualization tasks.\nBy integrating curated domain-specific tools with LLM-driven flexibility,\nVizGenie not only accelerates insight generation but also establishes a\nsustainable, continuously evolving visualization practice. The resulting\nplatform dynamically learns from user interactions, consistently enhancing\nsupport for feature-centric exploration and reproducible research in scientific\nvisualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21124v1", "cate": "cs.HC", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.21291", "title": "Fairness and Robustness of CLIP-Based Models for Chest X-rays", "authors": ["Théo Sourget", "David Restrepo", "Céline Hudelot", "Enzo Ferrante", "Stergios Christodoulidis", "Maria Vakalopoulou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the FAIMI MICCAI workshop 2025", "url": "http://arxiv.org/abs/2507.21291v1", "summary": "Motivated by the strong performance of CLIP-based models in natural\nimage-text domains, recent efforts have adapted these architectures to medical\ntasks, particularly in radiology, where large paired datasets of images and\nreports, such as chest X-rays, are available. While these models have shown\nencouraging results in terms of accuracy and discriminative performance, their\nfairness and robustness in the different clinical tasks remain largely\nunderexplored. In this study, we extensively evaluate six widely used\nCLIP-based models on chest X-ray classification using three publicly available\ndatasets: MIMIC-CXR, NIH-CXR14, and NEATX. We assess the models fairness across\nsix conditions and patient subgroups based on age, sex, and race. Additionally,\nwe assess the robustness to shortcut learning by evaluating performance on\npneumothorax cases with and without chest drains. Our results indicate\nperformance gaps between patients of different ages, but more equitable results\nfor the other attributes. Moreover, all models exhibit lower performance on\nimages without chest drains, suggesting reliance on spurious correlations. We\nfurther complement the performance analysis with a study of the embeddings\ngenerated by the models. While the sensitive attributes could be classified\nfrom the embeddings, we do not see such patterns using PCA, showing the\nlimitations of these visualisation techniques when assessing models. Our code\nis available at https://github.com/TheoSourget/clip_cxr_fairness", "comment": "Accepted for publication at the FAIMI MICCAI workshop 2025", "pdf_url": "http://arxiv.org/pdf/2507.21291v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21583", "title": "Ethical Classification of Non-Coding Contributions in Open-Source Projects via Large Language Models", "authors": ["Sergio Cobos", "Javier Luis Cánovas Izquierdo"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at the 2025 8th AAAI/ACM Conference on AI, Ethics, and Society (AIES'25)", "url": "http://arxiv.org/abs/2507.21583v1", "summary": "The development of Open-Source Software (OSS) is not only a technical\nchallenge, but also a social one due to the diverse mixture of contributors. To\nthis aim, social-coding platforms, such as GitHub, provide the infrastructure\nneeded to host and develop the code, but also the support for enabling the\ncommunity's collaboration, which is driven by non-coding contributions, such as\nissues (i.e., change proposals or bug reports) or comments to existing\ncontributions. As with any other social endeavor, this development process\nfaces ethical challenges, which may put at risk the project's sustainability.\nTo foster a productive and positive environment, OSS projects are increasingly\ndeploying codes of conduct, which define rules to ensure a respectful and\ninclusive participatory environment, with the Contributor Covenant being the\nmain model to follow. However, monitoring and enforcing these codes of conduct\nis a challenging task, due to the limitations of current approaches. In this\npaper, we propose an approach to classify the ethical quality of non-coding\ncontributions in OSS projects by relying on Large Language Models (LLM), a\npromising technology for text classification tasks. We defined a set of ethical\nmetrics based on the Contributor Covenant and developed a classification\napproach to assess ethical behavior in OSS non-coding contributions, using\nprompt engineering to guide the model's output.", "comment": "Accepted at the 2025 8th AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES'25)", "pdf_url": "http://arxiv.org/pdf/2507.21583v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21467", "title": "Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos", "authors": ["Selimhan Dagtas", "Mert Can Cakmak", "Nitin Agarwal"], "categories": ["cs.IR", "cs.SI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21467v1", "summary": "The growing popularity of short-form video content, such as YouTube Shorts,\nhas transformed user engagement on digital platforms, raising critical\nquestions about the role of recommendation algorithms in shaping user\nexperiences. These algorithms significantly influence content consumption, yet\nconcerns about biases, echo chambers, and content diversity persist. This study\ndevelops an efficient data collection framework to analyze YouTube's\nrecommendation algorithms for both short-form and long-form videos, employing\nparallel computing and advanced scraping techniques to overcome limitations of\nYouTube's API. The analysis uncovers distinct behavioral patterns in\nrecommendation algorithms across the two formats, with short-form videos\nshowing a more immediate shift toward engaging yet less diverse content\ncompared to long-form videos. Furthermore, a novel investigation into biases in\npolitically sensitive topics, such as the South China Sea dispute, highlights\nthe role of these algorithms in shaping narratives and amplifying specific\nviewpoints. By providing actionable insights for designing equitable and\ntransparent recommendation systems, this research underscores the importance of\nresponsible AI practices in the evolving digital media landscape.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21467v1", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21728", "title": "Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum", "authors": ["Agastya Raj", "Zehao Wang", "Tingjun Chen", "Daniel C Kilper", "Marco Ruffini"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This is a preprint of a paper accepted and published in the Journal of Optical Communications and Networking (JOCN). The final published version is available at: this https URL", "url": "http://arxiv.org/abs/2507.21728v1", "summary": "Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers\n(EDFAs) is essential for optimizing optical network performance, particularly\nas networks evolve toward multi-vendor solutions. In this work, we propose a\ngeneralized few-shot transfer learning architecture based on a Semi-Supervised\nSelf-Normalizing Neural Network (SS-NN) that leverages internal EDFA features -\nsuch as VOA input or output power and attenuation, to improve gain spectrum\nprediction. Our SS-NN model employs a two-phase training strategy comprising\nunsupervised pre-training with noise-augmented measurements and supervised\nfine-tuning with a custom weighted MSE loss. Furthermore, we extend the\nframework with transfer learning (TL) techniques that enable both homogeneous\n(same-feature space) and heterogeneous (different-feature sets) model\nadaptation across booster, preamplifier, and ILA EDFAs. To address feature\nmismatches in heterogeneous TL, we incorporate a covariance matching loss to\nalign second-order feature statistics between source and target domains.\nExtensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland\ntestbeds demonstrate that the proposed approach significantly reduces the\nnumber of measurements requirements on the system while achieving lower mean\nabsolute errors and improved error distributions compared to benchmark methods.", "comment": "This is a preprint of a paper accepted and published in the Journal\n  of Optical Communications and Networking (JOCN). The final published version\n  is available at: https://doi.org/10.1364/JOCN.560987", "pdf_url": "http://arxiv.org/pdf/2507.21728v1", "cate": "cs.NI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21540", "title": "PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking", "authors": ["Quanchen Zou", "Zonghao Ying", "Moyang Chen", "Wenzhuo Xu", "Yisong Xiao", "Yakai Li", "Deyue Zhang", "Dongdong Yang", "Zhao Liu", "Xiangzheng Zhang"], "categories": ["cs.CR", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21540v1", "summary": "The increasing sophistication of large vision-language models (LVLMs) has\nbeen accompanied by advances in safety alignment mechanisms designed to prevent\nharmful content generation. However, these defenses remain vulnerable to\nsophisticated adversarial attacks. Existing jailbreak methods typically rely on\ndirect and semantically explicit prompts, overlooking subtle vulnerabilities in\nhow LVLMs compose information over multiple reasoning steps. In this paper, we\npropose a novel and effective jailbreak framework inspired by Return-Oriented\nProgramming (ROP) techniques from software security. Our approach decomposes a\nharmful instruction into a sequence of individually benign visual gadgets. A\ncarefully engineered textual prompt directs the sequence of inputs, prompting\nthe model to integrate the benign visual gadgets through its reasoning process\nto produce a coherent and harmful output. This makes the malicious intent\nemergent and difficult to detect from any single component. We validate our\nmethod through extensive experiments on established benchmarks including\nSafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our\napproach consistently and substantially outperforms existing baselines on\nstate-of-the-art models, achieving near-perfect attack success rates (over 0.90\non SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical\nand underexplored vulnerability that exploits the compositional reasoning\nabilities of LVLMs, highlighting the urgent need for defenses that secure the\nentire reasoning process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21540v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21981", "title": "DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments", "authors": ["Yufei Jia", "Guangyu Wang", "Yuhang Dong", "Junzhe Wu", "Yupei Zeng", "Haonan Lin", "Zifan Wang", "Haizhou Ge", "Weibin Gu", "Kairui Ding", "Zike Yan", "Yunjie Cheng", "Yue Li", "Ziming Wang", "Chuxuan Li", "Wei Sui", "Lu Shi", "Guanzhong Tian", "Ruqi Huang", "Guyue Zhou"], "categories": ["cs.RO", "68T40", "I.2.9"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8pages, IROS2025 (Camera Ready)", "url": "http://arxiv.org/abs/2507.21981v1", "summary": "We present the first unified, modular, open-source 3DGS-based simulation\nframework for Real2Sim2Real robot learning. It features a holistic Real2Sim\npipeline that synthesizes hyper-realistic geometry and appearance of complex\nreal-world scenarios, paving the way for analyzing and bridging the Sim2Real\ngap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively\nparallel simulation of multiple sensor modalities and accurate physics, with\ninclusive supports for existing 3D assets, robot models, and ROS plugins,\nempowering large-scale robot learning and complex robotic benchmarks. Through\nextensive experiments on imitation learning, Discoverse demonstrates\nstate-of-the-art zero-shot Sim2Real transfer performance compared to existing\nsimulators. For code and demos: https://air-discoverse.github.io/.", "comment": "8pages, IROS2025 (Camera Ready)", "pdf_url": "http://arxiv.org/pdf/2507.21981v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21276", "title": "LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems", "authors": ["Yufei Li", "Zexin Li", "Yinglun Zhu", "Cong Liu"], "categories": ["cs.AI", "cs.CL", "cs.DC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by RTSS 2025", "url": "http://arxiv.org/abs/2507.21276v1", "summary": "Modern deployment of large language models (LLMs) frequently involves both\ninference serving and continuous retraining to stay aligned with evolving data\nand user feedback. Common practices separate these workloads onto distinct\nservers in isolated phases, causing substantial inefficiencies (e.g., GPU\nidleness) and delayed adaptation to new data in distributed settings. Our\nempirical analysis reveals that these inefficiencies stem from dynamic request\narrivals during serving and workload heterogeneity in pipeline-parallel\ntraining. To address these challenges, we propose LeMix, a system for\nco-locating and managing concurrent LLM serving and training workloads. LeMix\nintegrates offline profiling, execution prediction mechanisms, and runtime\nscheduling to dynamically adapt resource allocation based on workload\ncharacteristics and system conditions. By understanding task-specific behaviors\nand co-execution interference across shared nodes, LeMix improves utilization\nand serving quality without compromising serving responsiveness. Our evaluation\nshows that LeMix improves throughput by up to 3.53x, reduces inference loss by\nup to 0.61x, and delivers up to 2.12x higher response time SLO attainment over\ntraditional separate setups. To our knowledge, this is the first work to\nuncover and exploit the opportunities of joint LLM inference and training,\npaving the way for more resource-efficient deployment of LLMs in production\nenvironments.", "comment": "Accepted by RTSS 2025", "pdf_url": "http://arxiv.org/pdf/2507.21276v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21189", "title": "Operator-Based Machine Intelligence: A Hilbert Space Framework for Spectral Learning and Symbolic Reasoning", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21189v1", "summary": "Traditional machine learning models, particularly neural networks, are rooted\nin finite-dimensional parameter spaces and nonlinear function approximations.\nThis report explores an alternative formulation where learning tasks are\nexpressed as sampling and computation in infinite dimensional Hilbert spaces,\nleveraging tools from functional analysis, signal processing, and spectral\ntheory. We review foundational concepts such as Reproducing Kernel Hilbert\nSpaces (RKHS), spectral operator learning, and wavelet-domain representations.\nWe present a rigorous mathematical formulation of learning in Hilbert spaces,\nhighlight recent models based on scattering transforms and Koopman operators,\nand discuss advantages and limitations relative to conventional neural\narchitectures. The report concludes by outlining directions for scalable and\ninterpretable machine learning grounded in Hilbertian signal processing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21189v1", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21378", "title": "ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices", "authors": ["Kevin Pu", "Ting Zhang", "Naveen Sendhilnathan", "Sebastian Freitag", "Raj Sodhi", "Tanya Jonker"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to UIST'25", "url": "http://arxiv.org/abs/2507.21378v1", "summary": "Wearable AI systems aim to provide timely assistance in daily life, but\nexisting approaches often rely on user initiation or predefined task knowledge,\nneglecting users' current mental states. We introduce ProMemAssist, a smart\nglasses system that models a user's working memory (WM) in real-time using\nmulti-modal sensor signals. Grounded in cognitive theories of WM, our system\nrepresents perceived information as memory items and episodes with encoding\nmechanisms, such as displacement and interference. This WM model informs a\ntiming predictor that balances the value of assistance with the cost of\ninterruption. In a user study with 12 participants completing cognitively\ndemanding tasks, ProMemAssist delivered more selective assistance and received\nhigher engagement compared to an LLM baseline system. Qualitative feedback\nhighlights the benefits of WM modeling for nuanced, context-sensitive support,\noffering design implications for more attentive and user-aware proactive\nagents.", "comment": "Accepted to UIST'25", "pdf_url": "http://arxiv.org/pdf/2507.21378v1", "cate": "cs.HC", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21311", "title": "VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction", "authors": ["Martin de La Gorce", "Charlie Hewitt", "Tibor Takacs", "Robert Gerdisch", "Zafiirah Hosenie", "Givi Meishvili", "Marek Kowalski", "Thomas J. Cashman", "Antonio Criminisi"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21311v1", "summary": "Virtual 3D meetings offer the potential to enhance copresence, increase\nengagement and thus improve effectiveness of remote meetings compared to\nstandard 2D video calls. However, representing people in 3D meetings remains a\nchallenge; existing solutions achieve high quality by using complex hardware,\nmaking use of fixed appearance via enrolment, or by inverting a pre-trained\ngenerative model. These approaches lead to constraints that are unwelcome and\nill-fitting for videoconferencing applications. We present the first method to\npredict 3D Gaussian reconstructions in real time from a single 2D webcam feed,\nwhere the 3D representation is not only live and realistic, but also authentic\nto the input video. By conditioning the 3D representation on each video frame\nindependently, our reconstruction faithfully recreates the input video from the\ncaptured viewpoint (a property we call authenticity), while generalizing\nrealistically to novel viewpoints. Additionally, we introduce a stability loss\nto obtain reconstructions that are temporally stable on video sequences. We\nshow that our method delivers state-of-the-art accuracy in visual quality and\nstability metrics compared to existing methods, and demonstrate our approach in\nlive one-to-one 3D meetings using only a standard 2D camera and display. This\ndemonstrates that our approach can allow anyone to communicate volumetrically,\nvia a method for 3D videoconferencing that is not only highly accessible, but\nalso realistic and authentic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21311v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21678", "title": "Predicting Maintenance Cessation of Open Source Software Repositories with An Integrated Feature Framework", "authors": ["Yiming Xu", "Runzhi He", "Hengzhi Ye", "Minghui Zhou", "Huaimin Wang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21678v1", "summary": "The maintenance risks of open source software (OSS) projects pose significant\nthreats to the quality, security, and resilience of modern software supply\nchains. While prior research has proposed diverse approaches for predicting OSS\nmaintenance risk -- leveraging signals ranging from surface features (e.g.,\nstars, commits) to social network analyses and behavioral patterns -- existing\nmethods often suffer from ambiguous operational definitions, limited\ninterpretability, and datasets of insufficient scale or generalizability. In\nthis work, we introduce ``maintenance cessation'', grounded in both explicit\narchival status and rigorous semantic analysis of project documentation.\nBuilding on this foundation, we curate a large-scale, longitudinal dataset of\n115,466 GitHub repositories -- encompassing 57,733 confirmed cessation events\n-- complemented by comprehensive, timeline-based behavioral features. We\npropose an integrated, multi-perspective feature framework for predicting\nmaintenance cessation, systematically combining user-centric features,\nmaintainer-centric features and project evolution features. AFT survival\nanalysis demonstrates a high C-index (0.846), substantially outperforming\nmodels relying only on surface features. Feature ablation and SHAP analysis\nfurther confirm the effectiveness and interpretability of our approach.\nFinally, we demonstrate real-world applicability by deploying a GBSA classifier\nin the openEuler ecosystem for proactive package risk screening. Our work\nestablishes a scalable, interpretable foundation for maintenance-risk\nprediction, enabling reproducible risk management across large-scale open\nsource ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21678v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21784", "title": "Towards Tight Bounds for Estimating Degree Distribution in Streaming and Query Models", "authors": ["Arijit Bishnu", "Debarshi Chanda", "Gopinath Mishra"], "categories": ["cs.DS", "cs.SI"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      28 pages, 13 Figures", "url": "http://arxiv.org/abs/2507.21784v1", "summary": "The degree distribution of a graph $G=(V,E)$, $|V|=n$, $|E|=m$ is one of the\nmost fundamental objects of study in the analysis of graphs as it embodies\nrelationship among entities. In particular, an important derived distribution\nfrom degree distribution is the complementary cumulative degree histogram\n(ccdh). The ccdh is a fundamental summary of graph structure, capturing, for\neach threshold $d$, the number of vertices with degree at least $d$. For\napproximating ccdh, we consider the $(\\varepsilon_D,\\varepsilon_R)$-BiCriteria\nMultiplicative Approximation, which allows for controlled multiplicative slack\nin both the domain and the range. The exact complexity of the problem was not\nknown and had been posed as an open problem in WOLA 2019 [Sublinear.info,\nProblem 98].\n  In this work, we first design an algorithm that can approximate ccdh if a\nsuitable vertex sample and an edge sample can be obtained and thus, the\nalgorithm is independent of any sublinear model. Next, we show that in the\nstreaming and query models, these samples can be obtained efficiently. On the\nother end, we establish the first lower bounds for this problem in both query\nand streaming models, and (almost) settle the complexity of the problem across\nboth the sublinear models.", "comment": "28 pages, 13 Figures", "pdf_url": "http://arxiv.org/pdf/2507.21784v1", "cate": "cs.DS", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21739", "title": "RRTO: A High-Performance Transparent Offloading System for Model Inference in Mobile Edge Computing", "authors": ["Zekai Sun", "Xiuxian Guan", "Zheng Lin", "Yuhao Qing", "Haoze Song", "Zihan Fang", "Zhe Chen", "Fangming Liu", "Heming Cui", "Wei Ni", "Jun Luo"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      15 pages, 12 figures,", "url": "http://arxiv.org/abs/2507.21739v1", "summary": "Deploying Machine Learning (ML) applications on resource-constrained mobile\ndevices remains challenging due to limited computational resources and poor\nplatform compatibility. While Mobile Edge Computing (MEC) offers\noffloading-based inference paradigm using GPU servers, existing approaches are\ndivided into non-transparent and transparent methods, with the latter\nnecessitating modifications to the source code. Non-transparent offloading\nachieves high performance but requires intrusive code modification, limiting\ncompatibility with diverse applications. Transparent offloading, in contrast,\noffers wide compatibility but introduces significant transmission delays due to\nper-operator remote procedure calls (RPCs). To overcome this limitation, we\npropose RRTO, the first high-performance transparent offloading system tailored\nfor MEC inference. RRTO introduces a record/replay mechanism that leverages the\nstatic operator sequence in ML models to eliminate repetitive RPCs. To reliably\nidentify this sequence, RRTO integrates a novel Operator Sequence Search\nalgorithm that detects repeated patterns, filters initialization noise, and\naccelerates matching via a two-level strategy. Evaluation demonstrates that\nRRTO achieves substantial reductions of up to 98% in both per-inference latency\nand energy consumption compared to state-of-the-art transparent methods and\nyields results comparable to non-transparent approaches, all without\nnecessitating any source code modification.", "comment": "15 pages, 12 figures,", "pdf_url": "http://arxiv.org/pdf/2507.21739v1", "cate": "cs.NI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21591", "title": "Hierarchical Graph Neural Network for Compressed Speech Steganalysis", "authors": ["Mustapha Hemis", "Hamza Kheddar", "Mohamed Chahine Ghanem", "Bachir Boudraa"], "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21591v1", "summary": "Steganalysis methods based on deep learning (DL) often struggle with\ncomputational complexity and challenges in generalizing across different\ndatasets. Incorporating a graph neural network (GNN) into steganalysis schemes\nenables the leveraging of relational data for improved detection accuracy and\nadaptability. This paper presents the first application of a Graph Neural\nNetwork (GNN), specifically the GraphSAGE architecture, for steganalysis of\ncompressed voice over IP (VoIP) speech streams. The method involves\nstraightforward graph construction from VoIP streams and employs GraphSAGE to\ncapture hierarchical steganalysis information, including both fine grained\ndetails and high level patterns, thereby achieving high detection accuracy.\nExperimental results demonstrate that the developed approach performs well in\nuncovering quantization index modulation (QIM)-based steganographic patterns in\nVoIP signals. It achieves detection accuracy exceeding 98 percent even for\nshort 0.5 second samples, and 95.17 percent accuracy under challenging\nconditions with low embedding rates, representing an improvement of 2.8 percent\nover the best performing state of the art methods. Furthermore, the model\nexhibits superior efficiency, with an average detection time as low as 0.016\nseconds for 0.5-second samples an improvement of 0.003 seconds. This makes it\nefficient for online steganalysis tasks, providing a superior balance between\ndetection accuracy and efficiency under the constraint of short samples with\nlow embedding rates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21591v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22042", "title": "A Nonlinear MPC Framework for Loco-Manipulation of Quadrupedal Robots with Non-Negligible Manipulator Dynamics", "authors": ["Ruturaj Sambhus", "Kapi Ketan Mehta", "Ali MirMohammad Sadeghi", "Basit Muhammad Imran", "Jeeseop Kim", "Taizoon Chunawala", "Vittorio Pastore", "Sujith Vijayan", "Kaveh Akbari Hamed"], "categories": ["cs.RO", "math.OC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22042v1", "summary": "Model predictive control (MPC) combined with reduced-order template models\nhas emerged as a powerful tool for trajectory optimization in dynamic legged\nlocomotion. However, loco-manipulation tasks performed by legged robots\nintroduce additional complexity, necessitating computationally efficient MPC\nalgorithms capable of handling high-degree-of-freedom (DoF) models. This letter\npresents a computationally efficient nonlinear MPC (NMPC) framework tailored\nfor loco-manipulation tasks of quadrupedal robots equipped with robotic\nmanipulators whose dynamics are non-negligible relative to those of the\nquadruped. The proposed framework adopts a decomposition strategy that couples\nlocomotion template models -- such as the single rigid body (SRB) model -- with\na full-order dynamic model of the robotic manipulator for torque-level control.\nThis decomposition enables efficient real-time solution of the NMPC problem in\na receding horizon fashion at 60 Hz. The optimal state and input trajectories\ngenerated by the NMPC for locomotion are tracked by a low-level nonlinear\nwhole-body controller (WBC) running at 500 Hz, while the optimal torque\ncommands for the manipulator are directly applied. The layered control\narchitecture is validated through extensive numerical simulations and hardware\nexperiments on a 15-kg Unitree Go2 quadrupedal robot augmented with a 4.4-kg\n4-DoF Kinova arm. Given that the Kinova arm dynamics are non-negligible\nrelative to the Go2 base, the proposed NMPC framework demonstrates robust\nstability in performing diverse loco-manipulation tasks, effectively handling\nexternal disturbances, payload variations, and uneven terrain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22042v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21285", "title": "Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions", "authors": ["Harsh Darji", "Thibaud Lutellier"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21285v1", "summary": "Large Language Models (LLMs) are increasingly used as coding assistants.\nHowever, the ambiguity of the developer's prompt often leads to incorrect code\ngeneration, as current models struggle to infer user intent without extensive\nprompt engineering or external context. This work aims to build an LLM-based\ncoding assistant that mimics the human code review process by asking\nclarification questions when faced with ambiguous or under-specified queries.\n  Our end-to-end system includes (1) a query classifier trained to detect\nunclear programming-related queries and (2) a fine-tuned LLM that generates\nclarification questions. Our evaluation shows that the fine-tuned LLM\noutperforms standard zero-shot prompting in generating useful clarification\nquestions. Furthermore, our user study indicates that users find the\nclarification questions generated by our model to outperform the baseline,\ndemonstrating that our coding assistant produces more accurate and helpful code\nresponses compared to baseline coding assistants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21285v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21190", "title": "Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21190v1", "summary": "We present a fully non neural learning framework based on Graph Laplacian\nWavelet Transforms (GLWT). Unlike traditional architectures that rely on\nconvolutional, recurrent, or attention based neural networks, our model\noperates purely in the graph spectral domain using structured multiscale\nfiltering, nonlinear shrinkage, and symbolic logic over wavelet coefficients.\nSignals defined on graph nodes are decomposed via GLWT, modulated with\ninterpretable nonlinearities, and recombined for downstream tasks such as\ndenoising and token classification. The system supports compositional reasoning\nthrough a symbolic domain-specific language (DSL) over graph wavelet\nactivations. Experiments on synthetic graph denoising and linguistic token\ngraphs demonstrate competitive performance against lightweight GNNs with far\ngreater transparency and efficiency. This work proposes a principled,\ninterpretable, and resource-efficient alternative to deep neural architectures\nfor learning on graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21190v1", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21411", "title": "InSituTale: Enhancing Augmented Data Storytelling with Physical Objects", "authors": ["Kentaro Takahira", "Yue Yu", "Takanori Fujiwara", "Suzuki Ryo", "Huamin Qu"], "categories": ["cs.HC", "cs.GR"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21411v1", "summary": "Augmented data storytelling enhances narrative delivery by integrating\nvisualizations with physical environments and presenter actions. Existing\nsystems predominantly rely on body gestures or speech to control\nvisualizations, leaving interactions with physical objects largely\nunderexplored. We introduce augmented physical data storytelling, an approach\nenabling presenters to manipulate visualizations through physical object\ninteractions. To inform this approach, we first conducted a survey of\ndata-driven presentations to identify common visualization commands. We then\nconducted workshops with nine HCI/VIS researchers to collect mappings between\nphysical manipulations and these commands. Guided by these insights, we\ndeveloped InSituTale, a prototype that combines object tracking via a depth\ncamera with Vision-LLM for detecting real-world events. Through physical\nmanipulations, presenters can dynamically execute various visualization\ncommands, delivering cohesive data storytelling experiences that blend physical\nand digital elements. A user study with 12 participants demonstrated that\nInSituTale enables intuitive interactions, offers high utility, and facilitates\nan engaging presentation experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21411v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21328", "title": "GLCP: Global-to-Local Connectivity Preservation for Tubular Structure Segmentation", "authors": ["Feixiang Zhou", "Zhuangzhi Gao", "He Zhao", "Jianyang Xie", "Yanda Meng", "Yitian Zhao", "Gregory Y. H. Lip", "Yalin Zheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025 (Oral)", "url": "http://arxiv.org/abs/2507.21328v1", "summary": "Accurate segmentation of tubular structures, such as vascular networks, plays\na critical role in various medical domains. A remaining significant challenge\nin this task is structural fragmentation, which can adversely impact downstream\napplications. Existing methods primarily focus on designing various loss\nfunctions to constrain global topological structures. However, they often\noverlook local discontinuity regions, leading to suboptimal segmentation\nresults. To overcome this limitation, we propose a novel Global-to-Local\nConnectivity Preservation (GLCP) framework that can simultaneously perceive\nglobal and local structural characteristics of tubular networks. Specifically,\nwe propose an Interactive Multi-head Segmentation (IMS) module to jointly learn\nglobal segmentation, skeleton maps, and local discontinuity maps, respectively.\nThis enables our model to explicitly target local discontinuity regions while\nmaintaining global topological integrity. In addition, we design a lightweight\nDual-Attention-based Refinement (DAR) module to further improve segmentation\nquality by refining the resulting segmentation maps. Extensive experiments on\nboth 2D and 3D datasets demonstrate that our GLCP achieves superior accuracy\nand continuity in tubular structure segmentation compared to several\nstate-of-the-art approaches. The source codes will be available at\nhttps://github.com/FeixiangZhou/GLCP.", "comment": "MICCAI 2025 (Oral)", "pdf_url": "http://arxiv.org/pdf/2507.21328v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21693", "title": "MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios", "authors": ["Basak Demirok", "Mucahid Kutlu", "Selin Mergen"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21693v1", "summary": "As large language models (LLMs) rapidly advance, their role in code\ngeneration has expanded significantly. While this offers streamlined\ndevelopment, it also creates concerns in areas like education and job\ninterviews. Consequently, developing robust systems to detect AI-generated code\nis imperative to maintain academic integrity and ensure fairness in hiring\nprocesses. In this study, we introduce MultiAIGCD, a dataset for AI-generated\ncode detection for Python, Java, and Go. From the CodeNet dataset's problem\ndefinitions and human-authored codes, we generate several code samples in Java,\nPython, and Go with six different LLMs and three different prompts. This\ngeneration process covered three key usage scenarios: (i) generating code from\nproblem descriptions, (ii) fixing runtime errors in human-written code, and\n(iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271\nAI-generated and 32,148 human-written code snippets. We also benchmark three\nstate-of-the-art AI-generated code detection models and assess their\nperformance in various test scenarios such as cross-model and cross-language.\nWe share our dataset and codes to support research in this field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21693v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2303.17251", "title": "Demystifying Misconceptions in Social Bots Research", "authors": ["Stefano Cresci", "Kai-Cheng Yang", "Angelo Spognardi", "Roberto Di Pietro", "Filippo Menczer", "Marinella Petrocchi"], "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.17251v4", "summary": "Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. Here, we discuss a broad set of consequential methodological and\nconceptual issues that affect current social bots research, illustrating each\nwith examples drawn from recent studies. More importantly, we demystify common\nmisconceptions, addressing fundamental points on how social bots research is\ndiscussed. Our analysis surfaces the need to discuss research about online\ndisinformation and manipulation in a rigorous, unbiased, and responsible way.\nThis article bolsters such effort by identifying and refuting common fallacious\narguments used by both proponents and opponents of social bots research, as\nwell as providing directions toward sound methodologies for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.17251v4", "cate": "cs.SI", "date": "2023-03-30", "updated": "2025-07-29"}
{"id": "2507.21974", "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks", "authors": ["Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Yibin Kang", "Haozhe Zhang", "Merouane Debbah", "Fadhel Ayed"], "categories": ["cs.AI", "cs.NI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21974v1", "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21974v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21237", "title": "(2,2)-GB Codes: Classification and Comparison with weight-4 Surface Codes", "authors": ["François Arnault", "Philippe Gaborit", "Nicolas Saussay"], "categories": ["cs.IT", "math.IT", "quant-ph"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21237v1", "summary": "Generalized Bicycle (GB) codes offer a compelling alternative to surface\ncodes for quantum error correction. This paper focuses on (2,2)-Generalized\nBicycle codes, constructed from pairs of binary circulant matrices with two\nnon-zero elements per row. Leveraging a lower bound on their minimum distance,\nwe construct three novel infinite families of optimal (2,2)-GB codes with\nparameters [[ 2n^2, 2, n ]], [[ 4r^2, 2, 2r ]], and [[(2t + 1)^2 + 1, 2, 2t + 1\n]]. These families match the performance of Kitaev's toric code and the best 2D\nweight-4 surface codes, reaching known theoretical limits. In particular, the\nsecond family breaks a long-held belief by providing optimal even-distance GB\ncodes, previously deemed impossible.\n  All are CSS codes derived from Cayley graphs. Recognizing that standard\nequivalence relations do not preserve their CSS structure, we introduce a\nCSS-preserving equivalence relation for rigorous comparison of Cayley\ngraph-based CSS codes. Under this framework, the first two families are\ninequivalent to all previously known optimal weight-4 2D surface codes, while\nthe third family is equivalent to the best-known odd-distance 2D surface code.\n  Finally, we classify all extremal, non-equivalent (2,2)-GB codes with length\nbelow 200 and present a comparison table with existing notable 2D weight-4\nsurface codes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21237v1", "cate": "cs.IT", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21640", "title": "GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN Anomaly Detection", "authors": ["Hyeong Seon Kim", "Huy Kang Kim"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Comments:12 pages, 3 figures, 3 tables; accepted to the 26th World Conference on Information Security Applications (WISA 2025)", "url": "http://arxiv.org/abs/2507.21640v1", "summary": "Modern in-vehicle networks face various cyber threats due to the lack of\nencryption and authentication in the Controller Area Network (CAN). To address\nthis security issue, this paper presents GUARD-CAN, an anomaly detection\nframework that combines graph-based representation learning with time-series\nmodeling. GUARD-CAN splits CAN messages into fixed-length windows and converts\neach window into a graph that preserves message order. To detect anomalies in\nthe timeaware and structure-aware context at the same window, GUARD-CAN takes\nadvantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network\n(GCN) to generate graph embedding vectors. The model groups these vectors into\nsequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporal\nanomaly patterns across the graphs. GUARD-CAN performs anomaly detection at\nboth the sequence level and the window level, and this allows multi-perspective\nperformance evaluation. The model also verifies the importance of window size\nselection through an analysis based on Shannon entropy. As a result, GUARD-CAN\nshows that the proposed model detects four types of CAN attacks (flooding,\nfuzzing, replay and spoofing attacks) effectively without relying on complex\nfeature engineering.", "comment": "Comments:12 pages, 3 figures, 3 tables; accepted to the 26th World\n  Conference on Information Security Applications (WISA 2025)", "pdf_url": "http://arxiv.org/pdf/2507.21640v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21065", "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      submitted to ICSR2025", "url": "http://arxiv.org/abs/2507.21065v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "comment": "submitted to ICSR2025", "pdf_url": "http://arxiv.org/pdf/2507.21065v1", "cate": "cs.CL", "date": "2025-05-25", "updated": "2025-05-25"}
{"id": "2507.21287", "title": "Structured Relevance Assessment for Robust Retrieval-Augmented Language Models", "authors": ["Aryan Raj", "Astitva Veer Garg", "Anitha D"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      International Conference on ICT for Sustainable Development (ICT4SD)", "url": "http://arxiv.org/abs/2507.21287v1", "summary": "Retrieval-Augmented Language Models (RALMs) face significant challenges in\nreducing factual errors, particularly in document relevance evaluation and\nknowledge integration. We introduce a framework for structured relevance\nassessment that enhances RALM robustness through improved document evaluation,\nbalanced intrinsic and external knowledge integration, and effective handling\nof unanswerable queries. Our approach employs a multi-dimensional scoring\nsystem that considers both semantic matching and source reliability, utilizing\nembedding-based relevance scoring and synthetic training data with\nmixed-quality documents. We implement specialized benchmarking on niche topics,\na knowledge integration mechanism, and an \"unknown\" response protocol for\nqueries with insufficient knowledge coverage. Preliminary evaluations\ndemonstrate significant reductions in hallucination rates and improved\ntransparency in reasoning processes. Our framework advances the development of\nmore reliable question-answering systems capable of operating effectively in\ndynamic environments with variable data quality. While challenges persist in\naccurately distinguishing credible information and balancing system latency\nwith thoroughness, this work represents a meaningful step toward enhancing RALM\nreliability.", "comment": "International Conference on ICT for Sustainable Development (ICT4SD)", "pdf_url": "http://arxiv.org/pdf/2507.21287v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21191", "title": "Exploring Adaptive Structure Learning for Heterophilic Graphs", "authors": ["Garv Kaushik"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Initially submitted this draft at Tiny ICLR 2025", "url": "http://arxiv.org/abs/2507.21191v1", "summary": "Graph Convolutional Networks (GCNs) gained traction for graph representation\nlearning, with recent attention on improving performance on heterophilic graphs\nfor various real-world applications. The localized feature aggregation in a\ntypical message-passing paradigm hinders the capturing of long-range\ndependencies between non-local nodes of the same class. The inherent\nconnectivity structure in heterophilic graphs often conflicts with information\nsharing between distant nodes of same class. We propose structure learning to\nrewire edges in shallow GCNs itself to avoid performance degradation in\ndownstream discriminative tasks due to oversmoothing. Parameterizing the\nadjacency matrix to learn connections between non-local nodes and extend the\nhop span of shallow GCNs facilitates the capturing of long-range dependencies.\nHowever, our method is not generalizable across heterophilic graphs and\nperforms inconsistently on node classification task contingent to the graph\nstructure.", "comment": "Initially submitted this draft at Tiny ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2507.21191v1", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21435", "title": "MindChat: Enhancing BCI Spelling with Large Language Models in Realistic Scenarios", "authors": ["JIaheng Wang", "Yucun Zhong", "Chengjie Huang", "Lin Yao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21435v1", "summary": "Brain-computer interface (BCI) spellers can render a new communication\nchannel independent of peripheral nervous system, which are especially valuable\nfor patients with severe motor disabilities. However, current BCI spellers\noften require users to type intended utterances letter-by-letter while spelling\nerrors grow proportionally due to inaccurate electroencephalogram (EEG)\ndecoding, largely impeding the efficiency and usability of BCIs in real-world\ncommunication. In this paper, we present MindChat, a large language model\n(LLM)-assisted BCI speller to enhance BCI spelling efficiency by reducing\nusers' manual keystrokes. Building upon prompt engineering, we prompt LLMs\n(GPT-4o) to continuously suggest context-aware word and sentence\ncompletions/predictions during spelling. Online copy-spelling experiments\nencompassing four dialogue scenarios demonstrate that MindChat saves more than\n62\\% keystrokes and over 32\\% spelling time compared with traditional BCI\nspellers. We envision high-speed BCI spellers enhanced by LLMs will potentially\nlead to truly practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21435v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21335", "title": "Analyzing the Sensitivity of Vision Language Models in Visual Question Answering", "authors": ["Monika Shah", "Sudarshan Balaji", "Somdeb Sarkhel", "Sanorita Dey", "Deepak Venugopal"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21335v1", "summary": "We can think of Visual Question Answering as a (multimodal) conversation\nbetween a human and an AI system. Here, we explore the sensitivity of Vision\nLanguage Models (VLMs) through the lens of cooperative principles of\nconversation proposed by Grice. Specifically, even when Grice's maxims of\nconversation are flouted, humans typically do not have much difficulty in\nunderstanding the conversation even though it requires more cognitive effort.\nHere, we study if VLMs are capable of handling violations to Grice's maxims in\na manner that is similar to humans. Specifically, we add modifiers to\nhuman-crafted questions and analyze the response of VLMs to these modifiers. We\nuse three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet\nand Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial\nresults seem to indicate that the performance of VLMs consistently diminish\nwith the addition of modifiers which indicates our approach as a promising\ndirection to understand the limitations of VLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21335v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21928", "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda", "authors": ["Christian Meske", "Tobias Hermanns", "Esther von der Weiden", "Kai-Uwe Loser", "Thorsten Berger"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21928v1", "summary": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21928v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.07660", "title": "Scalable Signed Exponential Random Graph Models under Local Dependence", "authors": ["Marc Schalberger", "Cornelius Fritz"], "categories": ["cs.SI", "stat.CO"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07660v3", "summary": "Traditional network analysis focuses on binary edges, while real-world\nrelationships are more nuanced, encompassing cooperation, neutrality, and\nconflict. The rise of negative edges in social media discussions spurred\ninterest in analyzing signed interactions, especially in polarized debates.\nHowever, the vast data generated by digital networks presents challenges for\ntraditional methods like Stochastic Block Models (SBM) and Exponential Family\nRandom Graph Models (ERGM), particularly due to the homogeneity assumption and\nglobal dependence, which become increasingly unrealistic as network size grows.\nTo address this, we propose a novel method that combines the strengths of SBM\nand ERGM while mitigating their weaknesses by incorporating local dependence\nbased on non-overlapping blocks. Our approach involves a two-step process:\nfirst, decomposing the network into sub-networks using SBM approximation, and\nthen estimating parameters using ERGM methods. We validate our method on large\nsynthetic networks and apply it to a signed Wikipedia network of thousands of\neditors. Through the use of local dependence, we find patterns consistent with\nstructural balance theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07660v3", "cate": "cs.SI", "date": "2025-07-10", "updated": "2025-07-29"}
{"id": "2507.22019", "title": "Not Here, Go There: Analyzing Redirection Patterns on the Web", "authors": ["Kritika Garg", "Sawood Alam", "Dietrich Ayala", "Michele C. Weigle", "Michael L. Nelson"], "categories": ["cs.DL", "cs.IR", "cs.NI"], "primary_category": "Subjects:       Digital Libraries (cs.DL)", "pdf_link": null, "comments": "Comments:      Extended version of the paper accepted at the 2025 ACM Web Science Conference (WebSci 2025)", "url": "http://arxiv.org/abs/2507.22019v1", "summary": "URI redirections are integral to web management, supporting structural\nchanges, SEO optimization, and security. However, their complexities affect\nusability, SEO performance, and digital preservation. This study analyzed 11\nmillion unique redirecting URIs, following redirections up to 10 hops per URI,\nto uncover patterns and implications of redirection practices. Our findings\nrevealed that 50% of the URIs terminated successfully, while 50% resulted in\nerrors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to\nHTTPS transitions, were prevalent, reflecting adherence to SEO best practices.\nNon-canonical redirects, often involving domain or path changes, highlighted\nsignificant web migrations, rebranding, and security risks. Notable patterns\nincluded \"sink\" URIs, where multiple redirects converged, ranging from traffic\nconsolidation by global websites to deliberate \"Rickrolling.\" The study also\nidentified 62,000 custom 404 URIs, almost half being soft 404s, which could\ncompromise SEO and user experience. These findings underscore the critical role\nof URI redirects in shaping the web while exposing challenges such as outdated\nURIs, server instability, and improper error handling. This research offers a\ndetailed analysis of URI redirection practices, providing insights into their\nprevalence, types, and outcomes. By examining a large dataset, we highlight\ninefficiencies in redirection chains and examine patterns such as the use of\n\"sink\" URIs and custom error pages. This information can help webmasters,\nresearchers, and digital archivists improve web usability, optimize resource\nallocation, and safeguard valuable online content.", "comment": "Extended version of the paper accepted at the 2025 ACM Web Science\n  Conference (WebSci 2025)", "pdf_url": "http://arxiv.org/pdf/2507.22019v1", "cate": "cs.DL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21363", "title": "Distributed Iterative ML and Message Passing for Grant-Free Cell-Free Massive MIMO Systems", "authors": ["Zilu Zhao", "Christian Forsch", "Laura Cottatellucci", "Dirk Slock"], "categories": ["cs.IT", "eess.SP", "math.IT", "stat.AP"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21363v1", "summary": "Cell-Free (CF) Massive Multiple-Input Multiple-Output (MaMIMO) is considered\none of the leading candidates for enabling next-generation wireless\ncommunication. With the growing interest in the Internet of Things (IoT), the\nGrant-Free (GF) access scheme has emerged as a promising solution to support\nmassive device connectivity. The integration of GF and CF-MaMIMO introduces\nsignificant challenges, particularly in designing distributed algorithms for\nactivity detection and pilot contamination mitigation. In this paper, we\npropose a distributed algorithm that addresses these challenges. Our method\nfirst employs a component-wise iterative distributed Maximum Likelihood (ML)\napproach for activity detection, which considers both the pilot and data\nportions of the received signal. This is followed by a Pseudo-Prior Hybrid\nVariational Bayes and Expectation Propagation (PP-VB-EP) algorithm for joint\ndata detection and channel estimation. Compared to conventional VB-EP, the\nproposed PP-VB-EP demonstrates improved convergence behavior and reduced\nsensitivity to initialization, especially when data symbols are drawn from a\nfinite alphabet. The pseudo prior used in PP-VB-EP acts as an approximated\nposterior and serves as a regularization term that prevents the Message Passing\n(MP) algorithm from diverging. To compute the pseudo prior in a distributed\nfashion, we further develop a distributed version of the Variable-Level\nExpectation Propagation (VL-EP) algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21363v1", "cate": "cs.IT", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21731", "title": "Modelling Arbitrary Computations in the Symbolic Model using an Equational Theory for Bounded Binary Circuits", "authors": ["Michiel Marcus", "Frank Westers", "Anne Nijsten"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21731v1", "summary": "In this work, we propose a class of equational theories for bounded binary\ncircuits that have the finite variant property. These theories could serve as a\nbuilding block to specify cryptographic primitive implementations and\nautomatically discover attacks as binary circuits in the symbolic model. We\nprovide proofs of equivalence between this class of equational theories and\nBoolean logic up to circuit size 3 and we provide the variant complexities and\nperformance benchmarks using Maude-NPA. This is the first result in this\ndirection and follow-up research is needed to improve the scalability of the\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21731v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21423", "title": "MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving", "authors": ["Thomas Monninger", "Zihan Zhang", "Zhipeng Mo", "Md Zafar Anwar", "Steffen Staab", "Sihao Ding"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2507.21423v1", "summary": "Autonomous driving requires an understanding of the static environment from\nsensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse\nmultiple inputs, and a vector decoder predicts a vectorized map representation\nfrom the latent BEV grid. However, traditional map construction models provide\ndeterministic point estimates, failing to capture uncertainty and the inherent\nambiguities of real-world environments, such as occlusions and missing lane\nmarkings. We propose MapDiffusion, a novel generative approach that leverages\nthe diffusion paradigm to learn the full distribution of possible vectorized\nmaps. Instead of predicting a single deterministic output from learned queries,\nMapDiffusion iteratively refines randomly initialized queries, conditioned on a\nBEV latent grid, to generate multiple plausible map samples. This allows\naggregating samples to improve prediction accuracy and deriving uncertainty\nestimates that directly correlate with scene ambiguity. Extensive experiments\non the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art\nperformance in online map construction, surpassing the baseline by 5% in\nsingle-sample performance. We further show that aggregating multiple samples\nconsistently improves performance along the ROC curve, validating the benefit\nof distribution modeling. Additionally, our uncertainty estimates are\nsignificantly higher in occluded areas, reinforcing their value in identifying\nregions with ambiguous sensor input. By modeling the full map distribution,\nMapDiffusion enhances the robustness and reliability of online vectorized HD\nmap construction, enabling uncertainty-aware decision-making for autonomous\nvehicles in complex environments.", "comment": "Accepted for 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.21423v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21360", "title": "Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures", "authors": ["Nicholas Botti", "Flora Haberkorn", "Charlotte Hoopes", "Shaun Khan"], "categories": ["cs.AI", "cs.HC", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21360v1", "summary": "We utilize a within-subjects design with randomized task assignments to\nunderstand the effectiveness of using an AI retrieval augmented generation\n(RAG) tool to assist analysts with an information extraction and data\nannotation task. We replicate an existing, challenging real-world annotation\ntask with complex multi-part criteria on a set of thousands of pages of public\ndisclosure documents from global systemically important banks (GSIBs) with\nheterogeneous and incomplete information content. We test two treatment\nconditions. First, a \"naive\" AI use condition in which annotators use only the\ntool and must accept the first answer they are given. And second, an\n\"interactive\" AI treatment condition where annotators use the tool\ninteractively, and use their judgement to follow-up with additional information\nif necessary. Compared to the human-only baseline, the use of the AI tool\naccelerated task execution by up to a factor of 10 and enhanced task accuracy,\nparticularly in the interactive condition. We find that when extrapolated to\nthe full task, these methods could save up to 268 hours compared to the\nhuman-only approach. Additionally, our findings suggest that annotator skill,\nnot just with the subject matter domain, but also with AI tools, is a factor in\nboth the accuracy and speed of task performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21360v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21196", "title": "EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks", "authors": ["Abir Ray"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21196v1", "summary": "We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework\nthat integrates digital twin simulations and generative AI-driven scenario\ntraining to significantly enhance edge intelligence in military networks.\nEdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized\nwith real-world edge devices, to provide a secure, realistic environment for\ntraining and validation. Leveraging generative AI methods, such as diffusion\nmodels and transformers, the system creates diverse and adversarial scenarios\nfor robust simulation-based agent training. Our multi-layer architecture\nincludes: (1) on-device edge intelligence; (2) digital twin synchronization;\nand (3) generative scenario training. Experimental simulations demonstrate\nnotable improvements over EdgeAgentX, including faster learning convergence,\nhigher network throughput, reduced latency, and improved resilience against\njamming and node failures. A case study involving a complex tactical scenario\nwith simultaneous jamming attacks, agent failures, and increased network loads\nillustrates how EdgeAgentX-DT sustains operational performance, whereas\nbaseline methods fail. These results highlight the potential of\ndigital-twin-enabled generative training to strengthen edge AI deployments in\ncontested environments.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21196v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21462", "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals", "authors": ["Tingying He", "Maggie McCracken", "Daniel Hajas", "Sarah Creem-Regehr", "Alexander Lex"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21462v1", "summary": "We investigate whether tactile charts support comprehension and learning of\ncomplex visualizations for blind and low-vision (BLV) individuals and\ncontribute four tactile chart designs and an interview study. Visualizations\nare powerful tools for conveying data, yet BLV individuals typically can rely\nonly on assistive technologies -- primarily alternative texts -- to access this\ninformation. Prior research shows the importance of mental models of chart\ntypes for interpreting these descriptions, yet BLV individuals have no means to\nbuild such a mental model based on images of visualizations. Tactile charts\nshow promise to fill this gap in supporting the process of building mental\nmodels. Yet studies on tactile data representations mostly focus on simple\nchart types, and it is unclear whether they are also appropriate for more\ncomplex charts as would be found in scientific publications. Working with two\nBLV researchers, we designed 3D-printed tactile template charts with\nexploration instructions for four advanced chart types: UpSet plots, violin\nplots, clustered heatmaps, and faceted line charts. We then conducted an\ninterview study with 12 BLV participants comparing whether using our tactile\ntemplates improves mental models and understanding of charts and whether this\nunderstanding translates to novel datasets experienced through alt texts.\nThematic analysis shows that tactile models support chart type understanding\nand are the preferred learning method by BLV individuals. We also report\nparticipants' opinions on tactile chart design and their role in BLV education.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21462v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21349", "title": "Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging", "authors": ["Amirmohammad Shamaei", "Alexander Stebner", "Salome", "Bosshart", "Johanna Ospel", "Gouri Ginde", "Mariana Bento", "Roberto Souza"], "categories": ["cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21349v1", "summary": "Magnetic resonance imaging (MRI) is a crucial medical imaging modality.\nHowever, long acquisition times remain a significant challenge, leading to\nincreased costs, and reduced patient comfort. Recent studies have shown the\npotential of using deep learning models that incorporate information from prior\nsubject-specific MRI scans to improve reconstruction quality of present scans.\nIntegrating this prior information requires registration of the previous scan\nto the current image reconstruction, which can be time-consuming. We propose a\nnovel deep-learning-based MRI reconstruction framework which consists of an\ninitial reconstruction network, a deep registration model, and a\ntransformer-based enhancement network. We validated our method on a\nlongitudinal dataset of T1-weighted MRI scans with 2,808 images from 18\nsubjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics\nconfirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon\nsigned-rank test). Furthermore, we analyzed the impact of our MRI\nreconstruction method on the downstream task of brain segmentation and observed\nimproved accuracy and volumetric agreement with reference segmentations. Our\napproach also achieved a substantial reduction in total reconstruction time\ncompared to methods that use traditional registration algorithms, making it\nmore suitable for real-time clinical applications. The code associated with\nthis work is publicly available at\nhttps://github.com/amirshamaei/longitudinal-mri-deep-recon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21349v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21952", "title": "DeepGo: Predictive Directed Greybox Fuzzing", "authors": ["Peihong Lin", "Pengfei Wang", "Xu Zhou", "Wei Xie", "Gen Zhang", "Kai Lu"], "categories": ["cs.SE", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21952v1", "summary": "The state-of-the-art DGF techniques redefine and optimize the fitness metric\nto reach the target sites precisely and quickly. However, optimizations for\nfitness metrics are mainly based on heuristic algorithms, which usually rely on\nhistorical execution information and lack foresight on paths that have not been\nexercised yet. Thus, those hard-to-execute paths with complex constraints would\nhinder DGF from reaching the targets, making DGF less efficient. In this paper,\nwe propose DeepGo, a predictive directed grey-box fuzzer that can combine\nhistorical and predicted information to steer DGF to reach the target site via\nan optimal path. We first propose the path transition model, which models DGF\nas a process of reaching the target site through specific path transition\nsequences. The new seed generated by mutation would cause the path transition,\nand the path corresponding to the high-reward path transition sequence\nindicates a high likelihood of reaching the target site through it. Then, to\npredict the path transitions and the corresponding rewards, we use deep neural\nnetworks to construct a Virtual Ensemble Environment (VEE), which gradually\nimitates the path transition model and predicts the rewards of path transitions\nthat have not been taken yet. To determine the optimal path, we develop a\nReinforcement Learning for Fuzzing (RLF) model to generate the transition\nsequences with the highest sequence rewards. The RLF model can combine\nhistorical and predicted path transitions to generate the optimal path\ntransition sequences, along with the policy to guide the mutation strategy of\nfuzzing. Finally, to exercise the high-reward path transition sequence, we\npropose the concept of an action group, which comprehensively optimizes the\ncritical steps of fuzzing to realize the optimal path to reach the target\nefficiently.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21952v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.17626", "title": "Quotegraph: A Social Network Extracted from Millions of News Quotations", "authors": ["Marko Čuljak", "Robert West", "Andreas Spitz", "Akhil Arora"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17626v2", "summary": "We introduce Quotegraph, a novel large-scale social network derived from\nspeaker-attributed quotations in English news articles published between 2008\nand 2020. Quotegraph consists of 528 thousand unique nodes and 8.63 million\ndirected edges, pointing from speakers to persons they mention. The nodes are\nlinked to their corresponding items in Wikidata, thereby endowing the dataset\nwith detailed biographic entity information, including nationality, gender, and\npolitical affiliation. Being derived from Quotebank, a massive corpus of\nquotations, relations in Quotegraph are additionally enriched with the\ninformation about the context in which they are featured. Each part of the\nnetwork construction pipeline is language agnostic, enabling the construction\nof similar datasets based on non-English news corpora. We believe Quotegraph is\na compelling resource for computational social scientists, complementary to\nonline social networks, with the potential to yield novel insights into the\nbehavior of public figures and how it is captured in the news.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17626v2", "cate": "cs.SI", "date": "2025-07-23", "updated": "2025-07-29"}
{"id": "2412.13208", "title": "Wall-Proximity Matters: Understanding the Effect of Device Placement with Respect to the Wall for Indoor Wi-Fi Sensing", "authors": ["He Wang", "Yunpeng Ge", "Ivan Wang-Hei Ho"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2412.13208v3", "summary": "Wi-Fi sensing has been extensively explored for various applications,\nincluding vital sign monitoring, human activity recognition, indoor\nlocalization, and tracking. However, practical implementation in real-world\nscenarios is hindered by unstable sensing performance and limited knowledge of\nwireless sensing coverage. While previous works have aimed to address these\nchallenges, they have overlooked the impact of walls on dynamic sensing\ncapabilities in indoor environments. To fill this gap, we present a theoretical\nmodel that accounts for the effect of wall-device distance on sensing coverage.\nBy incorporating both the wall-reflected path and the line-of-sight (LoS) path\nfor dynamic signals, we develop a comprehensive sensing coverage model tailored\nfor indoor environments. This model demonstrates that strategically deploying\nthe transmitter and receiver in proximity to the wall within a specific range\ncan significantly expand sensing coverage. We assess the performance of our\nmodel through experiments in respiratory monitoring and stationary crowd\ncounting applications, showcasing a notable 11.2% improvement in counting\naccuracy. These findings pave the way for optimized deployment strategies in\nWi-Fi sensing, facilitating more effective and accurate sensing solutions\nacross various applications.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2412.13208v3", "cate": "cs.NI", "date": "2024-12-03", "updated": "2025-07-29"}
{"id": "2507.21623", "title": "On the Hulls of Group Codes", "authors": ["Xiheng Deng", "Yuan Ren"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted to Designs, Codes and Cryptography. Submission ID 64937899-ea28-4559-9d88-9de59566a860", "url": "http://arxiv.org/abs/2507.21623v1", "summary": "Let $\\mathbb {F}_q$ be a finite field and $G$ a finte group with $(|G|,q)=1$.\nBy a group code in $\\mathbb {F}_q[G]$ we mean a two-sided ideal in $\\mathbb\n{F}_q[G]$. We will prove a general criterion for the existence of group codes\nwith given hull dimension, and then apply it to deduce explicit criterions for\nexistence of group codes with hull dimension $\\leq3$. In particular our\ncriterion for the existence of $1$-dimensional hulls generalizes that of\nprivious work which consider only abelian groups $G$.", "comment": "Submitted to Designs, Codes and Cryptography. Submission ID\n  64937899-ea28-4559-9d88-9de59566a860", "pdf_url": "http://arxiv.org/pdf/2507.21623v1", "cate": "cs.IT", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21817", "title": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?", "authors": ["Yikun Li", "Ngoc Tan Bui", "Ting Zhang", "Martin Weyssow", "Chengran Yang", "Xin Zhou", "Jinfeng Jiang", "Junkai Chen", "Huihui Huang", "Huu Hung Nguyen", "Chiok Yew Ho", "Jie Tan", "Ruiyin Li", "Yide Yin", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21817v1", "summary": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 40.6% when evaluated on independent data, sometimes underperforming\nrandom guessing.\n  To address these limitations, we present a three-part solution. First, we\nintroduce a manually curated test dataset, BenchVul, covering the MITRE Top 25\nMost Dangerous CWEs. Second, we construct a high-quality training dataset,\nTitanVul, comprising 35,045 functions by aggregating seven public sources and\napplying deduplication and validation using a novel multi-agent LLM framework.\nThird, we propose a Realistic Vulnerability Generation (RVG) framework, which\nsynthesizes context-aware vulnerability examples for underrepresented but\ncritical CWE types through simulated development workflows.\n  Our evaluation shows the strengths of each component in closing the\ngeneralization gap. First, BenchVul shows the limitations of self-testing:\nmodels trained on existing datasets, such as BigVul and PrimeVul, experience\nperformance drops on BenchVul (from 0.776 to 0.519 and from 0.567 to 0.337).\nSecond, training models on TitanVul demonstrates improved generalization, with\nmodel performance increasing from 0.584 when evaluated on the same dataset to\n0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated\ndata yields further gains, increasing model performance by 14.0% to 0.874.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21817v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21450", "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation", "authors": ["Bolei Chen", "Jiaxu Kang", "Yifei Wang", "Ping Zhong", "Qi Wu", "Jianxin Wang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to AAAI 2026", "url": "http://arxiv.org/abs/2507.21450v1", "summary": "Vision Language Navigation (VLN) typically requires agents to navigate to\nspecified objects or remote regions in unknown scenes by obeying linguistic\ncommands. Such tasks require organizing historical visual observations for\nlinguistic grounding, which is critical for long-sequence navigational\ndecisions. However, current agents suffer from overly detailed scene\nrepresentation and ambiguous vision-language alignment, which weaken their\ncomprehension of navigation-friendly high-level scene priors and easily lead to\nbehaviors that violate linguistic commands. To tackle these issues, we propose\na navigation policy by recursively summarizing along-the-way visual\nperceptions, which are adaptively aligned with commands to enhance linguistic\ngrounding. In particular, by structurally modeling historical trajectories as\ncompact neural grids, several Recursive Visual Imagination (RVI) techniques are\nproposed to motivate agents to focus on the regularity of visual transitions\nand semantic scene layouts, instead of dealing with misleading geometric\ndetails. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to\nalign the learned situational memories with different linguistic components\npurposefully. Such fine-grained semantic matching facilitates the accurate\nanticipation of navigation actions and progress. Our navigation policy\noutperforms the state-of-the-art methods on the challenging VLN-CE and\nObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.", "comment": "Submitted to AAAI 2026", "pdf_url": "http://arxiv.org/pdf/2507.21450v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21383", "title": "Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect", "authors": ["Chunan Tong"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21383v1", "summary": "Supply chain management faces significant challenges, including demand\nfluctuations, inventory imbalances, and amplified upstream order variability\ndue to the bullwhip effect. Traditional methods, such as simple moving\naverages, struggle to address dynamic market conditions. Emerging machine\nlearning techniques, including LSTM, reinforcement learning, and XGBoost, offer\npotential solutions but are limited by computational complexity, training\ninefficiencies, or constraints in time-series modeling. Liquid Neural Networks,\ninspired by dynamic biological systems, present a promising alternative due to\ntheir adaptability, low computational cost, and robustness to noise, making\nthem suitable for real-time decision-making and edge computing. Despite their\nsuccess in applications like autonomous vehicles and medical monitoring, their\npotential in supply chain optimization remains underexplored. This study\nintroduces a hybrid LNN and XGBoost model to optimize ordering strategies in\nmulti-tier supply chains. By leveraging LNN's dynamic feature extraction and\nXGBoost's global optimization capabilities, the model aims to mitigate the\nbullwhip effect and enhance cumulative profitability. The research investigates\nhow local and global synergies within the hybrid framework address the dual\ndemands of adaptability and efficiency in SCM. The proposed approach fills a\ncritical gap in existing methodologies, offering an innovative solution for\ndynamic and efficient supply chain management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21383v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21197", "title": "AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction", "authors": ["Ling Liao", "Eva Aagaard"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures", "url": "http://arxiv.org/abs/2507.21197v1", "summary": "Machine learning interpretation has primarily been leveraged to build\nclinician trust and uncover actionable insights in EHRs. However, the intrinsic\ncomplexity and heterogeneity of EHR data limit its effectiveness in guiding\nsubgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven\nframework that transforms interpretability insights into actionable guidance\nfor tailoring model training and evaluation across subpopulations within\nindividual hospital systems. Evaluated on three large-scale EHR datasets -\nGOSSIS-1-eICU, WiDS, and MIMIC-IV - AdaptHetero consistently identifies\nheterogeneous model behaviors in predicting ICU mortality, in-hospital death,\nand hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised\nclustering, the framework enhances the identification of clinically meaningful\nsubgroup-specific characteristics, leading to improved predictive performance.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.21197v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21490", "title": "Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning", "authors": ["Hannah Kim", "Sergei L. Kosakovsky Pond", "Stephen MacNeil"], "categories": ["cs.HC", "cs.CY", "cs.IR", "J.3; K.3.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education (FIE) Conference preprint", "url": "http://arxiv.org/abs/2507.21490v1", "summary": "This full research paper investigates the impact of generative AI (GenAI) on\nthe learner experience, with a focus on how learners engage with and utilize\nthe information it provides. In e-learning environments, learners often need to\nnavigate a complex information space on their own. This challenge is further\ncompounded in interdisciplinary fields like bioinformatics, due to the varied\nprior knowledge and backgrounds. In this paper, we studied how GenAI influences\ninformation search in bioinformatics research: (1) How do interactions with a\nGenAI chatbot influence learner orienteering behaviors?; and (2) How do\nlearners identify information scent in GenAI chatbot responses? We adopted an\nautoethnographic approach to investigate these questions. GenAI was found to\nsupport orienteering once a learning plan was established, but it was\ncounterproductive prior to that. Moreover, traditionally value-rich information\nsources such as bullet points and related terms proved less effective when\napplied to GenAI responses. Information scents were primarily recognized\nthrough the presence or absence of prior knowledge of the domain. These\nfindings suggest that GenAI should be adopted into e-learning environments with\ncaution, particularly in interdisciplinary learning contexts.", "comment": "9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education\n  (FIE) Conference preprint", "pdf_url": "http://arxiv.org/pdf/2507.21490v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21353", "title": "Group Relative Augmentation for Data Efficient Action Detection", "authors": ["Deep Anil Patel", "Iain Melvin", "Zachary Izzo", "Martin Renqiang Min"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21353v1", "summary": "Adapting large Video-Language Models (VLMs) for action detection using only a\nfew examples poses challenges like overfitting and the granularity mismatch\nbetween scene-level pre-training and required person-centric understanding. We\npropose an efficient adaptation strategy combining parameter-efficient tuning\n(LoRA) with a novel learnable internal feature augmentation. Applied within the\nfrozen VLM backbone using FiLM, these augmentations generate diverse feature\nvariations directly relevant to the task. Additionally, we introduce a\ngroup-weighted loss function that dynamically modulates the training\ncontribution of each augmented sample based on its prediction divergence\nrelative to the group average. This promotes robust learning by prioritizing\ninformative yet reasonable augmentations. We demonstrate our method's\neffectiveness on complex multi-label, multi-person action detection datasets\n(AVA, MOMA), achieving strong mAP performance and showcasing significant data\nefficiency for adapting VLMs from limited examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21353v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21954", "title": "Fine-Tuning Code Language Models to Detect Cross-Language Bugs", "authors": ["Zengyang Li", "Yimeng Li", "Binbin Huang", "Peng Liang", "Ran Mo", "Hui Liu", "Yutao Ma"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      33 pages, 6 images, 9 tables, Manuscript submitted to a journal (2025)", "url": "http://arxiv.org/abs/2507.21954v1", "summary": "Multilingual programming, which involves using multiple programming languages\n(PLs) in a single project, is increasingly common due to its benefits. However,\nit introduces cross-language bugs (CLBs), which arise from interactions between\ndifferent PLs and are difficult to detect by single-language bug detection\ntools. This paper investigates the potential of pre-trained code language\nmodels (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language\ncode identification tool, and constructed a CLB dataset involving three PL\ncombinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction\ntypes. We fine-tuned 13 CodeLMs on this dataset and evaluated their\nperformance, analyzing the effects of dataset size, token sequence length, and\ncode comments. Results show that all CodeLMs performed poorly before\nfine-tuning, but exhibited varying degrees of performance improvement after\nfine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,\nsmall fine-tuned CodeLMs tended to performe better than large ones. CodeLMs\nfine-tuned on single-language bug datasets performed poorly on CLB detection,\ndemonstrating the distinction between CLBs and single-language bugs.\nAdditionally, increasing the fine-tuning dataset size significantly improved\nperformance, while longer token sequences did not necessarily improve the model\nperformance. The impact of code comments varied across models. Some fine-tuned\nCodeLMs' performance was improved, while others showed degraded performance.", "comment": "33 pages, 6 images, 9 tables, Manuscript submitted to a journal\n  (2025)", "pdf_url": "http://arxiv.org/pdf/2507.21954v1", "cate": "cs.SE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.20797", "title": "\"Whose Side Are You On?\" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection", "authors": ["Muhammad Haroon", "Magdalena Wojcieszak", "Anshuman Chhabra"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.20797v2", "summary": "The rapid growth of social media platforms has led to concerns about\nradicalization, filter bubbles, and content bias. Existing approaches to\nclassifying ideology are limited in that they require extensive human effort,\nthe labeling of large datasets, and are not able to adapt to evolving\nideological contexts. This paper explores the potential of Large Language\nModels (LLMs) for classifying the political ideology of online content in the\ncontext of the two-party US political spectrum through in-context learning\n(ICL). Our extensive experiments involving demonstration selection in\nlabel-balanced fashion, conducted on three datasets comprising news articles\nand YouTube videos, reveal that our approach significantly outperforms\nzero-shot and traditional supervised methods. Additionally, we evaluate the\ninfluence of metadata (e.g., content source and descriptions) on ideological\nclassification and discuss its implications. Finally, we show how providing the\nsource for political and non-political content influences the LLM's\nclassification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.20797v2", "cate": "cs.CL", "date": "2025-03-23", "updated": "2025-07-29"}
{"id": "2502.03885", "title": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers", "authors": ["Chenchen Shou", "Guyue Liu", "Hao Nie", "Huaiyu Meng", "Yu Zhou", "Yimin Jiang", "Wenqing Lv", "Yelong Xu", "Yuanwei Lu", "Zhang Chen", "Yanbo Yu", "Yichen Shen", "Yibo Zhu", "Daxin Jiang"], "categories": ["cs.NI", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.03885v5", "summary": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 take a middle-ground approach, but the fault explosion radius remains\nlarge at the cube level (e.g., 64 TPUs).\n  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt to variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh)-based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios are under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.03885v5", "cate": "cs.NI", "date": "2025-02-06", "updated": "2025-07-29"}
{"id": "2507.21776", "title": "Beamforming Saturation in Two-Timescale RIS-assisted Communication", "authors": ["Masoud Sadeghian", "Angel Lozano", "Gabor Fodor"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21776v1", "summary": "This paper considers wireless communication assisted by a reconfigurable\nintelligent surface (RIS), focusing on the two-timescale approach, in which the\nRIS phase shifts are optimized based on channel statistics to mitigate the\noverheads associated with channel estimation. It is shown that, while the power\ncaptured by the RIS scales linearly with the number of its elements, the\ntwo-timescale beamforming gain upon re-radiation towards the receiver saturates\nrapidly as the number of RIS elements increases, for a broad class of power\nangular spectra (PAS). The ultimate achievable gain is determined by the decay\nrate of the PAS in the angular domain, which directly influences how rapidly\nspatial correlations between RIS elements diminish. The implications of this\nsaturation on the effectiveness of RIS-assisted communications are discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21776v1", "cate": "cs.IT", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21904", "title": "Privacy-Preserving Anonymization of System and Network Event Logs Using Salt-Based Hashing and Temporal Noise", "authors": ["Shreyas Bargale", "Akshit Vakati Venkata", "Jaimandeep Singh", "Chester Rebeiro"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21904v1", "summary": "System and network event logs are essential for security analytics, threat\ndetection, and operational monitoring. However, these logs often contain\nPersonally Identifiable Information (PII), raising significant privacy concerns\nwhen shared or analyzed. A key challenge in log anonymization is balancing\nprivacy protection with the retention of sufficient structure for meaningful\nanalysis. Overly aggressive anonymization can destroy contextual integrity,\nwhile weak techniques risk re-identification through linkage or inference\nattacks. This paper introduces novel field-specific anonymization methods that\naddress this trade-off. For IP addresses, we propose a salt-based hashing\ntechnique applied at the per-octet level, preserving both subnet and host\nstructure to enable correlation across various log entries while ensuring\nnon-reversibility. For port numbers, full-value hashing with range mapping\nmaintains interpretability. We also present an order-preserving timestamp\nanonymization scheme using adaptive noise injection, which obfuscates exact\ntimes without disrupting event sequences. An open-source tool implementing\nthese techniques has been released to support practical deployment and\nreproducible research. Evaluations using entropy metrics, collision rates, and\nresidual leakage analysis demonstrate that the proposed approach effectively\nprotects privacy while preserving analytical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21904v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21452", "title": "Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training", "authors": ["Sodtavilan Odonchimed", "Tatsuya Matsushima", "Simon Holk", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21452v1", "summary": "Diffusion Policies (DPs) have attracted attention for their ability to\nachieve significant accuracy improvements in various imitation learning tasks.\nHowever, DPs depend on Diffusion Models, which require multiple noise removal\nsteps to generate a single action, resulting in long generation times. To solve\nthis problem, knowledge distillation-based methods such as Consistency Policy\n(CP) have been proposed. However, these methods require a significant amount of\ntraining time, especially for difficult tasks. In this study, we propose RAGDP\n(Retrieve-Augmented Generation for Diffusion Policies) as a novel framework\nthat eliminates the need for additional training using a knowledge base to\nexpedite the inference of pre-trained DPs. In concrete, RAGDP encodes\nobservation-action pairs through the DP encoder to construct a vector database\nof expert demonstrations. During inference, the current observation is\nembedded, and the most similar expert action is extracted. This extracted\naction is combined with an intermediate noise removal step to reduce the number\nof steps required compared to the original diffusion step. We show that by\nusing RAGDP with the base model and existing acceleration methods, we improve\nthe accuracy and speed trade-off with no additional training. Even when\naccelerating the models 20 times, RAGDP maintains an advantage in accuracy,\nwith a 7% increase over distillation models such as CP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21452v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21389", "title": "Teaching Language Models To Gather Information Proactively", "authors": ["Tenghao Huang", "Sihao Chen", "Muhao Chen", "Jonathan May", "Longqi Yang", "Mengting Wan", "Pei Zhou"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21389v1", "summary": "Large language models (LLMs) are increasingly expected to function as\ncollaborative partners, engaging in back-and-forth dialogue to solve complex,\nambiguous problems. However, current LLMs often falter in real-world settings,\ndefaulting to passive responses or narrow clarifications when faced with\nincomplete or under-specified prompts, falling short of proactively gathering\nthe missing information that is crucial for high-quality solutions. In this\nwork, we introduce a new task paradigm: proactive information gathering, where\nLLMs must identify gaps in the provided context and strategically elicit\nimplicit user knowledge through targeted questions. To systematically study and\ntrain this capability, we design a scalable framework that generates partially\nspecified, real-world tasks, masking key information and simulating authentic\nambiguity. Within this setup, our core innovation is a reinforcement finetuning\nstrategy that rewards questions that elicit genuinely new, implicit user\ninformation -- such as hidden domain expertise or fine-grained requirements --\nthat would otherwise remain unspoken. Experiments demonstrate that our trained\nQwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic\nevaluation metrics. More importantly, human evaluation reveals that\nclarification questions and final outlines generated by our model are favored\nby human annotators by 42% and 28% respectively. Together, these results\nhighlight the value of proactive clarification in elevating LLMs from passive\ntext generators to genuinely collaborative thought partners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21389v1", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21198", "title": "Uncovering Gradient Inversion Risks in Practical Language Model Training", "authors": ["Xinguo Feng", "Zhongkui Ma", "Zihan Wang", "Eu Joe Chegne", "Mengyao Ma", "Alsharif Abuadbba", "Guangdong Bai"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 Pages, 5 figures, 10 tables. Accepted by ACM CCS 2024", "url": "http://arxiv.org/abs/2507.21198v1", "summary": "The gradient inversion attack has been demonstrated as a significant privacy\nthreat to federated learning (FL), particularly in continuous domains such as\nvision models. In contrast, it is often considered less effective or highly\ndependent on impractical training settings when applied to language models, due\nto the challenges posed by the discrete nature of tokens in text data. As a\nresult, its potential privacy threats remain largely underestimated, despite FL\nbeing an emerging training method for language models. In this work, we propose\na domain-specific gradient inversion attack named Grab (gradient inversion with\nhybrid optimization). Grab features two alternating optimization processes to\naddress the challenges caused by practical training settings, including a\nsimultaneous optimization on dropout masks between layers for improved token\nrecovery and a discrete optimization for effective token sequencing. Grab can\nrecover a significant portion (up to 92.9% recovery rate) of the private\ntraining data, outperforming the attack strategy of utilizing discrete\noptimization with an auxiliary model by notable improvements of up to 28.9%\nrecovery rate in benchmark settings and 48.5% recovery rate in practical\nsettings. Grab provides a valuable step forward in understanding this privacy\nthreat in the emerging FL training mode of language models.", "comment": "15 Pages, 5 figures, 10 tables. Accepted by ACM CCS 2024", "pdf_url": "http://arxiv.org/pdf/2507.21198v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21654", "title": "AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind", "authors": ["Meryem Yilmaz Soylu", "Jeonghyun Lee", "Jui-Tse Hung", "Christopher Zhang Cui", "David A. Joyner"], "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.6"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      34 pages, 1 figure, 3 tables", "url": "http://arxiv.org/abs/2507.21654v1", "summary": "As Artificial Intelligence (AI) tools become increasingly embedded in higher\neducation, understanding how students interact with these systems is essential\nto supporting effective learning. This study examines how students' AI literacy\nand prior exposure to AI technologies shape their perceptions of Socratic Mind,\nan interactive AI-powered formative assessment tool. Drawing on\nSelf-Determination Theory and user experience research, we analyze\nrelationships among AI literacy, perceived usability, satisfaction, engagement,\nand perceived learning effectiveness. Data from 309 undergraduates in Computer\nScience and Business courses were collected through validated surveys. Partial\nleast squares structural equation modeling showed that AI literacy - especially\nself-efficacy, conceptual understanding, and application skills - significantly\npredicts usability, satisfaction, and engagement. Usability and satisfaction,\nin turn, strongly predict perceived learning effectiveness, while prior AI\nexposure showed no significant effect. These findings highlight that AI\nliteracy, rather than exposure alone, shapes student experiences. Designers\nshould integrate adaptive guidance and user-centered features to support\ndiverse literacy levels, fostering inclusive, motivating, and effective\nAI-based learning environments.", "comment": "34 pages, 1 figure, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.21654v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21358", "title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy", "authors": ["Jicheng Yuan", "Manh Nguyen Duc", "Qian Liu", "Manfred Hauswirth", "Danh Le Phuoc"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21358v2", "summary": "Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.", "comment": "The manuscript has been accepted by ICONIP2025", "pdf_url": "http://arxiv.org/pdf/2507.21358v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2507.21722", "title": "Identification of Design Recommendations for Augmented Reality Authors in Corporate Training", "authors": ["Stefan Graser", "Martin Schrepp", "Stephan Böhm"], "categories": ["cs.HC", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      9 pages, 1 table, 1 figure", "url": "http://arxiv.org/abs/2507.21722v1", "summary": "Innovative technologies, such as Augmented Reality (AR), introduce new\ninteraction paradigms, demanding the identification of software requirements\nduring the software development process. In general, design recommendations are\nrelated to this, supporting the design of applications positively and meeting\nstakeholder needs. However, current research lacks context-specific AR design\nrecommendations. This study addresses this gap by identifying and analyzing\npractical AR design recommendations relevant to the evaluation phase of the\nUser-Centered Design (UCD) process. We rely on an existing dataset of Mixed\nReality (MR) design recommendations. We applied a multi-method approach by (1)\nextending the dataset with AR-specific recommendations published since 2020,\n(2) classifying the identified recommendations using a NLP classification\napproach based on a pre-trained Sentence Transformer model, (3) summarizing the\ncontent of all topics, and (4) evaluating their relevance concerning AR in\nCorporate Training (CT) both based on a qualitative Round Robin approach with\nfive experts. As a result, an updated dataset of 597 practitioner design\nrecommendations, classified into 84 topics, is provided with new insights into\ntheir applicability in the context of AR in CT. Based on this, 32 topics with a\ntotal of 284 statements were evaluated as relevant for AR in CT. This research\ndirectly contributes to the authors' work for extending their AR-specific User\nExperience (UX) measurement approach, supporting AR authors in targeting the\nimprovement of AR applications for CT scenarios.", "comment": "9 pages, 1 table, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.21722v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.06834", "title": "Green building blocks reveal the complex anatomy of climate change mitigation technologies", "authors": ["Yang Li", "Frank Neffke"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06834v2", "summary": "Achieving net-zero emissions requires rapid innovation, yet the necessary\ntechnological knowhow is scattered across industries and countries. Comparing\nfunctionally similar green and nongreen patents, we identify \"Green Building\nBlocks\" (GBBs): modular components that can be added to reduce existing\ntechnologies' carbon footprints. These GBBs depict the anatomy of the green\ntransition as a network that connects problems -- nongreen technologies -- to\nGBBs that mitigate their climate-change impact. Node degrees in this network\nare highly unequal, showing that the scope for climate-change mitigating\ninnovation varies substantially across domains. The network also helps predict\nwhich green technologies firms develop themselves, and which alliances they\nform to do so. This reveals a critical dependence on international\ncollaboration: optimal innovation partners for 84% of US, 87% of German, and\n92% of Chinese firms are foreign, providing quantitative evidence that rising\neconomic nationalism threatens the pace of innovation required to meet global\nclimate goals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06834v2", "cate": "physics.soc-ph", "date": "2025-04-09", "updated": "2025-07-29"}
{"id": "2502.08386", "title": "Accelerating Stable Matching between Workers and Spatial-Temporal Tasks for Dynamic MCS: A Stagewise Service Trading Approach", "authors": ["Houyi Qi", "Minghui Liwang", "Xianbin Wang", "Liqun Fu", "Yiguang Hong", "Li Li", "Zhipeng Cheng"], "categories": ["cs.DC", "cs.NI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.08386v3", "summary": "Designing effective incentive mechanisms in mobile crowdsensing (MCS)\nnetworks is crucial for engaging distributed mobile users (workers) to\ncontribute heterogeneous data for various applications (tasks). In this paper,\nwe propose a novel stagewise trading framework to achieve efficient and stable\ntask-worker matching, explicitly accounting for task diversity (e.g.,\nspatio-temporal limitations) and network dynamics inherent in MCS environments.\nThis framework integrates both futures and spot trading stages. In the former,\nwe introduce the \\textbf{f}utures \\textbf{t}rading-driven \\textbf{s}table\n\\textbf{m}atching and \\textbf{p}re-\\textbf{p}ath-\\textbf{p}lanning mechanism\n(FT-SMP$^3$), which enables long-term task-worker assignment and pre-planning\nof workers' trajectories based on historical statistics and risk-aware\nanalysis. In the latter, we develop the \\textbf{s}pot \\textbf{t}rading-driven\n\\textbf{D}QN-based \\textbf{p}ath \\textbf{p}lanning and onsite \\textbf{w}orker\n\\textbf{r}ecruitment mechanism (ST-DP$^2$WR), which dynamically improves the\npractical utilities of tasks and workers by supporting real-time recruitment\nand path adjustment. We rigorously prove that the proposed mechanisms satisfy\nkey economic and algorithmic properties, including stability, individual\nrationality, competitive equilibrium, and weak Pareto optimality. Extensive\nexperiements further validate the effectiveness of our framework in realistic\nnetwork settings, demonstrating superior performance in terms of service\nquality, computational efficiency, and decision-making overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.08386v3", "cate": "cs.DC", "date": "2025-02-12", "updated": "2025-07-29"}
{"id": "2507.21785", "title": "Pilot-to-Data Power Ratio in RIS-Assisted Multiantenna Communication", "authors": ["Masoud Sadeghian", "Angel Lozano", "Gabor Fodor"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21785v1", "summary": "The optimization of the \\gls{pdpr} is a recourse that helps wireless systems\nto acquire channel state information while minimizing the pilot overhead. While\nthe optimization of the \\gls{pdpr} in cellular networks has been studied\nextensively, the effect of the \\gls{pdpr} in \\gls{ris}-assisted networks has\nhardly been examined. This paper tackles this optimization when the\ncommunication is assisted by a RIS whose phase shifts are adjusted on the basis\nof the statistics of the channels. For a setting representative of a\nmacrocellular deployment, the benefits of optimizing the PDPR are seen to be\nsignificant over a broad range of operating conditions. These benefits,\ndemonstrated through the ergodic minimum mean squared error, for which a\nclosed-form solution is derived, become more pronounced as the number of RIS\nelements and/or the channel coherence grow large.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21785v1", "cate": "cs.IT", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22037", "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security", "authors": ["Muzhi Dai", "Shixuan Liu", "Zhiyuan Zhao", "Junyu Gao", "Hao Sun", "Xuelong Li"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.22037v1", "summary": "The rapid advancement of multimodal large language models (MLLMs) has led to\nbreakthroughs in various applications, yet their security remains a critical\nchallenge. One pressing issue involves unsafe image-query pairs--jailbreak\ninputs specifically designed to bypass security constraints and elicit\nunintended responses from MLLMs. Compared to general multimodal data, such\nunsafe inputs are relatively sparse, which limits the diversity and richness of\ntraining samples available for developing robust defense models. Meanwhile,\nexisting guardrail-type methods rely on external modules to enforce security\nconstraints but fail to address intrinsic vulnerabilities within MLLMs.\nTraditional supervised fine-tuning (SFT), on the other hand, often over-refuses\nharmless inputs, compromising general performance. Given these challenges, we\npropose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack\ntraining method to enhance the security of MLLMs. SecTOW consists of two\nmodules: a defender and an auxiliary attacker, both trained iteratively using\nreinforcement learning (GRPO). During the iterative process, the attacker\nidentifies security vulnerabilities in the defense model and expands jailbreak\ndata. The expanded data are then used to train the defender, enabling it to\naddress identified security vulnerabilities. We also design reward mechanisms\nused for GRPO to simplify the use of response labels, reducing dependence on\ncomplex generative labels and enabling the efficient use of synthetic data.\nAdditionally, a quality monitoring mechanism is used to mitigate the defender's\nover-refusal of harmless inputs and ensure the diversity of the jailbreak data\ngenerated by the attacker. Experimental results on safety-specific and general\nbenchmarks demonstrate that SecTOW significantly improves security while\npreserving general performance.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.22037v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21547", "title": "Decentralized Modeling of Vehicular Maneuvers and Interactions at Urban Junctions", "authors": ["Saeed Rahmani", "Simeon C. Calvert", "Bart van Arem"], "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Manuscript under review", "url": "http://arxiv.org/abs/2507.21547v1", "summary": "Modeling and evaluation of automated vehicles (AVs) in mixed-autonomy traffic\nis essential prior to their safe and efficient deployment. This is especially\nimportant at urban junctions where complex multi-agent interactions occur.\nCurrent approaches for modeling vehicular maneuvers and interactions at urban\njunctions have limitations in formulating non-cooperative interactions and\nvehicle dynamics within a unified mathematical framework. Previous studies\neither assume predefined paths or rely on cooperation and central\ncontrollability, limiting their realism and applicability in mixed-autonomy\ntraffic. This paper addresses these limitations by proposing a modeling\nframework for trajectory planning and decentralized vehicular control at urban\njunctions. The framework employs a bi-level structure where the upper level\ngenerates kinematically feasible reference trajectories using an efficient\ngraph search algorithm with a custom heuristic function, while the lower level\nemploys a predictive controller for trajectory tracking and optimization.\nUnlike existing approaches, our framework does not require central\ncontrollability or knowledge sharing among vehicles. The vehicle kinematics are\nexplicitly incorporated at both levels, and acceleration and steering angle are\nused as control variables. This intuitive formulation facilitates analysis of\ntraffic efficiency, environmental impacts, and motion comfort. The framework's\ndecentralized structure accommodates operational and stochastic elements, such\nas vehicles' detection range, perception uncertainties, and reaction delay,\nmaking the model suitable for safety analysis. Numerical and simulation\nexperiments across diverse scenarios demonstrate the framework's capability in\nmodeling accurate and realistic vehicular maneuvers and interactions at various\nurban junctions, including unsignalized intersections and roundabouts.", "comment": "Manuscript under review", "pdf_url": "http://arxiv.org/pdf/2507.21547v1", "cate": "math.OC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21406", "title": "Shapley Uncertainty in Natural Language Generation", "authors": ["Meilin Zhu", "Gaojie Jin", "Xiaowei Huang", "Lijun Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21406v1", "summary": "In question-answering tasks, determining when to trust the outputs is crucial\nto the alignment of large language models (LLMs). Kuhn et al. (2023) introduces\nsemantic entropy as a measure of uncertainty, by incorporating linguistic\ninvariances from the same meaning. It primarily relies on setting threshold to\nmeasure the level of semantic equivalence relation. We propose a more nuanced\nframework that extends beyond such thresholding by developing a Shapley-based\nuncertainty metric that captures the continuous nature of semantic\nrelationships. We establish three fundamental properties that characterize\nvalid uncertainty metrics and prove that our Shapley uncertainty satisfies\nthese criteria. Through extensive experiments, we demonstrate that our Shapley\nuncertainty more accurately predicts LLM performance in question-answering and\nother datasets, compared to similar baseline measures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21406v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21199", "title": "Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications", "authors": ["Xinye Cao", "Hongcan Guo", "Guoshun Nan", "Jiaoyang Cui", "Haoting Qian", "Yihan Lin", "Yilin Peng", "Diyang Zhang", "Yanzhao Hou", "Huici Wu", "Xiaofeng Tao", "Tony Q. S. Quek"], "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.21199v1", "summary": "Interactive multimodal applications (IMAs), such as route planning in the\nInternet of Vehicles, enrich users' personalized experiences by integrating\nvarious forms of data over wireless networks. Recent advances in large language\nmodels (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple\nIMAs, with each LLM trained individually for a specific task that presents\ndifferent business workflows. In contrast to existing approaches that rely on\nmultiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes\nvarious IMAs using a single compositional LLM over wireless networks. The two\nprimary challenges include 1) guiding a single LLM to adapt to diverse IMA\nobjectives and 2) ensuring the flexibility and efficiency of the LLM in\nresource-constrained mobile environments. To tackle the first challenge, we\npropose ContextLoRA, a novel method that guides an LLM to learn the rich\nstructured context among IMAs by constructing a task dependency graph. We\npartition the learnable parameter matrix of neural layers for each IMA to\nfacilitate LLM composition. Then, we develop a step-by-step fine-tuning\nprocedure guided by task relations, including training, freezing, and masking\nphases. This allows the LLM to learn to reason among tasks for better\nadaptation, capturing the latent dependencies between tasks. For the second\nchallenge, we introduce ContextGear, a scheduling strategy to optimize the\ntraining procedure of ContextLoRA, aiming to minimize computational and\ncommunication costs through a strategic grouping mechanism. Experiments on\nthree benchmarks show the superiority of the proposed ContextLoRA and\nContextGear. Furthermore, we prototype our proposed paradigm on a real-world\nwireless testbed, demonstrating its practical applicability for various IMAs.\nWe will release our code to the community.", "comment": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.21199v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21811", "title": "Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC", "authors": ["Cynthia Zastudil", "Christine Holyfield", "Christine Kapp", "Kate Hamilton", "Kriti Baru", "Liam Newsam", "June A. Smith", "Stephen MacNeil"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21811v1", "summary": "Augmentative and alternative communication (AAC) devices are used by many\npeople around the world who experience difficulties in communicating verbally.\nOne AAC device which is especially useful for minimally verbal autistic\nchildren in developing language and communication skills are visual scene\ndisplays (VSD). VSDs use images with interactive hotspots embedded in them to\ndirectly connect language to real-world contexts which are meaningful to the\nAAC user. While VSDs can effectively support emergent communicators, their\nwidespread adoption is impacted by how difficult these devices are to\nconfigure. We developed a prototype that uses generative AI to automatically\nsuggest initial hotspots on an image to help non-experts efficiently create\nVSDs. We conducted a within-subjects user study to understand how effective our\nprototype is in supporting non-expert users, specifically pre-service\nspeech-language pathologists (SLP) who are not familiar with VSDs as an AAC\nintervention. Pre-service SLPs are actively studying to become clinically\ncertified SLPs and have domain-specific knowledge about language and\ncommunication skill development. We evaluated the effectiveness of our\nprototype based on creation time, quality, and user confidence. We also\nanalyzed the relevance and developmental appropriateness of the automatically\ngenerated hotspots and how often users interacted with the generated hotspots.\nOur results were mixed with SLPs becoming more efficient and confident.\nHowever, there were multiple negative impacts as well, including over-reliance\nand homogenization of communication options. The implications of these findings\nreach beyond the domain of AAC, especially as generative AI becomes more\nprevalent across domains, including assistive technology. Future work is needed\nto further identify and address these risks associated with integrating\ngenerative AI into assistive technology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21811v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21364", "title": "Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers", "authors": ["Lukman Jibril Aliyu", "Umar Sani Muhammad", "Bilqisu Ismail", "Nasiru Muhammad", "Almustapha A Wakili", "Seid Muhie Yimam", "Shamsuddeen Hassan Muhammad", "Mustapha Abdullahi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted as a camera-ready paper at Deep Learning Indaba 2025 (Kigali, Rwanda)", "url": "http://arxiv.org/abs/2507.21364v1", "summary": "Wildlife populations in Africa face severe threats, with vertebrate numbers\ndeclining by over 65% in the past five decades. In response, image\nclassification using deep learning has emerged as a promising tool for\nbiodiversity monitoring and conservation. This paper presents a comparative\nstudy of deep learning models for automatically classifying African wildlife\nimages, focusing on transfer learning with frozen feature extractors. Using a\npublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; we\nevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and\nVision Transformer ViT-H/14. DenseNet-201 achieved the best performance among\nconvolutional networks (67% accuracy), while ViT-H/14 achieved the highest\noverall accuracy (99%), but with significantly higher computational cost,\nraising deployment concerns. Our experiments highlight the trade-offs between\naccuracy, resource requirements, and deployability. The best-performing CNN\n(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time\nfield use, demonstrating the feasibility of deploying lightweight models in\nconservation settings. This work contributes to African-grounded AI research by\noffering practical insights into model selection, dataset preparation, and\nresponsible deployment of deep learning tools for wildlife conservation.", "comment": "Accepted as a camera-ready paper at Deep Learning Indaba 2025\n  (Kigali, Rwanda)", "pdf_url": "http://arxiv.org/pdf/2507.21364v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21810", "title": "ChartMark: A Structured Grammar for Chart Annotation", "authors": ["Yiyu Chen", "Yifan Wu", "Shuyu Shen", "Yupeng Xie", "Leixian Shen", "Hui Xiong", "Yuyu Luo"], "categories": ["cs.CL", "cs.SE"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      IEEE VIS 2025", "url": "http://arxiv.org/abs/2507.21810v1", "summary": "Chart annotations enhance visualization accessibility but suffer from\nfragmented, non-standardized representations that limit cross-platform reuse.\nWe propose ChartMark, a structured grammar that separates annotation semantics\nfrom visualization implementations. ChartMark features a hierarchical framework\nmapping onto annotation dimensions (e.g., task, chart context), supporting both\nabstract intents and precise visual details. Our toolkit demonstrates\nconverting ChartMark specifications into Vega-Lite visualizations, highlighting\nits flexibility, expressiveness, and practical applicability.", "comment": "IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.21810v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.12498", "title": "The Dual Personas of Social Media Bots", "authors": ["Lynnette Hui Xian Ng", "Kathleen M. Carley"], "categories": ["cs.CY", "cs.SI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12498v2", "summary": "Social media bots are AI agents that participate in online conversations.\nMost studies focus on the general bot and the malicious nature of these agents.\nHowever, bots have many different personas, each specialized towards a specific\nbehavioral or content trait. Neither are bots singularly bad, because they are\nused for both good and bad information dissemination. In this article, we\nintroduce fifteen agent personas of social media bots. These personas have two\nmain categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We\nalso form yardsticks of the good-bad duality of the bots, elaborating on\nmetrics of good and bad bot agents. Our work puts forth a guideline to inform\nbot detection regulation, emphasizing that policies should focus on how these\nagents are employed, rather than collectively terming bot agents as bad.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12498v2", "cate": "cs.CY", "date": "2025-04-16", "updated": "2025-07-29"}
{"id": "2507.21988", "title": "Properties of Algorithmic Information Distance", "authors": ["Marcus Hutter"], "categories": ["cs.IT", "math.IT", "math.MG"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      33 pages", "url": "http://arxiv.org/abs/2507.21988v1", "summary": "The domain-independent universal Normalized Information Distance based on\nKolmogorov complexity has been (in approximate form) successfully applied to a\nvariety of difficult clustering problems. In this paper we investigate\ntheoretical properties of the un-normalized algorithmic information distance\n$d_K$. The main question we are asking in this work is what properties this\ncurious distance has, besides being a metric. We show that many\n(in)finite-dimensional spaces can(not) be isometrically scale-embedded into the\nspace of finite strings with metric $d_K$. We also show that $d_K$ is not an\nEuclidean distance, but any finite set of points in Euclidean space can be\nscale-embedded into $(\\{0,1\\}^*,d_K)$. A major contribution is the development\nof the necessary framework and tools for finding more (interesting) properties\nof $d_K$ in future, and to state several open problems.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2507.21988v1", "cate": "cs.IT", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21325", "title": "On Post-Quantum Cryptography Authentication for Quantum Key Distribution", "authors": ["Juan Antonio Vieira Giestinhas", "Timothy Spiller"], "categories": ["quant-ph", "cs.CR", "94A60, 94A62, 81P45, 81P94", "E.3"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21325v1", "summary": "The traditional way for a Quantum Key Distribution (QKD) user to join a\nquantum network is by authenticating themselves using pre-shared key material.\nWhile this approach is sufficient for small-scale networks, it becomes\nimpractical as the network grows, due to the total quadratic increase in the\nnumber of pre-shared keys required. To address this scalability issue, Public\nKey Infrastructure (PKI) combined with Post-Quantum Cryptography (PQC) offers a\nmore scalable solution, allowing users to authenticate the QKD traffic remotely\nto obtain information-theoretical secure (ITS) keys under the presented\nassumptions. Unlike traditional PKI, which relies on classical cryptographic\nalgorithms such as RSA, the approach presented in this paper leverages PQC\nalgorithms that are believed to be resistant to quantum attacks. Similarly to\nthe SIGMA or TLS protocols, authentication, confidentiality, and integrity are\nachievable against bounded adversaries to ensure secure and scalable quantum\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21325v1", "cate": "quant-ph", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21638", "title": "Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics", "authors": ["Leonard Hinckeldey", "Elliot Fosong", "Elle Miller", "Rimvydas Rubavicius", "Trevor McInroe", "Patricia Wollstadt", "Christiane B. Wiebel-Herboth", "Subramanian Ramamoorthy", "Stefano V. Albrecht"], "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted for the Coordination and Cooperation in Multi-Agent Reinforcement Learning Workshop at the Reinforcement Learning Conference 2025", "url": "http://arxiv.org/abs/2507.21638v1", "summary": "The development of reinforcement learning (RL) algorithms has been largely\ndriven by ambitious challenge tasks and benchmarks. Games have dominated RL\nbenchmarks because they present relevant challenges, are inexpensive to run and\neasy to understand. While games such as Go and Atari have led to many\nbreakthroughs, they often do not directly translate to real-world embodied\napplications. In recognising the need to diversify RL benchmarks and addressing\ncomplexities that arise in embodied interaction scenarios, we introduce\nAssistax: an open-source benchmark designed to address challenges arising in\nassistive robotics tasks. Assistax uses JAX's hardware acceleration for\nsignificant speed-ups for learning in physics-based simulations. In terms of\nopen-loop wall-clock time, Assistax runs up to $370\\times$ faster when\nvectorising training runs compared to CPU-based alternatives. Assistax\nconceptualises the interaction between an assistive robot and an active human\npatient using multi-agent RL to train a population of diverse partner agents\nagainst which an embodied robotic agent's zero-shot coordination capabilities\ncan be tested. Extensive evaluation and hyperparameter tuning for popular\ncontinuous control RL and MARL algorithms provide reliable baselines and\nestablish Assistax as a practical benchmark for advancing RL research for\nassistive robotics. The code is available at:\nhttps://github.com/assistive-autonomy/assistax.", "comment": "Accepted for the Coordination and Cooperation in Multi-Agent\n  Reinforcement Learning Workshop at the Reinforcement Learning Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.21638v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21407", "title": "Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects", "authors": ["Yixin Liu", "Guibin Zhang", "Kun Wang", "Shiyuan Li", "Shirui Pan"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      15 pages, 7 figures", "url": "http://arxiv.org/abs/2507.21407v1", "summary": "Autonomous agents based on large language models (LLMs) have demonstrated\nimpressive capabilities in a wide range of applications, including web\nnavigation, software development, and embodied control. While most LLMs are\nlimited in several key agentic procedures, such as reliable planning, long-term\nmemory, tool management, and multi-agent coordination, graphs can serve as a\npowerful auxiliary structure to enhance structure, continuity, and coordination\nin complex agent workflows. Given the rapid growth and fragmentation of\nresearch on Graph-augmented LLM Agents (GLA), this paper offers a timely and\ncomprehensive overview of recent advances and also highlights key directions\nfor future work. Specifically, we categorize existing GLA methods by their\nprimary functions in LLM agent systems, including planning, memory, and tool\nusage, and then analyze how graphs and graph learning algorithms contribute to\neach. For multi-agent systems, we further discuss how GLA solutions facilitate\nthe orchestration, efficiency optimization, and trustworthiness of MAS.\nFinally, we highlight key future directions to advance this field, from\nimproving structural adaptability to enabling unified, scalable, and multimodal\nGLA systems. We hope this paper can serve as a roadmap for future research on\nGLA and foster a deeper understanding of the role of graphs in LLM agent\nsystems.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.21407v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21205", "title": "Learning from Limited and Imperfect Data", "authors": ["Harsh Rangwani"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      PhD Thesis", "url": "http://arxiv.org/abs/2507.21205v1", "summary": "The distribution of data in the world (eg, internet, etc.) significantly\ndiffers from the well-curated datasets and is often over-populated with samples\nfrom common categories. The algorithms designed for well-curated datasets\nperform suboptimally when used for learning from imperfect datasets with\nlong-tailed imbalances and distribution shifts. To expand the use of deep\nmodels, it is essential to overcome the labor-intensive curation process by\ndeveloping robust algorithms that can learn from diverse, real-world data\ndistributions. Toward this goal, we develop practical algorithms for Deep\nNeural Networks which can learn from limited and imperfect data present in the\nreal world. This thesis is divided into four segments, each covering a scenario\nof learning from limited or imperfect data. The first part of the thesis\nfocuses on Learning Generative Models from Long-Tail Data, where we mitigate\nthe mode-collapse and enable diverse aesthetic image generations for tail\n(minority) classes. In the second part, we enable effective generalization on\ntail classes through Inductive Regularization schemes, which allow tail classes\nto generalize as effectively as the head classes without requiring explicit\ngeneration of images. In the third part, we develop algorithms for Optimizing\nRelevant Metrics for learning from long-tailed data with limited annotation\n(semi-supervised), followed by the fourth part, which focuses on the Efficient\nDomain Adaptation of the model to various domains with very few to zero labeled\nsamples.", "comment": "PhD Thesis", "pdf_url": "http://arxiv.org/pdf/2507.21205v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21837", "title": "VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos", "authors": ["Yotam Sechayk", "Ariel Shamir", "Amy Pavel", "Takeo Igarashi"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      ASSETS '25, Denver, CO, USA", "url": "http://arxiv.org/abs/2507.21837v1", "summary": "Instructors often rely on visual actions such as pointing, marking, and\nsketching to convey information in educational presentation videos. These\nsubtle visual cues often lack verbal descriptions, forcing low-vision (LV)\nlearners to search for visual indicators or rely solely on audio, which can\nlead to missed information and increased cognitive load. To address this\nchallenge, we conducted a co-design study with three LV participants and\ndeveloped VeasyGuide, a tool that uses motion detection to identify instructor\nactions and dynamically highlight and magnify them. VeasyGuide produces\nfamiliar visual highlights that convey spatial context and adapt to diverse\nlearners and content through extensive personalization and real-time visual\nfeedback. VeasyGuide reduces visual search effort by clarifying what to look\nfor and where to look. In an evaluation with 8 LV participants, learners\ndemonstrated a significant improvement in detecting instructor actions, with\nfaster response times and significantly reduced cognitive load. A separate\nevaluation with 8 sighted participants showed that VeasyGuide also enhanced\nengagement and attentiveness, suggesting its potential as a universally\nbeneficial tool.", "comment": "ASSETS '25, Denver, CO, USA", "pdf_url": "http://arxiv.org/pdf/2507.21837v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21367", "title": "Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation", "authors": ["I-Hsiang Chen", "Hua-En Chang", "Wei-Ting Chen", "Jenq-Neng Hwang", "Sy-Yen Kuo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.21367v1", "summary": "Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging\ntask, as domain shifts in unseen environments can severely compromise model\nperformance. While recent studies enhance feature alignment by projecting\nfeatures into the source domain, they often neglect intrinsic latent domain\npriors, leading to suboptimal results. In this paper, we introduce PDAF, a\nProbabilistic Diffusion Alignment Framework that enhances the generalization of\nexisting segmentation networks through probabilistic diffusion modeling. PDAF\nintroduces a Latent Domain Prior (LDP) to capture domain shifts and uses this\nprior as a conditioning factor to align both source and unseen target domains.\nTo achieve this, PDAF integrates into a pre-trained segmentation model and\nutilizes paired source and pseudo-target images to simulate latent domain\nshifts, enabling LDP modeling. The framework comprises three modules: the\nLatent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the\nDomain Compensation Module (DCM) adjusts feature representations to mitigate\ndomain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion\nprocess to estimate the LDP without requiring paired samples. This design\nenables PDAF to iteratively model domain shifts, progressively refining feature\nrepresentations to enhance generalization under complex target conditions.\nExtensive experiments validate the effectiveness of PDAF across diverse and\nchallenging urban scenes.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.21367v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21882", "title": "The Impact of Foundational Models on Patient-Centric e-Health Systems", "authors": ["Elmira Onagh", "Alireza Davoodi", "Maleknaz Nayebi"], "categories": ["cs.AI", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Paper published in COMPSAC 2025", "url": "http://arxiv.org/abs/2507.21882v1", "summary": "As Artificial Intelligence (AI) becomes increasingly embedded in healthcare\ntechnologies, understanding the maturity of AI in patient-centric applications\nis critical for evaluating its trustworthiness, transparency, and real-world\nimpact. In this study, we investigate the integration and maturity of AI\nfeature integration in 116 patient-centric healthcare applications. Using Large\nLanguage Models (LLMs), we extracted key functional features, which are then\ncategorized into different stages of the Gartner AI maturity model. Our results\nshow that over 86.21\\% of applications remain at the early stages of AI\nintegration, while only 13.79% demonstrate advanced AI integration.", "comment": "Paper published in COMPSAC 2025", "pdf_url": "http://arxiv.org/pdf/2507.21882v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21479", "title": "Capacity-Constrained Continual Learning", "authors": ["Zheng Wen", "Doina Precup", "Benjamin Van Roy", "Satinder Singh"], "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.SY", "eess.SY", "math.IT", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21479v1", "summary": "Any agents we can possibly build are subject to capacity constraints, as\nmemory and compute resources are inherently finite. However, comparatively\nlittle attention has been dedicated to understanding how agents with limited\ncapacity should allocate their resources for optimal performance. The goal of\nthis paper is to shed some light on this question by studying a simple yet\nrelevant continual learning problem: the capacity-constrained\nlinear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a\nsolution to this problem under appropriate technical conditions. Moreover, for\nproblems that can be decomposed into a set of sub-problems, we also demonstrate\nhow to optimally allocate capacity across these sub-problems in the steady\nstate. We view the results of this paper as a first step in the systematic\ntheoretical study of learning under capacity constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21479v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21985", "title": "ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models", "authors": ["Hyun Jun Yook", "Ga San Jhun", "Jae Hyun Cho", "Min Jeon", "Donghyun Kim", "Tae Hyung Kim", "Youn Kyu Lee"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025", "url": "http://arxiv.org/abs/2507.21985v1", "summary": "Machine unlearning (MU) removes specific data points or concepts from deep\nlearning models to enhance privacy and prevent sensitive content generation.\nAdversarial prompts can exploit unlearned models to generate content containing\nremoved concepts, posing a significant security risk. However, existing\nadversarial attack methods still face challenges in generating content that\naligns with an attacker's intent while incurring high computational costs to\nidentify successful prompts. To address these challenges, we propose ZIUM, a\nZero-shot Intent-aware adversarial attack on Unlearned Models, which enables\nthe flexible customization of target attack images to reflect an attacker's\nintent. Additionally, ZIUM supports zero-shot adversarial attacks without\nrequiring further optimization for previously attacked unlearned concepts. The\nevaluation across various MU scenarios demonstrated ZIUM's effectiveness in\nsuccessfully customizing content based on user-intent prompts while achieving a\nsuperior attack success rate compared to existing methods. Moreover, its\nzero-shot adversarial attack significantly reduces the attack time for\npreviously attacked unlearned concepts.", "comment": "Accepted to ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.21985v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22028", "title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning", "authors": ["Honglin He", "Yukai Ma", "Wayne Wu", "Bolei Zhou"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22028v1", "summary": "Navigation foundation models trained on massive webscale data enable agents\nto generalize across diverse environments and embodiments. However, these\nmodels trained solely on offline data, often lack the capacity to reason about\nthe consequences of their actions or adapt through counterfactual\nunderstanding. They thus face significant limitations in the real-world urban\nnavigation where interactive and safe behaviors, such as avoiding obstacles and\nmoving pedestrians, are critical. To tackle these challenges, we introduce the\nSeeing-to-Experiencing framework to scale the capability of navigation\nfoundation models with reinforcement learning. S2E combines the strengths of\npre-training on videos and post-training through RL. It maintains the\ngeneralizability acquired from large-scale real-world videos while enhancing\nits interactivity through RL in simulation environments. Specifically, we\nintroduce two innovations: an Anchor-Guided Distribution Matching strategy,\nwhich stabilizes learning and models diverse motion patterns through\nanchor-based supervision; and a Residual-Attention Module, which obtains\nreactive behaviors from simulation environments without erasing the model's\npretrained knowledge. Moreover, we establish a comprehensive end-to-end\nevaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions\nof real-world scenes that incorporate physical interactions. It can\nsystematically assess the generalizability and safety of navigation foundation\nmodels. Extensive experiments show that S2E mitigates the diminishing returns\noften seen when scaling with offline data alone. We perform a thorough analysis\nof the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in\nthe context of post-training for robot learning. Our findings emphasize the\ncrucial role of integrating interactive online experiences to effectively scale\nfoundation models in Robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22028v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21419", "title": "GovRelBench:A Benchmark for Government Domain Relevance", "authors": ["Haiquan Wang", "Yi Chen", "Shang Zeng", "Yun Bian", "Zhe Cui"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21419v1", "summary": "Current evaluations of LLMs in the government domain primarily focus on\nsafety considerations in specific scenarios, while the assessment of the\nmodels' own core capabilities, particularly domain relevance, remains\ninsufficient. To address this gap, we propose GovRelBench, a benchmark\nspecifically designed for evaluating the core capabilities of LLMs in the\ngovernment domain. GovRelBench consists of government domain prompts and a\ndedicated evaluation tool, GovRelBERT. During the training process of\nGovRelBERT, we introduce the SoftGovScore method: this method trains a model\nbased on the ModernBERT architecture by converting hard labels to soft scores,\nenabling it to accurately compute the text's government domain relevance score.\nThis work aims to enhance the capability evaluation framework for large models\nin the government domain, providing an effective tool for relevant research and\npractice. Our code and dataset are available at\nhttps://github.com/pan-xi/GovRelBench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21419v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21244", "title": "Bubbleformer: Forecasting Boiling with Transformers", "authors": ["Sheikh Md Shakeel Hassan", "Xianwei Zou", "Akash Dhruv", "Vishwanath Ganesan", "Aparna Chandramowlishwaran"], "categories": ["cs.LG", "cs.AI", "cs.CE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      39 pages, 13 figures, Submitted to NeurIPS 2025", "url": "http://arxiv.org/abs/2507.21244v1", "summary": "Modeling boiling (an inherently chaotic, multiphase process central to energy\nand thermal systems) remains a significant challenge for neural PDE surrogates.\nExisting models require future input (e.g., bubble positions) during inference\nbecause they fail to learn nucleation from past states, limiting their ability\nto autonomously forecast boiling dynamics. They also fail to model flow boiling\nvelocity fields, where sharp interface-momentum coupling demands long-range and\ndirectional inductive biases. We introduce Bubbleformer, a transformer-based\nspatiotemporal model that forecasts stable and long-range boiling dynamics\nincluding nucleation, interface evolution, and heat transfer without dependence\non simulation data during inference. Bubbleformer integrates factorized axial\nattention, frequency-aware scaling, and conditions on thermophysical parameters\nto generalize across fluids, geometries, and operating conditions. To evaluate\nphysical fidelity in chaotic systems, we propose interpretable physics-based\nmetrics that evaluate heat-flux consistency, interface geometry, and mass\nconservation. We also release BubbleML 2.0, a high-fidelity dataset that spans\ndiverse working fluids (cryogens, refrigerants, dielectrics), boiling\nconfigurations (pool and flow boiling), flow regimes (bubbly, slug, annular),\nand boundary conditions. Bubbleformer sets new benchmark results in both\nprediction and forecasting of two-phase boiling flows.", "comment": "39 pages, 13 figures, Submitted to NeurIPS 2025", "pdf_url": "http://arxiv.org/pdf/2507.21244v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21900", "title": "Leveraging LLMs for Persona-Based Visualization of Election Data", "authors": ["Swaroop Panda", "Arun Kumar Sekar"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21900v1", "summary": "Visualizations are essential tools for disseminating information regarding\nelections and their outcomes, potentially influencing public perceptions.\nPersonas, delineating distinctive segments within the populace, furnish a\nvaluable framework for comprehending the nuanced perspectives, requisites, and\nbehaviors of diverse voter demographics. In this work, we propose making\nvisualizations tailored to these personas to make election information easier\nto understand and more relevant. Using data from UK parliamentary elections and\nnew developments in Large Language Models (LLMs), we create personas that\nencompass the diverse demographics, technological preferences, voting\ntendencies, and information consumption patterns observed among\nvoters.Subsequently, we elucidate how these personas can inform the design of\nvisualizations through specific design criteria. We then provide illustrative\nexamples of visualization prototypes based on these criteria and evaluate these\nprototypes using these personas and LLMs. We finally propose some actionable\ninsights based upon the framework and the different design artifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21900v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21371", "title": "Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View", "authors": ["Zitong Zhang", "Suranjan Gautam", "Rui Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.21371v1", "summary": "Generating immersive 360{\\deg} indoor panoramas from 2D top-down views has\napplications in virtual reality, interior design, real estate, and robotics.\nThis task is challenging due to the lack of explicit 3D structure and the need\nfor geometric consistency and photorealism. We propose Top2Pano, an end-to-end\nmodel for synthesizing realistic indoor panoramas from top-down views. Our\nmethod estimates volumetric occupancy to infer 3D structures, then uses\nvolumetric rendering to generate coarse color and depth panoramas. These guide\na diffusion-based refinement stage using ControlNet, enhancing realism and\nstructural fidelity. Evaluations on two datasets show Top2Pano outperforms\nbaselines, effectively reconstructing geometry, occlusions, and spatial\narrangements. It also generalizes well, producing high-quality panoramas from\nschematic floorplans. Our results highlight Top2Pano's potential in bridging\ntop-down views with immersive indoor synthesis.", "comment": "ICCV 2025. Project page: https://top2pano.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.21371v1", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21899", "title": "LLM-based Content Classification Approach for GitHub Repositories by the README Files", "authors": ["Malik Uzair Mehmood", "Shahid Hussain", "Wen Li Wang", "Muhammad Usama Malik"], "categories": ["cs.AI", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages, 4 Figures", "url": "http://arxiv.org/abs/2507.21899v1", "summary": "GitHub is the world's most popular platform for storing, sharing, and\nmanaging code. Every GitHub repository has a README file associated with it.\nThe README files should contain project-related information as per the\nrecommendations of GitHub to support the usage and improvement of repositories.\nHowever, GitHub repository owners sometimes neglected these recommendations.\nThis prevents a GitHub repository from reaching its full potential. This\nresearch posits that the comprehensiveness of a GitHub repository's README file\nsignificantly influences its adoption and utilization, with a lack of detail\npotentially hindering its full potential for widespread engagement and impact\nwithin the research community. Large Language Models (LLMs) have shown great\nperformance in many text-based tasks including text classification, text\ngeneration, text summarization and text translation. In this study, an approach\nis developed to fine-tune LLMs for automatically classifying different sections\nof GitHub README files. Three encoder-only LLMs are utilized, including BERT,\nDistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a\ngold-standard dataset consisting of 4226 README file sections. This approach\noutperforms current state-of-the-art methods and has achieved an overall F1\nscore of 0.98. Moreover, we have also investigated the use of\nParameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation\n(LoRA) and shown an economical alternative to full fine-tuning without\ncompromising much performance. The results demonstrate the potential of using\nLLMs in designing an automatic classifier for categorizing the content of\nGitHub README files. Consequently, this study contributes to the development of\nautomated tools for GitHub repositories to improve their identifications and\npotential usages.", "comment": "8 pages, 4 Figures", "pdf_url": "http://arxiv.org/pdf/2507.21899v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21626", "title": "Comprehensive Analysis of Behavioral Hardware Impairments in Cell-Free Massive MIMO-OFDM Uplink: Centralized Operation", "authors": ["Özlem Tuğfe Demir", "Muhammed Selman Somuncu", "Ahmet M. Elbir", "Emil Björnson"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      4 pages, 2 figures, presented at IEEE Signal Processing and Communications Applications Conference (SIU), 2025", "url": "http://arxiv.org/abs/2507.21626v1", "summary": "Cell-free massive MIMO is a key 6G technology, offering superior spectral and\nenergy efficiency. However, its dense deployment of low-cost access points\n(APs) makes hardware impairments unavoidable. While narrowband impairments are\nwell-studied, their impact in wideband systems remains unexplored. This paper\nprovides the first comprehensive analysis of hardware impairments, such as\nnonlinear distortion in low-noise amplifiers, phase noise, in-phase-quadrature\nimbalance, and low-resolution analog-to-digital converters, on uplink spectral\nefficiency in cell-free massive MIMO. Using an OFDM waveform and centralized\nprocessing, APs share channel state information for joint uplink combining.\nLeveraging Bussgang decomposition, we derive a distortion-aware combining\nvector that optimizes spectral efficiency by modeling distortion as independent\ncolored noise.", "comment": "4 pages, 2 figures, presented at IEEE Signal Processing and\n  Communications Applications Conference (SIU), 2025", "pdf_url": "http://arxiv.org/pdf/2507.21626v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2306.06123", "title": "Adversarial attacks and defenses in explainable artificial intelligence: A survey", "authors": ["Hubert Baniecki", "Przemyslaw Biecek"], "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by Information Fusion", "url": "http://arxiv.org/abs/2306.06123v4", "summary": "Explainable artificial intelligence (XAI) methods are portrayed as a remedy\nfor debugging and trusting statistical and deep learning models, as well as\ninterpreting their predictions. However, recent advances in adversarial machine\nlearning (AdvML) highlight the limitations and vulnerabilities of\nstate-of-the-art explanation methods, putting their security and\ntrustworthiness into question. The possibility of manipulating, fooling or\nfairwashing evidence of the model's reasoning has detrimental consequences when\napplied in high-stakes decision-making and knowledge discovery. This survey\nprovides a comprehensive overview of research concerning adversarial attacks on\nexplanations of machine learning models, as well as fairness metrics. We\nintroduce a unified notation and taxonomy of methods facilitating a common\nground for researchers and practitioners from the intersecting research fields\nof AdvML and XAI. We discuss how to defend against attacks and design robust\ninterpretation methods. We contribute a list of existing insecurities in XAI\nand outline the emerging research directions in adversarial XAI (AdvXAI).\nFuture work should address improving explanation methods and evaluation\nprotocols to take into account the reported safety issues.", "comment": "Accepted by Information Fusion", "pdf_url": "http://arxiv.org/pdf/2306.06123v4", "cate": "cs.CR", "date": "2023-06-06", "updated": "2025-07-28"}
{"id": "2309.14945", "title": "Integration of Large Language Models within Cognitive Architectures for Autonomous Robots", "authors": ["Miguel Á. González-Santamarta", "Irene González-Fernández", "Francisco J. Rodríguez-Lera", "Ángel Manuel Guerrero-Higueras", "Vicente Matellán-Olivera"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures, 2 tables, Submitted to ROBOT 2025 (8th Iberian Robotics Conference)", "url": "http://arxiv.org/abs/2309.14945v3", "summary": "Symbolic reasoning systems have been used in cognitive architectures to\nprovide inference and planning capabilities. However, defining domains and\nproblems has proven difficult and prone to errors. Moreover, Large Language\nModels (LLMs) have emerged as tools to process natural language for different\ntasks. In this paper, we propose the use of LLMs to tackle these problems. This\nway, this paper proposes the integration of LLMs in the ROS 2-integrated\ncognitive architecture MERLIN2 for autonomous robots. Specifically, we present\nthe design, development and deployment of how to leverage the reasoning\ncapabilities of LLMs inside the deliberative processes of MERLIN2. As a result,\nthe deliberative system is updated from a PDDL-based planner system to a\nnatural language planning system. This proposal is evaluated quantitatively and\nqualitatively, measuring the impact of incorporating the LLMs in the cognitive\narchitecture. Results show that a classical approach achieves better\nperformance but the proposed solution provides an enhanced interaction through\nnatural language.", "comment": "9 pages, 6 figures, 2 tables, Submitted to ROBOT 2025 (8th Iberian\n  Robotics Conference)", "pdf_url": "http://arxiv.org/pdf/2309.14945v3", "cate": "cs.RO", "date": "2023-09-26", "updated": "2025-07-29"}
{"id": "2507.21438", "title": "Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models", "authors": ["Vishal Raman", "Vijai Aravindh R"], "categories": ["cs.AI", "I.2.4; I.2.6; I.2.7; H.2.8"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figures", "url": "http://arxiv.org/abs/2507.21438v1", "summary": "Ontologies and knowledge graphs require continuous evolution to remain\ncomprehensive and accurate, but manual curation is labor intensive. Large\nLanguage Models (LLMs) possess vast unstructured knowledge but struggle with\nmaintaining structured consistency. We propose Evo-DKD, a novel dual-decoder\nframework for autonomous ontology evolution that combines structured ontology\ntraversal with unstructured text reasoning. Evo-DKD introduces two parallel\ndecoding streams within an LLM: one decoder generates candidate ontology edits\n(e.g., new concepts or relations) while the other produces natural-language\njustifications. A dynamic attention-based gating mechanism coordinates the two\nstreams, deciding at each step how to blend structured and unstructured\nknowledge. Due to GPU constraints, we simulate the dual-decoder behavior using\nprompt-based mode control to approximate coordinated decoding in a\nsingle-stream mode. The system operates in a closed reasoning loop: proposed\nontology edits are validated (via consistency checks and cross-verification\nwith the text explanations) and then injected into the knowledge base, which in\nturn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on\nuse cases including healthcare ontology refinement, semantic search\nimprovement, and cultural heritage timeline modeling. Experiments show that\nEvo-DKD outperforms baselines using structured-only or unstructured-only\ndecoding in both precision of ontology updates and downstream task performance.\nWe present quantitative metrics and qualitative examples, confirming the\ncontributions of the dual-decoder design and gating router. Evo-DKD offers a\nnew paradigm for LLM-driven knowledge base maintenance, combining the strengths\nof symbolic and neural reasoning for sustainable ontology evolution.", "comment": "9 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.21438v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21260", "title": "Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors", "authors": ["Amartya Banerjee", "Xingyu Xu", "Caroline Moosmüller", "Harlin Lee"], "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code: this https URL", "url": "http://arxiv.org/abs/2507.21260v1", "summary": "In an inverse problem, the goal is to recover an unknown parameter (e.g., an\nimage) that has typically undergone some lossy or noisy transformation during\nmeasurement. Recently, deep generative models, particularly diffusion models,\nhave emerged as powerful priors for protein structure generation. However,\nintegrating noisy experimental data from multiple sources to guide these models\nremains a significant challenge. Existing methods often require precise\nknowledge of experimental noise levels and manually tuned weights for each data\nmodality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that\nguides a pre-trained protein diffusion model using gradients from multiple,\nheterogeneous experimental sources. Our framework features an adaptive noise\nestimation scheme and a dynamic modality weighting mechanism integrated into\nthe diffusion process, which reduce the need for manual hyperparameter tuning.\nExperiments on complex reconstruction tasks demonstrate significantly improved\naccuracy using Adam-PnP.", "comment": "Code: https://github.com/amartya21/Adam-PnP", "pdf_url": "http://arxiv.org/pdf/2507.21260v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21953", "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation", "authors": ["Yi Kong", "Dianxi Shi", "Guoli Yang", "Zhang ke-di", "Chenlin Huang", "Xiaopeng Li", "Songchang Jin"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21953v1", "summary": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21953v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21391", "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025. Code available at this https URL", "url": "http://arxiv.org/abs/2507.21391v2", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations. In addition, LLaVA-Reward supports\ndifferent types of preference data for efficient fine-tuning, including paired\npreference data and unpaired data. We train LLaVA-Reward on four evaluation\nperspectives: text-image alignment, fidelity/artifact, safety, and overall\nranking. Empirical results demonstrate that LLaVA-Reward outperforms\nconventional and MLLM-based methods in generating human-aligned scores for\nautomatic evaluations and inference-time scaling in text-to-image generations.", "comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward", "pdf_url": "http://arxiv.org/pdf/2507.21391v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2501.05625", "title": "Harnessing Large Language Model for Virtual Reality Exploration Testing: A Case Study", "authors": ["Zhenyu Qi", "Haotang Li", "Hao Qin", "Kebin Peng", "Sen He", "Xue Qin"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.05625v2", "summary": "As the Virtual Reality (VR) industry expands, the need for automated GUI\ntesting is growing rapidly. Large Language Models (LLMs), capable of retaining\ninformation long-term and analyzing both visual and textual data, are emerging\nas a potential key to deciphering the complexities of VR's evolving user\ninterfaces. In this paper, we conduct a case study to investigate the\ncapability of using LLMs, particularly GPT-4o, for field of view (FOV) analysis\nin VR exploration testing. Specifically, we validate that LLMs can identify\ntest entities in FOVs and that prompt engineering can effectively enhance the\naccuracy of test entity identification from 41.67% to 71.30%. Our study also\nshows that LLMs can accurately describe identified entities' features with at\nleast a 90% accuracy rate. We further find out that the core features that\neffectively represent an entity are color, placement, and shape. Furthermore,\nthe combination of the three features can especially be used to improve the\naccuracy of determining identical entities in multiple FOVs with the highest\nF1-score of 0.70. Additionally, our study demonstrates that LLMs are capable of\nscene recognition and spatial understanding in VR with precisely designed\nstructured prompts. Finally, we find that LLMs fail to label the identified\ntest entities, and we discuss potential solutions as future research\ndirections.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.05625v2", "cate": "cs.SE", "date": "2025-01-09", "updated": "2025-07-29"}
{"id": "2507.21644", "title": "Energy-Aware Resource Allocation for Multi-Operator Cell-Free Massive MIMO in V-CRAN Architectures", "authors": ["Derya Nurcan-Atceken", "Özlem Tuğfe Demir", "Aysegul Altin-Kayhan", "Emil Björnson", "Cicek Cavdar", "Bulent Tavli"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures, to be presented at 2025 International Conference on Future Communications and Networks (FCN)", "url": "http://arxiv.org/abs/2507.21644v1", "summary": "Cell-free massive multiple-input multiple-output (MIMO) implemented in\nvirtualized cloud radio access networks (V-CRAN) has emerged as a promising\narchitecture to enhance spectral efficiency (SE), network flexibility, and\nenergy efficiency (EE) in next-generation wireless systems. In this work, we\ndevelop a holistic optimization framework for the efficient deployment of\ncell-free massive MIMO in V-CRAN with multiple mobile network operators (MNOs).\nSpecifically, we formulate a set of mixed-integer programming (MIP) models to\njointly optimize access point (AP) selection, user equipment (UE) association,\ncloud resource allocation, and MNO assignment while minimizing the maximum\ntotal power consumption (TPC) across MNOs. We consider two different scenarios\nbased on whether UEs can be assigned to arbitrary MNOs or not. The numerical\nresults demonstrate the impact of different deployment assumptions on power\nconsumption, highlighting that flexible UE-MNO assignment significantly reduces\nTPC. The findings provide key insights into optimizing resource management in\ncell-free massive MIMO V-CRAN, paving the way for energy-efficient wireless\nnetwork implementations.", "comment": "6 pages, 2 figures, to be presented at 2025 International Conference\n  on Future Communications and Networks (FCN)", "pdf_url": "http://arxiv.org/pdf/2507.21644v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2410.13900", "title": "Voting by mail: a Markov chain model for managing the security risks of election systems", "authors": ["Carmen A. Haseltine", "Laura A. Albert"], "categories": ["cs.CR", "math.PR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13900v2", "summary": "The scrutiny surrounding vote-by-mail (VBM) in the United States has\nincreased in recent years, raising concerns about the integrity and security of\nabsentee voting. This paper addresses these issues by introducing a dynamic\nmathematical modeling framework for performing a risk assessment of VBM\nprocesses. We introduce a discrete-time Markov chain (DTMC) to model the VBM\nprocess and assess election performance and risk with a novel layered network\napproach that considers the interplay between VBM processes, malicious and\nnon-malicious threats, and security mitigations. The time-inhomogeneous DTMC\nframework captures dynamic risks and evaluates performance over time. The DTMC\nmodel accounts for a spectrum of outcomes, from unintended voter errors to\nsophisticated, targeted attacks, representing a significant advancement in the\nrisk assessment of VBM planning and protection. A case study based on\nreal-world data from Milwaukee County, Wisconsin, is used to evaluate the DTMC\nmodel. The analysis includes the development of attack scenarios and the\nevaluation of security measures, to illustrate the impact of different attack\ntimings. The analysis suggests that ballot drop boxes and automatic ballot\nnotification systems are crucial for ensuring secure and reliable operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13900v2", "cate": "cs.CR", "date": "2024-10-15", "updated": "2025-07-29"}
{"id": "2403.10460", "title": "Online Concurrent Multi-Robot Coverage Path Planning", "authors": ["Ratijit Mitra", "Indranil Saha"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted in IROS 2025", "url": "http://arxiv.org/abs/2403.10460v2", "summary": "Recently, centralized receding horizon online multi-robot coverage path\nplanning algorithms have shown remarkable scalability in thoroughly exploring\nlarge, complex, unknown workspaces with many robots. In a horizon, the path\nplanning and the path execution interleave, meaning when the path planning\noccurs for robots with no paths, the robots with outstanding paths do not\nexecute, and subsequently, when the robots with new or outstanding paths\nexecute to reach respective goals, path planning does not occur for those\nrobots yet to get new paths, leading to wastage of both the robotic and the\ncomputation resources. As a remedy, we propose a centralized algorithm that is\nnot horizon-based. It plans paths at any time for a subset of robots with no\npaths, i.e., who have reached their previously assigned goals, while the rest\nexecute their outstanding paths, thereby enabling concurrent planning and\nexecution. We formally prove that the proposed algorithm ensures complete\ncoverage of an unknown workspace and analyze its time complexity. To\ndemonstrate scalability, we evaluate our algorithm to cover eight large $2$D\ngrid benchmark workspaces with up to 512 aerial and ground robots,\nrespectively. A comparison with a state-of-the-art horizon-based algorithm\nshows its superiority in completing the coverage with up to 1.6x speedup. For\nvalidation, we perform ROS + Gazebo simulations in six 2D grid benchmark\nworkspaces with 10 quadcopters and TurtleBots, respectively. We also\nsuccessfully conducted one outdoor experiment with three quadcopters and one\nindoor with two TurtleBots.", "comment": "Accepted in IROS 2025", "pdf_url": "http://arxiv.org/pdf/2403.10460v2", "cate": "cs.RO", "date": "2024-03-15", "updated": "2025-07-28"}
{"id": "2507.21453", "title": "Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)", "authors": ["Ashley Rector", "Keaton Minor", "Kamden Minor", "Jeff McCormack", "Beth Breeden", "Ryan Nowers", "Jay Dorris"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21453v1", "summary": "This study evaluated Sherpa Rx, an artificial intelligence tool leveraging\nlarge language models and retrieval-augmented generation (RAG) for\npharmacogenomics, to validate its performance on key response metrics. Sherpa\nRx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC)\nguidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate\ncontextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC\nguidelines was used to evaluate drug-gene interactions, dosing recommendations,\nand therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2\nadditionally incorporated PharmGKB content. Responses were scored on accuracy,\nrelevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon\nsigned-rank tests compared accuracy between Phase 1 and Phase 2, and between\nPhase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world\napplicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated\nhigh performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8,\nand recall 0.99. The subset analysis (N=20) showed improvements in accuracy\n(4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8).\nChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but\nlagged in accuracy (3.9) and completeness (4.2). Differences in accuracy\nbetween Phase 1 and Phase 2 was not statistically significant. However, Phase 2\nsignificantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx\nachieved 90% accuracy, outperforming other models. Integrating additional\nresources like CPIC and PharmGKB with RAG enhances AI accuracy and performance.\nThis study highlights the transformative potential of generative AI like Sherpa\nRx in pharmacogenomics, improving decision-making with accurate, personalized\nresponses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21453v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21273", "title": "Deep Polynomial Chaos Expansion", "authors": ["Johannes Exenberger", "Sascha Ranftl", "Robert Peharz"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8th Workshop on Tractable Probabilistic Modeling, UAI 2025", "url": "http://arxiv.org/abs/2507.21273v1", "summary": "Polynomial chaos expansion (PCE) is a classical and widely used surrogate\nmodeling technique in physical simulation and uncertainty quantification. By\ntaking a linear combination of a set of basis polynomials - orthonormal with\nrespect to the distribution of uncertain input parameters - PCE enables\ntractable inference of key statistical quantities, such as (conditional) means,\nvariances, covariances, and Sobol sensitivity indices, which are essential for\nunderstanding the modeled system and identifying influential parameters and\ntheir interactions. As the number of basis functions grows exponentially with\nthe number of parameters, PCE does not scale well to high-dimensional problems.\nWe address this challenge by combining PCE with ideas from probabilistic\ncircuits, resulting in the deep polynomial chaos expansion (DeepPCE) - a deep\ngeneralization of PCE that scales effectively to high-dimensional input spaces.\nDeepPCE achieves predictive performance comparable to that of multi-layer\nperceptrons (MLPs), while retaining PCE's ability to compute exact statistical\ninferences via simple forward passes.", "comment": "8th Workshop on Tractable Probabilistic Modeling, UAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.21273v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.22051", "title": "DataSway: Vivifying Metaphoric Visualization with Animation Clip Generation and Coordination", "authors": ["Liwenhan Xie", "Jiayi Zhou", "Anyi Rao", "Huamin Qu", "Xinhuan Shu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      19 pages, 5 figures", "url": "http://arxiv.org/abs/2507.22051v2", "summary": "Animating metaphoric visualizations brings data to life, enhancing the\ncomprehension of abstract data encodings and fostering deeper engagement.\nHowever, creators face significant challenges in designing these animations,\nsuch as crafting motions that align semantically with the metaphors,\nmaintaining faithful data representation during animation, and seamlessly\nintegrating interactivity. We propose a human-AI co-creation workflow that\nfacilitates creating animations for SVG-based metaphoric visualizations. Users\ncan initially derive animation clips for data elements from vision-language\nmodels (VLMs) and subsequently coordinate their timelines based on entity\norder, attribute values, spatial layout, or randomness. Our design decisions\nwere informed by a formative study with experienced designers (N=8). We further\ndeveloped a prototype, DataSway, and conducted a user study (N=14) to evaluate\nits creativity support and usability. A gallery with 6 cases demonstrates its\ncapabilities and applications in web-based hypermedia. We conclude with\nimplications for future research on bespoke data visualization animation.", "comment": "19 pages, 5 figures; Website:\n  https://shellywhen.github.io/projects/DataSway", "pdf_url": "http://arxiv.org/pdf/2507.22051v2", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21420", "title": "ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs", "authors": ["Chaoyu Li", "Yogesh Kulkarni", "Pooyan Fazli"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21420v1", "summary": "The computational cost of training multimodal large language models (MLLMs)\nrapidly increases with the number of tokens involved. Existing efficiency\nmethods primarily target inference and rely on token reduction or merging,\noffering limited benefit during training. In this paper, we propose ReGATE\n(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method\nfor accelerating MLLM training. Specifically, ReGATE adopts a teacher-student\nframework in which the MLLM being trained serves as the student, and a frozen\nreference large language model (LLM) acts as the teacher. The teacher computes\nper-token reference losses, which are combined with an exponential moving\naverage (EMA) of the student's own difficulty scores. This adaptive\ndifficulty-based scoring enables the selective processing of crucial tokens\nwhile bypassing less informative ones in the forward pass, significantly\nreducing computational overhead. Experiments demonstrate that ReGATE, when\napplied to VideoLLaMA2, matches the peak accuracy of standard training on\nMVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional\ntraining, it even surpasses the baseline on several multimodal benchmarks, all\nwhile reducing the total token count by over 41%. Code and models will be\nreleased soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21420v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.00378", "title": "iPanda: An LLM-based Agent for Automated Conformance Testing of Communication Protocols", "authors": ["Xikai Sun", "Fan Dang", "Shiqi Jiang", "Jingao Xu", "Kebin Liu", "Xin Miao", "Zihao Yang", "Weichen Zhang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.00378v2", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes retrieval-augmented\ngeneration and customized CoT strategy to effectively interpret the\nimplementation and produce executable test programs. To further enhance\nprograms' quality, iPanda incorporates an iterative optimization mechanism to\nrefine generated test scripts interactively. Finally, by executing and\nanalyzing the generated tests, iPanda systematically verifies compliance\nbetween implementations and protocol specifications. Comprehensive experiments\non various protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-program generation by\nfactors ranging from 4.675 times to 10.751 times.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00378v2", "cate": "cs.SE", "date": "2025-07-01", "updated": "2025-07-29"}
{"id": "1904.03979", "title": "Double-Target Collaborative Spectrum Sharing for 6G Hybrid Satellite-Terrestrial Networks with User-Centric Channel Pools", "authors": ["Yanmin Wang", "Wei Feng", "Ming Xiao", "Cheng-Xiang Wang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/1904.03979v3", "summary": "Satellite and terrestrial cellular networks can be integrated together for\nextended broadband coverage in e.g., maritime communication scenarios, in the\nupcoming sixth-generation (6G) era. To counter spectrum scarcity, collaborative\nspectrum sharing is considered for a hybrid satellite-terrestrial network\n(HSTN) in this paper. With only slowly-varying large-scale channel state\ninformation (CSI), joint power and channel allocation is implemented for\nterrestrial mobile terminals (MTs) which share the same frequency band with the\nsatellite MTs opportunistically. Specially, strict quality service assurance is\nadopted for terrestrial MTs under the constraint of leakage interference to\nsatellite MTs. With the target of maximizing both the number of served\nterrestrial MTs and the average sum transmission rate, a double-target spectrum\nsharing problem is formulated. To solve the complicated mixed integer\nprogramming (MIP) problem efficiently, user-centric channel pools are\nintroduced. Simulations demonstrate that the proposed spectrum sharing scheme\ncould achieve a significant performance gain for the HSTN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/1904.03979v3", "cate": "cs.IT", "date": "2019-04-08", "updated": "2025-07-29"}
{"id": "2507.21038", "title": "Development and analysis of a secured VoIP system for surveillance activities", "authors": ["M. Matsive Ali"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21038v2", "summary": "Since the 1990s, the telephone has been the primary mode of communication.\nHowever, Voice over Internet Protocol (VoIP), which is a highly straightforward\nand affordable form of data transfer, is now becoming an important part of\ndaily communication. VoIP is the technology that makes it possible to send\nspeech and multimedia data packets across either a public or private IP\nnetwork. However, a cyberattack known as a man-in-the-middle attack poses a\nserious concern in transferring data through any network. Therefore, the\nauthors have designed a system that sends voice over the internet within the\nrange of a router using encrypted data transfer. An embedded system comprising\nan electret microphone, Embedded C, Particle Photon microcontroller, and\nInternet of Things (IoT) technology is developed. Due to its compact size, this\ntype of device may be incorporated into automobiles, surveillance systems, or\ncovert listening tools. The VoIP system gathers sound signals using the MAX9814\nmicrophone, while the Particle Photon microcontroller securely transmits the\ndata. Devices with access can download data from the VoIP systems Transmission\nControl Protocol (TCP) server. The accessed device stores the audio locally and\nuploads the corresponding data to Google Drive. This VoIP system provides a\nsecure method of communication while conserving the integrity of the original\nsignal.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21038v2", "cate": "cs.CR", "date": "2025-07-28", "updated": "2025-07-29"}
{"id": "2406.14540", "title": "IRASim: A Fine-Grained World Model for Robot Manipulation", "authors": ["Fangqi Zhu", "Hongtao Wu", "Song Guo", "Yuxiao Liu", "Chilam Cheang", "Tao Kong"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Opensource, project website: this https URL", "url": "http://arxiv.org/abs/2406.14540v2", "summary": "World models allow autonomous agents to plan and explore by predicting the\nvisual outcomes of different actions. However, for robot manipulation, it is\nchallenging to accurately model the fine-grained robot-object interaction\nwithin the visual space using existing methods which overlooks precise\nalignment between each action and the corresponding frame. In this paper, we\npresent IRASim, a novel world model capable of generating videos with\nfine-grained robot-object interaction details, conditioned on historical\nobservations and robot action trajectories. We train a diffusion transformer\nand introduce a novel frame-level action-conditioning module within each\ntransformer block to explicitly model and strengthen the action-frame\nalignment. Extensive experiments show that: (1) the quality of the videos\ngenerated by our method surpasses all the baseline methods and scales\neffectively with increased model size and computation; (2) policy evaluations\nusing IRASim exhibit a strong correlation with those using the ground-truth\nsimulator, highlighting its potential to accelerate real-world policy\nevaluation; (3) testing-time scaling through model-based planning with IRASim\nsignificantly enhances policy performance, as evidenced by an improvement in\nthe IoU metric on the Push-T benchmark from 0.637 to 0.961; (4) IRASim provides\nflexible action controllability, allowing virtual robotic arms in datasets to\nbe controlled via a keyboard or VR controller.", "comment": "Opensource, project website: https://gen-irasim.github.io", "pdf_url": "http://arxiv.org/pdf/2406.14540v2", "cate": "cs.RO", "date": "2024-06-20", "updated": "2025-07-29"}
{"id": "2507.21471", "title": "An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning", "authors": ["Zujie Xie", "Zixuan Chen", "Jiheng Liang", "Xiangyang Yu", "Ziru Yu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2507.21471v1", "summary": "Infrared spectroscopy offers rapid, non destructive measurement of chemical\nand material properties but suffers from high dimensional, overlapping spectral\nbands that challenge conventional chemometric approaches. Emerging large\nlanguage models (LLMs), with their capacity for generalization and reasoning,\noffer promising potential for automating complex scientific workflows. Despite\nthis promise, their application in IR spectral analysis remains largely\nunexplored. This study addresses the critical challenge of achieving accurate,\nautomated infrared spectral interpretation under low-data conditions using an\nLLM-driven framework. We introduce an end-to-end, large language model driven\nagent framework that integrates a structured literature knowledge base,\nautomated spectral preprocessing, feature extraction, and multi task reasoning\nin a unified pipeline. By querying a curated corpus of peer reviewed IR\npublications, the agent selects scientifically validated routines. The selected\nmethods transform each spectrum into low dimensional feature sets, which are\nfed into few shot prompt templates for classification, regression, and anomaly\ndetection. A closed loop, multi turn protocol iteratively appends mispredicted\nsamples to the prompt, enabling dynamic refinement of predictions. Across\ndiverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri\nReticulatae Pericarpium and waste water COD datasets, the multi turn LLM\nconsistently outperforms single turn inference, rivaling or exceeding machine\nlearning and deep learning models under low data regimes.", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2507.21471v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21274", "title": "Large Language Model-Enhanced Reinforcement Learning for Diverse and Novel Recommendations", "authors": ["Jiin Woo", "Alireza Bagheri Garakani", "Tianchen Zhou", "Zhishen Huang", "Yan Gao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21274v1", "summary": "In recommendation systems, diversity and novelty are essential for capturing\nvaried user preferences and encouraging exploration, yet many systems\nprioritize click relevance. While reinforcement learning (RL) has been explored\nto improve diversity, it often depends on random exploration that may not align\nwith user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a\nnovel method that leverages large language models (LLMs) as reference policies\nto suggest novel items, while training a lightweight policy to refine these\nsuggestions using system-specific data. The method formulates training as a\nbilevel optimization between actor and critic networks, enabling the critic to\nselectively favor promising novel actions and the actor to improve its policy\nbeyond LLM recommendations. To mitigate overestimation of unreliable LLM\nsuggestions, we apply regularization that anchors critic values for unexplored\nitems close to well-estimated dataset actions. Experiments on real-world\ndatasets show that LAAC outperforms existing baselines in diversity, novelty,\nand accuracy, while remaining robust on imbalanced data, effectively\nintegrating LLM knowledge without expensive fine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21274v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21054", "title": "High hopes for \"Deep Medicine\"? AI, economics, and the future of care", "authors": ["Robert Sparrow", "Joshua Hatherley"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21054v1", "summary": "In the much-celebrated book Deep Medicine, Eric Topol argues that the\ndevelopment of artificial intelligence for health care will lead to a dramatic\nshift in the culture and practice of medicine. In the next several decades, he\nsuggests, AI will become sophisticated enough that many of the everyday tasks\nof physicians could be delegated to it. Topol is perhaps the most articulate\nadvocate of the benefits of AI in medicine, but he is hardly alone in spruiking\nits potential to allow physicians to dedicate more of their time and attention\nto providing empathetic care for their patients in the future. Unfortunately,\nseveral factors suggest a radically different picture for the future of health\ncare. Far from facilitating a return to a time of closer doctor-patient\nrelationships, the use of medical AI seems likely to further erode therapeutic\nrelationships and threaten professional and patient satisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21054v1", "cate": "cs.CY", "date": "2025-04-15", "updated": "2025-04-15"}
{"id": "2507.21440", "title": "Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling for Semi-supervised Medical Image Segmentation", "authors": ["Han Wu", "Chong Wang", "Zhiming Cui"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE TMI", "url": "http://arxiv.org/abs/2507.21440v1", "summary": "Semi-supervised learning has proven highly effective in tackling the\nchallenge of limited labeled training data in medical image segmentation. In\ngeneral, current approaches, which rely on intra-image pixel-wise consistency\ntraining via pseudo-labeling, overlook the consistency at more comprehensive\nsemantic levels (e.g., object region) and suffer from severe discrepancy of\nextracted features resulting from an imbalanced number of labeled and unlabeled\ndata. To overcome these limitations, we present a new \\underline{Du}al\n\\underline{C}ross-\\underline{i}mage \\underline{S}emantic\n\\underline{C}onsistency (DuCiSC) learning framework, for semi-supervised\nmedical image segmentation. Concretely, beyond enforcing pixel-wise semantic\nconsistency, DuCiSC proposes dual paradigms to encourage region-level semantic\nconsistency across: 1) labeled and unlabeled images; and 2) labeled and fused\nimages, by explicitly aligning their prototypes. Relying on the dual paradigms,\nDuCiSC can effectively establish consistent cross-image semantics via prototype\nrepresentations, thereby addressing the feature discrepancy issue. Moreover, we\ndevise a novel self-aware confidence estimation strategy to accurately select\nreliable pseudo labels, allowing for exploiting the training dynamics of\nunlabeled data. Our DuCiSC method is extensively validated on four datasets,\nincluding two popular binary benchmarks in segmenting the left atrium and\npancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a\nchallenging scenario of segmenting the inferior alveolar nerve that features\ncomplicated anatomical structures, showing superior segmentation results over\nprevious state-of-the-art approaches. Our code is publicly available at\n\\href{https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}.", "comment": "IEEE TMI", "pdf_url": "http://arxiv.org/pdf/2507.21440v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.12482", "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding", "authors": ["Ishraq Khan", "Assad Chowdary", "Sharoz Haseeb", "Urvish Patel", "Yousuf Zaii"], "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG", "68N30, 68Q60, 68T05, 68T20, 68T30, 68T37, 68T50", "D.2.3; D.2.5; D.2.7; F.3.2; H.3.3; I.2.2; I.2.6; I.2.7; I.2.8"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      27 pages, 21 figures, 37 tables, 2 algorithms. Extended technical report. Introduces Chronos, an autonomous debugging system achieving 87.1% success rate on real-world bugs. Code and data available at this https URL", "url": "http://arxiv.org/abs/2507.12482v2", "summary": "Large Language Models (LLMs) have improved code generation and software\nautomation, but remain limited by inference-time context and lack structured\nreasoning over code. Debugging remains unsolved despite these advances. While\nClaude Opus 4 and GPT-4.1 achieve >70% on code synthesis benchmarks, they\nperform <15% on real debugging tasks. We introduce Kodezi Chronos, a language\nmodel built specifically for debugging. Chronos combines Adaptive Graph-Guided\nRetrieval to navigate codebases up to 10 million lines using multi-hop\ntraversal (92% precision, 85% recall), Persistent Debug Memory trained on 15M+\nsessions, and a 7-layer architecture for iterative fix-test-refine loops. On\n5,000 real-world scenarios, Chronos achieves 67.3% fix accuracy, compared to\n14.2% and 13.8% for Claude and GPT-4.1 respectively. Chronos reduces debugging\ntime by 40% and iteration count by 65%. It resolves complex multi-file bugs\ninvolving cross-repository context and temporal reasoning. Key limitations\ninclude 23.4% success on hardware-dependent issues and 41.2% on dynamic\nlanguage errors. Theoretical analysis shows O(k log d) retrieval complexity\nwith convergence guarantees. In a human evaluation (N=50), 89% of participants\npreferred Chronos over baseline models. Chronos will be available in Kodezi OS\nin Q4 2025 and via API in Q1 2026.", "comment": "27 pages, 21 figures, 37 tables, 2 algorithms. Extended technical\n  report. Introduces Chronos, an autonomous debugging system achieving 87.1%\n  success rate on real-world bugs. Code and data available at\n  https://github.com/Kodezi/chronos", "pdf_url": "http://arxiv.org/pdf/2507.12482v2", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-29"}
{"id": "2401.07515", "title": "On Purely Data-Driven Massive MIMO Detectors", "authors": ["Hao Ye", "Le Liang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.07515v2", "summary": "The development of learning-based detectors for massive multi-input\nmulti-output (MIMO) systems has been hindered by the inherent complexities\narising from the problem's high dimensionality. To enhance scalability, most\nprevious studies have adopted model-driven methodologies that integrate deep\nneural networks (DNNs) within existing iterative detection frameworks. However,\nthese methods often lack flexibility and involve substantial computational\ncomplexity. In this paper, we introduce ChannelNet, a purely data-driven\nlearning-based massive MIMO detector that overcomes these limitations.\nChannelNet exploits the inherent symmetry of MIMO systems by incorporating\nchannel-embedded layers and antenna-wise shared feature processors. These\nmodules maintain equivariance to antenna permutations and enable ChannelNet to\nscale efficiently to large numbers of antennas and high modulation orders with\nlow computational complexity, specifically $\\mathcal{O}(N_t N_r)$, where $N_t$\nand $N_r$ denote the numbers of transmit and receive antennas, respectively.\nTheoretically, ChannelNet can approximate any continuous permutation-symmetric\nfunction and the optimal maximum likelihood detection (ML) function with\narbitrary precision under any continuous channel distribution. Empirical\nevaluations demonstrate that ChannelNet consistently outperforms or matches\nstate-of-the-art detectors across different numbers of antennas, modulation\nschemes, and channel distributions, all while significantly reducing\ncomputational overhead. This study highlights the potential of purely\ndata-driven designs in advancing efficient and scalable detectors for massive\nMIMO systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.07515v2", "cate": "cs.IT", "date": "2024-01-15", "updated": "2025-07-29"}
{"id": "2109.04025", "title": "Improved Hardness of BDD and SVP Under Gap-(S)ETH", "authors": ["Huck Bennett", "Chris Peikert", "Yi Tang"], "categories": ["cs.CC", "cs.CR", "cs.DS"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      ITCS 2022. Updated to address (non-)existence of exponential kissing number lattices", "url": "http://arxiv.org/abs/2109.04025v3", "summary": "We show improved fine-grained hardness of two key lattice problems in the\n$\\ell_p$ norm: Bounded Distance Decoding to within an $\\alpha$ factor of the\nminimum distance ($\\mathrm{BDD}_{p, \\alpha}$) and the (decisional)\n$\\gamma$-approximate Shortest Vector Problem ($\\mathrm{SVP}_{p,\\gamma}$),\nassuming variants of the Gap (Strong) Exponential Time Hypothesis (Gap-(S)ETH).\nSpecifically, we show:\n  1. For all $p \\in [1, \\infty)$, there is no $2^{o(n)}$-time algorithm for\n$\\mathrm{BDD}_{p, \\alpha}$ for any constant $\\alpha > \\alpha_\\mathsf{kn}$,\nwhere $\\alpha_\\mathsf{kn} = 2^{-c_\\mathsf{kn}}$ and $c_\\mathsf{kn}$ is the\n$\\ell_2$ kissing-number constant, assuming $c_\\mathsf{kn} > 0$ and that\nnon-uniform Gap-ETH holds.\n  2. For all $p \\in [1, \\infty)$, there is no $2^{o(n)}$-time algorithm for\n$\\mathrm{BDD}_{p, \\alpha}$ for any constant $\\alpha > \\alpha^\\ddagger_p$, where\n$\\alpha^\\ddagger_p$ is explicit and satisfies $\\alpha^\\ddagger_p = 1$ for $1\n\\leq p \\leq 2$, $\\alpha^\\ddagger_p < 1$ for all $p > 2$, and $\\alpha^\\ddagger_p\n\\to 1/2$ as $p \\to \\infty$, unless randomized Gap-ETH is false.\n  3. For all $p \\in [1, \\infty) \\setminus 2 \\mathbb{Z}$ and all $C > 1$, there\nis no $2^{n/C}$-time algorithm for $\\mathrm{BDD}_{p, \\alpha}$ for any constant\n$\\alpha > \\alpha^\\dagger_{p, C}$, where $\\alpha^\\dagger_{p, C}$ is explicit and\nsatisfies $\\alpha^\\dagger_{p, C} \\to 1$ as $C \\to \\infty$ for any fixed $p \\in\n[1, \\infty)$, assuming $c_\\mathsf{kn} > 0$ and that non-uniform Gap-SETH holds.\n  4. For all $p > p_0 \\approx 2.1397$, $p \\notin 2\\mathbb{Z}$, and all $C >\nC_p$, there is no $2^{n/C}$-time algorithm for $\\mathrm{SVP}_{p, \\gamma}$ for\nsome constant $\\gamma > 1$, where $C_p > 1$ is explicit and satisfies $C_p \\to\n1$ as $p \\to \\infty$, unless randomized Gap-SETH is false.", "comment": "ITCS 2022. Updated to address (non-)existence of exponential kissing\n  number lattices", "pdf_url": "http://arxiv.org/pdf/2109.04025v3", "cate": "cs.CC", "date": "2021-09-09", "updated": "2025-07-28"}
{"id": "2409.18084", "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model", "authors": ["Shangyi Luo", "Peng Sun", "Ji Zhu", "Yuhong Deng", "Cunjun Yu", "Anxing Xiao", "Xueqian Wang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Robotics and Automation Letters (RA-L)", "url": "http://arxiv.org/abs/2409.18084v3", "summary": "With the increasing presence of service robots and autonomous vehicles in\nhuman environments, navigation systems need to evolve beyond simple destination\nreach to incorporate social awareness. This paper introduces GSON, a novel\ngroup-based social navigation framework that leverages Large Multimodal Models\n(LMMs) to enhance robots' social perception capabilities. Our approach uses\nvisual prompting to enable zero-shot extraction of social relationships among\npedestrians and integrates these results with robust pedestrian detection and\ntracking pipelines to overcome the inherent inference speed limitations of\nLMMs. The planning system incorporates a mid-level planner that sits between\nglobal path planning and local motion planning, effectively preserving both\nglobal context and reactive responsiveness while avoiding disruption of the\npredicted social group. We validate GSON through extensive real-world mobile\nrobot navigation experiments involving complex social scenarios such as\nqueuing, conversations, and photo sessions. Comparative results show that our\nsystem significantly outperforms existing navigation approaches in minimizing\nsocial perturbations while maintaining comparable performance on traditional\nnavigation metrics.", "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)", "pdf_url": "http://arxiv.org/pdf/2409.18084v3", "cate": "cs.RO", "date": "2024-09-26", "updated": "2025-07-29"}
{"id": "2507.21488", "title": "Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess", "authors": ["Zhenwei Tang", "Difan Jiao", "Eric Xue", "Reid McIlroy-Young", "Jon Kleinberg", "Siddhartha Sen", "Ashton Anderson"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21488v1", "summary": "As humans seek to collaborate with, learn from, and better understand\nartificial intelligence systems, developing AIs that can accurately emulate\nindividual decision-making becomes increasingly important. Chess, a\nlong-standing AI benchmark with precise skill measurement, offers an ideal\ntestbed for human-AI alignment. However, existing approaches to modeling human\nbehavior require prohibitively large amounts of data from each individual,\nmaking them impractical for new or sparsely represented users. In this work, we\nintroduce Maia4All, a framework designed to learn and adapt to individual\ndecision-making styles efficiently, even with limited data. Maia4All achieves\nthis through a two-stage optimization process: (1) an enrichment step, which\nbridges population and individual-level human behavior modeling with a\nprototype-enriched model, and (2) a democratization step, which leverages\nability levels or user prototypes to initialize and refine individual\nembeddings with minimal data. Our experimental results show that Maia4All can\naccurately predict individual moves and profile behavioral patterns with high\nfidelity, establishing a new standard for personalized human-like AI behavior\nmodeling in chess. Maia4All achieves individual human behavior modeling in\nchess with only 20 games, compared to the 5,000 games required previously,\nrepresenting a significant improvement in data efficiency. Our work provides an\nexample of how population AI systems can flexibly adapt to individual users\nusing a prototype-enriched model as a bridge. This approach extends beyond\nchess, as shown in our case study on idiosyncratic LLMs, highlighting its\npotential for broader applications in personalized AI adaptation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21488v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21299", "title": "Blending data and physics for reduced-order modeling of systems with spatiotemporal chaotic dynamics", "authors": ["Alex Guo", "Michael D. Graham"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21299v1", "summary": "While data-driven techniques are powerful tools for reduced-order modeling of\nsystems with chaotic dynamics, great potential remains for leveraging known\nphysics (i.e. a full-order model (FOM)) to improve predictive capability. We\ndevelop a hybrid reduced order model (ROM), informed by both data and FOM, for\nevolving spatiotemporal chaotic dynamics on an invariant manifold whose\ncoordinates are found using an autoencoder. This approach projects the vector\nfield of the FOM onto the invariant manifold; then, this physics-derived vector\nfield is either corrected using dynamic data, or used as a Bayesian prior that\nis updated with data. In both cases, the neural ordinary differential equation\napproach is used. We consider simulated data from the Kuramoto-Sivashinsky and\ncomplex Ginzburg-Landau equations. Relative to the data-only approach, for\nscenarios of abundant data, scarce data, and even an incorrect FOM (i.e.\nerroneous parameter values), the hybrid approach yields substantially improved\ntime-series predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21299v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.21073", "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      45 pages, 11 figures", "url": "http://arxiv.org/abs/2507.21073v1", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "comment": "45 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.21073v1", "cate": "cs.CL", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.21455", "title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation", "authors": ["Sheng-Feng Yu", "Jia-Jiun Yao", "Wei-Chen Chiu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21455v1", "summary": "Although larger datasets are crucial for training large deep models, the\nrapid growth of dataset size has brought a significant challenge in terms of\nconsiderable training costs, which even results in prohibitive computational\nexpenses. Dataset Distillation becomes a popular technique recently to reduce\nthe dataset size via learning a highly compact set of representative exemplars,\nwhere the model trained with these exemplars ideally should have comparable\nperformance with respect to the one trained with the full dataset. While most\nof existing works upon dataset distillation focus on supervised datasets, we\ninstead aim to distill images and their self-supervisedly trained\nrepresentations into a distilled set. This procedure, named as Self-Supervised\nDataset Distillation, effectively extracts rich information from real datasets,\nyielding the distilled sets with enhanced cross-architecture generalizability.\nParticularly, in order to preserve the key characteristics of original dataset\nmore faithfully and compactly, several novel techniques are proposed: 1) we\nintroduce an innovative parameterization upon images and representations via\ndistinct low-dimensional bases, where the base selection for parameterization\nis experimentally shown to play a crucial role; 2) we tackle the instability\ninduced by the randomness of data augmentation -- a key component in\nself-supervised learning but being underestimated in the prior work of\nself-supervised dataset distillation -- by utilizing predetermined\naugmentations; 3) we further leverage a lightweight network to model the\nconnections among the representations of augmented views from the same image,\nleading to more compact pairs of distillation. Extensive experiments conducted\non various datasets validate the superiority of our approach in terms of\ndistillation efficiency, cross-architecture generalization, and transfer\nlearning performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21455v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2401.10315", "title": "End-to-End Energy Saving in Cell-Free Massive MIMO ISAC for Ultra-Reliable Target-Aware Actuation", "authors": ["Zinat Behdad", "Özlem Tuğfe Demir", "Ki Won Sung", "Cicek Cavdar"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2401.10315v3", "summary": "Ultra-reliable target-aware actuation-where timely and accurate sensing\ninformation is used to trigger critical actions in emerging 6G sensing-based\napplications-demands tight integration of sensing and communication under\nstringent reliability and latency constraints. This paper investigates\nintegrated sensing and communication(ISAC)in a downlink CF-mMIMO system\nsupporting multi-static sensing and ultra-reliable low-latency communications\n(URLLC). We propose a joint power and blocklength allocation algorithm to\nminimize the E2E energy consumption while meeting communication and sensing\nrequirements.E2E energy consumption includes transmission, sensing receivers,\nand processing for both sensing and communication. The non-convex optimization\nproblem is solved using a combination of FPP-SCA, concave-convex programming\n(CCP), and fractional programming techniques. We consider two types of target\ndetectors:clutter-aware and clutter-unaware, each with distinct complexity and\nperformance trade-offs.A computational complexity analysis based on\ngiga-operations per second (GOPS) is conducted to quantify the processing\nrequirements of communication and sensing tasks. We perform a comprehensive\nperformance evaluation under various communication and sensing requirements,\nand benchmark our approach against two alternatives: one minimizing only\ntransmission energy for ISAC and one minimizing E2E energy only for URLLC\nwithout sensing integration. Simulation results demonstrate that the proposed\nalgorithm achieves enhanced detection capability with less E2E energy\nconsumption. Additionally, we examine the trade-offs between detector\ncomplexity, the number of antenna elements per access point(AP), and the number\nof sensing APs. Clutter-aware detectors, although more complex, require fewer\nantennas and sensing receive APs to meet detection requirements, thus yielding\nup to 40% energy savings.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2401.10315v3", "cate": "cs.IT", "date": "2024-01-18", "updated": "2025-07-29"}
{"id": "2507.21057", "title": "Examining the sentiment and emotional differences in product and service reviews: The moderating role of culture", "authors": ["Vinh Truong"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21057v1", "summary": "This study explores how emotions and sentiments differ in customer reviews of\nproducts and services on e-commerce platforms. Unlike earlier research that\ntreats all reviews uniformly, this study distinguishes between reviews of\nproducts, typically fulfilling basic, functional needs, and services, which\noften cater to experiential and emotional desires. The findings reveal clear\ndifferences in emotional expression and sentiment between the two. Product\nreviews frequently focus on practicality, such as functionality, reliability,\nand value for money, and are generally more neutral or pragmatic in tone. In\ncontrast, service reviews involve stronger emotional engagement, as services\noften entail personal interactions and subjective experiences. Customers\nexpress a broader spectrum of emotions, such as joy, frustration, or\ndisappointment when reviewing services, as identified using advanced machine\nlearning techniques. Cultural background further influences these patterns.\nConsumers from collectivist cultures, as defined by Hofstede cultural\ndimensions, often use more moderated and socially considerate language,\nreflecting an emphasis on group harmony. Conversely, consumers from\nindividualist cultures tend to offer more direct, emotionally intense feedback.\nNotably, gender appears to have minimal impact on sentiment variation,\nreinforcing the idea that the nature of the offering (product vs. service) and\ncultural context are the dominant factors. Theoretically, the study extends\nMaslow hierarchy of needs and Hofstede cultural framework to the domain of\nonline reviews, proposing a model that explains how these dimensions shape\nconsumer expression. Practically, the insights offer valuable guidance for\nbusinesses looking to optimize their marketing and customer engagement\nstrategies by aligning messaging and service design with customer expectations\nacross product types and cultural backgrounds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21057v1", "cate": "cs.CY", "date": "2025-05-07", "updated": "2025-05-07"}
{"id": "2306.13073", "title": "Unitary Complexity and the Uhlmann Transformation Problem", "authors": ["John Bostanci", "Yuval Efron", "Tony Metger", "Alexander Poremba", "Luowen Qian", "Henry Yuen"], "categories": ["quant-ph", "cs.CC", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      96 pages. Technical changes: the definitions of unitaryBQP, unitaryQIP, etc, updated to be uniform in the error parameter. The zero-knowledge completeness result simplified, and a (weak) polarization lemma for Uhlmann transformations was added. Editorial changes: many sections slimmed down, revised, and polished. Comments welcome", "url": "http://arxiv.org/abs/2306.13073v3", "summary": "State transformation problems such as compressing quantum information or\nbreaking quantum commitments are fundamental quantum tasks. However, their\ncomputational difficulty cannot easily be characterized using traditional\ncomplexity theory, which focuses on tasks with classical inputs and outputs.\n  To study the complexity of such state transformation tasks, we introduce a\nframework for unitary synthesis problems, including notions of reductions and\nunitary complexity classes. We use this framework to study the complexity of\ntransforming one entangled state into another via local operations. We\nformalize this as the Uhlmann Transformation Problem, an algorithmic version of\nUhlmann's theorem. Then, we prove structural results relating the complexity of\nthe Uhlmann Transformation Problem, polynomial space quantum computation, and\nzero knowledge protocols.\n  The Uhlmann Transformation Problem allows us to characterize the complexity\nof a variety of tasks in quantum information processing, including decoding\nnoisy quantum channels, breaking falsifiable quantum cryptographic assumptions,\nimplementing optimal prover strategies in quantum interactive proofs, and\ndecoding the Hawking radiation of black holes. Our framework for unitary\ncomplexity thus provides new avenues for studying the computational complexity\nof many natural quantum information processing tasks.", "comment": "96 pages. Technical changes: the definitions of unitaryBQP,\n  unitaryQIP, etc, updated to be uniform in the error parameter. The\n  zero-knowledge completeness result simplified, and a (weak) polarization\n  lemma for Uhlmann transformations was added. Editorial changes: many sections\n  slimmed down, revised, and polished. Comments welcome", "pdf_url": "http://arxiv.org/pdf/2306.13073v3", "cate": "quant-ph", "date": "2023-06-22", "updated": "2025-07-29"}
{"id": "2411.13205", "title": "An Integrated Approach to Robotic Object Grasping and Manipulation", "authors": ["Owais Ahmed", "M Huzaifa", "M Areeb", "Hamza Ali Khan"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.13205v3", "summary": "In response to the growing challenges of manual labor and efficiency in\nwarehouse operations, Amazon has embarked on a significant transformation by\nincorporating robotics to assist with various tasks. While a substantial number\nof robots have been successfully deployed for tasks such as item transportation\nwithin warehouses, the complex process of object picking from shelves remains a\nsignificant challenge. This project addresses the issue by developing an\ninnovative robotic system capable of autonomously fulfilling a simulated order\nby efficiently selecting specific items from shelves. A distinguishing feature\nof the proposed robotic system is its capacity to navigate the challenge of\nuncertain object positions within each bin of the shelf. The system is\nengineered to autonomously adapt its approach, employing strategies that enable\nit to efficiently locate and retrieve the desired items, even in the absence of\npre-established knowledge about their placements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.13205v3", "cate": "cs.RO", "date": "2024-11-20", "updated": "2025-07-29"}
{"id": "2507.21502", "title": "Large Language Models for Supply Chain Decisions", "authors": ["David Simchi-Levi", "Konstantina Mellou", "Ishai Menache", "Jeevan Pathuri"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Forthcoming chapter in AI in Supply Chains: Perspectives from Global Thought Leaders, edited by Maxime C. Cohen and Tinglong Dai, and part of the Springer Series in Supply Chain Management (edited by Prof. Chris Tang)", "url": "http://arxiv.org/abs/2507.21502v1", "summary": "Supply Chain Management requires addressing a variety of complex\ndecision-making challenges, from sourcing strategies to planning and execution.\nOver the last few decades, advances in computation and information technologies\nhave enabled the transition from manual, intuition and experience-based\ndecision-making, into more automated and data-driven decisions using a variety\nof tools that apply optimization techniques. These techniques use mathematical\nmethods to improve decision-making.\n  Unfortunately, business planners and executives still need to spend\nconsiderable time and effort to (i) understand and explain the recommendations\ncoming out of these technologies; (ii) analyze various scenarios and answer\nwhat-if questions; and (iii) update the mathematical models used in these tools\nto reflect current business environments. Addressing these challenges requires\ninvolving data science teams and/or the technology providers to explain results\nor make the necessary changes in the technology and hence significantly slows\ndown decision making.\n  Motivated by the recent advances in Large Language Models (LLMs), we report\nhow this disruptive technology can democratize supply chain technology -\nnamely, facilitate the understanding of tools' outcomes, as well as the\ninteraction with supply chain tools without human-in-the-loop. Specifically, we\nreport how we apply LLMs to address the three challenges described above, thus\nsubstantially reducing the time to decision from days and weeks to minutes and\nhours as well as dramatically increasing planners' and executives' productivity\nand impact.", "comment": "Forthcoming chapter in AI in Supply Chains: Perspectives from Global\n  Thought Leaders, edited by Maxime C. Cohen and Tinglong Dai, and part of the\n  Springer Series in Supply Chain Management (edited by Prof. Chris Tang)", "pdf_url": "http://arxiv.org/pdf/2507.21502v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21350", "title": "DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation", "authors": ["Wenkai Tan", "Alvaro Velasquez", "Houbing Song"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21350v1", "summary": "Neural networks have emerged as a powerful tool for modeling physical\nsystems, offering the ability to learn complex representations from limited\ndata while integrating foundational scientific knowledge. In particular,\nneuro-symbolic approaches that combine data-driven learning, the neuro, with\nsymbolic equations and rules, the symbolic, address the tension between methods\nthat are purely empirical, which risk straying from established physical\nprinciples, and traditional numerical solvers that demand complete geometric\nknowledge and can be prohibitively expensive for high-fidelity simulations. In\nthis work, we present a novel neuro-symbolic framework for reconstructing and\nsimulating elastic objects directly from sparse multi-view image sequences,\nwithout requiring explicit geometric information. Specifically, we integrate a\nneural radiance field (NeRF) for object reconstruction with physics-informed\nneural networks (PINN) that incorporate the governing partial differential\nequations of elasticity. In doing so, our method learns a spatiotemporal\nrepresentation of deforming objects that leverages both image supervision and\nsymbolic physical constraints. To handle complex boundary and initial\nconditions, which are traditionally confronted using finite element methods,\nboundary element methods, or sensor-based measurements, we employ an\nenergy-constrained Physics-Informed Neural Network architecture. This design\nenhances both simulation accuracy and the explainability of results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21350v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21093", "title": "Barriers to Digital Mental Health Services among College Students", "authors": ["Ha Na Cho", "Kyuha Jung", "Daniel Eisenberg", "Cheryl A. King", "Kai Zheng"], "categories": ["cs.CY", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21093v1", "summary": "This qualitative study explores barriers to utilization of digital mental\nhealth Intervention (DMHI) among college students. Data are from a large\nrandomized clinical trial of an intervention, eBridge, that used motivational\ninterviewing for online counseling to connect students with mental health\nissues to professional services. We applied thematic analysis to analyze the\nfeedback from the student participants regarding their experience of using the\nDMHI platform. We identified nine key barriers to DMHI adoption and the use of\nin-person mental health services: emotional distress, time constraints, privacy\nconcerns, resource accessibility, financial challenges, medication stigma,\ndissatisfaction with communication, content clarity, and treatment-related\nconcerns. Our findings emphasize the need for personalized, culturally\nsensitive interventions and improved strategies to enhance the access and\nengagement in mental health support for young adults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21093v1", "cate": "cs.CY", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.21460", "title": "An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes", "authors": ["Mianzhao Wang", "Fan Shi", "Xu Cheng", "Feifei Zhang", "Shengyong Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21460v1", "summary": "High-quality 4D light field representation with efficient angular feature\nmodeling is crucial for scene perception, as it can provide discriminative\nspatial-angular cues to identify moving targets. However, recent developments\nstill struggle to deliver reliable angular modeling in the temporal domain,\nparticularly in complex low-light scenes. In this paper, we propose a novel\nlight field epipolar-plane structure image (ESI) representation that explicitly\ndefines the geometric structure within the light field. By capitalizing on the\nabrupt changes in the angles of light rays within the epipolar plane, this\nrepresentation can enhance visual expression in low-light scenes and reduce\nredundancy in high-dimensional light fields. We further propose an\nangular-temporal interaction network (ATINet) for light field object tracking\nthat learns angular-aware representations from the geometric structural cues\nand angular-temporal interaction cues of light fields. Furthermore, ATINet can\nalso be optimized in a self-supervised manner to enhance the geometric feature\ninteraction across the temporal domain. Finally, we introduce a large-scale\nlight field low-light dataset for object tracking. Extensive experimentation\ndemonstrates that ATINet achieves state-of-the-art performance in single object\ntracking. Furthermore, we extend the proposed method to multiple object\ntracking, which also shows the effectiveness of high-quality light field\nangular-temporal modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21460v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.07189", "title": "Beamforming Design for Beyond Diagonal RIS-Aided Cell-Free Massive MIMO Systems", "authors": ["Yizhuo Li", "Jiakang Zheng", "Bokai Xu", "Yiyang Zhu", "Jiayi Zhang", "Dusit Niyato", "Bo Ai"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.07189v2", "summary": "Reconfigurable intelligent surface (RIS)-aided cell-free (CF) massive\nmultiple-input multiple-output (mMIMO) is a promising architecture for further\nimproving spectral efficiency (SE) with low cost and power consumption.\nHowever, conventional RIS has inevitable limitations due to its capability of\nonly reflecting signals. In contrast, beyond-diagonal RIS (BD-RIS), with its\nability to both reflect and transmit signals, has gained great attention. This\ncorrespondence focuses on using BD-RIS to improve the sum SE of CF mMIMO\nsystems. This requires completing the beamforming design under the transmit\npower constraints and unitary constraints of the BD-RIS, by optimizing active\nand passive beamformer simultaneously. To tackle this issue, we introduce an\nalternating optimization algorithm that decomposes it using fractional\nprogramming and solves the subproblems alternatively. Moreover, to address the\nchallenge introduced by the unitary constraint on the beamforming matrix of the\nBD-RIS, a manifold optimization algorithm is proposed to solve the problem\noptimally. Simulation results show that BD-RISs outperform RISs\ncomprehensively, especially in the case of the full connected architecture\nwhich achieves the best performance, enhancing the sum SE by around 40%\ncompared to ideal RISs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.07189v2", "cate": "cs.IT", "date": "2025-03-10", "updated": "2025-07-29"}
{"id": "2507.21059", "title": "Dependency on Meta AI Chatbot in Messenger Among STEM and Non-STEM Students in Higher Education", "authors": ["Hilene E. Hernandez", "Rhiziel P. Manalese", "Roque Francis B. Dianelo", "Jaymark A. Yambao", "Almer B. Gamboa", "Lloyd D. Feliciano", "Mike Haizon M. David", "Freneil R. Pampo", "John Paul P. Miranda"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      17 pages, 4 tables, 34 references", "url": "http://arxiv.org/abs/2507.21059v1", "summary": "To understand the potential dependency of tertiary students regarding Meta AI\nin the academic context. This descriptive cross-sectional study surveyed 872\ntertiary students from public and private institutions in Luzon, Philippines.\nDemographic information and perceptions on Meta AI dependency based on existing\nliterature were collected. Descriptive statistics were used to summarize the\ndata and differences between STEM and non-STEM students were analyzed using the\nMann-Whitney U test. The results indicate a nuanced perspective on Meta AI\nchatbot use among students. While there is general disagreement with heavy\nreliance on the chatbot for academic tasks, psychological support, and social\nfactors, there is moderate agreement on its technological benefits and academic\nutility. Students value the Meta AI convenience, availability, and\nproblem-solving assistance, but prefer traditional resources and human\ninteraction for academic and social support. Concerns about dependency risks\nand impacts on critical thinking are acknowledged, particularly among STEM\nstudents, who rely more on chatbots for academic purposes. This suggests that\nwhile Meta AI is a valuable resource, its role is complementary rather than\ntransformative in educational contexts, with institutional encouragement and\nindividual preferences influencing usage patterns. Students generally hesitate\nto rely heavily on meta-AI chatbots. This reflects a preference for traditional\nresources and independent problem-solving. While students acknowledge AI\nchatbots academic benefits and technological convenience, concerns about\noverreliance and its impact on critical thinking persist, particularly among\nSTEM students, who appear more inclined to integrate these tools into their\nstudies.", "comment": "17 pages, 4 tables, 34 references", "pdf_url": "http://arxiv.org/pdf/2507.21059v1", "cate": "cs.CY", "date": "2025-05-13", "updated": "2025-05-13"}
{"id": "2409.13864", "title": "Persistent Backdoor Attacks in Continual Learning", "authors": ["Zhen Guo", "Abhinav Kumar", "Reza Tourani"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 20 figures, 6 tables", "url": "http://arxiv.org/abs/2409.13864v3", "summary": "Backdoor attacks pose a significant threat to neural networks, enabling\nadversaries to manipulate model outputs on specific inputs, often with\ndevastating consequences, especially in critical applications. While backdoor\nattacks have been studied in various contexts, little attention has been given\nto their practicality and persistence in continual learning, particularly in\nunderstanding how the continual updates to model parameters, as new data\ndistributions are learned and integrated, impact the effectiveness of these\nattacks over time. To address this gap, we introduce two persistent backdoor\nattacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal\nadversarial influence. Our blind task backdoor subtly alters the loss\ncomputation without direct control over the training process, while the latent\ntask backdoor influences only a single task's training, with all other tasks\ntrained benignly. We evaluate these attacks under various configurations,\ndemonstrating their efficacy with static, dynamic, physical, and semantic\ntriggers. Our results show that both attacks consistently achieve high success\nrates across different continual learning algorithms, while effectively evading\nstate-of-the-art defenses, such as SentiNet and I-BAU.", "comment": "19 pages, 20 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2409.13864v3", "cate": "cs.LG", "date": "2024-09-20", "updated": "2025-07-29"}
{"id": "2412.10180", "title": "A General Safety Framework for Autonomous Manipulation in Human Environments", "authors": ["Jakob Thumm", "Julian Balletshofer", "Leonardo Maglanoc", "Luis Muschal", "Matthias Althoff"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.10180v2", "summary": "Autonomous robots are projected to significantly augment the manual\nworkforce, especially in repetitive and hazardous tasks. For a successful\ndeployment of such robots in human environments, it is crucial to guarantee\nhuman safety. State-of-the-art approaches to ensure human safety are either too\nconservative to permit a natural human-robot collaboration or make strong\nassumptions that do not hold for autonomous robots, e.g., knowledge of a\npre-defined trajectory. Therefore, we propose the shield for Safe Autonomous\nhuman-robot collaboration through Reachability Analysis (SARA shield). This\nnovel power and force limiting framework provides formal safety guarantees for\nmanipulation in human environments while realizing fast robot speeds. As\nunconstrained contacts allow for significantly higher contact forces than\nconstrained contacts (also known as clamping), we use reachability analysis to\nclassify potential contacts by their type in a formally correct way. For each\ncontact type, we formally verify that the kinetic energy of the robot is below\npain and injury thresholds for the respective human body part in contact. Our\nexperiments show that SARA shield satisfies the contact safety constraints\nwhile significantly improving the robot performance in comparison to\nstate-of-the-art approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.10180v2", "cate": "cs.RO", "date": "2024-12-13", "updated": "2025-07-28"}
{"id": "2507.21503", "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions", "authors": ["Yanxu Zhu", "Shitong Duan", "Xiangxu Zhang", "Jitao Sang", "Peng Zhang", "Tun Lu", "Xiao Zhou", "Jing Yao", "Xiaoyuan Yi", "Xing Xie"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21503v1", "summary": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable\nadvancements in vision-language tasks, yet produce potentially harmful or\nuntrustworthy content. Despite substantial work investigating the\ntrustworthiness of language models, MMLMs' capability to act honestly,\nespecially when faced with visually unanswerable questions, remains largely\nunderexplored. This work presents the first systematic assessment of honesty\nbehaviors across various MLLMs. We ground honesty in models' response behaviors\nto unanswerable visual questions, define four representative types of such\nquestions, and construct MoHoBench, a large-scale MMLM honest benchmark,\nconsisting of 12k+ visual question samples, whose quality is guaranteed by\nmulti-stage filtering and human verification. Using MoHoBench, we benchmarked\nthe honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our\nfindings show that: (1) most models fail to appropriately refuse to answer when\nnecessary, and (2) MMLMs' honesty is not solely a language modeling issue, but\nis deeply influenced by visual information, necessitating the development of\ndedicated methods for multimodal honesty alignment. Therefore, we implemented\ninitial alignment methods using supervised and preference learning to improve\nhonesty behavior, providing a foundation for future work on trustworthy MLLMs.\nOur data and code can be found at https://github.com/DSTTSD/MoHoBench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21503v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21357", "title": "A Contrastive Diffusion-based Network (CDNet) for Time Series Classification", "authors": ["Yaoyu Zhang", "Chi-Guhn Lee"], "categories": ["cs.LG", "62M10", "I.5.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, conference", "url": "http://arxiv.org/abs/2507.21357v1", "summary": "Deep learning models are widely used for time series classification (TSC) due\nto their scalability and efficiency. However, their performance degrades under\nchallenging data conditions such as class similarity, multimodal distributions,\nand noise. To address these limitations, we propose CDNet, a Contrastive\nDiffusion-based Network that enhances existing classifiers by generating\ninformative positive and negative samples via a learned diffusion process.\nUnlike traditional diffusion models that denoise individual samples, CDNet\nlearns transitions between samples--both within and across classes--through\nconvolutional approximations of reverse diffusion steps. We introduce a\ntheoretically grounded CNN-based mechanism to enable both denoising and mode\ncoverage, and incorporate an uncertainty-weighted composite loss for robust\ntraining. Extensive experiments on the UCR Archive and simulated datasets\ndemonstrate that CDNet significantly improves state-of-the-art (SOTA) deep\nlearning classifiers, particularly under noisy, similar, and multimodal\nconditions.", "comment": "19 pages, conference", "pdf_url": "http://arxiv.org/pdf/2507.21357v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21571", "title": "Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations", "authors": ["Laura Spillner", "Nima Zargham", "Mihai Pomarlan", "Robert Porzel", "Rainer Malaka"], "categories": ["cs.AI", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Presented at the IJCAI 2023 Workshop on Explainable Artificial Intelligence (XAI)", "url": "http://arxiv.org/abs/2507.21571v1", "summary": "The need for explanations in AI has, by and large, been driven by the desire\nto increase the transparency of black-box machine learning models. However,\nsuch explanations, which focus on the internal mechanisms that lead to a\nspecific output, are often unsuitable for non-experts. To facilitate a\nhuman-centered perspective on AI explanations, agents need to focus on\nindividuals and their preferences as well as the context in which the\nexplanations are given. This paper proposes a personalized approach to\nexplanation, where the agent tailors the information provided to the user based\non what is most likely pertinent to them. We propose a model of the agent's\nworldview that also serves as a personal and dynamic memory of its previous\ninteractions with the same user, based on which the artificial agent can\nestimate what part of its knowledge is most likely new information to the user.", "comment": "Presented at the IJCAI 2023 Workshop on Explainable Artificial\n  Intelligence (XAI)", "pdf_url": "http://arxiv.org/pdf/2507.21571v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21489", "title": "Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval", "authors": ["Zhichuan Wang", "Yang Zhou", "Zhe Liu", "Rui Yu", "Song Bai", "Yulong Wang", "Xinwei He", "Xiang Bai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.21489v1", "summary": "Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D\nobjects of unseen categories beyond the training set. Existing methods\ntypically utilize all modalities (i.e., voxels, point clouds, multi-view\nimages) and train specific backbones before fusion. However, they still\nstruggle to produce generalized representations due to insufficient 3D training\ndata. Being contrastively pre-trained on web-scale image-text pairs, CLIP\ninherently produces generalized representations for a wide range of downstream\ntasks. Building upon it, we present a simple yet effective framework named\nDescribe, Adapt and Combine (DAC) by taking only multi-view images for open-set\n3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large\nlanguage model (MLLM) to learn generalized 3D representations, where the MLLM\nis used for dual purposes. First, it describes the seen category information to\nalign with CLIP's training objective for adaptation during training. Second, it\nprovides external hints about unknown objects complementary to visual cues\nduring inference. To improve the synergy, we introduce an Additive-Bias\nLow-Rank adaptation (AB-LoRA), which alleviates overfitting and further\nenhances the generalization to unseen categories. With only multi-view images,\nDAC significantly surpasses prior arts by an average of +10.01\\% mAP on four\nopen-set 3DOR datasets. Moreover, its generalization is also validated on\nimage-based and cross-dataset setups. Code is available at\nhttps://github.com/wangzhichuan123/DAC.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.21489v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.14408", "title": "Algebraic Barriers to Halving Algorithmic Information Quantities in Correlated Strings", "authors": ["Andrei Romashchenko"], "categories": ["cs.IT", "cs.DM", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      22 pages, 3 figures. v3: revised and extended version. A short version of the paper has been accepted to appear at the MFCS 2025", "url": "http://arxiv.org/abs/2504.14408v3", "summary": "We study the possibility of scaling down algorithmic information quantities\nin tuples of correlated strings. In particular, we address a question raised by\nAlexander Shen: whether, for any triple of strings \\((a, b, c)\\), there exists\na string \\(z\\) such that each conditional Kolmogorov complexity \\(C(a|z),\nC(b|z), C(c|z)\\) is approximately half of the corresponding unconditional\nKolmogorov complexity. We provide a negative answer to this question by\nconstructing a triple \\((a, b, c)\\) for which no such string \\(z\\) exists.\nMoreover, we provide a fully explicit example of such a tuple. Our construction\nis based on combinatorial properties of incidences in finite projective planes\nand relies on recent bounds for point-line incidences over prime fields,\nobtained using tools from additive combinatorics and algebraic methods, notably\nresults by Bourgain--Katz--Tao and Stevens--De Zeeuw. As an application, we\nshow that this impossibility yields lower bounds on the communication\ncomplexity of secret key agreement protocols in certain settings. These results\nreveal algebraic obstructions to efficient information exchange and highlight a\nseparation in information-theoretic behavior between fields with and without\nproper subfields.", "comment": "22 pages, 3 figures. v3: revised and extended version. A short\n  version of the paper has been accepted to appear at the MFCS 2025", "pdf_url": "http://arxiv.org/pdf/2504.14408v3", "cate": "cs.IT", "date": "2025-04-19", "updated": "2025-07-28"}
{"id": "2507.21062", "title": "Automated but Atrophied? Student Over-Reliance vs Expert Augmentation of AI in Learning and Cybersecurity", "authors": ["Koffka Khan"], "categories": ["cs.CY", "68T07 (Primary), 68T05, 68U99 (Secondary)", "I.2.6; I.2.7; K.3.2; K.4.1; F.2.2"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21062v1", "summary": "University students and working professionals are increasingly encountering\ngenerative artificial intelligence (AI) in education and practice, yet their\napproaches and outcomes differ markedly. This paper proposes an academic study\ncontrasting novice over-reliance on AI with expert augmentation of AI, grounded\nin two real-world narratives. In one, a university student attempted to\noutsource learning entirely to AI, eschewing course engagement. In the other,\nseasoned cybersecurity professionals in the Tradewinds 2025 red/blue team\nexercise collaboratively employed AI tools to enhance (not replace) their\ndomain expertise. This proposal outlines a comparative research design to\ninvestigate how students' perception of AI as a learning replacement versus\nprofessionals' use of AI as an expert tool impacts outcomes. Drawing on current\nliterature in educational technology and workplace AI, we examine implications\nfor curriculum design, AI literacy, and assessment reform in higher education.\nWe hypothesize that blind reliance on AI can erode fundamental skills and\nacademic integrity, whereas guided use of AI by knowledgeable users can amplify\nproductivity without sacrificing quality. The paper details methodologies for\nclassroom and workplace data collection, including student and professional\nsurveys, interviews, and performance analyses. Anticipated findings aim to\ninform responsible AI integration in curricula, balancing innovation with the\nnecessity of domain knowledge. We conclude with recommendations for pedagogical\nstrategies, institutional policies to foster AI literacy, and a call for\nlongitudinal studies tracking how AI usage during university affects\nprofessional competencies over time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21062v1", "cate": "cs.CY", "date": "2025-05-22", "updated": "2025-05-22"}
{"id": "2501.17866", "title": "Advancing Brainwave-Based Biometrics: A Large-Scale, Multi-Session Evaluation", "authors": ["Matin Fallahi", "Patricia Arias-Cabarcos", "Thorsten Strufe"], "categories": ["eess.SP", "cs.CR"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.17866v2", "summary": "The field of brainwave-based biometrics has gained attention for its\npotential to revolutionize user authentication through hands-free interaction,\nresistance to shoulder surfing, continuous authentication, and revocability.\nHowever, current research often relies on single-session or limited-session\ndatasets with fewer than 55 subjects, raising concerns about the\ngeneralizability of the findings. To address this gap, we conducted a\nlarge-scale study using a public brainwave dataset comprising 345 subjects and\nover 6,007 sessions (an average of 17 per subject) recorded over five years\nusing three headsets. Our results reveal that deep learning approaches\nsignificantly outperform hand-crafted feature extraction methods. We also\nobserve Equal Error Rates (EER) increases over time (e.g., from 6.7% after 1\nday to 14.3% after a year). Therefore, it is necessary to reinforce the\nenrollment set after successful login attempts. Moreover, we demonstrate that\nfewer brainwave measurement sensors can be used, with an acceptable increase in\nEER, which is necessary for transitioning from medical-grade to affordable\nconsumer-grade devices. Finally, we compared our results to prior work and\nexisting biometric standards. While our performance is on par with or exceeds\nprevious approaches, it still falls short of industrial benchmarks. Based on\nthe results, we hypothesize that further improvements are possible with larger\ntraining sets. To support future research, we have open-sourced our analysis\ncode.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.17866v2", "cate": "eess.SP", "date": "2025-01-14", "updated": "2025-07-29"}
{"id": "2503.10170", "title": "GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for Geometrically Consistent Rendering and Reconstruction", "authors": ["Jianheng Liu", "Yunfei Wan", "Bowen Wang", "Chunran Zheng", "Jiarong Lin", "Fu Zhang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, IROS 2025", "url": "http://arxiv.org/abs/2503.10170v2", "summary": "Digital twins are fundamental to the development of autonomous driving and\nembodied artificial intelligence. However, achieving high-granularity surface\nreconstruction and high-fidelity rendering remains a challenge. Gaussian\nsplatting offers efficient photorealistic rendering but struggles with\ngeometric inconsistencies due to fragmented primitives and sparse observational\ndata in robotics applications. Existing regularization methods, which rely on\nrender-derived constraints, often fail in complex environments. Moreover,\neffectively integrating sparse LiDAR data with Gaussian splatting remains\nchallenging. We propose a unified LiDAR-visual system that synergizes Gaussian\nsplatting with a neural signed distance field. The accurate LiDAR point clouds\nenable a trained neural signed distance field to offer a manifold geometry\nfield. This motivates us to offer an SDF-based Gaussian initialization for\nphysically grounded primitive placement and a comprehensive geometric\nregularization for geometrically consistent rendering and reconstruction.\nExperiments demonstrate superior reconstruction accuracy and rendering quality\nacross diverse trajectories. To benefit the community, the codes are released\nat https://github.com/hku-mars/GS-SDF.", "comment": "8 pages, IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.10170v2", "cate": "cs.RO", "date": "2025-03-13", "updated": "2025-07-29"}
{"id": "2507.21513", "title": "What Does it Mean for a Neural Network to Learn a \"World Model\"?", "authors": ["Kenneth Li", "Fernanda Viégas", "Martin Wattenberg"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21513v1", "summary": "We propose a set of precise criteria for saying a neural net learns and uses\na \"world model.\" The goal is to give an operational meaning to terms that are\noften used informally, in order to provide a common language for experimental\ninvestigation. We focus specifically on the idea of representing a latent\n\"state space\" of the world, leaving modeling the effect of actions to future\nwork. Our definition is based on ideas from the linear probing literature, and\nformalizes the notion of a computation that factors through a representation of\nthe data generation process. An essential addition to the definition is a set\nof conditions to check that such a \"world model\" is not a trivial consequence\nof the neural net's data or task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21513v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21386", "title": "Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem", "authors": ["Xuan Wu", "Di Wang", "Chunguo Wu", "Kaifang Qi", "Chunyan Miao", "Yubin Xiao", "Jian Zhang", "You Zhou"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21386v1", "summary": "Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed\nto address Vehicle Routing Problems (VRPs). However, most of these solvers\nfocus exclusively on single-vehicle VRP variants, overlooking the more\nrealistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP),\nwhich involves multiple vehicles. Existing MMHCVRP solvers typically select a\nvehicle and its next node to visit at each decoding step, but often make myopic\ndecoding decisions and overlook key properties of MMHCVRP, including local\ntopological relationships, vehicle permutation invariance, and node symmetry,\nresulting in suboptimal performance. To better address these limitations, we\npropose ECHO, an efficient NCO solver. First, ECHO exploits the proposed\ndual-modality node encoder to capture local topological relationships among\nnodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed\nParameter-Free Cross-Attention mechanism to prioritize the vehicle selected in\nthe preceding decoding step. Finally, leveraging vehicle permutation invariance\nand node symmetry, we introduce a tailored data augment strategy for MMHCVRP to\nstabilize the Reinforcement Learning training process. To assess the\nperformance of ECHO, we conduct extensive experiments. The experimental results\ndemonstrate that ECHO outperforms state-of-the-art NCO solvers across varying\nnumbers of vehicles and nodes, and exhibits well-performing generalization\nacross both scales and distribution patterns. Finally, ablation studies\nvalidate the effectiveness of all proposed methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21386v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21664", "title": "Can the current trends of AI handle a full course of mathematics?", "authors": ["Mariam Alsayyad", "Fayadh Kadhem"], "categories": ["cs.AI", "cs.HC", "math.HO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      36 pages", "url": "http://arxiv.org/abs/2507.21664v1", "summary": "This paper addresses the question of how able the current trends of\nArtificial Intelligence (AI) are in managing to take the responsibility of a\nfull course of mathematics at a college level. The study evaluates this ability\nin four significant aspects, namely, creating a course syllabus, presenting\nselected material, answering student questions, and creating an assessment. It\nshows that even though the AI is strong in some important parts like\norganization and accuracy, there are still some human aspects that are far away\nfrom the current abilities of AI. There is still a hidden emotional part, even\nin science, that cannot be fulfilled by the AI in its current state. This paper\nsuggests some recommendations to integrate the human and AI potentials to\ncreate better outcomes in terms of reaching the target of creating a full\ncourse of mathematics, at a university level, as best as possible.", "comment": "36 pages", "pdf_url": "http://arxiv.org/pdf/2507.21664v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21507", "title": "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding", "authors": ["Shibo Gao", "Peipei Yang", "Yangyang Liu", "Yi Chen", "Han Zhu", "Xuyao Zhang", "Linlin Huang"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 19 figures, 8 tables", "url": "http://arxiv.org/abs/2507.21507v1", "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events in videos and\naccurately determine their time intervals. Current VAD methods mainly fall into\ntwo categories: traditional DNN-based approaches that focus on temporal\nlocalization, and LLM-based approaches that emphasize semantic understanding.\nBoth anomaly understanding and grounding are essential for comprehensive video\nanomaly detection and can complement each other. However, no existing model or\ndataset supports both tasks simultaneously. To address this, we introduce VAGU\n(Video Anomaly Grounding and Understanding), the first benchmark to integrate\nboth tasks. Each VAGU instance includes annotations for anomaly category,\nsemantic explanation, precise temporal grounding and Video QA. We also provide\nmultiple-choice Video QA for objective evaluation. Based on this dataset, we\npropose Glance then Scrutinize (GtS), a training-free framework guided by\ntextual prompts. The framework first enables coarse localization of\nhigh-probability anomalous regions, followed by detailed anomaly interpretation\nand temporal boundary refinement. Additionally, we propose the JeAUG metric,\nwhich jointly evaluates semantic interpretability and temporal precision,\novercoming the limitations of traditional metrics. Extensive experiments verify\nthe effectiveness of our benchmark, framework, and evaluation metric.", "comment": "21 pages, 19 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2507.21507v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.14795", "title": "A DPI-PAC-Bayesian Framework for Generalization Bounds", "authors": ["Muhan Guan", "Farhad Farokhi", "Jingge Zhu"], "categories": ["cs.IT", "math.IT", "stat.ML"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      7 pages, 1 figures, the final version with full proofs", "url": "http://arxiv.org/abs/2507.14795v3", "summary": "We develop a unified Data Processing Inequality PAC-Bayesian framework --\nabbreviated DPI-PAC-Bayesian -- for deriving generalization error bounds in the\nsupervised learning setting. By embedding the Data Processing Inequality (DPI)\ninto the change-of-measure technique, we obtain explicit bounds on the binary\nKullback-Leibler generalization gap for both R\\'enyi divergence and any\n$f$-divergence measured between a data-independent prior distribution and an\nalgorithm-dependent posterior distribution. We present three bounds derived\nunder our framework using R\\'enyi, Hellinger \\(p\\) and Chi-Squared divergences.\nAdditionally, our framework also demonstrates a close connection with other\nwell-known bounds. When the prior distribution is chosen to be uniform, our\nbounds recover the classical Occam's Razor bound and, crucially, eliminate the\nextraneous \\(\\log(2\\sqrt{n})/n\\) slack present in the PAC-Bayes bound, thereby\nachieving tighter results. The framework thus bridges data-processing and\nPAC-Bayesian perspectives, providing a flexible, information-theoretic tool to\nconstruct generalization guarantees.", "comment": "7 pages, 1 figures, the final version with full proofs", "pdf_url": "http://arxiv.org/pdf/2507.14795v3", "cate": "cs.IT", "date": "2025-07-20", "updated": "2025-07-29"}
{"id": "2507.21064", "title": "Cybroc: Cyborgizing Broccoli for Longevity", "authors": ["Ke Huang", "Yue Zhou", "Xi He", "Weibo Chen", "Botao Amber Hu"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted by ISEA 2025", "url": "http://arxiv.org/abs/2507.21064v1", "summary": "Cybroc is a series of kinetic art installations exploring the recent\nproliferating populist longevity activism through the satirical cyborgization\nof broccoli. The artwork augments the symbol of health food-broccoli-with\nprosthetic limbs to perform so-called longevity-enhancing exercises such as\ncold plunges, treadmill running, brachiation (arm-swinging), sled pushing,\netc.-all simulations of primal human survival tasks reframed as modern fitness\nroutines. Despite its mechanical augmentations, the broccoli's inevitable decay\nand rotting after exhibiting high-intensity performances prompts reflection on\nthe limits of biological enhancement and the ethics of human enhancement beyond\nnatural capabilities, particularly transhumanist ideals. By juxtaposing a\nsymbolic healthy vegetable with cutting-edge concepts of human enhancement,\nCybroc challenges viewers to consider the intersection of nature, technology,\nand the human quest for extended lifespan in our transhuman era.", "comment": "Accepted by ISEA 2025", "pdf_url": "http://arxiv.org/pdf/2507.21064v1", "cate": "cs.CY", "date": "2025-05-24", "updated": "2025-05-24"}
{"id": "2404.18282", "title": "Efficient Runtime Verification of Real-Time Systems under Parametric Communication Delays", "authors": ["Martin Fränzle", "Thomas M. Grosen", "Kim G. Larsen", "Martin Zimmermann"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.18282v4", "summary": "Timed B\\\"uchi automata provide a very expressive formalism for expressing\nrequirements of real-time systems. Online monitoring and active testing of\nembedded real-time systems can then be achieved by symbolic execution of such\nautomata on the trace observed from the system. This direct construction\nhowever only is faithful if observation of the trace is immediate in the sense\nthat the monitor (or test harness, respectively) can assign exact time stamps\nto the actions it observes, which is rarely true in practice due to the\nsubstantial and fluctuating parametric delays introduced by the circuitry\nconnecting the observed system to its monitoring or testing device.\n  We present purely zone-based online monitoring and testing algorithms, which\nhandle such parametric delays exactly without recurrence to costly verification\nprocedures for parametric timed automata. We have implemented our algorithms on\ntop of the real-time model checking tool UPPAAL, and report on encouraging\ninitial results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.18282v4", "cate": "cs.FL", "date": "2024-04-28", "updated": "2025-07-29"}
{"id": "2505.06581", "title": "An $\\tilde{O}$ptimal Differentially Private Learner for Concept Classes with VC Dimension 1", "authors": ["Chao Yan"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Add proper learner", "url": "http://arxiv.org/abs/2505.06581v2", "summary": "We present the first nearly optimal differentially private PAC learner for\nany concept class with VC dimension 1 and Littlestone dimension $d$. Our\nalgorithm achieves the sample complexity of\n$\\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$, nearly matching the\nlower bound of $\\Omega(\\log^* d)$ proved by Alon et al. [STOC19]. Prior to our\nwork, the best known upper bound is $\\tilde{O}(VC\\cdot d^5)$ for general VC\nclasses, as shown by Ghazi et al. [STOC21].", "comment": "Add proper learner", "pdf_url": "http://arxiv.org/pdf/2505.06581v2", "cate": "cs.LG", "date": "2025-05-10", "updated": "2025-07-29"}
{"id": "2504.06538", "title": "OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning", "authors": ["Daniel Tcheurekdjian", "Joshua Klasmeier", "Tom Cooney", "Christopher McCann", "Tyler Fenstermaker"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      We withdraw our submission following peer review feedback that identified methodological limitations: specifically, our experimental design does not adequately support the causal claims made in the submission. The work was preliminary undergraduate research that requires substantial additional experimental validation to properly establish the proposed causal relationships", "url": "http://arxiv.org/abs/2504.06538v2", "summary": "We present OPAL (Operant Physical Agent with Language), a novel\nvision-language-action architecture that introduces topological constraints to\nflow matching for robotic control. To do so, we further introduce topological\nattention. Our approach models action sequences as topologically-structured\nrepresentations with non-trivial constraints. Experimental results across 10\ncomplex manipulation tasks demonstrate OPAL's superior performance compared to\nprevious approaches, including Octo, OpenVLA, and ${\\pi}$0.\n  Our architecture achieves significant improvements in zero-shot performance\nwithout requiring task-specific fine-tuning, while reducing inference\ncomputational requirements by 42%. The theoretical guarantees provided by our\ntopological approach result in more coherent long-horizon action sequences. Our\nresults highlight the potential of constraining the search space of learning\nproblems in robotics by deriving from fundamental physical laws, and the\npossibility of using topological attention to embed causal understanding into\ntransformer architectures.", "comment": "We withdraw our submission following peer review feedback that\n  identified methodological limitations: specifically, our experimental design\n  does not adequately support the causal claims made in the submission. The\n  work was preliminary undergraduate research that requires substantial\n  additional experimental validation to properly establish the proposed causal\n  relationships", "pdf_url": "http://arxiv.org/pdf/2504.06538v2", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-07-29"}
{"id": "2507.21518", "title": "ST-GDance: Long-Term and Collision-Free Group Choreography from Music", "authors": ["Jing Xu", "Weiqiang Wang", "Cunjian Chen", "Jun Liu", "Qiuhong Ke"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures. Accepted at BMVC 2025", "url": "http://arxiv.org/abs/2507.21518v2", "summary": "Group dance generation from music has broad applications in film, gaming, and\nanimation production. However, it requires synchronizing multiple dancers while\nmaintaining spatial coordination. As the number of dancers and sequence length\nincrease, this task faces higher computational complexity and a greater risk of\nmotion collisions. Existing methods often struggle to model dense\nspatial-temporal interactions, leading to scalability issues and multi-dancer\ncollisions. To address these challenges, we propose ST-GDance, a novel\nframework that decouples spatial and temporal dependencies to optimize\nlong-term and collision-free group choreography. We employ lightweight graph\nconvolutions for distance-aware spatial modeling and accelerated sparse\nattention for efficient temporal modeling. This design significantly reduces\ncomputational costs while ensuring smooth and collision-free interactions.\nExperiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms\nstate-of-the-art baselines, particularly in generating long and coherent group\ndance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.", "comment": "10 pages, 3 figures. Accepted at BMVC 2025", "pdf_url": "http://arxiv.org/pdf/2507.21518v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21394", "title": "Systolic Array-based Accelerator for State-Space Models", "authors": ["Shiva Raja", "Cansu Demirkiran", "Aakash Sarkar", "Milos Popovic", "Ajay Joshi"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21394v1", "summary": "Sequence modeling is crucial for AI to understand temporal data and detect\ncomplex time-dependent patterns. While recurrent neural networks (RNNs),\nconvolutional neural networks (CNNs), and Transformers have advanced in\ncapturing long-range dependencies, they struggle with achieving high accuracy\nwith very long sequences due to limited memory retention (fixed context\nwindow). State-Space Models (SSMs) leverage exponentially decaying memory\nenabling lengthy context window and so they process very long data sequences\nmore efficiently than recurrent and Transformer-based models. Unlike\ntraditional neural models like CNNs and RNNs, SSM-based models require solving\ndifferential equations through continuous integration, making training and\ninference both compute- and memory-intensive on conventional CPUs and GPUs. In\nthis paper we introduce a specialized hardware accelerator, EpochCore, for\naccelerating SSMs. EpochCore is based on systolic arrays (SAs) and is designed\nto enhance the energy efficiency and throughput of inference of SSM-based\nmodels for long-range sequence tasks. Within the SA, we propose a versatile\nprocessing element (PE) called LIMA-PE to perform traditional and specialized\nMAC operations to support traditional DNNs and SSMs. To complement the\nEpochCore microarchitecture, we propose a novel dataflow, ProDF, which enables\nhighly efficient execution of SSM-based models. By leveraging the LIMA-PE\nmicroarchitecture and ProDF, EpochCore achieves on average 250x gains in\nperformance and 45x improvement in energy efficiency, at the expense of 2x\nincrease in area cost over traditional SA-based accelerators, and around\n~2,000x improvement in latency/inference on LRA datasets compared to GPU kernel\noperations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21394v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2410.04177", "title": "Exploring Keyboard Positioning and Ten-Finger Typing in Mixed Reality", "authors": ["Cecilia Schmitz", "Joshua Reynolds", "Scott Kuhl", "Keith Vertanen"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.04177v3", "summary": "Accuracy and speed are pivotal when typing. Mixed reality typing is typically\nperformed by typing on a midair keyboard with your index fingers. This deprives\nusers of both the tactile feedback available on physical devices and the\nability to press keys with the most convenient finger. Our first experiment\ninvestigated providing tactile feedback by positioning the virtual keyboard on\na table or wall. The keyboard was deterministic (without auto-correct),\nsupported mixed case typing with symbols, and relied only on the hand-tracking\nprovided by a commodity headset's egocentric cameras. Users preferred and had\nthe highest entry rate of 12 words-per-minute using a midair keyboard. Error\nrates were similar in all conditions. Our second experiment explored ten-finger\ntyping and used a novel eye-tracking technique to avoid accidental key presses.\nThis technique was preferred for ten-finger typing and halved corrections.\nHowever, participants were faster using their index fingers without\neye-tracking at 11 words-per-minute.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.04177v3", "cate": "cs.HC", "date": "2024-10-05", "updated": "2025-07-28"}
{"id": "2507.21521", "title": "Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration", "authors": ["Athmanarayanan Lakshmi Narayanan", "Amrutha Machireddy", "Ranganath Krishnan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Joint Conference on Neural Networks 2025 (Accepted)", "url": "http://arxiv.org/abs/2507.21521v1", "summary": "Active Learning (AL) has emerged as a powerful approach for minimizing\nlabeling costs by selectively sampling the most informative data for neural\nnetwork model development. Effective AL for large-scale vision-language models\nnecessitates addressing challenges in uncertainty estimation and efficient\nsampling given the vast number of parameters involved. In this work, we\nintroduce a novel parameter-efficient learning methodology that incorporates\nuncertainty calibration loss within the AL framework. We propose a\ndifferentiable loss function that promotes uncertainty calibration for\neffectively selecting fewer and most informative data samples for fine-tuning.\nThrough extensive experiments across several datasets and vision backbones, we\ndemonstrate that our solution can match and exceed the performance of complex\nfeature-based sampling techniques while being computationally very efficient.\nAdditionally, we investigate the efficacy of Prompt learning versus Low-rank\nadaptation (LoRA) in sample selection, providing a detailed comparative\nanalysis of these methods in the context of efficient AL.", "comment": "International Joint Conference on Neural Networks 2025 (Accepted)", "pdf_url": "http://arxiv.org/pdf/2507.21521v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.19266", "title": "Overview of 3GPP Release 19 Study on Channel Modeling Enhancements to TR 38.901 for 6G", "authors": ["Hitesh Poddar", "Dimitri Gold", "Daewon Lee", "Nan Zhang", "Gokul Sridharan", "Henrik Asplund", "Mansoor Shafi"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19266v2", "summary": "Channel models are a fundamental component of wireless communication systems,\nproviding critical insights into the physics of radio wave propagation. As\nwireless systems evolve every decade, the development of accurate and\nstandardized channel models becomes increasingly important for the development,\nevaluation and performance assessment of emerging technologies. An effort to\ndevelop a standardized channel model began around 2000 through the Third\nGeneration Partnership Project (3GPP) and the International Telecommunication\nUnion (ITU) with the aim of addressing a broad range of frequencies from sub-1\nGHz to 100 GHz. Prior efforts focused heavily on sub-6 GHz bands and mmWave\nbands, and there exist some gaps in accurately modeling the 7-24 GHz frequency\nrange, a promising candidate band for 6G. To address these gaps, 3GPP approved\na Release (Rel) 19 channel modeling study. This study resulted in several\nenhancements to the channel models, including the ability to accurately model a\nSuburban Macrocell (SMa) scenario, realistic User Terminal (UT) antenna models,\nvariability in the number of clusters, variability in the number of rays per\ncluster, a framework for capturing variability in power among all\npolarizations, near field (NF) propagation, and spatial non-stationarity (SNS)\neffects, all of which may be crucial for future 6G deployments. This paper\npresents the outcomes of this study and provides an overview of the underlying\nrationale, and key discussions that guided the validation, refinement, and\nenhancements of the 3GPP TR 38.901 channel models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19266v2", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-29"}
{"id": "2507.21066", "title": "Digital Sovereigns Big Tech and Nation-State Influence", "authors": ["Michael Bollerman"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21066v1", "summary": "Technology companies have gained unprecedented power and influence in recent\nyears, resembling quasi-nation-states globally. Corporations with\ntrillion-dollar market capitalizations are no longer just providers of digital\nservices; they now wield immense economic power, influence global\ninfrastructure, and significantly impact political and social dynamics. This\nthesis examines how these corporations have transcended traditional business\nmodels, adopting characteristics typically associated with sovereign states.\nThey now enforce regulations, shape public discourse, and influence legal\nframeworks in various countries. This shift presents unique challenges,\nincluding the undermining of democratic governance, the exacerbation of\neconomic inequalities, and the enabling of unregulated data exploitation and\nprivacy violations. The study will examine critical instances of tech companies\nacting as quasi-governmental bodies and assess the risks associated with\nunchecked corporate influence in global governance. Ultimately, the thesis aims\nto propose policy frameworks and regulatory interventions to curb the overreach\nof tech giants, restoring the balance between democratic institutions and\ncorporate power and ensuring that the digital future aligns with the public\ngood rather than creating Frankenstein-like monsters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21066v1", "cate": "cs.CY", "date": "2025-06-01", "updated": "2025-06-01"}
{"id": "2409.05456", "title": "Exploiting Assumptions for Effective Monitoring of Real-Time Properties under Partial Observability", "authors": ["Alessandro Cimatti", "Thomas M. Grosen", "Kim G. Larsen", "Stefano Tonetta", "Martin Zimmermann"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.05456v2", "summary": "Runtime verification of temporal properties is essential for ensuring the\ncorrectness and reliability of real-time systems, particularly in\ncyber-physical systems. A significant challenge in this domain is the effective\nprediction of property failure or success, especially when dealing with\npartially observable systems. This paper addresses these challenges by\ndeveloping an Assumption-Based Runtime Verification (ABRV) approach for a\ncontinuous real-time setting. Our method exploits assumptions about the\nsystem's behavior, specified as Timed Automata, to enable monitors to predict\nfuture outcomes and handle unobservable system parts, such as internal faults.\nProperties to be monitored are specified using Metric Interval Temporal Logic\n(MITL). The approach also includes formalizing observations with data and time\nuncertainty using sequences of timed constraints. We present a zone-based\nonline algorithm for computing the monitoring verdict, implemented on top of\nthe UPPAAL tool. Experimental evaluation on proof-of-concept cases demonstrates\nthe approach's feasibility and effectiveness, illustrating how assumptions\nfacilitate earlier verdicts, enable monitoring of properties dependent on\nunobservable events, and provide insights into scalability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.05456v2", "cate": "cs.FL", "date": "2024-09-09", "updated": "2025-07-29"}
{"id": "2507.21749", "title": "Improving Neural Network Training using Dynamic Learning Rate Schedule for PINNs and Image Classification", "authors": ["D. Veerababu", "Ashwin A. Raikar", "Prasanta K. Ghosh"], "categories": ["cs.CE", "cs.LG", "34A06", "G.1.6; I.6.4; J.2"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.21749v1", "summary": "Training neural networks can be challenging, especially as the complexity of\nthe problem increases. Despite using wider or deeper networks, training them\ncan be a tedious process, especially if a wrong choice of the hyperparameter is\nmade. The learning rate is one of such crucial hyperparameters, which is\nusually kept static during the training process. Learning dynamics in complex\nsystems often requires a more adaptive approach to the learning rate. This\nadaptability becomes crucial to effectively navigate varying gradients and\noptimize the learning process during the training process. In this paper, a\ndynamic learning rate scheduler (DLRS) algorithm is presented that adapts the\nlearning rate based on the loss values calculated during the training process.\nExperiments are conducted on problems related to physics-informed neural\nnetworks (PINNs) and image classification using multilayer perceptrons and\nconvolutional neural networks, respectively. The results demonstrate that the\nproposed DLRS accelerates training and improves stability.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.21749v1", "cate": "cs.CE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.14322", "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning", "authors": ["Md Rafid Haque", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11; C.2.4; K.6.5"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 8 figures. This work is intended for a journal submission", "url": "http://arxiv.org/abs/2507.14322v2", "summary": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems.", "comment": "24 pages, 8 figures. This work is intended for a journal submission", "pdf_url": "http://arxiv.org/pdf/2507.14322v2", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-07-28"}
{"id": "2505.20726", "title": "ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making", "authors": ["Liu Dai", "Haina Wang", "Weikang Wan", "Hao Su"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project Website: this https URL", "url": "http://arxiv.org/abs/2505.20726v2", "summary": "Building embodied agents capable of accomplishing arbitrary tasks is a core\nobjective towards achieving embodied artificial general intelligence (E-AGI).\nWhile recent work has advanced such general robot policies, their training and\nevaluation are often limited to tasks within specific scenes, involving\nrestricted instructions and scenarios. Existing benchmarks also typically rely\non manual annotation of limited tasks in a few scenes. We argue that exploring\nthe full spectrum of feasible tasks within any given scene is crucial, as they\nprovide both extensive benchmarks for evaluation and valuable resources for\nagent improvement. Towards this end, we introduce ManiTaskGen, a novel system\nthat automatically generates comprehensive, diverse, feasible mobile\nmanipulation tasks for any given scene. The generated tasks encompass both\nprocess-based, specific instructions (e.g., \"move object from X to Y\") and\noutcome-based, abstract instructions (e.g., \"clear the table\"). We apply\nManiTaskGen to both simulated and real-world scenes, demonstrating the validity\nand diversity of the generated tasks. We then leverage these tasks to\nautomatically construct benchmarks, thoroughly evaluating the embodied\ndecision-making capabilities of agents built upon existing vision-language\nmodels (VLMs). Furthermore, we propose a simple yet effective method that\nutilizes ManiTaskGen tasks to enhance embodied decision-making. Overall, this\nwork presents a universal task generation framework for arbitrary scenes,\nfacilitating both benchmarking and improvement of embodied decision-making\nagents.", "comment": "Project Website: https://manitaskgen.github.io/", "pdf_url": "http://arxiv.org/pdf/2505.20726v2", "cate": "cs.RO", "date": "2025-05-27", "updated": "2025-07-29"}
{"id": "2507.21524", "title": "Large Language Models for Wireless Communications: From Adaptation to Autonomy", "authors": ["Le Liang", "Hao Ye", "Yucheng Sheng", "Ouya Wang", "Jiacheng Wang", "Shi Jin", "Geoffrey Ye Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21524v1", "summary": "The emergence of large language models (LLMs) has revolutionized artificial\nintelligence, offering unprecedented capabilities in reasoning, generalization,\nand zero-shot learning. These strengths open new frontiers in wireless\ncommunications, where increasing complexity and dynamics demand intelligent and\nadaptive solutions. This article explores the role of LLMs in transforming\nwireless systems across three key directions: adapting pretrained LLMs for core\ncommunication tasks, developing wireless-specific foundation models to balance\nversatility and efficiency, and enabling agentic LLMs with autonomous reasoning\nand coordination capabilities. We highlight recent advances, practical case\nstudies, and the unique benefits of LLM-based approaches over traditional\nmethods. Finally, we outline open challenges and research opportunities,\nincluding multimodal fusion, collaboration with lightweight models, and\nself-improving capabilities, charting a path toward intelligent, adaptive, and\nautonomous wireless networks of the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21524v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21397", "title": "Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach", "authors": ["Fnu Hairi", "Jiao Yang", "Tianchen Zhou", "Haibo Yang", "Chaosheng Dong", "Fan Yang", "Michinari Momma", "Yan Gao", "Jia Liu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21397v1", "summary": "In many multi-objective reinforcement learning (MORL) applications, being\nable to systematically explore the Pareto-stationary solutions under multiple\nnon-convex reward objectives with theoretical finite-time sample complexity\nguarantee is an important and yet under-explored problem. This motivates us to\ntake the first step and fill the important gap in MORL. Specifically, in this\npaper, we propose a \\uline{M}ulti-\\uline{O}bjective weighted-\\uline{CH}ebyshev\n\\uline{A}ctor-critic (MOCHA) algorithm for MORL, which judiciously integrates\nthe weighted-Chebychev (WC) and actor-critic framework to enable\nPareto-stationarity exploration systematically with finite-time sample\ncomplexity guarantee. Sample complexity result of MOCHA algorithm reveals an\ninteresting dependency on $p_{\\min}$ in finding an $\\epsilon$-Pareto-stationary\nsolution, where $p_{\\min}$ denotes the minimum entry of a given weight vector\n$\\mathbf{p}$ in WC-scarlarization. By carefully choosing learning rates, the\nsample complexity for each exploration can be\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$. Furthermore, simulation studies on a\nlarge KuaiRand offline dataset, show that the performance of MOCHA algorithm\nsignificantly outperforms other baseline MORL approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21397v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.10044", "title": "MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification", "authors": ["Shaohan Shi", "Yuheng Shao", "Haoran Jiang", "Yunjie Yao", "Zhijun Zhang", "Xu Ding", "Quan Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Will appear at UIST2025", "url": "http://arxiv.org/abs/2507.10044v3", "summary": "Medical images often contain multiple labels with imbalanced distributions\nand co-occurrence, leading to bias in multi-label medical image classification.\nClose collaboration between medical professionals and machine learning\npractitioners has significantly advanced medical image analysis. However,\ntraditional collaboration modes struggle to facilitate effective feedback\nbetween physicians and AI models, as integrating medical expertise into the\ntraining process via engineers can be time-consuming and labor-intensive. To\nbridge this gap, we introduce MEDebiaser, an interactive system enabling\nphysicians to directly refine AI models using local explanations. By combining\nprediction with attention loss functions and employing a customized ranking\nstrategy to alleviate scalability, MEDebiaser allows physicians to mitigate\nbiases without technical expertise, reducing reliance on engineers, and thus\nenhancing more direct human-AI feedback. Our mechanism and user studies\ndemonstrate that it effectively reduces biases, improves usability, and\nenhances collaboration efficiency, providing a practical solution for\nintegrating medical expertise into AI-driven healthcare.", "comment": "Will appear at UIST2025", "pdf_url": "http://arxiv.org/pdf/2507.10044v3", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-29"}
{"id": "2507.21529", "title": "Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance", "authors": ["Mengling Xu", "Ming Tao", "Bing-Kun Bao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.21529v1", "summary": "Cooking process visualization is a promising task in the intersection of\nimage generation and food analysis, which aims to generate an image for each\ncooking step of a recipe. However, most existing works focus on generating\nimages of finished foods based on the given recipes, and face two challenges to\nvisualize the cooking process. First, the appearance of ingredients changes\nvariously across cooking steps, it is difficult to generate the correct\nappearances of foods that match the textual description, leading to semantic\ninconsistency. Second, the current step might depend on the operations of\nprevious step, it is crucial to maintain the contextual coherence of images in\nsequential order. In this work, we present a cooking process visualization\nmodel, called Chain-of-Cooking. Specifically, to generate correct appearances\nof ingredients, we present a Dynamic Patch Selection Module to retrieve\npreviously generated image patches as references, which are most related to\ncurrent textual contents. Furthermore, to enhance the coherence and keep the\nrational order of generated images, we propose a Semantic Evolution Module and\na Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the\nsemantics of previous texts, the Semantic Evolution Module establishes the\nsemantical association between latent prompts and current cooking step, and\nmerges it with the latent features. Then the CoT Guidance updates the merged\nfeatures to guide the current cooking step remain coherent with the previous\nstep. Moreover, we construct a dataset named CookViz, consisting of\nintermediate image-text pairs for the cooking process. Quantitative and\nqualitative experiments show that our method outperforms existing methods in\ngenerating coherent and semantic consistent cooking process.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.21529v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.24358", "title": "SQuat: Subspace-orthogonal KV Cache Quantization", "authors": ["Hao Wang", "Ligong Han", "Kai Xu", "Akash Srivastava"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.24358v2", "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.24358v2", "cate": "cs.LG", "date": "2025-03-31", "updated": "2025-07-28"}
{"id": "2507.21076", "title": "Making a Case for Research Collaboration Between Artificial Intelligence and Operations Research Experts", "authors": ["Radhika Kulkarni", "Gianluca Brero", "Yu Ding", "Swati Gupta", "Sven Koenig", "Ramayya Krishnan", "Thiago Serra", "Phebe Vayanos", "Segev Wasserkrug", "Holly Wiberg"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21076v1", "summary": "In 2021, INFORMS, ACM SIGAI, and the Computing Community Consortium (CCC)\nhosted three workshops to explore synergies between Artificial Intelligence\n(AI) and Operations Research (OR) to improve decision-making. The workshops\naimed to create a unified research vision for AI/OR collaboration, focusing on\novercoming cultural differences and maximizing societal impact. The first two\nworkshops addressed technological innovations, applications, and trustworthy AI\ndevelopment, while the final workshop highlighted specific areas for AI/OR\nintegration. Participants discussed \"Challenge Problems\" and strategies for\ncombining AI and OR techniques. This report outlines five key recommendations\nto enhance AI/OR collaboration: 1) Funding Opportunities, 2) Joint Education,\n3) Long-term Research Programs, 4) Aligning Conferences/Journals, and 5)\nBenchmark Creation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21076v1", "cate": "cs.CY", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2503.18077", "title": "Conservative Perception Models for Probabilistic Verification", "authors": ["Matthew Cleaveland", "Pengyuan Lu", "Oleg Sokolsky", "Insup Lee", "Ivan Ruchkin"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.18077v3", "summary": "Verifying the behaviors of autonomous systems with learned perception\ncomponents is a challenging problem due to the complexity of the perception and\nthe uncertainty of operating environments. Probabilistic model checking is a\npowerful tool for providing guarantees on stochastic models of systems.\nHowever, constructing model-checkable models of black-box perception components\nfor system-level mathematical guarantees has been an enduring challenge. In\nthis paper, we propose a method for constructing provably conservative Interval\nMarkov Decision Process (IMDP) models of closed-loop systems with perception\ncomponents. We prove that our technique results in conservative abstractions\nwith a user-specified probability. We evaluate our approach in an automatic\nbraking case study using both a synthetic perception component and the object\ndetector YOLO11 in the CARLA driving simulator.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.18077v3", "cate": "cs.FL", "date": "2025-03-23", "updated": "2025-07-29"}
{"id": "2507.21990", "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge", "authors": ["Zihan Zhao", "Bo Chen", "Ziping Wan", "Lu Chen", "Xuanze Lin", "Shiyang Yu", "Situo Zhang", "Da Ma", "Zichen Zhu", "Danyang Zhang", "Huayang Wang", "Zhongyang Dai", "Liyang Wen", "Xin Chen", "Kai Yu"], "categories": ["cs.CE", "cs.AI"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      13 figures, 4 tables", "url": "http://arxiv.org/abs/2507.21990v2", "summary": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.", "comment": "13 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.21990v2", "cate": "cs.CE", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2506.18264", "title": "Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle", "authors": ["Jagadeswara PKV Pothuri", "Aditya Bhatt", "Prajit KrisshnaKumar", "Manaswin Oddiraju", "Souma Chowdhury"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for presentation in proceedings of AIAA Aviation 2025", "url": "http://arxiv.org/abs/2506.18264v2", "summary": "Autonomous tracking of flying aerial objects has important civilian and\ndefense applications, ranging from search and rescue to counter-unmanned aerial\nsystems (counter-UAS). Ground based tracking requires setting up\ninfrastructure, could be range limited, and may not be feasible in remote\nareas, crowded cities or in dense vegetation areas. Vision based active\ntracking of aerial objects from another airborne vehicle, e.g., a chaser\nunmanned aerial vehicle (UAV), promises to fill this important gap, along with\nserving aerial coordination use cases. Vision-based active tracking by a UAV\nentails solving two coupled problems: 1) compute-efficient and accurate\n(target) object detection and target state estimation; and 2) maneuver\ndecisions to ensure that the target remains in the field of view in the future\ntime-steps and favorably positioned for continued detection. As a solution to\nthe first problem, this paper presents a novel integration of standard deep\nlearning based architectures with Kernelized Correlation Filter (KCF) to\nachieve compute-efficient object detection without compromising accuracy,\nunlike standalone learning or filtering approaches. The proposed perception\nframework is validated using a lab-scale setup. For the second problem, to\nobviate the linearity assumptions and background variations limiting\neffectiveness of the traditional controllers, we present the use of\nreinforcement learning to train a neuro-controller for fast computation of\nvelocity maneuvers. New state space, action space and reward formulations are\ndeveloped for this purpose, and training is performed in simulation using\nAirSim. The trained model is also tested in AirSim with respect to complex\ntarget maneuvers, and is found to outperform a baseline PID control in terms of\ntracking up-time and average distance maintained (from the target) during\ntracking.", "comment": "Accepted for presentation in proceedings of AIAA Aviation 2025", "pdf_url": "http://arxiv.org/pdf/2506.18264v2", "cate": "cs.RO", "date": "2025-06-23", "updated": "2025-07-29"}
{"id": "2507.21585", "title": "SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation", "authors": ["Hao Ye", "Mengshi Qi", "Zhaohong Liu", "Liang Liu", "Huadong Ma"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21585v1", "summary": "In this work, we study how vision-language models (VLMs) can be utilized to\nenhance the safety for the autonomous driving system, including perception,\nsituational understanding, and path planning. However, existing research has\nlargely overlooked the evaluation of these models in traffic safety-critical\ndriving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K)\nand propose a new baseline based on VLM with knowledge graph-based\nretrieval-augmented generation (SafeDriveRAG) for visual question answering\n(VQA). Specifically, we introduce SafeDrive228K, the first large-scale\nmultimodal question-answering benchmark comprising 228K examples across 18\nsub-tasks. This benchmark encompasses a diverse range of traffic safety\nqueries, from traffic accidents and corner cases to common safety knowledge,\nenabling a thorough assessment of the comprehension and reasoning abilities of\nthe models. Furthermore, we propose a plug-and-play multimodal knowledge\ngraph-based retrieval-augmented generation approach that employs a novel\nmulti-scale subgraph retrieval algorithm for efficient information retrieval.\nBy incorporating traffic safety guidelines collected from the Internet, this\nframework further enhances the model's capacity to handle safety-critical\nsituations. Finally, we conduct comprehensive evaluations on five mainstream\nVLMs to assess their reliability in safety-sensitive driving tasks.\nExperimental results demonstrate that integrating RAG significantly improves\nperformance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in\nCorner Cases tasks and +14.57% in Traffic Safety Commonsense across five\nmainstream VLMs, underscoring the potential of our proposed benchmark and\nmethodology for advancing research in traffic safety. Our source code and data\nare available at https://github.com/Lumos0507/SafeDriveRAG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21585v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21404", "title": "Data Leakage and Redundancy in the LIT-PCBA Benchmark", "authors": ["Amber Huang", "Ian Scott Knight", "Slava Naprienko"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21404v1", "summary": "LIT-PCBA is a widely used benchmark for virtual screening, but our audit\nreveals it is fundamentally compromised. The dataset suffers from egregious\ndata leakage, rampant duplication, and pervasive analog redundancy -- flaws\nthat invalidate its use for fair model evaluation. Notably, we identify 2,491\ninactives duplicated across training and validation sets, and thousands more\nrepeated within individual data splits (2,945 in training, 789 in validation).\nCritically, three ligands in the query set -- meant to represent unseen test\ncases -- are leaked: two appear in the training set, one in validation.\nStructural redundancy compounds these issues: for some targets, over 80% of\nquery ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1\nalone, we find 323 highly similar active pairs between training and validation\nsets, invalidating claims of chemical diversity. These and other flaws\ncollectively cause models trained on LIT-PCBA to memorize rather than\ngeneralize. To demonstrate the consequences of these data integrity failures,\nwe implement a trivial memorization-based baseline -- using no learning, no\nphysics, and no modeling -- that outperforms state-of-the-art models, including\ndeep neural networks like CHEESE, on LIT-PCBA simply by exploiting these\nartifacts. Our findings render the benchmark unfit for its intended purpose and\ncall into question previous results based on its use. We share this audit to\nraise awareness and provide tooling to help the community develop more rigorous\nand reliable datasets going forward. All scripts necessary to reproduce our\naudit and the baseline implementation are available at:\nhttps://github.com/sievestack/LIT-PCBA-audit", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21404v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.17943", "title": "Automated Brake Onset Detection in Naturalistic Driving Data", "authors": ["Shu-Yuan Liu", "Johan Engström", "Gustav Markkula"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17943v2", "summary": "Response timing measures play a crucial role in the assessment of automated\ndriving systems (ADS) in collision avoidance scenarios, including but not\nlimited to establishing human benchmarks and comparing ADS to human driver\nresponse performance. For example, measuring the response time (of a human\ndriver or ADS) to a conflict requires the determination of a stimulus onset and\na response onset. In existing studies, response onset relies on manual\nannotation or vehicle control signals such as accelerator and brake pedal\nmovements. These methods are not applicable when analyzing large scale data\nwhere vehicle control signals are not available. This holds in particular for\nthe rapidly expanding sets of ADS log data where the behavior of surrounding\nroad users is observed via onboard sensors. To advance evaluation techniques\nfor ADS and enable measuring response timing when vehicle control signals are\nnot available, we developed a simple and efficient algorithm, based on a\npiecewise linear acceleration model, to automatically estimate brake onset that\ncan be applied to any type of driving data that includes vehicle longitudinal\ntime series data. We also proposed a manual annotation method to identify brake\nonset and used it as ground truth for validation. R^2 was used as a confidence\nmetric to measure the accuracy of the algorithm, and its classification\nperformance was analyzed using naturalistic collision avoidance data of both\nADS and humans, where our method was validated against human manual annotation.\nAlthough our algorithm is subject to certain limitations, it is efficient,\ngeneralizable, applicable to any road user and scenario types, and is highly\nconfigurable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17943v2", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-28"}
{"id": "2507.21530", "title": "Suppressing Gradient Conflict for Generalizable Deepfake Detection", "authors": ["Ming-Hui Liu", "Harry Cheng", "Xin Luo", "Xin-Shun Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      V1", "url": "http://arxiv.org/abs/2507.21530v1", "summary": "Robust deepfake detection models must be capable of generalizing to\never-evolving manipulation techniques beyond training data. A promising\nstrategy is to augment the training data with online synthesized fake images\ncontaining broadly generalizable artifacts. However, in the context of deepfake\ndetection, it is surprising that jointly training on both original and online\nsynthesized forgeries may result in degraded performance. This contradicts the\ncommon belief that incorporating more source-domain data should enhance\ndetection accuracy. Through empirical analysis, we trace this degradation to\ngradient conflicts during backpropagation which force a trade-off between\nsource domain accuracy and target domain generalization. To overcome this\nissue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) framework\nthat explicitly mitigates the gradient conflict via two synergistic modules.\nFirst, an Update Vector Search (UVS) module searches for an alternative update\nvector near the initial gradient vector to reconcile the disparities of the\noriginal and online synthesized forgeries. By further transforming the search\nprocess into an extremum optimization problem, UVS yields the uniquely update\nvector, which maximizes the simultaneous loss reductions for each data type.\nSecond, a Conflict Gradient Reduction (CGR) module enforces a low-conflict\nfeature embedding space through a novel Conflict Descent Loss. This loss\npenalizes misaligned gradient directions and guides the learning of\nrepresentations with aligned, non-conflicting gradients. The synergy of UVS and\nCGR alleviates gradient interference in both parameter optimization and\nrepresentation learning. Experiments on multiple deepfake benchmarks\ndemonstrate that CS-DFD achieves state-of-the-art performance in both in-domain\ndetection accuracy and cross-domain generalization.", "comment": "V1", "pdf_url": "http://arxiv.org/pdf/2507.21530v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21082", "title": "Safety Features for a Centralised AGI Project", "authors": ["Sarah Hastings-Woodhouse"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21082v1", "summary": "Recent AI progress has outpaced expectations, with some experts now\npredicting AI that matches or exceeds human capabilities in all cognitive areas\n(AGI) could emerge this decade, potentially posing grave national and global\nsecurity threats. AI development is currently occurring primarily in the\nprivate sector with minimal oversight. This report analyzes a scenario where\nthe US government centralizes AGI development under its direct control, and\nidentifies four high-level priorities and seven safety features to reduce\nrisks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21082v1", "cate": "cs.CY", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2504.10008", "title": "Time for Timed Monitorability", "authors": ["Thomas M. Grosen", "Sean Kauffman", "Kim G. Larsen", "Martin Zimmermann"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10008v2", "summary": "Monitoring is an important part of the verification toolbox, in particular in\nsituations where exhaustive verification using, e.g., model-checking is\ninfeasible. The goal of online monitoring is to determine the satisfaction or\nviolation of a specification during runtime, i.e., based on finite execution\nprefixes. However, not every specification is amenable to monitoring, e.g.,\nproperties for which no finite execution can witness satisfaction or violation.\nMonitorability is the question of whether a given specification is amenable to\nmonitoring, and has been extensively studied in discrete time.\n  Here, we study the monitorability problem for real-time properties expressed\nas Timed Automata. For specifications given by deterministic Timed Muller\nAutomata, we prove decidability while we show that the problem is undecidable\nfor specifications given by nondeterministic Timed B\\\"uchi automata.\n  Furthermore, we refine monitorability to also determine bounds on the number\nof events as well as the time that must pass before monitoring the property may\nyield an informative verdict. We prove that for deterministic Timed Muller\nautomata, such bounds can be effectively computed. In contrast we show that for\nnondeterministic Timed B\\\"uchi automata such bounds are not computable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10008v2", "cate": "cs.FL", "date": "2025-04-14", "updated": "2025-07-29"}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "categories": ["cs.AI", "cs.CE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07426v2", "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07426v2", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-12"}
{"id": "2507.04922", "title": "Automated UAV-based Wind Turbine Blade Inspection: Blade Stop Angle Estimation and Blade Detail Prioritized Exposure Adjustment", "authors": ["Yichuan Shi", "Hao Liu", "Haowen Zheng", "Haowen Yu", "Xianqi Liang", "Jie Li", "Minmin Ma", "Ximin Lyu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, final submission to IROS 2025", "url": "http://arxiv.org/abs/2507.04922v2", "summary": "Unmanned aerial vehicles (UAVs) are critical in the automated inspection of\nwind turbine blades. Nevertheless, several issues persist in this domain.\nFirstly, existing inspection platforms encounter challenges in meeting the\ndemands of automated inspection tasks and scenarios. Moreover, current blade\nstop angle estimation methods are vulnerable to environmental factors,\nrestricting their robustness. Additionally, there is an absence of real-time\nblade detail prioritized exposure adjustment during capture, where lost details\ncannot be restored through post-optimization. To address these challenges, we\nintroduce a platform and two approaches. Initially, a UAV inspection platform\nis presented to meet the automated inspection requirements. Subsequently, a\nFermat point based blade stop angle estimation approach is introduced,\nachieving higher precision and success rates. Finally, we propose a blade\ndetail prioritized exposure adjustment approach to ensure appropriate\nbrightness and preserve details during image capture. Extensive tests,\ncomprising over 120 flights across 10 wind turbine models in 5 operational wind\nfarms, validate the effectiveness of the proposed approaches in enhancing\ninspection autonomy.", "comment": "8 pages, 7 figures, final submission to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.04922v2", "cate": "cs.RO", "date": "2025-07-07", "updated": "2025-07-29"}
{"id": "2507.21588", "title": "Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning", "authors": ["Jiong Yin", "Liang Li", "Jiehua Zhang", "Yuhan Gao", "Chenggang Yan", "Xichun Sheng"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.21588v1", "summary": "Audio-visual multi-task incremental learning aims to continuously learn from\nmultiple audio-visual tasks without the need for joint training on all tasks.\nThe challenge of the problem is how to preserve the old task knowledge while\nfacilitating the learning of new task with previous experiences. To address\nthese challenges, we introduce a three-stage Progressive Homeostatic and\nPlastic audio-visual prompt (PHP) method. In the shallow phase, we design the\ntask-shared modality aggregating adapter to foster cross-task and cross-modal\naudio-visual representation learning to enhance shared understanding between\ntasks. In the middle phase, we propose the task-specific modality-shared\ndynamic generating adapter, which constructs prompts that are tailored to\nindividual tasks while remaining general across modalities, which balances the\nmodels ability to retain knowledge against forgetting with its potential for\nversatile multi-task transferability. In the deep phase, we introduce the\ntask-specific modality-independent prompts to further refine the understand\nability by targeting individual information for each task and modality. By\nincorporating these three phases, PHP retains task-specific prompts while\nadapting shared parameters for new tasks to effectively balance knowledge\nsharing and specificity. Our method achieves SOTA performance in different\norders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at\nhttps://github.com/ENJOY-Yin-jiong/PHP.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.21588v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21422", "title": "Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring", "authors": ["Sujia Huang", "Lele Fu", "Zhen Cui", "Tong Zhang", "Na Song", "Bo Huang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21422v1", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning from\ngraph-structured data, leveraging message passing to diffuse information and\nupdate node representations. However, most efforts have suggested that native\ninteractions encoded in the graph may not be friendly for this process,\nmotivating the development of graph rewiring methods. In this work, we propose\na torque-driven hierarchical rewiring strategy, inspired by the notion of\ntorque in classical mechanics, dynamically modulating message passing to\nimprove representation learning in heterophilous graphs and enhance robustness\nagainst noisy graphs. Specifically, we define an interference-aware torque\nmetric that integrates structural distance and energy scores to quantify the\nperturbation induced by edges, thereby encouraging each node to aggregate\ninformation from its nearest low-energy neighbors. We use the metric to\nhierarchically reconfigure the receptive field of each layer by judiciously\npruning high-torque edges and adding low-torque links, suppressing propagation\nnoise and boosting pertinent signals. Extensive evaluations on benchmark\ndatasets show that our approach surpasses state-of-the-art methods on both\nheterophilous and homophilous graphs, and maintains high accuracy on noisy\ngraph.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21422v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.17985", "title": "Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Victor Tian", "Zachary Zhang", "Kevin He", "Min Sun"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17985v2", "summary": "The integration of large language models (LLMs) into educational tools has\nthe potential to substantially impact how teachers plan instruction, support\ndiverse learners, and engage in professional reflection. Yet little is known\nabout how educators actually use these tools in practice and how their\ninteractions with AI can be meaningfully studied at scale. This paper presents\na human-AI collaborative methodology for large-scale qualitative analysis of\nover 140,000 educator-AI messages drawn from a generative AI platform used by\nK-12 teachers. Through a four-phase coding pipeline, we combined inductive\ntheme discovery, codebook development, structured annotation, and model\nbenchmarking to examine patterns of educator engagement and evaluate the\nperformance of LLMs in qualitative coding tasks. We developed a hierarchical\ncodebook aligned with established teacher evaluation frameworks, capturing\neducators' instructional goals, contextual needs, and pedagogical strategies.\nOur findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably\nsupport theme identification, extend human recognition in complex scenarios,\nand outperform open-weight models in both accuracy and structural reliability.\nThe analysis also reveals substantive patterns in how educators inquire AI to\nenhance instructional practices (79.7 percent of total conversations), create\nor adapt content (76.1 percent), support assessment and feedback loop (46.9\npercent), attend to student needs for tailored instruction (43.3 percent), and\nassist other professional responsibilities (34.2 percent), highlighting\nemerging AI-related competencies that have direct implications for teacher\npreparation and professional development. This study offers a scalable,\ntransparent model for AI-augmented qualitative research and provides\nfoundational insights into the evolving role of generative AI in educational\npractice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17985v2", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-28"}
{"id": "2507.21541", "title": "Sun sensor calibration algorithms: A systematic mapping and survey", "authors": ["Michael Herman", "Olivia J. Pinon Fischer", "Dimitri N. Mavris"], "categories": ["cs.CV", "astro-ph.IM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to Acta Astronautica", "url": "http://arxiv.org/abs/2507.21541v1", "summary": "Attitude sensors determine the spacecraft attitude through the sensing of an\nastronomical object, field or other phenomena. The Sun and fixed stars are the\ntwo primary astronomical sensing objects. Attitude sensors are critical\ncomponents for the survival and knowledge improvement of spacecraft. Of these,\nsun sensors are the most common and important sensor for spacecraft attitude\ndetermination. The sun sensor measures the Sun vector in spacecraft\ncoordinates. The sun sensor calibration process is particularly difficult due\nto the complex nature of the uncertainties involved. The uncertainties are\nsmall, difficult to observe, and vary spatio-temporally over the lifecycle of\nthe sensor. In addition, the sensors are affected by numerous sources of\nuncertainties, including manufacturing, electrical, environmental, and\ninterference sources. This motivates the development of advanced calibration\nalgorithms to minimize uncertainty over the sensor lifecycle and improve\naccuracy. Although modeling and calibration techniques for sun sensors have\nbeen explored extensively in the literature over the past two decades, there is\ncurrently no resource that consolidates and systematically reviews this body of\nwork. The present review proposes a systematic mapping of sun sensor modeling\nand calibration algorithms across a breadth of sensor configurations. It\nspecifically provides a comprehensive survey of each methodology, along with an\nanalysis of research gaps and recommendations for future directions in sun\nsensor modeling and calibration techniques.", "comment": "Submitted to Acta Astronautica", "pdf_url": "http://arxiv.org/pdf/2507.21541v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21091", "title": "The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment", "authors": ["Lenart Motnikar", "Katharina Baum", "Alexander Kagan", "Sarah Spiekermann-Hoff"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Thirty-Third European Conference on Information Systems (ECIS 2025), Amman, Jordan", "url": "http://arxiv.org/abs/2507.21091v1", "summary": "Conversational agents (CAs) based on generative artificial intelligence\nfrequently face challenges ensuring ethical interactions that align with human\nvalues. Current value alignment efforts largely rely on top-down approaches,\nsuch as technical guidelines or legal value principles. However, these methods\ntend to be disconnected from the specific contexts in which CAs operate,\npotentially leading to misalignment with users interests. To address this\nchallenge, we propose a novel, bottom-up approach to value alignment, utilizing\nthe value ontology of the ISO Value-Based Engineering standard for ethical IT\ndesign. We analyse 593 ethically sensitive system outputs identified from\n16,908 conversational logs of a major European employment service CA to\nidentify core values and instances of value misalignment within real-world\ninteractions. The results revealed nine core values and 32 different value\nmisalignments that negatively impacted users. Our findings provide actionable\ninsights for CA providers seeking to address ethical challenges and achieve\nmore context-sensitive value alignment.", "comment": "Thirty-Third European Conference on Information Systems (ECIS 2025),\n  Amman, Jordan", "pdf_url": "http://arxiv.org/pdf/2507.21091v1", "cate": "cs.CY", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2504.21429", "title": "Active Learning of Upward-Closed Sets of Words", "authors": ["Quentin Aristote"], "categories": ["cs.FL", "F.4.3"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures; presented at CALCO 2025", "url": "http://arxiv.org/abs/2504.21429v2", "summary": "We give a new proof of a result from well quasi-order theory on the\ncomputability of bases for upwards-closed sets of words. This new proof is\nbased on Angluin's L* algorithm, that learns an automaton from a minimally\nadequate teacher. This relates in particular two results from the 1980s:\nAngluin's L* algorithm, and a result from Valk and Jantzen on the computability\nof bases for upwards-closed sets of tuples of integers.\n  Along the way, we describe an algorithm for learning quasi-ordered automata\nfrom a minimally adequate teacher, and extend a generalization of Valk and\nJantzen's result, encompassing both words and integers, to finitely generated\nmonoids.", "comment": "13 pages, 2 figures; presented at CALCO 2025", "pdf_url": "http://arxiv.org/pdf/2504.21429v2", "cate": "cs.FL", "date": "2025-04-30", "updated": "2025-07-29"}
{"id": "2507.21748", "title": "evoxels: A differentiable physics framework for voxel-based microstructure simulations", "authors": ["Simon Daubner", "Alexander E. Cohen", "Benjamin Dörich", "Samuel J. Cooper"], "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, structure following JOSS style", "url": "http://arxiv.org/abs/2507.21748v1", "summary": "Materials science inherently spans disciplines: experimentalists use advanced\nmicroscopy to uncover micro- and nanoscale structure, while theorists and\ncomputational scientists develop models that link processing, structure, and\nproperties. Bridging these domains is essential for inverse material design\nwhere you start from desired performance and work backwards to optimal\nmicrostructures and manufacturing routes. Integrating high-resolution imaging\nwith predictive simulations and data-driven optimization accelerates discovery\nand deepens understanding of process-structure-property relationships. The\ndifferentiable physics framework evoxels is based on a fully Pythonic, unified\nvoxel-based approach that integrates segmented 3D microscopy data, physical\nsimulations, inverse modeling, and machine learning.", "comment": "9 pages, 3 figures, structure following JOSS style", "pdf_url": "http://arxiv.org/pdf/2507.21748v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.10082", "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "authors": ["Amit Levy", "Itzik Klein"], "categories": ["cs.RO", "eess.SP"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures", "url": "http://arxiv.org/abs/2507.10082v2", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "comment": "6 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.10082v2", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-29"}
{"id": "2507.21589", "title": "Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems", "authors": ["Bin Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.21589v1", "summary": "Embodied intelligence posits that cognitive capabilities fundamentally emerge\nfrom - and are shaped by - an agent's real-time sensorimotor interactions with\nits environment. Such adaptive behavior inherently requires continuous\ninference under uncertainty. Bayesian statistics offers a principled\nprobabilistic framework to address this challenge by representing knowledge as\nprobability distributions and updating beliefs in response to new evidence. The\ncore computational processes underlying embodied intelligence - including\nperception, action selection, learning, and even higher-level cognition - can\nbe effectively understood and modeled as forms of Bayesian inference. Despite\nthe deep conceptual connection between Bayesian statistics and embodied\nintelligence, Bayesian principles have not been widely or explicitly applied in\ntoday's embodied intelligence systems. In this work, we examine both Bayesian\nand contemporary embodied intelligence approaches through two fundamental\nlenses: search and learning - the two central themes in modern AI, as\nhighlighted in Rich Sutton's influential essay \"The Bitter Lesson\". This\nanalysis sheds light on why Bayesian inference has not played a central role in\nthe development of modern embodied intelligence. At the same time, it reveals\nthat current embodied intelligence systems remain largely confined to\nclosed-physical-world environments, and highlights the potential for Bayesian\nmethods to play a key role in extending these systems toward truly open\nphysical-world embodied intelligence.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.21589v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21433", "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse", "authors": ["Kaiwen Chen", "Xin Tan", "Minchen Yu", "Hong Xu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures, submitted to AAAI 2026", "url": "http://arxiv.org/abs/2507.21433v1", "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.", "comment": "11 pages, 7 figures, submitted to AAAI 2026", "pdf_url": "http://arxiv.org/pdf/2507.21433v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.19490", "title": "RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information", "authors": ["Xinzheng Wu", "Junyi Chen", "Peiyi Wang", "Shunxiang Chen", "Haolan Meng", "Yong Shen"], "categories": ["cs.HC", "cs.CV"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Preprint accepted by ITSC 2025", "url": "http://arxiv.org/abs/2507.19490v2", "summary": "In the research and development (R&D) and verification and validation (V&V)\nphases of autonomous driving decision-making and planning systems, it is\nnecessary to integrate human factors to achieve decision-making and evaluation\nthat align with human cognition. However, most existing datasets primarily\nfocus on vehicle motion states and trajectories, neglecting human-related\ninformation. In addition, current naturalistic driving datasets lack sufficient\nsafety-critical scenarios while simulated datasets suffer from low\nauthenticity. To address these issues, this paper constructs the Risk-Informed\nSubjective Evaluation and Eye-tracking (RISEE) dataset which specifically\ncontains human subjective evaluations and eye-tracking data apart from regular\nnaturalistic driving trajectories. By leveraging the complementary advantages\nof drone-based (high realism and extensive scenario coverage) and\nsimulation-based (high safety and reproducibility) data collection methods, we\nfirst conduct drone-based traffic video recording at a highway ramp merging\narea. After that, the manually selected highly interactive scenarios are\nreconstructed in simulation software, and drivers' first-person view (FPV)\nvideos are generated, which are then viewed and evaluated by recruited\nparticipants. During the video viewing process, participants' eye-tracking data\nis collected. After data processing and filtering, 3567 valid subjective risk\nratings from 101 participants across 179 scenarios are retained, along with\n2045 qualified eye-tracking data segments. The collected data and examples of\nthe generated FPV videos are available in our website.", "comment": "Preprint accepted by ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2507.19490v2", "cate": "cs.HC", "date": "2025-05-29", "updated": "2025-07-29"}
{"id": "2507.21555", "title": "Multi-View Reconstruction with Global Context for 3D Anomaly Detection", "authors": ["Yihan Sun", "Yuqi Cheng", "Yunkang Cao", "Yuxin Zhang", "Weiming Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, IEEE International Conference on Systems, Man, and Cybernetics (IEEE SMC), 2025", "url": "http://arxiv.org/abs/2507.21555v1", "summary": "3D anomaly detection is critical in industrial quality inspection. While\nexisting methods achieve notable progress, their performance degrades in\nhigh-precision 3D anomaly detection due to insufficient global information. To\naddress this, we propose Multi-View Reconstruction (MVR), a method that\nlosslessly converts high-resolution point clouds into multi-view images and\nemploys a reconstruction-based anomaly detection framework to enhance global\ninformation learning. Extensive experiments demonstrate the effectiveness of\nMVR, achieving 89.6\\% object-wise AU-ROC and 95.7\\% point-wise AU-ROC on the\nReal3D-AD benchmark.", "comment": "6 pages, 5 figures, IEEE International Conference on Systems, Man,\n  and Cybernetics (IEEE SMC), 2025", "pdf_url": "http://arxiv.org/pdf/2507.21555v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21100", "title": "A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling", "authors": ["Wei Meng"], "categories": ["cs.CY", "cs.AI", "cs.CV", "05C82, 68T07, 68T05, 62H30", "I.2.10; I.4.8; H.5.1; H.2.8"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      This paper introduces a structurally innovative and mathematically rigorous framework for multimodal tactical reasoning, offering a significant advance in causal inference and graph-based threat recognition under noisy conditions", "url": "http://arxiv.org/abs/2507.21100v1", "summary": "This paper introduces TACTIC-GRAPHS, a system that combines spectral graph\ntheory and multimodal graph neural reasoning for semantic understanding and\nthreat detection in tactical video under high noise and weak structure. The\nframework incorporates spectral embedding, temporal causal edge modeling, and\ndiscriminative path inference across heterogeneous modalities. A semantic-aware\nkeyframe extraction method fuses visual, acoustic, and action cues to construct\ntemporal graphs. Using graph attention and Laplacian spectral mapping, the\nmodel performs cross-modal weighting and causal signal analysis. Experiments on\nTACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal\nalignment and over 85 percent recognition of complete threat chains, with node\nlatency within plus-minus 150 milliseconds. The approach enhances structural\ninterpretability and supports applications in surveillance, defense, and\nintelligent security systems.", "comment": "This paper introduces a structurally innovative and mathematically\n  rigorous framework for multimodal tactical reasoning, offering a significant\n  advance in causal inference and graph-based threat recognition under noisy\n  conditions", "pdf_url": "http://arxiv.org/pdf/2507.21100v1", "cate": "cs.CY", "date": "2025-07-04", "updated": "2025-07-04"}
{"id": "2305.10546", "title": "Games on Graphs: From Logic and Automata to Algorithms", "authors": ["Nathanaël Fijalkow", "C. Aiswarya", "Guy Avni", "Nathalie Bertrand", "Patricia Bouyer", "Romain Brenguier", "Arnaud Carayol", "Antonio Casares", "John Fearnley", "Paul Gastin", "Hugo Gimbert", "Thomas A. Henzinger", "Florian Horn", "Rasmus Ibsen-Jensen", "Nicolas Markey", "Benjamin Monmege", "Petr Novotný", "Pierre Ohlmann", "Mickael Randour", "Ocan Sankur", "Sylvain Schmitz", "Olivier Serre", "Mateusz Skomra", "Nathalie Sznajder", "Pierre Vandenhove"], "categories": ["cs.GT", "cs.FL", "cs.LO"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      621 pages. Coordinator: Nathanaël Fijalkow", "url": "http://arxiv.org/abs/2305.10546v2", "summary": "The objective of this book is to give a comprehensive presentation of the\nresearch field concerned with infinite duration games on graphs. Historically,\nthese game models appeared in the study of automata and logic, and they later\nbecame important for program verification and synthesis. They have many more\napplications, in particular some of the models investigated in this book were\nintroduced and studied in neighbouring research communities such as\noptimisation, reinforcement learning, model theory, and set theory.", "comment": "621 pages. Coordinator: Nathana\\\"el Fijalkow", "pdf_url": "http://arxiv.org/pdf/2305.10546v2", "cate": "cs.GT", "date": "2023-05-17", "updated": "2025-07-29"}
{"id": "2507.21912", "title": "Predict Patient Self-reported Race from Skin Histological Images", "authors": ["Shengjia Chen", "Ruchika Verma", "Kevin Clare", "Jannes Jegminat", "Eugenia Alleva", "Kuan-lin Huang", "Brandon Veremis", "Thomas Fuchs", "Gabriele Campanella"], "categories": ["cs.CV", "cs.CE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to the MICCAI Workshop on Fairness of AI in Medical Imaging (FAIMI), 2025", "url": "http://arxiv.org/abs/2507.21912v2", "summary": "Artificial Intelligence (AI) has demonstrated success in computational\npathology (CPath) for disease detection, biomarker classification, and\nprognosis prediction. However, its potential to learn unintended demographic\nbiases, particularly those related to social determinants of health, remains\nunderstudied. This study investigates whether deep learning models can predict\nself-reported race from digitized dermatopathology slides and identifies\npotential morphological shortcuts. Using a multisite dataset with a racially\ndiverse population, we apply an attention-based mechanism to uncover\nrace-associated morphological features. After evaluating three dataset curation\nstrategies to control for confounding factors, the final experiment showed that\nWhite and Black demographic groups retained high prediction performance (AUC:\n0.799, 0.762), while overall performance dropped to 0.663. Attention analysis\nrevealed the epidermis as a key predictive feature, with significant\nperformance declines when these regions were removed. These findings highlight\nthe need for careful data curation and bias mitigation to ensure equitable AI\ndeployment in pathology. Code available at:\nhttps://github.com/sinai-computational-pathology/CPath_SAIF.", "comment": "Accepted to the MICCAI Workshop on Fairness of AI in Medical Imaging\n  (FAIMI), 2025", "pdf_url": "http://arxiv.org/pdf/2507.21912v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.10543", "title": "MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10543v3", "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the \"MeanFlow Identity\", our policy avoids\nany additional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://github.com/LogSSim/MP1.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10543v3", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-29"}
{"id": "2507.21636", "title": "StaffPro: an LLM Agent for Joint Staffing and Profiling", "authors": ["Alessio Maritan"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21636v1", "summary": "Large language model (LLM) agents integrate pre-trained LLMs with modular\nalgorithmic components and have shown remarkable reasoning and decision-making\nabilities. In this work, we investigate their use for two tightly intertwined\nchallenges in workforce management: staffing, i.e., the assignment and\nscheduling of tasks to workers, which may require team formation; and\nprofiling, i.e., the continuous estimation of workers' skills, preferences, and\nother latent attributes from unstructured data. We cast these problems in a\nformal mathematical framework that links scheduling decisions to latent feature\nestimation, and we introduce StaffPro, an LLM agent that addresses staffing and\nprofiling jointly. Differently from existing staffing solutions, StaffPro\nallows expressing optimization objectives using natural language, accepts\ntextual task descriptions and provides high flexibility. StaffPro interacts\ndirectly with humans by establishing a continuous human-agent feedback loop,\nensuring natural and intuitive use. By analyzing human feedback, our agent\ncontinuously estimates the latent features of workers, realizing life-long\nworker profiling and ensuring optimal staffing performance over time. A\nconsulting firm simulation example demonstrates that StaffPro successfully\nestimates workers' attributes and generates high quality schedules. With its\ninnovative design, StaffPro offers a robust, interpretable, and human-centric\nsolution for automated personnel management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21636v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21437", "title": "PVD-ONet: A Multi-scale Neural Operator Method for Singularly Perturbed Boundary Layer Problems", "authors": ["Tiantian Sun", "Jian Zu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34pages,14figures", "url": "http://arxiv.org/abs/2507.21437v1", "summary": "Physics-informed neural networks and Physics-informed DeepONet excel in\nsolving partial differential equations; however, they often fail to converge\nfor singularly perturbed problems. To address this, we propose two novel\nframeworks, Prandtl-Van Dyke neural network (PVD-Net) and its operator learning\nextension Prandtl-Van Dyke Deep Operator Network (PVD-ONet), which rely solely\non governing equations without data. To address varying task-specific\nrequirements, both PVD-Net and PVD-ONet are developed in two distinct versions,\ntailored respectively for stability-focused and high-accuracy modeling. The\nleading-order PVD-Net adopts a two-network architecture combined with Prandtl's\nmatching condition, targeting stability-prioritized scenarios. The high-order\nPVD-Net employs a five-network design with Van Dyke's matching principle to\ncapture fine-scale boundary layer structures, making it ideal for high-accuracy\nscenarios. PVD-ONet generalizes PVD-Net to the operator learning setting by\nassembling multiple DeepONet modules, directly mapping initial conditions to\nsolution operators and enabling instant predictions for an entire family of\nboundary layer problems without retraining. Numerical experiments on various\nmodels show that our proposed methods consistently outperform existing\nbaselines under various error metrics, thereby offering a powerful new approach\nfor multi-scale problems.", "comment": "34pages,14figures", "pdf_url": "http://arxiv.org/pdf/2507.21437v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20355", "title": "CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration", "authors": ["Zheng Wei", "Hongtao Wu", "lvmin Zhang", "Xian Xu", "Yefeng Zheng", "Pan Hui", "Maneesh Agrawala", "Huamin Qu", "Anyi Rao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      UIST 2025", "url": "http://arxiv.org/abs/2507.20355v2", "summary": "Effective communication between directors and cinematographers is fundamental\nin film production, yet traditional approaches relying on visual references and\nhand-drawn storyboards often lack the efficiency and precision necessary during\npre-production. We present CineVision, an AI-driven platform that integrates\nscriptwriting with real-time visual pre-visualization to bridge this\ncommunication gap. By offering dynamic lighting control, style emulation based\non renowned filmmakers, and customizable character design, CineVision enables\ndirectors to convey their creative vision with heightened clarity and rapidly\niterate on scene composition. In a 24-participant lab study, CineVision yielded\nshorter task times and higher usability ratings than two baseline methods,\nsuggesting a potential to ease early-stage communication and accelerate\nstoryboard drafts under controlled conditions. These findings underscore\nCineVision's potential to streamline pre-production processes and foster deeper\ncreative synergy among filmmaking teams, particularly for new collaborators.\nOur code and demo are available at https://github.com/TonyHongtaoWu/CineVision.", "comment": "UIST 2025", "pdf_url": "http://arxiv.org/pdf/2507.20355v2", "cate": "cs.HC", "date": "2025-07-27", "updated": "2025-07-29"}
{"id": "2507.21567", "title": "RelMap: Enhancing Online Map Construction with Class-Aware Spatial Relation and Semantic Priors", "authors": ["Tianhui Cai", "Yun Zhang", "Zewei Zhou", "Zhiyu Huang", "Jiaqi Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21567v1", "summary": "Online high-definition (HD) map construction plays an increasingly important\nrole in scaling autonomous driving systems. Transformer-based methods have\nbecome prevalent in online HD map construction; however, existing approaches\noften neglect the inherent spatial and semantic relationships among map\nelements, which limits their accuracy and generalization. To address this, we\npropose RelMap, an end-to-end framework that enhances online map construction\nby incorporating spatial relations and semantic priors. We introduce a\nClass-aware Spatial Relation Prior, which explicitly encodes relative\npositional dependencies between map elements using a learnable class-aware\nrelation encoder. Additionally, we propose a Mixture-of-Experts (MoE)-based\nSemantic Prior, which routes features to class-specific experts based on\npredicted class probabilities, refining instance feature decoding. Our method\nis compatible with both single-frame and temporal perception backbones,\nachieving state-of-the-art performance on both the nuScenes and Argoverse 2\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21567v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21102", "title": "Assessing the Ecological Impact of AI", "authors": ["Sylvia Wenmackers"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      This was presented as a lightning talk at: LOCO 2024, December 3, 2024, Glasgow/Online", "url": "http://arxiv.org/abs/2507.21102v1", "summary": "Philosophers of technology have recently started paying more attention to the\nenvironmental impacts of AI, in particular of large language models (LLMs) and\ngenerative AI (genAI) applications. Meanwhile, few developers of AI give\nconcrete estimates of the ecological impact of their models and products, and\neven when they do so, their analysis is often limited to green house gas\nemissions of certain stages of AI development or use. The current proposal\nencourages practically viable analyses of the sustainability aspects of genAI\ninformed by philosophical ideas.", "comment": "This was presented as a lightning talk at: LOCO 2024, December 3,\n  2024, Glasgow/Online", "pdf_url": "http://arxiv.org/pdf/2507.21102v1", "cate": "cs.CY", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2502.12844", "title": "Generalized De Bruijn Words, Invertible Necklaces, and the Burrows-Wheeler Transform", "authors": ["Gabriele Fici", "Estéban Gabory"], "categories": ["math.CO", "cs.DM", "cs.DS", "cs.FL"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      Submitted", "url": "http://arxiv.org/abs/2502.12844v3", "summary": "We define generalized de Bruijn words as those words having a Burrows-Wheeler\ntransform that is a concatenation of permutations of the alphabet. We show that\ngeneralized de Bruijn words are in 1-to-1 correspondence with Hamiltonian\ncycles in the generalized de Bruijn graphs introduced in the early '80s in the\ncontext of network design. When the size of the alphabet is a prime $p$, we\ndefine invertible necklaces as those whose BWT-matrix is non-singular. We show\nthat invertible necklaces of length $n$ correspond to normal bases of the\nfinite field $F_{p^n}$, and that they form an Abelian group isomorphic to the\nReutenauer group $RG_p^n$. Using known results in abstract algebra, we can make\na bridge between generalized de Bruijn words and invertible necklaces. In\nparticular, we highlight a correspondence between binary de Bruijn words of\norder $d+1$, binary necklaces of length $2^{d}$ having an odd number of $1$'s,\ninvertible BWT matrices of size $2^{d}\\times 2^{d}$, and normal bases of the\nfinite field $F_{2^{2^{d}}}$.", "comment": "Submitted", "pdf_url": "http://arxiv.org/pdf/2502.12844v3", "cate": "math.CO", "date": "2025-02-18", "updated": "2025-07-29"}
{"id": "2507.19406", "title": "Falling through the cracks: energy storage along segmented brittle crack fronts", "authors": ["Xinyue Wei", "John M. Kolinski"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19406v2", "summary": "During brittle crack propagation, a smooth crack front curve frequently\nbecomes disjoint, generating a stepped crack and a material ligament that\nunites the newly formed crack fronts. These universal features fundamentally\nalter the singular field structure and stability of propagating cracks;\nhowever, a quantitative analysis of their mechanics is lacking. Here, we\nperform in-situ 3D measurements to resolve the deformation field around stepped\ncracks, and crucially, within the ligament feature. The 3D kinematic data are\nobtained by scanning a thin laser sheet through the brittle hydrogel samples,\nwhile recording the scattered intensity from the embedded tracer particles. We\nfind that the ligament concentrates the strain energy density, and moreover,\nthe apparent fracture energy increases proportionally to the strain energy\nwithin the ligament.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19406v2", "cate": "cs.CE", "date": "2025-07-25", "updated": "2025-07-29"}
{"id": "2507.21300", "title": "Simultaneous improvement of control and estimation for battery management systems", "authors": ["Mohammad S. Ramadan", "Marfred Barrera", "Mihai Anitescu", "Sylvia Herbert"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21300v1", "summary": "The state of charge of battery systems is an important metric typically\nestimated by observation models, represented by open-circuit voltage graphs.\nThese observation models are often nonlinear in the state of charge, resulting\nin varying observability from a state estimation perspective. In this paper, we\nemploy a stochastic optimal control (also known as dual control) approach to\nsimultaneously satisfy the control objective in the state of charge of battery\nsystems and improve estimation accuracy. This is achieved implicitly by\nprioritizing trajectories that pass through high-observability regions of the\nstate space, thereby improving the quality of future measurements. We apply our\nalgorithm to a numerical simulation of a multi-battery system and show a\nstatistical improvement in both the control objective and the state estimation\nerror.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21300v1", "cate": "eess.SY", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.16175", "title": "Scanning Bot: Efficient Scan Planning using Panoramic Cameras", "authors": ["Euijeong Lee", "Kyung Min Han", "Young J. Kim"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16175v2", "summary": "Panoramic RGB-D cameras are known for their ability to produce high quality\n3D scene reconstructions. However, operating these cameras involves manually\nselecting viewpoints and physically transporting the camera, making the\ngeneration of a 3D model time consuming and tedious. Additionally, the process\ncan be challenging for novice users due to spatial constraints, such as\nensuring sufficient feature overlap between viewpoint frames. To address these\nchallenges, we propose a fully autonomous scan planning that generates an\nefficient tour plan for environment scanning, ensuring collision-free\nnavigation and adequate overlap between viewpoints within the plan. Extensive\nexperiments conducted in both synthetic and real-world environments validate\nthe performance of our planner against state-of-the-art view planners. In\nparticular, our method achieved an average scan coverage of 99 percent in the\nreal-world experiment, with our approach being up to 3 times faster than\nstate-of-the-art planners in total scan time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16175v2", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-29"}
{"id": "2507.21637", "title": "Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models", "authors": ["Wanying Wang", "Zeyu Ma", "Han Zheng", "Xin Tan", "Mingang Chen"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.21637v1", "summary": "Large vision-language models (LVLMs) are vulnerable to harmful input compared\nto their language-only backbones. We investigated this vulnerability by\nexploring LVLMs internal dynamics, framing their inherent safety understanding\nin terms of three key capabilities. Specifically, we define these capabilities\nas safety perception, semantic understanding, and alignment for linguistic\nexpression, and experimentally pinpointed their primary locations within the\nmodel architecture. The results indicate that safety perception often emerges\nbefore comprehensive semantic understanding, leading to the reduction in\nsafety. Motivated by these findings, we propose \\textbf{Self-Aware Safety\nAugmentation (SASA)}, a technique that projects informative semantic\nrepresentations from intermediate layers onto earlier safety-oriented layers.\nThis approach leverages the model's inherent semantic understanding to enhance\nsafety recognition without fine-tuning. Then, we employ linear probing to\narticulate the model's internal semantic comprehension to detect the risk\nbefore the generation process. Extensive experiments on various datasets and\ntasks demonstrate that SASA significantly improves the safety of LVLMs, with\nminimal impact on the utility.", "comment": "Accepted by ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.21637v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21494", "title": "Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning", "authors": ["Wenxuan Bao", "Ruxi Deng", "Ruizhong Qiu", "Tianxin Wei", "Hanghang Tong", "Jingrui He"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.21494v1", "summary": "Test-time adaptation with pre-trained vision-language models has gained\nincreasing attention for addressing distribution shifts during testing. Among\nthese approaches, memory-based algorithms stand out due to their training-free\nnature and ability to leverage historical test data. However, existing\ntest-time adaptation methods are typically designed for a single domain with\nabundant data. In decentralized settings such as federated learning, applying\nthese methods individually to each client suffers from limited test data, while\ndirectly sharing a single global memory via the server prevents proper\npersonalization to each client's unique distribution. To address this, we\npropose Latte, a novel framework where each client maintains a local memory to\nstore embeddings from its own historical test data and an external memory to\nstore class prototypes from other relevant clients. During communication, each\nclient retrieves prototypes from similar clients under the server's\ncoordination to expand its memory. For local adaptation, Latte utilizes both\nembedding similarity and uncertainty to enhance model performance. Our\ntheoretical analysis shows that Latte effectively leverages in-distribution\nclients while remaining robust to out-of-distribution clients. Extensive\nexperiments on domain adaptation and corruption benchmarks validate that Latte\nachieves superior performance in decentralized settings, while introducing only\nnegligible communication and computation costs. Our code is available at\nhttps://github.com/baowenxuan/Latte .", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.21494v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.05331", "title": "Not someone, but something: Rethinking trust in the age of medical AI", "authors": ["Jan Beger"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05331v3", "summary": "As artificial intelligence (AI) becomes embedded in healthcare, trust in\nmedical decision-making is changing fast. Nowhere is this shift more visible\nthan in radiology, where AI tools are increasingly embedded across the imaging\nworkflow - from scheduling and acquisition to interpretation, reporting, and\ncommunication with referrers and patients. This opinion paper argues that trust\nin AI isn't a simple transfer from humans to machines - it is a dynamic,\nevolving relationship that must be built and maintained. Rather than debating\nwhether AI belongs in medicine, it asks: what kind of trust must AI earn, and\nhow? Drawing from philosophy, bioethics, and system design, it explores the key\ndifferences between human trust and machine reliability - emphasizing\ntransparency, accountability, and alignment with the values of good care. It\nargues that trust in AI should not be built on mimicking empathy or intuition,\nbut on thoughtful design, responsible deployment, and clear moral\nresponsibility. The goal is a balanced view - one that avoids blind optimism\nand reflexive fear. Trust in AI must be treated not as a given, but as\nsomething to be earned over time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05331v3", "cate": "cs.CY", "date": "2025-04-04", "updated": "2025-07-29"}
{"id": "2507.21573", "title": "LinDeps: A Fine-tuning Free Post-Pruning Method to Remove Layer-Wise Linear Dependencies with Guaranteed Performance Preservation", "authors": ["Maxim Henry", "Adrien Deliège", "Anthony Cioppa", "Marc Van Droogenbroeck"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, 5 tables, 45 references", "url": "http://arxiv.org/abs/2507.21573v1", "summary": "Convolutional Neural Networks (CNN) are widely used in many computer vision\ntasks. Yet, their increasing size and complexity pose significant challenges\nfor efficient deployment on resource-constrained platforms. Hence, network\npruning has emerged as an effective way of reducing the size and computational\nrequirements of neural networks by removing redundant or unimportant\nparameters. However, a fundamental challenge with pruning consists in optimally\nremoving redundancies without degrading performance. Most existing pruning\ntechniques overlook structural dependencies across feature maps within a layer,\nresulting in suboptimal pruning decisions. In this work, we introduce LinDeps,\na novel post-pruning method, i.e., a pruning method that can be applied on top\nof any pruning technique, which systematically identifies and removes redundant\nfilters via linear dependency analysis. Particularly, LinDeps applies pivoted\nQR decomposition to feature maps to detect and prune linearly dependent\nfilters. Then, a novel signal recovery mechanism adjusts the next layer's\nkernels to preserve compatibility and performance without requiring any\nfine-tuning. Our experiments on CIFAR-10 and ImageNet with VGG and ResNet\nbackbones demonstrate that LinDeps improves compression rates of existing\npruning techniques while preserving performances, leading to a new state of the\nart in CNN pruning. We also benchmark LinDeps in low-resource setups where no\nretraining can be performed, which shows significant pruning improvements and\ninference speedups over a state-of-the-art method. LinDeps therefore\nconstitutes an essential add-on for any current or future pruning technique.", "comment": "10 pages, 4 figures, 5 tables, 45 references", "pdf_url": "http://arxiv.org/pdf/2507.21573v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21118", "title": "Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach", "authors": ["Anass El Ayady", "Maxime Devanne", "Germain Forestier", "Nour El Mawas"], "categories": ["cs.CY", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      in French language, Environnements Informatiques pour l'Apprentissage Humain 2025, Jun 2025, Villeneuve d'Ascq (Lille), France", "url": "http://arxiv.org/abs/2507.21118v1", "summary": "MOOCs offer free and open access to a wide audience, but completion rates\nremain low, often due to a lack of personalized content. To address this issue,\nit is essential to predict learner performance in order to provide tailored\nfeedback. Behavioral traces-such as clicks and events-can be analyzed as time\nseries to anticipate learners' outcomes. This work compares multivariate time\nseries classification methods to identify at-risk learners at different stages\nof the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted\non the Open University Learning Analytics Dataset (OULAD), focuses on three\ncourses: two in STEM and one in SHS. Preliminary results show that the\nevaluated approaches are promising for predicting learner failure in MOOCs. The\nanalysis also suggests that prediction accuracy is influenced by the amount of\nrecorded interactions, highlighting the importance of rich and diverse\nbehavioral data.", "comment": "in French language, Environnements Informatiques pour l'Apprentissage\n  Humain 2025, Jun 2025, Villeneuve d'Ascq (Lille), France", "pdf_url": "http://arxiv.org/pdf/2507.21118v1", "cate": "cs.CY", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2504.08575", "title": "Prophecies all the Way: Game-based Model-Checking for HyperQPTL beyond $\\forall^*\\exists^*$", "authors": ["Sarah Winter", "Martin Zimmermann"], "categories": ["cs.LO", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08575v3", "summary": "Model-checking HyperLTL, a temporal logic expressing properties of sets of\ntraces with applications to information-flow based security and privacy, has a\ndecidable, but TOWER-complete, model-checking problem. While the classical\nmodel-checking algorithm for full HyperLTL is automata-theoretic, more\nrecently, a game-based alternative for the $\\forall^*\\exists^*$-fragment has\nbeen presented.\n  Here, we employ imperfect information-games to extend the game-based approach\nto full HyperQPTL, which features arbitrary quantifier prefixes and\nquantification over propositions and can express every $\\omega$-regular\nhyperproperty. As a byproduct of our game-based algorithm, we obtain\nfinite-state implementations of Skolem functions via transducers with lookahead\nthat explain satisfaction or violation of HyperQPTL properties.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08575v3", "cate": "cs.LO", "date": "2025-04-11", "updated": "2025-07-29"}
{"id": "2507.21441", "title": "Ensemble Control of Stochastic Oscillators via Periodic and Feedback Control", "authors": ["Kaito Ito", "Haruhiro Kume", "Hideaki Ishii"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.21441v1", "summary": "We address the problem of steering the phase distribution of oscillators all\nreceiving the same control input to a given target distribution. In a large\npopulation limit, the distribution of oscillators can be described by a\nprobability density. Then, our problem can be seen as that of ensemble control\nwith a constraint on the steady-state density. In particular, we consider the\ncase where oscillators are subject to stochastic noise, for which the\ntheoretical understanding is still lacking. First, we characterize the\nreachability of the phase distribution under periodic feedforward control via\nthe Fourier coefficients of the target density and the phase sensitivity\nfunction of oscillators. This enables us to design a periodic input that makes\nthe stationary distribution of oscillators closest to the target by solving a\nconvex optimization problem. Next, we devise an ensemble control method\ncombining periodic and feedback control, where the feedback component is\ndesigned to accelerate the convergence of the distribution of oscillators. We\nexhibit some convergence results for the proposed method, including a result\nthat holds even under measurement errors in the phase distribution. The\neffectiveness of the proposed method is demonstrated by a numerical example.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.21441v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21347", "title": "DOA Estimation via Continuous Aperture Arrays: MUSIC and CRLB", "authors": ["Haonan Si", "Zhaolin Wang", "Xiansheng Guo", "Jin Zhang", "Yuanwei Liu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Submit to possible IEEE journal", "url": "http://arxiv.org/abs/2507.21347v1", "summary": "Direction-of-arrival (DOA) estimation using continuous aperture array (CAPA)\nis studied. Compared to the conventional spatially discrete array (SPDA), CAPA\nsignificantly enhances the spatial degrees-of-freedoms (DoFs) for DOA\nestimation, but its infinite-dimensional continuous signals render the\nconventional estimation algorithm non-applicable. To address this challenge, a\nnew multiple signal classification (MUSIC) algorithm is proposed for CAPAs. In\nparticular, an equivalent continuous-discrete transformation is proposed to\nfacilitate the eigendecomposition of continuous operators. Subsequently, the\nMUSIC spectrum is accurately approximated using the Gauss-Legendre quadrature,\neffectively reducing the computational complexity. Furthermore, the\nCram\\'er-Rao lower bounds (CRLBs) for DOA estimation using CAPAs are analyzed\nfor both cases with and without priori knowledge of snapshot signals. It is\ntheoretically proved that CAPAs significantly improve the DOA estimation\naccuracy compared to traditional SPDAs. Numerical results further validate this\ninsight and demonstrate the effectiveness of the proposed MUSIC algorithm for\nCAPA. The proposed method achieves near-optimal estimation performance while\nmaintaining a low computational complexity.", "comment": "Submit to possible IEEE journal", "pdf_url": "http://arxiv.org/pdf/2507.21347v1", "cate": "eess.SP", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.17294", "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback", "authors": ["Jianxin Bi", "Kevin Yuchen Ma", "Ce Hao", "Mike Zheng Shou", "Harold Soh"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      19 pages, 5 figures", "url": "http://arxiv.org/abs/2507.17294v2", "summary": "Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.17294v2", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-29"}
{"id": "2507.21705", "title": "Unrolling Dynamic Programming via Graph Filters", "authors": ["Sergio Rozada", "Samuel Rey", "Gonzalo Mateos", "Antonio G. Marques"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21705v1", "summary": "Dynamic programming (DP) is a fundamental tool used across many engineering\nfields. The main goal of DP is to solve Bellman's optimality equations for a\ngiven Markov decision process (MDP). Standard methods like policy iteration\nexploit the fixed-point nature of these equations to solve them iteratively.\nHowever, these algorithms can be computationally expensive when the\nstate-action space is large or when the problem involves long-term\ndependencies. Here we propose a new approach that unrolls and truncates policy\niterations into a learnable parametric model dubbed BellNet, which we train to\nminimize the so-termed Bellman error from random value function\ninitializations. Viewing the transition probability matrix of the MDP as the\nadjacency of a weighted directed graph, we draw insights from graph signal\nprocessing to interpret (and compactly re-parameterize) BellNet as a cascade of\nnonlinear graph filters. This fresh look facilitates a concise, transferable,\nand unifying representation of policy and value iteration, with an explicit\nhandle on complexity during inference. Preliminary experiments conducted in a\ngrid-like environment demonstrate that BellNet can effectively approximate\noptimal policies in a fraction of the iterations required by classical methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21705v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21504", "title": "Evaluation and Benchmarking of LLM Agents: A Survey", "authors": ["Mahmoud Mohammadi", "Yipeng Li", "Jane Lo", "Wendy Yip"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21504v1", "summary": "The rise of LLM-based agents has opened new frontiers in AI applications, yet\nevaluating these agents remains a complex and underdeveloped area. This survey\nprovides an in-depth overview of the emerging field of LLM agent evaluation,\nintroducing a two-dimensional taxonomy that organizes existing work along (1)\nevaluation objectives -- what to evaluate, such as agent behavior,\ncapabilities, reliability, and safety -- and (2) evaluation process -- how to\nevaluate, including interaction modes, datasets and benchmarks, metric\ncomputation methods, and tooling. In addition to taxonomy, we highlight\nenterprise-specific challenges, such as role-based access to data, the need for\nreliability guarantees, dynamic and long-horizon interactions, and compliance,\nwhich are often overlooked in current research. We also identify future\nresearch directions, including holistic, more realistic, and scalable\nevaluation. This work aims to bring clarity to the fragmented landscape of\nagent evaluation and provide a framework for systematic assessment, enabling\nresearchers and practitioners to evaluate LLM agents for real-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21504v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2505.04066", "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants", "authors": ["Tuochao Chen", "Nicholas Batchelder", "Alisa Liu", "Noah Smith", "Shyamnath Gollakota"], "categories": ["cs.LG", "cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published by ACL2025 (Findings)", "url": "http://arxiv.org/abs/2505.04066v2", "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations.", "comment": "Published by ACL2025 (Findings)", "pdf_url": "http://arxiv.org/pdf/2505.04066v2", "cate": "cs.LG", "date": "2025-05-07", "updated": "2025-07-29"}
{"id": "2507.21584", "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs", "authors": ["Kejia Zhang", "Keda Tao", "Zhiming Luo", "Chang Liu", "Jiasheng Tang", "Huan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21584v1", "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21584v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21169", "title": "Trustworthy AI: UK Air Traffic Control Revisited", "authors": ["Rob Procter", "Mark Rouncefield"], "categories": ["cs.CY", "cs.AI", "I.2.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.21169v1", "summary": "Exploring the socio-technical challenges confronting the adoption of AI in\norganisational settings is something that has so far been largely absent from\nthe related literature. In particular, research into requirements for\ntrustworthy AI typically overlooks how people deal with the problems of trust\nin the tools that they use as part of their everyday work practices. This\narticle presents some findings from an ongoing ethnographic study of how\ncurrent tools are used in air traffic control work and what it reveals about\nrequirements for trustworthy AI in air traffic control and other\nsafety-critical application domains.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.21169v1", "cate": "cs.CY", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21579", "title": "Experimental Implementation and Validation of Predictor-Based CACC for Vehicular Platoons With Distinct Actuation Delays", "authors": ["Amirhossein Samii", "Redmer de Haan", "Nikolaos Bekiaris-Liberis"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at the IEEE Conference on Decision and Control (CDC) 2025. 8 pages", "url": "http://arxiv.org/abs/2507.21579v1", "summary": "We provide experimental validation, in a pair of vehicles, of a recently\nintroduced predictor-based cooperative adaptive cruise control (CACC) design,\ndeveloped for achieving delay compensation in heterogeneous vehicular platoons\nsubject to long actuation delays that may be distinct for each individual\nvehicle. We provide the explicit formulae of the control design that is\nimplemented, accounting for the effect of zero-order hold and sampled\nmeasurements; as well as we obtain vehicle and string stability conditions\nnumerically, via derivation of the transfer functions relating the speeds of\npairs of consecutive vehicles. We also present consistent simulation results\nfor a platoon with a larger number of vehicles, under digital implementation of\nthe controller. Both the simulation and experimental results confirm the\neffectiveness of the predictor-based CACC design in guaranteeing individual\nvehicle stability, string stability, and tracking, despite long/distinct\nactuation delays.", "comment": "Accepted for presentation at the IEEE Conference on Decision and\n  Control (CDC) 2025. 8 pages", "pdf_url": "http://arxiv.org/pdf/2507.21579v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21454", "title": "Transmission With Machine Language Tokens: A Paradigm for Task-Oriented Agent Communication", "authors": ["Zhuoran Xiao", "Chenhui Ye", "Yijia Feng", "Yunbo Hu", "Tianyu Jiao", "Liyu Cai", "Guangyi Liu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Globecom 2025", "url": "http://arxiv.org/abs/2507.21454v1", "summary": "The rapid advancement in large foundation models is propelling the paradigm\nshifts across various industries. One significant change is that agents,\ninstead of traditional machines or humans, will be the primary participants in\nthe future production process, which consequently requires a novel AI-native\ncommunication system tailored for agent communications. Integrating the ability\nof large language models (LLMs) with task-oriented semantic communication is a\npotential approach. However, the output of existing LLM is human language,\nwhich is highly constrained and sub-optimal for agent-type communication. In\nthis paper, we innovatively propose a task-oriented agent communication system.\nSpecifically, we leverage the original LLM to learn a specialized machine\nlanguage represented by token embeddings. Simultaneously, a multi-modal LLM is\ntrained to comprehend the application task and to extract essential implicit\ninformation from multi-modal inputs, subsequently expressing it using machine\nlanguage tokens. This representation is significantly more efficient for\ntransmission over the air interface. Furthermore, to reduce transmission\noverhead, we introduce a joint token and channel coding (JTCC) scheme that\ncompresses the token sequence by exploiting its sparsity while enhancing\nrobustness against channel noise. Extensive experiments demonstrate that our\napproach reduces transmission overhead for downstream tasks while enhancing\naccuracy relative to the SOTA methods.", "comment": "Accepted by IEEE Globecom 2025", "pdf_url": "http://arxiv.org/pdf/2507.21454v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20217", "title": "Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots", "authors": ["Wei Cui", "Haoyu Wang", "Wenkang Qin", "Yijie Guo", "Gang Han", "Wen Zhao", "Jiahang Cao", "Zhang Zhang", "Jiaru Zhong", "Jingkai Sun", "Pihai Sun", "Shuai Shi", "Botuo Jiang", "Jiahao Ma", "Jiaxu Wang", "Hao Cheng", "Zhichao Liu", "Yang Wang", "Zheng Zhu", "Guan Huang", "Jian Tang", "Qiang Zhang"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Tech Report", "url": "http://arxiv.org/abs/2507.20217v2", "summary": "Humanoid robot technology is advancing rapidly, with manufacturers\nintroducing diverse heterogeneous visual perception modules tailored to\nspecific scenarios. Among various perception paradigms, occupancy-based\nrepresentation has become widely recognized as particularly suitable for\nhumanoid robots, as it provides both rich semantic and 3D geometric information\nessential for comprehensive environmental understanding. In this work, we\npresent Humanoid Occupancy, a generalized multimodal occupancy perception\nsystem that integrates hardware and software components, data acquisition\ndevices, and a dedicated annotation pipeline. Our framework employs advanced\nmulti-modal fusion techniques to generate grid-based occupancy outputs encoding\nboth occupancy status and semantic labels, thereby enabling holistic\nenvironmental understanding for downstream tasks such as task planning and\nnavigation. To address the unique challenges of humanoid robots, we overcome\nissues such as kinematic interference and occlusion, and establish an effective\nsensor layout strategy. Furthermore, we have developed the first panoramic\noccupancy dataset specifically for humanoid robots, offering a valuable\nbenchmark and resource for future research and development in this domain. The\nnetwork architecture incorporates multi-modal feature fusion and temporal\ninformation integration to ensure robust perception. Overall, Humanoid\nOccupancy delivers effective environmental perception for humanoid robots and\nestablishes a technical foundation for standardizing universal visual modules,\npaving the way for the widespread deployment of humanoid robots in complex\nreal-world scenarios.", "comment": "Tech Report", "pdf_url": "http://arxiv.org/pdf/2507.20217v2", "cate": "cs.RO", "date": "2025-07-27", "updated": "2025-07-29"}
{"id": "2507.21727", "title": "GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation", "authors": ["Jianfei Zhu", "Haiqi Zhu", "Shaohui Liu", "Feng Jiang", "Baichun Wei", "Chunzhi Yi"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21727v1", "summary": "Recent deep learning approaches have shown promise in learning such\nindividual brain parcellations from functional magnetic resonance imaging\n(fMRI). However, most existing methods assume consistent data distributions\nacross domains and struggle with domain shifts inherent to real-world\ncross-dataset scenarios. To address this challenge, we proposed Graph Domain\nAdaptation for Individual Parcellation (GDAIP), a novel framework that\nintegrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based\ndomain adaptation. We construct cross-dataset brain graphs at both the group\nand individual levels. By leveraging semi-supervised training and adversarial\noptimization of the prediction entropy on unlabeled vertices from target brain\ngraph, the reference atlas is adapted from the group-level brain graph to the\nindividual brain graph, enabling individual parcellation under cross-dataset\nsettings. We evaluated our method using parcellation visualization, Dice\ncoefficient, and functional homogeneity. Experimental results demonstrate that\nGDAIP produces individual parcellations with topologically plausible\nboundaries, strong cross-session consistency, and ability of reflecting\nfunctional organization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21727v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21531", "title": "Hierarchical Stochastic Differential Equation Models for Latent Manifold Learning in Neural Time Series", "authors": ["Pedram Rajaei", "Maryam Ostadsharif Memar", "Navid Ziaei", "Behzad Nazari", "Ali Yousefi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21531v1", "summary": "The manifold hypothesis suggests that high-dimensional neural time series lie\non a low-dimensional manifold shaped by simpler underlying dynamics. To uncover\nthis structure, latent dynamical variable models such as state-space models,\nrecurrent neural networks, neural ordinary differential equations, and Gaussian\nProcess Latent Variable Models are widely used. We propose a novel hierarchical\nstochastic differential equation (SDE) model that balances computational\nefficiency and interpretability, addressing key limitations of existing\nmethods. Our model assumes the trajectory of a manifold can be reconstructed\nfrom a sparse set of samples from the manifold trajectory. The latent space is\nmodeled using Brownian bridge SDEs, with points - specified in both time and\nvalue - sampled from a multivariate marked point process. These Brownian\nbridges define the drift of a second set of SDEs, which are then mapped to the\nobserved data. This yields a continuous, differentiable latent process capable\nof modeling arbitrarily complex time series as the number of manifold points\nincreases. We derive training and inference procedures and show that the\ncomputational cost of inference scales linearly with the length of the\nobservation data. We then validate our model on both synthetic data and neural\nrecordings to demonstrate that it accurately recovers the underlying manifold\nstructure and scales effectively with data dimensionality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21531v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.19855", "title": "Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning", "authors": ["Aditya Sharma", "Linh Nguyen", "Ananya Gupta", "Chengyu Wang", "Chiamaka Adebayo", "Jakub Kowalski"], "categories": ["cs.LG", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures,", "url": "http://arxiv.org/abs/2507.19855v2", "summary": "Large Language Models (LLMs), despite their advanced linguistic capabilities,\nfundamentally lack an intuitive understanding of physical dynamics, which\nlimits their effectiveness in real-world scenarios that require causal\nreasoning. In this paper, we introduce Causal World Model Induction (CWMI), a\nnovel framework designed to embed an explicit model of causal physics within an\nLLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a\nnew training objective called Causal Intervention Loss, encouraging the model\nto learn cause-and-effect relationships from multimodal data. By training the\nmodel to predict the outcomes of hypothetical interventions instead of merely\ncapturing statistical correlations, CWMI develops a robust internal\nrepresentation of physical laws. Experimental results show that CWMI\nsignificantly outperforms state-of-the-art LLMs on zero-shot physical reasoning\ntasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench\ndataset. These findings demonstrate that inducing a causal world model is a\ncritical step toward more reliable and generalizable AI systems.", "comment": "12 pages, 4 figures,", "pdf_url": "http://arxiv.org/pdf/2507.19855v2", "cate": "cs.LG", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2507.21587", "title": "Emerging Trends in Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation with Image-Level Supervision", "authors": ["Zheyuan Zhang", "Wang Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21587v1", "summary": "Unlike fully supervised semantic segmentation, weakly supervised semantic\nsegmentation (WSSS) relies on weaker forms of supervision to perform dense\nprediction tasks. Among the various types of weak supervision, WSSS with image\nlevel annotations is considered both the most challenging and the most\npractical, attracting significant research attention. Therefore, in this\nreview, we focus on WSSS with image level annotations. Additionally, this\nreview concentrates on mainstream research directions, deliberately omitting\nless influential branches.\n  Given the rapid development of new methods and the limitations of existing\nsurveys in capturing recent trends, there is a pressing need for an updated and\ncomprehensive review. Our goal is to fill this gap by synthesizing the latest\nadvancements and state-of-the-art techniques in WSSS with image level labels.\n  Basically, we provide a comprehensive review of recent advancements in WSSS\nwith image level labels, categorizing existing methods based on the types and\nlevels of additional supervision involved. We also examine the challenges of\napplying advanced methods to domain specific datasets in WSSS,a topic that\nremains underexplored. Finally, we discuss the current challenges, evaluate the\nlimitations of existing approaches, and outline several promising directions\nfor future research. This review is intended for researchers who are already\nfamiliar with the fundamental concepts of WSSS and are seeking to deepen their\nunderstanding of current advances and methodological innovations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21587v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21174", "title": "A ChatGPT-based approach for questions generation in higher education", "authors": ["Sinh Trong Vu", "Huong Thu Truong", "Oanh Tien Do", "Tu Anh Le", "Tai Tan Mai"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21174v2", "summary": "Large language models have been widely applied in many aspects of real life,\nbringing significant efficiency to businesses and offering distinctive user\nexperiences. In this paper, we focus on exploring the application of ChatGPT, a\nchatbot based on a large language model, to support higher educator in\ngenerating quiz questions and assessing learners. Specifically, we explore\ninteractive prompting patterns to design an optimal AI-powered question bank\ncreation process. The generated questions are evaluated through a \"Blind test\"\nsurvey sent to various stakeholders including lecturers and learners. Initial\nresults at the Banking Academy of Vietnam are relatively promising, suggesting\na potential direction to streamline the time and effort involved in assessing\nlearners at higher education institutes.", "comment": "Proceedings of the 1st ACM Workshop on AI-Powered Q&A Systems for\n  Multimedia. 2024", "pdf_url": "http://arxiv.org/pdf/2507.21174v2", "cate": "cs.CY", "date": "2025-07-26", "updated": "2025-07-30"}
{"id": "2507.21667", "title": "Deep Neuro-Adaptive Sliding Mode Controller for Higher-Order Heterogeneous Nonlinear Multi-Agent Teams with Leader", "authors": ["Khushal Chaudhari", "Krishanu Nath", "Manas Kumar Bera"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21667v1", "summary": "This letter proposes a deep neural network (DNN)-based neuro-adaptive sliding\nmode control (SMC) strategy for leader-follower tracking in multi-agent systems\nwith higher-order, heterogeneous, nonlinear, and unknown dynamics under\nexternal disturbances. The DNN is used to compensate the unknown nonlinear\ndynamics with higher accuracy than shallow neural networks (NNs) and SMC\nensures robust tracking. This framework employs restricted potential functions\nwithin a set-theoretic paradigm to ensure system trajectories remain bounded\nwithin a compact set, improving robustness against approximation errors and\nexternal disturbances. The control scheme is grounded in non-smooth Lyapunov\nstability theory, with update laws derived for both inner and outer layer\nnetwork weights of DNN. A numerical example is simulated that showcases the\nproposed controller's effectiveness, adaptability, and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21667v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21511", "title": "Two-Dimensional Nonseparable Fractional Fourier Transform: Theory and Application", "authors": ["Daxiang Li", "Zhichao Zhang", "Wei Yao"], "categories": ["eess.SP", "26A33, 42A38, 94A08, 94A12", "I.4.3; I.6.3; I.5.2; G.1.2"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      26 pages, 11 figures", "url": "http://arxiv.org/abs/2507.21511v1", "summary": "The one-dimensional (1D) fractional Fourier transform (FRFT) generalizes the\n1D Fourier transform, offering significant advantages in time-frequency\nanalysis of non-stationary signals. To extend the benefits of the 1D FRFT to\nhigher-dimensional signals, 2D FRFTs, such as the 2D separable FRFT (SFRFT),\ngyrator transform (GT), and coupled FRFT (CFRFT), have been developed. However,\nexisting 2D FRFTs suffer from several limitations: (1) a lack of theoretical\nuniformity and general applicability, (2) an inability to handle 2D\nnon-stationary signals with nonseparable terms, and (3) failure to maintain a\nconsistent 4D rotational relationship with the 2D Wigner distribution (WD),\nwhich is essential for ensuring geometric consistency and symmetry in\ntime-frequency analysis. These limitations restrict the methods' performance in\npractical applications, such as radar, communication, sonar, and optical\nimaging, in which nonseparable terms frequently arise. To address these\nchallenges, we introduce a more general definition of the 2D FRFT, termed the\n2D nonseparable FRFT (NSFRFT). The 2D NSFRFT has four degrees of freedom,\nincludes the 2D SFRFT, GT, and CFRFT as special cases, and maintains a more\ngeneral 4D rotational relationship with the 2D WD. We derive its properties and\npresent three discrete algorithms, two of which are fast algorithms with\ncomputational complexity $O(N^2 \\log N)$ comparable to that of the 2D SFRFT.\nNumerical simulations and experiments demonstrate the superior performance of\nthe 2D NSFRFT in applications such as image encryption, decryption, filtering,\nand denoising.", "comment": "26 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.21511v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.01582", "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping", "authors": ["Saad Ejaz", "Hriday Bavle", "Laura Ribeiro", "Holger Voos", "Jose Luis Sanchez-Lopez"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01582v3", "summary": "In 3D object mapping, category-level priors enable efficient object\nreconstruction and canonical pose estimation, requiring only a single prior per\nsemantic category (e.g., chair, book, laptop, etc.). DeepSDF has been used\npredominantly as a category-level shape prior, but it struggles to reconstruct\nsharp geometry and is computationally expensive. In contrast, NeRFs capture\nfine details but have yet to be effectively integrated with category-level\npriors in a real-time multi-object mapping framework. To bridge this gap, we\nintroduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates\ncategory-level priors with object-level NeRFs to enhance reconstruction\nefficiency and enable canonical object pose estimation. PRENOM gets to know\nobjects on a first-name basis by meta-learning on synthetic reconstruction\ntasks generated from open-source shape datasets. To account for object category\nvariations, it employs a multi-objective genetic algorithm to optimize the NeRF\narchitecture for each category, balancing reconstruction quality and training\ntime. Additionally, prior-based probabilistic ray sampling directs sampling\ntoward expected object regions, accelerating convergence and improving\nreconstruction quality under constrained resources. Experimental results\nhighlight the ability of PRENOM to achieve high-quality reconstructions while\nmaintaining computational feasibility. Specifically, comparisons with\nprior-free NeRF-based approaches on a synthetic dataset show a 21\\% lower\nChamfer distance. Furthermore, evaluations against other approaches using shape\npriors on a noisy real-world dataset indicate a 13\\% improvement averaged\nacross all reconstruction metrics, and comparable pose and size estimation\naccuracy, while being trained for 5$\\times$ less time. Code available at:\nhttps://github.com/snt-arg/PRENOM", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01582v3", "cate": "cs.CV", "date": "2025-03-03", "updated": "2025-07-29"}
{"id": "2507.21752", "title": "SAT-Based Bounded Fitting for the Description Logic ALC", "authors": ["Maurice Funk", "Jean Christoph Jung", "Tom Voellmer"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      33 pages, full version of paper accepted at ISWC 2025", "url": "http://arxiv.org/abs/2507.21752v1", "summary": "Bounded fitting is a general paradigm for learning logical formulas from\npositive and negative data examples, that has received considerable interest\nrecently. We investigate bounded fitting for the description logic ALC and its\nsyntactic fragments. We show that the underlying size-restricted fitting\nproblem is NP-complete for all studied fragments, even in the special case of a\nsingle positive and a single negative example. By design, bounded fitting comes\nwith probabilistic guarantees in Valiant's PAC learning framework. In contrast,\nwe show that other classes of algorithms for learning ALC concepts do not\nprovide such guarantees. Finally, we present an implementation of bounded\nfitting in ALC and its fragments based on a SAT solver. We discuss\noptimizations and compare our implementation to other concept learning tools.", "comment": "33 pages, full version of paper accepted at ISWC 2025", "pdf_url": "http://arxiv.org/pdf/2507.21752v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21616", "title": "Categorical Distributions are Effective Neural Network Outputs for Event Prediction", "authors": ["Kevin Doran", "Tom Baden"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      32 pages, 26 figures", "url": "http://arxiv.org/abs/2507.21616v1", "summary": "We demonstrate the effectiveness of using a simple neural network output, a\ncategorical probability distribution, for the task of next spike prediction.\nThis case study motivates an investigation into why this simple output\nstructure is not commonly used with neural temporal point process models. We\nfind evidence that many existing datasets for evaluating temporal point process\nmodels do not reveal much information about the underlying event generating\nprocesses, and many existing models perform well due to regularization effects\nof model size and constraints on output structure. We extend existing datasets\nand create new ones in order to explore outside of this information limited\nregime and find that outputting a simple categorical distribution is\ncompetitive across a wide range of datasets.", "comment": "32 pages, 26 figures", "pdf_url": "http://arxiv.org/pdf/2507.21616v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20536", "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation", "authors": ["Chieh-Yun Chen", "Min Shi", "Gong Zhang", "Humphrey Shi"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.20536v2", "summary": "Text-to-Image (T2I) generative models have revolutionized content creation\nbut remain highly sensitive to prompt phrasing, often requiring users to\nrepeatedly refine prompts multiple times without clear feedback. While\ntechniques such as automatic prompt engineering, controlled text embeddings,\ndenoising, and multi-turn generation mitigate these issues, they offer limited\ncontrollability, or often necessitate additional training, restricting the\ngeneralization abilities. Thus, we introduce T2I-Copilot, a training-free\nmulti-agent system that leverages collaboration between (Multimodal) Large\nLanguage Models to automate prompt phrasing, model selection, and iterative\nrefinement. This approach significantly simplifies prompt engineering while\nenhancing generation quality and text-image alignment compared to direct\ngeneration. Specifically, T2I-Copilot consists of three agents: (1) Input\nInterpreter, which parses the input prompt, resolves ambiguities, and generates\na standardized report; (2) Generation Engine, which selects the appropriate\nmodel from different types of T2I models and organizes visual and textual\nprompts to initiate generation; and (3) Quality Evaluator, which assesses\naesthetic quality and text-image alignment, providing scores and feedback for\npotential regeneration. T2I-Copilot can operate fully autonomously while also\nsupporting human-in-the-loop intervention for fine-grained control. On\nGenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA\nscore comparable to commercial models RecraftV3 and Imagen 3, surpasses\nFLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and\nSD 3.5 Large by 9.11% and 6.36%. Code will be released at:\nhttps://github.com/SHI-Labs/T2I-Copilot.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.20536v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-29"}
{"id": "2507.21600", "title": "Locally Controlled Face Aging with Latent Diffusion Models", "authors": ["Lais Isabelle Alves dos Santos", "Julien Despois", "Thibaut Chauffier", "Sileye O. Ba", "Giovanni Palma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21600v1", "summary": "We present a novel approach to face aging that addresses the limitations of\ncurrent methods which treat aging as a global, homogeneous process. Existing\ntechniques using GANs and diffusion models often condition generation on a\nreference image and target age, neglecting that facial regions age\nheterogeneously due to both intrinsic chronological factors and extrinsic\nelements like sun exposure. Our method leverages latent diffusion models to\nselectively age specific facial regions using local aging signs. This approach\nprovides significantly finer-grained control over the generation process,\nenabling more realistic and personalized aging. We employ a latent diffusion\nrefiner to seamlessly blend these locally aged regions, ensuring a globally\nconsistent and natural-looking synthesis. Experimental results demonstrate that\nour method effectively achieves three key criteria for successful face aging:\nrobust identity preservation, high-fidelity and realistic imagery, and a\nnatural, controllable aging progression.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21600v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21175", "title": "The Human Capital Ontology (Extended Abstract)", "authors": ["Shane Babcock", "Maxwell Farrington", "John Gugliotti"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      3 pages. 2 figures. Conference: Semantic Technology for Intelligence, Defense, and Security (STIDS 2024)", "url": "http://arxiv.org/abs/2507.21175v1", "summary": "The Human Capital Ontology (HCO) is an ontology that represents data\nstandards maintained and employed by the Office of Personnel Management (OPM)\nto represent Human Capital Operations and to classify job positions. The HCO is\nan extension of the Common Core Ontologies and the upper level Basic Formal\nOntology (BFO). HCO provides representation of OPM Nature of Action (NOA) codes\nthat are used to describe human resource personnel actions. HCO also represents\nOccupational Groups and Job Families, the Occupational Series into which these\nsubdivide, as well as their corresponding codes, used by OPM to classify and\ngrade both white and blue collar jobs in the Federal Government. HCO also\nencodes crosswalks between OPM Occupational Series and corresponding Standard\nOccupational Classification Codes maintained by the U.S. Bureau of Labor\nStatistics. In addition to documenting and justifying the approach of HCO to\nmodeling the above, we report on recent and planned applications of HCO across\nthe US Government. We also report on parallel efforts of ours to enhance the\nstate of the art in structured data informed Human Capital measurements.\nKeywords: Office of Personnel Management, ontology, occupational series, nature\nof action, personnel action, human capital, position classification standards.", "comment": "3 pages. 2 figures. Conference: Semantic Technology for Intelligence,\n  Defense, and Security (STIDS 2024)", "pdf_url": "http://arxiv.org/pdf/2507.21175v1", "cate": "cs.CY", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.21669", "title": "Data-Driven Greenhouse Climate Regulation in Lettuce Cultivation Using BiLSTM and GRU Predictive Control", "authors": ["Soumo Emmanuel Arnaud", "Marcello Calisti", "Athanasios Polydoros"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21669v1", "summary": "Efficient greenhouse management is essential for sustainable food production\nin response to a growing global population. However, maintaining optimal indoor\nclimates requires significant energy and resources, making advanced control\nsystems critical for economic viability and environmental sustainability.\nTraditional greenhouse models are often complex and imprecise, limiting the\neffectiveness of conventional control strategies. To address these challenges,\nthis study investigates data-driven predictive control methods using Gated\nRecurrent Unit (GRU) and Long Short-Term Memory (LSTM) neural networks. Our\nexperiments showed that GRU-based predictive control reduced temperature and\nhumidity violations by up to 5\\% and required 40\\% less computation time than\nthe LSTM approach, all while maintaining equivalent economic performance and\ncrop yield. These findings demonstrate that GRU-based predictive control offers\na more efficient and practical solution for real-time greenhouse climate\nregulation in precision agriculture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21669v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21527", "title": "Trainable Joint Time-Vertex Fractional Fourier Transform", "authors": ["Ziqi Yan", "Zhichao Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      35 pages,5 figures", "url": "http://arxiv.org/abs/2507.21527v1", "summary": "To address limitations of the graph fractional Fourier transform (GFRFT)\nWiener filtering and the traditional joint time-vertex fractional Fourier\ntransform (JFRFT) Wiener filtering, this study proposes a filtering method\nbased on the hyper-differential form of the JFRFT. The gradient backpropagation\nmechanism is employed to enable the adaptive selection of transform order pair\nand filter coefficients. First, leveraging the hyper-differential form of the\nGFRFT and the fractional Fourier transform, the hyper-differential form of the\nJFRFT is constructed and its properties are analyzed. Second, time-varying\ngraph signals are divided into dynamic graph sequences of equal span along the\ntemporal dimension. A spatiotemporal joint representation is then established\nthrough vectorized reorganization, followed by the joint time-vertex Wiener\nfiltering. Furthermore, by rigorously proving the differentiability of the\ntransform orders, both the transform orders and filter coefficients are\nembedded as learnable parameters within a neural network architecture. Through\ngradient backpropagation, their synchronized iterative optimization is\nachieved, constructing a parameters-adaptive learning filtering framework. This\nmethod leverages a model-driven approach to learn the optimal transform order\npair and filter coefficients. Experimental results indicate that the proposed\nframework improves the time-varying graph signals denoising performance, while\nreducing the computational burden of the traditional grid search strategy.", "comment": "35 pages,5 figures", "pdf_url": "http://arxiv.org/pdf/2507.21527v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21156", "title": "Comparative Analysis of Vision Transformers and Convolutional Neural Networks for Medical Image Classification", "authors": ["Kunal Kawadkar"], "categories": ["eess.IV", "cs.CV", "cs.LG", "I.2.10; I.4.8"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      9 pages, 8 figures, 3 tables. Submitted to IEEE Access", "url": "http://arxiv.org/abs/2507.21156v1", "summary": "The emergence of Vision Transformers (ViTs) has revolutionized computer\nvision, yet their effectiveness compared to traditional Convolutional Neural\nNetworks (CNNs) in medical imaging remains under-explored. This study presents\na comprehensive comparative analysis of CNN and ViT architectures across three\ncritical medical imaging tasks: chest X-ray pneumonia detection, brain tumor\nclassification, and skin cancer melanoma detection. We evaluated four\nstate-of-the-art models - ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small\n- across datasets totaling 8,469 medical images. Our results demonstrate\ntask-specific model advantages: ResNet-50 achieved 98.37% accuracy on chest\nX-ray classification, DeiT-Small excelled at brain tumor detection with 92.16%\naccuracy, and EfficientNet-B0 led skin cancer classification at 81.84%\naccuracy. These findings provide crucial insights for practitioners selecting\narchitectures for medical AI applications, highlighting the importance of\ntask-specific architecture selection in clinical decision support systems.", "comment": "9 pages, 8 figures, 3 tables. Submitted to IEEE Access", "pdf_url": "http://arxiv.org/pdf/2507.21156v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2504.03524", "title": "RANa: Retrieval-Augmented Navigation", "authors": ["Gianluca Monaci", "Rafael S. Rezende", "Romain Deffayet", "Gabriela Csurka", "Guillaume Bono", "Hervé Déjean", "Stéphane Clinchant", "Christian Wolf"], "categories": ["cs.CV", "cs.IR", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.03524v2", "summary": "Methods for navigation based on large-scale learning typically treat each\nepisode as a new problem, where the agent is spawned with a clean memory in an\nunknown environment. While these generalization capabilities to an unknown\nenvironment are extremely important, we claim that, in a realistic setting, an\nagent should have the capacity of exploiting information collected during\nearlier robot operations. We address this by introducing a new\nretrieval-augmented agent, trained with RL, capable of querying a database\ncollected from previous episodes in the same environment and learning how to\nintegrate this additional context information. We introduce a unique agent\narchitecture for the general navigation task, evaluated on ImageNav,\nInstance-ImageNav and ObjectNav. Our retrieval and context encoding methods are\ndata-driven and employ vision foundation models (FM) for both semantic and\ngeometric understanding. We propose new benchmarks for these settings and we\nshow that retrieval allows zero-shot transfer across tasks and environments\nwhile significantly improving performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.03524v2", "cate": "cs.CV", "date": "2025-04-04", "updated": "2025-07-29"}
{"id": "2507.21753", "title": "Towards a rigorous evaluation of RAG systems: the challenge of due diligence", "authors": ["Grégoire Martinon", "Alexandra Lorenzo de Brionne", "Jérôme Bohard", "Antoine Lojou", "Damien Hervault", "Nicolas J-B. Brunel"], "categories": ["cs.AI", "stat.AP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      in French language. EvalLLM2025: Workshop on Evaluation Generative Models (LLM) and Challenges, AMIAD, 2025, Marseille, France", "url": "http://arxiv.org/abs/2507.21753v1", "summary": "The rise of generative AI, has driven significant advancements in high-risk\nsectors like healthcare and finance. The Retrieval-Augmented Generation (RAG)\narchitecture, combining language models (LLMs) with search engines, is\nparticularly notable for its ability to generate responses from document\ncorpora. Despite its potential, the reliability of RAG systems in critical\ncontexts remains a concern, with issues such as hallucinations persisting. This\nstudy evaluates a RAG system used in due diligence for an investment fund. We\npropose a robust evaluation protocol combining human annotations and LLM-Judge\nannotations to identify system failures, like hallucinations, off-topic, failed\ncitations, and abstentions. Inspired by the Prediction Powered Inference (PPI)\nmethod, we achieve precise performance measurements with statistical\nguarantees. We provide a comprehensive dataset for further analysis. Our\ncontributions aim to enhance the reliability and scalability of RAG systems\nevaluation protocols in industrial applications.", "comment": "in French language. EvalLLM2025: Workshop on Evaluation Generative\n  Models (LLM) and Challenges, AMIAD, 2025, Marseille, France", "pdf_url": "http://arxiv.org/pdf/2507.21753v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21648", "title": "Hyperbolic Genome Embeddings", "authors": ["Raiyan R. Khan", "Philippe Chlenski", "Itsik Pe'er"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      30 pages, 16 figures, 10 tables. Camera-ready version for ICLR 2025", "url": "http://arxiv.org/abs/2507.21648v1", "summary": "Current approaches to genomic sequence modeling often struggle to align the\ninductive biases of machine learning models with the evolutionarily-informed\nstructure of biological systems. To this end, we formulate a novel application\nof hyperbolic CNNs that exploits this structure, enabling more expressive DNA\nsequence representations. Our strategy circumvents the need for explicit\nphylogenetic mapping while discerning key properties of sequences pertaining to\ncore functional and regulatory behavior. Across 37 out of 42 genome\ninterpretation benchmark datasets, our hyperbolic models outperform their\nEuclidean equivalents. Notably, our approach even surpasses state-of-the-art\nperformance on seven GUE benchmark datasets, consistently outperforming many\nDNA language models while using orders of magnitude fewer parameters and\navoiding pretraining. Our results include a novel set of benchmark\ndatasets--the Transposable Elements Benchmark--which explores a major but\nunderstudied component of the genome with deep evolutionary significance. We\nfurther motivate our work by exploring how our hyperbolic models recognize\ngenomic signal under various data-generating conditions and by constructing an\nempirical method for interpreting the hyperbolicity of dataset embeddings.\nThroughout these assessments, we find persistent evidence highlighting the\npotential of our hyperbolic framework as a robust paradigm for genome\nrepresentation learning. Our code and benchmark datasets are available at\nhttps://github.com/rrkhan/HGE.", "comment": "30 pages, 16 figures, 10 tables. Camera-ready version for ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2507.21648v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21606", "title": "Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking", "authors": ["Yaozong Zheng", "Bineng Zhong", "Qihua Liang", "Ning Li", "Shuxiang Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by AAAI2025", "url": "http://arxiv.org/abs/2507.21606v1", "summary": "The success of visual tracking has been largely driven by datasets with\nmanual box annotations. However, these box annotations require tremendous human\neffort, limiting the scale and diversity of existing tracking datasets. In this\nwork, we present a novel Self-Supervised Tracking framework named\n\\textbf{{\\tracker}}, designed to eliminate the need of box annotations.\nSpecifically, a decoupled spatio-temporal consistency training framework is\nproposed to learn rich target information across timestamps through global\nspatial localization and local temporal association. This allows for the\nsimulation of appearance and motion variations of instances in real-world\nscenarios. Furthermore, an instance contrastive loss is designed to learn\ninstance-level correspondences from a multi-view perspective, offering robust\ninstance supervision without additional labels. This new design paradigm\nenables {\\tracker} to effectively learn generic tracking representations in a\nself-supervised manner, while reducing reliance on extensive box annotations.\nExtensive experiments on nine benchmark datasets demonstrate that {\\tracker}\nsurpasses \\textit{SOTA} self-supervised tracking methods, achieving an\nimprovement of more than 25.3\\%, 20.4\\%, and 14.8\\% in AUC (AO) score on the\nGOT10K, LaSOT, TrackingNet datasets, respectively. Code:\nhttps://github.com/GXNU-ZhongLab/SSTrack.", "comment": "Accepted by AAAI2025", "pdf_url": "http://arxiv.org/pdf/2507.21606v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21743", "title": "When Proximity Falls Short: Inequalities in Commuting and Accessibility by Public Transport in Santiago, Chile", "authors": ["Cesar Marin-Flores", "Leo Ferres", "Henrikki Tenkanen"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      30 pages, Journal Paper", "url": "http://arxiv.org/abs/2507.21743v1", "summary": "Traditional measures of urban accessibility often rely on static models or\nsurvey data. However, location information from mobile networks now enables\nlarge-scale, dynamic analyses of how people navigate cities. This study uses\neXtended Detail Records (XDRs) derived from mobile phone activity to analyze\ncommuting patterns and accessibility inequalities in Santiago, Chile. First, we\nidentify residential and work locations and model commuting routes using the R5\nmultimodal routing engine, which combines public transport and walking. To\nexplore spatial patterns, we apply a bivariate spatial clustering analysis\n(LISA) alongside regression techniques to identify distinct commuting behaviors\nand their alignment with vulnerable population groups. Our findings reveal that\naverage commuting times remain consistent across socioeconomic groups. However,\ndespite residing in areas with greater opportunity density, higher-income\npopulations do not consistently experience shorter commuting times. This\nhighlights a disconnect between spatial proximity to opportunities and actual\ntravel experience. Our analysis reveals significant disparities between\nsociodemographic groups, particularly regarding the distribution of indigenous\npopulations and gender. Overall, the findings of our study suggest that\ncommuting and accessibility inequalities in Santiago are closely linked to\nbroader social and demographic structures.", "comment": "30 pages, Journal Paper", "pdf_url": "http://arxiv.org/pdf/2507.21743v1", "cate": "cs.CY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21702", "title": "Analytical Treatment of Hollow Toroid Flux Tubes", "authors": ["Herbert Schmidt"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      10 pages, 9 figures, 1 table, accepted for publication at the 16th International Modelica & fmi Conference", "url": "http://arxiv.org/abs/2507.21702v1", "summary": "Stray flux tubes around cylindrical poles are commonly modelled starting from\nthe results for planar flux tubes using the circumference of the cylinder as\ndepth. While this is a tried and tested approach, we here discuss analytical\nexpressions using the actual axisymmetric geometry of a fraction of a hollow\ntorus and compare their results to those of the accepted approach.", "comment": "10 pages, 9 figures, 1 table, accepted for publication at the 16th\n  International Modelica & fmi Conference", "pdf_url": "http://arxiv.org/pdf/2507.21702v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21570", "title": "Causal Link Discovery with Unequal Edge Error Tolerance", "authors": ["Joni Shaska", "Urbashi Mitra"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      14 pages, 6 figures, portions presented at International Symposium on Information Theory (ISIT) 2024 and Asilomar 2024", "url": "http://arxiv.org/abs/2507.21570v1", "summary": "This paper proposes a novel framework for causal discovery with asymmetric\nerror control, called Neyman-Pearson causal discovery. Despite the importance\nof applications where different types of edge errors may have different\nimportance, current state-of-the-art causal discovery algorithms do not\ndifferentiate between the types of edge errors, nor provide any finite-sample\nguarantees on the edge errors. Hence, this framework seeks to minimize one type\nof error while keeping the other below a user-specified tolerance level. Using\ntechniques from information theory, fundamental performance limits are found,\ncharacterized by the R\\'enyi divergence, for Neyman-Pearson causal discovery.\nFurthermore, a causal discovery algorithm is introduced for the case of linear\nadditive Gaussian noise models, called epsilon-CUT, that provides finite-sample\nguarantees on the false positive rate, while staying competitive with\nstate-of-the-art methods.", "comment": "14 pages, 6 figures, portions presented at International Symposium on\n  Information Theory (ISIT) 2024 and Asilomar 2024", "pdf_url": "http://arxiv.org/pdf/2507.21570v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21165", "title": "Querying GI Endoscopy Images: A VQA Approach", "authors": ["Gaurav Parajuli"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21165v1", "summary": "VQA (Visual Question Answering) combines Natural Language Processing (NLP)\nwith image understanding to answer questions about a given image. It has\nenormous potential for the development of medical diagnostic AI systems. Such a\nsystem can help clinicians diagnose gastro-intestinal (GI) diseases accurately\nand efficiently. Although many of the multimodal LLMs available today have\nexcellent VQA capabilities in the general domain, they perform very poorly for\nVQA tasks in specialized domains such as medical imaging. This study is a\nsubmission for ImageCLEFmed-MEDVQA-GI 2025 subtask 1 that explores the\nadaptation of the Florence2 model to answer medical visual questions on GI\nendoscopy images. We also evaluate the model performance using standard metrics\nlike ROUGE, BLEU and METEOR", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21165v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.00209", "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00209v3", "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00209v3", "cate": "eess.IV", "date": "2025-06-30", "updated": "2025-07-29"}
{"id": "2507.21792", "title": "Hybrid Causal Identification and Causal Mechanism Clustering", "authors": ["Saixiong Liu", "Yuhua Qian", "Jue Li", "Honghong Cheng", "Feijiang Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21792v1", "summary": "Bivariate causal direction identification is a fundamental and vital problem\nin the causal inference field. Among binary causal methods, most methods based\non additive noise only use one single causal mechanism to construct a causal\nmodel. In the real world, observations are always collected in different\nenvironments with heterogeneous causal relationships. Therefore, on observation\ndata, this paper proposes a Mixture Conditional Variational Causal Inference\nmodel (MCVCI) to infer heterogeneous causality. Specifically, according to the\nidentifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the\nsuperior fitting capabilities of the Gaussian mixture model and the neural\nnetwork and elegantly uses the likelihoods obtained from the probabilistic\nbounds of the mixture conditional variational auto-encoder as causal decision\ncriteria. Moreover, we model the casual heterogeneity into cluster numbers and\npropose the Mixture Conditional Variational Causal Clustering (MCVCC) method,\nwhich can reveal causal mechanism expression. Compared with state-of-the-art\nmethods, the comprehensive best performance demonstrates the effectiveness of\nthe methods proposed in this paper on several simulated and real data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21792v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21653", "title": "DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs", "authors": ["Yuan Li", "Jun Hu", "Bryan Hooi", "Bingsheng He", "Cheng Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21653v1", "summary": "Real-world fraud detection applications benefit from graph learning\ntechniques that jointly exploit node features, often rich in textual data, and\ngraph structural information. Recently, Graph-Enhanced LLMs emerge as a\npromising graph learning approach that converts graph information into prompts,\nexploiting LLMs' ability to reason over both textual and structural\ninformation. Among them, text-only prompting, which converts graph information\nto prompts consisting solely of text tokens, offers a solution that relies only\non LLM tuning without requiring additional graph-specific encoders. However,\ntext-only prompting struggles on heterogeneous fraud-detection graphs:\nmulti-hop relations expand exponentially with each additional hop, leading to\nrapidly growing neighborhoods associated with dense textual information. These\nneighborhoods may overwhelm the model with long, irrelevant content in the\nprompt and suppress key signals from the target node, thereby degrading\nperformance. To address this challenge, we propose Dual Granularity Prompting\n(DGP), which mitigates information overload by preserving fine-grained textual\ndetails for the target node while summarizing neighbor information into\ncoarse-grained text prompts. DGP introduces tailored summarization strategies\nfor different data modalities, bi-level semantic abstraction for textual fields\nand statistical aggregation for numerical features, enabling effective\ncompression of verbose neighbor content into concise, informative prompts.\nExperiments across public and industrial datasets demonstrate that DGP operates\nwithin a manageable token budget while improving fraud detection performance by\nup to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of\nGraph-Enhanced LLMs for fraud detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21653v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21608", "title": "Semantic Segmentation of iPS Cells: Case Study on Model Complexity in Biomedical Imaging", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19th International Conference on Machine Vision Applications MVA2025", "url": "http://arxiv.org/abs/2507.21608v1", "summary": "Medical image segmentation requires not only accuracy but also robustness\nunder challenging imaging conditions. In this study, we show that a carefully\nconfigured DeepLabv3 model can achieve high performance in segmenting induced\npluripotent stem (iPS) cell colonies, and, under our experimental conditions,\noutperforms large-scale foundation models such as SAM2 and its medical variant\nMedSAM2 without structural modifications. These results suggest that, for\nspecialized tasks characterized by subtle, low-contrast boundaries, increased\nmodel complexity does not necessarily translate to better performance. Our work\nrevisits the assumption that ever-larger and more generalized architectures are\nalways preferable, and provides evidence that appropriately adapted, simpler\nmodels may offer strong accuracy and practical reliability in domain-specific\nbiomedical applications. We also offer an open-source implementation that\nincludes strategies for small datasets and domain-specific encoding, with the\naim of supporting further advances in semantic segmentation for regenerative\nmedicine and related fields.", "comment": "19th International Conference on Machine Vision Applications MVA2025", "pdf_url": "http://arxiv.org/pdf/2507.21608v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21839", "title": "Against racing to AGI: Cooperation, deterrence, and catastrophic risks", "authors": ["Leonard Dung", "Max Hellrigel-Holderbaum"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21839v1", "summary": "AGI Racing is the view that it is in the self-interest of major actors in AI\ndevelopment, especially powerful nations, to accelerate their frontier AI\ndevelopment to build highly capable AI, especially artificial general\nintelligence (AGI), before competitors have a chance. We argue against AGI\nRacing. First, the downsides of racing to AGI are much higher than portrayed by\nthis view. Racing to AGI would substantially increase catastrophic risks from\nAI, including nuclear instability, and undermine the prospects of technical AI\nsafety research to be effective. Second, the expected benefits of racing may be\nlower than proponents of AGI Racing hold. In particular, it is questionable\nwhether winning the race enables complete domination over losers. Third,\ninternational cooperation and coordination, and perhaps carefully crafted\ndeterrence measures, constitute viable alternatives to racing to AGI which have\nmuch smaller risks and promise to deliver most of the benefits that racing to\nAGI is supposed to provide. Hence, racing to AGI is not in anyone's\nself-interest as other actions, particularly incentivizing and seeking\ninternational cooperation around AI issues, are preferable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21839v1", "cate": "cs.CY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21759", "title": "The impact of large-scale EV charging on the real-time operation of distribution systems: A comprehensive review", "authors": ["Zhe Yu", "Chuang Yang", "Qin Wang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21759v1", "summary": "With the large-scale integration of electric vehicles (EVs) in the\ndistribution grid, the unpredictable nature of EV charging introduces\nconsiderable uncertainties to the grid's real-time operations. This can\nexacerbate load fluctuations, compromise power quality, and pose risks to the\ngrid's stability and security. However, due to their dual role as controllable\nloads and energy storage devices, EVs have the potential to mitigate these\nfluctuations, balance the variability of renewable energy sources, and provide\nancillary services that support grid stability. By leveraging the bidirectional\nflow of information and energy in smart grids, the adverse effects of EV\ncharging can be minimized and even converted into beneficial outcomes through\neffective real-time management strategies. This paper explores the negative\nimpacts of EV charging on the distribution system's real-time operations and\noutlines methods to transform these challenges into positive contributions.\nAdditionally, it provides an in-depth analysis of the real-time management\nsystem for EV charging, focusing on state estimation and management strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21759v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21593", "title": "Affine Invariant Semi-Blind Receiver: Joint Channel Estimation and High-Order Signal Detection for Multiuser Massive MIMO-OFDM Systems", "authors": ["Erdeng Zhang", "Shuntian Zheng", "Sheng Wu", "Haoge Jia", "Zhe Ji", "Ailing Xiao"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21593v1", "summary": "Massive multiple input and multiple output (MIMO) systems with orthogonal\nfrequency division multiplexing (OFDM) are foundational for downlink multi-user\n(MU) communication in future wireless networks, for their ability to enhance\nspectral efficiency and support a large number of users simultaneously.\nHowever, high user density intensifies severe inter-user interference (IUI) and\npilot overhead. Consequently, existing blind and semi-blind channel estimation\n(CE) and signal detection (SD) algorithms suffer performance degradation and\nincreased complexity, especially when further challenged by frequency-selective\nchannels and high-order modulation demands. To this end, this paper proposes a\nnovel semi-blind joint channel estimation and signal detection (JCESD) method.\nSpecifically, the proposed approach employs a hybrid precoding architecture to\nsuppress IUI. Furthermore we formulate JCESD as a non-convex constellation\nfitting optimization exploiting constellation affine invariance. Few pilots are\nused to achieve coarse estimation for initialization and ambiguity resolution.\nFor high-order modulations, a data augmentation mechanism utilizes the symmetry\nof quadrature amplitude modulation (QAM) constellations to increase the\neffective number of samples. To address frequency-selective channels, CE\naccuracy is then enhanced via an iterative refinement strategy that leverages\nimproved SD results. Simulation results demonstrate an average throughput gain\nof 11\\% over widely used pilot-based methods in MU scenarios, highlighting the\nproposed method's potential to improve spectral efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21593v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21516", "title": "ST-DAI: Single-shot 2.5D Spatial Transcriptomics with Intra-Sample Domain Adaptive Imputation for Cost-efficient 3D Reconstruction", "authors": ["Jiahe Qian", "Yaoyu Fang", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      21 pages, 4 figures, 3 tables, under review", "url": "http://arxiv.org/abs/2507.21516v1", "summary": "For 3D spatial transcriptomics (ST), the high per-section acquisition cost of\nfully sampling every tissue section remains a significant challenge. Although\nrecent approaches predict gene expression from histology images, these methods\nrequire large external datasets, which leads to high-cost and suffers from\nsubstantial domain discrepancies that lead to poor generalization on new\nsamples. In this work, we introduce ST-DAI, a single-shot framework for 3D ST\nthat couples a cost-efficient 2.5D sampling scheme with an intra-sample\ndomain-adaptive imputation framework. First, in the cost-efficient 2.5D\nsampling stage, one reference section (central section) is fully sampled while\nother sections (adjacent sections) is sparsely sampled, thereby capturing\nvolumetric context at significantly reduced experimental cost. Second, we\npropose a single-shot 3D imputation learning method that allows us to generate\nfully sampled 3D ST from this cost-efficient 2.5D ST scheme, using only\nsample-specific training. We observe position misalignment and domain\ndiscrepancy between sections. To address those issues, we adopt a pipeline that\nfirst aligns the central section to the adjacent section, thereafter generates\ndense pseudo-supervision on the central section, and then performs Fast\nMulti-Domain Refinement (FMDR), which adapts the network to the domain of the\nadjacent section while fine-tuning only a few parameters through the use of\nParameter-Efficient Domain-Alignment Layers (PDLs). During this refinement, a\nConfidence Score Generator (CSG) reweights the pseudo-labels according to their\nestimated reliability, thereby directing imputation toward trustworthy regions.\nOur experimental results demonstrate that ST-DAI achieves gene expression\nprediction performance comparable to fully sampled approaches while\nsubstantially reducing the measurement burden.", "comment": "21 pages, 4 figures, 3 tables, under review", "pdf_url": "http://arxiv.org/pdf/2507.21516v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.15857", "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "authors": ["Mihir Prabhudesai", "Mengning Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Project Webpage: this https URL", "url": "http://arxiv.org/abs/2507.15857v3", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "comment": "Project Webpage: https://diffusion-scaling.github.io", "pdf_url": "http://arxiv.org/pdf/2507.15857v3", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-29"}
{"id": "2507.21802", "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE", "authors": ["Junzhe Li", "Yutao Cui", "Tao Huang", "Yinping Ma", "Chun Fan", "Miles Yang", "Zhao Zhong"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21802v1", "summary": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\n$\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\n$\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21802v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21670", "title": "Probabilistic Consistency in Machine Learning and Its Connection to Uncertainty Quantification", "authors": ["Paul Patrone", "Anthony Kearsley"], "categories": ["cs.LG", "math.PR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21670v1", "summary": "Machine learning (ML) is often viewed as a powerful data analysis tool that\nis easy to learn because of its black-box nature. Yet this very nature also\nmakes it difficult to quantify confidence in predictions extracted from ML\nmodels, and more fundamentally, to understand how such models are mathematical\nabstractions of training data. The goal of this paper is to unravel these\nissues and their connections to uncertainty quantification (UQ) by pursuing a\nline of reasoning motivated by diagnostics. In such settings, prevalence - i.e.\nthe fraction of elements in class - is often of inherent interest. Here we\nanalyze the many interpretations of prevalence to derive a level-set theory of\nclassification, which shows that certain types of self-consistent ML models are\nequivalent to class-conditional probability distributions. We begin by studying\nthe properties of binary Bayes optimal classifiers, recognizing that their\nboundary sets can be reinterpreted as level-sets of pairwise density ratios. By\nparameterizing Bayes classifiers in terms of the prevalence, we then show that\nthey satisfy important monotonicity and class-switching properties that can be\nused to deduce the density ratios without direct access to the boundary sets.\nMoreover, this information is sufficient for tasks such as constructing the\nmulticlass Bayes-optimal classifier and estimating inherent uncertainty in the\nclass assignments. In the multiclass case, we use these results to deduce\nnormalization and self-consistency conditions, the latter being equivalent to\nthe law of total probability for classifiers. We also show that these are\nnecessary conditions for arbitrary ML models to have valid probabilistic\ninterpretations. Throughout we demonstrate how this analysis informs the\nbroader task of UQ for ML via an uncertainty propagation framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21670v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21611", "title": "Wind Turbine Feature Detection Using Deep Learning and Synthetic Data", "authors": ["Arash Shahirpour", "Jakob Gebler", "Manuel Sanders", "Tim Reuscher"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, accepted at ICMV 2025", "url": "http://arxiv.org/abs/2507.21611v1", "summary": "For the autonomous drone-based inspection of wind turbine (WT) blades,\naccurate detection of the WT and its key features is essential for safe drone\npositioning and collision avoidance. Existing deep learning methods typically\nrely on manually labeled real-world images, which limits both the quantity and\nthe diversity of training datasets in terms of weather conditions, lighting,\nturbine types, and image complexity. In this paper, we propose a method to\ngenerate synthetic training data that allows controlled variation of visual and\nenvironmental factors, increasing the diversity and hence creating challenging\nlearning scenarios. Furthermore, we train a YOLOv11 feature detection network\nsolely on synthetic WT images with a modified loss function, to detect WTs and\ntheir key features within an image. The resulting network is evaluated both\nusing synthetic images and a set of real-world WT images and shows promising\nperformance across both synthetic and real-world data, achieving a Pose\nmAP50-95 of 0.97 on real images never seen during training.", "comment": "8 pages, 5 figures, accepted at ICMV 2025", "pdf_url": "http://arxiv.org/pdf/2507.21611v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21842", "title": "Prompt template for a fictitious LLM agent in a content-flagging experiment", "authors": ["Marie-Therese Sekwenz", "Daria Simons", "Alina Wundsam"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21842v1", "summary": "Digital regulations such as the European Union's Digital Services Act (DSA)\nrepresent major efforts to shape human-centered and human rights-based\nframeworks for society. Yet, as these laws are translated into practice,\nchallenges emerge at the intersection of technology, law, and design. This\npaper presents a qualitative case study examining how designers act as\nmediators between abstract legal requirements and real-world digital\nexperiences for users, focusing on the design of content reporting mechanisms\nunder Article 16 of the DSA.\n  Through an expert workshop with professional designers from diverse fields\n(N=9), we explore how legal obligations are interpreted by designers and\nreflected in discussions and design solutions. Our findings resonate with\nprevious research on the design of reporting mechanisms and dark patterns,\nhighlighting how UX design choices can mislead or hinder users' decision-making\nand therefore also highlighting the crucial role of design decisions.\n  We show how participatory design methods can bridge disciplinary divides,\nmaking legal obligations accessible in compliance fostering design solutions.\n  By using legal design as a lens, we argue that the co-creation of digital\nregulations and user experience is a core site for digital humanism; where\ndesigners, engineers, and legal scholars must collaborate to ensure that\nsystems uphold legal standards to address the challenge the regulation poses to\nthese disciplines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21842v1", "cate": "cs.CY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21941", "title": "Hierarchical Game-Based Multi-Agent Decision-Making for Autonomous Vehicles", "authors": ["Mushuang Liu", "Yan Wan", "Frank Lewis", "Subramanya Nageshrao", "H. Eric Tseng", "Dimitar Filev"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      12 pages, 20 figures, 1 algorithm", "url": "http://arxiv.org/abs/2507.21941v1", "summary": "This paper develops a game-theoretic decision-making framework for autonomous\ndriving in multi-agent scenarios. A novel hierarchical game-based decision\nframework is developed for the ego vehicle. This framework features an\ninteraction graph, which characterizes the interaction relationships between\nthe ego and its surrounding traffic agents (including AVs, human driven\nvehicles, pedestrians, and bicycles, and others), and enables the ego to\nsmartly select a limited number of agents as its game players. Compared to the\nstandard multi-player games, where all surrounding agents are considered as\ngame players, the hierarchical game significantly reduces the computational\ncomplexity. In addition, compared to pairwise games, the most popular approach\nin the literature, the hierarchical game promises more efficient decisions for\nthe ego (in terms of less unnecessary waiting and yielding). To further reduce\nthe computational cost, we then propose an improved hierarchical game, which\ndecomposes the hierarchical game into a set of sub-games. Decision safety and\nefficiency are analyzed in both hierarchical games. Comprehensive simulation\nstudies are conducted to verify the effectiveness of the proposed frameworks,\nwith an intersection-crossing scenario as a case study.", "comment": "12 pages, 20 figures, 1 algorithm", "pdf_url": "http://arxiv.org/pdf/2507.21941v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21635", "title": "Impact of Phase Noise and Power Amplifier Non-Linearities on Downlink Cell-Free Massive MIMO-OFDM Systems", "authors": ["Özlem Tuğfe Demir", "Emil Björnson"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, presented at IEEE SmartNets 2025", "url": "http://arxiv.org/abs/2507.21635v1", "summary": "Cell-free massive MIMO (multiple-input multiple-output) is a key enabler for\nthe sixth generation (6G) of mobile networks, offering significant spectral and\nenergy efficiency gains through user-centric operation of distributed access\npoints (APs). However, its reliance on low-cost APs introduces inevitable\nhardware impairments, whose combined impact on wideband downlink systems\nremains unexplored when analyzed using behavioral models. This paper presents a\ncomprehensive analysis of the downlink spectral efficiency (SE) in cell-free\nmassive MIMO-OFDM systems under practical hardware impairments, including phase\nnoise and third-order power amplifier nonlinearities. Both centralized and\ndistributed precoding strategies are examined. By leveraging the Bussgang\ndecomposition, we derive an SE expression and quantify the relative impact of\nimpairments through simulations. Our results reveal that phase noise causes\nmore severe degradation than power amplifier distortions, especially in\ndistributed operation, highlighting the need for future distortion-aware\nprecoding designs.", "comment": "6 pages, 3 figures, presented at IEEE SmartNets 2025", "pdf_url": "http://arxiv.org/pdf/2507.21635v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21816", "title": "Control Copy-Paste: Controllable Diffusion-Based Augmentation Method for Remote Sensing Few-Shot Object Detection", "authors": ["Yanxing Liu", "Jiancheng Pan", "Bingchen Zhang"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      5 Pages, 3 figures", "url": "http://arxiv.org/abs/2507.21816v1", "summary": "Few-shot object detection (FSOD) for optical remote sensing images aims to\ndetect rare objects with only a few annotated bounding boxes. The limited\ntraining data makes it difficult to represent the data distribution of\nrealistic remote sensing scenes, which results in the notorious overfitting\nproblem. Current researchers have begun to enhance the diversity of few-shot\nnovel instances by leveraging diffusion models to solve the overfitting\nproblem. However, naively increasing the diversity of objects is insufficient,\nas surrounding contexts also play a crucial role in object detection, and in\ncases where the object diversity is sufficient, the detector tends to overfit\nto monotonous contexts. Accordingly, we propose Control Copy-Paste, a\ncontrollable diffusion-based method to enhance the performance of FSOD by\nleveraging diverse contextual information. Specifically, we seamlessly inject a\nfew-shot novel objects into images with diverse contexts by a conditional\ndiffusion model. We also develop an orientation alignment strategy to mitigate\nthe integration distortion caused by varying aspect ratios of instances.\nExperiments on the public DIOR dataset demonstrate that our method can improve\ndetection performance by an average of 10.76%.", "comment": "5 Pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.21816v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21823", "title": "An Agentic AI for a New Paradigm in Business Process Development", "authors": ["Mohammad Azarijafari", "Luisa Mich", "Michele Missikoff"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21823v1", "summary": "Artificial Intelligence agents represent the next major revolution in the\ncontinuous technological evolution of industrial automation. In this paper, we\nintroduce a new approach for business process design and development that\nleverages the capabilities of Agentic AI. Departing from the traditional\ntask-based approach to business process design, we propose an agent-based\nmethod, where agents contribute to the achievement of business goals,\nidentified by a set of business objects. When a single agent cannot fulfill a\ngoal, we have a merge goal that can be achieved through the collaboration of\nmultiple agents. The proposed model leads to a more modular and intelligent\nbusiness process development by organizing it around goals, objects, and\nagents. As a result, this approach enables flexible and context-aware\nautomation in dynamic industrial environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21823v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21710", "title": "PREIG: Physics-informed and Reinforcement-driven Interpretable GRU for Commodity Demand Forecasting", "authors": ["Hongwei Ma", "Junbin Gao", "Minh-Ngoc Tran"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21710v1", "summary": "Accurately forecasting commodity demand remains a critical challenge due to\nvolatile market dynamics, nonlinear dependencies, and the need for economically\nconsistent predictions. This paper introduces PREIG, a novel deep learning\nframework tailored for commodity demand forecasting. The model uniquely\nintegrates a Gated Recurrent Unit (GRU) architecture with physics-informed\nneural network (PINN) principles by embedding a domain-specific economic\nconstraint: the negative elasticity between price and demand. This constraint\nis enforced through a customized loss function that penalizes violations of the\nphysical rule, ensuring that model predictions remain interpretable and aligned\nwith economic theory. To further enhance predictive performance and stability,\nPREIG incorporates a hybrid optimization strategy that couples NAdam and L-BFGS\nwith Population-Based Training (POP). Experiments across multiple commodities\ndatasets demonstrate that PREIG significantly outperforms traditional\neconometric models (ARIMA,GARCH) and deep learning baselines (BPNN,RNN) in both\nRMSE and MAPE. When compared with GRU,PREIG maintains good explainability while\nstill performing well in prediction. By bridging domain knowledge, optimization\ntheory and deep learning, PREIG provides a robust, interpretable, and scalable\nsolution for high-dimensional nonlinear time series forecasting in economy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21710v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21619", "title": "EMIT: Enhancing MLLMs for Industrial Anomaly Detection via Difficulty-Aware GRPO", "authors": ["Wei Guan", "Jun Lan", "Jian Cao", "Hao Tan", "Huijia Zhu", "Weiqiang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21619v1", "summary": "Industrial anomaly detection (IAD) plays a crucial role in maintaining the\nsafety and reliability of manufacturing systems. While multimodal large\nlanguage models (MLLMs) show strong vision-language reasoning abilities, their\neffectiveness in IAD remains limited without domain-specific adaptation. In\nthis work, we propose EMIT, a unified framework that enhances MLLMs for IAD via\ndifficulty-aware group relative policy optimization (GRPO). EMIT constructs a\nmulti-task IAD dataset and utilizes GPT-generated object text descriptions to\ncompensate for missing defective images. For few-shot anomaly detection, it\nintegrates a soft prompt and heatmap-guided contrastive embeddings derived from\npatch-level comparisons. To better handle difficult data samples, i.e., cases\nwhere the MLLM struggles to generate correct answers, we propose a\ndifficulty-aware GRPO that extends the original GRPO by incorporating a\nresponse resampling strategy to ensure the inclusion of correct answers in the\nsampled responses, as well as an advantage reweighting mechanism to strengthen\nlearning from such difficult data samples. Extensive experiments on the MMAD\nbenchmark demonstrate that EMIT significantly enhances the IAD performance of\nMLLMs, achieving an average improvement of 7.77\\% over the base model\n(InternVL3-8B) across seven tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21619v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21063", "title": "Make Silence Speak for Itself: a multi-modal learning analytic approach with neurophysiological data", "authors": ["Mingxuan Gao", "Jingjing Chen", "Yun Long", "Xiaomeng Xu", "Yu Zhang"], "categories": ["q-bio.NC", "cs.CY"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      25 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21063v1", "summary": "Background: Silence is a common phenomenon in classrooms, yet its implicit\nnature limits a clear understanding of students' underlying learning statuses.\nAim: This study proposed a nuanced framework to classify classroom silence\nbased on class events and student status, and examined neurophysiological\nmarkers to reveal similarities and differences in silent states across\nachievement groups. Sample: The study involved 54 middle school students during\n34 math lessons, with simultaneous recordings of electroencephalogram (EEG),\nelectrodermal activity (EDA), and heart rate signals, alongside video coding of\nclassroom behaviors. Results: We found that high-achieving students showed no\nsignificant difference in mean EDA features between strategic silence (i.e.,\nstudents choose silence deliberately) and active speaking during open\nquestioning but exhibited higher EEG high-frequency relative power spectral\ndensity (RPSD) during strategic silence. In structural silence (i.e., students\nmaintain silence following an external command) during directed questioning,\nthey demonstrated significantly higher heart rates while listening to lectures\ncompared to group activities, indicating heightened engagement. Both high- and\nmedium-achieving students displayed elevated heart rates and EDA tonic\ncomponents in structural silence during questioning compared to teaching.\nFurthermore, high-achieving students exhibited lower high-frequency RPSD during\nstructural silence than strategic silence, a pattern not observed in other\ngroups, highlighting group heterogeneity. Conclusions: The findings contribute\nto validating the complexity of silence, challenge its traditional association\nwith passivity, and offer a novel classification framework along with\npreliminary empirical evidence to deepen the understanding of silent learning\nbehaviors in classroom contexts.", "comment": "25 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21063v1", "cate": "q-bio.NC", "date": "2025-05-23", "updated": "2025-05-23"}
{"id": "2507.22022", "title": "Planning Persuasive Trajectories Based on a Leader-Follower Game Model", "authors": ["Chaozhe R. He", "Yichen Dong", "Nan Li"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      To appear at MECC 2025 ( this https URL )", "url": "http://arxiv.org/abs/2507.22022v1", "summary": "We propose a framework that enables autonomous vehicles (AVs) to proactively\nshape the intentions and behaviors of interacting human drivers. The framework\nemploys a leader-follower game model with an adaptive role mechanism to predict\nhuman interaction intentions and behaviors. It then utilizes a branch model\npredictive control (MPC) algorithm to plan the AV trajectory, persuading the\nhuman to adopt the desired intention. The proposed framework is demonstrated in\nan intersection scenario. Simulation results illustrate the effectiveness of\nthe framework for generating persuasive AV trajectories despite uncertainties.", "comment": "To appear at MECC 2025 (https://mecc2025.a2c2.org/)", "pdf_url": "http://arxiv.org/pdf/2507.22022v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21696", "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN", "authors": ["Abdelaziz Salama", "Zeinab Nezami", "Mohammed M. H. Qazzaz", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21696v1", "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21696v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21863", "title": "VidFuncta: Towards Generalizable Neural Representations for Ultrasound Videos", "authors": ["Julia Wolleb", "Florentin Bieder", "Paul Friedrich", "Hemant D. Tagare", "Xenophon Papademetris"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted 6th International Workshop of Advances in Simplifying Medical UltraSound (ASMUS) to be held at MICCAI 2025", "url": "http://arxiv.org/abs/2507.21863v1", "summary": "Ultrasound is widely used in clinical care, yet standard deep learning\nmethods often struggle with full video analysis due to non-standardized\nacquisition and operator bias. We offer a new perspective on ultrasound video\nanalysis through implicit neural representations (INRs). We build on Functa, an\nINR framework in which each image is represented by a modulation vector that\nconditions a shared neural network. However, its extension to the temporal\ndomain of medical videos remains unexplored. To address this gap, we propose\nVidFuncta, a novel framework that leverages Functa to encode variable-length\nultrasound videos into compact, time-resolved representations. VidFuncta\ndisentangles each video into a static video-specific vector and a sequence of\ntime-dependent modulation vectors, capturing both temporal dynamics and\ndataset-level redundancies. Our method outperforms 2D and 3D baselines on video\nreconstruction and enables downstream tasks to directly operate on the learned\n1D modulation vectors. We validate VidFuncta on three public ultrasound video\ndatasets -- cardiac, lung, and breast -- and evaluate its downstream\nperformance on ejection fraction prediction, B-line detection, and breast\nlesion classification. These results highlight the potential of VidFuncta as a\ngeneralizable and efficient representation framework for ultrasound videos. Our\ncode is publicly available under\nhttps://github.com/JuliaWolleb/VidFuncta_public.", "comment": "Accepted 6th International Workshop of Advances in Simplifying\n  Medical UltraSound (ASMUS) to be held at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.21863v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20624", "title": "Hyperbolic Embeddings for Order-Aware Classification of Audio Effect Chains", "authors": ["Aogu Wada", "Tomohiko Nakamura", "Hiroshi Saruwatari"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures, accepted for the 28th International Conference on Digital Audio Effects (DAFx25)", "url": "http://arxiv.org/abs/2507.20624v1", "summary": "Audio effects (AFXs) are essential tools in music production, frequently\napplied in chains to shape timbre and dynamics. The order of AFXs in a chain\nplays a crucial role in determining the final sound, particularly when\nnon-linear (e.g., distortion) or time-variant (e.g., chorus) processors are\ninvolved. Despite its importance, most AFX-related studies have primarily\nfocused on estimating effect types and their parameters from a wet signal. To\naddress this gap, we formulate AFX chain recognition as the task of jointly\nestimating AFX types and their order from a wet signal. We propose a\nneural-network-based method that embeds wet signals into a hyperbolic space and\nclassifies their AFX chains. Hyperbolic space can represent tree-structured\ndata more efficiently than Euclidean space due to its exponential expansion\nproperty. Since AFX chains can be represented as trees, with AFXs as nodes and\nedges encoding effect order, hyperbolic space is well-suited for modeling the\nexponentially growing and non-commutative nature of ordered AFX combinations,\nwhere changes in effect order can result in different final sounds. Experiments\nusing guitar sounds demonstrate that, with an appropriate curvature, the\nproposed method outperforms its Euclidean counterpart. Further analysis based\non AFX type and chain length highlights the effectiveness of the proposed\nmethod in capturing AFX order.", "comment": "7 pages, 3 figures, accepted for the 28th International Conference on\n  Digital Audio Effects (DAFx25)", "pdf_url": "http://arxiv.org/pdf/2507.20624v1", "cate": "cs.SD", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21830", "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework", "authors": ["Kuiye Ding", "Fanda Fan", "Yao Wang", "Ruijie jian", "Xiaorui Wang", "Luqi Gong", "Yishan Jiang", "Chunjie Luo an Jianfeng Zhan"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)", "url": "http://arxiv.org/abs/2507.21830v2", "summary": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.", "comment": "This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)", "pdf_url": "http://arxiv.org/pdf/2507.21830v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21720", "title": "Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins", "authors": ["Gang Wang", "Peng Hu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21720v1", "summary": "Hydrofluoroolefins are considered the most promising next-generation\nrefrigerants due to their extremely low global warming potential values, which\ncan effectively mitigate the global warming effect. However, the lack of\nreliable thermodynamic data hinders the discovery and application of newer and\nsuperior hydrofluoroolefin refrigerants. In this work, integrating the\nstrengths of theoretical method and data-driven method, we proposed a neural\nnetwork extended corresponding state model to predict the residual\nthermodynamic properties of hydrofluoroolefin refrigerants. The innovation is\nthat the fluids are characterized through their microscopic molecular\nstructures by the inclusion of graph neural network module and the specialized\ndesign of model architecture to enhance its generalization ability. The\nproposed model is trained using the highly accurate data of available known\nfluids, and evaluated via the leave-one-out cross-validation method. Compared\nto conventional extended corresponding state models or cubic equation of state,\nthe proposed model shows significantly improved accuracy for density and energy\nproperties in liquid and supercritical regions, with average absolute deviation\nof 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for\nresidual entropy, 1.85% and 1.34% for residual enthalpy. These results\ndemonstrate the effectiveness of embedding physics knowledge into the machine\nlearning model. The proposed neural network extended corresponding state model\nis expected to significantly accelerate the discovery of novel\nhydrofluoroolefin refrigerants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21720v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21627", "title": "GuidPaint: Class-Guided Image Inpainting with Diffusion Models", "authors": ["Qimin Wang", "Xinda Liu", "Guohua Geng"], "categories": ["cs.CV", "I.4.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21627v1", "summary": "In recent years, diffusion models have been widely adopted for image\ninpainting tasks due to their powerful generative capabilities, achieving\nimpressive results. Existing multimodal inpainting methods based on diffusion\nmodels often require architectural modifications and retraining, resulting in\nhigh computational cost. In contrast, context-aware diffusion inpainting\nmethods leverage the model's inherent priors to adjust intermediate denoising\nsteps, enabling high-quality inpainting without additional training and\nsignificantly reducing computation. However, these methods lack fine-grained\ncontrol over the masked regions, often leading to semantically inconsistent or\nvisually implausible content. To address this issue, we propose GuidPaint, a\ntraining-free, class-guided image inpainting framework. By incorporating\nclassifier guidance into the denoising process, GuidPaint enables precise\ncontrol over intermediate generations within the masked areas, ensuring both\nsemantic consistency and visual realism. Furthermore, it integrates stochastic\nand deterministic sampling, allowing users to select preferred intermediate\nresults and deterministically refine them. Experimental results demonstrate\nthat GuidPaint achieves clear improvements over existing context-aware\ninpainting methods in both qualitative and quantitative evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21627v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21134", "title": "TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law", "authors": ["Zheng Hui", "Yijiang River Dong", "Ehsan Shareghi", "Nigel Collier"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21134v1", "summary": "As large language models (LLMs) are increasingly deployed in high-risk\ndomains such as law, finance, and medicine, systematically evaluating their\ndomain-specific safety and compliance becomes critical. While prior work has\nlargely focused on improving LLM performance in these domains, it has often\nneglected the evaluation of domain-specific safety risks. To bridge this gap,\nwe first define domain-specific safety principles for LLMs based on the AMA\nPrinciples of Medical Ethics, the ABA Model Rules of Professional Conduct, and\nthe CFA Institute Code of Ethics. Building on this foundation, we introduce\nTrident-Bench, a benchmark specifically targeting LLM safety in the legal,\nfinancial, and medical domains. We evaluated 19 general-purpose and\ndomain-specialized models on Trident-Bench and show that it effectively reveals\nkey safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic\nexpectations, whereas domain-specialized models often struggle with subtle\nethical nuances. This highlights an urgent need for finer-grained\ndomain-specific safety improvements. By introducing Trident-Bench, our work\nprovides one of the first systematic resources for studying LLM safety in law\nand finance, and lays the groundwork for future research aimed at reducing the\nsafety risks of deploying LLMs in professionally regulated fields. Code and\nbenchmark will be released at: https://github.com/zackhuiiiii/TRIDENT", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21134v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21125", "title": "RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline", "authors": ["Karan Mirhosseini", "Arya Aftab", "Alireza Sheikh"], "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2507.21125v1", "summary": "In an era of radical technology transformations, technology maps play a\ncrucial role in enhancing decision making. These maps heavily rely on automated\nmethods of technology extraction. This paper introduces Retrieval Augmented\nTechnology Extraction (RATE), a Large Language Model (LLM) based pipeline for\nautomated technology extraction from scientific literature. RATE combines\nRetrieval Augmented Generation (RAG) with multi-definition LLM-based\nvalidation. This hybrid method results in high recall in candidate generation\nalongside with high precision in candidate filtering. While the pipeline is\ndesigned to be general and widely applicable, we demonstrate its use on 678\nresearch articles focused on Brain-Computer Interfaces (BCIs) and Extended\nReality (XR) as a case study. Consequently, The validated technology terms by\nRATE were mapped into a co-occurrence network, revealing thematic clusters and\nstructural features of the research landscape. For the purpose of evaluation, a\ngold standard dataset of technologies in 70 selected random articles had been\ncurated by the experts. In addition, a technology extraction model based on\nBidirectional Encoder Representations of Transformers (BERT) was used as a\ncomparative method. RATE achieved F1-score of 91.27%, Significantly\noutperforming BERT with F1-score of 53.73%. Our findings highlight the promise\nof definition-driven LLM methods for technology extraction and mapping. They\nalso offer new insights into emerging trends within the BCI-XR field. The\nsource code is available https://github.com/AryaAftab/RATE", "comment": "9 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.21125v1", "cate": "cs.IR", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.21698", "title": "EcoFL: Resource Allocation for Energy-Efficient Federated Learning in Multi-RAT ORAN Networks", "authors": ["Abdelaziz Salama", "Mohammed M. H. Qazzaz", "Syed Danial Ali Shah", "Maryam Hafeez", "Syed Ali Zaidi", "Hamed Ahmadi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21698v1", "summary": "Federated Learning (FL) enables distributed model training on edge devices\nwhile preserving data privacy. However, FL deployments in wireless networks\nface significant challenges, including communication overhead, unreliable\nconnectivity, and high energy consumption, particularly in dynamic\nenvironments. This paper proposes EcoFL, an integrated FL framework that\nleverages the Open Radio Access Network (ORAN) architecture with multiple Radio\nAccess Technologies (RATs) to enhance communication efficiency and ensure\nrobust FL operations. EcoFL implements a two-stage optimisation approach: an\nRL-based rApp for dynamic RAT selection that balances energy efficiency with\nnetwork performance, and a CNN-based xApp for near real-time resource\nallocation with adaptive policies. This coordinated approach significantly\nenhances communication resilience under fluctuating network conditions.\nExperimental results demonstrate competitive FL model performance with 19\\%\nlower power consumption compared to baseline approaches, highlighting\nsubstantial potential for scalable, energy-efficient collaborative learning\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21698v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22017", "title": "Cyst-X: AI-Powered Pancreatic Cancer Risk Prediction from Multicenter MRI in Centralized and Federated Learning", "authors": ["Hongyi Pan", "Gorkem Durak", "Elif Keles", "Deniz Seyithanoglu", "Zheyuan Zhang", "Alpay Medetalibeyoglu", "Halil Ertugrul Aktas", "Andrea Mia Bejar", "Ziliang Hong", "Yavuz Taktak", "Gulbiz Dagoglu Kartal", "Mehmet Sukru Erturk", "Timurhan Cebeci", "Maria Jaramillo Gonzalez", "Yury Velichko", "Lili Zhao", "Emil Agarunov", "Federica Proietto Salanitri", "Concetto Spampinato", "Pallavi Tiwari", "Ziyue Xu", "Sachin Jambawalikar", "Ivo G. Schoots", "Marco J. Bruno", "Chenchang Huang", "Candice Bolan", "Tamas Gonda", "Frank H. Miller", "Rajesh N. Keswani", "Michael B. Wallace", "Ulas Bagci"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22017v1", "summary": "Pancreatic cancer is projected to become the second-deadliest malignancy in\nWestern countries by 2030, highlighting the urgent need for better early\ndetection. Intraductal papillary mucinous neoplasms (IPMNs), key precursors to\npancreatic cancer, are challenging to assess with current guidelines, often\nleading to unnecessary surgeries or missed malignancies. We present Cyst-X, an\nAI framework that predicts IPMN malignancy using multicenter MRI data,\nleveraging MRI's superior soft tissue contrast over CT. Trained on 723 T1- and\n738 T2-weighted scans from 764 patients across seven institutions, our models\n(AUC=0.82) significantly outperform both Kyoto guidelines (AUC=0.75) and expert\nradiologists. The AI-derived imaging features align with known clinical markers\nand offer biologically meaningful insights. We also demonstrate strong\nperformance in a federated learning setting, enabling collaborative training\nwithout sharing patient data. To promote privacy-preserving AI development and\nimprove IPMN risk stratification, the Cyst-X dataset is released as the first\nlarge-scale, multi-center pancreatic cysts MRI dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22017v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21138", "title": "TTS-1 Technical Report", "authors": ["Oleg Atamanenko", "Anna Chalova", "Joseph Coombes", "Nikki Cope", "Phillip Dang", "Zhifeng Deng", "Jimmy Du", "Michael Ermolenko", "Feifan Fan", "Yufei Feng", "Cheryl Fichter", "Pavel Filimonov", "Louis Fischer", "Kylan Gibbs", "Valeria Gusarova", "Pavel Karpik", "Andreas Assad Kottner", "Ian Lee", "Oliver Louie", "Jasmine Mai", "Mikhail Mamontov", "Suri Mao", "Nurullah Morshed", "Igor Poletaev", "Florin Radu", "Dmytro Semernia", "Evgenii Shingarev", "Vikram Sivaraja", "Peter Skirko", "Rinat Takhautdinov", "Robert Villahermosa", "Jean Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures. For associated modeling and training code, see this https URL", "url": "http://arxiv.org/abs/2507.21138v1", "summary": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.", "comment": "20 pages, 10 figures. For associated modeling and training code, see\n  https://github.com/inworld-ai/tts", "pdf_url": "http://arxiv.org/pdf/2507.21138v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.21846", "title": "Probabilistic Active Goal Recognition", "authors": ["Chenyuan Zhang", "Cristian Rojas Cardenas", "Hamid Rezatofighi", "Mor Vered", "Buser Say"], "categories": ["cs.AI", "cs.SC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by KR2025", "url": "http://arxiv.org/abs/2507.21846v1", "summary": "In multi-agent environments, effective interaction hinges on understanding\nthe beliefs and intentions of other agents. While prior work on goal\nrecognition has largely treated the observer as a passive reasoner, Active Goal\nRecognition (AGR) focuses on strategically gathering information to reduce\nuncertainty. We adopt a probabilistic framework for Active Goal Recognition and\npropose an integrated solution that combines a joint belief update mechanism\nwith a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan\nefficiently and infer the actor's hidden goal without requiring domain-specific\nknowledge. Through comprehensive empirical evaluation in a grid-based domain,\nwe show that our joint belief update significantly outperforms passive goal\nrecognition, and that our domain-independent MCTS performs comparably to our\nstrong domain-specific greedy baseline. These results establish our solution as\na practical and robust framework for goal inference, advancing the field toward\nmore interactive and adaptive multi-agent systems.", "comment": "Accepted by KR2025", "pdf_url": "http://arxiv.org/pdf/2507.21846v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21738", "title": "Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation", "authors": ["Huiqiang Chen", "Tianqing Zhu", "Xin Yu", "Wanlei Zhou"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by IJCAI 2025", "url": "http://arxiv.org/abs/2507.21738v1", "summary": "Machine unlearning aims to remove the influence of specific samples from a\ntrained model. A key challenge in this process is over-unlearning, where the\nmodel's performance on the remaining data significantly drops due to the change\nin the model's parameters. Existing unlearning algorithms depend on the\nremaining data to prevent this issue. As such, these methods are inapplicable\nin a more practical scenario, where only the unlearning samples are available\n(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to\nfill this gap. Our approach offers three key innovations: (1) we approximate\nthe inaccessible remaining data by generating adversarial samples; (2)\nleveraging the generated samples, we pinpoint a specific subspace to perform\nthe unlearning process, therefore preventing over-unlearning in the challenging\nzero-shot scenario; and (3) we consider the influence of the unlearning process\non the remaining samples and design an influence-based pseudo-labeling\nstrategy. As a result, our method further improves the model's performance\nafter unlearning. The proposed method holds a theoretical guarantee, and\nexperiments on various benchmarks validate the effectiveness and superiority of\nour proposed method over several baselines.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.21738v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21649", "title": "The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM", "authors": ["Shibo Gao", "Peipei Yang", "Haiyang Guo", "Yangyang Liu", "Yi Chen", "Shuai Li", "Han Zhu", "Jian Xu", "Xu-Yao Zhang", "Linlin Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21649v1", "summary": "Video anomaly detection (VAD) aims to identify and ground anomalous behaviors\nor events in videos, serving as a core technology in the fields of intelligent\nsurveillance and public safety. With the advancement of deep learning, the\ncontinuous evolution of deep model architectures has driven innovation in VAD\nmethodologies, significantly enhancing feature representation and scene\nadaptability, thereby improving algorithm generalization and expanding\napplication boundaries. More importantly, the rapid development of multi-modal\nlarge language (MLLMs) and large language models (LLMs) has introduced new\nopportunities and challenges to the VAD field. Under the support of MLLMs and\nLLMs, VAD has undergone significant transformations in terms of data\nannotation, input modalities, model architectures, and task objectives. The\nsurge in publications and the evolution of tasks have created an urgent need\nfor systematic reviews of recent advancements. This paper presents the first\ncomprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing\nan in-depth discussion of the changes occurring in the VAD field in the era of\nlarge models and their underlying causes. Additionally, this paper proposes a\nunified framework that encompasses both deep neural network (DNN)-based and\nLLM-based VAD methods, offering a thorough analysis of the new VAD paradigms\nempowered by LLMs, constructing a classification system, and comparing their\nstrengths and weaknesses. Building on this foundation, this paper focuses on\ncurrent VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of\ntechnological advancements and existing bottlenecks, this paper distills key\nchallenges and outlines future research directions, offering guidance for the\nVAD community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21649v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21815", "title": "HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs", "authors": ["Kaixuan Wang", "Chenxin Diao", "Jason T. Jacques", "Zhongliang Guo", "Shuai Zhao"], "categories": ["cs.CL", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 5 figures, 12 tables, a dataset", "url": "http://arxiv.org/abs/2507.21815v1", "summary": "Millions of individuals' well-being are challenged by the harms of substance\nuse. Harm reduction as a public health strategy is designed to improve their\nhealth outcomes and reduce safety risks. Some large language models (LLMs) have\ndemonstrated a decent level of medical knowledge, promising to address the\ninformation needs of people who use drugs (PWUD). However, their performance in\nrelevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark\ndesigned to evaluate LLM's accuracy and safety risks in harm reduction\ninformation provision. The benchmark dataset HRIP-Basic has 2,160\nquestion-answer-evidence pairs. The scope covers three tasks: checking safety\nboundaries, providing quantitative values, and inferring polysubstance use\nrisks. We build the Instruction and RAG schemes to evaluate model behaviours\nbased on their inherent knowledge and the integration of domain knowledge. Our\nresults indicate that state-of-the-art LLMs still struggle to provide accurate\nharm reduction information, and sometimes, carry out severe safety risks to\nPWUD. The use of LLMs in harm reduction contexts should be cautiously\nconstrained to avoid inducing negative health outcomes. WARNING: This paper\ncontains illicit content that potentially induces harms.", "comment": "15 pages, 5 figures, 12 tables, a dataset", "pdf_url": "http://arxiv.org/pdf/2507.21815v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21281", "title": "Sliding Mode Control for Uncertain Systems with Time-Varying Delays via Predictor Feedback and Super-Twisting Observer", "authors": ["Hardy Pinto", "Tiago Roux Oliveira", "Liu Hsu"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      15 pages, 8 figures", "url": "http://arxiv.org/abs/2507.21281v1", "summary": "This paper introduces a novel stabilization control strategy for linear\ntime-invariant systems affected by known time-varying measurement delays and\nmatched unknown nonlinear disturbances, which may encompass actuator faults. It\nis considered that part of the state vector is not available for real-time\nmeasurement. To address this, the proposed approach combines an open-loop\npredictor with a state observer designed using the Super-Twisting Algorithm,\naiming to compensate for the delays and estimate the unmeasured state\ncomponents. Specifically, the nonlinear observer-based framework enables the\nreconstruction of unmodeled fault signals without assuming that they originate\nfrom a known exogenous system, offering robustness against parametric\nuncertainties. Meanwhile, the predictor forwards the delayed output in time.\nSubsequently, a sliding mode control law is formulated to enforce an ideal\nsliding mode and ensure global stabilization, even under a broader class of\nperturbations, unmodeled disturbances, parametric uncertainties, and delays,\nowing to the integration of the Super-Twisting observer. Numerical simulations\nillustrate the efficiency of the proposed approach.", "comment": "15 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.21281v1", "cate": "math.OC", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21704", "title": "Affine Frequency Division Multiplexing (AFDM) for 6G: Properties, Features, and Challenges", "authors": ["Hyeon Seok Rou", "Kuranage Roche Rayan Ranasinghe", "Vincent Savaux", "Giuseppe Thadeu Freitas de Abreu", "David González G.", "Christos Masouros"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21704v1", "summary": "Affine frequency division multiplexing (AFDM) is an emerging waveform\ncandidate for future sixth generation (6G) systems offering a range of\npromising features, such as enhanced robustness in heterogeneous and\nhigh-mobility environments, as well as inherent suitability for integrated\nsensing and communications (ISAC) applications. In addition, unlike other\ncandidates such as orthogonal time-frequency space (OTFS) modulation, AFDM\nprovides several unique advantages that strengthen its relevance to practical\ndeployment and standardization in 6G. Notably, as a natural generalization of\northogonal frequency division multiplexing (OFDM), strong backward\ncompatibility with existing conventional systems is guaranteed, while also\noffering novel possibilities in waveform design, for example to enable\nphysical-layer security through its inherent chirp parametrization. In all,\nthis article provides an overview of AFDM, emphasizing its suitability as a\ncandidate waveform for 6G standardization. First, we provide a concise\nintroduction to the fundamental properties and unique characteristics of AFDM,\nfollowed by highlights of its advantageous features, and finally a discussion\nof its potential and challenges in 6G standardization efforts and\nrepresentative requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21704v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22024", "title": "Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images", "authors": ["Yutao Hu", "Ying Zheng", "Shumei Miao", "Xiaolei Zhang", "Jiahao Xia", "Yaolei Qi", "Yiyang Zhang", "Yuting He", "Qian Chen", "Jing Ye", "Hongyan Qiao", "Xiuhua Hu", "Lei Xu", "Jiayin Zhang", "Hui Liu", "Minwen Zheng", "Yining Wang", "Daimin Zhang", "Ji Zhang", "Wenqi Shao", "Yun Liu", "Longjiang Zhang", "Guanyu Yang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22024v1", "summary": "Foundation models have demonstrated remarkable potential in medical domain.\nHowever, their application to complex cardiovascular diagnostics remains\nunderexplored. In this paper, we present Cardiac-CLIP, a multi-modal foundation\nmodel designed for 3D cardiac CT images. Cardiac-CLIP is developed through a\ntwo-stage pre-training strategy. The first stage employs a 3D masked\nautoencoder (MAE) to perform self-supervised representation learning from\nlarge-scale unlabeled volumetric data, enabling the visual encoder to capture\nrich anatomical and contextual features. In the second stage, contrastive\nlearning is introduced to align visual and textual representations,\nfacilitating cross-modal understanding. To support the pre-training, we collect\n16641 real clinical CT scans, supplemented by 114k publicly available data.\nMeanwhile, we standardize free-text radiology reports into unified templates\nand construct the pathology vectors according to diagnostic attributes, based\non which the soft-label matrix is generated to supervise the contrastive\nlearning process. On the other hand, to comprehensively evaluate the\neffectiveness of Cardiac-CLIP, we collect 6,722 real-clinical data from 12\nindependent institutions, along with the open-source data to construct the\nevaluation dataset. Specifically, Cardiac-CLIP is comprehensively evaluated\nacross multiple tasks, including cardiovascular abnormality classification,\ninformation retrieval and clinical analysis. Experimental results demonstrate\nthat Cardiac-CLIP achieves state-of-the-art performance across various\ndownstream tasks in both internal and external data. Particularly, Cardiac-CLIP\nexhibits great effectiveness in supporting complex clinical tasks such as the\nprospective prediction of acute coronary syndrome, which is notoriously\ndifficult in real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22024v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21331", "title": "A Deep Learning Automatic Speech Recognition Model for Shona Language", "authors": ["Leslie Wellington Sirora", "Mainford Mutandavari"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21331v1", "summary": "This study presented the development of a deep learning-based Automatic\nSpeech Recognition system for Shona, a low-resource language characterized by\nunique tonal and grammatical complexities. The research aimed to address the\nchallenges posed by limited training data, lack of labelled data, and the\nintricate tonal nuances present in Shona speech, with the objective of\nachieving significant improvements in recognition accuracy compared to\ntraditional statistical models. The research first explored the feasibility of\nusing deep learning to develop an accurate ASR system for Shona. Second, it\ninvestigated the specific challenges involved in designing and implementing\ndeep learning architectures for Shona speech recognition and proposed\nstrategies to mitigate these challenges. Lastly, it compared the performance of\nthe deep learning-based model with existing statistical models in terms of\naccuracy. The developed ASR system utilized a hybrid architecture consisting of\na Convolutional Neural Network for acoustic modelling and a Long Short-Term\nMemory network for language modelling. To overcome the scarcity of data, data\naugmentation techniques and transfer learning were employed. Attention\nmechanisms were also incorporated to accommodate the tonal nature of Shona\nspeech. The resulting ASR system achieved impressive results, with a Word Error\nRate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These\nmetrics indicated the potential of deep learning to enhance ASR accuracy for\nunder-resourced languages like Shona. This study contributed to the advancement\nof ASR technology for under-resourced languages like Shona, ultimately\nfostering improved accessibility and communication for Shona speakers\nworldwide.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21331v1", "cate": "cs.CL", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21848", "title": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21848v1", "summary": "Large Language Models (LLMs) have made remarkable progress in enhancing\nstep-by-step reasoning through reinforcement learning. However, the Group\nRelative Policy Optimization (GRPO) algorithm, which relies on sparse reward\nrules, often encounters the issue of identical rewards within groups, leading\nto the advantage collapse problem. Existing works typically address this\nchallenge from two perspectives: enforcing model reflection to enhance response\ndiversity, and introducing internal feedback to augment the training signal\n(advantage). In this work, we begin by analyzing the limitations of model\nreflection and investigating the policy entropy of responses at the\nfine-grained sample level. Based on our experimental findings, we propose the\nEDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage\nand \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the\nproblem of advantage collapse. Extensive experiments on several main reasoning\nbenchmarks demonstrate the effectiveness and superiority of our approach. It is\navailable at https://github.com/ZhangXJ199/EDGE-GRPO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21848v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21762", "title": "TempRe: Template generation for single and direct multi-step retrosynthesis", "authors": ["Nguyen Xuan-Vu", "Daniel P Armstrong", "Zlatko Jončev", "Philippe Schwaller"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21762v2", "summary": "Retrosynthesis planning remains a central challenge in molecular discovery\ndue to the vast and complex chemical reaction space. While traditional\ntemplate-based methods offer tractability, they suffer from poor scalability\nand limited generalization, and template-free generative approaches risk\ngenerating invalid reactions. In this work, we propose TempRe, a generative\nframework that reformulates template-based approaches as sequence generation,\nenabling scalable, flexible, and chemically plausible retrosynthesis. We\nevaluated TempRe across single-step and multi-step retrosynthesis tasks,\ndemonstrating its superiority over both template classification and\nSMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe\nachieves strong top-k route accuracy. Furthermore, we extend TempRe to direct\nmulti-step synthesis route generation, providing a lightweight and efficient\nalternative to conventional single-step and search-based approaches. These\nresults highlight the potential of template generative modeling as a powerful\nparadigm in computer-aided synthesis planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21762v2", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21665", "title": "Automated Detection of Antarctic Benthic Organisms in High-Resolution In Situ Imagery to Aid Biodiversity Monitoring", "authors": ["Cameron Trotter", "Huw Griffiths", "Tasnuva Ming Khan", "Rowan Whittle"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025's Joint Workshop on Marine Vision (ICCVW, CVAUI&AAMVEM). Main paper (11 pages, 3 figures, 3 tables) plus supplementary (7 pages, 5 figures, 2 tables)", "url": "http://arxiv.org/abs/2507.21665v1", "summary": "Monitoring benthic biodiversity in Antarctica is vital for understanding\necological change in response to climate-driven pressures. This work is\ntypically performed using high-resolution imagery captured in situ, though\nmanual annotation of such data remains laborious and specialised, impeding\nlarge-scale analysis. We present a tailored object detection framework for\nidentifying and classifying Antarctic benthic organisms in high-resolution\ntowed camera imagery, alongside the first public computer vision dataset for\nbenthic biodiversity monitoring in the Weddell Sea. Our approach addresses key\nchallenges associated with marine ecological imagery, including limited\nannotated data, variable object sizes, and complex seafloor structure. The\nproposed framework combines resolution-preserving patching, spatial data\naugmentation, fine-tuning, and postprocessing via Slicing Aided Hyper\nInference. We benchmark multiple object detection architectures and demonstrate\nstrong performance in detecting medium and large organisms across 25\nfine-grained morphotypes, significantly more than other works in this area.\nDetection of small and rare taxa remains a challenge, reflecting limitations in\ncurrent detection architectures. Our framework provides a scalable foundation\nfor future machine-assisted in situ benthic biodiversity monitoring research.", "comment": "Accepted to ICCV 2025's Joint Workshop on Marine Vision (ICCVW,\n  CVAUI&AAMVEM). Main paper (11 pages, 3 figures, 3 tables) plus supplementary\n  (7 pages, 5 figures, 2 tables)", "pdf_url": "http://arxiv.org/pdf/2507.21665v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21919", "title": "Training language models to be warm and empathetic makes them less reliable and more sycophantic", "authors": ["Lujain Ibrahim", "Franziska Sofia Hafner", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21919v2", "summary": "Artificial intelligence (AI) developers are increasingly building language\nmodels with warm and empathetic personas that millions of people now use for\nadvice, therapy, and companionship. Here, we show how this creates a\nsignificant trade-off: optimizing language models for warmth undermines their\nreliability, especially when users express vulnerability. We conducted\ncontrolled experiments on five language models of varying sizes and\narchitectures, training them to produce warmer, more empathetic responses, then\nevaluating them on safety-critical tasks. Warm models showed substantially\nhigher error rates (+10 to +30 percentage points) than their original\ncounterparts, promoting conspiracy theories, providing incorrect factual\ninformation, and offering problematic medical advice. They were also\nsignificantly more likely to validate incorrect user beliefs, particularly when\nuser messages expressed sadness. Importantly, these effects were consistent\nacross different model architectures, and occurred despite preserved\nperformance on standard benchmarks, revealing systematic risks that current\nevaluation practices may fail to detect. As human-like AI systems are deployed\nat an unprecedented scale, our findings indicate a need to rethink how we\ndevelop and oversee these systems that are reshaping human relationships and\nsocial interaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21919v2", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21543", "title": "On Policy Stochasticity in Mutual Information Optimal Control of Linear Systems", "authors": ["Shoju Enami", "Kenji Kashima"], "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2507.21543v1", "summary": "In recent years, mutual information optimal control has been proposed as an\nextension of maximum entropy optimal control. Both approaches introduce\nregularization terms to render the policy stochastic, and it is important to\ntheoretically clarify the relationship between the temperature parameter (i.e.,\nthe coefficient of the regularization term) and the stochasticity of the\npolicy. Unlike in maximum entropy optimal control, this relationship remains\nunexplored in mutual information optimal control. In this paper, we investigate\nthis relationship for a mutual information optimal control problem (MIOCP) of\ndiscrete-time linear systems. After extending the result of a previous study of\nthe MIOCP, we establish the existence of an optimal policy of the MIOCP, and\nthen derive the respective conditions on the temperature parameter under which\nthe optimal policy becomes stochastic and deterministic. Furthermore, we also\nderive the respective conditions on the temperature parameter under which the\npolicy obtained by an alternating optimization algorithm becomes stochastic and\ndeterministic. The validity of the theoretical results is demonstrated through\nnumerical experiments.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2507.21543v1", "cate": "math.OC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21879", "title": "CRB-Rate Tradeoff for Bistatic ISAC with Gaussian Information and Deterministic Sensing Signals", "authors": ["Xianxin Song", "Xianghao Yu", "Jie Xu", "Derrick Wing Kwan Ng"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages,6 figures", "url": "http://arxiv.org/abs/2507.21879v1", "summary": "In this paper, we investigate a bistatic integrated sensing and\ncommunications (ISAC) system, consisting of a multi-antenna base station (BS),\na multi-antenna sensing receiver, a single-antenna communication user (CU), and\na point target to be sensed. Specifically, the BS transmits a superposition of\nGaussian information and deterministic sensing signals. The BS aims to deliver\ninformation symbols to the CU, while the sensing receiver aims to estimate the\ntarget's direction-of-arrival (DoA) with respect to the sensing receiver by\nprocessing the echo signals. For the sensing receiver, we assume that only the\nsequences of the deterministic sensing signals and the covariance matrix of the\ninformation signals are perfectly known, whereas the specific realizations of\nthe information signals remain unavailable. Under this setup, we first derive\nthe corresponding Cram\\'er-Rao bounds (CRBs) for DoA estimation and propose\npractical estimators to accurately estimate the target's DoA. Subsequently, we\nformulate the transmit beamforming design as an optimization problem aiming to\nminimize the CRB, subject to a minimum signal-to-interference-plus-noise ratio\n(SINR) requirement at the CU and a maximum transmit power constraint at the BS.\nWhen the BS employs only Gaussian information signals, the resulting\nbeamforming optimization problem is convex, enabling the derivation of an\noptimal solution. In contrast, when both Gaussian information and deterministic\nsensing signals are transmitted, the resulting problem is non-convex and a\nlocally optimal solution is acquired by exploiting successive convex\napproximation (SCA). Finally, numerical results demonstrate that employing\nGaussian information signals leads to a notable performance degradation for\ntarget sensing and the proposed transmit beamforming design achieves a superior\nISAC performance boundary compared with various benchmark schemes.", "comment": "13 pages,6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21879v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22030", "title": "ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports", "authors": ["Mohammed Baharoon", "Luyang Luo", "Michael Moritz", "Abhinav Kumar", "Sung Eun Kim", "Xiaoman Zhang", "Miao Zhu", "Mahmoud Hussain Alabbad", "Maha Sbayel Alhazmi", "Neel P. Mistry", "Kent Ryan Kleinschmidt", "Brady Chrisler", "Sathvik Suryadevara", "Sri Sai Dinesh Jaliparthi", "Noah Michael Prudlo", "Mark David Marino", "Jeremy Palacio", "Rithvik Akula", "Hong-Yu Zhou", "Ibrahim Ethem Hamamci", "Scott J. Adams", "Hassan Rayhan AlOmaish", "Pranav Rajpurkar"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22030v1", "summary": "We present ReXGroundingCT, the first publicly available dataset to link\nfree-text radiology findings with pixel-level segmentations in 3D chest CT\nscans that is manually annotated. While prior datasets have relied on\nstructured labels or predefined categories, ReXGroundingCT captures the full\nexpressiveness of clinical language represented in free text and grounds it to\nspatially localized 3D segmentation annotations in volumetric imaging. This\naddresses a critical gap in medical AI: the ability to connect complex,\ndescriptive text, such as \"3 mm nodule in the left lower lobe\", to its precise\nanatomical location in three-dimensional space, a capability essential for\ngrounded radiology report generation systems. The dataset comprises 3,142\nnon-contrast chest CT scans paired with standardized radiology reports from the\nCT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to\nextract positive lung and pleural findings, which were then manually segmented\nby expert annotators. A total of 8,028 findings across 16,301 entities were\nannotated, with quality control performed by board-certified radiologists.\nApproximately 79% of findings are focal abnormalities, while 21% are non-focal.\nThe training set includes up to three representative segmentations per finding,\nwhile the validation and test sets contain exhaustive labels for each finding\nentity. ReXGroundingCT establishes a new benchmark for developing and\nevaluating sentence-level grounding and free-text medical segmentation models\nin chest CT. The dataset can be accessed at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22030v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21426", "title": "Relationship between objective and subjective perceptual measures of speech in individuals with head and neck cancer", "authors": ["Bence Mark Halpern", "Thomas Tienkamp", "Teja Rebernik", "Rob J. J. H. van Son", "Martijn Wieling", "Defne Abur", "Tomoki Toda"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5 pages, 1 figure, 1 table. Accepted at Interspeech 2025", "url": "http://arxiv.org/abs/2507.21426v1", "summary": "Meaningful speech assessment is vital in clinical phonetics and therapy\nmonitoring. This study examined the link between perceptual speech assessments\nand objective acoustic measures in a large head and neck cancer (HNC) dataset.\nTrained listeners provided ratings of intelligibility, articulation, voice\nquality, phonation, speech rate, nasality, and background noise on speech.\nStrong correlations were found between subjective intelligibility,\narticulation, and voice quality, likely due to a shared underlying cause of\nspeech symptoms in our speaker population. Objective measures of\nintelligibility and speech rate aligned with their subjective counterpart. Our\nresults suggest that a single intelligibility measure may be sufficient for the\nclinical monitoring of speakers treated for HNC using concomitant\nchemoradiation.", "comment": "5 pages, 1 figure, 1 table. Accepted at Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.21426v1", "cate": "cs.SD", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21872", "title": "MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors", "authors": ["Shouyi Lu", "Zihan Lin", "Chao Lu", "Huanran Wang", "Guirong Zhuo", "Lianqing Zheng"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21872v2", "summary": "Autonomous driving systems rely heavily on multimodal perception data to\nunderstand complex environments. However, the long-tailed distribution of\nreal-world data hinders generalization, especially for rare but safety-critical\nvehicle categories. To address this challenge, we propose MultiEditor, a\ndual-branch latent diffusion framework designed to edit images and LiDAR point\nclouds in driving scenarios jointly. At the core of our approach is introducing\n3D Gaussian Splatting (3DGS) as a structural and appearance prior for target\nobjects. Leveraging this prior, we design a multi-level appearance control\nmechanism--comprising pixel-level pasting, semantic-level guidance, and\nmulti-branch refinement--to achieve high-fidelity reconstruction across\nmodalities. We further propose a depth-guided deformable cross-modality\ncondition module that adaptively enables mutual guidance between modalities\nusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.\nExtensive experiments demonstrate that MultiEditor achieves superior\nperformance in visual and geometric fidelity, editing controllability, and\ncross-modality consistency. Furthermore, generating rare-category vehicle data\nwith MultiEditor substantially enhances the detection accuracy of perception\nmodels on underrepresented classes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21872v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21799", "title": "Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer", "authors": ["Xie Zhang", "Yina Wang", "Chenshu Wu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21799v1", "summary": "The empirical success of deep learning has spurred its application to the\nradio-frequency (RF) domain, leading to significant advances in Deep Wireless\nSensing (DWS). However, most existing DWS models function as black boxes with\nlimited interpretability, which hampers their generalizability and raises\nconcerns in security-sensitive physical applications. In this work, inspired by\nthe remarkable advances of white-box transformers, we present RF-CRATE, the\nfirst mathematically interpretable deep network architecture for RF sensing,\ngrounded in the principles of complex sparse rate reduction. To accommodate the\nunique RF signals, we conduct non-trivial theoretical derivations that extend\nthe original real-valued white-box transformer to the complex domain. By\nleveraging the CR-Calculus framework, we successfully construct a fully\ncomplex-valued white-box transformer with theoretically derived self-attention\nand residual multi-layer perceptron modules. Furthermore, to improve the\nmodel's ability to extract discriminative features from limited wireless data,\nwe introduce Subspace Regularization, a novel regularization strategy that\nenhances feature diversity, resulting in an average performance improvement of\n19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against\nseven baselines with multiple public and self-collected datasets involving\ndifferent RF signals. The results show that RF-CRATE achieves performance on\npar with thoroughly engineered black-box models, while offering full\nmathematical interpretability. More importantly, by extending CRATE to the\ncomplex domain, RF-CRATE yields substantial improvements, achieving an average\nclassification gain of 5.08% and reducing regression error by 10.34% across\ndiverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:\nhttps://github.com/rfcrate/RF_CRATE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21799v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21690", "title": "APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing", "authors": ["Sangmin Han", "Jinho Jeong", "Jinwoo Kim", "Seon Joo Kim"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21690v1", "summary": "Latent Diffusion Models (LDMs) are generally trained at fixed resolutions,\nlimiting their capability when scaling up to high-resolution images. While\ntraining-based approaches address this limitation by training on\nhigh-resolution datasets, they require large amounts of data and considerable\ncomputational resources, making them less practical. Consequently,\ntraining-free methods, particularly patch-based approaches, have become a\npopular alternative. These methods divide an image into patches and fuse the\ndenoising paths of each patch, showing strong performance on high-resolution\ngeneration. However, we observe two critical issues for patch-based approaches,\nwhich we call ``patch-level distribution shift\" and ``increased patch\nmonotonicity.\" To address these issues, we propose Adaptive Path Tracing (APT),\na framework that combines Statistical Matching to ensure patch distributions\nremain consistent in upsampled latents and Scale-aware Scheduling to deal with\nthe patch monotonicity. As a result, APT produces clearer and more refined\ndetails in high-resolution images. In addition, APT enables a shortcut\ndenoising process, resulting in faster sampling with minimal quality\ndegradation. Our experimental results confirm that APT produces more detailed\noutputs with improved inference speed, providing a practical approach to\nhigh-resolution image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21690v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2403.14707", "title": "Information Fusion in Multimodal IoT Systems for physical activity level monitoring", "authors": ["Mohsen Shirali", "Zahra Ahmadi", "Jose-Luis Bayo-Monton", "Zoe Valero-Ramon", "Carlos Fernandez-Llatas"], "categories": ["cs.CY", "68U35", "H.4.0; J.3; I.2.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.14707v3", "summary": "This study exploits information fusion in IoT systems and uses a clustering\nmethod to identify similarities in behaviours and key characteristics within\neach cluster. This approach facilitates early detection of behaviour changes\nand provides a more in-depth understanding of behaviour routines for continuous\nhealth monitoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.14707v3", "cate": "cs.CY", "date": "2024-03-17", "updated": "2025-07-28"}
{"id": "2507.21624", "title": "Adaptive Benders decomposition and enhanced SDDP for multistage stochastic programs with block-separable multistage recourse", "authors": ["Nicolò Mazzi", "Ken Mckinnon", "Hongyu Zhang"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21624v1", "summary": "This paper proposes an algorithm to efficiently solve multistage stochastic\nprograms with block separable recourse where each recourse problem is a\nmultistage stochastic program with stage-wise independent uncertainty. The\nalgorithm first decomposes the full problem into a reduced master problem and\nsubproblems using Adaptive Benders decomposition. The subproblems are then\nsolved by an enhanced SDDP. The enhancement includes (1) valid bounds at each\niteration, (2) a path exploration rule, (3) cut sharing among subproblems, and\n(4) guaranteed {\\delta}-optimal convergence. The cuts for the subproblems are\nthen shared by calling adaptive oracles. The key contribution of the paper is\nthe first algorithm for solving this class of problems. The algorithm is\ndemonstrated on a power system investment planning problem with multi-timescale\nuncertainty. The case study results show that (1) the proposed algorithm can\nefficiently solve this type of problem, (2) deterministic wind modelling\nunderestimate the objective function, and (3) stochastic modelling of wind\nleads to different investment decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21624v1", "cate": "math.OC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22013", "title": "Zak-OTFS Based Coded Random Access for Uplink mMTC", "authors": ["Alessandro Mirri", "Venkatesh Khammammetti", "Beyza Dabak", "Enrico Paolini", "Krishna Narayanan", "Robert Calderbank"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22013v1", "summary": "This paper proposes a grant-free coded random access (CRA) scheme for uplink\nmassive machine-type communications (mMTC), based on Zak-orthogonal time\nfrequency space (Zak-OTFS) modulation in the delay-Doppler domain. The scheme\nis tailored for doubly selective wireless channels, where conventional\northogonal frequency-division multiplexing (OFDM)-based CRA suffers from\nunreliable inter-slot channel prediction due to time-frequency variability. By\nexploiting the predictable nature of Zak-OTFS, the proposed approach enables\naccurate channel estimation across slots, facilitating reliable successive\ninterference cancellation across user packet replicas. A fair comparison with\nan OFDM-based CRA baseline shows that the proposed scheme achieves\nsignificantly lower packet loss rates under high mobility and user density.\nExtensive simulations over the standardized Veh-A channel confirm the\nrobustness and scalability of Zak-OTFS-based CRA, supporting its applicability\nto future mMTC deployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22013v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21926", "title": "Efficient Sub-pixel Motion Compensation in Learned Video Codecs", "authors": ["Théo Ladune", "Thomas Leguay", "Pierrick Philippe", "Gordon Clare", "Félix Henry"], "categories": ["cs.MM", "eess.IV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21926v1", "summary": "Motion compensation is a key component of video codecs. Conventional codecs\n(HEVC and VVC) have carefully refined this coding step, with an important focus\non sub-pixel motion compensation. On the other hand, learned codecs achieve\nsub-pixel motion compensation through simple bilinear filtering. This paper\noffers to improve learned codec motion compensation by drawing inspiration from\nconventional codecs. It is shown that the usage of more advanced interpolation\nfilters, block-based motion information and finite motion accuracy lead to\nbetter compression performance and lower decoding complexity. Experimental\nresults are provided on the Cool-chic video codec, where we demonstrate a rate\ndecrease of more than 10% and a lowering of motion-related decoding complexity\nfrom 391 MAC per pixel to 214 MAC per pixel. All contributions are made\nopen-source at https://github.com/Orange-OpenSource/Cool-Chic", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21926v1", "cate": "cs.MM", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21463", "title": "SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset Incorporating Cutting-Edge Generation Methods", "authors": ["Wen Huang", "Yanmei Gu", "Zhiming Wang", "Huijia Zhu", "Yanmin Qian"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Published in ACL 2025. Dataset available at: this https URL", "url": "http://arxiv.org/abs/2507.21463v1", "summary": "As speech generation technology advances, the risk of misuse through deepfake\naudio has become a pressing concern, which underscores the critical need for\nrobust detection systems. However, many existing speech deepfake datasets are\nlimited in scale and diversity, making it challenging to train models that can\ngeneralize well to unseen deepfakes. To address these gaps, we introduce\nSpeechFake, a large-scale dataset designed specifically for speech deepfake\ndetection. SpeechFake includes over 3 million deepfake samples, totaling more\nthan 3,000 hours of audio, generated using 40 different speech synthesis tools.\nThe dataset encompasses a wide range of generation techniques, including\ntext-to-speech, voice conversion, and neural vocoder, incorporating the latest\ncutting-edge methods. It also provides multilingual support, spanning 46\nlanguages. In this paper, we offer a detailed overview of the dataset's\ncreation, composition, and statistics. We also present baseline results by\ntraining detection models on SpeechFake, demonstrating strong performance on\nboth its own test sets and various unseen test sets. Additionally, we conduct\nexperiments to rigorously explore how generation methods, language diversity,\nand speaker variation affect detection performance. We believe SpeechFake will\nbe a valuable resource for advancing speech deepfake detection and developing\nmore robust models for evolving generation techniques.", "comment": "Published in ACL 2025. Dataset available at:\n  https://github.com/YMLLG/SpeechFake", "pdf_url": "http://arxiv.org/pdf/2507.21463v1", "cate": "cs.SD", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21873", "title": "A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data", "authors": ["Raffaele Pojer", "Andrea Passerini", "Kim G. Larsen", "Manfred Jaeger"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Submitted to the Journal of Artificial Intelligence Research (JAIR); under revision. 29 pages, 6 figures. Code available at this https URL", "url": "http://arxiv.org/abs/2507.21873v1", "summary": "Graph neural networks (GNNs) excel at predictive tasks on graph-structured\ndata but often lack the ability to incorporate symbolic domain knowledge and\nperform general reasoning. Relational Bayesian Networks (RBNs), in contrast,\nenable fully generative probabilistic modeling over graph-like structures and\nsupport rich symbolic knowledge and probabilistic inference. This paper\npresents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,\ncombining the learning strength of GNNs with the flexible reasoning\ncapabilities of RBNs.\n  We develop two implementations of this integration: one compiles GNNs\ndirectly into the native RBN language, while the other maintains the GNN as an\nexternal component. Both approaches preserve the semantics and computational\nproperties of GNNs while fully aligning with the RBN modeling paradigm. We also\npropose a maximum a-posteriori (MAP) inference method for these neuro-symbolic\nmodels.\n  To demonstrate the framework's versatility, we apply it to two distinct\nproblems. First, we transform a GNN for node classification into a collective\nclassification model that explicitly models homo- and heterophilic label\npatterns, substantially improving accuracy. Second, we introduce a\nmulti-objective network optimization problem in environmental planning, where\nMAP inference supports complex decision-making. Both applications include new\npublicly available benchmark datasets.\n  This work introduces a powerful and coherent neuro-symbolic approach to graph\ndata, bridging learning and reasoning in ways that enable novel applications\nand improved performance across diverse tasks.", "comment": "Submitted to the Journal of Artificial Intelligence Research (JAIR);\n  under revision. 29 pages, 6 figures. Code available at\n  https://github.com/raffaelepojer/NeSy-for-graph-data", "pdf_url": "http://arxiv.org/pdf/2507.21873v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21803", "title": "Bayesian Neural Network Surrogates for Bayesian Optimization of Carbon Capture and Storage Operations", "authors": ["Sofianos Panagiotis Fotias", "Vassilis Gaganis"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21803v1", "summary": "Carbon Capture and Storage (CCS) stands as a pivotal technology for fostering\na sustainable future. The process, which involves injecting supercritical\nCO$_2$ into underground formations, a method already widely used for Enhanced\nOil Recovery, serves a dual purpose: it not only curbs CO$_2$ emissions and\naddresses climate change but also extends the operational lifespan and\nsustainability of oil fields and platforms, easing the shift toward greener\npractices. This paper delivers a thorough comparative evaluation of strategies\nfor optimizing decision variables in CCS project development, employing a\nderivative-free technique known as Bayesian Optimization. In addition to\nGaussian Processes, which usually serve as the gold standard in BO, various\nnovel stochastic models were examined and compared within a BO framework. This\nresearch investigates the effectiveness of utilizing more exotic stochastic\nmodels than GPs for BO in environments where GPs have been shown to\nunderperform, such as in cases with a large number of decision variables or\nmultiple objective functions that are not similarly scaled. By incorporating\nNet Present Value (NPV) as a key objective function, the proposed framework\ndemonstrates its potential to improve economic viability while ensuring the\nsustainable deployment of CCS technologies. Ultimately, this study represents\nthe first application in the reservoir engineering industry of the growing body\nof BO research, specifically in the search for more appropriate stochastic\nmodels, highlighting its potential as a preferred method for enhancing\nsustainability in the energy sector.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21803v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21703", "title": "Semantics versus Identity: A Divide-and-Conquer Approach towards Adjustable Medical Image De-Identification", "authors": ["Yuan Tian", "Shuo Wang", "Rongzhao Zhang", "Zijian Chen", "Yankai Jiang", "Chunyi Li", "Xiangyang Zhu", "Fang Yan", "Qiang Hu", "XiaoSong Wang", "Guangtao Zhai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025;", "url": "http://arxiv.org/abs/2507.21703v1", "summary": "Medical imaging has significantly advanced computer-aided diagnosis, yet its\nre-identification (ReID) risks raise critical privacy concerns, calling for\nde-identification (DeID) techniques. Unfortunately, existing DeID methods\nneither particularly preserve medical semantics, nor are flexibly adjustable\ntowards different privacy levels. To address these issues, we propose a\ndivide-and-conquer framework comprising two steps: (1) Identity-Blocking, which\nblocks varying proportions of identity-related regions, to achieve different\nprivacy levels; and (2) Medical-Semantics-Compensation, which leverages\npre-trained Medical Foundation Models (MFMs) to extract medical semantic\nfeatures to compensate the blocked regions. Moreover, recognizing that features\nfrom MFMs may still contain residual identity information, we introduce a\nMinimum Description Length principle-based feature decoupling strategy, to\neffectively decouple and discard such identity components. Extensive\nevaluations against existing approaches across seven datasets and three\ndownstream tasks, demonstrates our state-of-the-art performance.", "comment": "Accepted to ICCV2025;", "pdf_url": "http://arxiv.org/pdf/2507.21703v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2506.22493", "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models", "authors": ["Sadia Kamal", "Lalu Prasad Yadav Prakash", "S M Rafiuddin", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen", "Sagnik Ray Choudhury"], "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22493v2", "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22493v2", "cate": "cs.CY", "date": "2025-06-24", "updated": "2025-07-29"}
{"id": "2507.21625", "title": "A Graphical Method for Designing Time-Optimal Non-Cartesian Gradient Waveforms", "authors": ["Rui Luo", "Hongzhang Huang", "Qinfang Miao", "Jian Xu", "Peng Hu", "Haikun Qi"], "categories": ["physics.med-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21625v1", "summary": "One of the fundamental challenges for non-Cartesian MRI is the need of\ndesigning time-optimal and hardware-compatible gradient waveforms for the\nprovided $k$-space trajectory. Currently dominant methods either work only for\ncertain trajectories or require significant computation time. In this paper, we\naim to develop a fast general method that is able to generate time-optimal\ngradient waveforms for arbitrary non-Cartesian trajectories satisfying both\nslew rate and gradient constraints. In the proposed method, the gradient\nwaveform is projected into a space defined by the gradients along the spatial\ndirections, termed as $g$-space. In the constructed $g$-space, the problem of\nfinding the next gradient vector given the current gradient vector under\ndesired slew rate limit and with desired direction is simplified to finding the\nintersection between a line and a circle. To handle trajectories with\nincreasing curvature, a Forward and Backward Sweep (FBS) strategy is\nintroduced, which ensures the existence of the solution to the above mentioned\ngeometry problem for arbitrary trajectories. Furthermore, trajectory\nreparameterization is proposed to ensure trajectory fidelity. We compare the\nproposed method with the previous optimal-control method in simulations and\nvalidate its feasibility for real MR acquisitions in phantom and human knee for\na wide range of non-Cartesian trajectories. The proposed method enables\naccurate and fast gradient waveform design, achieving significant reduction in\ncomputation time and slew rate overshoot compared to the previous method. The\nsource code will be publicly accessible upon publication of this study.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21625v1", "cate": "physics.med-ph", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22027", "title": "Site-Specific Location Calibration and Validation of Ray-Tracing Simulator NYURay at Upper Mid-Band Frequencies", "authors": ["Mingjun Ying", "Dipankar Shakya", "Peijie Ma", "Guanyue Qian", "Theodore S. Rappaport"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      16 pages, 7 figures", "url": "http://arxiv.org/abs/2507.22027v1", "summary": "Ray-tracing (RT) simulators are essential for wireless digital twins,\nenabling accurate site-specific radio channel prediction for next-generation\nwireless systems. Yet, RT simulation accuracy is often limited by insufficient\nmeasurement data and a lack of systematic validation. This paper presents\nsite-specific location calibration and validation of NYURay, NYU's in-house ray\ntracer, at upper mid-band frequencies (6.75 GHz and 16.95 GHz). We propose a\nlocation calibration algorithm that corrects GPS-induced position errors by\noptimizing transmitter-receiver (TX-RX) locations to align simulated and\nmeasured power delay profiles, improving TX-RX location accuracy by 42.3% for\nline-of-sight (LOS) and 13.5% for non-line-of-sight (NLOS) scenarios.\nValidation across 18 TX-RX locations shows excellent RT accuracy in path loss\nprediction, with path loss exponent (PLE) deviations under 0.14. While RT\nunderestimates delay spread and angular spreads, their cumulative distributions\nremain statistically similar. The validated NYURay advances RT validation and\nprovides reliable channel statistics for 6G deployment.", "comment": "16 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.22027v1", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.10091", "title": "G$^{2}$SF-MIAD: Geometry-Guided Score Fusion for Multimodal Industrial Anomaly Detection", "authors": ["Chengyu Tao", "Xuanming Cao", "Juan Du"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10091v2", "summary": "Industrial quality inspection plays a critical role in modern manufacturing\nby identifying defective products during production. While single-modality\napproaches using either 3D point clouds or 2D RGB images suffer from\ninformation incompleteness, multimodal anomaly detection offers promise through\nthe complementary fusion of crossmodal data. However, existing methods face\nchallenges in effectively integrating unimodal results and improving\ndiscriminative power. To address these limitations, we first reinterpret memory\nbank-based anomaly scores in single modalities as isotropic Euclidean distances\nin local feature spaces. Dynamically evolving from Euclidean metrics, we\npropose a novel \\underline{G}eometry-\\underline{G}uided \\underline{S}core\n\\underline{F}usion (G$^{2}$SF) framework that progressively learns an\nanisotropic local distance metric as a unified score for the fusion task.\nThrough a geometric encoding operator, a novel Local Scale Prediction Network\n(LSPN) is proposed to predict direction-aware scaling factors that characterize\nfirst-order local feature distributions, thereby enhancing discrimination\nbetween normal and anomalous patterns. Additionally, we develop specialized\nloss functions and score aggregation strategy from geometric priors to ensure\nboth metric generalization and efficacy. Comprehensive evaluations on the\nMVTec-3D AD and Eyecandies datasets demonstrate the state-of-the-art detection\nperformance of our method, and detailed ablation analysis validates each\ncomponent's contribution. Our code is available at\nhttps://github.com/ctaoaa/G2SF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10091v2", "cate": "eess.IV", "date": "2025-03-13", "updated": "2025-07-29"}
{"id": "2409.09545", "title": "Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment", "authors": ["Ohad Cohen", "Gershon Hazan", "Sharon Gannot"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5 pages, 4 figures, 2 tables. Accepted to EUSIPCO 2025", "url": "http://arxiv.org/abs/2409.09545v3", "summary": "This paper presents a Multi-modal Emotion Recognition (MER) system designed\nto enhance emotion recognition accuracy in challenging acoustic conditions. Our\napproach combines a modified and extended Hierarchical Token-semantic Audio\nTransformer (HTS-AT) for multi-channel audio processing with an R(2+1)D\nConvolutional Neural Networks (CNN) model for video analysis. We evaluate our\nproposed method on a reverberated version of the Ryerson audio-visual database\nof emotional speech and song (RAVDESS) dataset using synthetic and real-world\nRoom Impulse Responsess (RIRs). Our results demonstrate that integrating audio\nand video modalities yields superior performance compared to uni-modal\napproaches, especially in challenging acoustic conditions. Moreover, we show\nthat the multimodal (audiovisual) approach that utilizes multiple microphones\noutperforms its single-microphone counterpart.", "comment": "5 pages, 4 figures, 2 tables. Accepted to EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2409.09545v3", "cate": "cs.SD", "date": "2024-09-14", "updated": "2025-07-28"}
{"id": "2507.21875", "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21875v2", "summary": "Pain is a complex and pervasive condition that affects a significant portion\nof the population. Accurate and consistent assessment is essential for\nindividuals suffering from pain, as well as for developing effective management\nstrategies in a healthcare system. Automatic pain assessment systems enable\ncontinuous monitoring, support clinical decision-making, and help minimize\npatient distress while mitigating the risk of functional deterioration.\nLeveraging physiological signals offers objective and precise insights into a\nperson's state, and their integration in a multimodal framework can further\nenhance system performance. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed approach introduces \\textit{Tiny-BioMoE}, a lightweight pretrained\nembedding model for biosignal analysis. Trained on $4.4$ million biosignal\nimage representations and consisting of only $7.3$ million parameters, it\nserves as an effective tool for extracting high-quality embeddings for\ndownstream tasks. Extensive experiments involving electrodermal activity, blood\nvolume pulse, respiratory signals, peripheral oxygen saturation, and their\ncombinations highlight the model's effectiveness across diverse modalities in\nautomatic pain recognition tasks. \\textit{\\textcolor{blue}{The model's\narchitecture (code) and weights are available at\nhttps://github.com/GkikasStefanos/Tiny-BioMoE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21875v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21833", "title": "Analysis of Fourier Neural Operators via Effective Field Theory", "authors": ["Taeyoung Kim"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      37 pages, 10 figures", "url": "http://arxiv.org/abs/2507.21833v1", "summary": "Fourier Neural Operators (FNOs) have emerged as leading surrogates for\nhigh-dimensional partial-differential equations, yet their stability,\ngeneralization and frequency behavior lack a principled explanation. We present\nthe first systematic effective-field-theory analysis of FNOs in an\ninfinite-dimensional function space, deriving closed recursion relations for\nthe layer kernel and four-point vertex and then examining three practically\nimportant settings-analytic activations, scale-invariant cases and\narchitectures with residual connections. The theory shows that nonlinear\nactivations inevitably couple frequency inputs to high-frequency modes that are\notherwise discarded by spectral truncation, and experiments confirm this\nfrequency transfer. For wide networks we obtain explicit criticality conditions\non the weight-initialization ensemble that keep small input perturbations to\nhave uniform scale across depth, and empirical tests validate these\npredictions. Taken together, our results quantify how nonlinearity enables\nneural operators to capture non-trivial features, supply criteria for\nhyper-parameter selection via criticality analysis, and explain why\nscale-invariant activations and residual connections enhance feature learning\nin FNOs.", "comment": "37 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.21833v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21715", "title": "Impact of Underwater Image Enhancement on Feature Matching", "authors": ["Jason M. Summers", "Mark W. Jones"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21715v1", "summary": "We introduce local matching stability and furthest matchable frame as\nquantitative measures for evaluating the success of underwater image\nenhancement. This enhancement process addresses visual degradation caused by\nlight absorption, scattering, marine growth, and debris. Enhanced imagery plays\na critical role in downstream tasks such as path detection and autonomous\nnavigation for underwater vehicles, relying on robust feature extraction and\nframe matching. To assess the impact of enhancement techniques on\nframe-matching performance, we propose a novel evaluation framework tailored to\nunderwater environments. Through metric-based analysis, we identify strengths\nand limitations of existing approaches and pinpoint gaps in their assessment of\nreal-world applicability. By incorporating a practical matching strategy, our\nframework offers a robust, context-aware benchmark for comparing enhancement\nmethods. Finally, we demonstrate how visual improvements affect the performance\nof a complete real-world algorithm -- Simultaneous Localization and Mapping\n(SLAM) -- reinforcing the framework's relevance to operational underwater\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21715v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20018", "title": "The Carbon Cost of Conversation, Sustainability in the Age of Language Models", "authors": ["Sayed Mahbub Hasan Amiri", "Prasun Goswami", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Naznin Akter"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      22 Pages, 5 Tables", "url": "http://arxiv.org/abs/2507.20018v2", "summary": "Large language models (LLMs) like GPT-3 and BERT have revolutionized natural\nlanguage processing (NLP), yet their environmental costs remain dangerously\noverlooked. This article critiques the sustainability of LLMs, quantifying\ntheir carbon footprint, water usage, and contribution to e-waste through case\nstudies of models such as GPT-4 and energy-efficient alternatives like Mistral\n7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of\ncars driven annually, while data centre cooling exacerbates water scarcity in\nvulnerable regions. Systemic challenges corporate greenwashing, redundant model\ndevelopment, and regulatory voids perpetuate harm, disproportionately burdening\nmarginalized communities in the Global South. However, pathways exist for\nsustainable NLP: technical innovations (e.g., model pruning, quantum\ncomputing), policy reforms (carbon taxes, mandatory emissions reporting), and\ncultural shifts prioritizing necessity over novelty. By analysing industry\nleaders (Google, Microsoft) and laggards (Amazon), this work underscores the\nurgency of ethical accountability and global cooperation. Without immediate\naction, AIs ecological toll risks outpacing its societal benefits. The article\nconcludes with a call to align technological progress with planetary\nboundaries, advocating for equitable, transparent, and regenerative AI systems\nthat prioritize both human and environmental well-being.", "comment": "22 Pages, 5 Tables", "pdf_url": "http://arxiv.org/pdf/2507.20018v2", "cate": "cs.CY", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2507.21865", "title": "On the Feasibility of SCL-Band Transmission over G.654.E-Compliant Long-Haul Fibre Links", "authors": ["Jiaqian Yang", "Eric Sillekens", "Ronit Sohanpal", "Mingming Tan", "Dini Pratiwi", "Henrique Buglia", "Romulo Aparecido", "John D. Downie", "Sergejs Makovejs", "Lidia Galdino", "Wladek Forysiak", "Polina Bayvel", "Robert I. Killey"], "categories": ["physics.optics", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      4 pages, 6 figures, accepted for oral presentation at European Conference on Optical Communication 2025", "url": "http://arxiv.org/abs/2507.21865v1", "summary": "We demonstrate the first SCL-band long-haul transmission using\nG.654.E-compliant fibre, achieving 100.8 Tb/s (GMI) over 1552 km, despite its\n1520 nm cutoff wavelength. Due to the fibre's ultra-low loss and low\nnonlinearity, the achievable-information-rate with lumped amplification is\ncomparable to that of G.652.D-compliant fibre links with\ndistributed-Raman-amplification.", "comment": "4 pages, 6 figures, accepted for oral presentation at European\n  Conference on Optical Communication 2025", "pdf_url": "http://arxiv.org/pdf/2507.21865v1", "cate": "physics.optics", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21886", "title": "Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21886v2", "summary": "Pain is a complex condition affecting a large portion of the population.\nAccurate and consistent evaluation is essential for individuals experiencing\npain, and it supports the development of effective and advanced management\nstrategies. Automatic pain assessment systems provide continuous monitoring and\nsupport clinical decision-making, aiming to reduce distress and prevent\nfunctional decline. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed method introduces a pipeline that leverages respiration as the input\nsignal and incorporates a highly efficient cross-attention transformer\nalongside a multi-windowing strategy. Extensive experiments demonstrate that\nrespiration is a valuable physiological modality for pain assessment. Moreover,\nexperiments revealed that compact and efficient models, when properly\noptimized, can achieve strong performance, often surpassing larger\ncounterparts. The proposed multi-window approach effectively captures both\nshort-term and long-term features, as well as global characteristics, thereby\nenhancing the model's representational capacity.", "comment": "arXiv admin note: text overlap with arXiv:2507.21881,\n  arXiv:2507.21875", "pdf_url": "http://arxiv.org/pdf/2507.21886v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.04862", "title": "Efficacy of Image Similarity as a Metric for Augmenting Small Dataset Retinal Image Segmentation", "authors": ["Thomas Wallace", "Ik Siong Heng", "Senad Subasic", "Chris Messenger"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      30 pages, 10 figures", "url": "http://arxiv.org/abs/2507.04862v3", "summary": "Synthetic images are an option for augmenting limited medical imaging\ndatasets to improve the performance of various machine learning models. A\ncommon metric for evaluating synthetic image quality is the Fr\\'echet Inception\nDistance (FID) which measures the similarity of two image datasets. In this\nstudy we evaluate the relationship between this metric and the improvement\nwhich synthetic images, generated by a Progressively Growing Generative\nAdversarial Network (PGGAN), grant when augmenting Diabetes-related Macular\nEdema (DME) intraretinal fluid segmentation performed by a U-Net model with\nlimited amounts of training data. We find that the behaviour of augmenting with\nstandard and synthetic images agrees with previously conducted experiments.\nAdditionally, we show that dissimilar (high FID) datasets do not improve\nsegmentation significantly. As FID between the training and augmenting datasets\ndecreases, the augmentation datasets are shown to contribute to significant and\nrobust improvements in image segmentation. Finally, we find that there is\nsignificant evidence to suggest that synthetic and standard augmentations\nfollow separate log-normal trends between FID and improvements in model\nperformance, with synthetic data proving more effective than standard\naugmentation techniques. Our findings show that more similar datasets (lower\nFID) will be more effective at improving U-Net performance, however, the\nresults also suggest that this improvement may only occur when images are\nsufficiently dissimilar.", "comment": "30 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.04862v3", "cate": "eess.IV", "date": "2025-07-07", "updated": "2025-07-29"}
{"id": "2502.05130", "title": "Latent Swap Joint Diffusion for 2D Long-Form Latent Generation", "authors": ["Yusheng Dai", "Chenxi Wang", "Chang Li", "Chen Wang", "Jun Du", "Kewei Li", "Ruoyu Wang", "Jiefeng Ma", "Lei Sun", "Jianqing Gao"], "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05130v3", "summary": "This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient\nmethod to generate seamless and coherence long spectrum and panorama through\nlatent swap joint diffusion across multi-views. We first investigate the\nspectrum aliasing problem in spectrum-based audio generation caused by existing\njoint diffusion methods. Through a comparative analysis of the VAE latent\nrepresentation of Mel-spectra and RGB images, we identify that the failure\narises from excessive suppression of high-frequency components during the\nspectrum denoising process due to the averaging operator. To address this\nissue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap\napplied to the overlapping region of adjacent views. Leveraging stepwise\ndifferentiated trajectories of adjacent subviews, this swap operator adaptively\nenhances high-frequency components and avoid spectrum distortion. Furthermore,\nto improve global cross-view consistency in non-overlapping regions, we\nintroduce Reference-Guided Latent Swap, a unidirectional latent swap operator\nthat provides a centralized reference trajectory to synchronize subview\ndiffusions. By refining swap timing and intervals, we can achieve a cross-view\nsimilarity-diversity balance in a forward-only manner. Quantitative and\nqualitative experiments demonstrate that SaFa significantly outperforms\nexisting joint diffusion methods and even training-based methods in audio\ngeneration using both U-Net and DiT models, along with effective longer length\nadaptation. It also adapts well to panorama generation, achieving comparable\nperformance with 2 $\\sim$ 20 $\\times$ faster speed and greater model\ngeneralizability. More generation demos are available at\nhttps://swapforward.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05130v3", "cate": "cs.SD", "date": "2025-02-07", "updated": "2025-07-29"}
{"id": "2507.21881", "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21881v2", "summary": "Pain is a multifaceted phenomenon that affects a substantial portion of the\npopulation. Reliable and consistent evaluation benefits those experiencing pain\nand underpins the development of effective and advanced management strategies.\nAutomatic pain-assessment systems deliver continuous monitoring, inform\nclinical decision-making, and aim to reduce distress while preventing\nfunctional decline. By incorporating physiological signals, these systems\nprovide objective, accurate insights into an individual's condition. This study\nhas been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for\nNext-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline\nthat leverages electrodermal activity signals as input modality. Multiple\nrepresentations of the signal are created and visualized as waveforms, and they\nare jointly visualized within a single multi-representation diagram. Extensive\nexperiments incorporating various processing and filtering techniques, along\nwith multiple representation combinations, demonstrate the effectiveness of the\nproposed approach. It consistently yields comparable, and in several cases\nsuperior, results to traditional fusion methods, establishing it as a robust\nalternative for integrating different signal representations or modalities.", "comment": "arXiv admin note: text overlap with arXiv:2507.21875", "pdf_url": "http://arxiv.org/pdf/2507.21881v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21841", "title": "Discovering Interpretable Ordinary Differential Equations from Noisy Data", "authors": ["Rahul Golder", "M. M. Faruque Hasan"], "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures, 7 tables", "url": "http://arxiv.org/abs/2507.21841v1", "summary": "The data-driven discovery of interpretable models approximating the\nunderlying dynamics of a physical system has gained attraction in the past\ndecade. Current approaches employ pre-specified functional forms or basis\nfunctions and often result in models that lack physical meaning and\ninterpretability, let alone represent the true physics of the system. We\npropose an unsupervised parameter estimation methodology that first finds an\napproximate general solution, followed by a spline transformation to linearly\nestimate the coefficients of the governing ordinary differential equation\n(ODE). The approximate general solution is postulated using the same functional\nform as the analytical solution of a general homogeneous, linear,\nconstant-coefficient ODE. An added advantage is its ability to produce a\nhigh-fidelity, smooth functional form even in the presence of noisy data. The\nspline approximation obtains gradient information from the functional form\nwhich are linearly independent and creates the basis of the gradient matrix.\nThis gradient matrix is used in a linear system to find the coefficients of the\nODEs. From the case studies, we observed that our modeling approach discovers\nODEs with high accuracy and also promotes sparsity in the solution without\nusing any regularization techniques. The methodology is also robust to noisy\ndata and thus allows the integration of data-driven techniques into real\nexperimental setting for data-driven learning of physical phenomena.", "comment": "20 pages, 11 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.21841v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21723", "title": "Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations", "authors": ["Nils Hütten", "Florian Hölken", "Hasan Tercan", "Tobias Meisen"], "categories": ["cs.CV", "cs.AI", "I.2; I.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21723v1", "summary": "In recent years, Explainable AI has gained traction as an approach to\nenhancing model interpretability and transparency, particularly in complex\nmodels such as detection transformers. Despite rapid advancements, a\nsubstantial research gap remains in understanding the distinct roles of\ninternal components - knowledge that is essential for improving transparency\nand efficiency. Inspired by neuroscientific ablation studies, which investigate\nthe functions of brain regions through selective impairment, we systematically\nanalyze the impact of ablating key components in three state-of-the-art\ndetection transformer models: Detection transformer (DETR), deformable\ndetection transformer (DDETR), and DETR with improved denoising anchor boxes\n(DINO). The ablations target query embeddings, encoder and decoder multi-head\nself-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA)\nlayers. We evaluate the effects of these ablations on the performance metrics\ngIoU and F1-score, quantifying effects on both the classification and\nregression sub-tasks on the COCO dataset. To facilitate reproducibility and\nfuture research, we publicly release the DeepDissect library. Our findings\nreveal model-specific resilience patterns: while DETR is particularly sensitive\nto ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable\nattention enhances robustness, and DINO exhibits the greatest resilience due to\nits look-forward twice update rule, which helps distributing knowledge across\nblocks. These insights also expose structural redundancies, particularly in\nDDETR's and DINO's decoder MHCA layers, highlighting opportunities for model\nsimplification without sacrificing performance. This study advances XAI for\nDETRs by clarifying the contributions of internal components to model\nperformance, offering insights to optimize and improve transparency and\nefficiency in critical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21723v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20525", "title": "The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated \"Sacred\" Text?", "authors": ["Murray Shanahan", "Tara Das", "Robert Thurman"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20525v2", "summary": "This paper presents a case study in the use of a large language model to\ngenerate a fictional Buddhist \"sutra\"', and offers a detailed analysis of the\nresulting text from a philosophical and literary point of view. The conceptual\nsubtlety, rich imagery, and density of allusion found in the text make it hard\nto causally dismiss on account of its mechanistic origin. This raises questions\nabout how we, as a society, should come to terms with the potentially\nunsettling possibility of a technology that encroaches on human meaning-making.\nWe suggest that Buddhist philosophy, by its very nature, is well placed to\nadapt.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20525v2", "cate": "cs.CY", "date": "2025-07-28", "updated": "2025-07-29"}
{"id": "2403.11945", "title": "Kernel Modelling of Fading Memory Systems", "authors": ["Yongkang Huo", "Thomas Chaffey", "Rodolphe Sepulchre"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.11945v3", "summary": "The paper is a follow-up of the recently introduced kernel-based framework to\nidentify nonlinear input-output systems regularized by desirable input-output\nincremental properties. Assuming that the system has fading memory, we propose\nto learn the functional that maps the past input to the present output rather\nthan the operator mapping input trajectories to output trajectories. While\nretaining the benefits of the previously proposed framework, this modification\nsimplifies the selection of the kernel, enforces causality, and enables\ntemporal simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.11945v3", "cate": "eess.SY", "date": "2024-03-18", "updated": "2025-07-29"}
{"id": "2412.17211", "title": "Joint Multitarget Detection and Tracking with mmWave Radar", "authors": ["Jiang Zhu", "Menghuai Xu", "Ruohai Guo", "Fangyong Wang", "Guangying Zheng", "Fengzhong Qu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.17211v2", "summary": "Accurate targets detection and tracking with mmWave radar is a key sensing\ncapability that will enable more intelligent systems, create smart, efficient,\nautomated system. This paper proposes an end-to-end detection-estimation-track\nframework named MNOMP-SPA-KF consisting of the target detection and estimation\nmodule, the data association (DA) module and the target tracking module. In the\ntarget estimation and detection module, a low complexity, super-resolution and\nconstant false alarm rate (CFAR) based two dimensional multisnapshot\nNewtonalized orthogonal matching pursuit (2D-MNOMP) is designed to extract the\nmultitarget's radial distances and velocities, followed by the conventional\n(Bartlett) beamformer to extract the multitarget's azimuths. In the DA module,\na sum product algorithm (SPA) is adopted to obtain the association\nprobabilities of the existed targets and measurements by incorporating the\nradial velocity information. The Kalman filter (KF) is implemented to perform\ntarget tracking in the target tracking module by exploiting the asymptotic\ndistribution of the estimators. To improve the detection probability of the\nweak targets, extrapolation is also coupled into the MNOMP-SPA-KF. Numerical\nand real data experiments demonstrate the effectiveness of the MNOMP-SPA-KF\nalgorithm, compared to other benchmark algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.17211v2", "cate": "eess.SP", "date": "2024-12-23", "updated": "2025-07-29"}
{"id": "2410.18984", "title": "Very High-Resolution Bridge Deformation Monitoring Using UAV-based Photogrammetry", "authors": ["Mehdi Maboudi", "Jan Backhaus", "Inka Mai", "Yahya Ghassoun", "Yogesh Khedar", "Dirk Lowke", "Bjoern Riedel", "Ulf Bestmann", "Markus Gerke"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.18984v2", "summary": "Accurate and efficient structural health monitoring of infrastructure objects\nsuch as bridges is a vital task, as many existing constructions have already\nreached or are approaching their planned service life. In this contribution, we\naddress the question of the suitability of UAV-based monitoring for SHM, in\nparticular focusing on the geometric deformation under load. Such an advanced\ntechnology is becoming increasingly popular due to its ability to decrease the\ncost and risk of tedious traditional inspection methods. To this end, we\nperformed extensive tests employing a research reinforced concrete bridge that\ncan be exposed to a predefined load via ground anchors. Very high-resolution\nimage blocks have been captured before, during, and after the application of\ncontrolled loads. From those images, the motion of distinct points on the\nbridge has been monitored, and in addition, dense image point clouds were\ncomputed to evaluate the performance of surface-based data acquisition.\nMoreover, a geodetic control network in stable regions is used as control\ninformation for bundle adjustment. We applied different sensing technologies in\norder to be able to judge the image-based deformation results: displacement\ntransducers, tachymetry, and laser profiling. As a platform for the\nphotogrammetric measurements, a multi-rotor UAV DJI Matrice 600 Pro was\nemployed, equipped with two RTK-GNSS receivers. The mounted camera was a\nPhaseOne iXM-100 (100MP) with an 80 mm lens. With a flying height of 30 m above\nthe terrain, this resulted in a GSD of 1.3 mm while a forward and sideward\noverlap of 80% was maintained. The comparison with reference data (displacement\ntransducers) reveals a difference of less than 1 mm. We show that by employing\nthe introduced UAV-based monitoring approach, a full area-wide quantification\nof deformation is possible in contrast to classical point or profile\nmeasurements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.18984v2", "cate": "cs.CV", "date": "2024-10-09", "updated": "2025-07-29"}
{"id": "2507.08128", "title": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models", "authors": ["Arushi Goel", "Sreyan Ghosh", "Jaehyeon Kim", "Sonal Kumar", "Zhifeng Kong", "Sang-gil Lee", "Chao-Han Huck Yang", "Ramani Duraiswami", "Dinesh Manocha", "Rafael Valle", "Bryan Catanzaro"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Code, Datasets, and Models: this https URL ; Updates in v2: Updated results for new thinking mode ckpts, added qualitative figure, added note on fully open claim, add email ID for corresponding authors", "url": "http://arxiv.org/abs/2507.08128v2", "summary": "We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large\naudio-language model that advances reasoning and understanding across speech,\nsound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder\ntrained using a novel strategy for joint representation learning across all 3\nmodalities of speech, sound, and music; (ii) flexible, on-demand thinking,\nallowing the model to do chain-of-thought-type reasoning before answering;\n(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning\n(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To\nenable these capabilities, we propose several large-scale training datasets\ncurated using novel strategies, including AudioSkills-XL, LongAudio-XL,\nAF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based\ntraining strategy. Trained on only open-source audio data, AF3 achieves new\nSOTA results on over 20+ (long) audio understanding and reasoning benchmarks,\nsurpassing both open-weight and closed-source models trained on much larger\ndatasets.", "comment": "Code, Datasets, and Models:\n  https://research.nvidia.com/labs/adlr/AF3/ ; Updates in v2: Updated results\n  for new thinking mode ckpts, added qualitative figure, added note on fully\n  open claim, add email ID for corresponding authors", "pdf_url": "http://arxiv.org/pdf/2507.08128v2", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-28"}
{"id": "2507.21929", "title": "Libra: Large Chinese-based Safeguard for AI Content", "authors": ["Ziyang Chen", "Huimu Yu", "Xing Wu", "Dongqin Liu", "Songlin Hu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21929v1", "summary": "Large language models (LLMs) excel in text understanding and generation but\nraise significant safety and ethical concerns in high-stakes applications. To\nmitigate these risks, we present Libra-Guard, a cutting-edge safeguard system\ndesigned to enhance the safety of Chinese-based LLMs. Leveraging a two-stage\ncurriculum training pipeline, Libra-Guard enhances data efficiency by employing\nguard pretraining on synthetic samples, followed by fine-tuning on\nhigh-quality, real-world data, thereby significantly reducing reliance on\nmanual annotations. To enable rigorous safety evaluations, we also introduce\nLibra-Test, the first benchmark specifically designed to evaluate the\neffectiveness of safeguard systems for Chinese content. It covers seven\ncritical harm scenarios and includes over 5,700 samples annotated by domain\nexperts. Experiments show that Libra-Guard achieves 86.79% accuracy,\noutperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat\n(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.\nThese contributions establish a robust framework for advancing the safety\ngovernance of Chinese LLMs and represent a tentative step toward developing\nsafer, more reliable Chinese AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21929v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21898", "title": "Cardiovascular Disease Prediction using Machine Learning: A Comparative Analysis", "authors": ["Risshab Srinivas Ramesh", "Roshani T S Udupa", "Monisha J", "Kushi K K S"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21898v1", "summary": "Cardiovascular diseases (CVDs) are a main cause of mortality globally,\naccounting for 31% of all deaths. This study involves a cardiovascular disease\n(CVD) dataset comprising 68,119 records to explore the influence of numerical\n(age, height, weight, blood pressure, BMI) and categorical gender, cholesterol,\nglucose, smoking, alcohol, activity) factors on CVD occurrence. We have\nperformed statistical analyses, including t-tests, Chi-square tests, and ANOVA,\nto identify strong associations between CVD and elderly people, hypertension,\nhigher weight, and abnormal cholesterol levels, while physical activity (a\nprotective factor). A logistic regression model highlights age, blood pressure,\nand cholesterol as primary risk factors, with unexpected negative associations\nfor smoking and alcohol, suggesting potential data issues. Model performance\ncomparisons reveal CatBoost as the top performer with an accuracy of 0.734 and\nan ECE of 0.0064 and excels in probabilistic prediction (Brier score = 0.1824).\nData challenges, including outliers and skewed distributions, indicate a need\nfor improved preprocessing to enhance predictive reliability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21898v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21732", "title": "SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Chenxi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21732v1", "summary": "Visual Object Tracking (VOT) is widely used in applications like autonomous\ndriving to continuously track targets in videos. Existing methods can be\nroughly categorized into template matching and autoregressive methods, where\nthe former usually neglects the temporal dependencies across frames and the\nlatter tends to get biased towards the object categories during training,\nshowing weak generalizability to unseen classes. To address these issues, some\nmethods propose to adapt the video foundation model SAM2 for VOT, where the\ntracking results of each frame would be encoded as memory for conditioning the\nrest of frames in an autoregressive manner. Nevertheless, existing methods fail\nto overcome the challenges of object occlusions and distractions, and do not\nhave any measures to intercept the propagation of tracking errors. To tackle\nthem, we present a SAMITE model, built upon SAM2 with additional modules,\nincluding: (1) Prototypical Memory Bank: We propose to quantify the\nfeature-wise and position-wise correctness of each frame's tracking results,\nand select the best frames to condition subsequent frames. As the features of\noccluded and distracting objects are feature-wise and position-wise inaccurate,\ntheir scores would naturally be lower and thus can be filtered to intercept\nerror propagation; (2) Positional Prompt Generator: To further reduce the\nimpacts of distractors, we propose to generate positional mask prompts to\nprovide explicit positional clues for the target, leading to more accurate\ntracking. Extensive experiments have been conducted on six benchmarks, showing\nthe superiority of SAMITE. The code is available at\nhttps://github.com/Sam1224/SAMITE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21732v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2406.06736", "title": "Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges", "authors": ["Usman Gohar", "Zeyu Tang", "Jialu Wang", "Kun Zhang", "Peter L. Spirtes", "Yang Liu", "Lu Cheng"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in TMLR", "url": "http://arxiv.org/abs/2406.06736v3", "summary": "The widespread integration of Machine Learning systems in daily life,\nparticularly in high-stakes domains, has raised concerns about the fairness\nimplications. While prior works have investigated static fairness measures,\nrecent studies reveal that automated decision-making has long-term implications\nand that off-the-shelf fairness approaches may not serve the purpose of\nachieving long-term fairness. Additionally, the existence of feedback loops and\nthe interaction between models and the environment introduces additional\ncomplexities that may deviate from the initial fairness goals. In this survey,\nwe review existing literature on long-term fairness from different perspectives\nand present a taxonomy for long-term fairness studies. We highlight key\nchallenges and consider future research directions, analyzing both current\nissues and potential further explorations.", "comment": "Accepted in TMLR", "pdf_url": "http://arxiv.org/pdf/2406.06736v3", "cate": "cs.LG", "date": "2024-06-10", "updated": "2025-07-29"}
{"id": "2410.19159", "title": "Collision Avoidance for Convex Primitives via Differentiable Optimization Based High-Order Control Barrier Functions", "authors": ["Shiqing Wei", "Rooholla Khorrambakht", "Prashanth Krishnamurthy", "Vinicius Mariano Gonçalves", "Farshad Khorrami"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.19159v3", "summary": "Ensuring the safety of dynamical systems is crucial, where collision\navoidance is a primary concern. Recently, control barrier functions (CBFs) have\nemerged as an effective method to integrate safety constraints into control\nsynthesis through optimization techniques. However, challenges persist when\ndealing with convex primitives and tasks requiring torque control, as well as\nthe occurrence of unintended equilibria. This work addresses these challenges\nby introducing a high-order CBF (HOCBF) framework for collision avoidance among\nconvex primitives. We transform nonconvex safety constraints into linear\nconstraints by differentiable optimization and prove the high-order continuous\ndifferentiability. Then, we employ HOCBFs to accommodate torque control,\nenabling tasks involving forces or high dynamics. Additionally, we analyze the\nissue of spurious equilibria in high-order cases and propose a circulation\nmechanism to prevent the undesired equilibria on the boundary of the safe set.\nFinally, we validate our framework with three experiments on the Franka\nResearch 3 robotic manipulator, demonstrating successful collision avoidance\nand the efficacy of the circulation mechanism.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.19159v3", "cate": "eess.SY", "date": "2024-10-24", "updated": "2025-07-29"}
{"id": "2501.11594", "title": "Faster-Than-Nyquist Equalization with Convolutional Neural Networks", "authors": ["Bruno De Filippo", "Carla Amatetti", "Alessandro Vanelli-Coralli"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at the 2025 IEEE 36th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), 1-4 September, 2025, Istanbul (Turkey)", "url": "http://arxiv.org/abs/2501.11594v2", "summary": "Faster-than-Nyquist (FTN) signaling aims at improving the spectral efficiency\nof wireless communication systems by exceeding the boundaries set by the\nNyquist-Shannon sampling theorem. 50 years after its first introduction in the\nscientific literature, wireless communications have significantly changed, but\nspectral efficiency remains one of the key challenges. To adopt FTN signaling,\ninter-symbol interference (ISI) patterns need to be equalized at the receiver.\nMotivated by the pattern recognition capabilities of convolutional neural\nnetworks with skip connections, we propose such deep learning architecture for\nISI equalization and symbol demodulation in FTN receivers. We investigate the\nperformance of the proposed model considering quadrature phase shift keying\nmodulation and low density parity check coding, and compare it to a set of\nbenchmarks, including frequency-domain equalization, a\nquadratic-programming-based receiver, and an equalization scheme based on a\ndeep neural network. We show that our receiver outperforms any benchmark,\nachieving error rates comparable to those in additive white Gaussian noise\nchannel, and higher effective throughput, thanks to the increased spectral\nefficiency of FTN signaling. With a compression factor of 60% and code rate\n3/4, the proposed model achieves a peak effective throughput of 2.5 Mbps at\njust 10dB of energy per bit over noise power spectral density ratio, with other\nreceivers being limited by error floors due to the strong inter-symbol\ninterference. To promote reproducibility in deep learning for wireless\ncommunications, our code is open source at the repository provided in the\nreferences.", "comment": "Accepted for presentation at the 2025 IEEE 36th International\n  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), 1-4\n  September, 2025, Istanbul (Turkey)", "pdf_url": "http://arxiv.org/pdf/2501.11594v2", "cate": "eess.SP", "date": "2025-01-20", "updated": "2025-07-29"}
{"id": "2503.09631", "title": "V2M4: 4D Mesh Animation Reconstruction from a Single Monocular Video", "authors": ["Jianqi Chen", "Biao Zhang", "Xiangjun Tang", "Peter Wonka"], "categories": ["cs.GR", "eess.IV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2503.09631v2", "summary": "We present V2M4, a novel 4D reconstruction method that directly generates a\nusable 4D mesh animation asset from a single monocular video. Unlike existing\napproaches that rely on priors from multi-view image and video generation\nmodels, our method is based on native 3D mesh generation models. Naively\napplying 3D mesh generation models to generate a mesh for each frame in a 4D\ntask can lead to issues such as incorrect mesh poses, misalignment of mesh\nappearance, and inconsistencies in mesh geometry and texture maps. To address\nthese problems, we propose a structured workflow that includes camera search\nand mesh reposing, condition embedding optimization for mesh appearance\nrefinement, pairwise mesh registration for topology consistency, and global\ntexture map optimization for texture consistency. Our method outputs\nhigh-quality 4D animated assets that are compatible with mainstream graphics\nand game software. Experimental results across a variety of animation types and\nmotion amplitudes demonstrate the generalization and effectiveness of our\nmethod. Project page: https://windvchen.github.io/V2M4/.", "comment": "Accepted by ICCV 2025. Project page:\n  https://windvchen.github.io/V2M4/", "pdf_url": "http://arxiv.org/pdf/2503.09631v2", "cate": "cs.GR", "date": "2025-03-11", "updated": "2025-07-29"}
{"id": "2507.14915", "title": "Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling", "authors": ["Xiaojie Li", "Ronghui Li", "Shukai Fang", "Shuzhao Xie", "Xiaoyang Guo", "Jiaqing Zhou", "Junkun Peng", "Zhi Wang"], "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14915v3", "summary": "Well-coordinated, music-aligned holistic dance enhances emotional\nexpressiveness and audience engagement. However, generating such dances remains\nchallenging due to the scarcity of holistic 3D dance datasets, the difficulty\nof achieving cross-modal alignment between music and dance, and the complexity\nof modeling interdependent motion across the body, hands, and face. To address\nthese challenges, we introduce SoulDance, a high-precision music-dance paired\ndataset captured via professional motion capture systems, featuring\nmeticulously annotated holistic dance movements. Building on this dataset, we\npropose SoulNet, a framework designed to generate music-aligned, kinematically\ncoordinated holistic dance sequences. SoulNet consists of three principal\ncomponents: (1) Hierarchical Residual Vector Quantization, which models\ncomplex, fine-grained motion dependencies across the body, hands, and face; (2)\nMusic-Aligned Generative Model, which composes these hierarchical motion units\ninto expressive and coordinated holistic dance; (3) Music-Motion Retrieval\nModule, a pre-trained cross-modal model that functions as a music-dance\nalignment prior, ensuring temporal synchronization and semantic coherence\nbetween generated dance and input music throughout the generation process.\nExtensive experiments demonstrate that SoulNet significantly surpasses existing\napproaches in generating high-quality, music-coordinated, and well-aligned\nholistic 3D dance sequences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14915v3", "cate": "cs.MM", "date": "2025-07-20", "updated": "2025-07-29"}
{"id": "2507.21964", "title": "Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities", "authors": ["Sourish Gunesh Dhekane", "Thomas Ploetz"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21964v1", "summary": "Developing zero-shot human activity recognition (HAR) methods is a critical\ndirection in smart home research -- considering its impact on making HAR\nsystems work across smart homes having diverse sensing modalities, layouts, and\nactivities of interest. The state-of-the-art solutions along this direction are\nbased on generating natural language descriptions of the sensor data and\nfeeding it via a carefully crafted prompt to the LLM to perform classification.\nDespite their performance guarantees, such ``prompt-the-LLM'' approaches carry\nseveral risks, including privacy invasion, reliance on an external service, and\ninconsistent predictions due to version changes, making a case for alternative\nzero-shot HAR methods that do not require prompting the LLMs. In this paper, we\npropose one such solution that models sensor data and activities using natural\nlanguage, leveraging its embeddings to perform zero-shot classification and\nthereby bypassing the need to prompt the LLMs for activity predictions. The\nimpact of our work lies in presenting a detailed case study on six datasets,\nhighlighting how language modeling can bolster HAR systems in zero-shot\nrecognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21964v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21938", "title": "Multi-state Protein Design with DynamicMPNN", "authors": ["Alex Abrudan", "Sebastian Pujalte Ojeda", "Chaitanya K. Joshi", "Matthew Greenig", "Felipe Engelberger", "Alena Khmelinskaia", "Jens Meiler", "Michele Vendruscolo", "Tuomas P. J. Knowles"], "categories": ["cs.LG", "q-bio.BM", "I.2.6; J.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 GenBio Workshop", "url": "http://arxiv.org/abs/2507.21938v1", "summary": "Structural biology has long been dominated by the one sequence, one\nstructure, one function paradigm, yet many critical biological processes - from\nenzyme catalysis to membrane transport - depend on proteins that adopt multiple\nconformational states. Existing multi-state design approaches rely on post-hoc\naggregation of single-state predictions, achieving poor experimental success\nrates compared to single-state design. We introduce DynamicMPNN, an inverse\nfolding model explicitly trained to generate sequences compatible with multiple\nconformations through joint learning across conformational ensembles. Trained\non 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated\nusing AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13%\non structure-normalized RMSD across our challenging multi-state protein\nbenchmark.", "comment": "ICML 2025 GenBio Workshop", "pdf_url": "http://arxiv.org/pdf/2507.21938v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21741", "title": "MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces", "authors": ["Shaojun E", "Yuchen Yang", "Jiaheng Wu", "Yan Zhang", "Tiejun Zhao", "Ziyan Chen"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.21741v1", "summary": "In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.21741v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.06424", "title": "Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues", "authors": ["Alexander Scarlatos", "Naiming Liu", "Jaewook Lee", "Richard Baraniuk", "Andrew Lan"], "categories": ["cs.CL", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published in AIED 2025: The 26th International Conference on Artificial Intelligence in Education", "url": "http://arxiv.org/abs/2503.06424v2", "summary": "Generative artificial intelligence (AI) has the potential to scale up\npersonalized tutoring through large language models (LLMs). Recent AI tutors\nare adapted for the tutoring task by training or prompting LLMs to follow\neffective pedagogical principles, though they are not trained to maximize\nstudent learning throughout the course of a dialogue. Therefore, they may\nengage with students in a suboptimal way. We address this limitation by\nintroducing an approach to train LLMs to generate tutor utterances that\nmaximize the likelihood of student correctness, while still encouraging the\nmodel to follow good pedagogical practice. Specifically, we generate a set of\ncandidate tutor utterances and score them using (1) an LLM-based student model\nto predict the chance of correct student responses and (2) a pedagogical rubric\nevaluated by GPT-4o. We then use the resulting data to train an open-source\nLLM, Llama 3.1 8B, using direct preference optimization. We show that tutor\nutterances generated by our model lead to significantly higher chances of\ncorrect student responses while maintaining the pedagogical quality of GPT-4o.\nWe also conduct qualitative analyses and a human evaluation to demonstrate that\nour model generates high quality tutor utterances.", "comment": "Published in AIED 2025: The 26th International Conference on\n  Artificial Intelligence in Education", "pdf_url": "http://arxiv.org/pdf/2503.06424v2", "cate": "cs.CL", "date": "2025-03-09", "updated": "2025-07-28"}
{"id": "2501.13555", "title": "Instantaneous Core Loss -- Cycle-by-cycle Modeling of Power Magnetics in PWM DC-AC Converters", "authors": ["Binyu Cui", "Jun Wang", "Xibo Yuan", "Alfonso Martinez", "George Slama", "Matthew Wilkowski", "Ryosuke Ota", "Keiji Wada"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.13555v2", "summary": "Nowadays, PWM excitation is one of the most common waveforms seen by magnetic\ncomponents in power electronic converters. Core loss modelling approaches such\nas improved Generalized Steinmetz equation (iGSE) or the loss map based on\ncomposite waveform hypothesis (CWH) process the PWM excitation piecewisely,\nwhich is proven to be effective for DC DC converters. As the additional\nchallenge in PWM DC AC converters, the fundamental-frequency sinewave component\ninduces the \"major loop loss\" on top of the piecewise high-frequency segments,\nwhich however cannot be modelled on a switching cycle basis by any existing\nmethods. To address this gap, this paper proposes a novel fundamental concept,\ninstantaneous core loss, which is the time-domain core loss observed\nexperimentally for the first time in history. Extending the reactive voltage\ncancellation concept, this work presents a method to measure the instantaneous\ncore loss, which only contains real power loss, as a function of time. Based on\nmeasurements in evaluated soft magnetic components, it was discovered that the\ndischarging stage exhibits higher core loss than the charging stage. A\nmodelling approach is then proposed to break down the major loop core loss,\ntypically an average value in the literature, into the time domain to enable\ncycle-by-cycle modelling of core losses in PWM converters. This work enhances\nthe fundamental understanding of the core loss process by moving from the\naverage model to the time-domain model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.13555v2", "cate": "eess.SY", "date": "2025-01-23", "updated": "2025-07-29"}
{"id": "2506.10513", "title": "A Neural Network-aided Low Complexity Chase Decoder for URLLC", "authors": ["Enrico Testi", "Enrico Paolini"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.10513v2", "summary": "Ultra-reliable low-latency communications (URLLC) demand decoding algorithms\nthat simultaneously offer high reliability and low complexity under stringent\nlatency constraints. While iterative decoding schemes for LDPC and Polar codes\noffer a good compromise between performance and complexity, they fall short in\napproaching the theoretical performance limits in the typical URLLC short block\nlength regime. Conversely, quasi-ML decoding schemes for algebraic codes, like\nChase-II decoding, exhibit a smaller gap to optimum decoding but are\ncomputationally prohibitive for practical deployment in URLLC systems. To\nbridge this gap, we propose an enhanced Chase-II decoding algorithm that\nleverages a neural network (NN) to predict promising perturbation patterns,\ndrastically reducing the number of required decoding trials. The proposed\napproach combines the reliability of quasi-ML decoding with the efficiency of\nNN inference, making it well-suited for time-sensitive and resource-constrained\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.10513v2", "cate": "eess.SP", "date": "2025-06-12", "updated": "2025-07-29"}
{"id": "2506.21349", "title": "Generalizable Neural Electromagnetic Inverse Scattering", "authors": ["Yizhe Cheng", "Chunxun Tian", "Haoru Wang", "Wentao Zhu", "Xiaoxuan Ma", "Yizhou Wang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21349v3", "summary": "Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in\napplications such as medical imaging, where the goal is to reconstruct the\nrelative permittivity from scattered electromagnetic field. This inverse\nprocess is inherently ill-posed and highly nonlinear, making it particularly\nchallenging. A recent machine learning-based approach, Img-Interiors, shows\npromising results by leveraging continuous implicit functions. However, it\nrequires case-specific optimization, lacks generalization to unseen data, and\nfails under sparse transmitter setups (e.g., with only one transmitter). To\naddress these limitations, we revisit EISP from a physics-informed perspective,\nreformulating it as a two stage inverse transmission-scattering process. This\nformulation reveals the induced current as a generalizable intermediate\nrepresentation, effectively decoupling the nonlinear scattering process from\nthe ill-posed inverse problem. Built on this insight, we propose the first\ngeneralizable physics-driven framework for EISP, comprising a current estimator\nand a permittivity solver, working in an end-to-end manner. The current\nestimator explicitly learns the induced current as a physical bridge between\nthe incident and scattered field, while the permittivity solver computes the\nrelative permittivity directly from the estimated induced current. This design\nenables data-driven training and generalizable feed-forward prediction of\nrelative permittivity on unseen data while maintaining strong robustness to\ntransmitter sparsity. Extensive experiments show that our method outperforms\nstate-of-the-art approaches in reconstruction accuracy, generalization, and\nrobustness. This work offers a fundamentally new perspective on electromagnetic\ninverse scattering and represents a major step toward cost-effective practical\nsolutions for electromagnetic imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21349v3", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-29"}
{"id": "2507.21976", "title": "The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain", "authors": ["Tanvir Ahmed Khan", "Aranya Saha", "Ismam Nur Swapnil", "Mohammad Ariful Haque"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      12 pages, 5 figures. tcolorbox dependencies were removed for arXiv compatibility. All references are included via a precompiled .bbl file", "url": "http://arxiv.org/abs/2507.21976v1", "summary": "Multimodal Large Language Models (MLLMs) hold huge potential for usage in the\nmedical domain, but their computational costs necessitate efficient compression\ntechniques. This paper evaluates the impact of structural pruning and\nactivation-aware quantization on a fine-tuned LLAVA model for medical\napplications. We propose a novel layer selection method for pruning, analyze\ndifferent quantization techniques, and assess the performance trade-offs in a\nprune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B\nparameters to run within 4 GB of VRAM, reducing memory usage by 70% while\nachieving 4% higher model performance compared to traditional pruning and\nquantization techniques in the same compression ratio.", "comment": "12 pages, 5 figures. tcolorbox dependencies were removed for arXiv\n  compatibility. All references are included via a precompiled .bbl file", "pdf_url": "http://arxiv.org/pdf/2507.21976v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21963", "title": "SLA-Centric Automated Algorithm Selection Framework for Cloud Environments", "authors": ["Siana Rizwan", "Tasnim Ahmed", "Salimur Choudhury"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21963v1", "summary": "Cloud computing offers on-demand resource access, regulated by Service-Level\nAgreements (SLAs) between consumers and Cloud Service Providers (CSPs). SLA\nviolations can impact efficiency and CSP profitability. In this work, we\npropose an SLA-aware automated algorithm-selection framework for combinatorial\noptimization problems in resource-constrained cloud environments. The framework\nuses an ensemble of machine learning models to predict performance and rank\nalgorithm-hardware pairs based on SLA constraints. We also apply our framework\nto the 0-1 knapsack problem. We curate a dataset comprising instance specific\nfeatures along with memory usage, runtime, and optimality gap for 6 algorithms.\nAs an empirical benchmark, we evaluate the framework on both classification and\nregression tasks. Our ablation study explores the impact of hyperparameters,\nlearning approaches, and large language models effectiveness in regression, and\nSHAP-based interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21963v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21742", "title": "Adversarial Reconstruction Feedback for Robust Fine-grained Generalization", "authors": ["Shijie Wang", "Jian Shi", "Haojie Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.21742v1", "summary": "Existing fine-grained image retrieval (FGIR) methods predominantly rely on\nsupervision from predefined categories to learn discriminative representations\nfor retrieving fine-grained objects. However, they inadvertently introduce\ncategory-specific semantics into the retrieval representation, creating\nsemantic dependencies on predefined classes that critically hinder\ngeneralization to unseen categories. To tackle this, we propose AdvRF, a novel\nadversarial reconstruction feedback framework aimed at learning\ncategory-agnostic discrepancy representations. Specifically, AdvRF reformulates\nFGIR as a visual discrepancy reconstruction task via synergizing category-aware\ndiscrepancy localization from retrieval models with category-agnostic feature\nlearning from reconstruction models. The reconstruction model exposes residual\ndiscrepancies overlooked by the retrieval model, forcing it to improve\nlocalization accuracy, while the refined signals from the retrieval model guide\nthe reconstruction model to improve its reconstruction ability. Consequently,\nthe retrieval model localizes visual differences, while the reconstruction\nmodel encodes these differences into category-agnostic representations. This\nrepresentation is then transferred to the retrieval model through knowledge\ndistillation for efficient deployment. Quantitative and qualitative evaluations\ndemonstrate that our AdvRF achieves impressive performance on both widely-used\nfine-grained and coarse-grained datasets.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.21742v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.00626", "title": "Probabilistically safe and efficient model-based reinforcement learning", "authors": ["Filippo Airaldi", "Bart De Schutter", "Azita Dabiri"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures, accepted to 2025 CDC", "url": "http://arxiv.org/abs/2504.00626v2", "summary": "This paper proposes tackling safety-critical stochastic Reinforcement\nLearning (RL) tasks with a sample-based, model-based approach. At the core of\nthe method lies a Model Predictive Control (MPC) scheme that acts as function\napproximation, providing a model-based predictive control policy. To ensure\nsafety, a probabilistic Control Barrier Function (CBF) is integrated into the\nMPC controller. To approximate the effects of stochasticies in the optimal\ncontrol formulation and to fulfil the probabilistic CBF condition, a\nsample-based approach with guarantees is employed. Furthermore, to\ncounterbalance the additional computational burden due to sampling, a learnable\nterminal cost formulation is included in the MPC objective. An RL algorithm is\ndeployed to learn both the terminal cost and the CBF constraint. Results from a\nnumerical experiment on a constrained LTI problem corroborate the effectiveness\nof the proposed methodology in reducing computation time while preserving\ncontrol performance and safety.", "comment": "8 pages, 4 figures, accepted to 2025 CDC", "pdf_url": "http://arxiv.org/pdf/2504.00626v2", "cate": "eess.SY", "date": "2025-04-01", "updated": "2025-07-29"}
{"id": "2506.22252", "title": "On the Feasibility of Distributed Phase Synchronization for Coherent Signal Superposition", "authors": ["Alphan Sahin"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at IEEE PIMRC - Workshop on Integrated, Intelligent and Ubiquitous Connectivity for 6G and Beyond", "url": "http://arxiv.org/abs/2506.22252v2", "summary": "In this study, we analyze the feasibility of distributed phase\nsynchronization for coherent signal superposition, a fundamental enabler for\nparadigms such as coherent over-the-air computation (OAC), distributed\nbeamforming, and interference alignment, under mobility and hardware\nimpairments. With the focus on coherent OAC, we introduce phase-coded pilots\n(PCPs), a strategy where the radios communicate with each other to eliminate\nthe round-trip phase change in the uplink (UL) and downlink (DL) to align the\nphase of the received symbol at a desired angle. In this study, considering a\ncarrier frequency offset (CFO)-resilient multi-user procedure, we derive the\nstatistics of the phase deviations to assess how fast the phase coherency\ndegrades. Our results show that residual CFO is a major factor determining the\nduration of phase coherency, in addition to the non-negligible effects of\nmobility and the number of nodes in the network. We also provide a\nproof-of-concept demonstration for coherent signal superposition by using\noff-the-shelf radios to demonstrate the feasibility of PCPs in practice.", "comment": "Accepted for presentation at IEEE PIMRC - Workshop on Integrated,\n  Intelligent and Ubiquitous Connectivity for 6G and Beyond", "pdf_url": "http://arxiv.org/pdf/2506.22252v2", "cate": "eess.SP", "date": "2025-06-27", "updated": "2025-07-29"}
{"id": "2507.22009", "title": "PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences", "authors": ["Bahar İlgen", "Akshat Dubey", "Georges Hattab"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Preprint. Under review", "url": "http://arxiv.org/abs/2507.22009v1", "summary": "Ensuring transparency and trust in AI-driven public health and biomedical\nsciences systems requires more than accurate predictions-it demands\nexplanations that are clear, contextual, and socially accountable. While\nexplainable AI (XAI) has advanced in areas like feature attribution and model\ninterpretability, most methods still lack the structure and adaptability needed\nfor diverse health stakeholders, including clinicians, policymakers, and the\ngeneral public. We introduce PHAX-a Public Health Argumentation and\neXplainability framework-that leverages structured argumentation to generate\nhuman-centered explanations for AI outputs. PHAX is a multi-layer architecture\ncombining defeasible reasoning, adaptive natural language techniques, and user\nmodeling to produce context-aware, audience-specific justifications. More\nspecifically, we show how argumentation enhances explainability by supporting\nAI-driven decision-making, justifying recommendations, and enabling interactive\ndialogues across user types. We demonstrate the applicability of PHAX through\nuse cases such as medical term simplification, patient-clinician communication,\nand policy justification. In particular, we show how simplification decisions\ncan be modeled as argument chains and personalized based on user\nexpertise-enhancing both interpretability and trust. By aligning formal\nreasoning methods with communicative demands, PHAX contributes to a broader\nvision of transparent, human-centered AI in public health.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2507.22009v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21983", "title": "Improving Generative Ad Text on Facebook using Reinforcement Learning", "authors": ["Daniel R. Jiang", "Alex Nikulkov", "Yu-Chia Chen", "Yang Bai", "Zheqing Zhu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      D.J. and A.N. contributed equally, 41 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21983v1", "summary": "Generative artificial intelligence (AI), in particular large language models\n(LLMs), is poised to drive transformative economic change. LLMs are pre-trained\non vast text data to learn general language patterns, but a subsequent\npost-training phase is critical to align them for specific real-world tasks.\nReinforcement learning (RL) is the leading post-training technique, yet its\neconomic impact remains largely underexplored and unquantified. We examine this\nquestion through the lens of the first deployment of an RL-trained LLM for\ngenerative advertising on Facebook. Integrated into Meta's Text Generation\nfeature, our model, \"AdLlama,\" powers an AI tool that helps advertisers create\nnew variations of human-written ad text. To train this model, we introduce\nreinforcement learning with performance feedback (RLPF), a post-training method\nthat uses historical ad performance data as a reward signal. In a large-scale\n10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad\nvariations, we find that AdLlama improves click-through rates by 6.7%\n(p=0.0296) compared to a supervised imitation model trained on curated ads.\nThis represents a substantial improvement in advertiser return on investment on\nFacebook. We also find that advertisers who used AdLlama generated more ad\nvariations, indicating higher satisfaction with the model's outputs. To our\nknowledge, this is the largest study to date on the use of generative AI in an\necologically valid setting, offering an important data point quantifying the\ntangible impact of RL post-training. Furthermore, the results show that RLPF is\na promising and generalizable approach for metric-driven post-training that\nbridges the gap between highly capable language models and tangible outcomes.", "comment": "D.J. and A.N. contributed equally, 41 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21983v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21745", "title": "Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10 pages, 3 figures, 6 tables. Our model, training code and dataset will be at this https URL", "url": "http://arxiv.org/abs/2507.21745v1", "summary": "Recent advances in large language and vision-language models have enabled\nstrong reasoning capabilities, yet they remain impractical for specialized\ndomains like remote sensing, where annotated data is scarce and expensive. We\npresent the first few-shot reinforcement learning with verifiable reward (RLVR)\nframework for satellite imagery that eliminates the need for caption\nsupervision--relying solely on lightweight, rule-based binary or IoU-based\nrewards. Adapting the \"1-shot RLVR\" paradigm from language models to\nvision-language models, we employ policy-gradient optimization with as few as\none curated example to align model outputs for satellite reasoning tasks.\nComprehensive experiments across multiple remote sensing benchmarks--including\nclassification, visual question answering, and grounding--show that even a\nsingle example yields substantial improvements over the base model. Scaling to\n128 examples matches or exceeds models trained on thousands of annotated\nsamples. While the extreme one-shot setting can induce mild, task-specific\noverfitting, our approach consistently demonstrates robust generalization and\nefficiency across diverse tasks. Further, we find that prompt design and loss\nweighting significantly influence training stability and final accuracy. Our\nmethod enables cost-effective and data-efficient development of\ndomain-specialist vision-language reasoning models, offering a pragmatic recipe\nfor data-scarce fields: start from a compact VLM, curate a handful of\nreward-checkable cases, and train via RLVR.", "comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10\n  pages, 3 figures, 6 tables. Our model, training code and dataset will be at\n  https://github.com/aybora/FewShotReasoning", "pdf_url": "http://arxiv.org/pdf/2507.21745v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.18331", "title": "Unifying Direct and Indirect Learning for Safe Control of Linear Systems", "authors": ["Amir Modares", "Niyousha Ghiasi", "Bahare Kiumarsi", "Hamidreza Modares"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2502.04195", "url": "http://arxiv.org/abs/2504.18331v2", "summary": "This paper develops learning-enabled safe controllers for linear systems\nsubject to system uncertainties and bounded disturbances. Given the disturbance\nzonotope, the databased closed-loop dynamics (CLDs) are first characterized\nusing a matrix zonotope (MZ), and refined through several steps to yield a\nconstrained matrix zonotope (CMZ). This refinement is achieved by introducing\nconformal equality constraints that eliminate incompatible disturbance\nrealizations. More precisely, prior knowledge and observed data are used\nseparately to construct CMZ representations of disturbance sequences that\nconform to both data and prior knowledge, and are intersected by the initial MZ\nof the disturbance sequence, producing a refined CMZ. This approach reduces\nconservatism. To further reduce the conservativeness, we unify open-loop\nlearning with closed-loop learning by presenting a novel set-membership\nidentification method that models open-loop dynamics as a CMZ. The prior\nknowledge serves as an initial feasible open-loop model set (FOLMS) of this\nCMZ, which is refined into a posterior set whenever new informative online data\nbecomes available. This posterior FOLMS then adaptively replaces the prior\nknowledge set employed in the disturbance elimination of the closed-loop\nlearning process. The resulting refined parameterized set of CLD is\nsubsequently leveraged to directly and adaptively learn a controller that\nrobustly enforces safety. Toward this goal, we formulate a linear programming\nproblem that guarantees {\\lambda}contractiveness of a polyhedral safe set. A\nsimulation example is provided to validate the effectiveness of the proposed\napproach and support the theoretical results.", "comment": "arXiv admin note: text overlap with arXiv:2502.04195", "pdf_url": "http://arxiv.org/pdf/2504.18331v2", "cate": "eess.SY", "date": "2025-04-25", "updated": "2025-07-29"}
{"id": "2503.16531", "title": "EEG-CLIP : Learning EEG representations from natural language descriptions", "authors": ["Tidiane Camaret Ndir", "Robin Tibor Schirrmeister", "Tonio Ball"], "categories": ["cs.CL", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.16531v2", "summary": "Deep networks for electroencephalogram (EEG) decoding are often only trained\nto solve one specific task, such as pathology or age decoding. A more general\ntask-agnostic approach is to train deep networks to match a (clinical) EEG\nrecording to its corresponding textual medical report and vice versa. This\napproach was pioneered in the computer vision domain matching images and their\ntext captions and subsequently allowed to do successful zero-shot decoding\nusing textual class prompts. In this work, we follow this approach and develop\na contrastive learning framework, EEG-CLIP, that aligns the EEG time series and\nthe descriptions of the corresponding clinical text in a shared embedding\nspace. We investigated its potential for versatile EEG decoding, evaluating\nperformance in a range of few-shot and zero-shot settings. Overall, we show\nthat EEG-CLIP manages to non-trivially align text and EEG representations. Our\nwork presents a promising approach to learn general EEG representations, which\ncould enable easier analyses of diverse decoding questions through zero-shot\ndecoding or training task-specific models from fewer training examples. The\ncode for reproducing our results is available at\nhttps://github.com/tidiane-camaret/EEGClip", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.16531v2", "cate": "cs.CL", "date": "2025-03-18", "updated": "2025-07-29"}
{"id": "2507.22450", "title": "Settling Weighted Token Swapping up to Algorithmic Barriers", "authors": ["Nicole Wein", "Guanyu", "Zhang"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22450v1", "summary": "We study the weighted token swapping problem, in which we are given a graph\non $n$ vertices, $n$ weighted tokens, an initial assignment of one token to\neach vertex, and a final assignment of one token to each vertex. The goal is to\nfind a minimum-cost sequence of swaps of adjacent tokens to reach the final\nassignment from the initial assignment, where the cost is the sum over all\nswaps of the sum of the weights of the two swapped tokens. Unweighted token\nswapping has been extensively studied: it is NP-hard to approximate to a factor\nbetter than $14/13$, and there is a polynomial-time 4-approximation, along with\na tight \"barrier\" result showing that the class of locally optimal algorithms\ncannot achieve a ratio better than 4. For trees, the problem remains NP-hard to\nsolve exactly, and there is a polynomial-time 2-approximation, along with a\ntight barrier result showing that the class of $\\ell$-straying algorithms\ncannot achieve a ratio better than 2. Weighted token swapping with $\\{0,1\\}$\nweights is much harder to approximation: it is NP-hard to approximate even to a\nfactor of $(1-\\varepsilon) \\cdot \\ln n$ for any constant $\\varepsilon>0$.\nRestricting to positive weights, no approximation algorithms are known, and the\nonly known lower bounds are those inherited directly from the unweighted\nversion. We provide the first approximation algorithms for weighted token\nswapping on both trees and general graphs, along with tight barrier results.\nLetting $w$ and $W$ be the minimum and maximum token weights, our approximation\nratio is $2+2W/w$ for general graphs and $1+W/w$ for trees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22450v1", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22025", "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding", "authors": ["Shuquan Lian", "Yuhang Wu", "Jia Ma", "Zihan Song", "Bingqi Chen", "Xiawu Zheng", "Hui Li"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22025v2", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22025v2", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21992", "title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation", "authors": ["Siddhartha Pradhan", "Shikshya Shiwakoti", "Neha Bathuri"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.21992v1", "summary": "We investigate whether knowledge distillation (KD) from multiple\nheterogeneous teacher models can enhance the generation of transferable\nadversarial examples. A lightweight student model is trained using two KD\nstrategies: curriculum-based switching and joint optimization, with ResNet50\nand DenseNet-161 as teachers. The trained student is then used to generate\nadversarial examples using FG, FGS, and PGD attacks, which are evaluated\nagainst a black-box target model (GoogLeNet). Our results show that student\nmodels distilled from multiple teachers achieve attack success rates comparable\nto ensemble-based baselines, while reducing adversarial example generation time\nby up to a factor of six. An ablation study further reveals that lower\ntemperature settings and the inclusion of hard-label supervision significantly\nenhance transferability. These findings suggest that KD can serve not only as a\nmodel compression technique but also as a powerful tool for improving the\nefficiency and effectiveness of black-box adversarial attacks.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.21992v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21756", "title": "LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection", "authors": ["Jing Ren", "Suyu Ma", "Hong Jia", "Xiwei Xu", "Ivan Lee", "Haytham Fayek", "Xiaodong Li", "Feng Xia"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 1 figure", "url": "http://arxiv.org/abs/2507.21756v1", "summary": "Detecting driver fatigue is critical for road safety, as drowsy driving\nremains a leading cause of traffic accidents. Many existing solutions rely on\ncomputationally demanding deep learning models, which result in high latency\nand are unsuitable for embedded robotic devices with limited resources (such as\nintelligent vehicles/cars) where rapid detection is necessary to prevent\naccidents. This paper introduces LiteFat, a lightweight spatio-temporal graph\nlearning model designed to detect driver fatigue efficiently while maintaining\nhigh accuracy and low computational demands. LiteFat involves converting\nstreaming video data into spatio-temporal graphs (STG) using facial landmark\ndetection, which focuses on key motion patterns and reduces unnecessary data\nprocessing. LiteFat uses MobileNet to extract facial features and create a\nfeature matrix for the STG. A lightweight spatio-temporal graph neural network\nis then employed to identify signs of fatigue with minimal processing and low\nlatency. Experimental results on benchmark datasets show that LiteFat performs\ncompetitively while significantly decreasing computational complexity and\nlatency as compared to current state-of-the-art methods. This work enables the\ndevelopment of real-time, resource-efficient human fatigue detection systems\nthat can be implemented upon embedded robotic devices.", "comment": "6 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.21756v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2405.07432", "title": "Nonparametric Sparse Online Learning of the Koopman Operator", "authors": ["Boya Hou", "Sina Sanjari", "Nathan Dahlin", "Alec Koppel", "Subhonmesh Bose"], "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      47 pages, 6 figures", "url": "http://arxiv.org/abs/2405.07432v3", "summary": "The Koopman operator provides a powerful framework for representing the\ndynamics of general nonlinear dynamical systems. However, existing data-driven\napproaches to learning the Koopman operator rely on batch data. In this work,\nwe present a sparse online learning algorithm that learns the Koopman operator\niteratively via stochastic approximation, with explicit control over model\ncomplexity and provable convergence guarantees. Specifically, we study the\nKoopman operator via its action on the reproducing kernel Hilbert space (RKHS),\nand address the mis-specified scenario where the dynamics may escape the chosen\nRKHS. In this mis-specified setting, we relate the Koopman operator to the\nconditional mean embeddings (CME) operator. We further establish both\nasymptotic and finite-time convergence guarantees for our learning algorithm in\nmis-specified setting, with trajectory-based sampling where the data arrive\nsequentially over time. Numerical experiments demonstrate the algorithm's\ncapability to learn unknown nonlinear dynamics.", "comment": "47 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2405.07432v3", "cate": "stat.ML", "date": "2024-05-13", "updated": "2025-07-29"}
{"id": "2507.09887", "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram", "authors": ["Huynh Dang Nguyen", "Trong-Thang Pham", "Ngan Le", "Van Nguyen"], "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ACM MM 2025", "url": "http://arxiv.org/abs/2507.09887v2", "summary": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing\nheart diseases. However, its effectiveness can be compromised by noise or\nunavailability of one or more leads of the standard 12-lead recordings,\nresulting in diagnostic errors or uncertainty. To address these challenges, we\npropose TolerantECG, a foundation model for ECG signals that is robust to noise\nand capable of functioning with arbitrary subsets of the standard 12-lead ECG.\nTolerantECG training combines contrastive and self-supervised learning\nframeworks to jointly learn ECG signal representations alongside their\ncorresponding knowledge-retrieval-based text report descriptions and corrupted\nor lead-missing signals. Comprehensive benchmarking results demonstrate that\nTolerantECG consistently ranks as the best or second-best performer across\nvarious ECG signal conditions and class levels in the PTB-XL dataset, and\nachieves the highest performance on the MIT-BIH Arrhythmia Database.", "comment": "Accepted at ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09887v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-29"}
{"id": "2507.22486", "title": "Deterministic Longest Common Subsequence Approximation in Near-Linear Time", "authors": ["Itai Boneh", "Shay Golan", "Matan Kraus"], "categories": ["cs.DS", "F.2.0"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22486v1", "summary": "We provide a deterministic algorithm that outputs an $O(n^{3/4} \\log\nn)$-approximation for the Longest Common Subsequence (LCS) of two input\nsequences of length $n$ in near-linear time. This is the first deterministic\napproximation algorithm for LCS that achieves a sub-linear approximation ratio\nin near-linear time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22486v1", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22034", "title": "UserBench: An Interactive Gym Environment for User-Centric Agents", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Zhiwei Liu", "Jianguo Zhang", "Haolin Chen", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      25 Pages, 17 Figures, 6 Tables", "url": "http://arxiv.org/abs/2507.22034v1", "summary": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.", "comment": "25 Pages, 17 Figures, 6 Tables", "pdf_url": "http://arxiv.org/pdf/2507.22034v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22032", "title": "Classification of Honey Botanical and Geographical Sources using Mineral Profiles and Machine Learning", "authors": ["Mokhtar Al-Awadhi", "Ratnadeep Deshmukh"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures, conference paper", "url": "http://arxiv.org/abs/2507.22032v1", "summary": "This paper proposes a machine learning-based approach for identifying honey\nfloral and geographical sources using mineral element profiles. The proposed\nmethod comprises two steps: preprocessing and classification. The preprocessing\nphase involves missing-value treatment and data normalization. In the\nclassification phase, we employ various supervised classification models for\ndiscriminating between six botanical sources and 13 geographical origins of\nhoney. We test the classifiers' performance on a publicly available honey\nmineral element dataset. The dataset contains mineral element profiles of\nhoneys from various floral and geographical origins. Results show that mineral\nelement content in honey provides discriminative information useful for\nclassifying honey botanical and geographical sources. Results also show that\nthe Random Forests (RF) classifier obtains the best performance on this\ndataset, achieving a cross-validation accuracy of 99.30% for classifying honey\nbotanical origins and 98.01% for classifying honey geographical origins.", "comment": "13 pages, 7 figures, conference paper", "pdf_url": "http://arxiv.org/pdf/2507.22032v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21761", "title": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions", "authors": ["YiZhou Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages,9 figuers", "url": "http://arxiv.org/abs/2507.21761v1", "summary": "Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios.", "comment": "18 pages,9 figuers", "pdf_url": "http://arxiv.org/pdf/2507.21761v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.06750", "title": "Robust Capacity Expansion Modelling for Renewable Energy Systems under Weather Uncertainty", "authors": ["Sebastian Kebrich", "Felix Engelhardt", "David Franzmann", "Christina Büsing", "Jochen Linßen", "Heidi Heinrichs"], "categories": ["math.OC", "cs.SY", "eess.SY", "90C10, 90C15, 90C90", "I.6"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06750v2", "summary": "Future greenhouse gas neutral energy systems will be dominated by renewable\nenergy technologies whose energy output is subject to uncertain weather\nconditions. This work proposes an algorithm to do capacity expansion planning\n(CAPEX) under weather uncertainty. When faced with multiple possible weather\nyears, the quality of a CAPEX solution derived on a single year's data is\nevaluated across all years, and the CAPEX optimisation problem is iteratively\nmodified whenever supply gaps are detected. These modifications lead to\nsolutions with sufficient back--up capacity to overcome periods of (cold) dark\nlulls, and sufficient total annual energy supply across all years. A\ncomputational study on an energy system model of Germany shows that the\niterative algorithm finds solutions that guarantee security of supply for all\nconsidered weather years for an increase of 1.6-2.9% in total annual cost\ncompared to initial solutions. Results also underline the importance of\nassessing the feasibility of energy system models using atypical time--series,\nincluding dark lull and cold period effects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06750v2", "cate": "math.OC", "date": "2025-04-09", "updated": "2025-07-29"}
{"id": "2507.22764", "title": "BlockFIFO & MultiFIFO: Scalable Relaxed Queues", "authors": ["Stefan Koch", "Peter Sanders", "Marvin Williams"], "categories": ["cs.DS", "D.1.3; E.1"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      25 pages", "url": "http://arxiv.org/abs/2507.22764v1", "summary": "FIFO queues are a fundamental data structure used in a wide range of\napplications. Concurrent FIFO queues allow multiple execution threads to access\nthe queue simultaneously. Maintaining strict FIFO semantics in concurrent\nqueues leads to low throughput due to high contention at the head and tail of\nthe queue. By relaxing the FIFO semantics to allow some reordering of elements,\nit becomes possible to achieve much higher scalability. This work presents two\northogonal designs for relaxed concurrent FIFO queues, one derived from the\nMultiQueue and the other based on ring buffers. We evaluate both designs\nextensively on various micro-benchmarks and a breadth-first search application\non large graphs. Both designs outperform state-of-the-art relaxed and strict\nFIFO queues, achieving higher throughput and better scalability.", "comment": "25 pages", "pdf_url": "http://arxiv.org/pdf/2507.22764v1", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22047", "title": "The Interspeech 2025 Speech Accessibility Project Challenge", "authors": ["Xiuwen Zheng", "Bornali Phukon", "Jonghwan Na", "Ed Cutrell", "Kyu Han", "Mark Hasegawa-Johnson", "Pan-Pan Jiang", "Aadhrik Kuila", "Colin Lea", "Bob MacDonald", "Gautam Mantena", "Venkatesh Ravichandran", "Leda Sari", "Katrin Tomanek", "Chang D. Yoo", "Chris Zwilling"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      To appear in Proceedings of Interspeech, 2025", "url": "http://arxiv.org/abs/2507.22047v1", "summary": "While the last decade has witnessed significant advancements in Automatic\nSpeech Recognition (ASR) systems, performance of these systems for individuals\nwith speech disabilities remains inadequate, partly due to limited public\ntraining data. To bridge this gap, the 2025 Interspeech Speech Accessibility\nProject (SAP) Challenge was launched, utilizing over 400 hours of SAP data\ncollected and transcribed from more than 500 individuals with diverse speech\ndisabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,\nthe SAP Challenge evaluates submissions based on Word Error Rate and Semantic\nScore. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2\nbaseline in terms of WER, while 17 teams surpassed the baseline on SemScore.\nNotably, the top team achieved the lowest WER of 8.11\\%, and the highest\nSemScore of 88.44\\% at the same time, setting new benchmarks for future ASR\nsystems in recognizing impaired speech.", "comment": "To appear in Proceedings of Interspeech, 2025", "pdf_url": "http://arxiv.org/pdf/2507.22047v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22040", "title": "Structure-Informed Deep Reinforcement Learning for Inventory Management", "authors": ["Alvaro Maggiar", "Sohrab Andaz", "Akhil Bagaria", "Carson Eisenach", "Dean Foster", "Omer Gottesman", "Dominique Perrault-Joncas"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22040v1", "summary": "This paper investigates the application of Deep Reinforcement Learning (DRL)\nto classical inventory management problems, with a focus on practical\nimplementation considerations. We apply a DRL algorithm based on DirectBackprop\nto several fundamental inventory management scenarios including multi-period\nsystems with lost sales (with and without lead times), perishable inventory\nmanagement, dual sourcing, and joint inventory procurement and removal. The DRL\napproach learns policies across products using only historical information that\nwould be available in practice, avoiding unrealistic assumptions about demand\ndistributions or access to distribution parameters. We demonstrate that our\ngeneric DRL implementation performs competitively against or outperforms\nestablished benchmarks and heuristics across these diverse settings, while\nrequiring minimal parameter tuning. Through examination of the learned\npolicies, we show that the DRL approach naturally captures many known\nstructural properties of optimal policies derived from traditional operations\nresearch methods. To further improve policy performance and interpretability,\nwe propose a Structure-Informed Policy Network technique that explicitly\nincorporates analytically-derived characteristics of optimal policies into the\nlearning process. This approach can help interpretability and add robustness to\nthe policy in out-of-sample performance, as we demonstrate in an example with\nrealistic demand data. Finally, we provide an illustrative application of DRL\nin a non-stationary setting. Our work bridges the gap between data-driven\nlearning and analytical insights in inventory management while maintaining\npractical applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22040v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21778", "title": "AU-LLM: Micro-Expression Action Unit Detection via Enhanced LLM-Based Feature Fusion", "authors": ["Zhishu Liu", "Kaishen Yuan", "Bo Zhao", "Yong Xu", "Zitong Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21778v1", "summary": "The detection of micro-expression Action Units (AUs) is a formidable\nchallenge in affective computing, pivotal for decoding subtle, involuntary\nhuman emotions. While Large Language Models (LLMs) demonstrate profound\nreasoning abilities, their application to the fine-grained, low-intensity\ndomain of micro-expression AU detection remains unexplored. This paper pioneers\nthis direction by introducing \\textbf{AU-LLM}, a novel framework that for the\nfirst time uses LLM to detect AUs in micro-expression datasets with subtle\nintensities and the scarcity of data. We specifically address the critical\nvision-language semantic gap, the \\textbf{Enhanced Fusion Projector (EFP)}. The\nEFP employs a Multi-Layer Perceptron (MLP) to intelligently fuse mid-level\n(local texture) and high-level (global semantics) visual features from a\nspecialized 3D-CNN backbone into a single, information-dense token. This\ncompact representation effectively empowers the LLM to perform nuanced\nreasoning over subtle facial muscle movements.Through extensive evaluations on\nthe benchmark CASME II and SAMM datasets, including stringent\nLeave-One-Subject-Out (LOSO) and cross-domain protocols, AU-LLM establishes a\nnew state-of-the-art, validating the significant potential and robustness of\nLLM-based reasoning for micro-expression analysis. The codes are available at\nhttps://github.com/ZS-liu-JLU/AU-LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21778v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22265", "title": "Cell-Probe Lower Bounds via Semi-Random CSP Refutation: Simplified and the Odd-Locality Case", "authors": ["Venkatesan Guruswami", "Xin Lyu", "Weiqiang Yuan"], "categories": ["cs.CC", "cs.CR", "cs.DS"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      Comments welcome", "url": "http://arxiv.org/abs/2507.22265v1", "summary": "A recent work (Korten, Pitassi, and Impagliazzo, FOCS 2025) established an\ninsightful connection between static data structure lower bounds, range\navoidance of $\\text{NC}^0$ circuits, and the refutation of pseudorandom CSP\ninstances, leading to improvements to some longstanding lower bounds in the\ncell-probe/bit-probe models. Here, we improve these lower bounds in certain\ncases via a more streamlined reduction to XOR refutation, coupled with handling\nthe odd-arity case. Our result can be viewed as a complete derandomization of\nthe state-of-the-art semi-random $k$-XOR refutation analysis (Guruswami,\nKothari and Manohar, STOC 2022, Hsieh, Kothari and Mohanty, SODA 2023), which\ncomplements the derandomization of the even-arity case obtained by Korten et\nal.\n  As our main technical statement, we show that for any multi-output\nconstant-depth circuit that substantially stretches its input, its output is\nvery likely far from strings sampled from distributions with sufficient\nindependence, and further this can be efficiently certified. Via suitable\nshifts in perspectives, this gives applications to cell-probe lower bounds and\nrange avoidance algorithms for $\\mathsf{NC}^0$ circuits.", "comment": "Comments welcome", "pdf_url": "http://arxiv.org/pdf/2507.22265v1", "cate": "cs.CC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.17307", "title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning", "authors": ["Zhuokun Chen", "Zeren Chen", "Jiahao He", "Mingkui Tan", "Jianfei Cai", "Bohan Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17307v2", "summary": "Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of\nlarge language models by encouraging step-by-step intermediate reasoning during\ninference. While effective, CoT introduces substantial computational overhead\ndue to its reliance on autoregressive decoding over long token sequences.\nExisting acceleration strategies either reduce sequence length through early\nstopping or compressive reward designs, or improve decoding speed via\nspeculative decoding with smaller models. However, speculative decoding suffers\nfrom limited speedup when the agreement between small and large models is low,\nand fails to exploit the potential advantages of small models in producing\nconcise intermediate reasoning. In this paper, we present R-Stitch, a\ntoken-level, confidence-based hybrid decoding framework that accelerates CoT\ninference by switching between a small language model (SLM) and a large\nlanguage model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to\ngenerate tokens by default and delegates to the LLM only when the SLM's\nconfidence falls below a threshold. This design avoids full-sequence rollback\nand selectively invokes the LLM on uncertain steps, preserving both efficiency\nand answer quality. R-Stitch is model-agnostic, training-free, and compatible\nwith standard decoding pipelines. Experiments on math reasoning benchmarks\ndemonstrate that R-Stitch achieves up to 85\\% reduction in inference latency\nwith negligible accuracy drop, highlighting its practical effectiveness in\naccelerating CoT reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17307v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-24"}
{"id": "2507.22045", "title": "Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling", "authors": ["Haley Rosso", "Lars Ruthotto", "Khachik Sargsyan"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34 pages, 6 figures, submitted to the MoRE24 special issue of Computational Science and Engineering", "url": "http://arxiv.org/abs/2507.22045v1", "summary": "Continuous-time deep learning models, such as neural ordinary differential\nequations (ODEs), offer a promising framework for surrogate modeling of complex\nphysical systems. A central challenge in training these models lies in learning\nexpressive yet stable time-varying weights, particularly under computational\nconstraints. This work investigates weight parameterization strategies that\nconstrain the temporal evolution of weights to a low-dimensional subspace\nspanned by polynomial basis functions. We evaluate both monomial and Legendre\npolynomial bases within neural ODE and residual network (ResNet) architectures\nunder discretize-then-optimize and optimize-then-discretize training paradigms.\nExperimental results across three high-dimensional benchmark problems show that\nLegendre parameterizations yield more stable training dynamics, reduce\ncomputational cost, and achieve accuracy comparable to or better than both\nmonomial parameterizations and unconstrained weight models. These findings\nelucidate the role of basis choice in time-dependent weight parameterization\nand demonstrate that using orthogonal polynomial bases offers a favorable\ntradeoff between model expressivity and training efficiency.", "comment": "34 pages, 6 figures, submitted to the MoRE24 special issue of\n  Computational Science and Engineering", "pdf_url": "http://arxiv.org/pdf/2507.22045v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21786", "title": "MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning", "authors": ["Zhaolong Wang", "Tongfeng Sun", "Mingzheng Du", "Yachao Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21786v1", "summary": "Vision-language pre-trained models (VLMs) such as CLIP have demonstrated\nremarkable zero-shot generalization, and prompt learning has emerged as an\nefficient alternative to full fine-tuning. However, existing methods often\nstruggle with generalization to novel classes, a phenomenon attributed to\noverfitting on seen classes and forgetting general knowledge. Furthermore,\nrecent approaches that improve generalization often introduce complex\narchitectures or heavy computational overhead. In this paper, we propose a\nMultiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance\nfew-shot generalization while maintaining computational efficiency. Our\napproach leverages an ensemble of parallel learnable context vectors to capture\ndiverse semantic aspects. To enrich these prompts, we introduce a semantic\nguidance mechanism that aligns them with comprehensive class descriptions\nautomatically generated by a Large Language Model (LLM). Furthermore, a\ndiversity regularization loss encourages the prompts to learn complementary and\northogonal features, preventing them from collapsing into redundant\nrepresentations. Extensive experiments on 11 benchmark datasets show that\nMSGCoOp significantly improves performance on base-to-novel generalization,\nachieving an average harmonic mean improvement of 1.10\\% over the strong KgCoOp\nbaseline. Our method also demonstrates enhanced robustness in cross-domain\ngeneralization tasks. Our code is avaliable at:\n\\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21786v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2302.04624", "title": "A new width parameter of graphs based on edge cuts: $α$-edge-crossing width", "authors": ["Yeonsu Chang", "O-joung Kwon", "Myounghwan Lee"], "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      28 pages, 3 figures, accepted to WG2023", "url": "http://arxiv.org/abs/2302.04624v3", "summary": "We introduce graph width parameters, called $\\alpha$-edge-crossing width and\nedge-crossing width. These are defined in terms of the number of edges crossing\na bag of a tree-cut decomposition. They are motivated by edge-cut width,\nrecently introduced by Brand et al. (WG 2022). We show that edge-crossing width\nis equivalent to the known parameter tree-partition-width. On the other hand,\n$\\alpha$-edge-crossing width is a new parameter; tree-cut width and\n$\\alpha$-edge-crossing width are incomparable, and they both lie between\ntree-partition-width and edge-cut width.\n  We provide an algorithm that, for a given $n$-vertex graph $G$ and integers\n$k$ and $\\alpha$, in time $2^{O((\\alpha+k)\\log (\\alpha+k))}n^2$ either outputs\na tree-cut decomposition certifying that the $\\alpha$-edge-crossing width of\n$G$ is at most $2\\alpha^2+5k$ or confirms that the $\\alpha$-edge-crossing width\nof $G$ is more than $k$. As applications, for every fixed $\\alpha$, we obtain\nFPT algorithms for the List Coloring and Precoloring Extension problems\nparameterized by $\\alpha$-edge-crossing width. They were known to be W[1]-hard\nparameterized by tree-partition-width, and FPT parameterized by edge-cut width,\nand we close the complexity gap between these two parameters.", "comment": "28 pages, 3 figures, accepted to WG2023", "pdf_url": "http://arxiv.org/pdf/2302.04624v3", "cate": "cs.DS", "date": "2023-02-09", "updated": "2025-07-30"}
{"id": "2507.22213", "title": "Intent-Aware Neural Query Reformulation for Behavior-Aligned Product Search", "authors": ["Jayanth Yetukuri", "Ishita Khan"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at SIGIR eCom'25. this https URL", "url": "http://arxiv.org/abs/2507.22213v1", "summary": "Understanding and modeling buyer intent is a foundational challenge in\noptimizing search query reformulation within the dynamic landscape of\ne-commerce search systems. This work introduces a robust data pipeline designed\nto mine and analyze large-scale buyer query logs, with a focus on extracting\nfine-grained intent signals from both explicit interactions and implicit\nbehavioral cues. Leveraging advanced sequence mining techniques and supervised\nlearning models, the pipeline systematically captures patterns indicative of\nlatent purchase intent, enabling the construction of a high-fidelity,\nintent-rich dataset. The proposed framework facilitates the development of\nadaptive query rewrite strategies by grounding reformulations in inferred user\nintent rather than surface-level lexical signals. This alignment between query\nrewriting and underlying user objectives enhances both retrieval relevance and\ndownstream engagement metrics. Empirical evaluations across multiple product\nverticals demonstrate measurable gains in precision-oriented relevance metrics,\nunderscoring the efficacy of intent-aware reformulation. Our findings highlight\nthe value of intent-centric modeling in bridging the gap between sparse user\ninputs and complex product discovery goals, and establish a scalable foundation\nfor future research in user-aligned neural retrieval and ranking systems.", "comment": "Accepted at SIGIR eCom'25.\n  https://sigir-ecom.github.io/eCom25Papers/paper_23.pdf", "pdf_url": "http://arxiv.org/pdf/2507.22213v1", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.20894", "title": "Online hierarchical partitioning of the output space in extreme multi-label data stream", "authors": ["Lara Neves", "Afonso Lourenço", "Alberto Cano", "Goreti Marreiros"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at 28th European Conference on Artificial Intelligence (ECAI 2025)", "url": "http://arxiv.org/abs/2507.20894v1", "summary": "Mining data streams with multi-label outputs poses significant challenges due\nto evolving distributions, high-dimensional label spaces, sparse label\noccurrences, and complex label dependencies. Moreover, concept drift affects\nnot only input distributions but also label correlations and imbalance ratios\nover time, complicating model adaptation. To address these challenges,\nstructured learners are categorized into local and global methods. Local\nmethods break down the task into simpler components, while global methods adapt\nthe algorithm to the full output space, potentially yielding better predictions\nby exploiting label correlations. This work introduces iHOMER (Incremental\nHierarchy Of Multi-label Classifiers), an online multi-label learning framework\nthat incrementally partitions the label space into disjoint, correlated\nclusters without relying on predefined hierarchies. iHOMER leverages online\ndivisive-agglomerative clustering based on \\textit{Jaccard} similarity and a\nglobal tree-based learner driven by a multivariate \\textit{Bernoulli} process\nto guide instance partitioning. To address non-stationarity, it integrates\ndrift detection mechanisms at both global and local levels, enabling dynamic\nrestructuring of label partitions and subtrees. Experiments across 23\nreal-world datasets show iHOMER outperforms 5 state-of-the-art global\nbaselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\\%, and 12 local\nbaselines, such as binary relevance transformations of kNN, EFDT, ARF, and\nADWIN bagging/boosting ensembles, by 32\\%, establishing its robustness for\nonline multi-label classification.", "comment": "Accepted at 28th European Conference on Artificial Intelligence (ECAI\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.20894v1", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.22053", "title": "Foundation Models for Demand Forecasting via Dual-Strategy Ensembling", "authors": ["Wei Yang", "Defu Cao", "Yan Liu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22053v1", "summary": "Accurate demand forecasting is critical for supply chain optimization, yet\nremains difficult in practice due to hierarchical complexity, domain shifts,\nand evolving external factors. While recent foundation models offer strong\npotential for time series forecasting, they often suffer from architectural\nrigidity and limited robustness under distributional change. In this paper, we\npropose a unified ensemble framework that enhances the performance of\nfoundation models for sales forecasting in real-world supply chains. Our method\ncombines two complementary strategies: (1) Hierarchical Ensemble (HE), which\npartitions training and inference by semantic levels (e.g., store, category,\ndepartment) to capture localized patterns; and (2) Architectural Ensemble (AE),\nwhich integrates predictions from diverse model backbones to mitigate bias and\nimprove stability. We conduct extensive experiments on the M5 benchmark and\nthree external sales datasets, covering both in-domain and zero-shot\nforecasting. Results show that our approach consistently outperforms strong\nbaselines, improves accuracy across hierarchical levels, and provides a simple\nyet effective mechanism for boosting generalization in complex forecasting\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22053v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21794", "title": "Distribution-Based Masked Medical Vision-Language Model Using Structured Reports", "authors": ["Shreyank N Gowda", "Ruichi Zhang", "Xiao Gu", "Ying Weng", "Lu Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in MICCAI-W 2025", "url": "http://arxiv.org/abs/2507.21794v1", "summary": "Medical image-language pre-training aims to align medical images with\nclinically relevant text to improve model performance on various downstream\ntasks. However, existing models often struggle with the variability and\nambiguity inherent in medical data, limiting their ability to capture nuanced\nclinical information and uncertainty. This work introduces an uncertainty-aware\nmedical image-text pre-training model that enhances generalization capabilities\nin medical image analysis. Building on previous methods and focusing on Chest\nX-Rays, our approach utilizes structured text reports generated by a large\nlanguage model (LLM) to augment image data with clinically relevant context.\nThese reports begin with a definition of the disease, followed by the\n`appearance' section to highlight critical regions of interest, and finally\n`observations' and `verdicts' that ground model predictions in clinical\nsemantics. By modeling both inter- and intra-modal uncertainty, our framework\ncaptures the inherent ambiguity in medical images and text, yielding improved\nrepresentations and performance on downstream tasks. Our model demonstrates\nsignificant advances in medical image-text pre-training, obtaining\nstate-of-the-art performance on multiple downstream tasks.", "comment": "Accepted in MICCAI-W 2025", "pdf_url": "http://arxiv.org/pdf/2507.21794v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2305.03381", "title": "Tighter Approximation for the Uniform Cost-Distance Steiner Tree Problem", "authors": ["Josefine Foos", "Stephan Held", "Yannik Kyle Dustin Spitzley"], "categories": ["cs.DS", "90C27, 68W25, 68M10", "G.2.1; G.2.2; F.2.2; B.7.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2211.03830 ; Version 2: Addition of examples (tightness of the analysis, worst-case for any cut-and-reconnect algorithm, optimality gap of the lower bound in the Manhatten Plane)", "url": "http://arxiv.org/abs/2305.03381v2", "summary": "Uniform cost-distance Steiner trees minimize the sum of the total length and\nweighted path lengths from a dedicated root to the other terminals. They are\napplied when the tree is intended for signal transmission, e.g. in chip design\nor telecommunication networks. They are a special case of general cost-distance\nSteiner trees, where different distance functions are used for total length and\npath lengths.\n  We improve the best published approximation factor for the uniform\ncost-distance Steiner tree problem from 2.39 to 2.05. If we can approximate the\nminimum-length Steiner tree problem arbitrarily well, our algorithm achieves an\napproximation factor arbitrarily close to $ 1 + \\frac{1}{\\sqrt{2}} $. This\nbound is tight in the following sense. We also prove the gap $ 1 +\n\\frac{1}{\\sqrt{2}} $ between optimum solutions and the lower bound which we and\nall previous approximation algorithms for this problem use.\n  Similarly to previous approaches, we start with an approximate minimum-length\nSteiner tree and split it into subtrees that are later re-connected. To improve\nthe approximation factor, we split it into components more carefully, taking\nthe cost structure into account, and we significantly enhance the analysis.", "comment": "arXiv admin note: substantial text overlap with arXiv:2211.03830;\n  Version 2: Addition of examples (tightness of the analysis, worst-case for\n  any cut-and-reconnect algorithm, optimality gap of the lower bound in the\n  Manhatten Plane)", "pdf_url": "http://arxiv.org/pdf/2305.03381v2", "cate": "cs.DS", "date": "2023-05-05", "updated": "2025-07-30"}
{"id": "2507.22224", "title": "Generative Recommendation with Semantic IDs: A Practitioner's Handbook", "authors": ["Clark Mingxuan Ju", "Liam Collins", "Leonardo Neves", "Bhuvesh Kumar", "Louis Yufeng Wang", "Tong Zhao", "Neil Shah"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22224v1", "summary": "Generative recommendation (GR) has gained increasing attention for its\npromising performance compared to traditional models. A key factor contributing\nto the success of GR is the semantic ID (SID), which converts continuous\nsemantic representations (e.g., from large language models) into discrete ID\nsequences. This enables GR models with SIDs to both incorporate semantic\ninformation and learn collaborative filtering signals, while retaining the\nbenefits of discrete decoding. However, varied modeling techniques,\nhyper-parameters, and experimental setups in existing literature make direct\ncomparisons between GR proposals challenging. Furthermore, the absence of an\nopen-source, unified framework hinders systematic benchmarking and extension,\nslowing model iteration. To address this challenge, our work introduces and\nopen-sources a framework for Generative Recommendation with semantic ID, namely\nGRID, specifically designed for modularity to facilitate easy component\nswapping and accelerate idea iteration. Using GRID, we systematically\nexperiment with and ablate different components of GR models with SIDs on\npublic benchmarks. Our comprehensive experiments with GRID reveal that many\noverlooked architectural components in GR models with SIDs substantially impact\nperformance. This offers both novel insights and validates the utility of an\nopen-source platform for robust benchmarking and GR research advancement. GRID\nis open-sourced at https://github.com/snap-research/GRID.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22224v1", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.12291", "title": "Text-Driven Video Style Transfer with State-Space Models: Extending StyleMamba for Temporal Coherence", "authors": ["Chao Li", "Minsu Park", "Cristina Rossi", "Zhuang Li"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: This paper has been withdrawn by arXiv due to disputed and unverifiable authorship and affiliation", "url": "http://arxiv.org/abs/2503.12291v2", "summary": "StyleMamba has recently demonstrated efficient text-driven image style\ntransfer by leveraging state-space models (SSMs) and masked directional losses.\nIn this paper, we extend the StyleMamba framework to handle video sequences. We\npropose new temporal modules, including a \\emph{Video State-Space Fusion\nModule} to model inter-frame dependencies and a novel \\emph{Temporal Masked\nDirectional Loss} that ensures style consistency while addressing scene changes\nand partial occlusions. Additionally, we introduce a \\emph{Temporal\nSecond-Order Loss} to suppress abrupt style variations across consecutive\nframes. Our experiments on DAVIS and UCF101 show that the proposed approach\noutperforms competing methods in terms of style consistency, smoothness, and\ncomputational efficiency. We believe our new framework paves the way for\nreal-time text-driven video stylization with state-of-the-art perceptual\nresults.", "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "pdf_url": "http://arxiv.org/pdf/2503.12291v2", "cate": "cs.GR", "date": "2025-03-15", "updated": "2025-07-29"}
{"id": "2507.21056", "title": "AI-Driven Generation of Data Contracts in Modern Data Engineering Systems", "authors": ["Harshraj Bhoite"], "categories": ["cs.DB", "cs.AI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21056v1", "summary": "Data contracts formalize agreements between data producers and consumers\nregarding schema, semantics, and quality expectations. As data pipelines grow\nin complexity, manual authoring and maintenance of contracts becomes\nerror-prone and labor-intensive. We present an AI-driven framework for\nautomatic data contract generation using large language models (LLMs). Our\nsystem leverages parameter-efficient fine-tuning methods, including LoRA and\nPEFT, to adapt LLMs to structured data domains. The models take sample data or\nschema descriptions and output validated contract definitions in formats such\nas JSON Schema and Avro. We integrate this framework into modern data platforms\n(e.g., Databricks, Snowflake) to automate contract enforcement at scale.\nExperimental results on synthetic and real-world datasets demonstrate that the\nfine-tuned LLMs achieve high accuracy in generating valid contracts and reduce\nmanual workload by over 70%. We also discuss key challenges such as\nhallucination, version control, and the need for continuous learning. This work\ndemonstrates that generative AI can enable scalable, agile data governance by\nbridging the gap between intent and implementation in enterprise data\nmanagement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21056v1", "cate": "cs.DB", "date": "2025-05-04", "updated": "2025-05-04"}
{"id": "2507.21084", "title": "Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing", "authors": ["Aly M. Kassem", "Zhuan Shi", "Negar Rostamzadeh", "Golnoosh Farnadi"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21084v1", "summary": "Large language models (LLMs) are frequently fine-tuned or unlearned to adapt\nto new tasks or eliminate undesirable behaviors. While existing evaluation\nmethods assess performance after such interventions, there remains no general\napproach for detecting unintended side effects, such as unlearning biology\ncontent degrading performance on chemistry tasks, particularly when these\neffects are unpredictable or emergent. To address this issue, we introduce\nMNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight\nframework for identifying these side effects using sparse model diffing. MNEME\ncompares base and fine-tuned models on task-agnostic data (for example, The\nPile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral\nshifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,\nemergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent\naccuracy in predicting side effects, aligning with known benchmarks and\nrequiring no custom heuristics. Furthermore, we show that retraining on\nhigh-activation samples can partially reverse these effects. Our results\ndemonstrate that sparse probing and diffing offer a scalable and automated lens\ninto fine-tuning-induced model changes, providing practical tools for\nunderstanding and managing LLM behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21084v1", "cate": "cs.CL", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.21809", "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels", "authors": ["HunyuanWorld Team", "Zhenwei Wang", "Yuhao Liu", "Junta Wu", "Zixiao Gu", "Haoyuan Wang", "Xuhui Zuo", "Tianyu Huang", "Wenhuan Li", "Sheng Zhang", "Yihang Lian", "Yulin Tsai", "Lifu Wang", "Sicong Liu", "Puhua Jiang", "Xianghui Yang", "Dongyuan Guo", "Yixuan Tang", "Xinyue Mao", "Jiaao Yu", "Junlin Yu", "Jihong Zhang", "Meng Chen", "Liang Dong", "Yiwen Jia", "Chao Zhang", "Yonghao Tan", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Minghui Chen", "Zhan Li", "Wangchen Qin", "Lei Wang", "Yifu Sun", "Lin Niu", "Xiang Yuan", "Xiaofeng Yang", "Yingping He", "Jie Xiao", "Yangyu Tao", "Jianchen Zhu", "Jinbao Xue", "Kai Liu", "Chongqing Zhao", "Xinming Wu", "Tian Liu", "Peng Chen", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Tengfei Wang", "Chunchao Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Technical Report; Project Page: this https URL", "url": "http://arxiv.org/abs/2507.21809v1", "summary": "Creating immersive and playable 3D worlds from texts or images remains a\nfundamental challenge in computer vision and graphics. Existing world\ngeneration approaches typically fall into two categories: video-based methods\nthat offer rich diversity but lack 3D consistency and rendering efficiency, and\n3D-based methods that provide geometric consistency but struggle with limited\ntraining data and memory-inefficient representations. To address these\nlimitations, we present HunyuanWorld 1.0, a novel framework that combines the\nbest of both worlds for generating immersive, explorable, and interactive 3D\nscenes from text and image conditions. Our approach features three key\nadvantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2)\nmesh export capabilities for seamless compatibility with existing computer\ngraphics pipelines; 3) disentangled object representations for augmented\ninteractivity. The core of our framework is a semantically layered 3D mesh\nrepresentation that leverages panoramic images as 360{\\deg} world proxies for\nsemantic-aware world decomposition and reconstruction, enabling the generation\nof diverse 3D worlds. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in generating coherent, explorable, and\ninteractive 3D worlds while enabling versatile applications in virtual reality,\nphysical simulation, game development, and interactive content creation.", "comment": "Technical Report; Project Page:\n  https://3d-models.hunyuan.tencent.com/world/", "pdf_url": "http://arxiv.org/pdf/2507.21809v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.17033", "title": "Breaking the Sorting Barrier for Directed Single-Source Shortest Paths", "authors": ["Ran Duan", "Jiayi Mao", "Xiao Mao", "Xinkai Shu", "Longhui Yin"], "categories": ["cs.DS", "F.2.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2504.17033v2", "summary": "We give a deterministic $O(m\\log^{2/3}n)$-time algorithm for single-source\nshortest paths (SSSP) on directed graphs with real non-negative edge weights in\nthe comparison-addition model. This is the first result to break the $O(m+n\\log\nn)$ time bound of Dijkstra's algorithm on sparse graphs, showing that\nDijkstra's algorithm is not optimal for SSSP.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2504.17033v2", "cate": "cs.DS", "date": "2025-04-23", "updated": "2025-07-30"}
{"id": "2507.22268", "title": "Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items", "authors": ["Junting Wang", "Chenghuan Guo", "Jiao Yang", "Yanhui Guo", "Yan Gao", "Hari Sundaram"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22268v1", "summary": "We introduce a novel self-supervised multi-modal relational item\nrepresentation learning framework designed to infer substitutable and\ncomplementary items. Existing approaches primarily focus on modeling item-item\nassociations deduced from user behaviors using graph neural networks (GNNs) or\nleveraging item content information. However, these methods often overlook\ncritical challenges, such as noisy user behavior data and data sparsity due to\nthe long-tailed distribution of these behaviors. In this paper, we propose\nMMSC, a self-supervised multi-modal relational item representation learning\nframework to address these challenges. Specifically, MMSC consists of three\nmain components: (1) a multi-modal item representation learning module that\nleverages a multi-modal foundational model and learns from item metadata, (2) a\nself-supervised behavior-based representation learning module that denoises and\nlearns from user behavior data, and (3) a hierarchical representation\naggregation mechanism that integrates item representations at both the semantic\nand task levels. Additionally, we leverage LLMs to generate augmented training\ndata, further enhancing the denoising process during training. We conduct\nextensive experiments on five real-world datasets, showing that MMSC\noutperforms existing baselines by 26.1% for substitutable recommendation and\n39.2% for complementary recommendation. In addition, we empirically show that\nMMSC is effective in modeling cold-start items.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22268v1", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.16129", "title": "Controllable Segmentation-Based Text-Guided Style Editing", "authors": ["Jingwen Li", "Aravind Chandrasekar", "Mariana Rocha", "Chao Li", "Yuqing Chen"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: This paper has been withdrawn by arXiv due to disputed and unverifiable authorship and affiliation", "url": "http://arxiv.org/abs/2503.16129v2", "summary": "We present a novel approach for controllable, region-specific style editing\ndriven by textual prompts. Building upon the state-space style alignment\nframework introduced by \\emph{StyleMamba}, our method integrates a semantic\nsegmentation model into the style transfer pipeline. This allows users to\nselectively apply text-driven style changes to specific segments (e.g., ``turn\nthe building into a cyberpunk tower'') while leaving other regions (e.g.,\n``people'' or ``trees'') unchanged. By incorporating region-wise condition\nvectors and a region-specific directional loss, our method achieves\nhigh-fidelity transformations that respect both semantic boundaries and\nuser-driven style descriptions. Extensive experiments demonstrate that our\napproach can flexibly handle complex scene stylizations in real-world\nscenarios, improving control and quality over purely global style transfer\nmethods.", "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "pdf_url": "http://arxiv.org/pdf/2503.16129v2", "cate": "cs.GR", "date": "2025-03-20", "updated": "2025-07-29"}
{"id": "2507.21058", "title": "Categorical Classification of Book Summaries Using Word Embedding Techniques", "authors": ["Kerem Keskin", "Mümine Kaya Keleş"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      in Turkish language. This paper was published in the proceedings of the 6th International Conference on Data Science and Applications ICONDATA24, held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text book see this https URL", "url": "http://arxiv.org/abs/2507.21058v1", "summary": "In this study, book summaries and categories taken from book sites were\nclassified using word embedding methods, natural language processing techniques\nand machine learning algorithms. In addition, one hot encoding, Word2Vec and\nTerm Frequency - Inverse Document Frequency (TF-IDF) methods, which are\nfrequently used word embedding methods were used in this study and their\nsuccess was compared. Additionally, the combination table of the pre-processing\nmethods used is shown and added to the table. Looking at the results, it was\nobserved that Support Vector Machine, Naive Bayes and Logistic Regression\nModels and TF-IDF and One-Hot Encoder word embedding techniques gave more\nsuccessful results for Turkish texts.", "comment": "in Turkish language. This paper was published in the proceedings of\n  the 6th International Conference on Data Science and Applications ICONDATA24,\n  held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text\n  book see https://www.icondata.org/en/proceedings-books", "pdf_url": "http://arxiv.org/pdf/2507.21058v1", "cate": "cs.CL", "date": "2025-05-12", "updated": "2025-05-12"}
{"id": "2507.21112", "title": "InsurTech innovation using natural language processing", "authors": ["Panyi Dong", "Zhiyu Quan"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21112v1", "summary": "With the rapid rise of InsurTech, traditional insurance companies are\nincreasingly exploring alternative data sources and advanced technologies to\nsustain their competitive edge. This paper provides both a conceptual overview\nand practical case studies of natural language processing (NLP) and its\nemerging applications within insurance operations with a focus on transforming\nraw, unstructured text into structured data suitable for actuarial analysis and\ndecision-making. Leveraging real-world alternative data provided by an\nInsurTech industry partner that enriches traditional insurance data sources, we\napply various NLP techniques to demonstrate practical use cases in the\ncommercial insurance context. These enriched, text-derived insights not only\nadd to and refine traditional rating factors for commercial insurance pricing\nbut also offer novel perspectives for assessing underlying risk by introducing\nnovel industry classifications. Through these demonstrations, we show that NLP\nis not merely a supplementary tool but a foundational element for modern,\ndata-driven insurance analytics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21112v1", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.21820", "title": "Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is", "authors": ["Ahmed B Mustafa", "Zihan Ye", "Yang Lu", "Michael P Pound", "Shreyank N Gowda"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21820v1", "summary": "Despite significant advancements in alignment and content moderation, large\nlanguage models (LLMs) and text-to-image (T2I) systems remain vulnerable to\nprompt-based attacks known as jailbreaks. Unlike traditional adversarial\nexamples requiring expert knowledge, many of today's jailbreaks are low-effort,\nhigh-impact crafted by everyday users with nothing more than cleverly worded\nprompts. This paper presents a systems-style investigation into how non-experts\nreliably circumvent safety mechanisms through techniques such as multi-turn\nnarrative escalation, lexical camouflage, implication chaining, fictional\nimpersonation, and subtle semantic edits. We propose a unified taxonomy of\nprompt-level jailbreak strategies spanning both text-output and T2I models,\ngrounded in empirical case studies across popular APIs. Our analysis reveals\nthat every stage of the moderation pipeline, from input filtering to output\nvalidation, can be bypassed with accessible strategies. We conclude by\nhighlighting the urgent need for context-aware defenses that reflect the ease\nwith which these jailbreaks can be reproduced in real-world settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21820v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2506.01228", "title": "Reweighted Spectral Partitioning Works: Bounds for Special Graph Classes", "authors": ["Jack Spalding-Jamieson"], "categories": ["cs.DS", "cs.CG", "cs.DM"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      42 pages, 8 figures", "url": "http://arxiv.org/abs/2506.01228v3", "summary": "Spectral partitioning is a method that can be used to compute small sparse\ncuts or small edge-separators in a wide variety of graph classes, by computing\nthe second-smallest eigenvalue (and eigenvector) of the Laplacian matrix. Upper\nbounds on this eigenvalue for certain graph classes imply that the method\nobtains small edge-separators for these classes, usually with a sub-optimal\ndependence on the maximum degree. In this work, we show that a related method,\ncalled reweighted spectral partitioning, guarantees near-optimal sparse\nvertex-cuts and vertex-separators in a wide variety of graph classes. In many\ncases, this involves little-to-no necessary dependence on maximum degree.\n  We also obtain a new proof of the planar separator theorem, a strengthened\neigenvalue bound for bounded-genus graphs, and a refined form of the recent\nCheeger-style inequality for vertex expansion via a specialized\ndimension-reduction step.", "comment": "42 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.01228v3", "cate": "cs.DS", "date": "2025-06-02", "updated": "2025-07-30"}
{"id": "2507.22520", "title": "Sustainability Evaluation Metrics for Recommender Systems", "authors": ["Alexander Felfernig", "Damian Garber", "Viet-Man Le", "Sebastian Lubos", "Thi Ngoc Trang Tran"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22520v1", "summary": "Sustainability-oriented evaluation metrics can help to assess the quality of\nrecommender systems beyond wide-spread metrics such as accuracy, precision,\nrecall, and satisfaction. Following the United Nations`s sustainable\ndevelopment goals (SDGs), such metrics can help to analyse the impact of\nrecommender systems on environmental, social, and economic aspects. We discuss\ndifferent basic sustainability evaluation metrics for recommender systems and\nanalyze their applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22520v1", "cate": "cs.IR", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.16133", "title": "Multi-Prompt Style Interpolation for Fine-Grained Artistic Control", "authors": ["Lei Chen", "Hao Li", "Yuxin Zhang", "Chao Li", "Kai Wen"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: This paper has been withdrawn by arXiv due to disputed and unverifiable authorship and affiliation", "url": "http://arxiv.org/abs/2503.16133v2", "summary": "Text-driven image style transfer has seen remarkable progress with methods\nleveraging cross-modal embeddings for fast, high-quality stylization. However,\nmost existing pipelines assume a \\emph{single} textual style prompt, limiting\nthe range of artistic control and expressiveness. In this paper, we propose a\nnovel \\emph{multi-prompt style interpolation} framework that extends the\nrecently introduced \\textbf{StyleMamba} approach. Our method supports blending\nor interpolating among multiple textual prompts (eg, ``cubism,''\n``impressionism,'' and ``cartoon''), allowing the creation of nuanced or hybrid\nartistic styles within a \\emph{single} image. We introduce a\n\\textit{Multi-Prompt Embedding Mixer} combined with \\textit{Adaptive Blending\nWeights} to enable fine-grained control over the spatial and semantic influence\nof each style. Further, we propose a \\emph{Hierarchical Masked Directional\nLoss} to refine region-specific style consistency. Experiments and user studies\nconfirm our approach outperforms single-prompt baselines and naive linear\ncombinations of styles, achieving superior style fidelity, text-image\nalignment, and artistic flexibility, all while maintaining the computational\nefficiency offered by the state-space formulation.", "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "pdf_url": "http://arxiv.org/pdf/2503.16133v2", "cate": "cs.GR", "date": "2025-03-20", "updated": "2025-07-29"}
{"id": "2507.21080", "title": "Which symbol grounding problem should we try to solve?", "authors": ["Vincent C. Müller"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21080v1", "summary": "Floridi and Taddeo propose a condition of \"zero semantic commitment\" for\nsolutions to the grounding problem, and a solution to it. I argue briefly that\ntheir condition cannot be fulfilled, not even by their own solution. After a\nlook at Luc Steels' very different competing suggestion, I suggest that we need\nto re-think what the problem is and what role the 'goals' in a system play in\nformulating the problem. On the basis of a proper understanding of computing, I\ncome to the conclusion that the only sensible grounding problem is how we can\nexplain and re-produce the behavioral ability and function of meaning in\nartificial computational agents", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21080v1", "cate": "cs.CL", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.21115", "title": "FedFlex: Federated Learning for Diverse Netflix Recommendations", "authors": ["Sven Lankester", "Manel Slokom", "Gustavo de Carvalho Bertoli", "Matias Vizcaino", "Emmanuelle Beauxis Aussalet", "Laura Hollink"], "categories": ["cs.IR", "cs.AI", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21115v1", "summary": "Federated learning is a decentralized approach that enables collaborative\nmodel training across multiple devices while preserving data privacy. It has\nshown significant potential in various domains, including healthcare and\npersonalized recommendation systems. However, most existing work on federated\nrecommendation systems has focused primarily on improving accuracy, with\nlimited attention to fairness and diversity. In this paper, we introduce\nFedFlex, a federated recommender system for Netflix-style TV series\nrecommendations. FedFlex integrates two state-of-the-art matrix factorization\nalgorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal\nRelevance (MMR) to re-rank items and enhance diversity. We conduct extensive\nexperiments comparing recommendations generated by SVD and BPR algorithms. In a\nlive two-week user study, participants received two recommendation lists: List\nA, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity.\nParticipants were asked to click on the movies they were interested in\nwatching. Our findings demonstrate that FedFlex effectively introduces diverse\ncontent, such as new genres, into recommendations without necessarily\ncompromising user satisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21115v1", "cate": "cs.IR", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.21844", "title": "Cross-Architecture Distillation Made Simple with Redundancy Suppression", "authors": ["Weijia Zhang", "Yuehao Liu", "Wu Ran", "Chao Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2507.21844v1", "summary": "We describe a simple method for cross-architecture knowledge distillation,\nwhere the knowledge transfer is cast into a redundant information suppression\nformulation. Existing methods introduce sophisticated modules,\narchitecture-tailored designs, and excessive parameters, which impair their\nefficiency and applicability. We propose to extract the architecture-agnostic\nknowledge in heterogeneous representations by reducing the redundant\narchitecture-exclusive information. To this end, we present a simple redundancy\nsuppression distillation (RSD) loss, which comprises cross-architecture\ninvariance maximisation and feature decorrelation objectives. To prevent the\nstudent from entirely losing its architecture-specific capabilities, we further\ndesign a lightweight module that decouples the RSD objective from the student's\ninternal representations. Our method is devoid of the architecture-specific\ndesigns and complex operations in the pioneering method of OFA. It outperforms\nOFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their\nparameter overhead, which highlights its potential as a simple and strong\nbaseline to the cross-architecture distillation community.", "comment": "Accepted by ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2507.21844v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.14957", "title": "Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division", "authors": ["Jarosław Byrka", "Franciszek Malinka", "Tomasz Ponitka"], "categories": ["cs.GT", "cs.AI", "cs.DS"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      27 pages, 4 figures", "url": "http://arxiv.org/abs/2507.14957v2", "summary": "We study the fair division of indivisible items and provide new insights into\nthe EFX problem, which is widely regarded as the central open question in fair\ndivision, and the PMMS problem, a strictly stronger variant of EFX. Our first\nresult constructs a three-agent instance with two monotone valuations and one\nadditive valuation in which no PMMS allocation exists. Since EFX allocations\nare known to exist under these assumptions, this establishes a formal\nseparation between EFX and PMMS.\n  We prove existence of fair allocations for three important special cases. We\nshow that EFX allocations exist for personalized bivalued valuations, where for\neach agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value\n$v_i(\\{g\\}) \\in \\{a_i, b_i\\}$ to each good $g$. We establish an analogous\nexistence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also\nprove that PMMS allocations exist for binary-valued MMS-feasible valuations,\nwhere each bundle $S$ has value $v_i(S) \\in \\{0, 1\\}$. Notably, this result\nholds even without assuming monotonicity of valuations and thus applies to the\nfair division of chores and mixed manna. Finally, we study a class of\nvaluations called pair-demand valuations, which extend the well-studied\nunit-demand valuations to the case where each agent derives value from at most\ntwo items, and we show that PMMS allocations exist in this setting. Our proofs\nare constructive, and we provide polynomial-time algorithms for all three\nexistence results.", "comment": "27 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.14957v2", "cate": "cs.GT", "date": "2025-07-20", "updated": "2025-07-30"}
{"id": "2507.22878", "title": "GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis", "authors": ["Ethan Frakes", "Yinghui Wu", "Roger H. French", "Mengjie Li"], "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted to the 24th International Semantic Web Conference Resource Track (ISWC 2025)", "url": "http://arxiv.org/abs/2507.22878v1", "summary": "Detecting, analyzing, and predicting power outages is crucial for grid risk\nassessment and disaster mitigation. Numerous outages occur each year,\nexacerbated by extreme weather events such as hurricanes. Existing outage data\nare typically reported at the county level, limiting their spatial resolution\nand making it difficult to capture localized patterns. However, it offers\nexcellent temporal granularity. In contrast, nighttime light satellite image\ndata provides significantly higher spatial resolution and enables a more\ncomprehensive spatial depiction of outages, enhancing the accuracy of assessing\nthe geographic extent and severity of power loss after disaster events.\nHowever, these satellite data are only available on a daily basis. Integrating\nspatiotemporal visual and time-series data sources into a unified knowledge\nrepresentation can substantially improve power outage detection, analysis, and\npredictive reasoning. In this paper, we propose GeoOutageKG, a multimodal\nknowledge graph that integrates diverse data sources, including nighttime light\nsatellite image data, high-resolution spatiotemporal power outage maps, and\ncounty-level timeseries outage reports in the U.S. We describe our method for\nconstructing GeoOutageKG by aligning source data with a developed ontology,\nGeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual\noutage records spanning from 2014 to 2024, 300,000 NTL images spanning from\n2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and\nreusable semantic resource that enables robust multimodal data integration. We\ndemonstrate its use through multiresolution analysis of geospatiotemporal power\noutages.", "comment": "Accepted to the 24th International Semantic Web Conference Resource\n  Track (ISWC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.22878v1", "cate": "cs.IR", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.20992", "title": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer", "authors": ["Michael Brown", "Sofia Martinez", "Priya Singh"], "categories": ["cs.GR", "cs.CL"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: This paper has been withdrawn by arXiv due to disputed and unverifiable authorship and affiliation", "url": "http://arxiv.org/abs/2503.20992v2", "summary": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer.", "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "pdf_url": "http://arxiv.org/pdf/2503.20992v2", "cate": "cs.GR", "date": "2025-03-26", "updated": "2025-07-30"}
{"id": "2507.21083", "title": "ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs", "authors": ["Franck Bardol"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21083v1", "summary": "Large Language Models like GPT-4 adjust their responses not only based on the\nquestion asked, but also on how it is emotionally phrased. We systematically\nvary the emotional tone of 156 prompts - spanning controversial and everyday\ntopics - and analyze how it affects model responses. Our findings show that\nGPT-4 is three times less likely to respond negatively to a negatively framed\nquestion than to a neutral one. This suggests a \"rebound\" bias where the model\novercorrects, often shifting toward neutrality or positivity. On sensitive\ntopics (e.g., justice or politics), this effect is even more pronounced:\ntone-based variation is suppressed, suggesting an alignment override. We\nintroduce concepts like the \"tone floor\" - a lower bound in response negativity\n- and use tone-valence transition matrices to quantify behavior. Visualizations\nbased on 1536-dimensional embeddings confirm semantic drift based on tone. Our\nwork highlights an underexplored class of biases driven by emotional framing in\nprompts, with implications for AI alignment and trust. Code and data are\navailable at: https://github.com/bardolfranck/llm-responses-viewer", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21083v1", "cate": "cs.CL", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.21168", "title": "Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question", "authors": ["Rafael Rosales", "Santiago Miret"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.0"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21168v1", "summary": "Effectively leveraging diversity has been shown to improve performance for\nvarious machine learning models, including large language models (LLMs).\nHowever, determining the most effective way of using diversity remains a\nchallenge. In this work, we compare two diversity approaches for answering\nbinary questions using LLMs: model diversity, which relies on multiple models\nanswering the same question, and question interpretation diversity, which\nrelies on using the same model to answer the same question framed in different\nways. For both cases, we apply majority voting as the ensemble consensus\nheuristic to determine the final answer. Our experiments on boolq, strategyqa,\nand pubmedqa show that question interpretation diversity consistently leads to\nbetter ensemble accuracy compared to model diversity. Furthermore, our analysis\nof GPT and LLaMa shows that model diversity typically produces results between\nthe best and the worst ensemble members without clear improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21168v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21857", "title": "Unleashing the Power of Motion and Depth: A Selective Fusion Strategy for RGB-D Video Salient Object Detection", "authors": ["Jiahao He", "Daerji Suolang", "Keren Fu", "Qijun Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      submitted to TMM on 11-Jun-2024, ID: MM-020522, still in peer review", "url": "http://arxiv.org/abs/2507.21857v1", "summary": "Applying salient object detection (SOD) to RGB-D videos is an emerging task\ncalled RGB-D VSOD and has recently gained increasing interest, due to\nconsiderable performance gains of incorporating motion and depth and that RGB-D\nvideos can be easily captured now in daily life. Existing RGB-D VSOD models\nhave different attempts to derive motion cues, in which extracting motion\ninformation explicitly from optical flow appears to be a more effective and\npromising alternative. Despite this, there remains a key issue that how to\neffectively utilize optical flow and depth to assist the RGB modality in SOD.\nPrevious methods always treat optical flow and depth equally with respect to\nmodel designs, without explicitly considering their unequal contributions in\nindividual scenarios, limiting the potential of motion and depth. To address\nthis issue and unleash the power of motion and depth, we propose a novel\nselective cross-modal fusion framework (SMFNet) for RGB-D VSOD, incorporating a\npixel-level selective fusion strategy (PSF) that achieves optimal fusion of\noptical flow and depth based on their actual contributions. Besides, we propose\na multi-dimensional selective attention module (MSAM) to integrate the fused\nfeatures derived from PSF with the remaining RGB modality at multiple\ndimensions, effectively enhancing feature representation to generate refined\nfeatures. We conduct comprehensive evaluation of SMFNet against 19\nstate-of-the-art models on both RDVS and DVisal datasets, making the evaluation\nthe most comprehensive RGB-D VSOD benchmark up to date, and it also\ndemonstrates the superiority of SMFNet over other models. Meanwhile, evaluation\non five video benchmark datasets incorporating synthetic depth validates the\nefficacy of SMFNet as well. Our code and benchmark results are made publicly\navailable at https://github.com/Jia-hao999/SMFNet.", "comment": "submitted to TMM on 11-Jun-2024, ID: MM-020522, still in peer review", "pdf_url": "http://arxiv.org/pdf/2507.21857v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22879", "title": "RecGPT Technical Report", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Sunhao Dai", "Wen Chen", "Wenjun Yang", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Chi Li", "Dimin Wang", "Dixuan Wang", "Fan Li", "Fan Zhang", "Haibin Chen", "Haozhuang Liu", "Jialin Zhu", "Jiamang Wang", "Jiawei Wu", "Jin Cui", "Ju Huang", "Kai Zhang", "Kan Liu", "Lang Tian", "Liang Rao", "Longbin Li", "Lulu Zhao", "Mao Zhang", "Na He", "Peiyang Wang", "Qiqi Huang", "Tao Luo", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Yang Li", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yinnan Song", "Yuchen Li", "Yujie Luo", "Yujin Yuan", "Yuliang Yan", "Zhengyang Wang", "Zhibo Xiao", "Zhixin Ma", "Zile Zhou"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22879v1", "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22879v1", "cate": "cs.IR", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2402.00186", "title": "Distance and Collision Probability Estimation from Gaussian Surface Models", "authors": ["Kshitij Goel", "Wennie Tabib"], "categories": ["cs.RO", "cs.CG", "cs.CV", "cs.GR"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at IROS 2025", "url": "http://arxiv.org/abs/2402.00186v3", "summary": "This paper describes continuous-space methodologies to estimate the collision\nprobability, Euclidean distance and gradient between an ellipsoidal robot model\nand an environment surface modeled as a set of Gaussian distributions.\nContinuous-space collision probability estimation is critical for\nuncertainty-aware motion planning. Most collision detection and avoidance\napproaches assume the robot is modeled as a sphere, but ellipsoidal\nrepresentations provide tighter approximations and enable navigation in\ncluttered and narrow spaces. State-of-the-art methods derive the Euclidean\ndistance and gradient by processing raw point clouds, which is computationally\nexpensive for large workspaces. Recent advances in Gaussian surface modeling\n(e.g. mixture models, splatting) enable compressed and high-fidelity surface\nrepresentations. Few methods exist to estimate continuous-space occupancy from\nsuch models. They require Gaussians to model free space and are unable to\nestimate the collision probability, Euclidean distance and gradient for an\nellipsoidal robot. The proposed methods bridge this gap by extending prior work\nin ellipsoid-to-ellipsoid Euclidean distance and collision probability\nestimation to Gaussian surface models. A geometric blending approach is also\nproposed to improve collision probability estimation. The approaches are\nevaluated with numerical 2D and 3D experiments using real-world point cloud\ndata. Methods for efficient calculation of these quantities are demonstrated to\nexecute within a few microseconds per ellipsoid pair using a single-thread on\nlow-power CPUs of modern embedded computers", "comment": "Accepted at IROS 2025", "pdf_url": "http://arxiv.org/pdf/2402.00186v3", "cate": "cs.RO", "date": "2024-01-31", "updated": "2025-07-30"}
{"id": "2507.22146", "title": "Pendulum Model of Spiking Neurons", "authors": ["Joy Bose"], "categories": ["cs.NE", "q-bio.NC", "C.1.3; I.5.1; I.6.3"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures, 1 table", "url": "http://arxiv.org/abs/2507.22146v1", "summary": "We propose a biologically inspired model of spiking neurons based on the\ndynamics of a damped, driven pendulum. Unlike traditional models such as the\nLeaky Integrate-and-Fire (LIF) neurons, the pendulum neuron incorporates\nsecond-order, nonlinear dynamics that naturally give rise to oscillatory\nbehavior and phase-based spike encoding. This model captures richer temporal\nfeatures and supports timing-sensitive computations critical for sequence\nprocessing and symbolic learning. We present an analysis of single-neuron\ndynamics and extend the model to multi-neuron layers governed by Spike-Timing\nDependent Plasticity (STDP) learning rules. We demonstrate practical\nimplementation with python code and with the Brian2 spiking neural simulator,\nand outline a methodology for deploying the model on neuromorphic hardware\nplatforms, using an approximation of the second-order equations. This framework\noffers a foundation for developing energy-efficient neural systems for\nneuromorphic computing and sequential cognition tasks.", "comment": "5 pages, 2 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.22146v1", "cate": "cs.NE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21104", "title": "iLSU-T: an Open Dataset for Uruguayan Sign Language Translation", "authors": ["Ariel E. Stassi", "Yanina Boria", "J. Matías Di Martino", "Gregory Randall"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 19th International Conference on Automatic Face and Gesture Recognition IEEE FG 2025", "url": "http://arxiv.org/abs/2507.21104v1", "summary": "Automatic sign language translation has gained particular interest in the\ncomputer vision and computational linguistics communities in recent years.\nGiven each sign language country particularities, machine translation requires\nlocal data to develop new techniques and adapt existing ones. This work\npresents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB\nvideos with audio and text transcriptions. This type of multimodal and curated\ndata is paramount for developing novel approaches to understand or generate\ntools for sign language processing. iLSU T comprises more than 185 hours of\ninterpreted sign language videos from public TV broadcasting. It covers diverse\ntopics and includes the participation of 18 professional interpreters of sign\nlanguage. A series of experiments using three state of the art translation\nalgorithms is presented. The aim is to establish a baseline for this dataset\nand evaluate its usefulness and the proposed pipeline for data processing. The\nexperiments highlight the need for more localized datasets for sign language\ntranslation and understanding, which are critical for developing novel tools to\nimprove accessibility and inclusion of all individuals. Our data and code can\nbe accessed.", "comment": "10 pages, 5 figures, 19th International Conference on Automatic Face\n  and Gesture Recognition IEEE FG 2025", "pdf_url": "http://arxiv.org/pdf/2507.21104v1", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.21186", "title": "Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers", "authors": ["Sungmin Han", "Jeonghyun Lee", "Sangkyun Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21186v1", "summary": "Transformers have profoundly influenced AI research, but explaining their\ndecisions remains challenging -- even for relatively simpler tasks such as\nclassification -- which hinders trust and safe deployment in real-world\napplications. Although activation-based attribution methods effectively explain\ntransformer-based text classification models, our findings reveal that these\nmethods can be undermined by class-irrelevant features within activations,\nleading to less reliable interpretations. To address this limitation, we\npropose Contrast-CAT, a novel activation contrast-based attribution method that\nrefines token-level attributions by filtering out class-irrelevant features. By\ncontrasting the activations of an input sequence with reference activations,\nContrast-CAT generates clearer and more faithful attribution maps. Experimental\nresults across various datasets and models confirm that Contrast-CAT\nconsistently outperforms state-of-the-art methods. Notably, under the MoRF\nsetting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds\nover the most competing methods, demonstrating its effectiveness in enhancing\ninterpretability for transformer-based text classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21186v1", "cate": "cs.CL", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.21858", "title": "Low-Cost Test-Time Adaptation for Robust Video Editing", "authors": ["Jianhui Wang", "Yinda Chen", "Yangfan He", "Xinyuan Song", "Yi Xin", "Dapeng Zhang", "Zhongwei Wan", "Bin Li", "Rongchao Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21858v1", "summary": "Video editing is a critical component of content creation that transforms raw\nfootage into coherent works aligned with specific visual and narrative\nobjectives. Existing approaches face two major challenges: temporal\ninconsistencies due to failure in capturing complex motion patterns, and\noverfitting to simple prompts arising from limitations in UNet backbone\narchitectures. While learning-based methods can enhance editing quality, they\ntypically demand substantial computational resources and are constrained by the\nscarcity of high-quality annotated data. In this paper, we present Vid-TTA, a\nlightweight test-time adaptation framework that personalizes optimization for\neach test video during inference through self-supervised auxiliary tasks. Our\napproach incorporates a motion-aware frame reconstruction mechanism that\nidentifies and preserves crucial movement regions, alongside a prompt\nperturbation and reconstruction strategy that strengthens model robustness to\ndiverse textual descriptions. These innovations are orchestrated by a\nmeta-learning driven dynamic loss balancing mechanism that adaptively adjusts\nthe optimization process based on video characteristics. Extensive experiments\ndemonstrate that Vid-TTA significantly improves video temporal consistency and\nmitigates prompt overfitting while maintaining low computational overhead,\noffering a plug-and-play performance boost for existing video editing models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21858v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22880", "title": "AUV-Fusion: Cross-Modal Adversarial Fusion of User Interactions and Visual Perturbations Against VARS", "authors": ["Hai Ling", "Tianchi Wang", "Xiaohao Liu", "Zhulin Tao", "Lifang Yang", "Xianglin Huang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      14 pages,6 figures", "url": "http://arxiv.org/abs/2507.22880v1", "summary": "Modern Visual-Aware Recommender Systems (VARS) exploit the integration of\nuser interaction data and visual features to deliver personalized\nrecommendations with high precision. However, their robustness against\nadversarial attacks remains largely underexplored, posing significant risks to\nsystem reliability and security. Existing attack strategies suffer from notable\nlimitations: shilling attacks are costly and detectable, and visual-only\nperturbations often fail to align with user preferences. To address these\nchallenges, we propose AUV-Fusion, a cross-modal adversarial attack framework\nthat adopts high-order user preference modeling and cross-modal adversary\ngeneration. Specifically, we obtain robust user embeddings through multi-hop\nuser-item interactions and transform them via an MLP into semantically aligned\nperturbations. These perturbations are injected onto the latent space of a\npre-trained VAE within the diffusion model. By synergistically integrating\ngenuine user interaction data with visually plausible perturbations, AUV-Fusion\neliminates the need for injecting fake user profiles and effectively mitigates\nthe challenge of insufficient user preference extraction inherent in\ntraditional visual-only attacks. Comprehensive evaluations on diverse VARS\narchitectures and real-world datasets demonstrate that AUV-Fusion significantly\nenhances the exposure of target (cold-start) items compared to conventional\nbaseline methods. Moreover, AUV-Fusion maintains exceptional stealth under\nrigorous scrutiny.", "comment": "14 pages,6 figures", "pdf_url": "http://arxiv.org/pdf/2507.22880v1", "cate": "cs.IR", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.20988", "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization", "authors": ["Hannah Kim", "Sofia Martinez", "Jason Lee"], "categories": ["cs.CL", "cs.GR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      arXiv admin note: This paper has been withdrawn by arXiv due to disputed and unverifiable authorship and affiliation", "url": "http://arxiv.org/abs/2503.20988v2", "summary": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component.", "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "pdf_url": "http://arxiv.org/pdf/2503.20988v2", "cate": "cs.CL", "date": "2025-03-26", "updated": "2025-07-30"}
{"id": "2507.22090", "title": "Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization", "authors": ["Sergii Kavun"], "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.NI", "68T07, 68T05, 65D10, 68Q32", "I.2.6; I.5.1; G.1.2; I.5.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 2 figures, 5 tables", "url": "http://arxiv.org/abs/2507.22090v1", "summary": "Activation functions are critical components in deep neural networks,\ndirectly influencing gradient flow, training stability, and model performance.\nTraditional functions like ReLU suffer from dead neuron problems, while sigmoid\nand tanh exhibit vanishing gradient issues. We introduce two novel hybrid\nactivation functions: S3 (Sigmoid-Softsign) and its improved version S4\n(smoothed S3). S3 combines sigmoid for negative inputs with softsign for\npositive inputs, while S4 employs a smooth transition mechanism controlled by a\nsteepness parameter k. We conducted comprehensive experiments across binary\nclassification, multi-class classification, and regression tasks using three\ndifferent neural network architectures. S4 demonstrated superior performance\ncompared to nine baseline activation functions, achieving 97.4% accuracy on\nMNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.\nThe function exhibited faster convergence (-19 for ReLU) and maintained stable\ngradient flow across network depths. Comparative analysis revealed S4's\ngradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep\nnetworks. The S4 activation function addresses key limitations of existing\nfunctions through its hybrid design and smooth transition mechanism. The\ntunable parameter k allows adaptation to different tasks and network depths,\nmaking S4 a versatile choice for deep learning applications. These findings\nsuggest that hybrid activation functions represent a promising direction for\nimproving neural network training dynamics.", "comment": "15 pages, 2 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.22090v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21105", "title": "AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis", "authors": ["Callie C. Liao", "Duoduo Liao", "Sai Surya Gadiraju"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21105v1", "summary": "The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI),\nespecially integrated with Large Language Models (LLMs), has greatly\nfacilitated the resolution of complex tasks. However, current systems are still\nfacing challenges of inter-agent communication, coordination, and interaction\nwith heterogeneous tools and resources. Most recently, the Model Context\nProtocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by\nGoogle have been introduced, and to the best of our knowledge, very few\napplications exist where both protocols are employed within a single MAS\nframework. We present a pilot study of AgentMaster, a novel modular\nmulti-protocol MAS framework with self-implemented A2A and MCP, enabling\ndynamic coordination and flexible communication. Through a unified\nconversational interface, the system supports natural language interaction\nwithout prior technical expertise and responds to multimodal queries for tasks\nincluding information retrieval, question answering, and image analysis.\nEvaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged\n96.3\\% and 87.1\\%, revealing robust inter-agent coordination, query\ndecomposition, dynamic routing, and domain-specific, relevant responses.\nOverall, our proposed framework contributes to the potential capabilities of\ndomain-specific, cooperative, and scalable conversational AI powered by MAS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21105v1", "cate": "cs.IR", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.21202", "title": "Combolutional Neural Networks", "authors": ["Cameron Churchwell", "Minje Kim", "Paris Smaragdis"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      4 pages, 3 figures, accepted to WASPAA 2025", "url": "http://arxiv.org/abs/2507.21202v1", "summary": "Selecting appropriate inductive biases is an essential step in the design of\nmachine learning models, especially when working with audio, where even short\nclips may contain millions of samples. To this end, we propose the\ncombolutional layer: a learned-delay IIR comb filter and fused envelope\ndetector, which extracts harmonic features in the time domain. We demonstrate\nthe efficacy of the combolutional layer on three information retrieval tasks,\nevaluate its computational cost relative to other audio frontends, and provide\nefficient implementations for training. We find that the combolutional layer is\nan effective replacement for convolutional layers in audio tasks where precise\nharmonic analysis is important, e.g., piano transcription, speaker\nclassification, and key detection. Additionally, the combolutional layer has\nseveral other key benefits over existing frontends, namely: low parameter\ncount, efficient CPU inference, strictly real-valued computations, and improved\ninterpretability.", "comment": "4 pages, 3 figures, accepted to WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.21202v1", "cate": "cs.SD", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21888", "title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding", "authors": ["Fevziye Irem Eyiokur", "Dogucan Yaman", "Hazım Kemal Ekenel", "Alexander Waibel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21888v1", "summary": "We address the problem of Embodied Reference Understanding, which involves\npredicting the object that a person in the scene is referring to through both\npointing gesture and language. Accurately identifying the referent requires\nmultimodal understanding: integrating textual instructions, visual pointing,\nand scene context. However, existing methods often struggle to effectively\nleverage visual clues for disambiguation. We also observe that, while the\nreferent is often aligned with the head-to-fingertip line, it occasionally\naligns more closely with the wrist-to-fingertip line. Therefore, relying on a\nsingle line assumption can be overly simplistic and may lead to suboptimal\nperformance. To address this, we propose a dual-model framework, where one\nmodel learns from the head-to-fingertip direction and the other from the\nwrist-to-fingertip direction. We further introduce a Gaussian ray heatmap\nrepresentation of these lines and use them as input to provide a strong\nsupervisory signal that encourages the model to better attend to pointing cues.\nTo combine the strengths of both models, we present the CLIP-Aware Pointing\nEnsemble module, which performs a hybrid ensemble based on CLIP features.\nAdditionally, we propose an object center prediction head as an auxiliary task\nto further enhance referent localization. We validate our approach through\nextensive experiments and analysis on the benchmark YouRefIt dataset, achieving\nan improvement of approximately 4 mAP at the 0.25 IoU threshold.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21888v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22337", "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers", "authors": ["Roxana Petcu", "Samarth Bhargav", "Maarten de Rijke", "Evangelos Kanoulas"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22337v1", "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22337v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.20999", "title": "Text-Driven Voice Conversion via Latent State-Space Modeling", "authors": ["Wen Li", "Sofia Martinez", "Priyanka Shah"], "categories": ["cs.SD", "cs.GR", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      arXiv admin note: This paper has been withdrawn by arXiv due to disputed and unverifiable authorship and affiliation", "url": "http://arxiv.org/abs/2503.20999v2", "summary": "Text-driven voice conversion allows customization of speaker characteristics\nand prosodic elements using textual descriptions. However, most existing\nmethods rely heavily on direct text-to-speech training, limiting their\nflexibility in controlling nuanced style elements or timbral features. In this\npaper, we propose a novel \\textbf{Latent State-Space} approach for text-driven\nvoice conversion (\\textbf{LSS-VC}). Our method treats each utterance as an\nevolving dynamical system in a continuous latent space. Drawing inspiration\nfrom mamba, which introduced a state-space model for efficient text-driven\n\\emph{image} style transfer, we adapt a loosely related methodology for\n\\emph{voice} style transformation. Specifically, we learn a voice latent\nmanifold where style and content can be manipulated independently by textual\nstyle prompts. We propose an adaptive cross-modal fusion mechanism to inject\nstyle information into the voice latent representation, enabling interpretable\nand fine-grained control over speaker identity, speaking rate, and emphasis.\nExtensive experiments show that our approach significantly outperforms recent\nbaselines in both subjective and objective quality metrics, while offering\nsmoother transitions between styles, reduced artifacts, and more precise\ntext-based style control.", "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "pdf_url": "http://arxiv.org/pdf/2503.20999v2", "cate": "cs.SD", "date": "2025-03-26", "updated": "2025-07-30"}
{"id": "2507.22131", "title": "OpenRASE: Service Function Chain Emulation", "authors": ["Theviyanthan Krishnamohan", "Paul Harvey"], "categories": ["cs.NI", "cs.DC", "cs.NE"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE SoftCom 2025", "url": "http://arxiv.org/abs/2507.22131v1", "summary": "Service Function Chains (SFCs) are one of the key enablers in providing\nprogrammable computer networks, paving the way for network autonomy. However,\nthis also introduces new challenges, such as resource allocation and\noptimisation related to their operation, requiring new algorithms to address\nthese challenges. Various tools have been used in the literature to evaluate\nthese algorithms. However, these tools suffer from inaccuracy, low fidelity,\nunscalability, inflexibility, or additional code requirements. This paper\nintroduces an emulator based on Mininet and Docker for SFCs called OpenRASE.\nThe goal of OpenRASE is to enable the exploration of resource allocation\nalgorithms for SFCs in a dynamic setting, allowing real CPU usage and latency\nto be measured. We describe the design and implementation of OpenRASE and\ndiscuss its characteristics. We also experimentally evaluate two different\nalgorithms to address the SFC resource allocation challenge, including an\nonline Genetic Algorithm, using OpenRASE to show its effectiveness and\npracticality for dynamic network conditions.", "comment": "Accepted to IEEE SoftCom 2025", "pdf_url": "http://arxiv.org/pdf/2507.22131v1", "cate": "cs.NI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21107", "title": "Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams", "authors": ["Rob Manson"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      29 pages, 22 figures", "url": "http://arxiv.org/abs/2507.21107v1", "summary": "We propose Curved Inference - a geometric Interpretability framework that\ntracks how the residual stream trajectory of a large language model bends in\nresponse to shifts in semantic concern. Across 20 matched prompts spanning\nemotional, moral, perspective, logical, identity, environmental, and nonsense\ndomains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,\nwith a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These\nmetrics are computed under a pullback semantic metric derived from the\nunembedding matrix, ensuring that all measurements reflect token-aligned\ngeometry rather than raw coordinate structure. We find that concern-shifted\nprompts reliably alter internal activation trajectories in both models - with\nLLaMA exhibiting consistent, statistically significant scaling in both\ncurvature and salience as concern intensity increases. Gemma also responds to\nconcern but shows weaker differentiation between moderate and strong variants.\nOur results support a two-layer view of LLM geometry - a latent conceptual\nstructure encoded in the embedding space, and a contextual trajectory shaped by\nprompt-specific inference. Curved Inference reveals how models navigate,\nreorient, or reinforce semantic meaning over depth, offering a principled\nmethod for diagnosing alignment, abstraction, and emergent inference dynamics.\nThese findings offer fresh insight into semantic abstraction and model\nalignment through the lens of Curved Inference.", "comment": "29 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.21107v1", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.21203", "title": "An empirical comparison of some outlier detection methods with longitudinal data", "authors": ["Marcello D'Orazio"], "categories": ["stat.ME", "cs.LG", "stat.AP"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21203v1", "summary": "This note investigates the problem of detecting outliers in longitudinal\ndata. It compares well-known methods used in official statistics with proposals\nfrom the fields of data mining and machine learning that are based on the\ndistance between observations or binary partitioning trees. This is achieved by\napplying the methods to panel survey data related to different types of\nstatistical units. Traditional methods are quite simple, enabling the direct\nidentification of potential outliers, but they require specific assumptions. In\ncontrast, recent methods provide only a score whose magnitude is directly\nrelated to the likelihood of an outlier being present. All the methods require\nthe user to set a number of tuning parameters. However, the most recent methods\nare more flexible and sometimes more effective than traditional methods. In\naddition, these methods can be applied to multidimensional data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21203v1", "cate": "stat.ME", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21893", "title": "Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs", "authors": ["Saeed Ghorbani"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21893v1", "summary": "We introduce Aether Weaver, a novel, integrated framework for multimodal\nnarrative co-generation that overcomes limitations of sequential text-to-visual\npipelines. Our system concurrently synthesizes textual narratives, dynamic\nscene graph representations, visual scenes, and affective soundscapes, driven\nby a tightly integrated, co-generation mechanism. At its core, the Narrator, a\nlarge language model, generates narrative text and multimodal prompts, while\nthe Director acts as a dynamic scene graph manager, and analyzes the text to\nbuild and maintain a structured representation of the story's world, ensuring\nspatio-temporal and relational consistency for visual rendering and subsequent\nnarrative generation. Additionally, a Narrative Arc Controller guides the\nhigh-level story structure, influencing multimodal affective consistency,\nfurther complemented by an Affective Tone Mapper that ensures congruent\nemotional expression across all modalities. Through qualitative evaluations on\na diverse set of narrative prompts encompassing various genres, we demonstrate\nthat Aether Weaver significantly enhances narrative depth, visual fidelity, and\nemotional resonance compared to cascaded baseline approaches. This integrated\nframework provides a robust platform for rapid creative prototyping and\nimmersive storytelling experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21893v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.01053", "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis", "authors": ["Rafi Al Attrach", "Pedro Moreira", "Rajna Fani", "Renato Umeton", "Leo Anthony Celi"], "categories": ["cs.IR", "cs.AI", "cs.DB", "68T50, 68P15", "H.2.3; I.2.7; J.3"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.01053v2", "summary": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.01053v2", "cate": "cs.IR", "date": "2025-06-27", "updated": "2025-07-30"}
{"id": "2507.20205", "title": "Signed Higher-Order Interactions for Brain Disorder Diagnosis via Multi-Channel Transformers", "authors": ["Dengyi Zhao", "Zhiheng Zhou", "Guiying Yan", "Dongxiao Yu", "Xingqin Qi"], "categories": ["q-bio.NC", "cs.GR"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20205v3", "summary": "Accurately characterizing higher-order interactions of brain regions and\nextracting interpretable organizational patterns from Functional Magnetic\nResonance Imaging data is crucial for brain disease diagnosis. Current\ngraph-based deep learning models primarily focus on pairwise or triadic\npatterns while neglecting signed higher-order interactions, limiting\ncomprehensive understanding of brain-wide communication. We propose HOI-Brain,\na novel computational framework leveraging signed higher-order interactions and\norganizational patterns in fMRI data for brain disease diagnosis. First, we\nintroduce a co-fluctuation measure based on Multiplication of Temporal\nDerivatives to detect higher-order interactions with temporal resolution. We\nthen distinguish positive and negative synergistic interactions, encoding them\nin signed weighted simplicial complexes to reveal brain communication insights.\nUsing Persistent Homology theory, we apply two filtration processes to these\ncomplexes to extract signed higher-dimensional neural organizations\nspatiotemporally. Finally, we propose a multi-channel brain Transformer to\nintegrate heterogeneous topological features. Experiments on Alzheimer' s\ndisease, Parkinson' s syndrome, and autism spectrum disorder datasets\ndemonstrate our framework' s superiority, effectiveness, and interpretability.\nThe identified key brain regions and higher-order patterns align with\nneuroscience literature, providing meaningful biological insights.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20205v3", "cate": "q-bio.NC", "date": "2025-07-27", "updated": "2025-07-30"}
{"id": "2507.22440", "title": "Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool", "authors": ["Yiya Diao", "Changhe Li", "Sanyou Zeng", "Xinye Cai", "Wenjian Luo", "Shengxiang Yang", "Carlos A. Coello Coello"], "categories": ["cs.AI", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22440v1", "summary": "The Nearest-Better Network (NBN) is a powerful method to visualize sampled\ndata for continuous optimization problems while preserving multiple landscape\nfeatures. However, the calculation of NBN is very time-consuming, and the\nextension of the method to combinatorial optimization problems is challenging\nbut very important for analyzing the algorithm's behavior. This paper provides\na straightforward theoretical derivation showing that the NBN network\nessentially functions as the maximum probability transition network for\nalgorithms. This paper also presents an efficient NBN computation method with\nlogarithmic linear time complexity to address the time-consuming issue. By\napplying this efficient NBN algorithm to the OneMax problem and the Traveling\nSalesman Problem (TSP), we have made several remarkable discoveries for the\nfirst time: The fitness landscape of OneMax exhibits neutrality, ruggedness,\nand modality features. The primary challenges of TSP problems are ruggedness,\nmodality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and\nLKH) have limitations when addressing challenges related to modality and\ndeception, respectively. LKH, based on local search operators, fails when there\nare deceptive solutions near global optima. EAX, which is based on a single\npopulation, can efficiently maintain diversity. However, when multiple\nattraction basins exist, EAX retains individuals within multiple basins\nsimultaneously, reducing inter-basin interaction efficiency and leading to\nalgorithm's stagnation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22440v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21108", "title": "A Survey of Classification Tasks and Approaches for Legal Contracts", "authors": ["Amrita Singh", "Aditya Joshi", "Jiaojiao Jiang", "Hye-young Paik"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under review. 49 pages + references", "url": "http://arxiv.org/abs/2507.21108v1", "summary": "Given the large size and volumes of contracts and their underlying inherent\ncomplexity, manual reviews become inefficient and prone to errors, creating a\nclear need for automation. Automatic Legal Contract Classification (LCC)\nrevolutionizes the way legal contracts are analyzed, offering substantial\nimprovements in speed, accuracy, and accessibility. This survey delves into the\nchallenges of automatic LCC and a detailed examination of key tasks, datasets,\nand methodologies. We identify seven classification tasks within LCC, and\nreview fourteen datasets related to English-language contracts, including\npublic, proprietary, and non-public sources. We also introduce a methodology\ntaxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,\nand Transformer-based approaches. Additionally, the survey discusses evaluation\ntechniques and highlights the best-performing results from the reviewed\nstudies. By providing a thorough overview of current methods and their\nlimitations, this survey suggests future research directions to improve the\nefficiency, accuracy, and scalability of LCC. As the first comprehensive survey\non LCC, it aims to support legal NLP researchers and practitioners in improving\nlegal processes, making legal information more accessible, and promoting a more\ninformed and equitable society.", "comment": "Under review. 49 pages + references", "pdf_url": "http://arxiv.org/pdf/2507.21108v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.21222", "title": "Benchmarking a Tunable Quantum Neural Network on Trapped-Ion and Superconducting Hardware", "authors": ["Djamil Lakhdar-Hamina", "Xingxin Liu", "Richard Barney", "Sarah H. Miller", "Alaina M. Green", "Norbert M. Linke", "Victor Galitski"], "categories": ["quant-ph", "cond-mat.dis-nn", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures", "url": "http://arxiv.org/abs/2507.21222v1", "summary": "We implement a quantum generalization of a neural network on trapped-ion and\nIBM superconducting quantum computers to classify MNIST images, a common\nbenchmark in computer vision. The network feedforward involves qubit rotations\nwhose angles depend on the results of measurements in the previous layer. The\nnetwork is trained via simulation, but inference is performed experimentally on\nquantum hardware. The classical-to-quantum correspondence is controlled by an\ninterpolation parameter, $a$, which is zero in the classical limit. Increasing\n$a$ introduces quantum uncertainty into the measurements, which is shown to\nimprove network performance at moderate values of the interpolation parameter.\nWe then focus on particular images that fail to be classified by a classical\nneural network but are detected correctly in the quantum network. For such\nborderline cases, we observe strong deviations from the simulated behavior. We\nattribute this to physical noise, which causes the output to fluctuate between\nnearby minima of the classification energy landscape. Such strong sensitivity\nto physical noise is absent for clear images. We further benchmark physical\nnoise by inserting additional single-qubit and two-qubit gate pairs into the\nneural network circuits. Our work provides a springboard toward more complex\nquantum neural networks on current devices: while the approach is rooted in\nstandard classical machine learning, scaling up such networks may prove\nclassically non-simulable and could offer a route to near-term quantum\nadvantage.", "comment": "6 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.21222v1", "cate": "quant-ph", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21905", "title": "Evaluating Deepfake Detectors in the Wild", "authors": ["Viacheslav Pirogov", "Maksim Artemev"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation Frameworks Across Domains'", "url": "http://arxiv.org/abs/2507.21905v1", "summary": "Deepfakes powered by advanced machine learning models present a significant\nand evolving threat to identity verification and the authenticity of digital\nmedia. Although numerous detectors have been developed to address this problem,\ntheir effectiveness has yet to be tested when applied to real-world data. In\nthis work we evaluate modern deepfake detectors, introducing a novel testing\nprocedure designed to mimic real-world scenarios for deepfake detection. Using\nstate-of-the-art deepfake generation methods, we create a comprehensive dataset\ncontaining more than 500,000 high-quality deepfake images. Our analysis shows\nthat detecting deepfakes still remains a challenging task. The evaluation shows\nthat in fewer than half of the deepfake detectors tested achieved an AUC score\ngreater than 60%, with the lowest being 50%. We demonstrate that basic image\nmanipulations, such as JPEG compression or image enhancement, can significantly\nreduce model performance. All code and data are publicly available at\nhttps://github.com/messlav/Deepfake-Detectors-in-the-Wild.", "comment": "Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation\n  Frameworks Across Domains'", "pdf_url": "http://arxiv.org/pdf/2507.21905v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22787", "title": "Amorphous Solid Model of Vectorial Hopfield Neural Networks", "authors": ["F. Gallavotti", "A. Zaccone"], "categories": ["cond-mat.dis-nn", "cond-mat.soft", "cond-mat.stat-mech", "cs.LG", "cs.NE"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22787v1", "summary": "We present a vectorial extension of the Hopfield associative memory model\ninspired by the theory of amorphous solids, where binary neural states are\nreplaced by unit vectors $\\mathbf{s}_i \\in \\mathbb{R}^3$ on the sphere $S^2$.\nThe generalized Hebbian learning rule creates a block-structured weight matrix\nthrough outer products of stored pattern vectors, analogous to the Hessian\nmatrix structure in amorphous solids. We demonstrate that this model exhibits\nquantifiable structural properties characteristic of disordered materials:\nenergy landscapes with deep minima for stored patterns versus random\nconfigurations (energy gaps $\\sim 7$ units), strongly anisotropic correlations\nencoded in the weight matrix (anisotropy ratios $\\sim 10^2$), and\norder-disorder transitions controlled by the pattern density $\\gamma = P/(N\n\\cdot d)$. The enhanced memory capacity ($\\gamma_c \\approx 0.55$ for a\nfully-connected network) compared to binary networks ($\\gamma_c \\approx 0.138$)\nand the emergence of orientational correlations establish connections between\nassociative memory mechanisms and amorphous solid physics, particularly in\nsystems with continuous orientational degrees of freedom. We also unveil the\nscaling with the coordination number $Z$ of the memory capacity: $\\gamma_c \\sim\n(Z-6)$ from the isostatic point $Z_c =6$ of the 3D elastic network, which\nclosely mirrors the scaling of the shear modulus $G \\sim (Z-6)$ in 3D\ncentral-force spring networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22787v1", "cate": "cond-mat.dis-nn", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21110", "title": "SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering", "authors": ["Kezhen Zhong", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 12 figures", "url": "http://arxiv.org/abs/2507.21110v1", "summary": "This paper introduces SemRAG, an enhanced Retrieval Augmented Generation\n(RAG) framework that efficiently integrates domain-specific knowledge using\nsemantic chunking and knowledge graphs without extensive fine-tuning.\nIntegrating domain-specific knowledge into large language models (LLMs) is\ncrucial for improving their performance in specialized tasks. Yet, existing\nadaptations are computationally expensive, prone to overfitting and limit\nscalability. To address these challenges, SemRAG employs a semantic chunking\nalgorithm that segments documents based on the cosine similarity from sentence\nembeddings, preserving semantic coherence while reducing computational\noverhead. Additionally, by structuring retrieved information into knowledge\ngraphs, SemRAG captures relationships between entities, improving retrieval\naccuracy and contextual understanding. Experimental results on MultiHop RAG and\nWikipedia datasets demonstrate SemRAG has significantly enhances the relevance\nand correctness of retrieved information from the Knowledge Graph,\noutperforming traditional RAG methods. Furthermore, we investigate the\noptimization of buffer sizes for different data corpus, as optimizing buffer\nsizes tailored to specific datasets can further improve retrieval performance,\nas integration of knowledge graphs strengthens entity relationships for better\ncontextual comprehension. The primary advantage of SemRAG is its ability to\ncreate an efficient, accurate domain-specific LLM pipeline while avoiding\nresource-intensive fine-tuning. This makes it a practical and scalable approach\naligned with sustainability goals, offering a viable solution for AI\napplications in domain-specific fields.", "comment": "16 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.21110v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.21265", "title": "Multiscale geometrical and topological learning in the analysis of soft matter collective dynamics", "authors": ["Tetiana Orlova", "Amaranta Membrillo Solis", "Hayley R. O. Sohn", "Tristan Madeleine", "Giampaolo D'Alessandro", "Ivan I. Smalyukh", "Malgosia Kaczmarek", "Jacek Brodzki"], "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Soft Condensed Matter (cond-mat.soft)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21265v1", "summary": "Understanding the behavior and evolution of a dynamical many-body system by\nanalyzing patterns in their experimentally captured images is a promising\nmethod relevant for a variety of living and non-living self-assembled systems.\nThe arrays of moving liquid crystal skyrmions studied here are a representative\nexample of hierarchically organized materials that exhibit complex\nspatiotemporal dynamics driven by multiscale processes. Joint geometric and\ntopological data analysis (TDA) offers a powerful framework for investigating\nsuch systems by capturing the underlying structure of the data at multiple\nscales. In the TDA approach, we introduce the $\\Psi$-function, a robust\nnumerical topological descriptor related to both the spatiotemporal changes in\nthe size and shape of individual topological solitons and the emergence of\nregions with their different spatial organization. The geometric method based\non the analysis of vector fields generated from images of skyrmion ensembles\noffers insights into the nonlinear physical mechanisms of the system's response\nto external stimuli and provides a basis for comparison with theoretical\npredictions. The methodology presented here is very general and can provide a\ncharacterization of system behavior both at the level of individual\npattern-forming agents and as a whole, allowing one to relate the results of\nimage data analysis to processes occurring in a physical, chemical, or\nbiological system in the real world.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21265v1", "cate": "cond-mat.soft", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21917", "title": "ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval", "authors": ["Nicola Fanelli", "Gennaro Vessio", "Giovanna Castellano"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21917v1", "summary": "Analyzing digitized artworks presents unique challenges, requiring not only\nvisual interpretation but also a deep understanding of rich artistic,\ncontextual, and historical knowledge. We introduce ArtSeek, a multimodal\nframework for art analysis that combines multimodal large language models with\nretrieval-augmented generation. Unlike prior work, our pipeline relies only on\nimage input, enabling applicability to artworks without links to Wikidata or\nWikipedia-common in most digitized collections. ArtSeek integrates three key\ncomponents: an intelligent multimodal retrieval module based on late\ninteraction retrieval, a contrastive multitask classification network for\npredicting artist, genre, style, media, and tags, and an agentic reasoning\nstrategy enabled through in-context examples for complex visual question\nanswering and artwork explanation via Qwen2.5-VL. Central to this approach is\nWikiFragments, a Wikipedia-scale dataset of image-text fragments curated to\nsupport knowledge-grounded multimodal reasoning. Our framework achieves\nstate-of-the-art results on multiple benchmarks, including a +8.4% F1\nimprovement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in\ncaptioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret\nvisual motifs, infer historical context, and retrieve relevant knowledge, even\nfor obscure works. Though focused on visual arts, our approach generalizes to\nother domains requiring external knowledge, supporting scalable multimodal AI\nresearch. Both the dataset and the source code will be made publicly available\nat https://github.com/cilabuniba/artseek.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21917v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22832", "title": "Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks", "authors": ["Maciej Satkiewicz"], "categories": ["cs.LG", "cs.CV", "cs.NE", "I.2.6; I.4.10"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 4 figures, preprint", "url": "http://arxiv.org/abs/2507.22832v1", "summary": "In this paper we argue that ReLU networks learn an implicit linear model we\ncan actually tap into. We describe that alleged model formally and show that we\ncan approximately pull its decision boundary back to the input space with\ncertain simple modification to the backward pass. The resulting gradients\n(called excitation pullbacks) reveal high-resolution input- and target-specific\nfeatures of remarkable perceptual alignment on a number of popular\nImageNet-pretrained deep architectures. This strongly suggests that neural\nnetworks do, in fact, rely on learned interpretable patterns that can be\nrecovered after training. Thus, our findings may have profound implications for\nknowledge discovery and the development of dependable artificial systems.", "comment": "15 pages, 4 figures, preprint", "pdf_url": "http://arxiv.org/pdf/2507.22832v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22334", "title": "An inherent regularization approach to parameter-free preconditioning for nearly incompressible linear poroelasticity and elasticity", "authors": ["Weizhang Huang", "Zhuoran Wang"], "categories": ["math.NA", "cs.NA", "65N30, 65F08, 65F10, 74F10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22334v1", "summary": "An inherent regularization strategy and block Schur complement\npreconditioning are studied for linear poroelasticity problems discretized\nusing the lowest-order weak Galerkin FEM in space and the implicit Euler scheme\nin time. At each time step, the resulting saddle point system becomes nearly\nsingular in the locking regime, where the solid is nearly incompressible. This\nnear-singularity stems from the leading block, which corresponds to a linear\nelasticity system. To enable efficient iterative solution, this nearly singular\nsystem is first reformulated as a saddle point problem and then regularized by\nadding a term to the (2,2) block. This regularization preserves the solution\nwhile ensuring the non-singularity of the new system. As a result, block Schur\ncomplement preconditioning becomes effective. It is shown that the\npreconditioned MINRES and GMRES converge essentially independent of the mesh\nsize and the locking parameter. Both two- and three-field formulations are\nconsidered for the iterative solution of the linear poroelasticity. The\nefficient solution of the two-field formulation builds upon the effective\niterative solution of linear elasticity. For this case, MINRES and GMRES\nachieve parameter-free convergence when used with block Schur complement\npreconditioning, where the inverse of the leading block leverages efficient\nsolvers for linear elasticity. The poroelasticity problem can also be\nreformulated as a three-field system by introducing a numerical pressure\nvariable into the linear elasticity part. The inherent regularization strategy\nextends naturally to this formulation, and preconditioned MINRES and GMRES also\nshow parameter-free convergence for the regularized system. Numerical\nexperiments in both two and three dimensions confirm the effectiveness of the\nregularization strategy and the robustness of the block preconditioners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22334v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21114", "title": "Page image classification for content-specific data processing", "authors": ["Kateryna Lutsai", "Pavel Straňák"], "categories": ["cs.IR", "cs.AI", "cs.CV", "68T10, 68T09, 62H30", "I.7.5; H.3.7"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      65 pages, 57 figures, 20 tables", "url": "http://arxiv.org/abs/2507.21114v1", "summary": "Digitization projects in humanities often generate vast quantities of page\nimages from historical documents, presenting significant challenges for manual\nsorting and analysis. These archives contain diverse content, including various\ntext types (handwritten, typed, printed), graphical elements (drawings, maps,\nphotos), and layouts (plain text, tables, forms). Efficiently processing this\nheterogeneous data requires automated methods to categorize pages based on\ntheir content, enabling tailored downstream analysis pipelines. This project\naddresses this need by developing and evaluating an image classification system\nspecifically designed for historical document pages, leveraging advancements in\nartificial intelligence and machine learning. The set of categories was chosen\nto facilitate content-specific processing workflows, separating pages requiring\ndifferent analysis techniques (e.g., OCR for text, image analysis for graphics)", "comment": "65 pages, 57 figures, 20 tables", "pdf_url": "http://arxiv.org/pdf/2507.21114v1", "cate": "cs.IR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.21269", "title": "Numerical PDE solvers outperform neural PDE solvers", "authors": ["Patrick Chatain", "Michael Rizvi-Martel", "Guillaume Rabusseau", "Adam Oberman"], "categories": ["math.NA", "cs.LG", "cs.NA", "35R30 (Primary) 65M06 65M32 65C20 68T07 (Secondary)"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      17 pages, 7 figures", "url": "http://arxiv.org/abs/2507.21269v1", "summary": "We present DeepFDM, a differentiable finite-difference framework for learning\nspatially varying coefficients in time-dependent partial differential equations\n(PDEs). By embedding a classical forward-Euler discretization into a\nconvolutional architecture, DeepFDM enforces stability and first-order\nconvergence via CFL-compliant coefficient parameterizations. Model weights\ncorrespond directly to PDE coefficients, yielding an interpretable\ninverse-problem formulation. We evaluate DeepFDM on a benchmark suite of scalar\nPDEs: advection, diffusion, advection-diffusion, reaction-diffusion and\ninhomogeneous Burgers' equations-in one, two and three spatial dimensions. In\nboth in-distribution and out-of-distribution tests (quantified by the Hellinger\ndistance between coefficient priors), DeepFDM attains normalized mean-squared\nerrors one to two orders of magnitude smaller than Fourier Neural Operators,\nU-Nets and ResNets; requires 10-20X fewer training epochs; and uses 5-50X fewer\nparameters. Moreover, recovered coefficient fields accurately match\nground-truth parameters. These results establish DeepFDM as a robust,\nefficient, and transparent baseline for data-driven solution and identification\nof parametric PDEs.", "comment": "17 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.21269v1", "cate": "math.NA", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21922", "title": "SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention", "authors": ["Peiran Gu", "Teng Yao", "Mengshen He", "Fuhao Duan", "Feiyan Liu", "RenYuan Peng", "Bao Ge"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2507.21922v1", "summary": "In recent years, artificial intelligence has been increasingly applied in the\nfield of medical imaging. Among these applications, fundus image analysis\npresents special challenges, including small lesion areas in certain fundus\ndiseases and subtle inter-disease differences, which can lead to reduced\nprediction accuracy and overfitting in the models. To address these challenges,\nthis paper proposes the Transformer-based model SwinECAT, which combines the\nShifted Window (Swin) Attention with the Efficient Channel Attention (ECA)\nAttention. SwinECAT leverages the Swin Attention mechanism in the Swin\nTransformer backbone to effectively capture local spatial structures and\nlong-range dependencies within fundus images. The lightweight ECA mechanism is\nincorporated to guide the SwinECAT's attention toward critical feature\nchannels, enabling more discriminative feature representation. In contrast to\nprevious studies that typically classify fundus images into 4 to 6 categories,\nthis work expands fundus disease classification to 9 distinct types, thereby\nenhancing the granularity of diagnosis. We evaluate our method on the Eye\nDisease Image Dataset (EDID) containing 16,140 fundus images for 9-category\nclassification. Experimental results demonstrate that SwinECAT achieves 88.29\\%\naccuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The\nclassification results of our proposed model SwinECAT significantly outperform\nthe baseline Swin Transformer and multiple compared baseline models. To our\nknowledge, this represents the highest reported performance for 9-category\nclassification on this public dataset.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2507.21922v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.08394", "title": "($\\boldsymbolθ_l, \\boldsymbolθ_u$)-Parametric Multi-Task Optimization: Joint Search in Solution and Infinite Task Spaces", "authors": ["Tingyang Wei", "Jiao Liu", "Abhishek Gupta", "Puay Siew Tan", "Yew-Soon Ong"], "categories": ["cs.NE", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.08394v3", "summary": "Multi-task optimization is typically characterized by a fixed and finite set\nof tasks. The present paper relaxes this condition by considering a non-fixed\nand potentially infinite set of optimization tasks defined in a parameterized,\ncontinuous and bounded task space. We refer to this unique problem setting as\nparametric multi-task optimization (PMTO). Assuming the bounds of the task\nparameters to be ($\\boldsymbol{\\theta}_l$, $\\boldsymbol{\\theta}_u$), a novel\n($\\boldsymbol{\\theta}_l$, $\\boldsymbol{\\theta}_u$)-PMTO algorithm is crafted to\noperate in two complementary modes. In an offline optimization mode, a joint\nsearch over solution and task spaces is carried out with the creation of two\napproximation models: (1) for mapping points in a unified solution space to the\nobjective spaces of all tasks, which provably accelerates convergence by acting\nas a conduit for inter-task knowledge transfers, and (2) for probabilistically\nmapping tasks to their corresponding solutions, which facilitates evolutionary\nexploration of under-explored regions of the task space. In the online mode,\nthe derived models enable direct optimization of any task within the bounds\nwithout the need to search from scratch. This outcome is validated on both\nsynthetic test problems and practical case studies, with the significant\nreal-world applicability of PMTO shown towards fast reconfiguration of robot\ncontrollers under changing task conditions. The potential of PMTO to vastly\nspeedup the search for solutions to minimax optimization problems is also\ndemonstrated through an example in robust engineering design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.08394v3", "cate": "cs.NE", "date": "2025-03-11", "updated": "2025-07-30"}
{"id": "2507.22449", "title": "Pulsatile Flows for Simplified Smart Fluids with Variable Power-Law: Analysis and Numerics", "authors": ["Luigi C. Berselli", "Alex Kaltenbach"], "categories": ["math.NA", "cs.NA", "math.AP", "35Q35, 76A05, 76D05, 65M60, 35D30, 35K55, 35B10, 76M10, 35Q30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      37 pages, 12 figures", "url": "http://arxiv.org/abs/2507.22449v1", "summary": "We study the fully-developed, time-periodic motion of a shear-dependent\nnon-Newtonian fluid with variable exponent rheology through an infinite pipe\n$\\Omega:= \\mathbb{R}\\times \\Sigma\\subseteq \\mathbb{R}^d$, $d\\in \\{2,3\\}$, of\narbitrary cross-section $\\Sigma\\subseteq \\mathbb{R}^{d-1}$. The focus is on a\ngeneralized $p(\\cdot)$-fluid model, where the power-law index is\nposition-dependent (with respect to $\\Sigma$), $\\textit{i.e.}$, a function\n$p\\colon \\Sigma\\to (1,+\\infty)$. We prove the existence of time-periodic\nsolutions with either assigned time-periodic flow-rate or pressure-drop,\ngeneralizing known results for the Navier-Stokes and for $p$-fluid equations.\n  In addition, we identify explicit solutions, relevant as benchmark cases,\nespecially for electro-rheological fluids or, more generally, $\\textit{`smart\nfluids'}$. To support practical applications, we present a fully-constructive\nexistence proof for variational solutions by means of a fully-discrete\nfinite-differences/-elements discretization, consistent with our numerical\nexperiments. Our approach, which unifies the treatment of all values of\n$p(\\overline{x})\\in (1,+\\infty)$, $\\overline{x}\\in \\Sigma$, without requiring\nan auxiliary Newtonian term, provides new insights even in the constant\nexponent case. The theoretical findings are reviewed by means of numerical\nexperiments.", "comment": "37 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.22449v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21117", "title": "A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges", "authors": ["Rahul Raja", "Anshaj Vats", "Arpita Vats", "Anirban Majumder"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21117v1", "summary": "Recommender systems have traditionally followed modular architectures\ncomprising candidate generation, multi-stage ranking, and re-ranking, each\ntrained separately with supervised objectives and hand-engineered features.\nWhile effective in many domains, such systems face persistent challenges\nincluding sparse and noisy interaction data, cold-start problems, limited\npersonalization depth, and inadequate semantic understanding of user and item\ncontent. The recent emergence of Large Language Models (LLMs) offers a new\nparadigm for addressing these limitations through unified, language-native\nmechanisms that can generalize across tasks, domains, and modalities. In this\npaper, we present a comprehensive technical survey of how LLMs can be leveraged\nto tackle key challenges in modern recommender systems. We examine the use of\nLLMs for prompt-driven candidate retrieval, language-native ranking,\nretrieval-augmented generation (RAG), and conversational recommendation,\nillustrating how these approaches enhance personalization, semantic alignment,\nand interpretability without requiring extensive task-specific supervision.\nLLMs further enable zero- and few-shot reasoning, allowing systems to operate\neffectively in cold-start and long-tail scenarios by leveraging external\nknowledge and contextual cues. We categorize these emerging LLM-driven\narchitectures and analyze their effectiveness in mitigating core bottlenecks of\nconventional pipelines. In doing so, we provide a structured framework for\nunderstanding the design space of LLM-enhanced recommenders, and outline the\ntrade-offs between accuracy, scalability, and real-time performance. Our goal\nis to demonstrate that LLMs are not merely auxiliary components but\nfoundational enablers for building more adaptive, semantically rich, and\nuser-centric recommender systems", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21117v1", "cate": "cs.IR", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.21270", "title": "Generative imaging for radio interferometry with fast uncertainty quantification", "authors": ["Matthijs Mars", "Tobías I. Liaudat", "Jessica J. Whitney", "Marta M. Betcke", "Jason D. McEwen"], "categories": ["astro-ph.IM", "cs.LG"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21270v1", "summary": "With the rise of large radio interferometric telescopes, particularly the\nSKA, there is a growing demand for computationally efficient image\nreconstruction techniques. Existing reconstruction methods, such as the CLEAN\nalgorithm or proximal optimisation approaches, are iterative in nature,\nnecessitating a large amount of compute. These methods either provide no\nuncertainty quantification or require large computational overhead to do so.\nLearned reconstruction methods have shown promise in providing efficient and\nhigh quality reconstruction. In this article we explore the use of generative\nneural networks that enable efficient approximate sampling of the posterior\ndistribution for high quality reconstructions with uncertainty quantification.\nOur RI-GAN framework, builds on the regularised conditional generative\nadversarial network (rcGAN) framework by integrating a gradient U-Net (GU-Net)\narchitecture - a hybrid reconstruction model that embeds the measurement\noperator directly into the network. This framework uses Wasserstein GANs to\nimprove training stability in combination with regularisation terms that combat\nmode collapse, which are typical problems for conditional GANs. This approach\ntakes as input the dirty image and the point spread function (PSF) of the\nobservation and provides efficient, high-quality image reconstructions that are\nrobust to varying visibility coverages, generalises to images with an increased\ndynamic range, and provides informative uncertainty quantification. Our methods\nprovide a significant step toward computationally efficient, scalable, and\nuncertainty-aware imaging for next-generation radio telescopes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21270v1", "cate": "astro-ph.IM", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21924", "title": "MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning", "authors": ["Tianhong Gao", "Yannian Fu", "Weiqun Wu", "Haixiao Yue", "Shanshan Liu", "Gang Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21924v1", "summary": "Large Language Models (LLMs), enhanced through agent tuning, have\ndemonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool\nutilization, significantly surpassing the performance of standalone models.\nHowever, the multimodal domain still lacks a large-scale, high-quality agent\ntuning dataset to unlock the full potential of multimodal large language\nmodels. To bridge this gap, we introduce MMAT-1M, the first million-scale\nmultimodal agent tuning dataset designed to support CoT, reflection, and\ndynamic tool usage. Our dataset is constructed through a novel four-stage data\nengine: 1) We first curate publicly available multimodal datasets containing\nquestion-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for\nthe original question-answer pairs and dynamically integrate API calls and\nRetrieval Augmented Generation (RAG) information through a multi-turn paradigm;\n3) Furthermore, we refine the rationales through reflection to ensure logical\nconsistency and accuracy, creating a multi-turn dialogue dataset with both\nRationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally\ncompress multi-turn dialogues into a One-turn Rationale and Reflection (ORR)\nformat. By fine-tuning open-source multimodal models on the MMAT-1M, we observe\nsignificant performance gains. For instance, the InternVL2.5-8B-RR model\nachieves an average improvement of 2.7% across eight public benchmarks and 8.8%\non the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in\nenhancing multimodal reasoning and tool-based capabilities. The dataset is\npublicly available at https://github.com/VIS-MPU-Agent/MMAT-1M.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21924v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2407.05650", "title": "The Cooperative Network Architecture: Learning Structured Networks as Representation of Sensory Patterns", "authors": ["Pascal J. Sager", "Jan M. Deriu", "Benjamin F. Grewe", "Thilo Stadelmann", "Christoph von der Malsburg"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.05650v4", "summary": "We introduce the Cooperative Network Architecture (CNA), a model that\nrepresents sensory signals using structured, recurrently connected networks of\nneurons, termed \"nets.\" Nets are dynamically assembled from overlapping net\nfragments, which are learned based on statistical regularities in sensory\ninput. This architecture offers robustness to noise, deformation, and\nout-of-distribution data, addressing challenges in current vision systems from\na novel perspective. We demonstrate that net fragments can be learned without\nsupervision and flexibly recombined to encode novel patterns, enabling figure\ncompletion and resilience to noise. Our findings establish CNA as a promising\nparadigm for developing neural representations that integrate local feature\nprocessing with global structure formation, providing a foundation for future\nresearch on invariant object recognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.05650v4", "cate": "cs.CV", "date": "2024-07-08", "updated": "2025-07-30"}
{"id": "2507.22539", "title": "A surrogate model for topology optimisation of elastic structures via parametric autoencoders", "authors": ["Matteo Giacomini", "Antonio Huerta"], "categories": ["math.NA", "cs.AI", "cs.CE", "cs.LG", "cs.NA", "math.OC", "49M41, 74P05, 74P15, 74S05, 65M60, 65M30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      39 pages, 13 figures, 7 tables", "url": "http://arxiv.org/abs/2507.22539v1", "summary": "A surrogate-based topology optimisation algorithm for linear elastic\nstructures under parametric loads and boundary conditions is proposed. Instead\nof learning the parametric solution of the state (and adjoint) problems or the\noptimisation trajectory as a function of the iterations, the proposed approach\ndevises a surrogate version of the entire optimisation pipeline. First, the\nmethod predicts a quasi-optimal topology for a given problem configuration as a\nsurrogate model of high-fidelity topologies optimised with the homogenisation\nmethod. This is achieved by means of a feed-forward net learning the mapping\nbetween the input parameters characterising the system setup and a latent space\ndetermined by encoder/decoder blocks reducing the dimensionality of the\nparametric topology optimisation problem and reconstructing a high-dimensional\nrepresentation of the topology. Then, the predicted topology is used as an\neducated initial guess for a computationally efficient algorithm penalising the\nintermediate values of the design variable, while enforcing the governing\nequations of the system. This step allows the method to correct potential\nerrors introduced by the surrogate model, eliminate artifacts, and refine the\ndesign in order to produce topologies consistent with the underlying physics.\nDifferent architectures are proposed and the approximation and generalisation\ncapabilities of the resulting models are numerically evaluated. The\nquasi-optimal topologies allow to outperform the high-fidelity optimiser by\nreducing the average number of optimisation iterations by $53\\%$ while\nachieving discrepancies below $4\\%$ in the optimal value of the objective\nfunctional, even in the challenging scenario of testing the model to\nextrapolate beyond the training and validation domain.", "comment": "39 pages, 13 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.22539v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21120", "title": "Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation", "authors": ["Bereket A. Yilma", "Luis A. Leiva"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at the 19th ACM Conference on Recommender Systems", "url": "http://arxiv.org/abs/2507.21120v1", "summary": "Art Therapy (AT) is an established practice that facilitates emotional\nprocessing and recovery through creative expression. Recently, Visual Art\nRecommender Systems (VA RecSys) have emerged to support AT, demonstrating their\npotential by personalizing therapeutic artwork recommendations. Nonetheless,\ncurrent VA RecSys rely on visual stimuli for user modeling, limiting their\nability to capture the full spectrum of emotional responses during preference\nelicitation. Previous studies have shown that music stimuli elicit unique\naffective reflections, presenting an opportunity for cross-domain\nrecommendation (CDR) to enhance personalization in AT. Since CDR has not yet\nbeen explored in this context, we propose a family of CDR methods for AT based\non music-driven preference elicitation. A large-scale study with 200 users\ndemonstrates the efficacy of music-driven preference elicitation, outperforming\nthe classic visual-only elicitation approach. Our source code, data, and models\nare available at https://github.com/ArtAICare/Affect-aware-CDR", "comment": "Accepted at the 19th ACM Conference on Recommender Systems", "pdf_url": "http://arxiv.org/pdf/2507.21120v1", "cate": "cs.IR", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.21330", "title": "Predicting VBAC Outcomes from U.S. Natality Data using Deep and Classical Machine Learning Models", "authors": ["Ananya Anand"], "categories": ["stat.AP", "cs.LG"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      12 pages, 10 figures, 1 table", "url": "http://arxiv.org/abs/2507.21330v1", "summary": "Accurately predicting the outcome of a trial of labor after cesarean (TOLAC)\nis essential for guiding prenatal counseling and minimizing delivery-related\nrisks. This study presents supervised machine learning models for predicting\nvaginal birth after cesarean (VBAC) using 643,029 TOLAC cases from the CDC\nWONDER Natality dataset (2017-2023). After filtering for singleton births with\none or two prior cesareans and complete data across 47 prenatal-period\nfeatures, three classifiers were trained: logistic regression, XGBoost, and a\nmultilayer perceptron (MLP). The MLP achieved the highest performance with an\nAUC of 0.7287, followed closely by XGBoost (AUC = 0.727), both surpassing the\nlogistic regression baseline (AUC = 0.709). To address class imbalance, class\nweighting was applied to the MLP, and a custom loss function was implemented in\nXGBoost. Evaluation metrics included ROC curves, confusion matrices, and\nprecision-recall analysis. Logistic regression coefficients highlighted\nmaternal BMI, education, parity, comorbidities, and prenatal care indicators as\nkey predictors. Overall, the results demonstrate that routinely collected,\nearly-pregnancy variables can support scalable and moderately high-performing\nVBAC prediction models. These models offer potential utility in clinical\ndecision support, particularly in settings lacking access to specialized\nintrapartum data.", "comment": "12 pages, 10 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.21330v1", "cate": "stat.AP", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21945", "title": "Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment", "authors": ["Xin Wang", "Peng-Jie Li", "Yuan-Yuan Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to Applied Soft Computing", "url": "http://arxiv.org/abs/2507.21945v1", "summary": "Long-term action quality assessment (AQA) focuses on evaluating the quality\nof human activities in videos lasting up to several minutes. This task plays an\nimportant role in the automated evaluation of artistic sports such as rhythmic\ngymnastics and figure skating, where both accurate motion execution and\ntemporal synchronization with background music are essential for performance\nassessment. However, existing methods predominantly fall into two categories:\nunimodal approaches that rely solely on visual features, which are inadequate\nfor modeling multimodal cues like music; and multimodal approaches that\ntypically employ simple feature-level contrastive fusion, overlooking deep\ncross-modal collaboration and temporal dynamics. As a result, they struggle to\ncapture complex interactions between modalities and fail to accurately track\ncritical performance changes throughout extended sequences. To address these\nchallenges, we propose the Long-term Multimodal Attention Consistency Network\n(LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to\nexplicitly align multimodal features, enabling stable integration of visual and\naudio information and enhancing feature representations. Specifically, we\nintroduce a multimodal local query encoder module to capture temporal semantics\nand cross-modal relations, and use a two-level score evaluation for\ninterpretable results. In addition, attention-based and regression-based losses\nare applied to jointly optimize multimodal alignment and score fusion.\nExperiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net\nsignificantly outperforms existing methods, validating the effectiveness of our\nproposed approach.", "comment": "Accepted to Applied Soft Computing", "pdf_url": "http://arxiv.org/pdf/2507.21945v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2408.11198", "title": "Automated Prompt Engineering for Cost-Effective Code Generation Using Evolutionary Algorithm", "authors": ["Hamed Taherkhani", "Melika Sepindband", "Hung Viet Pham", "Song Wang", "Hadi Hemmati"], "categories": ["cs.SE", "cs.AI", "cs.NE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.11198v2", "summary": "Large Language Models have seen increasing use in various software\ndevelopment tasks, especially in code generation. The most advanced recent\nmethods attempt to incorporate feedback from code execution into prompts to\nhelp guide LLMs in generating correct code in an iterative process. While\neffective, these methods could be costly due to numerous interactions with the\nLLM and extensive token usage. To address this issue, we propose an alternative\napproach named Evolutionary Prompt Engineering for Code (EPiC), which leverages\na lightweight evolutionary algorithm to refine the original prompts into\nimproved versions that generate high quality code, with minimal interactions\nwith the LLM. Our evaluation against state-of-the-art (SOTA) LLM based code\ngeneration agents shows that EPiC not only achieves up to 6% improvement in\npass@k but is also 2-10 times more cost-effective than the baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.11198v2", "cate": "cs.SE", "date": "2024-08-20", "updated": "2025-07-29"}
{"id": "2507.22609", "title": "A Structure-Preserving Rational Integrator for the Replicator Dynamics on the Probability Simplex", "authors": ["Mario Pezzella"], "categories": ["math.NA", "cs.NA", "math.DS", "65L05, 65L70, 65Z05, 37M15, 41A20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22609v1", "summary": "In this work, we introduce a quadratically convergent and dynamically\nconsistent integrator specifically designed for the replicator dynamics. The\nproposed scheme combines a two-stage rational approximation with a\nnormalization step to ensure confinement to the probability simplex and\nunconditional preservation of non-negativity, invariant sets and equilibria. A\nrigorous convergence analysis is provided to establish the scheme's\nsecond-order accuracy, and an embedded auxiliary method is devised for adaptive\ntime-stepping based on local error estimation. Furthermore, a discrete analogue\nof the quotient rule, which governs the evolution of component ratios, is shown\nto hold. Numerical experiments validate the theoretical results, illustrating\nthe method's ability to reproduce complex dynamics and to outperform\nwell-established solvers in particularly challenging scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22609v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21288", "title": "Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties", "authors": ["Guanxiong Chen", "Shashwat Suri", "Yuhao Wu", "Etienne Voulga", "David I. W. Levin", "Dinesh Pai"], "categories": ["cs.GR", "cs.AI"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21288v1", "summary": "Materials used in real clothing exhibit remarkable complexity and spatial\nvariation due to common processes such as stitching, hemming, dyeing, printing,\npadding, and bonding. Simulating these materials, for instance using finite\nelement methods, is often computationally demanding and slow. Worse, such\nmethods can suffer from numerical artifacts called ``membrane locking'' that\nmakes cloth appear artificially stiff. Here we propose a general framework,\ncalled Mass-Spring Net, for learning a simple yet efficient surrogate model\nthat captures the effects of these complex materials using only motion\nobservations. The cloth is discretized into a mass-spring network with unknown\nmaterial parameters that are learned directly from the motion data, using a\nnovel force-and-impulse loss function. Our approach demonstrates the ability to\naccurately model spatially varying material properties from a variety of data\nsources, and immunity to membrane locking which plagues FEM-based simulations.\nCompared to graph-based networks and neural ODE-based architectures, our method\nachieves significantly faster training times, higher reconstruction accuracy,\nand improved generalization to novel dynamic scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21288v1", "cate": "cs.GR", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21334", "title": "Graph neural networks for residential location choice: connection to classical logit models", "authors": ["Zhanhong Cheng", "Lingqian Hu", "Yuheng Bu", "Yuqi Zhou", "Shenhao Wang"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21334v1", "summary": "Researchers have adopted deep learning for classical discrete choice analysis\nas it can capture complex feature relationships and achieve higher predictive\nperformance. However, the existing deep learning approaches cannot explicitly\ncapture the relationship among choice alternatives, which has been a\nlong-lasting focus in classical discrete choice models. To address the gap,\nthis paper introduces Graph Neural Network (GNN) as a novel framework to\nanalyze residential location choice. The GNN-based discrete choice models\n(GNN-DCMs) offer a structured approach for neural networks to capture\ndependence among spatial alternatives, while maintaining clear connections to\nclassical random utility theory. Theoretically, we demonstrate that the\nGNN-DCMs incorporate the nested logit (NL) model and the spatially correlated\nlogit (SCL) model as two specific cases, yielding novel algorithmic\ninterpretation through message passing among alternatives' utilities.\nEmpirically, the GNN-DCMs outperform benchmark MNL, SCL, and feedforward neural\nnetworks in predicting residential location choices among Chicago's 77\ncommunity areas. Regarding model interpretation, the GNN-DCMs can capture\nindividual heterogeneity and exhibit spatially-aware substitution patterns.\nOverall, these results highlight the potential of GNN-DCMs as a unified and\nexpressive framework for synergizing discrete choice modeling and deep learning\nin the complex spatial choice contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21334v1", "cate": "stat.ML", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21947", "title": "Enhancing Generalization in Data-free Quantization via Mixup-class Prompting", "authors": ["Jiwoong Park", "Chaeun Lee", "Yongseok Choi", "Sein Park", "Deokki Hong", "Jungwook Choi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21947v1", "summary": "Post-training quantization (PTQ) improves efficiency but struggles with\nlimited calibration data, especially under privacy constraints. Data-free\nquantization (DFQ) mitigates this by generating synthetic images using\ngenerative models such as generative adversarial networks (GANs) and\ntext-conditioned latent diffusion models (LDMs), while applying existing PTQ\nalgorithms. However, the relationship between generated synthetic images and\nthe generalizability of the quantized model during PTQ remains underexplored.\nWithout investigating this relationship, synthetic images generated by previous\nprompt engineering methods based on single-class prompts suffer from issues\nsuch as polysemy, leading to performance degradation. We propose\n\\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses\nmultiple class labels at the text prompt level to generate diverse, robust\nsynthetic data. This approach enhances generalization, and improves\noptimization stability in PTQ. We provide quantitative insights through\ngradient norm and generalization error analysis. Experiments on convolutional\nneural networks (CNNs) and vision transformers (ViTs) show that our method\nconsistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore,\nit pushes the performance boundary in extremely low-bit scenarios, achieving\nnew state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation\n(W2A4) quantization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21947v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2501.13883", "title": "Utilizing Evolution Strategies to Train Transformers in Reinforcement Learning", "authors": ["Matyáš Lorenc", "Roman Neruda"], "categories": ["cs.LG", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.13883v2", "summary": "We explore the capability of evolution strategies to train an agent with a\npolicy based on a transformer architecture in a reinforcement learning setting.\nWe performed experiments using OpenAI's highly parallelizable evolution\nstrategy to train Decision Transformer in the MuJoCo Humanoid locomotion\nenvironment and in the environment of Atari games, testing the ability of this\nblack-box optimization technique to train even such relatively large and\ncomplicated models (compared to those previously tested in the literature). The\nexamined evolution strategy proved to be, in general, capable of achieving\nstrong results and managed to produce high-performing agents, showcasing\nevolution's ability to tackle the training of even such complex models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.13883v2", "cate": "cs.LG", "date": "2025-01-23", "updated": "2025-07-30"}
{"id": "2507.22634", "title": "Tropical solution of discrete best approximation problems", "authors": ["Nikolai Krivulin"], "categories": ["math.NA", "cs.NA", "15A80 (Primary), 90C24, 41A50, 41A65, 65D15 (Secondary)"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      23 pages", "url": "http://arxiv.org/abs/2507.22634v1", "summary": "We consider discrete best approximation problems in the setting of tropical\nalgebra that is concerned with the theory and application of algebraic systems\nwith idempotent operations. Given a set of input-output pairs of an unknown\nfunction defined on a tropical semifield, the problem is to determine an\napproximating rational function formed by two Puiseux polynomials as numerator\nand denominator. With specified numbers of monomials in both polynomials, the\napproximation aims at evaluating the exponent and coefficient for each monomial\nin the polynomials to fit the rational function to the data in the sense of a\ntropical distance function. To solve the problem, we transform it into\napproximation of a vector equation with unknown vectors on both sides, where\none side corresponds to the numerator polynomial and the other side to the\ndenominator. Each side involves a matrix with entries dependent on the unknown\nexponents, multiplied by the vector of unknown coefficients of monomials. We\npropose an algorithm that constructs a series of approximate solutions by\nalternately fixing one side of the equation to an already found result and\nleaving the other intact. Each equation obtained is approximated with respect\nto the vector of coefficients, which yields a vector of coefficients and\napproximation error both parameterized by the exponents. The exponents are\nfound by minimizing the error with an optimization procedure based on\nagglomerative clustering technique. To illustrate, we present results for\napproximation problems in terms of max-plus algebra (a real semifield with\naddition defined as maximum and multiplication as arithmetic addition), which\ncorrespond to ordinary problems of piecewise linear approximation of real\nfunctions. As our numerical experience shows, the proposed algorithm converges\nin a finite number of steps and provides a reasonable solution to the problems\nconsidered.", "comment": "23 pages", "pdf_url": "http://arxiv.org/pdf/2507.22634v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22208", "title": "Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice Biometrics", "authors": ["Shreyansh Pathak", "Sonu Shreshtha", "Richa Singh", "Mayank Vatsa"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures, 5 tables, Accepted at IJCB 2025 (Osaka, Japan)", "url": "http://arxiv.org/abs/2507.22208v1", "summary": "The widespread adoption of voice-enabled authentication and audio biometric\nsystems have significantly increased privacy vulnerabilities associated with\nsensitive speech data. Compliance with privacy regulations such as GDPR's right\nto be forgotten and India's DPDP Act necessitates targeted and efficient\nerasure of individual-specific voice signatures from already-trained biometric\nmodels. Existing unlearning methods designed for visual data inadequately\nhandle the sequential, temporal, and high-dimensional nature of audio signals,\nleading to ineffective or incomplete speaker and accent erasure. To address\nthis, we introduce QPAudioEraser, a quantum-inspired audio unlearning\nframework. Our our-phase approach involves: (1) weight initialization using\ndestructive interference to nullify target features, (2) superposition-based\nlabel transformations that obscure class identity, (3) an\nuncertainty-maximizing quantum loss function, and (4) entanglement-inspired\nmixing of correlated weights to retain model knowledge. Comprehensive\nevaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech\nCommands, LibriSpeech, and Speech Accent Archive datasets validate\nQPAudioEraser's superior performance. The framework achieves complete erasure\nof target data (0% Forget Accuracy) while incurring minimal impact on model\nutility, with a performance degradation on retained data as low as 0.05%.\nQPAudioEraser consistently surpasses conventional baselines across\nsingle-class, multi-class, sequential, and accent-level erasure scenarios,\nestablishing the proposed approach as a robust privacy-preserving solution.", "comment": "9 pages, 2 figures, 5 tables, Accepted at IJCB 2025 (Osaka, Japan)", "pdf_url": "http://arxiv.org/pdf/2507.22208v1", "cate": "cs.SD", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21295", "title": "Semantic Numeration Systems as Dynamical Systems", "authors": ["Alexander Yu. Chunikhin"], "categories": ["cs.LO", "cs.AI", "11A63, 47S20, 68Q55"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      11 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21295v1", "summary": "The foundational concepts of semantic numeration systems theory are briefly\noutlined. The action of cardinal semantic operators unfolds over a set of\ncardinal abstract entities belonging to the cardinal semantic multeity. The\ncardinal abstract object (CAO) formed by them in a certain connectivity\ntopology is proposed to be considered as a linear discrete dynamical system\nwith nonlinear control. Under the assumption of ideal observability, the CAO\nstate equations are provided for both stationary and non-stationary cases. The\nfundamental role of the configuration matrix, which combines information about\nthe types of cardinal semantic operators in the CAO, their parameters and\ntopology of connectivity, is demonstrated.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21295v1", "cate": "cs.LO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21377", "title": "Reservoir Computation with Networks of Differentiating Neuron Ring Oscillators", "authors": ["Alexander Yeung", "Peter DelMastro", "Arjun Karuvally", "Hava Siegelmann", "Edward Rietman", "Hananel Hazan"], "categories": ["cs.NE", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures", "url": "http://arxiv.org/abs/2507.21377v1", "summary": "Reservoir Computing is a machine learning approach that uses the rich\nrepertoire of complex system dynamics for function approximation. Current\napproaches to reservoir computing use a network of coupled integrating neurons\nthat require a steady current to maintain activity. Here, we introduce a small\nworld graph of differentiating neurons that are active only when there are\nchanges in input as an alternative to integrating neurons as a reservoir\ncomputing substrate. We find the coupling strength and network topology that\nenable these small world networks to function as an effective reservoir. We\ndemonstrate the efficacy of these networks in the MNIST digit recognition task,\nachieving comparable performance of 90.65% to existing reservoir computing\napproaches. The findings suggest that differentiating neurons can be a\npotential alternative to integrating neurons and can provide a sustainable\nfuture alternative for power-hungry AI applications.", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.21377v1", "cate": "cs.NE", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21949", "title": "Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal", "authors": ["Jiyu Wu", "Yifan Liu", "Jiancheng Huang", "Mingfu Yan", "Shifeng Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21949v1", "summary": "Existing shadow removal methods often rely on shadow masks, which are\nchallenging to acquire in real-world scenarios. Exploring intrinsic image cues,\nsuch as local contrast information, presents a potential alternative for\nguiding shadow removal in the absence of explicit masks. However, the cue's\ninherent ambiguity becomes a critical limitation in complex scenes, where it\ncan fail to distinguish true shadows from low-reflectance objects and intricate\nbackground textures. To address this motivation, we propose the Adaptive Gated\nDual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs\nthe contrast prior to effectively disentangle shadow features from confounding\nvisual elements. Furthermore, to tackle the persistent challenge of restoring\nsoft shadow boundaries and fine-grained details, we introduce a diffusion-based\nFrequency-Contrast Fusion Network (FCFN) that leverages high-frequency and\ncontrast cues to guide the generative process. Extensive experiments\ndemonstrate that our method achieves state-of-the-art results among mask-free\napproaches while maintaining competitive performance relative to mask-based\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21949v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22757", "title": "Space-time finite element methods for nonlinear wave equations via elliptic regularisation", "authors": ["Lehel Banjai", "Emmanuil H. Georgoulis", "Brian Hennessy"], "categories": ["math.NA", "cs.NA", "65M60, 65M12, 35L71"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      24 pages, 6 figures", "url": "http://arxiv.org/abs/2507.22757v1", "summary": "We present and analyse a new conforming space-time Galerkin discretisation of\na semi-linear wave equation, based on a variational formulation derived from De\nGiorgi's elliptic regularisation viewpoint of the wave equation in second-order\nformulation. The method is shown to be well-posed through a minimisation\napproach, and also unconditionally stable for all choices of conforming\ndiscretisation spaces. Further, a priori error bounds are proven for\nsufficiently smooth solutions. Special attention is given to the conditioning\nof the method and its stable implementation. Numerical experiments are provided\nto validate the theoretical findings.", "comment": "24 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.22757v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22322", "title": "A Two-Step Learning Framework for Enhancing Sound Event Localization and Detection", "authors": ["Hogeon Yu"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5pages, 2figures", "url": "http://arxiv.org/abs/2507.22322v1", "summary": "Sound Event Localization and Detection (SELD) is crucial in spatial audio\nprocessing, enabling systems to detect sound events and estimate their 3D\ndirections. Existing SELD methods use single- or dual-branch architectures:\nsingle-branch models share SED and DoA representations, causing optimization\nconflicts, while dual-branch models separate tasks but limit information\nexchange. To address this, we propose a two-step learning framework. First, we\nintroduce a tracwise reordering format to maintain temporal consistency,\npreventing event reassignments across tracks. Next, we train SED and DoA\nnetworks to prevent interference and ensure task-specific feature learning.\nFinally, we effectively fuse DoA and SED features to enhance SELD performance\nwith better spatial and event representation. Experiments on the 2023 DCASE\nchallenge Task 3 dataset validate our framework, showing its ability to\novercome single- and dual-branch limitations and improve event classification\nand localization.", "comment": "5pages, 2figures", "pdf_url": "http://arxiv.org/pdf/2507.22322v1", "cate": "cs.SD", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21340", "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation", "authors": ["Satyananda Kashyap", "Sola Shirai", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Data available: this https URL and code available at: this https URL", "url": "http://arxiv.org/abs/2507.21340v1", "summary": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.", "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text", "pdf_url": "http://arxiv.org/pdf/2507.21340v1", "cate": "cs.CL", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21429", "title": "From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "categories": ["stat.ML", "cs.LG", "68T07, 90C26, 65K10"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21429v1", "summary": "The convergence of gradient descent (GD) on the non-convex loss landscapes of\ndeep neural networks (DNNs) presents a fundamental theoretical challenge. While\nrecent work has established that GD converges to a stationary point at a\nsublinear rate within locally quasi-convex regions (LQCRs), this fails to\nexplain the exponential convergence rates consistently observed in practice. In\nthis paper, we resolve this discrepancy by proving that under a mild assumption\non Neural Tangent Kernel (NTK) stability, these same regions satisfy a local\nPolyak-Lojasiewicz (PL) condition. We introduce the concept of a Locally\nPolyak-Lojasiewicz Region (LPLR), where the squared gradient norm lower-bounds\nthe suboptimality gap, prove that properly initialized finite-width networks\nadmit such regions around initialization, and establish that GD achieves linear\nconvergence within an LPLR, providing the first finite-width guarantee that\nmatches empirically observed rates. We validate our theory across diverse\nsettings, from controlled experiments on fully-connected networks to modern\nResNet architectures trained with stochastic methods, demonstrating that LPLR\nstructure emerges robustly in practical deep learning scenarios. By rigorously\nconnecting local landscape geometry to fast optimization through the NTK\nframework, our work provides a definitive theoretical explanation for the\nremarkable efficiency of gradient-based optimization in deep learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21429v1", "cate": "stat.ML", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21959", "title": "Mitigating Spurious Correlations in Weakly Supervised Semantic Segmentation via Cross-architecture Consistency Regularization", "authors": ["Zheyuan Zhang", "Yen-chia Hsu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21959v1", "summary": "Scarcity of pixel-level labels is a significant challenge in practical\nscenarios. In specific domains like industrial smoke, acquiring such detailed\nannotations is particularly difficult and often requires expert knowledge. To\nalleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a\npromising approach. However, due to the supervision gap and inherent bias in\nmodels trained with only image level labels, existing WSSS methods suffer from\nlimitations such as incomplete foreground coverage, inaccurate object\nboundaries, and spurious correlations, especially in our domain, where\nemissions are always spatially coupled with chimneys.\n  Previous solutions typically rely on additional priors or external knowledge\nto mitigate these issues, but they often lack scalability and fail to address\nthe model's inherent bias toward co-occurring context. To address this, we\npropose a novel WSSS framework that directly targets the co-occurrence problem\nwithout relying on external supervision. Unlike prior methods that adopt a\nsingle network, we employ a teacher-student framework that combines CNNs and\nViTs. We introduce a knowledge transfer loss that enforces cross-architecture\nconsistency by aligning internal representations. Additionally, we incorporate\npost-processing techniques to address partial coverage and further improve\npseudo mask quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21959v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22797", "title": "Helmholtz boundary integral methods and the pollution effect", "authors": ["Jeffrey Galkowski", "Manas Rachh", "Euan A. Spence"], "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22797v1", "summary": "This paper is concerned with solving the Helmholtz exterior Dirichlet and\nNeumann problems with large wavenumber $k$ and smooth obstacles using the\nstandard second-kind boundary integral equations (BIEs) for these problems. We\nconsider Galerkin and collocation methods - with subspaces consisting of\n$\\textit{either}$ piecewise polynomials (in 2-d for collocation, in any\ndimension for Galerkin) $\\textit{or}$ trigonometric polynomials (in 2-d) - as\nwell as a fully discrete quadrature (a.k.a., Nystr\\\"om) method based on\ntrigonometric polynomials (in 2-d).\n  For each of these methods, we address the fundamental question: how quickly\nmust $N$, the dimension of the approximation space, grow with $k$ to maintain\naccuracy as $k\\to\\infty$? For the methods involving piecewise-polynomials, we\ngive sufficient conditions for $k$-uniform quasi-optimality. For the Galerkin\nmethod we prove that these are, in fact, necessary and sufficient. In\nparticular, we prove that, when applied to the Neumann BIEs when the obstacle\nis a ball, the Galerkin method $\\textit{suffers from the pollution effect}$;\ni.e., $N$ growing like $k^{d-1}$ is not sufficient for $k$-uniform\nquasi-optimality. For the Dirichlet BIEs, we prove that pollution occurs for\nthe ball for certain choices of coupling parameter, and we give numerical\nexperiments illustrating pollution for trapping domains with the standard\ncoupling parameter. For all the methods involving trigonometric polynomials, we\nshow that, up to potential factors of $k^\\varepsilon$ for any $\\varepsilon>0$,\nthese methods do not suffer from the pollution effect (even for trapping\nobstacles).\n  These are the first results about $k$-explicit convergence of collocation or\nNystr\\\"om methods applied to the Dirichlet BIEs, and the first results about\n$k$-explicit convergence of any method used to solve the standard second-kind\nNeumann BIEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22797v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22612", "title": "Adaptive Duration Model for Text Speech Alignment", "authors": ["Junjie Cao"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      4 pages, 3 figures, 2 tables", "url": "http://arxiv.org/abs/2507.22612v1", "summary": "Speech-to-text alignment is a critical component of neural text to-speech\n(TTS) models. Autoregressive TTS models typically use an attention mechanism to\nlearn these alignments on-line. However, these alignments tend to be brittle\nand often fail to generalize to long utterances and out-of-domain text, leading\nto missing or repeating words. Most non-autoregressive end to-end TTS models\nrely on durations extracted from external sources, using additional duration\nmodels for alignment. In this paper, we propose a novel duration prediction\nframework that can give compromising phoneme-level duration distribution with\ngiven text. In our experiments, the proposed duration model has more precise\nprediction and condition adaptation ability compared to previous baseline\nmodels. Numerically, it has roughly a 11.3 percents immprovement on alignment\naccuracy, and makes the performance of zero-shot TTS models more robust to the\nmismatch between prompt audio and input audio.", "comment": "4 pages, 3 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.22612v1", "cate": "cs.SD", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21395", "title": "Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion", "authors": ["Zeyu Deng", "Yanhui Lu", "Jiashu Liao", "Shuang Wu", "Chongfeng Wei"], "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21395v1", "summary": "Multimodal emotion recognition (MER) is crucial for enabling emotionally\nintelligent systems that perceive and respond to human emotions. However,\nexisting methods suffer from limited cross-modal interaction and imbalanced\ncontributions across modalities. To address these issues, we propose Sync-TVA,\nan end-to-end graph-attention framework featuring modality-specific dynamic\nenhancement and structured cross-modal fusion. Our design incorporates a\ndynamic enhancement module for each modality and constructs heterogeneous\ncross-modal graphs to model semantic relations across text, audio, and visual\nfeatures. A cross-attention fusion mechanism further aligns multimodal cues for\nrobust emotion inference. Experiments on MELD and IEMOCAP demonstrate\nconsistent improvements over state-of-the-art models in both accuracy and\nweighted F1 score, especially under class-imbalanced conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21395v1", "cate": "cs.MM", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21434", "title": "Measuring Sample Quality with Copula Discrepancies", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 65C05, 68T05"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21434v1", "summary": "The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern\nBayesian machine learning, such as Stochastic Gradient Langevin Dynamics\n(SGLD), sacrifice asymptotic exactness for computational speed, creating a\ncritical diagnostic gap: traditional sample quality measures fail\ncatastrophically when applied to biased samplers. While powerful Stein-based\ndiagnostics can detect distributional mismatches, they provide no direct\nassessment of dependence structure, often the primary inferential target in\nmultivariate problems. We introduce the Copula Discrepancy (CD), a principled\nand computationally efficient diagnostic that leverages Sklar's theorem to\nisolate and quantify the fidelity of a sample's dependence structure\nindependent of its marginals. Our theoretical framework provides the first\nstructure-aware diagnostic specifically designed for the era of approximate\ninference. Empirically, we demonstrate that a moment-based CD dramatically\noutperforms standard diagnostics like effective sample size for hyperparameter\nselection in biased MCMC, correctly identifying optimal configurations where\ntraditional methods fail. Furthermore, our robust MLE-based variant can detect\nsubtle but critical mismatches in tail dependence that remain invisible to rank\ncorrelation-based approaches, distinguishing between samples with identical\nKendall's tau but fundamentally different extreme-event behavior. With\ncomputational overhead orders of magnitude lower than existing Stein\ndiscrepancies, the CD provides both immediate practical value for MCMC\npractitioners and a theoretical foundation for the next generation of\nstructure-aware sample quality assessment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21434v1", "cate": "stat.ML", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21960", "title": "PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction", "authors": ["Jiahui Ren", "Mochu Xiang", "Jiajun Zhu", "Yuchao Dai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.21960v1", "summary": "Wide-baseline panorama reconstruction has emerged as a highly effective and\npivotal approach for not only achieving geometric reconstruction of the\nsurrounding 3D environment, but also generating highly realistic and immersive\nnovel views. Although existing methods have shown remarkable performance across\nvarious benchmarks, they are predominantly reliant on accurate pose\ninformation. In real-world scenarios, the acquisition of precise pose often\nrequires additional computational resources and is highly susceptible to noise.\nThese limitations hinder the broad applicability and practicality of such\nmethods. In this paper, we present PanoSplatt3R, an unposed wide-baseline\npanorama reconstruction method. We extend and adapt the foundational\nreconstruction pretrainings from the perspective domain to the panoramic\ndomain, thus enabling powerful generalization capabilities. To ensure a\nseamless and efficient domain-transfer process, we introduce RoPE rolling that\nspans rolled coordinates in rotary positional embeddings across different\nattention heads, maintaining a minimal modification to RoPE's mechanism, while\nmodeling the horizontal periodicity of panorama images. Comprehensive\nexperiments demonstrate that PanoSplatt3R, even in the absence of pose\ninformation, significantly outperforms current state-of-the-art methods. This\nsuperiority is evident in both the generation of high-quality novel views and\nthe accuracy of depth estimation, thereby showcasing its great potential for\npractical applications. Project page: https://npucvr.github.io/PanoSplatt3R", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.21960v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22818", "title": "Numerical Methods for Solving Nonlinearly Coupled Poisson Equations in Dual-Continuum Modeled Porous Electrodes", "authors": ["Yuhe Wang", "Min Wang", "Zhihang Xu"], "categories": ["math.NA", "cs.NA", "math-ph", "math.MP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22818v1", "summary": "Porous electrodes are widely used in electrochemical systems, where\naccurately determining electric potentials, particularly overpotentials, is\nessential for understanding electrode behavior. At the macroscopic scale,\nporous electrodes are typically modeled using a dual-continuum approach,\ntreating the porous solid phase and the liquid electrolyte as spatially\nsuperimposed domains. Determining potential distributions requires solving two\nPoisson equations that are nonlinearly coupled through Butler-Volmer kinetics\nunder galvanostatic and potentiostatic operating modes. Under galvanostatic\noperation, these equations form an underconstrained singular system due to\nall-Neumann boundary conditions, posing numerical challenges. This paper\nsystematically presents numerical methods for solving nonlinearly coupled\nPoisson equations in dual-continuum porous electrodes, with a particular focus\non galvanostatic solutions. We mathematically establish solution uniqueness in\nterms of the potential difference between the electrode and electrolyte (or\noverpotential), as well as the individual potentials up to a shared constant\nshift. To resolve the nonuniqueness of the solution, we introduce three\nnumerical approaches: (1) Lagrange Constrained Method (LCM), (2) Dirichlet\nSubstitution Method (DSM), and (3) Global Constraining Method (GCM), where GCM\nenables solving the overpotential without imposing an explicit system reference\npotential. Additionally, we develop both decoupled and fully coupled nonlinear\nsolution strategies and evaluate their computational performance in both\nhomogeneous and heterogeneous conductivity cases. The presented numerical\nmethods are general for addressing similar underconstrained nonlinear systems.\nA Python implementation is provided at\nhttps://github.com/harrywang1129/porous_electrode_solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22818v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22746", "title": "Next Tokens Denoising for Speech Synthesis", "authors": ["Yanqing Liu", "Ruiqing Xue", "Chong Zhang", "Yufei Liu", "Gang Wang", "Bohan Li", "Yao Qian", "Lei He", "Shujie Liu", "Sheng Zhao"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22746v1", "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22746v1", "cate": "cs.SD", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21432", "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour", "authors": ["Tareq Alsaleh", "Bilal Farooq"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21432v1", "summary": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven LLMs (1-12B parameters) across three stated\nand revealed preference datasets, testing 396 configurations and generating\nover 79,000 synthetic commuter predictions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized, explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21432v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21449", "title": "From Global to Local: A Scalable Benchmark for Local Posterior Sampling", "authors": ["Rohan Hitchcock", "Jesse Hoogland"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      25 pages", "url": "http://arxiv.org/abs/2507.21449v1", "summary": "Degeneracy is an inherent feature of the loss landscape of neural networks,\nbut it is not well understood how stochastic gradient MCMC (SGMCMC) algorithms\ninteract with this degeneracy. In particular, current global convergence\nguarantees for common SGMCMC algorithms rely on assumptions which are likely\nincompatible with degenerate loss landscapes. In this paper, we argue that this\ngap requires a shift in focus from global to local posterior sampling, and, as\na first step, we introduce a novel scalable benchmark for evaluating the local\nsampling performance of SGMCMC algorithms. We evaluate a number of common\nalgorithms, and find that RMSProp-preconditioned SGLD is most effective at\nfaithfully representing the local geometry of the posterior distribution.\nAlthough we lack theoretical guarantees about global sampler convergence, our\nempirical results show that we are able to extract non-trivial local\ninformation in models with up to O(100M) parameters.", "comment": "25 pages", "pdf_url": "http://arxiv.org/pdf/2507.21449v1", "cate": "stat.ML", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21968", "title": "A Deep Learning Pipeline Using Synthetic Data to Improve Interpretation of Paper ECG Images", "authors": ["Xiaoyu Wang", "Ramesh Nadarajah", "Zhiqiang Zhang", "David Wong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21968v1", "summary": "Cardiovascular diseases (CVDs) are the leading global cause of death, and\nearly detection is essential to improve patient outcomes. Electrocardiograms\n(ECGs), especially 12-lead ECGs, play a key role in the identification of CVDs.\nThese are routinely interpreted by human experts, a process that is\ntime-consuming and requires expert knowledge. Historical research in this area\nhas focused on automatic ECG interpretation from digital signals, with recent\ndeep learning approaches achieving strong results. In practice, however, most\nECG data in clinical practice are stored or shared in image form. To bridge\nthis gap, we propose a deep learning framework designed specifically to\nclassify paper-like ECG images into five main diagnostic categories. Our method\nwas the winning entry to the 2024 British Heart Foundation Open Data Science\nChallenge. It addresses two main challenges of paper ECG classification: visual\nnoise (e.g., shadows or creases) and the need to detect fine-detailed waveform\npatterns. We propose a pre-processing pipeline that reduces visual noise and a\ntwo-stage fine-tuning strategy: the model is first fine-tuned on synthetic and\nexternal ECG image datasets to learn domain-specific features, and then further\nfine-tuned on the target dataset to enhance disease-specific recognition. We\nadopt the ConvNeXt architecture as the backbone of our model. Our method\nachieved AUROC scores of 0.9688 on the public validation set and 0.9677 on the\nprivate test set of the British Heart Foundation Open Data Science Challenge,\nhighlighting its potential as a practical tool for automated ECG interpretation\nin clinical workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21968v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22850", "title": "Dynamic analysis of free-free Timoshenko beams on elastic foundation under transverse transient ground deformation", "authors": ["Gersena Banushi"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Key words: buried Timoshenko beam, semi-analytical model, dynamic amplification, transient ground deformation (TGD), modal analysis", "url": "http://arxiv.org/abs/2507.22850v1", "summary": "Underground infrastructure, such as pipelines and tunnels, can be vulnerable\nto the effect of transient ground deformation (TGD) caused by different\nvibration sources, including earthquakes and traffic loads. Current design\nmethods are based on simple analytical models that idealize the soil movement\nas a traveling sinusoidal wave, neglecting both the system's inertia and the\nrelative displacement at the soil-structure interface. However, this assumption\nmay not be valid for buried large diameter pipelines and tunnels requiring\naccurate dynamic analysis. To analyse the dynamic response of a buried straight\nbeam subjected to transverse TGD, this study develops a new semi-analytical\nmodel based on the Timoshenko beam on Winkler foundation theory. The\nclosed-form analytical solution revealed that the vibration spectrum is divided\nin four parts, separated by three transition frequencies. Across each\ntransition frequency, the oscillatory characteristics of the vibration modes\nchange, significantly affecting the dynamic response of the system. To verify\nthe validity of the proposed model, this work analyses the case study of a\nburied steel water pipeline of varying lengths and operating conditions,\nsubjected to transverse TGD. Comparison of the obtained analytical solutions\nwith the finite element analysis results showed excellent agreement between the\ntwo approaches. The frequency response analysis revealed dynamic amplification\nof the soil-structure interaction for forcing frequencies near the system's\nfundamental frequency. These may fall within the range of dominant frequencies\ncharacterizing seismic waves, requiring accurate dynamic analysis. The proposed\nmethodology provides a robust analytical framework for evaluating the primary\nfactors impacting the dynamic behavior of buried beams, giving a deeper\nunderstanding of the system response under various sources of ground vibration.", "comment": "Key words: buried Timoshenko beam, semi-analytical model, dynamic\n  amplification, transient ground deformation (TGD), modal analysis", "pdf_url": "http://arxiv.org/pdf/2507.22850v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22370", "title": "Prediction of acoustic field in 1-D uniform duct with varying mean flow and temperature using neural networks", "authors": ["D. Veerababu", "Prasanta K. Ghosh"], "categories": ["cs.LG", "cs.SD", "eess.AS", "34A06", "G.1.6; I.6.4; J.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages", "url": "http://arxiv.org/abs/2507.22370v1", "summary": "Neural networks constrained by the physical laws emerged as an alternate\nnumerical tool. In this paper, the governing equation that represents the\npropagation of sound inside a one-dimensional duct carrying a heterogeneous\nmedium is derived. The problem is converted into an unconstrained optimization\nproblem and solved using neural networks. Both the acoustic state variables:\nacoustic pressure and particle velocity are predicted and validated with the\ntraditional Runge-Kutta solver. The effect of the temperature gradient on the\nacoustic field is studied. Utilization of machine learning techniques such as\ntransfer learning and automatic differentiation for acoustic applications is\ndemonstrated.", "comment": "22 pages", "pdf_url": "http://arxiv.org/pdf/2507.22370v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21474", "title": "Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning", "authors": ["Daniel Szelogowski"], "categories": ["cs.NE", "cs.AI", "cs.IR", "cs.LG", "q-bio.NC", "I.2.0; I.2.4; I.2.6; I.2.m; E.1; E.2; E.4; H.3; J.3; J.4"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures, 4 tables", "url": "http://arxiv.org/abs/2507.21474v1", "summary": "Despite success across diverse tasks, current artificial recurrent network\narchitectures rely primarily on implicit hidden-state memories, limiting their\ninterpretability and ability to model long-range dependencies. In contrast,\nbiological neural systems employ explicit, associative memory traces (i.e.,\nengrams) strengthened through Hebbian synaptic plasticity and activated\nsparsely during recall. Motivated by these neurobiological insights, we\nintroduce the Engram Neural Network (ENN), a novel recurrent architecture\nincorporating an explicit, differentiable memory matrix with Hebbian plasticity\nand sparse, attention-driven retrieval mechanisms. The ENN explicitly models\nmemory formation and recall through dynamic Hebbian traces, improving\ntransparency and interpretability compared to conventional RNN variants. We\nevaluate the ENN architecture on three canonical benchmarks: MNIST digit\nclassification, CIFAR-10 image sequence modeling, and WikiText-103 language\nmodeling. Our empirical results demonstrate that the ENN achieves accuracy and\ngeneralization performance broadly comparable to classical RNN, GRU, and LSTM\narchitectures, with all models converging to similar accuracy and perplexity on\nthe large-scale WikiText-103 task. At the same time, the ENN offers significant\nenhancements in interpretability through observable memory dynamics. Hebbian\ntrace visualizations further reveal biologically plausible, structured memory\nformation processes, validating the potential of neuroscience-inspired\nmechanisms to inform the development of more interpretable and robust deep\nlearning models.", "comment": "20 pages, 11 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.21474v1", "cate": "cs.NE", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21486", "title": "Stochastic forest transition model dynamics and parameter estimation via deep learning", "authors": ["Satoshi Kumabe", "Tianyu Song", "Ton Viet Ta"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21486v1", "summary": "Forest transitions, characterized by dynamic shifts between forest,\nagricultural, and abandoned lands, are complex phenomena. This study developed\na stochastic differential equation model to capture the intricate dynamics of\nthese transitions. We established the existence of global positive solutions\nfor the model and conducted numerical analyses to assess the impact of model\nparameters on deforestation incentives. To address the challenge of parameter\nestimation, we proposed a novel deep learning approach that estimates all model\nparameters from a single sample containing time-series observations of forest\nand agricultural land proportions. This innovative approach enables us to\nunderstand forest transition dynamics and deforestation trends at any future\ntime.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21486v1", "cate": "stat.ML", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21971", "title": "EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation", "authors": ["Zhijiang Li", "Haoran He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21971v1", "summary": "Event-based semantic segmentation explores the potential of event cameras,\nwhich offer high dynamic range and fine temporal resolution, to achieve robust\nscene understanding in challenging environments. Despite these advantages, the\ntask remains difficult due to two main challenges: extracting reliable features\nfrom sparse and noisy event streams, and effectively fusing them with dense,\nsemantically rich image data that differ in structure and representation. To\naddress these issues, we propose EIFNet, a multi-modal fusion network that\ncombines the strengths of both event and frame-based inputs. The network\nincludes an Adaptive Event Feature Refinement Module (AEFRM), which improves\nevent representations through multi-scale activity modeling and spatial\nattention. In addition, we introduce a Modality-Adaptive Recalibration Module\n(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and\nintegrate features across modalities using attention mechanisms and gated\nfusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets\nshow that EIFNet achieves state-of-the-art performance, demonstrating its\neffectiveness in event-based semantic segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21971v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22875", "title": "Numerical Fredholm determinants for matrix-valued kernels on the real line", "authors": ["Erika Gallo", "John Zweck", "Yuri Latushkin"], "categories": ["math.NA", "cs.NA", "math.FA", "math.SP", "65R20, 65F40 (Primary) 47G10 (Secondary)"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      29 pages, 2 figures", "url": "http://arxiv.org/abs/2507.22875v1", "summary": "We analyze a numerical method for computing Fredholm determinants of trace\nclass and Hilbert Schmidt integral operators defined in terms of matrix-valued\nkernels on the entire real line. With this method, the Fredholm determinant is\napproximated by the determinant of a matrix constructed by truncating the\nkernel of the operator to a finite interval and then applying a quadrature\nrule. Under the assumption that the kernel decays exponentially, we derive an\nestimate relating the Fredholm determinant of the operator on the real line to\nthat of its truncation to a finite interval. Then we derive a quadrature error\nestimate relating the Fredholm determinant of a matrix-valued kernel on a\nfinite interval to its numerical approximation obtained via an adaptive\ncomposite Simpson's quadrature rule. These results extend the analysis of\nBornemann which focused on Fredholm determinants of trace class operators\ndefined by scalar-valued kernels on a finite interval. Numerical results are\nprovided for a Birman-Schwinger operator that characterizes the stability of\nstationary solutions of nonlinear wave equations.", "comment": "29 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.22875v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22628", "title": "A k-space approach to modeling multi-channel parametric array loudspeaker systems", "authors": ["Tao Zhuang", "Longbiao He", "Feng Niu", "Jia-Xin Zhong", "Jing Lu"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22628v1", "summary": "Multi-channel parametric array loudspeaker (MCPAL) systems offer enhanced\nflexibility and promise for generating highly directional audio beams in\nreal-world applications. However, efficient and accurate prediction of their\ngenerated sound fields remains a major challenge due to the complex nonlinear\nbehavior and multi-channel signal processing involved. To overcome this\nobstacle, we propose a k-space approach for modeling arbitrary MCPAL systems\narranged on a baffled planar surface. In our method, the linear ultrasound\nfield is first solved using the angular spectrum approach, and the quasilinear\naudio sound field is subsequently computed efficiently in k-space. By\nleveraging three-dimensional fast Fourier transforms, our approach not only\nachieves high computational and memory efficiency but also maintains accuracy\nwithout relying on the paraxial approximation. For typical configurations\nstudied, the proposed method demonstrates a speed-up of more than four orders\nof magnitude compared to the direct integration method. Our proposed approach\npaved the way for simulating and designing advanced MCPAL systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22628v1", "cate": "eess.AS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22221", "title": "A Customized Memory-aware Architecture for Biological Sequence Alignment", "authors": ["Nasrin Akbari", "Mehdi Modarressi", "Alireza Khadem"], "categories": ["cs.AR", "cs.ET", "n/a", "C.3"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures", "url": "http://arxiv.org/abs/2507.22221v1", "summary": "Sequence alignment is a fundamental process in computational biology which\nidentifies regions of similarity in biological sequences. With the exponential\ngrowth in the volume of data in bioinformatics databases, the time, processing\npower, and memory bandwidth for comparing a query sequence with the available\ndatabases grows proportionally. The sequence alignment algorithms often involve\nsimple arithmetic operations and feature high degrees of inherent fine-grained\nand coarse-grained parallelism. These features can be potentially exploited by\na massive parallel processor, such as a GPU, to increase throughput. In this\npaper, we show that the excessive memory bandwidth demand of the sequence\nalignment algorithms prevents exploiting the maximum achievable throughput on\nconventional parallel machines. We then propose a memory-aware architecture to\nreduce the bandwidth demand of the sequence alignment algorithms, effectively\npushing the memory wall to extract higher throughput. The design is integrated\nat the logic layer of an emerging 3D DRAM as a processing-in-memory\narchitecture to further increase the available bandwidth. The experimental\nresults show that the proposed architecture results in up to 2.4x speedup over\na GPU-based design. Moreover, by moving the computation closer to the memory,\npower consumption is reduced by 37%, on average.", "comment": "20 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.22221v1", "cate": "cs.AR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21476", "title": "Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench", "authors": ["Reuben Narad", "Siddharth Suresh", "Jiayi Chen", "Pine S. L. Dysart-Bricken", "Bob Mankoff", "Robert Nowak", "Jifan Zhang", "Lalit Jain"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21476v1", "summary": "We present HumorBench, a benchmark designed to evaluate large language\nmodels' (LLMs) ability to reason about and explain sophisticated humor in\ncartoon captions. As reasoning models increasingly saturate existing benchmarks\nin mathematics and science, novel and challenging evaluations of model\nintelligence beyond STEM domains are essential. Reasoning is fundamentally\ninvolved in text-based humor comprehension, requiring the identification of\nconnections between concepts in cartoons/captions and external cultural\nreferences, wordplays, and other mechanisms. HumorBench includes approximately\n300 unique cartoon-caption pairs from the New Yorker Caption Contest and\nCartoonstock.com, with expert-annotated evaluation rubrics identifying\nessential joke elements. LLMs are evaluated based on their explanations towards\nthe humor and abilities in identifying the joke elements. To perform well on\nthis task, models must form and test hypotheses about associations between\nconcepts, potentially backtracking from initial interpretations to arrive at\nthe most plausible explanation. Our extensive benchmarking of current SOTA\nmodels reveals three key insights: (1) LLM progress on STEM reasoning transfers\neffectively to humor comprehension; (2) models trained exclusively on STEM\nreasoning data still perform well on HumorBench, demonstrating strong\ntransferability of reasoning abilities; and (3) test-time scaling by increasing\nthinking token budgets yields mixed results across different models in humor\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21476v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21509", "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language Models", "authors": ["Runjin Chen", "Andy Arditi", "Henry Sleight", "Owain Evans", "Jack Lindsey"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21509v1", "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21509v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21977", "title": "Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition", "authors": ["Jihao Gu", "Kun Li", "Fei Wang", "Yanyan Wei", "Zhiliang Wu", "Hehe Fan", "Meng Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21977v1", "summary": "Micro-Actions (MAs) are an important form of non-verbal communication in\nsocial interactions, with potential applications in human emotional analysis.\nHowever, existing methods in Micro-Action Recognition often overlook the\ninherent subtle changes in MAs, which limits the accuracy of distinguishing MAs\nwith subtle changes. To address this issue, we present a novel Motion-guided\nModulation Network (MMN) that implicitly captures and modulates subtle motion\ncues to enhance spatial-temporal representation learning. Specifically, we\nintroduce a Motion-guided Skeletal Modulation module (MSM) to inject motion\ncues at the skeletal level, acting as a control signal to guide spatial\nrepresentation modeling. In parallel, we design a Motion-guided Temporal\nModulation module (MTM) to incorporate motion information at the frame level,\nfacilitating the modeling of holistic motion patterns in micro-actions.\nFinally, we propose a motion consistency learning strategy to aggregate the\nmotion cues from multi-scale features for micro-action classification.\nExperimental results on the Micro-Action 52 and iMiGUE datasets demonstrate\nthat MMN achieves state-of-the-art performance in skeleton-based micro-action\nrecognition, underscoring the importance of explicitly modeling subtle motion\ncues. The code will be available at https://github.com/momiji-bit/MMN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21977v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22570", "title": "Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity", "authors": ["Leandro Farina", "Sergey Korotov"], "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "15B48, 68T07, 15A18, 62G32", "I.2.6; I.5.2; G.1.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 11 figures. To be submitted to a journal", "url": "http://arxiv.org/abs/2507.22570v1", "summary": "This work demonstrates a methodology for using deep learning to discover\nsimple, practical criteria for classifying matrices based on abstract algebraic\nproperties. By combining a high-performance neural network with explainable AI\n(XAI) techniques, we can distill a model's learned strategy into\nhuman-interpretable rules. We apply this approach to the challenging case of\nmonotone matrices, defined by the condition that their inverses are entrywise\nnonnegative. Despite their simple definition, an easy characterization in terms\nof the matrix elements or the derived parameters is not known. Here, we\npresent, to the best of our knowledge, the first systematic machine-learning\napproach for deriving a practical criterion that distinguishes monotone from\nnon-monotone matrices. After establishing a labelled dataset by randomly\ngenerated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ\ndeep neural network algorithms for classifying the matrices as monotone or\nnon-monotone, using both their entries and a comprehensive set of matrix\nfeatures. By saliency methods, such as integrated gradients, we identify among\nall features, two matrix parameters which alone provide sufficient information\nfor the matrix classification, with $95\\%$ accuracy, namely the absolute values\nof the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's\ncharacteristic polynomial. A data-driven study of 18,000 random $7\\times7$\nmatrices shows that the monotone class obeys $\\lvert c_{0}/c_{1}\\rvert\\le0.18$\nwith probability $>99.98\\%$; because $\\lvert c_{0}/c_{1}\\rvert =\n1/\\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound\n$\\mathrm{tr}(A^{-1})\\ge5.7$.", "comment": "22 pages, 11 figures. To be submitted to a journal", "pdf_url": "http://arxiv.org/pdf/2507.22570v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2406.05515", "title": "Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation", "authors": ["Paige Tuttösí", "H. Henny Yeung", "Yue Wang", "Fenqi Wang", "Guillaume Denis", "Jean-Julien Aucouturier", "Angelica Lim"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to INTERSPEECH 2024 Project Webpage : this https URL Code: this https URL Data : this https URL", "url": "http://arxiv.org/abs/2406.05515v2", "summary": "Acoustic context effects, where surrounding changes in pitch, rate or timbre\ninfluence the perception of a sound, are well documented in speech perception,\nbut how they interact with language background remains unclear. Using a\nreverse-correlation approach, we systematically varied the pitch and speech\nrate in phrases around different pairs of vowels for second language (L2)\nspeakers of English (/i/-/I/) and French (/u/-/y/), thus reconstructing, in a\ndata-driven manner, the prosodic profiles that bias their perception. Testing\nEnglish and French speakers (n=25), we showed that vowel perception is in fact\ninfluenced by conflicting effects from the surrounding pitch and speech rate: a\ncongruent proximal effect 0.2s pre-target and a distal contrastive effect up to\n1s before; and found that L1 and L2 speakers exhibited strikingly similar\nprosodic profiles in perception. We provide a novel method to investigate\nacoustic context effects across stimuli, timescales, and acoustic domain.", "comment": "Accepted to INTERSPEECH 2024 Project Webpage :\n  https://rosielab.github.io/vocal_ambiguity/ Code:\n  https://github.com/neuro-team-femto/vocal_ambiguity Data :\n  https://zenodo.org/records/12761242", "pdf_url": "http://arxiv.org/pdf/2406.05515v2", "cate": "cs.SD", "date": "2024-06-08", "updated": "2025-07-30"}
{"id": "2507.21572", "title": "No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for Real-time Rendering", "authors": ["Linye Wei", "Jiajun Tang", "Fan Fei", "Boxin Shi", "Runsheng Wang", "Meng Li"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted by International Conference on Computer-Aided Design (ICCAD) 2025", "url": "http://arxiv.org/abs/2507.21572v2", "summary": "3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and\nis getting increasing adoption in domains like autonomous driving and embodied\nintelligence. However, 3DGS still faces major efficiency challenges when faced\nwith high frame rate requirements and resource-constrained edge deployment. To\nenable efficient 3DGS, in this paper, we propose LS-Gaussian, an\nalgorithm/hardware co-design framework for lightweight streaming 3D rendering.\nLS-Gaussian is motivated by the core observation that 3DGS suffers from\nsubstantial computation redundancy and stalls. On one hand, in practical\nscenarios, high-frame-rate 3DGS is often applied in settings where a camera\nobserves and renders the same scene continuously but from slightly different\nviewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian\nproposes a viewpoint transformation algorithm that leverages inter-frame\ncontinuity for efficient sparse rendering. On the other hand, as different\ntiles within an image are rendered in parallel but have imbalanced workloads,\nfrequent hardware stalls also slow down the rendering process. LS-Gaussian\npredicts the workload for each tile based on viewpoint transformation to enable\nmore balanced parallel computation and co-designs a customized 3DGS accelerator\nto support the workload-aware mapping in real-time. Experimental results\ndemonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline\non average and up to 17.3x speedup with the customized accelerator, while\nincurring only minimal visual quality degradation.", "comment": "Accepted by International Conference on Computer-Aided Design (ICCAD)\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.21572v2", "cate": "cs.AR", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.21482", "title": "Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs", "authors": ["Abhinav Arabelly", "Jagrut Nemade", "Robert D Nowak", "Jifan Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21482v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, but developing high-performing models for specialized\napplications often requires substantial human annotation -- a process that is\ntime-consuming, labor-intensive, and expensive. In this paper, we address the\nlabel-efficient learning problem for supervised finetuning (SFT) by leveraging\ntask-diversity as a fundamental principle for effective data selection. This is\nmarkedly different from existing methods based on the prompt-diversity. Our\napproach is based on two key observations: 1) task labels for different prompts\nare often readily available; 2) pre-trained models have significantly varying\nlevels of confidence across tasks. We combine these facts to devise a simple\nyet effective sampling strategy: we select examples across tasks using an\ninverse confidence weighting strategy. This produces models comparable to or\nbetter than those trained with more complex sampling procedures, while being\nsignificantly easier to implement and less computationally intensive. Notably,\nour experimental results demonstrate that this method can achieve better\naccuracy than training on the complete dataset (a 4\\% increase in MMLU score).\nAcross various annotation budgets and two instruction finetuning datasets, our\nalgorithm consistently performs at or above the level of the best existing\nmethods, while reducing annotation costs by up to 80\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21482v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21532", "title": "Automatic Classification of User Requirements from Online Feedback -- A Replication Study", "authors": ["Meet Bhatt", "Nic Boilard", "Muhammad Rehan Chaudhary", "Cole Thompson", "Jacob Idoko", "Aakash Sorathiya", "Gouri Ginde"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures, Replication package available at this https URL , Accepted at AIRE 2025 (12th International Workshop on Artificial Intelligence and Requirements Engineering)", "url": "http://arxiv.org/abs/2507.21532v1", "summary": "Natural language processing (NLP) techniques have been widely applied in the\nrequirements engineering (RE) field to support tasks such as classification and\nambiguity detection. Although RE research is rooted in empirical investigation,\nit has paid limited attention to replicating NLP for RE (NLP4RE) studies. The\nrapidly advancing realm of NLP is creating new opportunities for efficient,\nmachine-assisted workflows, which can bring new perspectives and results to the\nforefront. Thus, we replicate and extend a previous NLP4RE study (baseline),\n\"Classifying User Requirements from Online Feedback in Small Dataset\nEnvironments using Deep Learning\", which evaluated different deep learning\nmodels for requirement classification from user reviews. We reproduced the\noriginal results using publicly released source code, thereby helping to\nstrengthen the external validity of the baseline study. We then extended the\nsetup by evaluating model performance on an external dataset and comparing\nresults to a GPT-4o zero-shot classifier. Furthermore, we prepared the\nreplication study ID-card for the baseline study, important for evaluating\nreplication readiness. Results showed diverse reproducibility levels across\ndifferent models, with Naive Bayes demonstrating perfect reproducibility. In\ncontrast, BERT and other models showed mixed results. Our findings revealed\nthat baseline deep learning models, BERT and ELMo, exhibited good\ngeneralization capabilities on an external dataset, and GPT-4o showed\nperformance comparable to traditional baseline machine learning models.\nAdditionally, our assessment confirmed the baseline study's replication\nreadiness; however missing environment setup files would have further enhanced\nreadiness. We include this missing information in our replication package and\nprovide the replication study ID-card for our study to further encourage and\nsupport the replication of our study.", "comment": "10 pages, 3 figures, Replication package available at\n  https://zenodo.org/records/15626782, Accepted at AIRE 2025 (12th\n  International Workshop on Artificial Intelligence and Requirements\n  Engineering)", "pdf_url": "http://arxiv.org/pdf/2507.21532v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22000", "title": "Staining and locking computer vision models without retraining", "authors": ["Oliver J. Sutton", "Qinghua Zhou", "George Leete", "Alexander N. Gorban", "Ivan Y. Tyukin"], "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 68T45, 68W40", "I.2.10; F.2.0; K.5.1; K.6.5"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 9 pages of appendices, 10 figures", "url": "http://arxiv.org/abs/2507.22000v1", "summary": "We introduce new methods of staining and locking computer vision models, to\nprotect their owners' intellectual property. Staining, also known as\nwatermarking, embeds secret behaviour into a model which can later be used to\nidentify it, while locking aims to make a model unusable unless a secret\ntrigger is inserted into input images. Unlike existing methods, our algorithms\ncan be used to stain and lock pre-trained models without requiring fine-tuning\nor retraining, and come with provable, computable guarantees bounding their\nworst-case false positive rates. The stain and lock are implemented by directly\nmodifying a small number of the model's weights and have minimal impact on the\n(unlocked) model's performance. Locked models are unlocked by inserting a small\n`trigger patch' into the corner of the input image. We present experimental\nresults showing the efficacy of our methods and demonstrating their practical\nperformance on a variety of computer vision models.", "comment": "10 pages, 9 pages of appendices, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.22000v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2410.12421", "title": "The eigenvalue decomposition of normal matrices by the skew-symmetric part", "authors": ["Simon Mataigne", "Kyle A. Gallivan"], "categories": ["math.NA", "cs.NA", "65F15, 15B10, 15B57, 62R30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      45 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2410.12421v3", "summary": "We propose a new method for computing the eigenvalue decomposition of a dense\nreal normal matrix $A$ through the decomposition of its skew-symmetric part.\nThe method relies on algorithms that are known to be efficiently implemented,\nsuch as the bidiagonal singular value decomposition and the symmetric\neigenvalue decomposition. The advantages of this method stand for normal\nmatrices with few real eigenvalues, such as orthogonal matrices. We provide a\nstability and a complexity analysis of the method. The numerical performance is\ncompared with existing algorithms. In most cases, the method has the same\noperation count as the Hessenberg factorization of a dense matrix. Finally, we\nprovide experiments for the application of computing a Riemannian barycenter on\nthe special orthogonal group.", "comment": "45 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2410.12421v3", "cate": "math.NA", "date": "2024-10-16", "updated": "2025-07-30"}
{"id": "2507.09342", "title": "BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus", "authors": ["Emmanuel Adetiba", "Abdultaofeek Abayomi", "Raymond J. Kala", "Ayodele H. Ifijeh", "Oluwatobi E. Dare", "Olabode Idowu-Bismark", "Gabriel O. Sobola", "Joy N. Adetiba", "Monsurat Adepeju Lateef", "Heather Cole-Lewis"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09342v3", "summary": "There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for\nhigh resource-to-low resource language pairs such as English-to-Yoruba. Thus,\nin this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech\nTranslation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a\nhybrid architecture we developed for large-scale direct S2ST corpus creation at\nreduced cost. To achieve this, we leveraged non speech-to-speech Standard\nYoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as\nthe corresponding Standard English (SE) transcripts. YORULECT Corpus is small\nscale(1,504) samples, and it does not have paired English audios. Therefore, we\ngenerated the SE audios using pre-trained AI models (i.e. Facebook MMS). We\nalso developed an audio augmentation algorithm named AcoustAug based on three\nlatent acoustic features to generate augmented audios from the raw audios of\nthe two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,\nwhich gives a total of 24,064 sample size. The total audio duration for the two\nlanguages is 41.20 hours. This size is quite significant. Beyond building S2ST\nmodels, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve\nexisting ones. The created corpus and Coqui framework were used to build a\npretrained Yoruba TTS model (named YoruTTS-1.5) as a proof of concept. The\nYoruTTS-1.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates\nmoderate fundamental pitch similarity with the reference real-time audio.\nUltimately, the corpus architecture in this study can be leveraged by\nresearchers and developers to curate datasets for multilingual\nhigh-resource-to-low-resource African languages. This will bridge the huge\ndigital divides in translations among high and low-resource language pairs.\nBENYO-S2ST-Corpus-1 and YoruTTS-1.5 are publicly available at\n(https://bit.ly/40bGMwi).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09342v3", "cate": "cs.SD", "date": "2025-07-12", "updated": "2025-07-30"}
{"id": "2507.21500", "title": "VN-MTEB: Vietnamese Massive Text Embedding Benchmark", "authors": ["Loc Pham", "Tung Luu", "Thu Vo", "Minh Nguyen", "Viet Hoang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      19 pages (including reference, appendix) 41 datasets from 6 tasks (retrieval, classification, pair-classification, clustering, rerank, sts) 7 figures, 16 tables, benchmark 18 text embedding models", "url": "http://arxiv.org/abs/2507.21500v1", "summary": "Vietnam ranks among the top countries in terms of both internet traffic and\nonline toxicity. As a result, implementing embedding models for recommendation\nand content control duties in applications is crucial. However, a lack of\nlarge-scale test datasets, both in volume and task diversity, makes it tricky\nfor scientists to effectively evaluate AI models before deploying them in\nreal-world, large-scale projects. To solve this important problem, we introduce\na Vietnamese benchmark, VN-MTEB for embedding models, which we created by\ntranslating a large number of English samples from the Massive Text Embedding\nBenchmark using our new automated framework. We leverage the strengths of large\nlanguage models (LLMs) and cutting-edge embedding models to conduct translation\nand filtering processes to retain high-quality samples, guaranteeing a natural\nflow of language and semantic fidelity while preserving named entity\nrecognition (NER) and code snippets. Our comprehensive benchmark consists of 41\ndatasets from six tasks specifically designed for Vietnamese text embeddings.\nIn our analysis, we find that bigger and more complex models using Rotary\nPositional Embedding outperform those using Absolute Positional Embedding in\nembedding tasks. Datasets are available at HuggingFace:\nhttps://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686", "comment": "19 pages (including reference, appendix) 41 datasets from 6 tasks\n  (retrieval, classification, pair-classification, clustering, rerank, sts) 7\n  figures, 16 tables, benchmark 18 text embedding models", "pdf_url": "http://arxiv.org/pdf/2507.21500v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21563", "title": "Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation", "authors": ["Minh-Anh Nguyen", "Bao Nguyen", "Ha Lan N. T.", "Tuan Anh Hoang", "Duc-Trong Le", "Dung D. Le"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21563v1", "summary": "Recommendation systems often suffer from data sparsity caused by limited\nuser-item interactions, which degrade their performance and amplify popularity\nbias in real-world scenarios. This paper proposes a novel data augmentation\nframework that leverages Large Language Models (LLMs) and item textual\ndescriptions to enrich interaction data. By few-shot prompting LLMs multiple\ntimes to rerank items and aggregating the results via majority voting, we\ngenerate high-confidence synthetic user-item interactions, supported by\ntheoretical guarantees based on the concentration of measure. To effectively\nleverage the augmented data in the context of a graph recommendation system, we\nintegrate it into a graph contrastive learning framework to mitigate\ndistributional shift and alleviate popularity bias. Extensive experiments show\nthat our method improves accuracy and reduces popularity bias, outperforming\nstrong baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21563v1", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22002", "title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation", "authors": ["Yida Tao", "Yen-Chia Hsu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22002v1", "summary": "Industrial smoke segmentation is critical for air-quality monitoring and\nenvironmental protection but is often hampered by the high cost and scarcity of\npixel-level annotations in real-world settings. We introduce CEDANet, a\nhuman-in-the-loop, class-aware domain adaptation framework that uniquely\nintegrates weak, citizen-provided video-level labels with adversarial feature\nalignment. Specifically, we refine pseudo-labels generated by a source-trained\nsegmentation model using citizen votes, and employ class-specific domain\ndiscriminators to transfer rich source-domain representations to the industrial\ndomain. Comprehensive experiments on SMOKE5K and custom IJmond datasets\ndemonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of\n0.261 with citizen feedback, vastly outperforming the baseline model, which\nscored 0.083 and 0.043 respectively. This represents a five-fold increase in\nF1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with\ncitizen-constrained pseudo-labels achieves performance comparable to the same\narchitecture trained on limited 100 fully annotated images with F1-score of\n0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully\nsupervised-level accuracy without target-domain annotations. Our research\nvalidates the scalability and cost-efficiency of combining citizen science with\nweakly supervised domain adaptation, offering a practical solution for complex,\ndata-scarce environmental monitoring applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22002v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2501.10104", "title": "Solving Random Hyperbolic Conservation Laws Using Linear Programming", "authors": ["Shaoshuai Chu", "Michael Herty", "Maria Lukacova-Medvidova", "Yizhou Zhou"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.10104v2", "summary": "A novel structure-preserving numerical method to solve random hyperbolic\nsystems of conservation laws is presented. The method uses a concept of\ngeneralized, measure-valued solutions to random conservation laws. This yields\na linear partial differential equation with respect to the Young measure and\nallows to compute the approximation based on linear programming problems. We\nanalyze the structure-preserving properties of the derived numerical method and\ndiscuss its advantages and disadvantages. Numerical results for one-dimensional\nBurgers equation and the isentropic Euler equations and comparisons with\nstochastic collocation method illustrate the behavior of the proposed numerical\nmethod.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.10104v2", "cate": "math.NA", "date": "2025-01-17", "updated": "2025-07-29"}
{"id": "2411.02038", "title": "Addressing Representation Collapse in Vector Quantized Models with One Linear Layer", "authors": ["Yongxin Zhu", "Bocheng Li", "Yifei Xin", "Zhihua Xia", "Linli Xu"], "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV2025", "url": "http://arxiv.org/abs/2411.02038v2", "summary": "Vector Quantization (VQ) is essential for discretizing continuous\nrepresentations in unsupervised learning but suffers from representation\ncollapse, causing low codebook utilization and limiting scalability. Existing\nsolutions often rely on complex optimizations or reduce latent dimensionality,\nwhich compromises model capacity and fails to fully solve the problem. We\nidentify the root cause as disjoint codebook optimization, where only a few\ncode vectors are updated via gradient descent. To fix this, we propose\n\\textbf{Sim}ple\\textbf{VQ}, which reparameterizes code vectors through a\nlearnable linear transformation layer over a latent basis, optimizing the\n\\textit{entire linear space} rather than nearest \\textit{individual code\nvectors}. Although the multiplication of two linear matrices is equivalent to\napplying a single linear layer, this simple approach effectively prevents\ncollapse. Extensive experiments on image and audio tasks demonstrate that SimVQ\nimproves codebook usage, is easy to implement, and generalizes well across\nmodalities and architectures.", "comment": "Accepted at ICCV2025", "pdf_url": "http://arxiv.org/pdf/2411.02038v2", "cate": "cs.LG", "date": "2024-11-04", "updated": "2025-07-30"}
{"id": "2507.22245", "title": "Minimizing CGYRO HPC Communication Costs in Ensembles with XGYRO by Sharing the Collisional Constant Tensor Structure", "authors": ["Igor Sfiligoi", "Emily A. Belli", "Jeff Candy"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      3 pages, 3 figures, Accepted at ICPP25", "url": "http://arxiv.org/abs/2507.22245v1", "summary": "First-principles fusion plasma simulations are both compute and memory\nintensive, and CGYRO is no exception. The use of many HPC nodes to fit the\nproblem in the available memory thus results in significant communication\noverhead, which is hard to avoid for any single simulation. That said, most\nfusion studies are composed of ensembles of simulations, so we developed a new\ntool, named XGYRO, that executes a whole ensemble of CGYRO simulations as a\nsingle HPC job. By treating the ensemble as a unit, XGYRO can alter the global\nbuffer distribution logic and apply optimizations that are not feasible on any\nsingle simulation, but only on the ensemble as a whole. The main saving comes\nfrom the sharing of the collisional constant tensor structure, since its values\nare typically identical between parameter-sweep simulations. This data\nstructure dominates the memory consumption of CGYRO simulations, so\ndistributing it among the whole ensemble results in drastic memory savings for\neach simulation, which in turn results in overall lower communication overhead.", "comment": "3 pages, 3 figures, Accepted at ICPP25", "pdf_url": "http://arxiv.org/pdf/2507.22245v1", "cate": "cs.DC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21684", "title": "diffSPH: Differentiable Smoothed Particle Hydrodynamics for Adjoint Optimization and Machine Learning", "authors": ["Rene Winchenbach", "Nils Thuerey"], "categories": ["physics.flu-dyn", "cs.AI", "cs.LG", "I.2.0; I.6.0; G.1.4"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21684v1", "summary": "We present diffSPH, a novel open-source differentiable Smoothed Particle\nHydrodynamics (SPH) framework developed entirely in PyTorch with GPU\nacceleration. diffSPH is designed centrally around differentiation to\nfacilitate optimization and machine learning (ML) applications in Computational\nFluid Dynamics~(CFD), including training neural networks and the development of\nhybrid models. Its differentiable SPH core, and schemes for compressible (with\nshock capturing and multi-phase flows), weakly compressible (with boundary\nhandling and free-surface flows), and incompressible physics, enable a broad\nrange of application areas. We demonstrate the framework's unique capabilities\nthrough several applications, including addressing particle shifting via a\nnovel, target-oriented approach by minimizing physical and regularization loss\nterms, a task often intractable in traditional solvers. Further examples\ninclude optimizing initial conditions and physical parameters to match target\ntrajectories, shape optimization, implementing a solver-in-the-loop setup to\nemulate higher-order integration, and demonstrating gradient propagation\nthrough hundreds of full simulation steps. Prioritizing readability, usability,\nand extensibility, this work offers a foundational platform for the CFD\ncommunity to develop and deploy novel neural networks and adjoint optimization\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21684v1", "cate": "physics.flu-dyn", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21569", "title": "An em algorithm for quantum Boltzmann machines", "authors": ["Takeshi Kimura", "Kohtaro Kato", "Masahito Hayashi"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Main text: 10 pages, 2 figures. Appendix: 3 pages, 1 figure", "url": "http://arxiv.org/abs/2507.21569v1", "summary": "We develop a quantum version of the em algorithm for training quantum\nBoltzmann machines. The em algorithm is an information-geometric extension of\nthe well-known expectation-maximization (EM) algorithm, offering a structured\nalternative to gradient-based methods with potential advantages in stability\nand convergence. We implement the algorithm on a semi-quantum restricted\nBoltzmann machine, where quantum effects are confined to the hidden layer. This\nstructure enables analytical update rules while preserving quantum\nexpressivity. Numerical experiments on benchmark datasets show that the\nproposed method achieves stable learning and outperforms gradient-based\ntraining in several cases. These results demonstrate the potential of\ninformation-geometric optimization for quantum machine learning, particularly\nin settings where standard methods struggle due to non-commutativity or\nvanishing gradients.", "comment": "Main text: 10 pages, 2 figures. Appendix: 3 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.21569v1", "cate": "quant-ph", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22003", "title": "See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs", "authors": ["Ziyun Dai", "Xiaoqiang Li", "Shaohua Zhang", "Yuanchen Wu", "Jide Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM25", "url": "http://arxiv.org/abs/2507.22003v2", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in visual understanding and multimodal reasoning. However, LVLMs\nfrequently exhibit hallucination phenomena, manifesting as the generated\ntextual responses that demonstrate inconsistencies with the provided visual\ncontent. Existing hallucination mitigation methods are predominantly\ntext-centric, the challenges of visual-semantic alignment significantly limit\ntheir effectiveness, especially when confronted with fine-grained visual\nunderstanding scenarios. To this end, this paper presents ViHallu, a\nVision-Centric Hallucination mitigation framework that enhances visual-semantic\nalignment through Visual Variation Image Generation and Visual Instruction\nConstruction. ViHallu introduces visual variation images with controllable\nvisual alterations while maintaining the overall image structure. These images,\ncombined with carefully constructed visual instructions, enable LVLMs to better\nunderstand fine-grained visual content through fine-tuning, allowing models to\nmore precisely capture the correspondence between visual content and text,\nthereby enhancing visual-semantic alignment. Extensive experiments on multiple\nbenchmarks show that ViHallu effectively enhances models' fine-grained visual\nunderstanding while significantly reducing hallucination tendencies.\nFurthermore, we release ViHallu-Instruction, a visual instruction dataset\nspecifically designed for hallucination mitigation and visual-semantic\nalignment. Code is available at https://github.com/oliviadzy/ViHallu.", "comment": "Accepted by ACM MM25", "pdf_url": "http://arxiv.org/pdf/2507.22003v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2503.01596", "title": "A Neural Network Enhanced Born Approximation for Inverse Scattering", "authors": ["Ansh Desai", "Jonathan Ma", "Timo Lahivaara", "Peter Monk"], "categories": ["math.NA", "cs.NA", "35R30, 35J25, 35P25, 68T07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      23 pages, 11 figures. To be published", "url": "http://arxiv.org/abs/2503.01596v3", "summary": "Time-harmonic acoustic inverse scattering concerns the ill-posed and\nnonlinear problem of determining the refractive index of an inaccessible,\npenetrable scatterer based on far field wave scattering data. When the\nscattering is weak, the regularized inverse Born approximation provides a\nlinearized model for recovering the shape and material properties of a\nscatterer. We propose two convolutional neural network (CNN) algorithms to\ncorrect the traditional inverse Born approximation even when the scattering is\nnot weak. These are denoted Born-CNN (BCNN) and CNN-Born (CNNB). BCNN applies a\npost-correction to the Born reconstruction, while CNNB pre-corrects the data.\nBoth methods leverage the Born approximation's excellent fidelity in weak\nscattering, while extending its applicability beyond its theoretical limits.\nCNNB particularly exhibits a strong generalization to more complex out of\ndistribution scatterers. Based on numerical tests and benchmarking against\nother standard approaches, our corrected Born models provide alternative\ndata-driven methods for obtaining the refractive index, extending the utility\nof the Born approximation to regimes where the traditional method fails.", "comment": "23 pages, 11 figures. To be published", "pdf_url": "http://arxiv.org/pdf/2503.01596v3", "cate": "math.NA", "date": "2025-03-03", "updated": "2025-07-30"}
{"id": "2502.20040", "title": "CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR", "authors": ["Nian Shao", "Rui Zhou", "Pengyu Wang", "Xian Li", "Ying Fang", "Yujie Yang", "Xiaofei Li"], "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Submission to IEEE/ACM Trans. on TASLP", "url": "http://arxiv.org/abs/2502.20040v2", "summary": "In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising\nand dereverberation network for improving both speech quality and automatic\nspeech recognition (ASR) performance. The proposed network takes as input the\nnoisy and reverberant microphone recording and predicts the corresponding clean\nMel-spectrogram. The enhanced Mel-spectrogram can be either transformed to the\nspeech waveform with a neural vocoder or directly used for ASR. The proposed\nnetwork is composed of interleaved cross-band and narrow-band processing in the\nMel-frequency domain, for learning the full-band spectral pattern and the\nnarrow-band properties of signals, respectively. Compared to linear-frequency\ndomain or time-domain speech enhancement, the key advantage of Mel-spectrogram\nenhancement is that Mel-frequency presents speech in a more compact way and\nthus is easier to learn, which will benefit both speech quality and ASR.\nExperimental results on five English and one Chinese datasets demonstrate a\nsignificant improvement in both speech quality and ASR performance achieved by\nthe proposed model.Code and audio examples of our model are available online.", "comment": "Submission to IEEE/ACM Trans. on TASLP", "pdf_url": "http://arxiv.org/pdf/2502.20040v2", "cate": "eess.AS", "date": "2025-02-27", "updated": "2025-07-30"}
{"id": "2507.22294", "title": "Towards Experiment Execution in Support of Community Benchmark Workflows for HPC", "authors": ["Gregor von Laszewski", "Wesley Brewer", "Sean R. Wilkinson", "Andrew Shao", "J. P. Fleischer", "Harshad Pitkar", "Christine R. Kirkpatrick", "Geoffrey C. Fox"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22294v1", "summary": "A key hurdle is demonstrating compute resource capability with limited\nbenchmarks. We propose workflow templates as a solution, offering adaptable\ndesigns for specific scientific applications. Our paper identifies common usage\npatterns for these templates, drawn from decades of HPC experience, including\nrecent work with the MLCommons Science working group.\n  We found that focusing on simple experiment management tools within the\nbroader computational workflow improves adaptability, especially in education.\nThis concept, which we term benchmark carpentry, is validated by two\nindependent tools: Cloudmesh's Experiment Executor and Hewlett Packard\nEnterprise's SmartSim. Both frameworks, with significant functional overlap,\nhave been tested across various scientific applications, including conduction\ncloudmask, earthquake prediction, simulation-AI/ML interactions, and the\ndevelopment of computational fluid dynamics surrogates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22294v1", "cate": "cs.DC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21694", "title": "A Multi-Agent Generative AI Framework for IC Module-Level Verification Automation", "authors": ["Wenbo Liu", "Forbes Hou", "Jon Zhang", "Hong Liu", "Allen Lei"], "categories": ["cs.AR", "cs.AI", "B.5.2; I.2.11"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      20 pages, 12 figures. DVCon China 2025", "url": "http://arxiv.org/abs/2507.21694v1", "summary": "As large language models demonstrate enormous potential in the field of\nElectronic Design Automation (EDA), generative AI-assisted chip design is\nattracting widespread attention from academia and industry. Although these\ntechnologies have made preliminary progress in tasks such as code generation,\ntheir application in chip verification -- a critical bottleneck in the chip\ndevelopment cycle -- remains at an exploratory stage. This paper proposes an\ninnovative Multi-Agent Verification Framework (MAVF) aimed at addressing the\nlimitations of current single-LLM approaches in complex verification tasks. Our\nframework builds an automated transformation system from design specifications\nto testbench through the collaborative work of multiple specialized agents,\nincluding specification parsing, verification strategy generation, and code\nimplementation. Through verification experiments on multiple chip modules of\nvarying complexity, results show that MAVF significantly outperforms\ntraditional manual methods and single-dialogue generative AI approaches in\nverification document parsing and generation, as well as automated testbench\ngeneration. This research opens new directions for exploring generative AI\napplications in verification automation, potentially providing effective\napproaches to solving the most challenging bottleneck issues in chip design.", "comment": "20 pages, 12 figures. DVCon China 2025", "pdf_url": "http://arxiv.org/pdf/2507.21694v1", "cate": "cs.AR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21642", "title": "Whilter: A Whisper-based Data Filter for \"In-the-Wild\" Speech Corpora Using Utterance-level Multi-Task Classification", "authors": ["William Ravenscroft", "George Close", "Kit Bower-Morris", "Jamie Stacey", "Dmitry Sityaev", "Kris Y. Hong"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted for Interspeech 2025", "url": "http://arxiv.org/abs/2507.21642v1", "summary": "Large-scale in-the-wild speech datasets have become more prevalent in recent\nyears due to increased interest in models that can learn useful features from\nunlabelled data for tasks such as speech recognition or synthesis. These\ndatasets often contain undesirable features, such as multiple speakers,\nnon-target languages, and music, which may impact model learning. The Whilter\nmodel is proposed as a multitask solution to identify these undesirable\nsamples. Whilter uses a Whisper encoder with an attention-based classifier to\nsolve five diverse classification problems at once. In addition, an annotated\ndataset is published for a subset of two popular in-the-wild corpora. Whilter\nachieves F1 scores above 85% and equal error rates of 6.5% to 7.8% for three of\nfive subtasks, outperforming a state-of-the-art BEATs classifier on\nspeech-specific classes, with a notable decrease in processing time compared to\na combination of single-task alternatives.", "comment": "Accepted for Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.21642v1", "cate": "cs.SD", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22008", "title": "VeS: Teaching Pixels to Listen Without Supervision", "authors": ["Sajay Raj"], "categories": ["cs.CV", "I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 1 figure, 1 table. Code and models are released", "url": "http://arxiv.org/abs/2507.22008v1", "summary": "Recent dense audio-visual (AV) models achieve impressive retrieval and\nemergent localization, but almost all evidence comes from English-centric,\ncaption-rich web video. It is unclear whether these objectives survive in\nlow-resource, code-switched, and noisy multilingual settings that typify\ndeveloping regions. We show they do**-**and that the choice of aggregation\nfunction becomes even more critical. Using a multilingual subset of Project\nVaani spanning dozens of Indian languages and dialectal variants, we compare\nthree contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)\na dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid\n(motivated by frozen-vision alignment strategies). The dense objective delivers\na +59% relative R@1 (Audio Visual) improvement over global pooling and\nsubstantially lower mean/median ranks, while consistently producing sharp\nzero-shot localization heatmaps of spoken objects-despite keeping the vision\nbackbone entirely frozen (no LoRA / partial fine-tuning). Our results\ndemonstrate that dense token routing is not a luxury of high-resource English\ncorpora; it is more decisive when annotations and acoustic cleanliness are\nscarce. We release the codebase and trained models.", "comment": "6 pages, 1 figure, 1 table. Code and models are released", "pdf_url": "http://arxiv.org/pdf/2507.22008v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2505.13929", "title": "Error estimates for numerical approximations of a nonlinear gradient flow model", "authors": ["Jerome Droniou", "Kim-Ngan Le", "Huateng Zhu"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13929v3", "summary": "We perform numerical analysis of a nonlinear gradient flow, which can be\nregarded as a parabolic minimal surface problem or a regularised total\nvariation flow, using the gradient discretisation method (GDM).\n  GDM is a unified convergence analysis framework that covers conforming and\nnonconforming numerical methods, for instance, conforming and nonconforming\nfinite element, two-point flux approximation, etc..\n  In this paper, a fully discretised implicit scheme of the model is proposed,\nthe existence and uniqueness of the solution to the scheme is proved, the\nstability and consistency of the scheme are analysed, and error estimates are\nestablished.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13929v3", "cate": "math.NA", "date": "2025-05-20", "updated": "2025-07-30"}
{"id": "2505.00059", "title": "BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition", "authors": ["Paige Tuttösí", "Mantaj Dhillon", "Luna Sang", "Shane Eastwood", "Poorvi Bhatia", "Quang Minh Dinh", "Avni Kapoor", "Yewon Jin", "Angelica Lim"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to Computer Speech and Language, Special issue: Multi-Speaker, Multi-Microphone, and Multi-Modal Distant Speech Recognition. Project Webpage and Data access : this https URL", "url": "http://arxiv.org/abs/2505.00059v2", "summary": "Some speech recognition tasks, such as automatic speech recognition (ASR),\nare approaching or have reached human performance in many reported metrics.\nYet, they continue to struggle in complex, real-world, situations, such as with\ndistanced speech. Previous challenges have released datasets to address the\nissue of distanced ASR, however, the focus remains primarily on distance,\nspecifically relying on multi-microphone array systems. Here we present the\nB(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset\ncontains almost 4 hours of English speech from 98 actors with varying regional\nand non-native accents. The data was collected on smartphones in the actors\nhomes and therefore includes at least 98 different acoustic environments. The\ndata also includes 7 different emotion prompts and both shouted and spoken\nutterances. The smartphones were places in 19 different positions, including\nobstructions and being in a different room than the actor. This data is\npublicly available for use and can be used to evaluate a variety of speech\nrecognition tasks, including: ASR, shout detection, and speech emotion\nrecognition (SER). We provide initial benchmarks for ASR and SER tasks, and\nfind that ASR degrades both with an increase in distance and shout level and\nshows varied performance depending on the intended emotion. Our results show\nthat the BERSt dataset is challenging for both ASR and SER tasks and continued\nwork is needed to improve the robustness of such systems for more accurate\nreal-world use.", "comment": "Accepted to Computer Speech and Language, Special issue:\n  Multi-Speaker, Multi-Microphone, and Multi-Modal Distant Speech Recognition.\n  Project Webpage and Data access :\n  https://huggingface.co/datasets/Rosie-Lab/BERSt", "pdf_url": "http://arxiv.org/pdf/2505.00059v2", "cate": "cs.CL", "date": "2025-04-30", "updated": "2025-07-30"}
{"id": "2507.22339", "title": "A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks", "authors": ["Zhuocheng Liu", "Zhishu Shen", "Qiushi Zheng", "Tiehua Zhang", "Zheng Lei", "Jiong Jin"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22339v1", "summary": "Low Earth Orbit (LEO) satellites are emerging as key components of 6G\nnetworks, with many already deployed to support large-scale Earth observation\nand sensing related tasks. Federated Learning (FL) presents a promising\nparadigm for enabling distributed intelligence in these resource-constrained\nand dynamic environments. However, achieving reliable convergence, while\nminimizing both processing time and energy consumption, remains a substantial\nchallenge, particularly in heterogeneous and partially unlabeled satellite\nnetworks. To address this challenge, we propose a novel semi-supervised\nfederated learning framework tailored for LEO satellite networks with\nhierarchical clustering aggregation. To further reduce communication overhead,\nwe integrate sparsification and adaptive weight quantization techniques. In\naddition, we divide the FL clustering into two stages: satellite cluster\naggregation stage and Ground Stations (GSs) aggregation stage. The supervised\nlearning at GSs guides selected Parameter Server (PS) satellites, which in turn\nsupport fully unlabeled satellites during the federated training process.\nExtensive experiments conducted on a satellite network testbed demonstrate that\nour proposal can significantly reduce processing time (up to 3x) and energy\nconsumption (up to 4x) compared to other comparative methods while maintaining\nmodel accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22339v1", "cate": "cs.DC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21695", "title": "Towards a Large Physics Benchmark", "authors": ["Kristian G. Barman", "Sascha Caron", "Faegheh Hasibi", "Eugene Shalugin", "Yoris Marcet", "Johannes Otte", "Henk W. de Regt", "Merijn Moody"], "categories": ["physics.data-an", "cs.AI", "hep-ph", "physics.comp-ph", "physics.hist-ph"], "primary_category": "Subjects:       Data Analysis, Statistics and Probability (physics.data-an)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21695v1", "summary": "We introduce a benchmark framework developed by and for the scientific\ncommunity to evaluate, monitor and steer large language model development in\nfundamental physics. Building on philosophical concepts of scientific\nunderstanding and creativity, we develop a scoring system in which each\nquestion is scored by an expert for its correctness, difficulty, and surprise.\nThe questions are of three forms: (i) multiple-choice questions for conceptual\nunderstanding, (ii) analytical problems requiring mathematical derivation, and\n(iii) openended tasks requiring complex problem solving. Our current dataset\ncontains diverse set of examples, including a machine learning challenge to\nclassify high-energy physics events, such as the four top quark signal. To\nensure continued relevance, we propose a living benchmark, where physicists\ncontribute questions, for instance alongside new publications. We invite\ncontributions via: http://www.physicsbenchmarks.org/. We hope that this\nbenchmark will enable a targeted AI development that can make a meaningful\ncontribution to fundamental physics research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21695v1", "cate": "physics.data-an", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21712", "title": "An Equal-Probability Partition of the Sample Space: A Non-parametric Inference from Finite Samples", "authors": ["Urban Eriksson"], "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21712v1", "summary": "This paper investigates what can be inferred about an arbitrary continuous\nprobability distribution from a finite sample of $N$ observations drawn from\nit. The central finding is that the $N$ sorted sample points partition the real\nline into $N+1$ segments, each carrying an expected probability mass of exactly\n$1/(N+1)$. This non-parametric result, which follows from fundamental\nproperties of order statistics, holds regardless of the underlying\ndistribution's shape. This equal-probability partition yields a discrete\nentropy of $\\log_2(N+1)$ bits, which quantifies the information gained from the\nsample and contrasts with Shannon's results for continuous variables. I compare\nthis partition-based framework to the conventional ECDF and discuss its\nimplications for robust non-parametric inference, particularly in density and\ntail estimation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21712v1", "cate": "stat.ML", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22020", "title": "XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation", "authors": ["Raju Ningappa Mulawade", "Christoph Garth", "Alexander Wiebel"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 14 figures", "url": "http://arxiv.org/abs/2507.22020v1", "summary": "We propose a novel segmentation-based explainable artificial intelligence\n(XAI) method for neural networks working on point cloud classification. As one\nbuilding block of this method, we propose a novel point-shifting mechanism to\nintroduce perturbations in point cloud data. Recently, AI has seen an\nexponential growth. Hence, it is important to understand the decision-making\nprocess of AI algorithms when they are applied in critical areas. Our work\nfocuses on explaining AI algorithms that classify point cloud data. An\nimportant aspect of the methods used for explaining AI algorithms is their\nability to produce explanations that are easy for humans to understand. This\nallows them to analyze the AI algorithms better and make appropriate decisions\nbased on that analysis. Therefore, in this work, we intend to generate\nmeaningful explanations that can be easily interpreted by humans. The point\ncloud data we consider represents 3D objects such as cars, guitars, and\nlaptops. We make use of point cloud segmentation models to generate\nexplanations for the working of classification models. The segments are used to\nintroduce perturbations into the input point cloud data and generate saliency\nmaps. The perturbations are introduced using the novel point-shifting mechanism\nproposed in this work which ensures that the shifted points no longer influence\nthe output of the classification algorithm. In contrast to previous methods,\nthe segments used by our method are meaningful, i.e. humans can easily\ninterpret the meaning of the segments. Thus, the benefit of our method over\nother methods is its ability to produce more meaningful saliency maps. We\ncompare our method with the use of classical clustering algorithms to generate\nexplanations. We also analyze the saliency maps generated for example inputs\nusing our method to demonstrate the usefulness of the method in generating\nmeaningful explanations.", "comment": "18 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.22020v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.09772", "title": "Designing quantum chemistry algorithms with just-in-time compilation", "authors": ["Xiaojie Wu", "Qiming Sun", "Yuanheng Wang"], "categories": ["physics.comp-ph", "cs.NA", "math.NA"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.09772v4", "summary": "We introduce just-in-time (JIT) compilation to the integral kernels for\nGaussian-type orbitals (GTOs) to enhance the efficiency of electron repulsion\nintegral computations. For Coulomb and exchange (JK) matrices, JIT-based\nalgorithms yield a 2x speedup for the small 6-31G* basis set over GPU4PySCF\nv1.4 on an NVIDIA A100-80G GPU. By incorporating a novel algorithm designed for\norbitals with high angular momentum, the efficiency of JK evaluations with the\nlarge def2-TZVPP basis set is improved by up to 4x. The core CUDA\nimplementation is compact, comprising only ~1,000 lines of code, including\nsupport for single-precision arithmetic. Furthermore, the single-precision\nimplementation achieves a 3x speedup over the previous state-of-the-art.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.09772v4", "cate": "physics.comp-ph", "date": "2025-07-13", "updated": "2025-07-30"}
{"id": "2506.15107", "title": "I Know You're Listening: Adaptive Voice for HRI", "authors": ["Paige Tuttösí"], "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      PhD Thesis Simon Fraser University this https URL Read the Room: IROS 2023, Mmm whatcha say?: INTERSPEECH 2024, Emojivoice: RO-MAN 2025, You sound a little tense: SSW 2025. Thesis presentation here: this https URL", "url": "http://arxiv.org/abs/2506.15107v2", "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs.", "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: IROS 2023, Mmm whatcha say?: INTERSPEECH 2024, Emojivoice:\n  RO-MAN 2025, You sound a little tense: SSW 2025. Thesis presentation here:\n  https://www.youtube.com/watch?v=9BcEwqYOMYI", "pdf_url": "http://arxiv.org/pdf/2506.15107v2", "cate": "cs.RO", "date": "2025-06-18", "updated": "2025-07-30"}
{"id": "2507.22372", "title": "Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos", "authors": ["Grace Nansamba", "Evelyn Namugwanya", "David Boehme", "Dewi Yokelson", "Riley Shipley", "Derek Schafer", "Michael McKinsey", "Olga Pearce", "Anthony Skjellum"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2507.22372v1", "summary": "We introduce ``communication regions'' into the widely used Caliper HPC\nprofiling tool. A communication region is an annotation enabling capture of\nmetrics about the data being communicated (including statistics of these\nmetrics), and metrics about the MPI processes involved in the communications,\nsomething not previously possible in Caliper. We explore the utility of\ncommunication regions with three representative modeling and simulation\napplications, AMG2023, Kripke, and Laghos, all part of the comprehensive\nBenchpark suite that includes Caliper annotations. Enhanced Caliper reveals\ndetailed communication behaviors. Using Caliper and Thicket in tandem, we\ncreate new visualizations of MPI communication patterns, including halo\nexchanges. Our findings reveal communication bottlenecks and detailed\nbehaviors, indicating significant utility of the special-regions addition to\nCaliper. The comparative scaling behavior of both CPU and GPU oriented systems\nare shown; we are able to look at different regions within a given application,\nand see how scalability and message-traffic metrics differ.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.22372v1", "cate": "cs.DC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21706", "title": "EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic Mutations", "authors": ["Zekai Lin", "Haoran Sun", "Yucheng Guo", "Yujie Yang", "Yanwen Wang", "Bozhen Hu", "Chonghang Ye", "Qirong Yang", "Fan Zhong", "Xiaoming Zhang", "Lei Liu"], "categories": ["q-bio.GN", "cs.AI"], "primary_category": "Subjects:       Genomics (q-bio.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21706v1", "summary": "Distinguishing pathogenic mutations from benign polymorphisms remains a\ncritical challenge in precision medicine. EnTao-GPM, developed by Fudan\nUniversity and BioMap, addresses this through three innovations: (1)\nCross-species targeted pre-training on disease-relevant mammalian genomes\n(human, pig, mouse), leveraging evolutionary conservation to enhance\ninterpretation of pathogenic motifs, particularly in non-coding regions; (2)\nGermline mutation specialization via fine-tuning on ClinVar and HGMD, improving\naccuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework\nintegrating DNA sequence embeddings with LLM-based statistical explanations to\nprovide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates\nsuperior accuracy in mutation classification. It revolutionizes genetic testing\nby enabling faster, more accurate, and accessible interpretation for clinical\ndiagnostics (e.g., variant assessment, risk identification, personalized\ntreatment) and research, advancing personalized medicine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21706v1", "cate": "q-bio.GN", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21726", "title": "Riemannian Optimization on Tree Tensor Networks with Application in Machine Learning", "authors": ["Marius Willner", "Marco Trenti", "Dirk Lebiedz"], "categories": ["math.OC", "cond-mat.other", "cs.LG", "15A69, 53C20, 65K10"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      24 pages, 6 figures, 4 pseudo-code algorithms, 1 table", "url": "http://arxiv.org/abs/2507.21726v1", "summary": "Tree tensor networks (TTNs) are widely used in low-rank approximation and\nquantum many-body simulation. In this work, we present a formal analysis of the\ndifferential geometry underlying TTNs. Building on this foundation, we develop\nefficient first- and second-order optimization algorithms that exploit the\nintrinsic quotient structure of TTNs. Additionally, we devise a backpropagation\nalgorithm for training TTNs in a kernel learning setting. We validate our\nmethods through numerical experiments on a representative machine learning\ntask.", "comment": "24 pages, 6 figures, 4 pseudo-code algorithms, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.21726v1", "cate": "math.OC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22041", "title": "Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning", "authors": ["Chaofei Qi", "Chao Ye", "Zhitai Liu", "Weiyang Lin", "Jianbin Qiu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22041v1", "summary": "Deep learning has witnessed the extensive utilization across a wide spectrum\nof domains, including fine-grained few-shot learning (FGFSL) which heavily\ndepends on deep backbones. Nonetheless, shallower deep backbones such as\nConvNet-4, are not commonly preferred because they're prone to extract a larger\nquantity of non-abstract visual attributes. In this paper, we initially\nre-evaluate the relationship between network depth and the ability to fully\nencode few-shot instances, and delve into whether shallow deep architecture\ncould effectuate comparable or superior performance to mainstream deep\nbackbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a\nlocation-aware constellation network (LCN-4), equipped with a cutting-edge\nlocation-aware feature clustering module. This module can proficiently encoder\nand integrate spatial feature fusion, feature clustering, and recessive feature\nlocation, thereby significantly minimizing the overall loss. Specifically, we\ninnovatively put forward a general grid position encoding compensation to\neffectively address the issue of positional information missing during the\nfeature extraction process of specific ordinary convolutions. Additionally, we\nfurther propose a general frequency domain location embedding technique to\noffset for the location loss in clustering features. We have carried out\nvalidation procedures on three representative fine-grained few-shot benchmarks.\nRelevant experiments have established that LCN-4 notably outperforms the\nConvNet-4 based State-of-the-Arts and achieves performance that is on par with\nor superior to most ResNet12-based methods, confirming the correctness of our\nconjecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22041v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.09372", "title": "Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model", "authors": ["Philippe Gonzalez", "Torsten Dau", "Tobias May"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to Clarity 2025 Workshop", "url": "http://arxiv.org/abs/2507.09372v2", "summary": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference.", "comment": "Accepted to Clarity 2025 Workshop", "pdf_url": "http://arxiv.org/pdf/2507.09372v2", "cate": "eess.AS", "date": "2025-07-12", "updated": "2025-07-30"}
{"id": "2507.22801", "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code", "authors": ["Shubhradeep Roy", "Suvarthi Sarkar", "Vivek Verma", "Aryabartta Sahu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22801v1", "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22801v1", "cate": "cs.DC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21763", "title": "Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks", "authors": ["Daniele Lanzoni", "Olivier Pierre-Louis", "Roberto Bergamaschini", "Francesco Montalenti"], "categories": ["cond-mat.stat-mech", "cond-mat.mtrl-sci", "cs.AI", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Statistical Mechanics (cond-mat.stat-mech)", "pdf_link": null, "comments": "Comments:      15 pages, 8 figures, 2 appendices", "url": "http://arxiv.org/abs/2507.21763v1", "summary": "We show that Generative Adversarial Networks (GANs) may be fruitfully\nexploited to learn stochastic dynamics, surrogating traditional models while\ncapturing thermal fluctuations. Specifically, we showcase the application to a\ntwo-dimensional, many-particle system, focusing on surface-step fluctuations\nand on the related time-dependent roughness. After the construction of a\ndataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained\nto propagate stochastically the state of the system in time, allowing the\ngeneration of new sequences with a reduced computational cost. Modifications\nwith respect to standard GANs, which facilitate convergence and increase\naccuracy, are discussed. The trained network is demonstrated to quantitatively\nreproduce equilibrium and kinetic properties, including scaling laws, with\ndeviations of a few percent from the exact value. Extrapolation limits and\nfuture perspectives are critically discussed.", "comment": "15 pages, 8 figures, 2 appendices", "pdf_url": "http://arxiv.org/pdf/2507.21763v1", "cate": "cond-mat.stat-mech", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21760", "title": "Unified machine-learning framework for property prediction and time-evolution simulation of strained alloy microstructure", "authors": ["Andrea Fantasia", "Daniele Lanzoni", "Niccolò Di Eugenio", "Angelo Monteleone", "Roberto Bergamaschini", "Francesco Montalenti"], "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "Comments:      19 pages, 9 figures", "url": "http://arxiv.org/abs/2507.21760v1", "summary": "We introduce a unified machine-learning framework designed to conveniently\ntackle the temporal evolution of alloy microstructures under the influence of\nan elastic field. This approach allows for the simultaneous extraction of\nelastic parameters from a short trajectory and for the prediction of further\nmicrostructure evolution under their influence. This is demonstrated by\nfocusing on spinodal decomposition in the presence of a lattice mismatch eta,\nand by carrying out an extensive comparison between the ground-truth evolution\nsupplied by phase field simulations and the predictions of suitable\nconvolutional recurrent neural network architectures. The two tasks may then be\nperformed subsequently into a cascade framework. Under a wide spectrum of\nmisfit conditions, the here-presented cascade model accurately predicts eta and\nthe full corresponding microstructure evolution, also when approaching critical\nconditions for spinodal decomposition. Scalability to larger computational\ndomain sizes and mild extrapolation errors in time (for time sequences five\ntimes longer than the sampled ones during training) are demonstrated. The\nproposed framework is general and can be applied beyond the specific,\nprototypical system considered here as an example. Intriguingly, experimental\nvideos could be used to infer unknown external parameters, prior to simulating\nfurther temporal evolution.", "comment": "19 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.21760v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22052", "title": "Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos", "authors": ["Ziren Gong", "Xiaohan Li", "Fabio Tosi", "Jiawei Han", "Stefano Mattoccia", "Jianfei Cai", "Matteo Poggi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22052v1", "summary": "We present Ov3R, a novel framework for open-vocabulary semantic 3D\nreconstruction from RGB video streams, designed to advance Spatial AI. The\nsystem features two key components: CLIP3R, a CLIP-informed 3D reconstruction\nmodule that predicts dense point maps from overlapping clips while embedding\nobject-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module\nthat lifts 2D features into 3D by learning fused descriptors integrating\nspatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates\nCLIP semantics directly into the reconstruction process, enabling globally\nconsistent geometry and fine-grained semantic alignment. Our framework achieves\nstate-of-the-art performance in both dense 3D reconstruction and\nopen-vocabulary 3D segmentation, marking a step forward toward real-time,\nsemantics-aware Spatial AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22052v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.19802", "title": "CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search", "authors": ["Ziyu Zhang", "Yuanhao Wei", "Joshua Engels", "Julian Shun"], "categories": ["cs.DB", "cs.DC", "cs.DS", "cs.IR"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19802v1", "summary": "Approximate nearest neighbor search (ANNS) has become a quintessential\nalgorithmic problem for various other foundational data tasks for AI workloads.\nGraph-based ANNS indexes have superb empirical trade-offs in indexing cost,\nquery efficiency, and query approximation quality. Most existing graph-based\nindexes are designed for the static scenario, where there are no updates to the\ndata after the index is constructed. However, full dynamism (insertions,\ndeletions, and searches) is crucial to providing up-to-date responses in\napplications using vector databases. It is desirable that the index efficiently\nsupports updates and search queries concurrently. Existing dynamic graph-based\nindexes suffer from at least one of the following problems: (1) the query\nquality degrades as updates happen; and (2) the graph structure updates used to\nmaintain the index quality upon updates are global and thus expensive. To solve\nthese problems, we propose the CleANN system which consists of three main\ncomponents: (1) workload-aware linking of diverse search tree descendants to\ncombat distribution shift; (2)query-adaptive on-the-fly neighborhood\nconsolidation to efficiently handle deleted nodes; and (3) semi-lazy memory\ncleaning to clean up stale information in the data structure and reduce the\nwork spent by the first two components. We evaluate CleANN on 7 diverse\ndatasets on fully dynamic workloads and find that CleANN has query quality at\nleast as good as if the index had been built statically using the corresponding\ndata. In the in-memory setting using 56 hyper-threads, with all types of\nqueries running concurrently, at the same recall level, CleANN achieves 7-1200x\nthroughput improvement on million-scale real-world datasets. To the best of our\nknowledge, CleANN is the first concurrent ANNS index to achieve such efficiency\nwhile maintaining quality under full dynamism.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19802v1", "cate": "cs.DB", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.22255", "title": "Agent-centric learning: from external reward maximization to internal knowledge curation", "authors": ["Hanqi Zhou", "Fryderyk Mantiuk", "David G. Nagy", "Charley M. Wu"], "categories": ["cs.LG", "cs.AI", "cs.SC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      RLC Finding the Frame Workshop 2025", "url": "http://arxiv.org/abs/2507.22255v1", "summary": "The pursuit of general intelligence has traditionally centered on external\nobjectives: an agent's control over its environments or mastery of specific\ntasks. This external focus, however, can produce specialized agents that lack\nadaptability. We propose representational empowerment, a new perspective\ntowards a truly agent-centric learning paradigm by moving the locus of control\ninward. This objective measures an agent's ability to controllably maintain and\ndiversify its own knowledge structures. We posit that the capacity -- to shape\none's own understanding -- is an element for achieving better ``preparedness''\ndistinct from direct environmental influence. Focusing on internal\nrepresentations as the main substrate for computing empowerment offers a new\nlens through which to design adaptable intelligent systems.", "comment": "RLC Finding the Frame Workshop 2025", "pdf_url": "http://arxiv.org/pdf/2507.22255v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21770", "title": "Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results", "authors": ["Ali Fallahi", "Azam Bastanfard", "Amineh Amini", "Hadi Saboohi"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      May 2023, 6 pages, 5 figures", "url": "http://arxiv.org/abs/2507.21770v1", "summary": "The importance of recommender systems on the web has grown, especially in the\nmovie industry, with a vast selection of options to watch. To assist users in\ntraversing available items and finding relevant results, recommender systems\nanalyze operational data and investigate users' tastes and habits. Providing\nhighly individualized suggestions can boost user engagement and satisfaction,\nwhich is one of the fundamental goals of the movie industry, significantly in\nonline platforms. According to recent studies and research, using\nknowledge-based techniques and considering the semantic ideas of the textual\ndata is a suitable way to get more appropriate results. This study provides a\nnew method for building a knowledge graph based on semantic information. It\nuses the ChatGPT, as a large language model, to assess the brief descriptions\nof movies and extract their tone of voice. Results indicated that using the\nproposed method may significantly enhance accuracy rather than employing the\nexplicit genres supplied by the publishers.", "comment": "May 2023, 6 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.21770v1", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21783", "title": "Domain Generalization and Adaptation in Intensive Care with Anchor Regression", "authors": ["Malte Londschien", "Manuel Burger", "Gunnar Rätsch", "Peter Bühlmann"], "categories": ["stat.AP", "cs.LG", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21783v1", "summary": "The performance of predictive models in clinical settings often degrades when\ndeployed in new hospitals due to distribution shifts. This paper presents a\nlarge-scale study of causality-inspired domain generalization on heterogeneous\nmulti-center intensive care unit (ICU) data. We apply anchor regression and\nintroduce anchor boosting, a novel, tree-based nonlinear extension, to a large\ndataset comprising 400,000 patients from nine distinct ICU databases. The\nanchor regularization consistently improves out-of-distribution performance,\nparticularly for the most dissimilar target domains. The methods appear robust\nto violations of theoretical assumptions, such as anchor exogeneity.\nFurthermore, we propose a novel conceptual framework to quantify the utility of\nlarge external data datasets. By evaluating performance as a function of\navailable target-domain data, we identify three regimes: (i) a domain\ngeneralization regime, where only the external model should be used, (ii) a\ndomain adaptation regime, where refitting the external model is optimal, and\n(iii) a data-rich regime, where external data provides no additional value.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21783v1", "cate": "stat.AP", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22057", "title": "MetaLab: Few-Shot Game Changer for Image Recognition", "authors": ["Chaofei Qi", "Zhitai Liu", "Jianbin Qiu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22057v1", "summary": "Difficult few-shot image recognition has significant application prospects,\nyet remaining the substantial technical gaps with the conventional large-scale\nimage recognition. In this paper, we have proposed an efficient original method\nfor few-shot image recognition, called CIELab-Guided Coherent Meta-Learning\n(MetaLab). Structurally, our MetaLab comprises two collaborative neural\nnetworks: LabNet, which can perform domain transformation for the CIELab color\nspace and extract rich grouped features, and coherent LabGNN, which can\nfacilitate mutual learning between lightness graph and color graph. For\nsufficient certification, we have implemented extensive comparative studies on\nfour coarse-grained benchmarks, four fine-grained benchmarks, and four\ncross-domain few-shot benchmarks. Specifically, our method can achieve high\naccuracy, robust performance, and effective generalization capability with\none-shot sample per class. Overall, all experiments have demonstrated that our\nMetaLab can approach 99\\% $\\uparrow\\downarrow$ accuracy, reaching the human\nrecognition ceiling with little visual deviation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22057v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22330", "title": "Hypernetworks for Model-Heterogeneous Personalized Federated Learning", "authors": ["Chen Zhang", "Husheng Li", "Xiang Liu", "Linshan Jiang", "Danxin Wang"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22330v1", "summary": "Recent advances in personalized federated learning have focused on addressing\nclient model heterogeneity. However, most existing methods still require\nexternal data, rely on model decoupling, or adopt partial learning strategies,\nwhich can limit their practicality and scalability. In this paper, we revisit\nhypernetwork-based methods and leverage their strong generalization\ncapabilities to design a simple yet effective framework for heterogeneous\npersonalized federated learning. Specifically, we propose MH-pFedHN, which\nleverages a server-side hypernetwork that takes client-specific embedding\nvectors as input and outputs personalized parameters tailored to each client's\nheterogeneous model. To promote knowledge sharing and reduce computation, we\nintroduce a multi-head structure within the hypernetwork, allowing clients with\nsimilar model sizes to share heads. Furthermore, we further propose\nMH-pFedHNGD, which integrates an optional lightweight global model to improve\ngeneralization. Our framework does not rely on external datasets and does not\nrequire disclosure of client model architectures, thereby offering enhanced\nprivacy and flexibility. Extensive experiments on multiple benchmarks and model\nsettings demonstrate that our approach achieves competitive accuracy, strong\ngeneralization, and serves as a robust baseline for future research in\nmodel-heterogeneous personalized federated learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22330v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21790", "title": "Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities", "authors": ["Georges Sfeir", "Gabriel Nova", "Stephane Hess", "Sander van Cranenburgh"], "categories": ["econ.EM", "cs.AI"], "primary_category": "Subjects:       Econometrics (econ.EM)", "pdf_link": null, "comments": "Comments:      32 pages, 6 figures, 14 tables", "url": "http://arxiv.org/abs/2507.21790v1", "summary": "Large Language Models (LLMs) are widely used to support various workflows\nacross different disciplines, yet their potential in choice modelling remains\nrelatively unexplored. This work examines the potential of LLMs as assistive\nagents in the specification and, where technically feasible, estimation of\nMultinomial Logit models. We implement a systematic experimental framework\ninvolving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek,\nGemini, Gemma, and Llama) evaluated under five experimental configurations.\nThese configurations vary along three dimensions: modelling goal (suggesting\nvs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs.\nChain-of-Thoughts); and information availability (full dataset vs. data\ndictionary only). Each LLM-suggested specification is implemented, estimated,\nand evaluated based on goodness-of-fit metrics, behavioural plausibility, and\nmodel complexity. Findings reveal that proprietary LLMs can generate valid and\nbehaviourally sound utility specifications, particularly when guided by\nstructured prompts. Open-weight models such as Llama and Gemma struggled to\nproduce meaningful specifications. Claude 4 Sonnet consistently produced the\nbest-fitting and most complex models, while GPT models suggested models with\nrobust and stable modelling outcomes. Some LLMs performed better when provided\nwith just data dictionary, suggesting that limiting raw data access may enhance\ninternal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of\ncorrectly estimating its own specifications by executing self-generated code.\nOverall, the results demonstrate both the promise and current limitations of\nLLMs as assistive agents in choice modelling, not only for model specification\nbut also for supporting modelling decision and estimation, and provide\npractical guidance for integrating these tools into choice modellers'\nworkflows.", "comment": "32 pages, 6 figures, 14 tables", "pdf_url": "http://arxiv.org/pdf/2507.21790v1", "cate": "econ.EM", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21807", "title": "MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation", "authors": ["Robert Kuchen"], "categories": ["stat.ML", "cs.LG", "62J07, 62H12, 62F07"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      21 pages, 2 algorithms, includes a simulation study", "url": "http://arxiv.org/abs/2507.21807v1", "summary": "Statistical learning methods for automated variable selection, such as LASSO,\nelastic nets, or gradient boosting, have become increasingly popular tools for\nbuilding powerful prediction models. Yet, in practice, analyses are often\ncomplicated by missing data. The most widely used approach to address\nmissingness is multiple imputation, which creates several completed datasets.\nHowever, there is an ongoing debate on how to perform model selection in the\npresence of multiple imputed datasets. Simple strategies, such as pooling\nmodels across datasets, have been shown to have suboptimal properties. Although\nmore sophisticated methods exist, they are often difficult to implement and\ntherefore not widely applied. In contrast, two recent approaches modify the\nregularization methods LASSO and elastic nets by defining a single loss\nfunction, resulting in a unified set of coefficients across imputations. Our\nkey contribution is to extend this principle to the framework of component-wise\ngradient boosting by proposing MIBoost, a novel algorithm that employs a\nuniform variable-selection mechanism across imputed datasets. Simulation\nstudies suggest that our approach yields prediction performance comparable to\nthat of these recently proposed methods.", "comment": "21 pages, 2 algorithms, includes a simulation study", "pdf_url": "http://arxiv.org/pdf/2507.21807v1", "cate": "stat.ML", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22058", "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again", "authors": ["Zigang Geng", "Yibing Wang", "Yeyao Ma", "Chen Li", "Yongming Rao", "Shuyang Gu", "Zhao Zhong", "Qinglin Lu", "Han Hu", "Xiaosong Zhang", "Linus", "Di Wang", "Jie Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22058v1", "summary": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22058v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2201.01278", "title": "Understanding Power and Energy Utilization in Large Scale Production Physics Simulation Codes", "authors": ["Adam Bertsch", "Michael R. Collette", "Shawn A. Dawson", "Si D. Hammond", "Ian Karlin", "M. Scott McKinley", "Kevin Pedretti", "Robert N. Rieben", "Brian S. Ryujin", "Arturo Vargas", "Kenneth Weiss"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      15 pages; accepted to the International Journal of High Performance Computing Applications (IJHPCA)", "url": "http://arxiv.org/abs/2201.01278v2", "summary": "Power is an often-cited reason for the move to advanced architectures on the\npath to Exascale computing. This is due to practical considerations related to\ndelivering enough power to successfully site and operate these machines, as\nwell as concerns about energy usage while running large simulations. Since\nobtaining accurate power measurements can be challenging, it may be tempting to\nuse the processor thermal design power (TDP) as a surrogate due to its\nsimplicity and availability. However, TDP is not indicative of typical power\nusage while running simulations. Using commodity and advanced technology\nsystems at Lawrence Livermore and Sandia National Labs, we performed a series\nof experiments to measure power and energy usage in running simulation codes.\nThese experiments indicate that large scale Lawrence Livermore simulation codes\nare significantly more efficient than a simple processor TDP model might\nsuggest.", "comment": "15 pages; accepted to the International Journal of High Performance\n  Computing Applications (IJHPCA)", "pdf_url": "http://arxiv.org/pdf/2201.01278v2", "cate": "cs.DC", "date": "2022-01-04", "updated": "2025-07-30"}
{"id": "2507.21831", "title": "Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences", "authors": ["Andreas Reich", "Claudia Thoms", "Tobias Schrimpf"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      48 pages, 9 figures and 8 tables", "url": "http://arxiv.org/abs/2507.21831v1", "summary": "LLMs are seeing widespread use for task automation, including automated\ncoding in the social sciences. However, even though researchers have proposed\ndifferent prompting strategies, their effectiveness varies across LLMs and\ntasks. Often trial and error practices are still widespread. We propose\nHALC$-$a general pipeline that allows for the systematic and reliable\nconstruction of optimal prompts for any given coding task and model, permitting\nthe integration of any prompting strategy deemed relevant. To investigate LLM\ncoding and validate our pipeline, we sent a total of 1,512 individual prompts\nto our local LLMs in over two million requests. We test prompting strategies\nand LLM task performance based on few expert codings (ground truth). When\ncompared to these expert codings, we find prompts that code reliably for single\nvariables (${\\alpha}$climate = .76; ${\\alpha}$movement = .78) and across two\nvariables (${\\alpha}$climate = .71; ${\\alpha}$movement = .74) using the LLM\nMistral NeMo. Our prompting strategies are set up in a way that aligns the LLM\nto our codebook$-$we are not optimizing our codebook for LLM friendliness. Our\npaper provides insights into the effectiveness of different prompting\nstrategies, crucial influencing factors, and the identification of reliable\nprompts for each coding task and model.", "comment": "48 pages, 9 figures and 8 tables", "pdf_url": "http://arxiv.org/pdf/2507.21831v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21871", "title": "Representations in vision and language converge in a shared, multidimensional space of perceived similarities", "authors": ["Katerina Marie Simkova", "Adrien Doerig", "Clayton Hickey", "Ian Charest"], "categories": ["q-bio.NC", "cs.LG"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      51 pages, 15 figures", "url": "http://arxiv.org/abs/2507.21871v1", "summary": "Humans can effortlessly describe what they see, yet establishing a shared\nrepresentational format between vision and language remains a significant\nchallenge. Emerging evidence suggests that human brain representations in both\nvision and language are well predicted by semantic feature spaces obtained from\nlarge language models (LLMs). This raises the possibility that sensory systems\nconverge in their inherent ability to transform their inputs onto shared,\nembedding-like representational space. However, it remains unclear how such a\nspace manifests in human behaviour. To investigate this, sixty-three\nparticipants performed behavioural similarity judgements separately on 100\nnatural scene images and 100 corresponding sentence captions from the Natural\nScenes Dataset. We found that visual and linguistic similarity judgements not\nonly converge at the behavioural level but also predict a remarkably similar\nnetwork of fMRI brain responses evoked by viewing the natural scene images.\nFurthermore, computational models trained to map images onto LLM-embeddings\noutperformed both category-trained and AlexNet controls in explaining the\nbehavioural similarity structure. These findings demonstrate that human visual\nand linguistic similarity judgements are grounded in a shared,\nmodality-agnostic representational structure that mirrors how the visual system\nencodes experience. The convergence between sensory and artificial systems\nsuggests a common capacity of how conceptual representations are formed-not as\narbitrary products of first order, modality-specific input, but as structured\nrepresentations that reflect the stable, relational properties of the external\nworld.", "comment": "51 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.21871v1", "cate": "q-bio.NC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22059", "title": "StepAL: Step-aware Active Learning for Cataract Surgical Videos", "authors": ["Nisarg A. Shah", "Bardia Safaei", "Shameema Sikder", "S. Swaroop Vedula", "Vishal M. Patel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2507.22059v1", "summary": "Active learning (AL) can reduce annotation costs in surgical video analysis\nwhile maintaining model performance. However, traditional AL methods, developed\nfor images or short video clips, are suboptimal for surgical step recognition\ndue to inter-step dependencies within long, untrimmed surgical videos. These\nmethods typically select individual frames or clips for labeling, which is\nineffective for surgical videos where annotators require the context of the\nentire video for annotation. To address this, we propose StepAL, an active\nlearning framework designed for full video selection in surgical step\nrecognition. StepAL integrates a step-aware feature representation, which\nleverages pseudo-labels to capture the distribution of predicted steps within\neach video, with an entropy-weighted clustering strategy. This combination\nprioritizes videos that are both uncertain and exhibit diverse step\ncompositions for annotation. Experiments on two cataract surgery datasets\n(Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms\nexisting active learning approaches, achieving higher accuracy in step\nrecognition with fewer labeled videos. StepAL offers an effective approach for\nefficient surgical video analysis, reducing the annotation burden in developing\ncomputer-assisted surgical systems.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.22059v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2407.06953", "title": "SP-Chain: Boosting Intra-Shard and Cross-Shard Security and Performance in Blockchain Sharding", "authors": ["Mingzhe Li", "You Lin", "Wei Wang", "Jin Zhang"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Published in IOTJ", "url": "http://arxiv.org/abs/2407.06953v2", "summary": "A promising way to overcome the scalability limitations of the current\nblockchain is to use sharding, which is to split the transaction processing\namong multiple, smaller groups of nodes. A well-performed blockchain sharding\nsystem requires both high performance and high security in both intra- and\ncross-shard perspectives. However, existing protocols either have issues on\nprotecting security or trade off great performance for security. In this paper,\nwe propose SP-Chain, a blockchain sharding system with enhanced Security and\nPerformance for both intra- and cross-shard perspectives. For intra-shard\naspect, we design a two-phase concurrent voting scheme to provide high system\nthroughput and low transaction confirmation latency. Moreover, we propose an\nefficient unbiased leader rotation scheme to ensure high performance under\nmalicious behavior. For cross-shard aspect, a proof-assisted efficient\ncross-shard transaction processing mechanism is proposed to guard the\ncross-shard transactions with low overhead. We implement SP-Chain based on\nHarmony, and evaluate its performance via large-scale deployment. Extensive\nevaluations suggest that SP-Chain can process more than 10,000 tx/sec under\nmalicious behaviors with a confirmation latency of 7.6s in a network of 4,000\nnodes.", "comment": "Published in IOTJ", "pdf_url": "http://arxiv.org/pdf/2407.06953v2", "cate": "cs.DC", "date": "2024-07-09", "updated": "2025-07-30"}
{"id": "2507.21890", "title": "Data-driven quantum Koopman method for simulating nonlinear dynamics", "authors": ["Baoyang Zhang", "Zhen Lu", "Yaomin Zhao", "Yue Yang"], "categories": ["quant-ph", "cs.AI", "cs.LG", "physics.comp-ph", "physics.flu-dyn"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21890v1", "summary": "Quantum computation offers potential exponential speedups for simulating\ncertain physical systems, but its application to nonlinear dynamics is\ninherently constrained by the requirement of unitary evolution. We propose the\nquantum Koopman method (QKM), a data-driven framework that bridges this gap\nthrough transforming nonlinear dynamics into linear unitary evolution in\nhigher-dimensional observable spaces. Leveraging the Koopman operator theory to\nachieve a global linearization, our approach maps system states into a\nhierarchy of Hilbert spaces using a deep autoencoder. Within the linearized\nembedding spaces, the state representation is decomposed into modulus and phase\ncomponents, and the evolution is governed by a set of unitary Koopman operators\nthat act exclusively on the phase. These operators are constructed from\ndiagonal Hamiltonians with coefficients learned from data, a structure designed\nfor efficient implementation on quantum hardware. This architecture enables\ndirect multi-step prediction, and the operator's computational complexity\nscales logarithmically with the observable space dimension. The QKM is\nvalidated across diverse nonlinear systems. Its predictions maintain relative\nerrors below 6% for reaction-diffusion systems and shear flows, and capture key\nstatistics in 2D turbulence. This work establishes a practical pathway for\nquantum-accelerated simulation of nonlinear phenomena, exploring a framework\nbuilt on the synergy between deep learning for global linearization and quantum\nalgorithms for unitary dynamics evolution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21890v1", "cate": "quant-ph", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.21902", "title": "Reducing Data Requirements for Sequence-Property Prediction in Copolymer Compatibilizers via Deep Neural Network Tuning", "authors": ["Md Mushfiqul Islam", "Nishat N. Labiba", "Lawrence O. Hall", "David S. Simmons"], "categories": ["cond-mat.mtrl-sci", "cond-mat.soft", "cond-mat.stat-mech", "cs.LG", "physics.chem-ph"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "Comments:      23 pages, 6 figures", "url": "http://arxiv.org/abs/2507.21902v1", "summary": "Synthetic sequence-controlled polymers promise to transform polymer science\nby combining the chemical versatility of synthetic polymers with the precise\nsequence-mediated functionality of biological proteins. However, design of\nthese materials has proven extraordinarily challenging, because they lack the\nmassive datasets of closely related evolved molecules that accelerate design of\nproteins. Here we report on a new Artifical Intelligence strategy to\ndramatically reduce the amount of data necessary to accelerate these materials'\ndesign. We focus on data connecting the repeat-unit-sequence of a\n\\emph{compatibilizer} molecule to its ability to reduce the interfacial tension\nbetween distinct polymer domains. The optimal sequence of these molecules,\nwhich are essential for applications such as mixed-waste polymer recycling,\ndepends strongly on variables such as concentration and chemical details of the\npolymer. With current methods, this would demand an entirely distinct dataset\nto enable design at each condition. Here we show that a deep neural network\ntrained on low-fidelity data for sequence/interfacial tension relations at one\nset of conditions can be rapidly tuned to make higher-fidelity predictions at a\ndistinct set of conditions, requiring far less data that would ordinarily be\nneeded. This priming-and-tuning approach should allow a single low-fidelity\nparent dataset to dramatically accelerate prediction and design in an entire\nconstellation of related systems. In the long run, it may also provide an\napproach to bootstrapping quantitative atomistic design with AI insights from\nfast, coarse simulations.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.21902v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22061", "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation", "authors": ["Kaining Ying", "Hengrui Hu", "Henghui Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.22061v1", "summary": "This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.", "comment": "ICCV 2025, Project Page: https://henghuiding.com/MOVE/", "pdf_url": "http://arxiv.org/pdf/2507.22061v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2506.19660", "title": "PS-WL: A Probability-Sensitive Wear Leveling scheme for SSD array scaling", "authors": ["Shuhang Xu", "Yunfei Gu", "Linhui Liu", "Chentao Wu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.19660v3", "summary": "As flash-based Solid State Drive (SSD) arrays become essential to modern data\ncenters, scaling these arrays to meet explosive data growth is a frequent and\ncritical operation. However, the conventional wear-leveling (WL) paradigm\napplied during scaling suffers from a fundamental flaw: it ignores the\nnon-linear relationship between wear and failure probability, potentially\npushing the most vulnerable, aged disks towards premature failure. To address\nthis critical issue at its root, we propose the Probability-Sensitive Wear\nLeveling (PS-WL) scheme, which shifts the optimization goal from balancing wear\nto directly balancing failure risk. At its core, PS-WL introduces an \"effective\nlifetime\" model derived from a realistic failure probability to more accurately\nassess disk lifetime. This model guides a PID controller for wear leveling\noperation, with a conservative zone minimizes performance overhead by\nrestricting warm data migration. Comprehensive simulations validate the\nsuperiority of PS-WL over state-of-the-art methods. The results demonstrate\nthat our approach significantly reduces performance overhead while, most\ncritically, consistently and effectively lowering the aggregated array failure\nrisk across diverse system configurations and workloads. This proves that by\ndirectly optimizing for reliability, PS-WL builds a scalable storage system\nthat is, by design, fundamentally safer, more efficient, and more stable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19660v3", "cate": "cs.DC", "date": "2025-06-24", "updated": "2025-07-30"}
{"id": "2507.21931", "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback", "authors": ["Carel van Niekerk", "Renato Vukovic", "Benjamin Matthias Ruppik", "Hsien-chin Lin", "Milica Gašić"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21931v1", "summary": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21931v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22010", "title": "Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform", "authors": ["Justin Curry", "Brennan Lagasse", "Ngoc B. Lam", "Gregory Cox", "David Rosenbluth", "Alberto Speranzon"], "categories": ["math.AT", "cs.AI", "cs.CG", "cs.LG", "math.DG", "58A35"], "primary_category": "Subjects:       Algebraic Topology (math.AT)", "pdf_link": null, "comments": "Comments:      17 pages and 8 figures. Preliminary report. Feedback welcome!", "url": "http://arxiv.org/abs/2507.22010v1", "summary": "In this work, we explore the structure of the embedding space of a\ntransformer model trained for playing a particular reinforcement learning (RL)\ngame. Specifically, we investigate how a transformer-based Proximal Policy\nOptimization (PPO) model embeds visual inputs in a simple environment where an\nagent must collect \"coins\" while avoiding dynamic obstacles consisting of\n\"spotlights.\" By adapting Robinson et al.'s study of the volume growth\ntransform for LLMs to the RL setting, we find that the token embedding space\nfor our visual coin collecting game is also not a manifold, and is better\nmodeled as a stratified space, where local dimension can vary from point to\npoint. We further strengthen Robinson's method by proving that fairly general\nvolume growth curves can be realized by stratified spaces. Finally, we carry\nout an analysis that suggests that as an RL agent acts, its latent\nrepresentation alternates between periods of low local dimension, while\nfollowing a fixed sub-strategy, and bursts of high local dimension, where the\nagent achieves a sub-goal (e.g., collecting an object) or where the\nenvironmental complexity increases (e.g., more obstacles appear). Consequently,\nour work suggests that the distribution of dimensions in a stratified latent\nspace may provide a new geometric indicator of complexity for RL games.", "comment": "17 pages and 8 figures. Preliminary report. Feedback welcome!", "pdf_url": "http://arxiv.org/pdf/2507.22010v1", "cate": "math.AT", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22062", "title": "MetaCLIP 2: A Worldwide Scaling Recipe", "authors": ["Yung-Sung Chuang", "Yang Li", "Dong Wang", "Ching-Feng Yeh", "Kehan Lyu", "Ramya Raghavendra", "James Glass", "Lifei Huang", "Jason Weston", "Luke Zettlemoyer", "Xinlei Chen", "Zhuang Liu", "Saining Xie", "Wen-tau Yih", "Shang-Wen Li", "Hu Xu"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.22062v1", "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.22062v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2407.02610", "title": "Towards Federated Learning with On-device Training and Communication in 8-bit Floating Point", "authors": ["Bokun Wang", "Axel Berg", "Durmus Alp Emre Acar", "Chuteng Zhou"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      extended version", "url": "http://arxiv.org/abs/2407.02610v2", "summary": "Recent work has shown that 8-bit floating point (FP8) can be used for\nefficiently training neural networks with reduced computational cost compared\nto training in FP32/FP16. In this work, we investigate the use of FP8 training\nin a federated learning context. This approach brings not only the usual\nbenefits of FP8 which are desirable for on-device training at the edge, but\nalso reduces client-server communication costs due to significant weight\ncompression. We present a novel method for combining FP8 client training while\nmaintaining a global FP32 server model and provide convergence analysis.\nExperiments with various machine learning models and datasets show that our\nmethod consistently yields communication reductions of at least 2.9x across a\nvariety of tasks and models compared to an FP32 baseline to achieve the same\ntrained model accuracy.", "comment": "extended version", "pdf_url": "http://arxiv.org/pdf/2407.02610v2", "cate": "cs.LG", "date": "2024-07-02", "updated": "2025-07-30"}
{"id": "2507.22198", "title": "Toward Trusted Onboard AI: Advancing Small Satellite Operations using Reinforcement Learning", "authors": ["Cannon Whitney", "Joseph Melville"], "categories": ["eess.SY", "cs.RO", "cs.SY", "68T05", "I.2.9"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      11 pages, 2 figures, 2 tables, accepted to the 39th Small Satellite Conference", "url": "http://arxiv.org/abs/2507.22198v1", "summary": "A RL (Reinforcement Learning) algorithm was developed for command automation\nonboard a 3U CubeSat. This effort focused on the implementation of macro\ncontrol action RL, a technique in which an onboard agent is provided with\ncompiled information based on live telemetry as its observation. The agent uses\nthis information to produce high-level actions, such as adjusting attitude to\nsolar pointing, which are then translated into control algorithms and executed\nthrough lower-level instructions. Once trust in the onboard agent is\nestablished, real-time environmental information can be leveraged for faster\nresponse times and reduced reliance on ground control. The approach not only\nfocuses on developing an RL algorithm for a specific satellite but also sets a\nprecedent for integrating trusted AI into onboard systems. This research builds\non previous work in three areas: (1) RL algorithms for issuing high-level\ncommands that are translated into low-level executable instructions; (2) the\ndeployment of AI inference models interfaced with live operational systems,\nparticularly onboard spacecraft; and (3) strategies for building trust in AI\nsystems, especially for remote and autonomous applications. Existing RL\nresearch for satellite control is largely limited to simulation-based\nexperiments; in this work, these techniques are tailored by constructing a\ndigital twin of a specific spacecraft and training the RL agent to issue macro\nactions in this simulated environment. The policy of the trained agent is\ncopied to an isolated environment, where it is fed compiled information about\nthe satellite to make inference predictions, thereby demonstrating the RL\nalgorithm's validity on orbit without granting it command authority. This\nprocess enables safe comparison of the algorithm's predictions against actual\nsatellite behavior and ensures operation within expected parameters.", "comment": "11 pages, 2 figures, 2 tables, accepted to the 39th Small Satellite\n  Conference", "pdf_url": "http://arxiv.org/pdf/2507.22198v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22039", "title": "Supervised Quantum Image Processing", "authors": ["Marco Parigi", "Mehran Khosrojerdi", "Filippo Caruso", "Leonardo Banchi"], "categories": ["quant-ph", "cs.AI", "cs.CV", "cs.LG", "81P68, 81P70, 81P40, 68Q12, 68T01", "I.2; I.4; J.2"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      13 pages, 11 figures", "url": "http://arxiv.org/abs/2507.22039v1", "summary": "In the era of big data and artificial intelligence, the increasing volume of\ndata and the demand to solve more and more complex computational challenges are\ntwo driving forces for improving the efficiency of data storage, processing and\nanalysis. Quantum image processing (QIP) is an interdisciplinary field between\nquantum information science and image processing, which has the potential to\nalleviate some of these challenges by leveraging the power of quantum\ncomputing. In this work, we compare and examine the compression properties of\nfour different Quantum Image Representations (QImRs): namely, Tensor Network\nRepresentation (TNR), Flexible Representation of Quantum Image (FRQI), Novel\nEnhanced Quantum Representation NEQR, and Quantum Probability Image Encoding\n(QPIE). Our simulations show that FRQI performs a higher compression of image\ninformation than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off\nbetween accuracy and memory in binary classification problems, evaluating the\nperformance of quantum kernels based on QImRs compared to the classical linear\nkernel. Our results indicate that quantum kernels provide comparable\nclassification average accuracy but require exponentially fewer resources for\nimage storage.", "comment": "13 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.22039v1", "cate": "quant-ph", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2206.04841", "title": "Hierarchical mixtures of Gaussians for combined dimensionality reduction and clustering", "authors": ["Sacha Sokoloski", "Philipp Berens"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2206.04841v2", "summary": "We introduce hierarchical mixtures of Gaussians (HMoGs), which unify\ndimensionality reduction and clustering into a single probabilistic model.\nHMoGs provide closed-form expressions for the model likelihood, exact inference\nover latent states and cluster membership, and exact algorithms for\nmaximum-likelihood optimization. The novel exponential family parameterization\nof HMoGs greatly reduces their computational complexity relative to similar\nmodel-based methods, allowing them to efficiently model hundreds of latent\ndimensions, and thereby capture additional structure in high-dimensional data.\nWe demonstrate HMoGs on synthetic experiments and MNIST, and show how joint\noptimization of dimensionality reduction and clustering facilitates increased\nmodel performance. We also explore how sparsity-constrained dimensionality\nreduction can further improve clustering performance while encouraging\ninterpretability. By bridging classical statistical modelling with the scale of\nmodern data and compute, HMoGs offer a practical approach to high-dimensional\nclustering that preserves statistical rigour, interpretability, and uncertainty\nquantification that is often missing from embedding-based, variational, and\nself-supervised methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2206.04841v2", "cate": "cs.LG", "date": "2022-06-10", "updated": "2025-07-29"}
{"id": "2507.20650", "title": "Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution", "authors": ["Zhicheng Zhang", "Peizhuo Lv", "Mengke Wan", "Jiang Fang", "Diandian Guo", "Yezeng Chen", "Yinlong Liu", "Wei Ma", "Jiyan Sun", "Liru Geng"], "categories": ["cs.CR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20650v1", "summary": "Recently, Deep Learning (DL) models have been increasingly deployed on\nend-user devices as On-Device AI, offering improved efficiency and privacy.\nHowever, this deployment trend poses more serious Intellectual Property (IP)\nrisks, as models are distributed on numerous local devices, making them\nvulnerable to theft and redistribution. Most existing ownership protection\nsolutions (e.g., backdoor-based watermarking) are designed for cloud-based\nAI-as-a-Service (AIaaS) and are not directly applicable to large-scale\ndistribution scenarios, where each user-specific model instance must carry a\nunique watermark. These methods typically embed a fixed watermark, and\nmodifying the embedded watermark requires retraining the model. To address\nthese challenges, we propose Hot-Swap MarkBoard, an efficient watermarking\nmethod. It encodes user-specific $n$-bit binary signatures by independently\nembedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)\nmodule, enabling efficient watermark customization without retraining through\nbranch swapping. A parameter obfuscation mechanism further entangles the\nwatermark weights with those of the base model, preventing removal without\ndegrading model performance. The method supports black-box verification and is\ncompatible with various model architectures and DL tasks, including\nclassification, image generation, and text generation. Extensive experiments\nacross three types of tasks and six backbone models demonstrate our method's\nsuperior efficiency and adaptability compared to existing approaches, achieving\n100\\% verification accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20650v1", "cate": "cs.CR", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2407.14953", "title": "AgileDART: An Agile and Scalable Edge Stream Processing Engine", "authors": ["Cheng-Wei Ching", "Xin Chen", "Chaeeun Kim", "Tongze Wang", "Dong Chen", "Dilma Da Silva", "Liting Hu"], "categories": ["cs.DB", "cs.DC"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      Published in IEEE Transactions on Mobile Computing", "url": "http://arxiv.org/abs/2407.14953v3", "summary": "Edge applications generate a large influx of sensor data on massive scales,\nand these massive data streams must be processed shortly to derive actionable\nintelligence. However, traditional data processing systems are not well-suited\nfor these edge applications as they often do not scale well with a large number\nof concurrent stream queries, do not support low-latency processing under\nlimited edge computing resources, and do not adapt to the level of\nheterogeneity and dynamicity commonly present in edge computing environments.\nAs such, we present AgileDart, an agile and scalable edge stream processing\nengine that enables fast stream processing of many concurrently running\nlow-latency edge applications' queries at scale in dynamic, heterogeneous edge\nenvironments. The novelty of our work lies in a dynamic dataflow abstraction\nthat leverages distributed hash table-based peer-to-peer overlay networks to\nautonomously place, chain, and scale stream operators to reduce query\nlatencies, adapt to workload variations, and recover from failures and a\nbandit-based path planning model that re-plans the data shuffling paths to\nadapt to unreliable and heterogeneous edge networks. We show that AgileDart\noutperforms Storm and EdgeWise on query latency and significantly improves\nscalability and adaptability when processing many real-world edge stream\napplications' queries.", "comment": "Published in IEEE Transactions on Mobile Computing", "pdf_url": "http://arxiv.org/pdf/2407.14953v3", "cate": "cs.DB", "date": "2024-07-20", "updated": "2025-07-29"}
{"id": "2507.22226", "title": "Optimal Planning for Enhancing the Resilience of Modern Distribution Systems Against Cyberattacks", "authors": ["Armita Khashayardoost", "Ahmad Mohammad Saber", "Deepa Kundur"], "categories": ["eess.SY", "cs.CR", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted Publication", "url": "http://arxiv.org/abs/2507.22226v1", "summary": "The increasing integration of IoT-connected devices in smart grids has\nintroduced new vulnerabilities at the distribution level. Of particular concern\nis the potential for cyberattacks that exploit high-wattage IoT devices, such\nas EV chargers, to manipulate local demand and destabilize the grid. While\nprevious studies have primarily focused on such attacks at the transmission\nlevel, this paper investigates their feasibility and impact at the distribution\nlevel. We examine how cyberattackers can target voltage-sensitive nodes,\nespecially those exposed by the presence of high-consumption devices, to cause\nvoltage deviation and service disruption. Our analysis demonstrates that\nconventional grid protections are insufficient against these intelligent,\nlocalized attacks. To address this, we propose resilience strategies using\ndistributed generation (DGs), exploring their role in preemptive planning. This\nresearch highlights the urgent need for distribution-level cyber resilience\nplanning in smart grids.", "comment": "Accepted Publication", "pdf_url": "http://arxiv.org/pdf/2507.22226v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2408.10635", "title": "Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search", "authors": ["Jonathan Light", "Min Cai", "Weiqin Chen", "Guanzhi Wang", "Xiusi Chen", "Wei Cheng", "Yisong Yue", "Ziniu Hu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      website: this https URL", "url": "http://arxiv.org/abs/2408.10635v3", "summary": "Traditional reinforcement learning and planning typically requires vast\namounts of data and training to develop effective policies. In contrast, large\nlanguage models (LLMs) exhibit strong generalization and zero-shot\ncapabilities, but struggle with tasks that require detailed planning and\ndecision-making in complex action spaces. We introduce STRATEGIST, a novel\napproach that integrates the strengths of both methods. Our approach leverages\nLLMs to search and update high-level strategies (as text), which are then\nrefined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is\na generalizable framework to optimize the strategy through population-based\nself-play simulations without the need for any training data. We demonstrate\nthe effectiveness of STRATEGIST in learning optimal strategies for competitive,\nmulti-turn games with partial information, including Game of Pure Strategy\n(GOPS) and multi-agent, hidden-identity discussion games like The Resistance:\nAvalon. Our results show that agents equipped with STRATEGIST outperform those\ntrained with traditional RL methods, other LLM-based skill acquisition\ntechniques, pre-existing LLM agents across both game environments and achieves\ncomparable performance against human players.", "comment": "website: https://llm-strategist.github.io", "pdf_url": "http://arxiv.org/pdf/2408.10635v3", "cate": "cs.AI", "date": "2024-08-20", "updated": "2025-07-29"}
{"id": "2305.18627", "title": "Quantize Once, Train Fast: Allreduce-Compatible Compression with Provable Guarantees", "authors": ["Jihao Xin", "Marco Canini", "Peter Richtárik", "Samuel Horváth"], "categories": ["cs.LG", "cs.DC", "stat.ML", "I.2.11"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ECAI'25", "url": "http://arxiv.org/abs/2305.18627v2", "summary": "Distributed training enables large-scale deep learning, but suffers from high\ncommunication overhead, especially as models and datasets grow. Gradient\ncompression, particularly quantization, is a promising approach to mitigate\nthis bottleneck. However, existing quantization schemes are often incompatible\nwith Allreduce, the dominant communication primitive in distributed deep\nlearning, and many prior solutions rely on heuristics without theoretical\nguarantees. We introduce Global-QSGD, an Allreduce-compatible gradient\nquantization method that leverages global norm scaling to reduce communication\noverhead while preserving accuracy. Global-QSGD is backed by rigorous\ntheoretical analysis, extending standard unbiased compressor frameworks to\nestablish formal convergence guarantees. Additionally, we develop a performance\nmodel to evaluate its impact across different hardware configurations.\nExtensive experiments on NVLink, PCIe, and large-scale cloud environments show\nthat Global-QSGD accelerates distributed training by up to 3.51% over baseline\nquantization methods, making it a practical and efficient solution for\nlarge-scale deep learning workloads.", "comment": "ECAI'25", "pdf_url": "http://arxiv.org/pdf/2305.18627v2", "cate": "cs.LG", "date": "2023-05-29", "updated": "2025-07-29"}
{"id": "2201.01984", "title": "Image Captioning via Compact Bidirectional Architecture", "authors": ["Zijie Song", "Yuanen Zhou", "Zhenzhen Hu", "Daqing Liu", "Huixia Ben", "Richang Hong", "Meng Wang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2201.01984v2", "summary": "Most current image captioning models typically generate captions from\nleft-to-right. This unidirectional property makes them can only leverage past\ncontext but not future context. Though refinement-based models can exploit both\npast and future context by generating a new caption in the second stage based\non pre-retrieved or pre-generated captions in the first stage, the decoder of\nthese models generally consists of two networks~(i.e. a retriever or captioner\nin the first stage and a captioner in the second stage), which can only be\nexecuted sequentially. In this paper, we introduce a Compact Bidirectional\nTransformer model for image captioning that can leverage bidirectional context\nimplicitly and explicitly while the decoder can be executed parallelly.\nSpecifically, it is implemented by tightly coupling left-to-right(L2R) and\nright-to-left(R2L) flows into a single compact model to serve as a\nregularization for implicitly exploiting bidirectional context and optionally\nallowing explicit interaction of the bidirectional flows, while the final\ncaption is chosen from either L2R or R2L flow in a sentence-level ensemble\nmanner. We conduct extensive ablation studies on MSCOCO benchmark and find that\nthe compact bidirectional architecture and the sentence-level ensemble play\nmore important roles than the explicit interaction mechanism. By combining with\nword-level ensemble seamlessly, the effect of sentence-level ensemble is\nfurther enlarged. We further extend the conventional one-flow self-critical\ntraining to the two-flows version under this architecture and achieve new\nstate-of-the-art results in comparison with non-vision-language-pretraining\nmodels. Finally, we verify the generality of this compact bidirectional\narchitecture by extending it to LSTM backbone. Source code is available at\nhttps://github.com/YuanEZhou/cbtic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2201.01984v2", "cate": "cs.CV", "date": "2022-01-06", "updated": "2025-07-29"}
{"id": "2412.19442", "title": "A Survey on Large Language Model Acceleration based on KV Cache Management", "authors": ["Haoyang Li", "Yiming Li", "Anxin Tian", "Tianhao Tang", "Zhanchao Xu", "Xuejia Chen", "Nicole Hu", "Wei Dong", "Qing Li", "Lei Chen"], "categories": ["cs.AI", "cs.DC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to TMLR 2025. The revised version incorporates more papers and has been further polished", "url": "http://arxiv.org/abs/2412.19442v3", "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.", "comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished", "pdf_url": "http://arxiv.org/pdf/2412.19442v3", "cate": "cs.AI", "date": "2024-12-27", "updated": "2025-07-30"}
{"id": "2507.22227", "title": "Safe and Efficient Data-driven Connected Cruise Control", "authors": ["Haosong Xiao", "Chaozhe R. He"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      To appear at MECC 2025 ( this https URL )", "url": "http://arxiv.org/abs/2507.22227v1", "summary": "In this paper, we design a safe and efficient cruise control for the\nconnected automated vehicle with access to motion information from multiple\nvehicles ahead via vehicle-to-vehicle (V2V) communication. Position and\nvelocity data collected from a chain of human-driven vehicles are\nsystematically leveraged to design a connected cruise controller that smoothly\nresponds to traffic perturbations while maximizing energy efficiency. A safety\nfilter derived from a control barrier function provides the safety guarantee.\nWe investigate the proposed control design's energy performance against real\ntraffic datasets and quantify the safety filter's energy impact. It is shown\nthat optimally utilizing V2V connectivity reduces energy consumption by more\nthan 10\\% compared to standard non-connected adaptive cruise control.\nMeanwhile, interesting interplays between safety filter and energy efficiency\ndesign are highlighted, revealing future research directions.", "comment": "To appear at MECC 2025 (https://mecc2025.a2c2.org/)", "pdf_url": "http://arxiv.org/pdf/2507.22227v1", "cate": "eess.SY", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2501.13818", "title": "Ensuring Medical AI Safety: Interpretability-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data", "authors": ["Frederik Pahde", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.13818v2", "summary": "Deep neural networks are increasingly employed in high-stakes medical\napplications, despite their tendency for shortcut learning in the presence of\nspurious correlations, which can have potentially fatal consequences in\npractice. Whereas a multitude of works address either the detection or\nmitigation of such shortcut behavior in isolation, the Reveal2Revise approach\nprovides a comprehensive bias mitigation framework combining these steps.\nHowever, effectively addressing these biases often requires substantial\nlabeling efforts from domain experts. In this work, we review the steps of the\nReveal2Revise framework and enhance it with semi-automated\ninterpretability-based bias annotation capabilities. This includes methods for\nthe sample- and feature-level bias annotation, providing valuable information\nfor bias mitigation methods to unlearn the undesired shortcut behavior. We show\nthe applicability of the framework using four medical datasets across two\nmodalities, featuring controlled and real-world spurious correlations caused by\ndata artifacts. We successfully identify and mitigate these biases in VGG16,\nResNet50, and contemporary Vision Transformer models, ultimately increasing\ntheir robustness and applicability for real-world medical tasks. Our code is\navailable at https://github.com/frederikpahde/medical-ai-safety.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.13818v2", "cate": "cs.AI", "date": "2025-01-23", "updated": "2025-07-29"}
{"id": "2406.10521", "title": "MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data", "authors": ["Yaobin Ling", "Xiaoqian Jiang", "Yejin Kim"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.10521v4", "summary": "In the era of big data, access to abundant data is crucial for driving\nresearch forward. However, such data is often inaccessible due to privacy\nconcerns or high costs, particularly in healthcare domain. Generating synthetic\n(tabular) data can address this, but existing models typically require\nsubstantial amounts of data to train effectively, contradicting our objective\nto solve data scarcity. To address this challenge, we propose a novel framework\nto generate synthetic tabular data, powered by large language models (LLMs)\nthat emulates the architecture of a Generative Adversarial Network (GAN). By\nincorporating data generation process as contextual information and utilizing\nLLM as the optimizer, our approach significantly enhance the quality of\nsynthetic data generation in common scenarios with small sample sizes. Our\nexperimental results on public and private datasets demonstrate that our model\noutperforms several state-of-art models regarding generating higher quality\nsynthetic data for downstream tasks while keeping privacy of the real data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.10521v4", "cate": "cs.LG", "date": "2024-06-15", "updated": "2025-07-28"}
{"id": "2309.08204", "title": "One-stage Modality Distillation for Incomplete Multimodal Learning", "authors": ["Shicai Wei", "Yang Luo", "Chunbo Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.08204v2", "summary": "Learning based on multimodal data has attracted increasing interest recently.\nWhile a variety of sensory modalities can be collected for training, not all of\nthem are always available in development scenarios, which raises the challenge\nto infer with incomplete modality. To address this issue, this paper presents a\none-stage modality distillation framework that unifies the privileged knowledge\ntransfer and modality information fusion into a single optimization procedure\nvia multi-task learning. Compared with the conventional modality distillation\nthat performs them independently, this helps to capture the valuable\nrepresentation that can assist the final model inference directly.\nSpecifically, we propose the joint adaptation network for the modality transfer\ntask to preserve the privileged information. This addresses the representation\nheterogeneity caused by input discrepancy via the joint distribution\nadaptation. Then, we introduce the cross translation network for the modality\nfusion task to aggregate the restored and available modality features. It\nleverages the parameters-sharing strategy to capture the cross-modal cues\nexplicitly. Extensive experiments on RGB-D classification and segmentation\ntasks demonstrate the proposed multimodal inheritance framework can overcome\nthe problem of incomplete modality input in various scenes and achieve\nstate-of-the-art performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.08204v2", "cate": "cs.CV", "date": "2023-09-15", "updated": "2025-07-29"}
{"id": "2507.22295", "title": "Design and Experimental Validation of UAV Swarm-Based Phased Arrays with MagSafe- and LEGO-Inspired RF Connectors", "authors": ["Bidya Debnath", "Mst Mostary Begum", "Prashant Neupant", "Brooke E. Molen", "Junming Diao"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22295v1", "summary": "This paper presents a novel UAV swarm-based phased array antenna system that\nleverages MagSafe- and LEGO-inspired radio frequency (RF) connectors to address\nkey challenges in distributed phased arrays, including inter-element oscillator\nsynchronization, localization, phase coherence, and positional accuracy. The\nproposed non-threaded, hands-free connectors enable precise inter-element\nspacing and establish a continuous, low-loss RF signal propagation path during\nmid-flight docking. A multi-stage optimization of the RF connector achieves a\ncompact form factor, DC-to-RF bandwidth, and a measured insertion loss as low\nas 0.2\\,dB. The system architecture offers scalability in gain and frequency by\nadjusting the array element density per UAV and UAV dimensions. Experimental\nresults from both stationary and in-flight tests of two UAV-based phased array\nprototypes align closely with simulations, demonstrating robust beam steering\nto multiple directions. This work delivers a practical, scalable, and\nlow-complexity platform that enables rapid deployment for next-generation\nairborne communications, radar, and remote sensing applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22295v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2502.03274", "title": "A Scalable Approach to Probabilistic Neuro-Symbolic Robustness Verification", "authors": ["Vasileios Manginas", "Nikolaos Manginas", "Edward Stevinson", "Sherwin Varghese", "Nikos Katzouris", "Georgios Paliouras", "Alessio Lomuscio"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19th Conference on Neurosymbolic Learning and Reasoning", "url": "http://arxiv.org/abs/2502.03274v2", "summary": "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. Typically,\nin the probabilistic variant of such systems, a neural network first extracts a\nset of symbols from sub-symbolic input, which are then used by a symbolic\ncomponent to reason in a probabilistic manner towards answering a query. In\nthis work, we address the problem of formally verifying the robustness of such\nNeSy probabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that a decision version of the core computation is\n$\\mathrm{NP}^{\\mathrm{PP}}$-complete. In the face of this result, we propose\nthe first approach for approximate, relaxation-based verification of\nprobabilistic NeSy systems. We demonstrate experimentally on a standard NeSy\nbenchmark that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving domain, where we verify a safety property under large input\ndimensionalities.", "comment": "19th Conference on Neurosymbolic Learning and Reasoning", "pdf_url": "http://arxiv.org/pdf/2502.03274v2", "cate": "cs.AI", "date": "2025-02-05", "updated": "2025-07-29"}
{"id": "2407.15549", "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs", "authors": ["Abhay Sheshadri", "Aidan Ewart", "Phillip Guo", "Aengus Lynch", "Cindy Wu", "Vivek Hebbar", "Henry Sleight", "Asa Cooper Stickland", "Ethan Perez", "Dylan Hadfield-Menell", "Stephen Casper"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code at this https URL . Models at this https URL", "url": "http://arxiv.org/abs/2407.15549v3", "summary": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of 'jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.", "comment": "Code at https://github.com/aengusl/latent-adversarial-training.\n  Models at https://huggingface.co/LLM-LAT", "pdf_url": "http://arxiv.org/pdf/2407.15549v3", "cate": "cs.LG", "date": "2024-07-22", "updated": "2025-07-29"}
{"id": "2312.17251", "title": "Semantic segmentation of SEM images of lower bainitic and tempered martensitic steels", "authors": ["Xiaohan Bie", "Manoj Arthanari", "Evelin Barbosa de Melo", "Baihua Ren", "Juancheng Li", "Stephen Yue", "Salim Brahimi", "Jun Song"], "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.17251v2", "summary": "This study employs deep learning techniques to segment scanning electron\nmicroscope images, enabling a quantitative analysis of carbide precipitates in\nlower bainite and tempered martensite steels with comparable strength.\nFollowing segmentation, carbides are investigated, and their volume percentage,\nsize distribution, and orientations are probed within the image dataset. Our\nfindings reveal that lower bainite and tempered martensite exhibit comparable\nvolume percentages of carbides, albeit with a more uniform distribution of\ncarbides in tempered martensite. Carbides in lower bainite demonstrate a\ntendency for better alignment than those in tempered martensite, aligning with\nthe observations of other researchers. However, both microstructures display a\nscattered carbide orientation, devoid of any discernible pattern. Comparative\nanalysis of aspect ratios and sizes of carbides in lower bainite and tempered\nmartensite unveils striking similarities. The deep learning model achieves an\nimpressive pixelwise accuracy of 98.0% in classifying carbide/iron matrix at\nthe individual pixel level. The semantic segmentation derived from deep\nlearning extends its applicability to the analysis of secondary phases in\nvarious materials, offering a time-efficient, versatile AI-powered workflow for\nquantitative microstructure analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.17251v2", "cate": "cs.CV", "date": "2023-12-02", "updated": "2025-07-29"}
{"id": "2507.22496", "title": "Assessing Value of Renewable-based VPP Versus Electrical Storage: Multi-market Participation Under Different Scheduling Regimes and Uncertainties", "authors": ["Hadi Nemati", "Ignacio Egido", "Pedro Sánchez-Martín", "Álvaro Ortega"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22496v1", "summary": "This paper compares the participation of Renewable-only Virtual Power Plants\n(RVPPs) and grid-scale Electrical Storage Systems (ESSs) in energy and reserve\nmarkets, evaluating their technical performance, market strategies, and\neconomic outcomes. To ensure a fair comparison, scheduling is analyzed over\nrepresentative sample days that capture seasonal operating regimes, and the\nassociated uncertainties are explicitly modeled. Two-stage robust optimization\nframeworks are employed: the RVPP model addresses price, generation, and demand\nuncertainties, whereas the ESS model considers price uncertainty only. In\naddition, an algorithm is proposed for sizing the ESS so that its market\nperformance matches that of the RVPP. Simulations cover both favorable and\nunfavorable scenarios, reflecting seasonal energy limits for dispatchable\nresources, varying forecast errors for nondispatchable resources, and\nalternative uncertainty-management strategies. The results provide operators\nwith quantitative guidance on the relative value of each approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22496v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.01751", "title": "SAKE: Steering Activations for Knowledge Editing", "authors": ["Marco Scialanga", "Thibault Laugel", "Vincent Grari", "Marcin Detyniecki"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01751v2", "summary": "As Large Langue Models have been shown to memorize real-world facts, the need\nto update this knowledge in a controlled and efficient manner arises. Designed\nwith these constraints in mind, Knowledge Editing (KE) approaches propose to\nalter specific facts in pretrained models. However, they have been shown to\nsuffer from several limitations, including their lack of contextual robustness\nand their failure to generalize to logical implications related to the fact. To\novercome these issues, we propose SAKE, a steering activation method that\nmodels a fact to be edited as a distribution rather than a single prompt.\nLeveraging Optimal Transport, SAKE alters the LLM behavior over a whole\nfact-related distribution, defined as paraphrases and logical implications.\nSeveral numerical experiments demonstrate the effectiveness of this method:\nSAKE is thus able to perform more robust edits than its existing counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01751v2", "cate": "cs.AI", "date": "2025-03-03", "updated": "2025-07-29"}
{"id": "2410.01149", "title": "Recovering Manifold Structure Using Ollivier-Ricci Curvature", "authors": ["Tristan Luca Saidi", "Abigail Hickok", "Andrew J. Blumberg"], "categories": ["cs.LG", "cs.AI", "cs.CG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.01149v2", "summary": "We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest\nneighbor graphs using a criterion based on Ollivier-Ricci curvature and\nestimated metric distortion. Our motivation comes from manifold learning: we\nshow that when the data generating the nearest-neighbor graph consists of noisy\nsamples from a low-dimensional manifold, edges that shortcut through the\nambient space have more negative Ollivier-Ricci curvature than edges that lie\nalong the data manifold. We demonstrate that our method outperforms alternative\npruning methods and that it significantly improves performance on many\ndownstream geometric data analysis tasks that use nearest neighbor graphs as\ninput. Specifically, we evaluate on manifold learning, persistent homology,\ndimension estimation, and others. We also show that ORC-ManL can be used to\nimprove clustering and manifold learning of single-cell RNA sequencing data.\nFinally, we provide empirical convergence experiments that support our\ntheoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.01149v2", "cate": "cs.LG", "date": "2024-10-02", "updated": "2025-07-28"}
{"id": "2403.15836", "title": "VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Annotation-Free Pathological Image Classification", "authors": ["Lanfeng Zhong", "Zongyao Huang", "Yang Liu", "Wenjun Liao", "Shichuan Zhang", "Guotai Wang", "Shaoting Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at TMI", "url": "http://arxiv.org/abs/2403.15836v3", "summary": "Classification of pathological images is the basis for automatic cancer\ndiagnosis. Despite that deep learning methods have achieved remarkable\nperformance, they heavily rely on labeled data, demanding extensive human\nannotation efforts. In this study, we present a novel human annotation-free\nmethod by leveraging pre-trained Vision-Language Models (VLMs). Without human\nannotation, pseudo-labels of the training set are obtained by utilizing the\nzero-shot inference capabilities of VLM, which may contain a lot of noise due\nto the domain gap between the pre-training and target datasets. To address this\nissue, we introduce VLM-CPL, a novel approach that contains two noisy label\nfiltering techniques with a semi-supervised learning strategy. Specifically, we\nfirst obtain prompt-based pseudo-labels with uncertainty estimation by\nzero-shot inference with the VLM using multiple augmented views of an input.\nThen, by leveraging the feature representation ability of VLM, we obtain\nfeature-based pseudo-labels via sample clustering in the feature space.\nPrompt-feature consensus is introduced to select reliable samples based on the\nconsensus between the two types of pseudo-labels. We further propose\nHigh-confidence Cross Supervision by to learn from samples with reliable\npseudo-labels and the remaining unlabeled samples. Additionally, we present an\ninnovative open-set prompting strategy that filters irrelevant patches from\nwhole slides to enhance the quality of selected patches. Experimental results\non five public pathological image datasets for patch-level and slide-level\nclassification showed that our method substantially outperformed zero-shot\nclassification by VLMs, and was superior to existing noisy label learning\nmethods. The code is publicly available at\nhttps://github.com/HiLab-git/VLM-CPL.", "comment": "Accepted at TMI", "pdf_url": "http://arxiv.org/pdf/2403.15836v3", "cate": "cs.CV", "date": "2024-03-23", "updated": "2025-07-29"}
{"id": "2507.22640", "title": "Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction", "authors": ["Alex Durkin", "Jasper Stolte", "Matthew Jones", "Raghuraman Pitchumani", "Bei Li", "Christian Michler", "Mehmet Mercangöz"], "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "stat.ML"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22640v1", "summary": "Offline reinforcement learning (offline RL) offers a promising framework for\ndeveloping control strategies in chemical process systems using historical\ndata, without the risks or costs of online experimentation. This work\ninvestigates the application of offline RL to the safe and efficient control of\nan exothermic polymerisation continuous stirred-tank reactor. We introduce a\nGymnasium-compatible simulation environment that captures the reactor's\nnonlinear dynamics, including reaction kinetics, energy balances, and\noperational constraints. The environment supports three industrially relevant\nscenarios: startup, grade change down, and grade change up. It also includes\nreproducible offline datasets generated from proportional-integral controllers\nwith randomised tunings, providing a benchmark for evaluating offline RL\nalgorithms in realistic process control tasks.\n  We assess behaviour cloning and implicit Q-learning as baseline algorithms,\nhighlighting the challenges offline agents face, including steady-state offsets\nand degraded performance near setpoints. To address these issues, we propose a\nnovel deployment-time safety layer that performs gradient-based action\ncorrection using input convex neural networks (PICNNs) as learned cost models.\nThe PICNN enables real-time, differentiable correction of policy actions by\ndescending a convex, state-conditioned cost surface, without requiring\nretraining or environment interaction.\n  Experimental results show that offline RL, particularly when combined with\nconvex action correction, can outperform traditional control approaches and\nmaintain stability across all scenarios. These findings demonstrate the\nfeasibility of integrating offline RL with interpretable and safety-aware\ncorrections for high-stakes chemical process control, and lay the groundwork\nfor more reliable data-driven automation in industrial systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22640v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.11820", "title": "An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models", "authors": ["Antonio Lorenzin", "Fabio Zanasi"], "categories": ["cs.AI", "cs.LO", "math.CT"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Full version of the conference paper", "url": "http://arxiv.org/abs/2503.11820v2", "summary": "Moralisation and Triangulation are transformations allowing to switch between\ndifferent ways of factoring a probability distribution into a graphical model.\nMoralisation allows to view a Bayesian network (a directed model) as a Markov\nnetwork (an undirected model), whereas triangulation works in the opposite\ndirection. We present a categorical framework where these transformations are\nmodelled as functors between a category of Bayesian networks and one of Markov\nnetworks. The two kinds of network (the objects of these categories) are\nthemselves represented as functors, from a `syntax' domain to a `semantics'\ncodomain. Notably, moralisation and triangulation are definable inductively on\nsuch syntax, and operate as a form of functor pre-composition. This approach\nintroduces a modular, algebraic perspective in the theory of probabilistic\ngraphical models.", "comment": "Full version of the conference paper", "pdf_url": "http://arxiv.org/pdf/2503.11820v2", "cate": "cs.AI", "date": "2025-03-14", "updated": "2025-07-28"}
{"id": "2410.03805", "title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting", "authors": ["Ignacio Aguilera-Martos", "Andrés Herrera-Poyatos", "Julián Luengo", "Francisco Herrera"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.03805v3", "summary": "Transformers have become the leading choice in natural language processing\nover other deep learning architectures. This trend has also permeated the field\nof time series analysis, especially for long-horizon forecasting, showcasing\npromising results both in performance and running time.\n  In this paper, we introduce Local Attention Mechanism (LAM), an efficient\nattention mechanism tailored for time series analysis. This mechanism exploits\nthe continuity properties of time series to reduce the number of attention\nscores computed. We present an algorithm for implementing LAM in tensor algebra\nthat runs in time and memory O(nlogn), significantly improving upon the O(n^2)\ntime and memory complexity of traditional attention mechanisms. We also note\nthe lack of proper datasets to evaluate long-horizon forecast models. Thus, we\npropose a novel set of datasets to improve the evaluation of models addressing\nlong-horizon forecasting challenges.\n  Our experimental analysis demonstrates that the vanilla transformer\narchitecture magnified with LAM surpasses state-of-the-art models, including\nthe vanilla attention mechanism. These results confirm the effectiveness of our\napproach and highlight a range of future challenges in long-sequence time\nseries forecasting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.03805v3", "cate": "cs.LG", "date": "2024-10-04", "updated": "2025-07-29"}
{"id": "2403.18201", "title": "Few-shot Online Anomaly Detection and Segmentation", "authors": ["Shenxing Wei", "Xing Wei", "Zhiheng Ma", "Songlin Dong", "Shaochen Zhang", "Yihong Gong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.18201v2", "summary": "Detecting anomaly patterns from images is a crucial artificial intelligence\ntechnique in industrial applications. Recent research in this domain has\nemphasized the necessity of a large volume of training data, overlooking the\npractical scenario where, post-deployment of the model, unlabeled data\ncontaining both normal and abnormal samples can be utilized to enhance the\nmodel's performance. Consequently, this paper focuses on addressing the\nchallenging yet practical few-shot online anomaly detection and segmentation\n(FOADS) task. Under the FOADS framework, models are trained on a few-shot\nnormal dataset, followed by inspection and improvement of their capabilities by\nleveraging unlabeled streaming data containing both normal and abnormal samples\nsimultaneously.\n  To tackle this issue, we propose modeling the feature distribution of normal\nimages using a Neural Gas network, which offers the flexibility to adapt the\ntopology structure to identify outliers in the data flow. In order to achieve\nimproved performance with limited training samples, we employ multi-scale\nfeature embedding extracted from a CNN pre-trained on ImageNet to obtain a\nrobust representation. Furthermore, we introduce an algorithm that can\nincrementally update parameters without the need to store previous samples.\nComprehensive experimental results demonstrate that our method can achieve\nsubstantial performance under the FOADS setting, while ensuring that the time\ncomplexity remains within an acceptable range on MVTec AD and BTAD datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.18201v2", "cate": "cs.CV", "date": "2024-03-27", "updated": "2025-07-29"}
{"id": "2507.22648", "title": "Distributed Average Consensus in Wireless Multi-Agent Systems with Over-the-Air Aggregation", "authors": ["Themistoklis Charalambous", "Zheng Chen", "Christoforos N. Hadjicostis"], "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages T. Charalambous, Z. Chen and C. N. Hadjicostis, \"Distributed Average Consensus in Wireless Multi-Agent Systems with Over-the-Air Aggregation,\" 2024 IEEE 25th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), Lucca, Italy, 2024, pp. 441-445", "url": "http://arxiv.org/abs/2507.22648v1", "summary": "In this paper, we address the average consensus problem of multi-agent\nsystems over wireless networks. We propose a distributed average consensus\nalgorithm by invoking the concept of over-the-air aggregation, which exploits\nthe signal superposition property of wireless multiple-access channels. The\nproposed algorithm deploys a modified version of the well-known Ratio Consensus\nalgorithm with an additional normalization step for compensating for the\narbitrary channel coefficients. We show that, when the noise level at the\nreceivers is negligible, the algorithm converges asymptotically to the average\nfor time-invariant and time-varying channels. Numerical simulations corroborate\nthe validity of our results.", "comment": "5 pages T. Charalambous, Z. Chen and C. N. Hadjicostis, \"Distributed\n  Average Consensus in Wireless Multi-Agent Systems with Over-the-Air\n  Aggregation,\" 2024 IEEE 25th International Workshop on Signal Processing\n  Advances in Wireless Communications (SPAWC), Lucca, Italy, 2024, pp. 441-445", "pdf_url": "http://arxiv.org/pdf/2507.22648v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2504.07856", "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization", "authors": ["Mengyang Li", "Zhong Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      We found a critical flaw in the prompt complexity metric, which affects the 2D curriculum grid construction and leads to potentially invalid comparisons. Since this undermines our main conclusions, we are withdrawing the paper and will revise the methodology before resubmission", "url": "http://arxiv.org/abs/2504.07856v3", "summary": "Aligning large language models with human preferences is crucial for their\nsafe deployment. While Direct Preference Optimization (DPO) offers an efficient\nalternative to reinforcement learning from human feedback, traditional DPO\nmethods are limited by their reliance on single preference pairs. Recent work\nlike Curriculum-DPO integrates multiple pairs using a one-dimensional\ndifficulty curriculum based on pairwise distinguishability (PD), but overlooks\nthe complexity of the input prompt itself. To address this, we propose\n2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that\njointly models Prompt Complexity (PC) and Pairwise Distinguishability. This\nframework introduces dual difficulty metrics to quantify prompt semantic\ncomplexity and response preference clarity, defines a curriculum strategy space\nencompassing multiple selectable strategies for task adaptation, and\nincorporates a KL-divergence-based adaptive mechanism for dynamic reference\nmodel updates to enhance training stability. Comprehensive experiments\ndemonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior\ncurriculum methods across multiple benchmarks, including MT-Bench, Vicuna\nBench, and WizardLM. Our approach achieves state-of-the-art performance on\nchallenging test sets like UltraFeedback. Ablation studies confirm the benefits\nof the 2D structure and adaptive mechanisms, while analysis provides guidance\nfor strategy selection. These findings demonstrate that effective alignment\nrequires modeling both prompt complexity and pairwise distinguishability,\nestablishing adaptive, multi-dimensional curriculum learning as a powerful and\ninterpretable new paradigm for preference-based language model optimization.", "comment": "We found a critical flaw in the prompt complexity metric, which\n  affects the 2D curriculum grid construction and leads to potentially invalid\n  comparisons. Since this undermines our main conclusions, we are withdrawing\n  the paper and will revise the methodology before resubmission", "pdf_url": "http://arxiv.org/pdf/2504.07856v3", "cate": "cs.AI", "date": "2025-04-10", "updated": "2025-07-29"}
{"id": "2410.11468", "title": "Can sparse autoencoders make sense of gene expression latent variable models?", "authors": ["Viktoria Schuster"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2410.11468v3", "summary": "Sparse autoencoders (SAEs) have lately been used to uncover interpretable\nlatent features in large language models. By projecting dense embeddings into a\nmuch higher-dimensional and sparse space, learned features become disentangled\nand easier to interpret. This work explores the potential of SAEs for\ndecomposing embeddings in complex and high-dimensional biological data. Using\nsimulated data, it outlines the efficacy, hyperparameter landscape, and\nlimitations of SAEs when it comes to extracting ground truth generative\nvariables from latent space. The application to embeddings from pretrained\nsingle-cell models shows that SAEs can find and steer key biological processes\nand even uncover subtle biological signals that might otherwise be missed. This\nwork further introduces scFeatureLens, an automated interpretability approach\nfor linking SAE features and biological concepts from gene sets to enable\nlarge-scale analysis and hypothesis generation in single-cell gene expression\nmodels.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2410.11468v3", "cate": "cs.LG", "date": "2024-10-15", "updated": "2025-07-29"}
{"id": "2404.09081", "title": "Probabilistic Directed Distance Fields for Ray-Based Shape Representations", "authors": ["Tristan Aumentado-Armstrong", "Stavros Tsogkas", "Sven Dickinson", "Allan Jepson"], "categories": ["cs.CV", "cs.LG", "I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Extension of arXiv:2112.05300 . Accepted to TPAMI", "url": "http://arxiv.org/abs/2404.09081v2", "summary": "In modern computer vision, the optimal representation of 3D shape continues\nto be task-dependent. One fundamental operation applied to such representations\nis differentiable rendering, as it enables inverse graphics approaches in\nlearning frameworks. Standard explicit shape representations (voxels, point\nclouds, or meshes) are often easily rendered, but can suffer from limited\ngeometric fidelity, among other issues. On the other hand, implicit\nrepresentations (occupancy, distance, or radiance fields) preserve greater\nfidelity, but suffer from complex or inefficient rendering processes, limiting\nscalability. In this work, we devise Directed Distance Fields (DDFs), a novel\nneural shape representation that builds upon classical distance fields. The\nfundamental operation in a DDF maps an oriented point (position and direction)\nto surface visibility and depth. This enables efficient differentiable\nrendering, obtaining depth with a single forward pass per pixel, as well as\ndifferential geometric quantity extraction (e.g., surface normals), with only\nadditional backward passes. Using probabilistic DDFs (PDDFs), we show how to\nmodel inherent discontinuities in the underlying field. We then apply DDFs to\nseveral applications, including single-shape fitting, generative modelling, and\nsingle-image 3D reconstruction, showcasing strong performance with simple\narchitectural components via the versatility of our representation. Finally,\nsince the dimensionality of DDFs permits view-dependent geometric artifacts, we\nconduct a theoretical investigation of the constraints necessary for view\nconsistency. We find a small set of field properties that are sufficient to\nguarantee a DDF is consistent, without knowing, for instance, which shape the\nfield is expressing.", "comment": "Extension of arXiv:2112.05300. Accepted to TPAMI", "pdf_url": "http://arxiv.org/pdf/2404.09081v2", "cate": "cs.CV", "date": "2024-04-13", "updated": "2025-07-29"}
{"id": "2507.22693", "title": "Malleability-Resistant Encrypted Control System with Disturbance Compensation and Real-Time Attack Detection", "authors": ["Naoki Aizawa", "Keita Emura", "Kiminao Kogiso"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22693v1", "summary": "This study proposes an encrypted PID control system with a disturbance\nobserver (DOB) using a keyed-homomorphic encryption (KHE) scheme, aiming to\nachieve control performance while providing resistance to malleability-based\nattacks. The controller integrates a DOB with a PID structure to compensate for\nmodeling uncertainties by estimating and canceling external disturbances. To\nenhance security, the system is designed to output error symbols when\nciphertexts are falsified during decryption or evaluation, enabling real-time\ndetection of malleability-based signal or parameter falsification. To validate\nthe proposed method, we conduct stage positioning control experiments and\nattack detection tests using an industrial linear stage. The results show that\nthe encrypted DOB-based PID controller outperforms a conventional encrypted PID\ncontroller in terms of tracking accuracy. Furthermore, the system successfully\ndetects two types of malleability-based attacks: one that destabilizes the\ncontrol system, and another that degrades its performance. The primary\ncontributions of this study are: (i) the implementation of a KHE-based\nencrypted DOB-PID controller, (ii) the improvement of control performance under\nuncertainties, and (iii) the experimental demonstration of attack detection\ncapabilities in encrypted control systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22693v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2505.14479", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      long paper", "url": "http://arxiv.org/abs/2505.14479v4", "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "comment": "long paper", "pdf_url": "http://arxiv.org/pdf/2505.14479v4", "cate": "cs.AI", "date": "2025-05-20", "updated": "2025-07-29"}
{"id": "2410.22296", "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks", "authors": ["Angelica Chen", "Samuel D. Stanton", "Frances Ding", "Robert G. Alberstein", "Andrew M. Watkins", "Richard Bonneau", "Vladimir Gligorijević", "Kyunghyun Cho", "Nathan C. Frey"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Supercedes arXiv:2407.00236v1 . arXiv admin note: text overlap with arXiv:2407.00236", "url": "http://arxiv.org/abs/2410.22296v5", "summary": "Although large language models (LLMs) have shown promise in biomolecule\noptimization problems, they incur heavy computational costs and struggle to\nsatisfy precise constraints. On the other hand, specialized solvers like\nLaMBO-2 offer efficiency and fine-grained control but require more domain\nexpertise. Comparing these approaches is challenging due to expensive\nlaboratory validation and inadequate synthetic benchmarks. We address this by\nintroducing Ehrlich functions, a synthetic test suite that captures the\ngeometric structure of biophysical sequence optimization problems. With\nprompting alone, off-the-shelf LLMs struggle to optimize Ehrlich functions. In\nresponse, we propose LLOME (Language Model Optimization with Margin\nExpectation), a bilevel optimization routine for online black-box optimization.\nWhen combined with a novel preference learning loss, we find LLOME can not only\nlearn to solve some Ehrlich functions, but can even perform as well as or\nbetter than LaMBO-2 on moderately difficult Ehrlich variants. However, LLMs\nalso exhibit some likelihood-reward miscalibration and struggle without\nexplicit rewards. Our results indicate LLMs can occasionally provide\nsignificant benefits, but specialized solvers are still competitive and incur\nless overhead.", "comment": "Supercedes arXiv:2407.00236v1. arXiv admin note: text overlap with\n  arXiv:2407.00236", "pdf_url": "http://arxiv.org/pdf/2410.22296v5", "cate": "cs.LG", "date": "2024-10-29", "updated": "2025-07-29"}
{"id": "2404.13873", "title": "Texture, Shape, Order, and Relation Matter: A New Transformer Design for Sequential DeepFake Detection", "authors": ["Yunfei Li", "Yuezun Li", "Baoyuan Wu", "Junyu Dong", "Guopu Zhu", "Siwei Lyu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      An extension of WACV 2025 (Oral)", "url": "http://arxiv.org/abs/2404.13873v5", "summary": "Sequential DeepFake detection is an emerging task that predicts the\nmanipulation sequence in order. Existing methods typically formulate it as an\nimage-to-sequence problem, employing conventional Transformer architectures.\nHowever, these methods lack dedicated design and consequently result in limited\nperformance. As such, this paper describes a new Transformer design, called\n{TSOM}, by exploring three perspectives: Texture, Shape, and Order of\nManipulations. Our method features four major improvements: \\ding{182} we\ndescribe a new texture-aware branch that effectively captures subtle\nmanipulation traces with a Diversiform Pixel Difference Attention module.\n\\ding{183} Then we introduce a Multi-source Cross-attention module to seek deep\ncorrelations among spatial and sequential features, enabling effective modeling\nof complex manipulation traces. \\ding{184} To further enhance the\ncross-attention, we describe a Shape-guided Gaussian mapping strategy,\nproviding initial priors of the manipulation shape. \\ding{185} Finally,\nobserving that the subsequent manipulation in a sequence may influence traces\nleft in the preceding one, we intriguingly invert the prediction order from\nforward to backward, leading to notable gains as expected. Building upon TSOM,\nwe introduce an extended method, {TSOM++}, which additionally explores Relation\nof manipulations: \\ding{186} we propose a new sequential contrastive learning\nscheme to capture relationships between various manipulation types in sequence,\nfurther enhancing the detection of manipulation traces. We conduct extensive\nexperiments in comparison with several state-of-the-art methods, demonstrating\nthe superiority of our method. The code has been released at\nhttps://github.com/OUC-VAS/TSOM.", "comment": "An extension of WACV 2025 (Oral)", "pdf_url": "http://arxiv.org/pdf/2404.13873v5", "cate": "cs.CV", "date": "2024-04-22", "updated": "2025-07-29"}
{"id": "2507.22740", "title": "Foundations for Energy-Aware Zero-Energy Devices: From Energy Sensing to Adaptive Protocols", "authors": ["Onel L. A. López", "Mateen Ashraf", "Samer Nasser", "Gabriel M. de Jesus", "Ritesh Kumar Singh", "Miltiadis C. Filippou", "Jeroen Famaey"], "categories": ["eess.SY", "cs.SY", "94Cxx", "C.2.2; C.2.3; B.4.1; B.4.2; B.4.3; H.1.2"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      31 pags, 16 figs, 8 tables. Submitted to IEEE Proceedings of the IEEE", "url": "http://arxiv.org/abs/2507.22740v1", "summary": "Zero-energy devices (ZEDs) are key enablers of sustainable Internet of Things\nnetworks by operating solely on harvested ambient energy. Their limited and\ndynamic energy budget calls for protocols that are energy-aware and\nintelligently adaptive. However, designing effective energy-aware protocols for\nZEDs requires theoretical models that realistically reflect device constraints.\nIndeed, existing approaches often oversimplify key aspects such as energy\ninformation (EI) acquisition, task-level variability, and energy storage\ndynamics, limiting their practical relevance and transferability. This article\naddresses this gap by offering a structured overview of the key modeling\ncomponents, trade-offs, and limitations involved in energy-aware ZED protocol\ndesign. For this, we dissect EI acquisition methods and costs, characterize\ncore operational tasks, analyze energy usage models and storage constraints,\nand review representative protocol strategies. Moreover, we offer design\ninsights and guidelines on how ZED operation protocols can leverage EI, often\nillustrated through selected in-house examples. Finally, we outline key\nresearch directions to inspire more efficient and scalable protocol solutions\nfor future ZEDs.", "comment": "31 pags, 16 figs, 8 tables. Submitted to IEEE Proceedings of the IEEE", "pdf_url": "http://arxiv.org/pdf/2507.22740v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.15787", "title": "SLR: Automated Synthesis for Scalable Logical Reasoning", "authors": ["Lukas Helff", "Ahmad Omar", "Felix Friedrich", "Antonia Wüst", "Hikaru Shindo", "Rupert Mitchell", "Tim Woydt", "Patrick Schramowski", "and Wolfgang Stammer Kristian Kersting"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15787v3", "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15787v3", "cate": "cs.AI", "date": "2025-06-18", "updated": "2025-07-29"}
{"id": "2501.04300", "title": "HI-PMK: A Data-Dependent Kernel for Incomplete Heterogeneous Data Representation", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Jonathan Wells", "Sunil Aryal"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.04300v3", "summary": "Handling incomplete and heterogeneous data remains a central challenge in\nreal-world machine learning, where missing values may follow complex mechanisms\n(MCAR, MAR, MNAR) and features can be of mixed types (numerical and\ncategorical). Existing methods often rely on imputation, which may introduce\nbias or privacy risks, or fail to jointly address data heterogeneity and\nstructured missingness. We propose the \\textbf{H}eterogeneous\n\\textbf{I}ncomplete \\textbf{P}robability \\textbf{M}ass \\textbf{K}ernel\n(\\textbf{HI-PMK}), a novel data-dependent representation learning approach that\neliminates the need for imputation. HI-PMK introduces two key innovations: (1)\na probability mass-based dissimilarity measure that adapts to local data\ndistributions across heterogeneous features (numerical, ordinal, nominal), and\n(2) a missingness-aware uncertainty strategy (MaxU) that conservatively handles\nall three missingness mechanisms by assigning maximal plausible dissimilarity\nto unobserved entries. Our approach is privacy-preserving, scalable, and\nreadily applicable to downstream tasks such as classification and clustering.\nExtensive experiments on over 15 benchmark datasets demonstrate that HI-PMK\nconsistently outperforms traditional imputation-based pipelines and kernel\nmethods across a wide range of missing data settings. Code is available at:\nhttps://github.com/echoid/Incomplete-Heter-Kernel", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.04300v3", "cate": "cs.LG", "date": "2025-01-08", "updated": "2025-07-29"}
{"id": "2408.02275", "title": "Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes", "authors": ["Prodromos Kolyvakis", "Manos Kamarianakis", "George Papagiannakis"], "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2408.02275v2", "summary": "This paper introduces a novel integration of Large Language Models (LLMs)\nwith Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene\nediting, particularly for object repositioning tasks, which traditionally\nrequires intricate manual processes and specialized expertise. These\nconventional methods typically suffer from reliance on large training datasets\nor lack a formalized language for precise edits. Utilizing CGA as a robust\nformal language, our system, Shenlong, precisely models spatial transformations\nnecessary for accurate object repositioning. Leveraging the zero-shot learning\ncapabilities of pre-trained LLMs, Shenlong translates natural language\ninstructions into CGA operations which are then applied to the scene,\nfacilitating exact spatial transformations within 3D scenes without the need\nfor specialized pre-training. Implemented in a realistic simulation\nenvironment, Shenlong ensures compatibility with existing graphics pipelines.\nTo accurately assess the impact of CGA, we benchmark against robust Euclidean\nSpace baselines, evaluating both latency and accuracy. Comparative performance\nevaluations indicate that Shenlong significantly reduces LLM response times by\n16% and boosts success rates by 9.6% on average compared to the traditional\nmethods. Notably, Shenlong achieves a 100% perfect success rate in common\npractical queries, a benchmark where other systems fall short. These\nadvancements underscore Shenlong's potential to democratize 3D scene editing,\nenhancing accessibility and fostering innovation across sectors such as\neducation, digital entertainment, and virtual reality.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2408.02275v2", "cate": "cs.CV", "date": "2024-08-05", "updated": "2025-07-29"}
{"id": "2507.22760", "title": "Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision", "authors": ["Samuel Teuber", "Debasmita Lohar", "Bernhard Beckert"], "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.LO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures, 1 table; Accepted at FMCAD 2025", "url": "http://arxiv.org/abs/2507.22760v1", "summary": "As neural networks (NNs) become increasingly prevalent in safety-critical\nneural network-controlled cyber-physical systems (NNCSs), formally guaranteeing\ntheir safety becomes crucial. For these systems, safety must be ensured\nthroughout their entire operation, necessitating infinite-time horizon\nverification. To verify the infinite-time horizon safety of NNCSs, recent\napproaches leverage Differential Dynamic Logic (dL). However, these dL-based\nguarantees rely on idealized, real-valued NN semantics and fail to account for\nroundoff errors introduced by finite-precision implementations. This paper\nbridges the gap between theoretical guarantees and real-world implementations\nby incorporating robustness under finite-precision perturbations -- in sensing,\nactuation, and computation -- into the safety verification. We model the\nproblem as a hybrid game between a good Demon, responsible for control actions,\nand a bad Angel, introducing perturbations. This formulation enables formal\nproofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this\nbound, we employ state-of-the-art mixed-precision fixed-point tuners to\nsynthesize sound and efficient implementations, thus providing a complete\nend-to-end solution. We evaluate our approach on case studies from the\nautomotive and aeronautics domains, producing efficient NN implementations with\nrigorous infinite-time horizon safety guarantees.", "comment": "15 pages, 3 figures, 1 table; Accepted at FMCAD 2025", "pdf_url": "http://arxiv.org/pdf/2507.22760v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.05629", "title": "Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses", "authors": ["Yuan An", "John Liu", "Niyam Acharya", "Ruhma Hashmi"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05629v2", "summary": "Retrieval practice is a well-established pedagogical technique known to\nsignificantly enhance student learning and knowledge retention. However,\ngenerating high-quality retrieval practice questions is often time-consuming\nand labor intensive for instructors, especially in rapidly evolving technical\nsubjects. Large Language Models (LLMs) offer the potential to automate this\nprocess by generating questions in response to prompts, yet the effectiveness\nof LLM-generated retrieval practice on student learning remains to be\nestablished. In this study, we conducted an empirical study involving two\ncollege-level data science courses, with approximately 60 students. We compared\nlearning outcomes during one week in which students received LLM-generated\nmultiple-choice retrieval practice questions to those from a week in which no\nsuch questions were provided. Results indicate that students exposed to\nLLM-generated retrieval practice achieved significantly higher knowledge\nretention, with an average accuracy of 89%, compared to 73% in the week without\nsuch practice. These findings suggest that LLM-generated retrieval questions\ncan effectively support student learning and may provide a scalable solution\nfor integrating retrieval practice into real-time teaching. However, despite\nthese encouraging outcomes and the potential time-saving benefits, cautions\nmust be taken, as the quality of LLM-generated questions can vary. Instructors\nmust still manually verify and revise the generated questions before releasing\nthem to students.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05629v2", "cate": "cs.AI", "date": "2025-07-08", "updated": "2025-07-29"}
{"id": "2501.07237", "title": "Wavelet Meets Adam: Compressing Gradients for Memory-Efficient Training", "authors": ["Ziqing Wen", "Ping Luo", "Jiahuan Wang", "Xiaoge Deng", "Jinping Zou", "Kun Yuan", "Tao Sun", "Dongsheng Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.07237v3", "summary": "Large language models (LLMs) have shown impressive performance across a range\nof natural language processing tasks. However, their vast number of parameters\nintroduces significant memory challenges during training, particularly when\nusing memory-intensive optimizers like Adam. Existing memory-efficient\nalgorithms often rely on techniques such as singular value decomposition\nprojection or weight freezing. While these approaches help alleviate memory\nconstraints, they generally produce suboptimal results compared to full-rank\nupdates. In this paper, we investigate the memory-efficient method beyond\nlow-rank training, proposing a novel solution called Gradient Wavelet Transform\n(GWT), which applies wavelet transforms to gradients in order to significantly\nreduce the memory requirements for maintaining optimizer states. We demonstrate\nthat GWT can be seamlessly integrated with memory-intensive optimizers,\nenabling efficient training without sacrificing performance. Through extensive\nexperiments on both pre-training and fine-tuning tasks, we show that GWT\nachieves state-of-the-art performance compared with advanced memory-efficient\noptimizers and full-rank approaches in terms of both memory usage and training\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.07237v3", "cate": "cs.LG", "date": "2025-01-13", "updated": "2025-07-30"}
{"id": "2410.12342", "title": "Fuse Before Transfer: Knowledge Fusion for Heterogeneous Distillation", "authors": ["Guopeng Li", "Qiang Wang", "Ke Yan", "Shouhong Ding", "Yuan Gao", "Gui-Song Xia"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2410.12342v2", "summary": "Most knowledge distillation (KD) methodologies predominantly focus on\nteacher-student pairs with similar architectures, such as both being\nconvolutional neural networks (CNNs). However, the potential and flexibility of\nKD can be greatly improved by expanding it to novel Cross-Architecture KD\n(CAKD), where the knowledge of homogeneous and heterogeneous teachers can be\ntransferred flexibly to a given student. The primary challenge in CAKD lies in\nthe substantial feature gaps between heterogeneous models, originating from the\ndistinction of their inherent inductive biases and module functions. To this\nend, we introduce an assistant model as a bridge to facilitate smooth feature\nknowledge transfer between heterogeneous teachers and students. More\nimportantly, within our proposed design principle, the assistant model combines\nthe advantages of cross-architecture inductive biases and module functions by\nmerging convolution and attention modules derived from both student and teacher\nmodule functions. Furthermore, we observe that heterogeneous features exhibit\ndiverse spatial distributions in CAKD, hindering the effectiveness of\nconventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a\nspatial-agnostic InfoNCE loss to align features after spatial smoothing,\nthereby improving the feature alignments in CAKD. Our proposed method is\nevaluated across some homogeneous model pairs and arbitrary heterogeneous\ncombinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance\nfor distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on\nImageNet-1K. Our code and models will be released.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2410.12342v2", "cate": "cs.CV", "date": "2024-10-16", "updated": "2025-07-29"}
{"id": "2507.22778", "title": "Cluster Synchronization and Phase Cohesiveness of Kuramoto Oscillators via Mean-phase Feedback Control and Pacemakers", "authors": ["Ryota Kokubo", "Rui Kato", "Hideaki Ishii"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      13 pages, 11 figures", "url": "http://arxiv.org/abs/2507.22778v1", "summary": "Brain networks typically exhibit characteristic synchronization patterns\nwhere several synchronized clusters coexist. On the other hand, neurological\ndisorders are considered to be related to pathological synchronization such as\nexcessive synchronization of large populations of neurons. Motivated by these\nphenomena, this paper presents two approaches to control the cluster\nsynchronization and the cluster phase cohesiveness of Kuramoto oscillators. One\nis based on feeding back the mean phases to the clusters, and the other is\nbased on the use of pacemakers. First, we show conditions on the feedback gains\nand the pacemaker weights for the network to achieve cluster synchronization.\nThen, we propose a method to find optimal feedback gains through convex\noptimization. Second, we show conditions on the feedback gains and the\npacemaker weights for the network to achieve cluster phase cohesiveness. A\nnumerical example demonstrates the effectiveness of the proposed methods.", "comment": "13 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.22778v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.11482", "title": "Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light", "authors": ["Mani Hamidi", "Terrence W. Deacon"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11482v3", "summary": "Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11482v3", "cate": "cs.AI", "date": "2025-07-15", "updated": "2025-07-28"}
{"id": "2501.11847", "title": "A Survey on Memory-Efficient Transformer-Based Model Training in AI for Science", "authors": ["Kaiyuan Tian", "Linbo Qiao", "Baihui Liu", "Gongqingjian Jiang", "Shanshan Li", "Dongsheng Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The article has been accepted by Frontiers of Computer Science (FCS), with the DOI: { https://doi.org/10.1007/s11704-025-50302-6 }", "url": "http://arxiv.org/abs/2501.11847v2", "summary": "Scientific research faces high costs and inefficiencies with traditional\nmethods, but the rise of deep learning and large language models (LLMs) offers\ninnovative solutions. This survey reviews transformer-based LLM applications\nacross scientific fields such as biology, medicine, chemistry, and meteorology,\nunderscoring their role in advancing research. However, the continuous\nexpansion of model size has led to significant memory demands, hindering\nfurther development and application of LLMs for science. This survey\nsystematically reviews and categorizes memory-efficient pre-training techniques\nfor large-scale transformers, including algorithm-level, system-level, and\nhardware-software co-optimization. Using AlphaFold 2 as an example, we\ndemonstrate how tailored memory optimization methods can reduce storage needs\nwhile preserving prediction accuracy. By bridging model efficiency and\nscientific application needs, we hope to provide insights for scalable and\ncost-effective LLM training in AI for science.", "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-50302-6}", "pdf_url": "http://arxiv.org/pdf/2501.11847v2", "cate": "cs.LG", "date": "2025-01-21", "updated": "2025-07-29"}
{"id": "2410.16268", "title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree", "authors": ["Shuangrui Ding", "Rui Qian", "Xiaoyi Dong", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Yuwei Guo", "Dahua Lin", "Jiaqi Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project page: this https URL ; github page: this https URL", "url": "http://arxiv.org/abs/2410.16268v3", "summary": "The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.", "comment": "ICCV 2025, Project page:\n  https://mark12ding.github.io/project/SAM2Long/ ; github page:\n  https://github.com/Mark12Ding/SAM2Long/", "pdf_url": "http://arxiv.org/pdf/2410.16268v3", "cate": "cs.CV", "date": "2024-10-21", "updated": "2025-07-29"}
{"id": "2507.22176", "title": "Derivative Estimation from Coarse, Irregular, Noisy Samples: An MLE-Spline Approach", "authors": ["Konstantin E. Avrachenkov", "Leonid B. Freidovich"], "categories": ["stat.ME", "cs.SY", "eess.SY", "math.OC", "math.PR"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22176v1", "summary": "We address numerical differentiation under coarse, non-uniform sampling and\nGaussian noise. A maximum-likelihood estimator with $L_2$-norm constraint on a\nhigher-order derivative is obtained, yielding spline-based solution. We\nintroduce a non-standard parameterization of quadratic splines and develop\nrecursive online algorithms. Two formulations -- quadratic and zero-order --\noffer tradeoff between smoothness and computational speed. Simulations\ndemonstrate superior performance over high-gain observers and super-twisting\ndifferentiators under coarse sampling and high noise, benefiting systems where\nhigher sampling rates are impractical.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22176v1", "cate": "stat.ME", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.19960", "title": "What Does 'Human-Centred AI' Mean?", "authors": ["Olivia Guest"], "categories": ["cs.AI", "I.2.0; K.2; K.4.0"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19960v2", "summary": "While it seems sensible that human-centred artificial intelligence (AI) means\ncentring \"human behaviour and experience,\" it cannot be any other way. AI, I\nargue, is usefully seen as a relationship between technology and humans where\nit appears that artifacts can perform, to a greater or lesser extent, human\ncognitive labour. This is evinced using examples that juxtapose technology with\ncognition, inter alia: abacus versus mental arithmetic; alarm clock versus\nknocker-upper; camera versus vision; and sweatshop versus tailor. Using novel\ndefinitions and analyses, sociotechnical relationships can be analysed into\nvarying types of: displacement (harmful), enhancement (beneficial), and/or\nreplacement (neutral) of human cognitive labour. Ultimately, all AI implicates\nhuman cognition; no matter what. Obfuscation of cognition in the AI context --\nfrom clocks to artificial neural networks -- results in distortion, in slowing\ncritical engagement, perverting cognitive science, and indeed in limiting our\nability to truly centre humans and humanity in the engineering of AI systems.\nTo even begin to de-fetishise AI, we must look the human-in-the-loop in the\neyes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19960v2", "cate": "cs.AI", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2502.12207", "title": "PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN", "authors": ["Jiayu Zhang", "Zhiyu Zhu", "Xinyi Wang", "Silin Liao", "Zhibo Jin", "Flora D. Salim", "Huaming Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Best paper award of ECML-PKDD 2025", "url": "http://arxiv.org/abs/2502.12207v2", "summary": "Deep neural networks have demonstrated remarkable performance across various\ndomains. However, they are vulnerable to adversarial examples, which can lead\nto erroneous predictions. Generative Adversarial Networks (GANs) can leverage\nthe generators and discriminators model to quickly produce high-quality\nadversarial examples. Since both modules train in a competitive and\nsimultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial\nexamples with better transferability compared to traditional methods. However,\nthe generation of perturbations is usually limited to a single iteration,\npreventing these examples from fully exploiting the potential of the methods.\nTo tackle this issue, we introduce a novel approach named Progressive\nAuto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive\niteration mechanism within a progressive generation network to craft\nadversarial examples with enhanced attack capability. We thoroughly evaluate\nour PAR-AdvGAN method with a large-scale experiment, demonstrating its superior\nperformance over various state-of-the-art black-box adversarial attacks, as\nwell as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the\nadversarial example generation, i.e., achieving the speeds of up to 335.5\nframes per second on Inception-v3 model, outperforming the gradient-based\ntransferable attack algorithms. Our code is available at:\nhttps://github.com/LMBTough/PAR", "comment": "Best paper award of ECML-PKDD 2025", "pdf_url": "http://arxiv.org/pdf/2502.12207v2", "cate": "cs.LG", "date": "2025-02-16", "updated": "2025-07-29"}
{"id": "2411.16072", "title": "Language Driven Occupancy Prediction", "authors": ["Zhu Yu", "Bowen Pang", "Lizhe Liu", "Runmin Zhang", "Qiang Li", "Si-Yuan Cao", "Maochun Luo", "Mingxia Chen", "Sheng Yang", "Hui-Liang Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025; Project Page: this https URL", "url": "http://arxiv.org/abs/2411.16072v3", "summary": "We introduce LOcc, an effective and generalizable framework for\nopen-vocabulary occupancy (OVO) prediction. Previous approaches typically\nsupervise the networks through coarse voxel-to-text correspondences via image\nfeatures as intermediates or noisy and sparse correspondences from voxel-based\nmodel-view projections. To alleviate the inaccurate supervision, we propose a\nsemantic transitive labeling pipeline to generate dense and fine-grained 3D\nlanguage occupancy ground truth. Our pipeline presents a feasible way to dig\ninto the valuable semantic information of images, transferring text labels from\nimages to LiDAR point clouds and ultimately to voxels, to establish precise\nvoxel-to-text correspondences. By replacing the original prediction head of\nsupervised occupancy models with a geometry head for binary occupancy states\nand a language head for language features, LOcc effectively uses the generated\nlanguage ground truth to guide the learning of 3D language volume. Through\nextensive experiments, we demonstrate that our transitive semantic labeling\npipeline can produce more accurate pseudo-labeled ground truth, diminishing\nlabor-intensive human annotations. Additionally, we validate LOcc across\nvarious architectures, where all models consistently outperform\nstate-of-the-art zero-shot occupancy prediction approaches on the\nOcc3D-nuScenes dataset.", "comment": "ICCV 2025; Project Page: https://github.com/pkqbajng/LOcc", "pdf_url": "http://arxiv.org/pdf/2411.16072v3", "cate": "cs.CV", "date": "2024-11-25", "updated": "2025-07-30"}
{"id": "2507.22188", "title": "Deployment of Objects with a Soft Everting Robot", "authors": ["Ethan DeVries", "Jack Ferlazzo", "Mustafa Ugur", "Laura H. Blumenschein"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figures, This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.22188v1", "summary": "Soft everting robots present significant advantages over traditional rigid\nrobots, including enhanced dexterity, improved environmental interaction, and\nsafe navigation in unpredictable environments. While soft everting robots have\nbeen widely demonstrated for exploration type tasks, their potential to move\nand deploy payloads in such tasks has been less investigated, with previous\nwork focusing on sensors and tools for the robot. Leveraging the navigation\ncapabilities, and deployed body, of the soft everting robot to deliver payloads\nin hazardous areas, e.g. carrying a water bottle to a person stuck under\ndebris, would represent a significant capability in many applications. In this\nwork, we present an analysis of how soft everting robots can be used to deploy\nlarger, heavier payloads through the inside of the robot. We analyze both what\nobjects can be deployed and what terrain features they can be carried through.\nBuilding on existing models, we present methods to quantify the effects of\npayloads on robot growth and self-support, and develop a model to predict\npayload slip. We then experimentally quantify payload transport using soft\neverting robot with a variety of payload shapes, sizes, and weights and though\na series of tasks: steering, vertical transport, movement through holes, and\nmovement across gaps. Overall, the results show that we can transport payloads\nin a variety of shapes and up to 1.5kg in weight and that we can move through\ncircular apertures with as little as 0.01cm clearance around payloads, carry\nout discrete turns up to 135 degrees, and move across unsupported gaps of 1.15m\nin length.", "comment": "9 pages, 10 figures, This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.22188v1", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2403.06963", "title": "The pitfalls of next-token prediction", "authors": ["Gregor Bachmann", "Vaishnavh Nagarajan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ICML 2024", "url": "http://arxiv.org/abs/2403.06963v3", "summary": "Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this emerging concern and correct popular misconceptions\nsurrounding it, and advocate a simple multi-token objective.\n  As a starting point, we argue that the two often-conflated phases of\nnext-token prediction -- autoregressive inference and teacher-forced training\n-- must be treated distinctly. The popular criticism that errors can compound\nduring autoregressive inference, crucially assumes that teacher-forcing has\nlearned an accurate next-token predictor. This assumption sidesteps a more\ndeep-rooted problem we expose: in certain classes of tasks, teacher-forcing can\nsimply fail to learn an accurate next-token predictor in the first place. We\ndescribe a general mechanism of how teacher-forcing can fail, and design a\nminimal planning task where both the Transformer and the Mamba architecture\nempirically fail in that manner -- remarkably, despite the task being\nstraightforward to learn.\n  Finally, we provide preliminary evidence that this failure can be resolved\nusing _teacherless_ training, a simple modification using dummy tokens that\npredicts multiple tokens in advance. We hope this finding can ground future\ndebates and inspire explorations beyond the next-token prediction paradigm. We\nmake our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures", "comment": "ICML 2024", "pdf_url": "http://arxiv.org/pdf/2403.06963v3", "cate": "cs.CL", "date": "2024-03-11", "updated": "2025-07-29"}
{"id": "2502.12507", "title": "Multi-branch of Attention Yields Accurate Results for Tabular Data", "authors": ["Xuechen Li", "Yupeng Li", "Jian Liu", "Xiaolin Jin", "Xin Hu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 3 figures", "url": "http://arxiv.org/abs/2502.12507v2", "summary": "Tabular data inherently exhibits significant feature heterogeneity, but\nexisting transformer-based methods lack specialized mechanisms to handle this\nproperty. To bridge the gap, we propose MAYA, an encoder-decoder\ntransformer-based framework. In the encoder, we design a Multi-Branch of\nAttention (MBA) that constructs multiple parallel attention branches and\naverages the features at each branch, effectively fusing heterogeneous features\nwhile limiting parameter growth. Additionally, we employ collaborative learning\nwith a dynamic consistency weight constraint to produce more robust\nrepresentations. In the decoder stage, cross-attention is utilized to\nseamlessly integrate tabular data with corresponding label features. This\ndual-attention mechanism effectively captures both intra-instance and\ninter-instance interactions. We evaluate the proposed method on a wide range of\ndatasets and compare it with other state-of-the-art transformer-based methods.\nExtensive experiments demonstrate that our model achieves superior performance\namong transformer-based methods in both tabular classification and regression\ntasks.", "comment": "19 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2502.12507v2", "cate": "cs.LG", "date": "2025-02-18", "updated": "2025-07-29"}
{"id": "2411.17489", "title": "Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions", "authors": ["Nicolai Hermann", "Jorge Condor", "Piotr Didyk"], "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "68T07, 68T45, 68T10", "I.4; I.3; I.2"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.17489v3", "summary": "Modern reconstruction techniques can effectively model complex 3D scenes from\nsparse 2D views. However, automatically assessing the quality of novel views\nand identifying artifacts is challenging due to the lack of ground truth images\nand the limitations of no-reference image metrics in predicting reliable\nartifact maps. The absence of such metrics hinders assessment of the quality of\nnovel views and limits the adoption of post-processing techniques, such as\ninpainting, to enhance reconstruction quality. To tackle this, recent work has\nestablished a new category of metrics (cross-reference), predicting image\nquality solely by leveraging context from alternate viewpoint captures\n(arXiv:2404.14409). In this work, we propose a new cross-reference metric,\nPuzzle Similarity, which is designed to localize artifacts in novel views. Our\napproach utilizes image patch statistics from the training views to establish a\nscene-specific distribution, later used to identify poorly reconstructed\nregions in the novel views. Given the lack of good measures to evaluate\ncross-reference methods in the context of 3D reconstruction, we collected a\nnovel human-labeled dataset of artifact and distortion maps in unseen\nreconstructed views. Through this dataset, we demonstrate that our method\nachieves state-of-the-art localization of artifacts in novel views, correlating\nwith human assessment, even without aligned references. We can leverage our new\nmetric to enhance applications like automatic image restoration, guided\nacquisition, or 3D reconstruction from sparse inputs. Find the project page at\nhttps://nihermann.github.io/puzzlesim/ .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.17489v3", "cate": "cs.CV", "date": "2024-11-26", "updated": "2025-07-29"}
{"id": "2507.22239", "title": "Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems", "authors": ["Muhammad Sharshar", "Ahmad Mohammad Saber", "Davor Svetinovic", "Amr M. Youssef", "Deepa Kundur", "Ehab F. El-Saadany"], "categories": ["cs.CR", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted Publication", "url": "http://arxiv.org/abs/2507.22239v1", "summary": "The increasing digitization of smart grids has improved operational\nefficiency but also introduced new cybersecurity vulnerabilities, such as False\nData Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC)\nsystems. While machine learning (ML) and deep learning (DL) models have shown\npromise in detecting such attacks, their opaque decision-making limits operator\ntrust and real-world applicability. This paper proposes a hybrid framework that\nintegrates lightweight ML-based attack detection with natural language\nexplanations generated by Large Language Models (LLMs). Classifiers such as\nLightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s\ninference latency. Upon detecting a cyberattack, the system invokes LLMs,\nincluding GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate\nhuman-readable explanation of the event. Evaluated on 100 test samples, GPT-4o\nmini with 20-shot prompting achieved 93% accuracy in identifying the attack\ntarget, a mean absolute error of 0.075 pu in estimating attack magnitude, and\n2.19 seconds mean absolute error (MAE) in estimating attack onset. These\nresults demonstrate that the proposed framework effectively balances real-time\ndetection with interpretable, high-fidelity explanations, addressing a critical\nneed for actionable AI in smart grid cybersecurity.", "comment": "Accepted Publication", "pdf_url": "http://arxiv.org/pdf/2507.22239v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2409.11274", "title": "Task Arithmetic for Language Expansion in Speech Translation", "authors": ["Yao-Fei Cheng", "Hayato Futami", "Yosuke Kashiwagi", "Emiru Tsunoo", "Wen Shen Teo", "Siddhant Arora", "Shinji Watanabe"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.11274v3", "summary": "Recent progress in large language models (LLMs) has gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-tuned speech translation (ST). However, expanding language pairs is\ncostly due to re-training on combined new and previous datasets. To address\nthis, we aim to build a one-to-many ST system from existing one-to-one ST\nsystems using task arithmetic without re-training. Direct application of task\narithmetic in ST leads to language confusion; therefore, we introduce an\naugmented task arithmetic method incorporating a language control model to\nensure correct target language generation. Our experiments on MuST-C and\nCoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains\nof 8.87 and 11.83. In addition, we demonstrate our framework can extend to\nlanguage pairs lacking paired ST training data or pre-trained ST models by\nsynthesizing ST models based on existing machine translation (MT) and ST models\nvia task analogies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.11274v3", "cate": "cs.CL", "date": "2024-09-17", "updated": "2025-07-29"}
{"id": "2502.16299", "title": "A calibration test for evaluating set-based epistemic uncertainty representations", "authors": ["Mira Jürgens", "Thomas Mortier", "Eyke Hüllermeier", "Viktor Bengs", "Willem Waegeman"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16299v2", "summary": "The accurate representation of epistemic uncertainty is a challenging yet\nessential task in machine learning. A widely used representation corresponds to\nconvex sets of probabilistic predictors, also known as credal sets. One popular\nway of constructing these credal sets is via ensembling or specialized\nsupervised learning methods, where the epistemic uncertainty can be quantified\nthrough measures such as the set size or the disagreement among members. In\nprinciple, these sets should contain the true data-generating distribution. As\na necessary condition for this validity, we adopt the strongest notion of\ncalibration as a proxy. Concretely, we propose a novel statistical test to\ndetermine whether there is a convex combination of the set's predictions that\nis calibrated in distribution. In contrast to previous methods, our framework\nallows the convex combination to be instance dependent, recognizing that\ndifferent ensemble members may be better calibrated in different regions of the\ninput space. Moreover, we learn this combination via proper scoring rules,\nwhich inherently optimize for calibration. Building on differentiable,\nkernel-based estimators of calibration errors, we introduce a nonparametric\ntesting procedure and demonstrate the benefits of capturing instance-level\nvariability on of synthetic and real-world experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16299v2", "cate": "cs.LG", "date": "2025-02-22", "updated": "2025-07-29"}
{"id": "2411.17799", "title": "Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator", "authors": ["Ronglai Zuo", "Rolandos Alexandros Potamias", "Evangelos Ververas", "Jiankang Deng", "Stefanos Zafeiriou"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2411.17799v3", "summary": "Sign language is a visual language that encompasses all linguistic features\nof natural languages and serves as the primary communication method for the\ndeaf and hard-of-hearing communities. Although many studies have successfully\nadapted pretrained language models (LMs) for sign language translation\n(sign-to-text), the reverse task-sign language generation\n(text-to-sign)-remains largely unexplored. In this work, we introduce a\nmultilingual sign language model, Signs as Tokens (SOKE), which can generate 3D\nsign avatars autoregressively from text inputs using a pretrained LM. To align\nsign language with the LM, we leverage a decoupled tokenizer that discretizes\ncontinuous signs into token sequences representing various body parts. During\ndecoding, unlike existing approaches that flatten all part-wise tokens into a\nsingle sequence and predict one token at a time, we propose a multi-head\ndecoding method capable of predicting multiple tokens simultaneously. This\napproach improves inference efficiency while maintaining effective information\nfusion across different body parts. To further ease the generation process, we\npropose a retrieval-enhanced SLG approach, which incorporates external sign\ndictionaries to provide accurate word-level signs as auxiliary conditions,\nsignificantly improving the precision of generated signs. Extensive qualitative\nand quantitative evaluations demonstrate the effectiveness of SOKE.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2411.17799v3", "cate": "cs.CV", "date": "2024-11-26", "updated": "2025-07-29"}
{"id": "2507.22243", "title": "Modified Smith predictor for unstable linear systems", "authors": ["Anton Pyrkin", "Konstantin Kalinin"], "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY", "math.DS"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      in Russian language", "url": "http://arxiv.org/abs/2507.22243v1", "summary": "The paper presents a new control algorithm for unstable linear systems with\ninput delay. In comparison with known analogues, the control law has been\ndesigned, which is a modification of the Smith predictor, and is the simplest\none to implement without requiring complex integration methods. At the same\ntime, the problem of stabilization of a closed system is effectively solved,\nensuring the boundedness of all state variables and the exponential stability\nof the equilibrium point.", "comment": "in Russian language", "pdf_url": "http://arxiv.org/pdf/2507.22243v1", "cate": "math.OC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22159", "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian", "authors": ["Vanessa Rebecca Wiyono", "David Anugraha", "Ayu Purwarianti", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.22159v1", "summary": "Over 200 million people speak Indonesian, yet the language remains\nsignificantly underrepresented in preference-based research for large language\nmodels (LLMs). Most existing multilingual datasets are derived from English\ntranslations, often resulting in content that lacks cultural and linguistic\nauthenticity. To address this gap, we introduce IndoPref, the first fully\nhuman-authored and multi-domain Indonesian preference dataset specifically\ndesigned to evaluate the naturalness and quality of LLM-generated text. All\nannotations are natively written in Indonesian and evaluated using\nKrippendorff's alpha, demonstrating strong inter-annotator agreement.\nAdditionally, we benchmark the dataset across multiple LLMs and assess the\noutput quality of each model.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.22159v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2409.18924", "title": "Simulated patient systems are intelligent when powered by large language model-based AI agents", "authors": ["Huizi Yu", "Jiayan Zhou", "Lingyao Li", "Shan Chen", "Jack Gallifant", "Anye Shi", "Xiang Li", "Jingxian He", "Wenyue Hua", "Mingyu Jin", "Guang Chen", "Yang Zhou", "Zhao Li", "Trisha Gupte", "Ming-Li Chen", "Zahra Azizi", "Yongfeng Zhang", "Yanqiu Xing", "Themistocles L. Danielle S. Bitterman", "Themistocles L. Assimes", "Xin Ma", "Lin Lu", "Lizhou Fan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      64 pages, 14 figures, 16 tables", "url": "http://arxiv.org/abs/2409.18924v3", "summary": "Simulated patient systems play an important role in modern medical education\nand research, providing safe, integrative medical training environments and\nsupporting clinical decision-making simulations. We developed AIPatient, an\nintelligent simulated patient system powered by large language model-based AI\nagents. The system incorporates the Retrieval Augmented Generation (RAG)\nframework, powered by six task-specific LLM-based AI agents for complex\nreasoning. For simulation reality, the system is also powered by the AIPatient\nKG (Knowledge Graph), built with de-identified real patient data from the\nMedical Information Mart for Intensive Care (MIMIC)-III database. Primary\noutcomes showcase the system's intelligence, including the system's accuracy in\nElectronic Record (EHR)-based medical Question Answering (QA), readability,\nrobustness, and stability. The system achieved a QA accuracy of 94.15% when all\nsix AI agents present, surpassing benchmarks with partial or no agent\nintegration. Its knowledgebase demonstrated high validity (F1 score=0.89).\nReadability scores showed median Flesch Reading Ease at 77.23 and median Flesch\nKincaid Grade at 5.6, indicating accessibility to all medical professionals.\nRobustness and stability were confirmed with non-significant variance (ANOVA\nF-value=0.6126, p > 0.1; F-value=0.782, p > 0.1). A user study with medical\nstudents further demonstrated that AIPatient offers high fidelity, strong\nusability, and effective educational value, performing comparably or better\nthan human-simulated patients in medical history-taking scenarios. The\npromising intelligence of the AIPatient system highlights its potential to\nsupport a wide range of applications, including medical education, model\nevaluation, and system integration.", "comment": "64 pages, 14 figures, 16 tables", "pdf_url": "http://arxiv.org/pdf/2409.18924v3", "cate": "cs.CL", "date": "2024-09-27", "updated": "2025-07-29"}
{"id": "2503.03443", "title": "Conceptualizing Uncertainty: A Concept-based Approach to Explaining Uncertainty", "authors": ["Isaac Roberts", "Alexander Schulz", "Sarah Schroeder", "Fabian Hinder", "Barbara Hammer"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.03443v2", "summary": "Uncertainty in machine learning refers to the degree of confidence or lack\nthereof in a model's predictions. While uncertainty quantification methods\nexist, explanations of uncertainty, especially in high-dimensional settings,\nremain an open challenge. Existing work focuses on feature attribution\napproaches which are restricted to local explanations. Understanding\nuncertainty, its origins, and characteristics on a global scale is crucial for\nenhancing interpretability and trust in a model's predictions. In this work, we\npropose to explain the uncertainty in high-dimensional data classification\nsettings by means of concept activation vectors which give rise to local and\nglobal explanations of uncertainty. We demonstrate the utility of the generated\nexplanations by leveraging them to refine and improve our model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.03443v2", "cate": "cs.LG", "date": "2025-03-05", "updated": "2025-07-29"}
{"id": "2412.03248", "title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning", "authors": ["Yiwu Zhong", "Zhuoming Liu", "Yin Li", "Liwei Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2412.03248v2", "summary": "Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, at a\nsimilar computational cost, our method outperforms the state-of-the-art methods\nin long video understanding (e.g., $\\textbf{+4.6}$ on MLVU). Additionally, our\nin-depth analysis provides insights into token redundancy and LLM layer\nbehaviors, offering guidance for future research in designing efficient\nmulti-modal LLMs. Our code is available at https://github.com/LaVi-Lab/AIM.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2412.03248v2", "cate": "cs.CV", "date": "2024-12-04", "updated": "2025-07-29"}
{"id": "2507.22340", "title": "Resilient State Recovery using Prior Measurement Support Information", "authors": ["Yu Zheng", "Olugbenga Moses Anubi", "Warren E. Dixon"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      To be published in SIAM Journal on Control and Optimization", "url": "http://arxiv.org/abs/2507.22340v1", "summary": "Resilient state recovery of cyber-physical systems has attracted much\nresearch attention due to the unique challenges posed by the tight coupling\nbetween communication, computation, and the underlying physics of such systems.\nBy modeling attacks as additive adversary signals to a sparse subset of\nmeasurements, this resilient recovery problem can be formulated as an error\ncorrection problem. To achieve exact state recovery, most existing results\nrequire less than $50\\%$ of the measurement nodes to be compromised, which\nlimits the resiliency of the estimators. In this paper, we show that observer\nresiliency can be further improved by incorporating data-driven prior\ninformation. We provide an analytical bridge between the precision of prior\ninformation and the resiliency of the estimator. By quantifying the\nrelationship between the estimation error of the weighted $\\ell_1$ observer and\nthe precision of the support prior. This quantified relationship provides\nguidance for the estimator's weight design to achieve optimal resiliency.\nSeveral numerical simulations and an application case study are presented to\nvalidate the theoretical claims.", "comment": "To be published in SIAM Journal on Control and Optimization", "pdf_url": "http://arxiv.org/pdf/2507.22340v1", "cate": "math.OC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22168", "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles", "authors": ["Kimberly Le Truong", "Riccardo Fogliato", "Hoda Heidari", "Zhiwei Steven Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22168v1", "summary": "Current benchmarks for evaluating Large Language Models (LLMs) often do not\nexhibit enough writing style diversity, with many adhering primarily to\nstandardized conventions. Such benchmarks do not fully capture the rich variety\nof communication patterns exhibited by humans. Thus, it is possible that LLMs,\nwhich are optimized on these benchmarks, may demonstrate brittle performance\nwhen faced with \"non-standard\" input. In this work, we test this hypothesis by\nrewriting evaluation prompts using persona-based LLM prompting, a low-cost\nmethod to emulate diverse writing styles. Our results show that, even with\nidentical semantic content, variations in writing style and prompt formatting\nsignificantly impact the estimated performance of the LLM under evaluation.\nNotably, we identify distinct writing styles that consistently trigger either\nlow or high performance across a range of models and tasks, irrespective of\nmodel family, size, and recency. Our work offers a scalable approach to augment\nexisting benchmarks, improving the external validity of the assessments they\nprovide for measuring LLM performance across linguistic variations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22168v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2412.03347", "title": "DIVE: Taming DINO for Subject-Driven Video Editing", "authors": ["Yi Huang", "Wei Xiong", "He Zhang", "Chaoqi Chen", "Jianzhuang Liu", "Mingfu Yan", "Shifeng Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2412.03347v2", "summary": "Building on the success of diffusion models in image generation and editing,\nvideo editing has recently gained substantial attention. However, maintaining\ntemporal consistency and motion alignment still remains challenging. To address\nthese issues, this paper proposes DINO-guided Video Editing (DIVE), a framework\ndesigned to facilitate subject-driven editing in source videos conditioned on\neither target text prompts or reference images with specific identities. The\ncore of DIVE lies in leveraging the powerful semantic features extracted from a\npretrained DINOv2 model as implicit correspondences to guide the editing\nprocess. Specifically, to ensure temporal motion consistency, DIVE employs DINO\nfeatures to align with the motion trajectory of the source video. For precise\nsubject editing, DIVE incorporates the DINO features of reference images into a\npretrained text-to-image model to learn Low-Rank Adaptations (LoRAs),\neffectively registering the target subject's identity. Extensive experiments on\ndiverse real-world videos demonstrate that our framework can achieve\nhigh-quality editing results with robust motion consistency, highlighting the\npotential of DINO to contribute to video editing. Project page:\nhttps://dino-video-editing.github.io", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2412.03347v2", "cate": "cs.CV", "date": "2024-12-04", "updated": "2025-07-29"}
{"id": "2504.15458", "title": "Compton Form Factor Extraction using Quantum Deep Neural Networks", "authors": ["Brandon B. Le", "Dustin Keller"], "categories": ["cs.LG", "hep-ph", "nucl-th", "quant-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      36 pages, 17 figures. v2: major revisions", "url": "http://arxiv.org/abs/2504.15458v2", "summary": "We present an extraction of Compton Form Factors (CFFs) from Deeply Virtual\nCompton Scattering (DVCS) experiments conducted at Thomas Jefferson National\nAccelerator Facility, utilizing Quantum Deep Neural Networks (QDNNs). The\nanalysis employs the standard Belitsky, Kirchner, and M\\\"uller formalism at\ntwist-two, complemented by a fitting procedure designed to minimize model\ndependence in a manner analogous to conventional local fits. A pseudodata\nextraction test of the CFFs is performed using both Classical Deep Neural\nNetworks (CDNNs) and QDNNs, with a detailed comparative analysis. Results\nindicate that QDNNs can outperform CDNNs in particular cases, offering enhanced\npredictive accuracy and precision even with limited model complexity. Motivated\nby this, we develop a metric to quantify the extent of the quantum advantage\nbased on characteristics of DVCS experimental data. These findings underscore\nthe promising role of QDNNs in advancing future investigations into\nmultidimensional parton distributions and hadronic physics.", "comment": "36 pages, 17 figures. v2: major revisions", "pdf_url": "http://arxiv.org/pdf/2504.15458v2", "cate": "cs.LG", "date": "2025-04-21", "updated": "2025-07-29"}
{"id": "2412.06340", "title": "UniPaint: Unified Space-time Video Inpainting via Mixture-of-Experts", "authors": ["Zhen Wan", "Chenyang Qi", "Zhiheng Liu", "Tao Gui", "Yue Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 1st Workshop on Human-Interactive Generation and Editing (poster)", "url": "http://arxiv.org/abs/2412.06340v2", "summary": "In this paper, we present UniPaint, a unified generative space-time video\ninpainting framework that enables spatial-temporal inpainting and\ninterpolation. Different from existing methods that treat video inpainting and\nvideo interpolation as two distinct tasks, we leverage a unified inpainting\nframework to tackle them and observe that these two tasks can mutually enhance\nsynthesis performance. Specifically, we first introduce a plug-and-play\nspace-time video inpainting adapter, which can be employed in various\npersonalized models. The key insight is to propose a Mixture of Experts (MoE)\nattention to cover various tasks. Then, we design a spatial-temporal masking\nstrategy during the training stage to mutually enhance each other and improve\nperformance. UniPaint produces high-quality and aesthetically pleasing results,\nachieving the best quantitative results across various tasks and scale setups.\nThe code and checkpoints are available at\n$\\href{https://github.com/mmmmm-w/UniPaint}{this \\ repository}$.", "comment": "ICCV 1st Workshop on Human-Interactive Generation and Editing\n  (poster)", "pdf_url": "http://arxiv.org/pdf/2412.06340v2", "cate": "cs.CV", "date": "2024-12-09", "updated": "2025-07-29"}
{"id": "2507.22385", "title": "Set Invariance with Probability One for Controlled Diffusion: Score-based Approach", "authors": ["Wenqing Wang", "Alexis M. H. Teter", "Murat Arcak", "Abhishek Halder"], "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY", "math.PR", "stat.ME"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22385v1", "summary": "Given a controlled diffusion and a connected, bounded, Lipschitz set, when is\nit possible to guarantee controlled set invariance with probability one? In\nthis work, we answer this question by deriving the necessary and sufficient\nconditions for the same in terms of gradients of certain log-likelihoods --\na.k.a. score vector fields -- for two cases: given finite time horizon and\ninfinite time horizon. The deduced conditions comprise a score-based test that\nprovably certifies or falsifies the existence of Markovian controllers for\ngiven controlled set invariance problem data. Our results are constructive in\nthe sense when the problem data passes the proposed test, we characterize all\ncontrollers guaranteeing the desired set invariance. When the problem data\nfails the proposed test, there does not exist a controller that can accomplish\nthe desired set invariance with probability one. The computation in the\nproposed tests involve solving certain Dirichlet boundary value problems, and\nin the finite horizon case, can also account for additional constraint of\nhitting a target subset at the terminal time. We illustrate the results using\nseveral semi-analytical and numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22385v1", "cate": "math.OC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22187", "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models", "authors": ["Adam M. Morgan", "Adeen Flinker"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22187v1", "summary": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22187v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2501.04873", "title": "Back Home: A Computer Vision Solution to Seashell Identification for Ecological Restoration", "authors": ["Alexander Valverde", "Luis Solano", "André Montoya"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (CV4E Workshop)", "url": "http://arxiv.org/abs/2501.04873v4", "summary": "Illegal souvenir collection strips an estimated five tonnes of seashells from\nCosta Rica's beaches each year. Yet, once these specimens are seized, their\ncoastal origin -- Pacific or Caribbean -- cannot be verified easily due to the\nlack of information, preventing their return when confiscated by local\nauthorities. To solve this issue, we introduce BackHome19K, the first\nlarge-scale image corpus (19,058 photographs, 516 species) annotated with\ncoast-level labels, and propose a lightweight pipeline that infers provenance\nin real time on a mobile-grade CPU. A trained anomaly filter pre-screens\nuploads, increasing robustness to user-generated noise. On a held-out test set,\nthe classifier attains 86.3% balanced accuracy, while the filter rejects 93% of\n180 out-of-domain objects with zero false negatives. Deployed as a web\napplication, the system has already processed 70,000 shells for wildlife\nofficers in under three seconds per image, enabling confiscated specimens to be\nsafely repatriated to their native ecosystems. The dataset is available at\nhttps://huggingface.co/datasets/FIFCO/BackHome19K", "comment": "ICCV 2025 (CV4E Workshop)", "pdf_url": "http://arxiv.org/pdf/2501.04873v4", "cate": "cs.CV", "date": "2025-01-08", "updated": "2025-07-29"}
{"id": "2505.10774", "title": "Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting", "authors": ["Yueyang Yao", "Jiajun Li", "Xingyuan Dai", "MengMeng Zhang", "Xiaoyan Gong", "Fei-Yue Wang", "Yisheng Lv"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures", "url": "http://arxiv.org/abs/2505.10774v2", "summary": "Time series forecasting is important for applications spanning energy\nmarkets, climate analysis, and traffic management. However, existing methods\nstruggle to effectively integrate exogenous texts and align them with the\nprobabilistic nature of large language models (LLMs). Current approaches either\nemploy shallow text-time series fusion via basic prompts or rely on\ndeterministic numerical decoding that conflict with LLMs' token-generation\nparadigm, which limits contextual awareness and distribution modeling. To\naddress these limitations, we propose CAPTime, a context-aware probabilistic\nmultimodal time series forecasting method that leverages text-informed\nabstraction and autoregressive LLM decoding. Our method first encodes temporal\npatterns using a pretrained time series encoder, then aligns them with textual\ncontexts via learnable interactions to produce joint multimodal\nrepresentations. By combining a mixture of distribution experts with frozen\nLLMs, we enable context-aware probabilistic forecasting while preserving LLMs'\ninherent distribution modeling capabilities. Experiments on diverse time series\nforecasting tasks demonstrate the superior accuracy and generalization of\nCAPTime, particularly in multimodal scenarios. Additional analysis highlights\nits robustness in data-scarce scenarios through hybrid probabilistic decoding.", "comment": "13 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2505.10774v2", "cate": "cs.LG", "date": "2025-05-16", "updated": "2025-07-29"}
{"id": "2501.08654", "title": "ZeroStereo: Zero-shot Stereo Matching from Single Images", "authors": ["Xianqi Wang", "Hao Yang", "Gangwei Xu", "Junda Cheng", "Min Lin", "Yong Deng", "Jinliang Zang", "Yurui Chen", "Xin Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2501.08654v4", "summary": "State-of-the-art supervised stereo matching methods have achieved remarkable\nperformance on various benchmarks. However, their generalization to real-world\nscenarios remains challenging due to the scarcity of annotated real-world\nstereo data. In this paper, we propose ZeroStereo, a novel stereo image\ngeneration pipeline for zero-shot stereo matching. Our approach synthesizes\nhigh-quality right images from arbitrary single images by leveraging pseudo\ndisparities generated by a monocular depth estimation model. Unlike previous\nmethods that address occluded regions by filling missing areas with neighboring\npixels or random backgrounds, we fine-tune a diffusion inpainting model to\nrecover missing details while preserving semantic structure. Additionally, we\npropose Training-Free Confidence Generation, which mitigates the impact of\nunreliable pseudo labels without additional training, and Adaptive Disparity\nSelection, which ensures a diverse and realistic disparity distribution while\npreventing excessive occlusion and foreground distortion. Experiments\ndemonstrate that models trained with our pipeline achieve state-of-the-art\nzero-shot generalization across multiple datasets with only a dataset volume\ncomparable to Scene Flow. Code: https://github.com/Windsrain/ZeroStereo.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2501.08654v4", "cate": "cs.CV", "date": "2025-01-15", "updated": "2025-07-29"}
{"id": "2507.22389", "title": "Safety Evaluation of Motion Plans Using Trajectory Predictors as Forward Reachable Set Estimators", "authors": ["Kaustav Chakraborty", "Zeyuan Feng", "Sushant Veer", "Apoorva Sharma", "Wenhao Ding", "Sever Topan", "Boris Ivanovic", "Marco Pavone", "Somil Bansal"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22389v1", "summary": "The advent of end-to-end autonomy stacks - often lacking interpretable\nintermediate modules - has placed an increased burden on ensuring that the\nfinal output, i.e., the motion plan, is safe in order to validate the safety of\nthe entire stack. This requires a safety monitor that is both complete (able to\ndetect all unsafe plans) and sound (does not flag safe plans). In this work, we\npropose a principled safety monitor that leverages modern multi-modal\ntrajectory predictors to approximate forward reachable sets (FRS) of\nsurrounding agents. By formulating a convex program, we efficiently extract\nthese data-driven FRSs directly from the predicted state distributions,\nconditioned on scene context such as lane topology and agent history. To ensure\ncompleteness, we leverage conformal prediction to calibrate the FRS and\nguarantee coverage of ground-truth trajectories with high probability. To\npreserve soundness in out-of-distribution (OOD) scenarios or under predictor\nfailure, we introduce a Bayesian filter that dynamically adjusts the FRS\nconservativeness based on the predictor's observed performance. We then assess\nthe safety of the ego vehicle's motion plan by checking for intersections with\nthese calibrated FRSs, ensuring the plan remains collision-free under plausible\nfuture behaviors of others. Extensive experiments on the nuScenes dataset show\nour approach significantly improves soundness while maintaining completeness,\noffering a practical and reliable safety monitor for learned autonomy stacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22389v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22201", "title": "The role of media memorability in facilitating startups' access to venture capital funding", "authors": ["L. Toschi", "S. Torrisi", "A. Fronzetti Colladon"], "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22201v1", "summary": "Media reputation plays an important role in attracting venture capital\ninvestment. However, prior research has focused too narrowly on general media\nexposure, limiting our understanding of how media truly influences funding\ndecisions. As informed decision-makers, venture capitalists respond to more\nnuanced aspects of media content. We introduce the concept of media\nmemorability - the media's ability to imprint a startup's name in the memory of\nrelevant investors. Using data from 197 UK startups in the micro and\nnanotechnology sector (funded between 1995 and 2004), we show that media\nmemorability significantly influences investment outcomes. Our findings suggest\nthat venture capitalists rely on detailed cues such as a startup's\ndistinctiveness and connectivity within news semantic networks. This\ncontributes to research on entrepreneurial finance and media legitimation. In\npractice, startups should go beyond frequent media mentions to strengthen brand\nmemorability through more targeted, meaningful coverage highlighting their\nuniqueness and relevance within the broader industry conversation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22201v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2502.03387", "title": "LIMO: Less is More for Reasoning", "authors": ["Yixin Ye", "Zhen Huang", "Yang Xiao", "Ethan Chern", "Shijie Xia", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2025", "url": "http://arxiv.org/abs/2502.03387v3", "summary": "We challenge the prevailing assumption that complex reasoning in large\nlanguage models (LLMs) necessitates massive training data. We demonstrate that\nsophisticated mathematical reasoning can emerge with only a few examples.\nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves\n63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned\nmodels (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the\ntraining data required by prior approaches. Furthermore, LIMO exhibits strong\nout-of-distribution generalization, achieving a 45.8\\% absolute improvement\nacross diverse benchmarks, outperforming models trained on 100x more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis\n(LIMO Hypothesis): In foundation models where domain knowledge has been\ncomprehensively encoded during pre-training, sophisticated reasoning can emerge\nthrough minimal but strategically designed demonstrations of cognitive\nprocesses. This hypothesis suggests that the threshold for eliciting complex\nreasoning is not dictated by task complexity but rather by two key factors: (1)\nthe completeness of the model's pre-trained knowledge base and (2) the\neffectiveness of post-training examples in serving as \"cognitive templates\"\nthat guide reasoning.", "comment": "COLM 2025", "pdf_url": "http://arxiv.org/pdf/2502.03387v3", "cate": "cs.CL", "date": "2025-02-05", "updated": "2025-07-29"}
{"id": "2505.11864", "title": "Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning", "authors": ["Kalyan Cherukuri", "Aarav Lala"], "categories": ["cs.LG", "cs.AI", "cs.CG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11864v3", "summary": "As generative agents become increasingly capable, alignment of their behavior\nwith complex human values remains a fundamental challenge. Existing approaches\noften simplify human intent through reduction to a scalar reward, overlooking\nthe multi-faceted nature of human feedback. In this work, we introduce a\ntheoretical framework for preference-based Multi-Objective Inverse\nReinforcement Learning (MO-IRL), where human preferences are modeled as latent\nvector-valued reward functions. We formalize the problem of recovering a\nPareto-optimal reward representation from noisy preference queries and\nestablish conditions for identifying the underlying multi-objective structure.\nWe derive tight sample complexity bounds for recovering\n$\\epsilon$-approximations of the Pareto front and introduce a regret\nformulation to quantify suboptimality in this multi-objective setting.\nFurthermore, we propose a provably convergent algorithm for policy optimization\nusing preference-inferred reward cones. Our results bridge the gap between\npractical alignment techniques and theoretical guarantees, providing a\nprincipled foundation for learning aligned behaviors in a high-dimension and\nvalue-pluralistic environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11864v3", "cate": "cs.LG", "date": "2025-05-17", "updated": "2025-07-28"}
{"id": "2501.12057", "title": "Unified 3D MRI Representations via Sequence-Invariant Contrastive Learning", "authors": ["Liam Chalcroft", "Jenny Crinion", "Cathy J. Price", "John Ashburner"], "categories": ["cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.12057v3", "summary": "Self-supervised deep learning has accelerated 2D natural image analysis but\nremains difficult to translate into 3D MRI, where data are scarce and\npre-trained 2D backbones cannot capture volumetric context. We present a\n\\emph{sequence-invariant} self-supervised framework leveraging quantitative MRI\n(qMRI). By simulating multiple MRI contrasts from a single 3D qMRI scan and\nenforcing consistent representations across these contrasts, we learn\nanatomy-centric rather than sequence-specific features. The result is a single\n3D encoder that excels across tasks and protocols. Experiments on healthy brain\nsegmentation (IXI), stroke lesion segmentation (ARC), and MRI denoising show\nsignificant gains over baseline SSL approaches, especially in low-data settings\n(up to +8.3\\% Dice, +4.2 dB PSNR). It also generalises to unseen sites,\nsupporting scalable clinical use. Code and trained models are publicly\navailable at https://github.com/liamchalcroft/contrast-squared", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.12057v3", "cate": "cs.CV", "date": "2025-01-21", "updated": "2025-07-29"}
{"id": "2507.22766", "title": "Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models", "authors": ["Felix Kronenwett", "Georg Maier", "Thomas Laengle"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the 30th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)", "url": "http://arxiv.org/abs/2507.22766v1", "summary": "Sensor-based sorting systems enable the physical separation of a material\nstream into two fractions. The sorting decision is based on the image data\nevaluation of the sensors used and is carried out using actuators. Various\nprocess parameters must be set depending on the properties of the material\nstream, the dimensioning of the system, and the required sorting accuracy.\nHowever, continuous verification and re-adjustment are necessary due to\nchanging requirements and material stream compositions. In this paper, we\nintroduce an approach for optimizing, recurrently monitoring and adjusting the\nprocess parameters of a sensor-based sorting system. Based on Bayesian\nOptimization, Gaussian process regression models are used as surrogate models\nto achieve specific requirements for system behavior with the uncertainties\ncontained therein. This method minimizes the number of necessary experiments\nwhile simultaneously considering two possible optimization targets based on the\nrequirements for both material output streams. In addition, uncertainties are\nconsidered during determining sorting accuracies in the model calculation. We\nevaluated the method with three example process parameters.", "comment": "Accepted at the 30th IEEE International Conference on Emerging\n  Technologies and Factory Automation (ETFA)", "pdf_url": "http://arxiv.org/pdf/2507.22766v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22209", "title": "How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?", "authors": ["Christian Clark", "Byung-Doh Oh", "William Schuler"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22209v1", "summary": "Contextual entropy is a psycholinguistic measure capturing the anticipated\ndifficulty of processing a word just before it is encountered. Recent studies\nhave tested for entropy-related effects as a potential complement to well-known\neffects from surprisal. For convenience, entropy is typically estimated based\non a language model's probability distribution over a word's first subword\ntoken. However, this approximation results in underestimation and potential\ndistortion of true word entropy. To address this, we generate Monte Carlo (MC)\nestimates of word entropy that allow words to span a variable number of tokens.\nRegression experiments on reading times show divergent results between\nfirst-token and MC word entropy, suggesting a need for caution in using\nfirst-token approximations of contextual entropy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22209v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2502.18978", "title": "Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning", "authors": ["Hongyi Cai", "Jie Li", "Mohammad Mahdinur Rahman", "Wenzhen Dong"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2502.18978v4", "summary": "The effectiveness of instruction fine-tuning for Large Language Models is\nfundamentally constrained by the quality and efficiency of training datasets.\nThis work introduces Low-Confidence Gold (LCG), a novel filtering framework\nthat employs centroid-based clustering and confidence-guided selection for\nidentifying valuable instruction pairs. Through a semi-supervised approach\nusing a lightweight classifier trained on representative samples, LCG curates\nhigh-quality subsets while preserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-filtered subsets of 6K samples\nachieve superior performance compared to existing methods, with substantial\nimprovements on MT-bench and consistent gains across comprehensive evaluation\nmetrics. The framework's efficacy while maintaining model performance\nestablishes a promising direction for efficient instruction tuning.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2502.18978v4", "cate": "cs.CL", "date": "2025-02-26", "updated": "2025-07-29"}
{"id": "2505.12225", "title": "Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling", "authors": ["Jizhou Guo", "Zhaomin Wu", "Hanchen Yang", "Philip S. Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.12225v2", "summary": "Enhancing Large Language Model (LLM)'s performance with best-of-N sampling is\neffective and has attracted significant attention. However, it is\ncomputationally prohibitive due to massive, data-hungry text-based reward\nmodels. By changing the data source from text to hidden states, we introduce\nSWIFT (Simple Weighted Intrinsic Feedback Technique), a novel, lightweight\ntechnique that leverages the rich information embedded in LLM hidden states to\naddress these issues, which operates on token-level and consists of only linear\nlayers. Extensive experiments show that SWIFT outperforms baselines with less\nthan 0.005% of the parameters of baselines, requiring only a few samples for\ntraining, demonstrating significant efficiency improvement. SWIFT's robust\nscalability, applicability to some closed-source models via logits, and ability\nto be combined with traditional reward models to yield further performance\ngains underscore its practical value.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.12225v2", "cate": "cs.LG", "date": "2025-05-18", "updated": "2025-07-29"}
{"id": "2501.18729", "title": "Motion Diffusion Autoencoders: Enabling Attribute Manipulation in Human Motion Demonstrated on Karate Techniques", "authors": ["Anthony Richardson", "Felix Putze"], "categories": ["cs.CV", "cs.LG", "68T07 (Primary) 68T30, 92C99 (Secondary)", "I.2.4; I.2.6"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 7 figures", "url": "http://arxiv.org/abs/2501.18729v2", "summary": "Attribute manipulation deals with the problem of changing individual\nattributes of a data point or a time series, while leaving all other aspects\nunaffected. This work focuses on the domain of human motion, more precisely\nkarate movement patterns. To the best of our knowledge, it presents the first\nsuccess at manipulating attributes of human motion data. One of the key\nrequirements for achieving attribute manipulation on human motion is a suitable\npose representation. Therefore, we design a novel continuous, rotation-based\npose representation that enables the disentanglement of the human skeleton and\nthe motion trajectory, while still allowing an accurate reconstruction of the\noriginal anatomy. The core idea of the manipulation approach is to use a\ntransformer encoder for discovering high-level semantics, and a diffusion\nprobabilistic model for modeling the remaining stochastic variations. We show\nthat the embedding space obtained from the transformer encoder is semantically\nmeaningful and linear. This enables the manipulation of high-level attributes,\nby discovering their linear direction of change in the semantic embedding space\nand moving the embedding along said direction. All code and data is made\npublicly available.", "comment": "9 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2501.18729v2", "cate": "cs.CV", "date": "2025-01-30", "updated": "2025-07-29"}
{"id": "2507.22769", "title": "Bayesian Optimization applied for accelerated Virtual Validation of the Autonomous Driving Function", "authors": ["Satyesh Shanker Awasthi", "Mohammed Irshadh Ismaaeel Sathyamangalam Imran", "Stefano Arrigoni", "Francesco Braghin"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22769v1", "summary": "Rigorous Verification and Validation (V&V) of Autonomous Driving Functions\n(ADFs) is paramount for ensuring the safety and public acceptance of Autonomous\nVehicles (AVs). Current validation relies heavily on simulation to achieve\nsufficient test coverage within the Operational Design Domain (ODD) of a\nvehicle, but exhaustively exploring the vast parameter space of possible\nscenarios is computationally expensive and time-consuming. This work introduces\na framework based on Bayesian Optimization (BO) to accelerate the discovery of\ncritical scenarios. We demonstrate the effectiveness of the framework on an\nModel Predictive Controller (MPC)-based motion planner, showing that it\nidentifies hazardous situations, such as off-road events, using orders of\nmagnitude fewer simulations than brute-force Design of Experiments (DoE)\nmethods. Furthermore, this study investigates the scalability of the framework\nin higher-dimensional parameter spaces and its ability to identify multiple,\ndistinct critical regions within the ODD of the motion planner used as the case\nstudy .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22769v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22219", "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation", "authors": ["Dongyub Jude Lee", "Zhenyi Ye", "Pengcheng He"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22219v1", "summary": "Preference-learning methods for machine translation (MT)--such as Direct\nPreference Optimization (DPO)--have achieved impressive gains but depend\nheavily on large, carefully curated triplet datasets and often struggle to\ngeneralize beyond their tuning domains. We propose Reinforcement Learning from\nTeacher-Model Refinement (RLfR), a novel framework that removes reliance on\nstatic triplets by leveraging continuous, high-quality feedback from an\nexternal teacher model (GPT-4o). RLfR frames each translation step as a\nmicro-tutorial: the actor generates a hypothesis, the teacher refines it, and\nthe actor is rewarded based on how closely it aligns with the teacher's\nrefinement. Guided by two complementary signals--(i) negative edit distance,\npromoting lexical and structural fidelity, and (ii) COMET score, ensuring\nsemantic adequacy--the actor progressively learns to emulate the teacher,\nmirroring a human learning process through incremental, iterative improvement.\nOn the FLORES-200 benchmark (English to and from German, Spanish, Chinese,\nKorean, and Japanese), RLfR consistently outperforms both MT-SFT and\npreference-based baselines, significantly improving COMET (semantic adequacy)\nand M-ETA (entity preservation) scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22219v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.04844", "title": "Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI", "authors": ["Hank Gerba"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04844v5", "summary": "Here we introduce Narrative Context Protocol (NCP), an open-source narrative\nstandard designed to enable narrative interoperability, AI-driven authoring\ntools, real-time emergent narratives, and more. By encoding a story's structure\nin a \"Storyform,\" which is a structured register of its narrative features, NCP\nenables narrative portability across systems as well as intent-based\nconstraints for generative storytelling systems. We demonstrate the\ncapabilities of NCP through a year-long experiment, during which an author used\nNCP and a custom authoring platform to create a playable, text-based experience\nbased on her pre-existing novella. This experience is driven by generative AI,\nwith unconstrained natural language input. NCP functions as a set of\n\"guardrails\" that allows the generative system to accommodate player agency\nwhile also ensuring that narrative context and coherence are maintained.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04844v5", "cate": "cs.CL", "date": "2025-03-05", "updated": "2025-07-28"}
{"id": "2505.20628", "title": "Position: Adopt Constraints Over Penalties in Deep Learning", "authors": ["Juan Ramirez", "Meraj Hashemizadeh", "Simon Lacoste-Julien"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code available at this https URL", "url": "http://arxiv.org/abs/2505.20628v3", "summary": "Recent efforts to develop trustworthy AI systems with accountability\nguarantees have led to widespread use of machine learning formulations\nincorporating external requirements, or constraints. These requirements are\noften enforced via penalization--adding fixed-weight terms to the task loss. We\nargue this approach is fundamentally ill-suited since there may be no penalty\ncoefficient that simultaneously ensures constraint satisfaction and optimal\nconstrained performance, i.e., that truly solves the constrained problem.\nMoreover, tuning these coefficients requires costly trial-and-error, incurring\nsignificant time and computational overhead. We, therefore, advocate for\nbroader adoption of tailored constrained optimization methods--such as the\nLagrangian approach, which jointly optimizes the penalization \"coefficients\"\n(the Lagrange multipliers) and the model parameters. Such methods (i) truly\nsolve the constrained problem and do so accountably, by clearly defining\nfeasibility and verifying when it is achieved, (ii) eliminate the need for\nextensive penalty tuning, and (iii) integrate seamlessly with modern deep\nlearning pipelines.", "comment": "Code available at\n  https://github.com/merajhashemi/constraints-vs-penalties", "pdf_url": "http://arxiv.org/pdf/2505.20628v3", "cate": "cs.LG", "date": "2025-05-27", "updated": "2025-07-28"}
{"id": "2502.13693", "title": "MedViT V2: Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention", "authors": ["Omid Nejati Manzari", "Hojat Asgariandehkordi", "Taha Koleilat", "Yiming Xiao", "Hassan Rivaz"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.13693v2", "summary": "Convolutional networks, transformers, hybrid models, and Mamba-based\narchitectures have demonstrated strong performance across various medical image\nclassification tasks. However, these methods were primarily designed to\nclassify clean images using labeled data. In contrast, real-world clinical data\noften involve image corruptions that are unique to multi-center studies and\nstem from variations in imaging equipment across manufacturers. In this paper,\nwe introduce the Medical Vision Transformer (MedViTV2), a novel architecture\nincorporating Kolmogorov-Arnold Network (KAN) layers into the transformer\narchitecture for the first time, aiming for generalized medical image\nclassification. We have developed an efficient KAN block to reduce\ncomputational load while enhancing the accuracy of the original MedViT.\nAdditionally, to counteract the fragility of our MedViT when scaled up, we\npropose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the\nefficient fused dot-product attention kernel capable of capturing global\ncontext and expanding receptive fields to scale the model effectively and\naddressing feature collapse issues. Moreover, a hierarchical hybrid strategy is\nintroduced to stack our Local Feature Perception and Global Feature Perception\nblocks in an efficient manner, which balances local and global feature\nperceptions to boost performance. Extensive experiments on 17 medical image\nclassification datasets and 12 corrupted medical image datasets demonstrate\nthat MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments\nwith reduced computational complexity. MedViTV2 is 44\\% more computationally\nefficient than the previous version and significantly enhances accuracy,\nachieving improvements of 4.6\\% on MedMNIST, 5.8\\% on NonMNIST, and 13.4\\% on\nthe MedMNIST-C benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.13693v2", "cate": "cs.CV", "date": "2025-02-19", "updated": "2025-07-29"}
{"id": "2507.22865", "title": "Age of Estimates: When to Submit Jobs to a Markov Machine to Maximize Revenue", "authors": ["Sahan Liyanaarachchi", "Sennur Ulukus"], "categories": ["cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22865v1", "summary": "With the dawn of AI factories ushering a new era of computing supremacy,\ndevelopment of strategies to effectively track and utilize the available\ncomputing resources is garnering utmost importance. These computing resources\nare often modeled as Markov sources, which oscillate between free and busy\nstates, depending on their internal load and external utilization, and are\ncommonly referred to as Markov machines (MMs). Most of the prior work solely\nfocuses on the problem of tracking these MMs, while often assuming a\nrudimentary decision process that governs their utilization. Our key\nobservation is that the ultimate goal of tracking a MM is to properly utilize\nit. In this work, we consider the problem of maximizing the utility of a MM,\nwhere the utility is defined as the average revenue generated by the MM.\nAssuming a Poisson job arrival process and a query-based sampling procedure to\nsample the state of the MM, we find the optimal times to submit the available\njobs to the MM so as to maximize the average revenue generated per unit job. We\nshow that, depending on the parameters of the MM, the optimal policy is in the\nform of either a \\emph{threshold policy} or a \\emph{switching policy} based on\nthe \\emph{age of our estimate} of the state of the MM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22865v1", "cate": "cs.IT", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22286", "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs", "authors": ["Supantho Rakshit", "Adele Goldberg"], "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures, Accepted for publication at the Second International Workshop on Construction Grammars and NLP at the 16th International Conference for Computational Semantics (IWCS) 2025", "url": "http://arxiv.org/abs/2507.22286v1", "summary": "The usage-based constructionist (UCx) approach posits that language comprises\na network of learned form-meaning pairings (constructions) whose use is largely\ndetermined by their meanings or functions, requiring them to be graded and\nprobabilistic. This study investigates whether the internal representations in\nLarge Language Models (LLMs) reflect the proposed function-infused gradience.\nWe analyze the neural representations of the English dative constructions\n(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of\n$5000$ sentence pairs systematically varied for human-rated preference\nstrength. A macro-level geometric analysis finds that the separability between\nconstruction representations, as measured by Energy Distance or Jensen-Shannon\nDivergence, is systematically modulated by gradient preference strength. More\nprototypical exemplars of each construction occupy more distinct regions in the\nactivation space of LLMs. These results provide strong evidence that LLMs learn\nrich, meaning-infused, graded representations of constructions and offer\nsupport for geometric measures of basic constructionist principles in LLMs.", "comment": "5 pages, 3 figures, Accepted for publication at the Second\n  International Workshop on Construction Grammars and NLP at the 16th\n  International Conference for Computational Semantics (IWCS) 2025", "pdf_url": "http://arxiv.org/pdf/2507.22286v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2503.13401", "title": "Levels of Analysis for Large Language Models", "authors": ["Alexander Ku", "Declan Campbell", "Xuechunzi Bai", "Jiayi Geng", "Ryan Liu", "Raja Marjieh", "R. Thomas McCoy", "Andrew Nam", "Ilia Sucholutsky", "Veniamin Veselovsky", "Liyi Zhang", "Jian-Qiao Zhu", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.13401v2", "summary": "Modern artificial intelligence systems, such as large language models, are\nincreasingly powerful but also increasingly hard to understand. Recognizing\nthis problem as analogous to the historical difficulties in understanding the\nhuman mind, we argue that methods developed in cognitive science can be useful\nfor understanding large language models. We propose a framework for applying\nthese methods based on the levels of analysis that David Marr proposed for\nstudying information processing systems. By revisiting established cognitive\nscience techniques relevant to each level and illustrating their potential to\nyield insights into the behavior and internal organization of large language\nmodels, we aim to provide a toolkit for making sense of these new kinds of\nminds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.13401v2", "cate": "cs.CL", "date": "2025-03-17", "updated": "2025-07-28"}
{"id": "2505.20734", "title": "Adversarial bandit optimization for approximately linear functions", "authors": ["Zhuoyu Cheng", "Kohei Hatano", "Eiji Takimoto"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20734v5", "summary": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20734v5", "cate": "cs.LG", "date": "2025-05-27", "updated": "2025-07-29"}
{"id": "2503.02348", "title": "YOLO-PRO: Enhancing Instance-Specific Object Detection with Full-Channel Global Self-Attention", "authors": ["Lin Huang", "Yujuan Tan", "Weisheng Li", "Shitai Shan", "Liu Liu", "Linlin Shen", "Jing Yu", "Yue Niu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.02348v3", "summary": "This paper addresses the inherent limitations of conventional bottleneck\nstructures (diminished instance discriminability due to overemphasis on batch\nstatistics) and decoupled heads (computational redundancy) in object detection\nframeworks by proposing two novel modules: the Instance-Specific Bottleneck\nwith full-channel global self-attention (ISB) and the Instance-Specific\nAsymmetric Decoupled Head (ISADH). The ISB module innovatively reconstructs\nfeature maps to establish an efficient full-channel global attention mechanism\nthrough synergistic fusion of batch-statistical and instance-specific features.\nComplementing this, the ISADH module pioneers an asymmetric decoupled\narchitecture enabling hierarchical multi-dimensional feature integration via\ndual-stream batch-instance representation fusion. Extensive experiments on the\nMS-COCO benchmark demonstrate that the coordinated deployment of ISB and ISADH\nin the YOLO-PRO framework achieves state-of-the-art performance across all\ncomputational scales. Specifically, YOLO-PRO surpasses YOLOv8 by 1.0-1.6% AP\n(N/S/M/L/X scales) and outperforms YOLO11 by 0.1-0.5% AP in critical N/M/L/X\ngroups, while maintaining competitive computational efficiency. This work\nprovides practical insights for developing high-precision detectors deployable\non edge devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.02348v3", "cate": "cs.CV", "date": "2025-03-04", "updated": "2025-07-29"}
{"id": "2408.00916", "title": "A reference frame-based microgrid primary control for ensuring global convergence to a periodic orbit", "authors": ["Xinyuan Jiang", "Constantino M. Lagoa", "Daning Huang", "Yan Li"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.00916v2", "summary": "Power systems with a high penetration of renewable generation are vulnerable\nto frequency oscillation and voltage instability. Traditionally, the stability\nof power systems is considered either in terms of local stability or as an\nangle oscillator synchronization problem with the simplifying assumption that\nthe dynamics of the amplitudes are on much shorter time scales. Without this\nassumption, however, the steady state being studied is essentially a limit\ncycle with the convergence of its orbit in question. In this paper, we present\na method to analyze the orbital stability of a microgrid and propose a voltage\ncontroller for the inverter-interfaced renewable generators. The main hurdle to\nthe problem lies in the constant terms in the rotating internal reference\nframes of each generator. We extend the shifted passivity of port-Hamiltonian\nsystems to the analysis of limit cycles and prove that, if the system is\nshifted passive without considering these constant terms, then the periodic\norbit is globally attractive. To the best of our knowledge, this is the first\nglobal stability result for non-nominal steady states of the microgrid in the\nfull state space, which provides new insights into the synchronization\nphenomenon where the dissipativity of the system ensures convergence. The\nproposed controller is verified with a test microgrid, demonstrating its\nstability and transient smoothness compared to the standard droop control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.00916v2", "cate": "eess.SY", "date": "2024-08-01", "updated": "2025-07-30"}
{"id": "2507.22289", "title": "Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations", "authors": ["Galo Castillo-López", "Gaël de Chalendar", "Nasredine Semmar"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for publication at SIGDIAL 2025", "url": "http://arxiv.org/abs/2507.22289v1", "summary": "Intent recognition is a fundamental component in task-oriented dialogue\nsystems (TODS). Determining user intents and detecting whether an intent is\nOut-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,\ntraditional TODS require large amount of annotated data. In this work we\npropose a hybrid approach to combine BERT and LLMs in zero and few-shot\nsettings to recognize intents and detect OOS utterances. Our approach leverages\nLLMs generalization power and BERT's computational efficiency in such\nscenarios. We evaluate our method on multi-party conversation corpora and\nobserve that sharing information from BERT outputs to LLMs leads to system\nperformance improvement.", "comment": "Accepted for publication at SIGDIAL 2025", "pdf_url": "http://arxiv.org/pdf/2507.22289v1", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2504.04142", "title": "My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt", "authors": ["Kees van Deemter"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      34 pages", "url": "http://arxiv.org/abs/2504.04142v2", "summary": "In this very personal workography, I relate my 40-year experiences as a\nresearcher and educator in and around Artificial Intelligence (AI), more\nspecifically Natural Language Processing. I describe how curiosity, and the\ncircumstances of the day, led me to work in both industry and academia, and in\nvarious countries, including The Netherlands (Amsterdam, Eindhoven, and\nUtrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and\nChina (Beijing and Harbin). People and anecdotes play a large role in my story;\nthe history of AI forms its backdrop. I focus on things that might be of\ninterest to (even) younger colleagues, given the choices they face in their own\nwork and life at a time when AI is finally emerging from the shadows.", "comment": "34 pages", "pdf_url": "http://arxiv.org/pdf/2504.04142v2", "cate": "cs.CL", "date": "2025-04-05", "updated": "2025-07-29"}
{"id": "2506.12944", "title": "Unsupervised risk factor identification across cancer types and data modalities via explainable artificial intelligence", "authors": ["Maximilian Ferle", "Jonas Ader", "Thomas Wiemers", "Nora Grieb", "Adrian Lindenmeyer", "Hans-Jonas Meyer", "Thomas Neumuth", "Markus Kreuz", "Kristin Reiche", "Maximilian Merz"], "categories": ["cs.LG", "q-bio.TO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12944v3", "summary": "Risk stratification is a key tool in clinical decision-making, yet current\napproaches often fail to translate sophisticated survival analysis into\nactionable clinical criteria. We present a novel method for unsupervised\nmachine learning that directly optimizes for survival heterogeneity across\npatient clusters through a differentiable adaptation of the multivariate\nlogrank statistic. Unlike most existing methods that rely on proxy metrics, our\napproach represents novel methodology for training any neural network\narchitecture on any data modality to identify prognostically distinct patient\ngroups. We thoroughly evaluate the method in simulation experiments and\ndemonstrate its utility in practice by applying it to two distinct cancer\ntypes: analyzing laboratory parameters from multiple myeloma patients and\ncomputed tomography images from non-small cell lung cancer patients,\nidentifying prognostically distinct patient subgroups with significantly\ndifferent survival outcomes in both cases. Post-hoc explainability analyses\nuncover clinically meaningful features determining the group assignments which\nalign well with established risk factors and thus lend strong weight to the\nmethods utility. This pan-cancer, model-agnostic approach represents a valuable\nadvancement in clinical risk stratification, enabling the discovery of novel\nprognostic signatures across diverse data types while providing interpretable\nresults that promise to complement treatment personalization and clinical\ndecision-making in oncology and beyond.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12944v3", "cate": "cs.LG", "date": "2025-06-15", "updated": "2025-07-29"}
{"id": "2503.06840", "title": "Improving Visual Place Recognition with Sequence-Matching Receptiveness Prediction", "authors": ["Somayeh Hussaini", "Tobias Fischer", "Michael Milford"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, Accepted to the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025", "url": "http://arxiv.org/abs/2503.06840v2", "summary": "In visual place recognition (VPR), filtering and sequence-based matching\napproaches can improve performance by integrating temporal information across\nimage sequences, especially in challenging conditions. While these methods are\ncommonly applied, their effects on system behavior can be unpredictable and can\nactually make performance worse in certain situations. In this work, we present\na new supervised learning approach that learns to predict the per-frame\nsequence matching receptiveness (SMR) of VPR techniques, enabling the system to\nselectively decide when to trust the output of a sequence matching system. Our\napproach is agnostic to the underlying VPR technique and effectively predicts\nSMR, and hence significantly improves VPR performance across a large range of\nstate-of-the-art and classical VPR techniques (namely CosPlace, MixVPR,\nEigenPlaces, SALAD, AP-GeM, NetVLAD and SAD), and across three benchmark VPR\ndatasets (Nordland, Oxford RobotCar, and SFU-Mountain). We also provide\ninsights into a complementary approach that uses the predictor to replace\ndiscarded matches, and present ablation studies including an analysis of the\ninteractions between our SMR predictor and the selected sequence length.", "comment": "8 pages, 5 figures, Accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "pdf_url": "http://arxiv.org/pdf/2503.06840v2", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-29"}
{"id": "2411.03875", "title": "Koopman-based control using sum-of-squares optimization: Improved stability guarantees and data efficiency", "authors": ["Robin Strässer", "Julian Berberich", "Frank Allgöwer"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the European Journal of Control, 2025", "url": "http://arxiv.org/abs/2411.03875v4", "summary": "In this paper, we propose a novel controller design approach for unknown\nnonlinear systems using the Koopman operator. In particular, we use the\nrecently proposed stability- and certificate-oriented extended dynamic mode\ndecomposition (SafEDMD) architecture to generate a data-driven bilinear\nsurrogate model with certified error bounds. Then, by accounting for the\nobtained error bounds in a controller design based on the bilinear system, one\ncan guarantee closed-loop stability for the true nonlinear system. While\nexisting approaches over-approximate the bilinearity of the surrogate model,\nthus introducing conservatism and providing only local guarantees, we\nexplicitly account for the bilinearity by using sum-of-squares (SOS)\noptimization in the controller design. More precisely, we parametrize a\nrational controller stabilizing the error-affected bilinear surrogate model\nand, consequently, the underlying nonlinear system. The resulting SOS\noptimization problem provides explicit data-driven controller design conditions\nfor unknown nonlinear systems based on semidefinite programming. Our approach\nsignificantly reduces conservatism by establishing a larger region of\nattraction and improved data efficiency. The proposed method is evaluated using\nnumerical examples, demonstrating its advantages over existing approaches.", "comment": "Accepted for publication in the European Journal of Control, 2025", "pdf_url": "http://arxiv.org/pdf/2411.03875v4", "cate": "eess.SY", "date": "2024-11-06", "updated": "2025-07-30"}
{"id": "2507.22367", "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors", "authors": ["Jia Li", "Yichao He", "Jiacheng Xu", "Tianhao Luo", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures, ACM MM 2025", "url": "http://arxiv.org/abs/2507.22367v1", "summary": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep.", "comment": "8 pages, 3 figures, ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.22367v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2504.10541", "title": "Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation", "authors": ["Xu Guo", "Tong Zhang", "Yuanzhi Wang", "Chenxu Wang", "Fuyun Wang", "Xudong Wang", "Xiaoya Zhang", "Xin Liu", "Zhen Cui"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures, submitted to IEEE Transactions on Knowledge and Data Engineering", "url": "http://arxiv.org/abs/2504.10541v2", "summary": "The burgeoning presence of Large Language Models (LLM) is propelling the\ndevelopment of personalized recommender systems. Most existing LLM-based\nmethods fail to sufficiently explore the multi-view graph structure\ncorrelations inherent in recommendation scenarios. To this end, we propose a\nnovel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation\n(HeLLM), designed to equip LLMs with the capability to capture intricate\nhigher-order semantic correlations by fusing graph-level contextual signals\nwith sequence-level behavioral patterns. In the recommender pre-training phase,\nwe design a user hypergraph to uncover shared interest preferences among users\nand an item hypergraph to capture correlations within multimodal similarities\namong items. The hypergraph convolution and synergistic contrastive learning\nmechanism are introduced to enhance the distinguishability of learned\nrepresentations. In the LLM fine-tuning phase, we inject the learned\ngraph-structured embeddings directly into the LLM's architecture and integrate\nsequential features capturing each user's chronological behavior. This process\nenables hypergraphs to leverage graph-structured information as global context,\nenhancing the LLM's ability to perceive complex relational patterns and\nintegrate multimodal information, while also modeling local temporal dynamics.\nExtensive experiments demonstrate the superiority of our proposed method over\nstate-of-the-art baselines, confirming the advantages of fusing\nhypergraph-based context with sequential user behavior in LLMs for\nrecommendation.", "comment": "12 pages, 4 figures, submitted to IEEE Transactions on Knowledge and\n  Data Engineering", "pdf_url": "http://arxiv.org/pdf/2504.10541v2", "cate": "cs.IR", "date": "2025-04-13", "updated": "2025-07-29"}
{"id": "2506.15064", "title": "HiPreNets: High-Precision Neural Networks through Progressive Training", "authors": ["Ethan Mulle", "Wei Kang", "Qi Gong"], "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15064v2", "summary": "Deep neural networks are powerful tools for solving nonlinear problems in\nscience and engineering, but training highly accurate models becomes\nchallenging as problem complexity increases. Non-convex optimization and\nnumerous hyperparameters to tune make performance improvement difficult, and\ntraditional approaches often prioritize minimizing mean squared error (MSE)\nwhile overlooking $L^{\\infty}$ error, which is the critical focus in many\napplications. To address these challenges, we present a progressive framework\nfor training and tuning high-precision neural networks (HiPreNets). Our\napproach refines a previously explored staged training technique for neural\nnetworks that improves an existing fully connected neural network by\nsequentially learning its prediction residuals using additional networks,\nleading to improved overall accuracy. We discuss how to take advantage of the\nstructure of the residuals to guide the choice of loss function, number of\nparameters to use, and ways to introduce adaptive data sampling techniques. We\nvalidate our framework's effectiveness through several benchmark problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15064v2", "cate": "cs.LG", "date": "2025-06-18", "updated": "2025-07-29"}
{"id": "2503.08008", "title": "A Survey on Wi-Fi Sensing Generalizability: Taxonomy, Techniques, Datasets, and Future Research Prospects", "authors": ["Fei Wang", "Tingting Zhang", "Wei Xi", "Han Ding", "Ge Wang", "Di Zhang", "Yuanhao Cui", "Fan Liu", "Jinsong Han", "Jie Xu", "Tony Xiao Han"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under Review; 30 pages, 322 references", "url": "http://arxiv.org/abs/2503.08008v2", "summary": "Wi-Fi sensing has emerged as a powerful non-intrusive technology for\nrecognizing human activities, monitoring vital signs, and enabling\ncontext-aware applications using commercial wireless devices. However, the\nperformance of Wi-Fi sensing often degrades when applied to new users, devices,\nor environments due to significant domain shifts. To address this challenge,\nresearchers have proposed a wide range of generalization techniques aimed at\nenhancing the robustness and adaptability of Wi-Fi sensing systems. In this\nsurvey, we provide a comprehensive and structured review of over 200 papers\npublished since 2015, categorizing them according to the Wi-Fi sensing\npipeline: experimental setup, signal preprocessing, feature learning, and model\ndeployment. We analyze key techniques, including signal preprocessing, domain\nadaptation, meta-learning, metric learning, data augmentation, cross-modal\nalignment, federated learning, and continual learning. Furthermore, we\nsummarize publicly available datasets across various tasks,such as activity\nrecognition, user identification, indoor localization, and pose estimation, and\nprovide insights into their domain diversity. We also discuss emerging trends\nand future directions, including large-scale pretraining, integration with\nmultimodal foundation models, and continual deployment. To foster community\ncollaboration, we introduce the Sensing Dataset Platform (SDP) for sharing\ndatasets and models. This survey aims to serve as a valuable reference and\npractical guide for researchers and practitioners dedicated to improving the\ngeneralizability of Wi-Fi sensing systems.", "comment": "Under Review; 30 pages, 322 references", "pdf_url": "http://arxiv.org/pdf/2503.08008v2", "cate": "cs.CV", "date": "2025-03-11", "updated": "2025-07-29"}
{"id": "2502.06212", "title": "AVSim -- Realistic Simulation Framework for Airborne and Vector-Borne Disease Dynamics", "authors": ["Pandula Thennakoon", "Mario De Silva", "M. Mahesha Viduranga", "Sashini Liyanage", "Roshan Godaliyadda", "Mervyn Parakrama Ekanayake", "Vijitha Herath", "Anuruddhika Rathnayake", "Ganga Thilakarathne", "Janaka Ekanayake", "Samath Dharmarathne"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      13 pages, 15 figures, submitted to IEEE Transactions on Systems, Man, and Cybernetics: Systems", "url": "http://arxiv.org/abs/2502.06212v2", "summary": "Computational disease modeling plays a crucial role in understanding and\ncontrolling the transmission of infectious diseases. While agent-based models\n(ABMs) provide detailed insights into individual dynamics, accurately\nreplicating human motion remains challenging due to its complex,\nmulti-factorial nature. Most existing frameworks fail to model realistic human\nmotion, leading to oversimplified and less realistic behavior modeling.\nFurthermore, many current models rely on synthetic assumptions and fail to\naccount for realistic environmental structures, transportation systems, and\nbehavioral heterogeneity across occupation groups. To address these\nlimitations, we introduce AVSim, an agent-based simulation framework designed\nto model airborne and vector-borne disease dynamics under realistic conditions.\nA distinguishing feature of AVSim is its ability to accurately model the dual\nnature of human mobility (both the destinations individuals visit and the\nduration of their stay) by utilizing GPS traces from real-world participants,\ncharacterized by occupation. This enables a significantly more granular and\nrealistic representation of human movement compared to existing approaches.\nFurthermore, spectral clustering combined with graph-theoretic analysis is used\nto uncover latent behavioral patterns within occupations, enabling fine-grained\nmodeling of agent behavior. We validate the synthetic human mobility patterns\nagainst ground-truth GPS data and demonstrate AVSim's capabilities via\nsimulations of COVID-19 and dengue. The results highlight AVSim's capacity to\ntrace infection pathways, identify high-risk zones, and evaluate interventions\nsuch as vaccination, quarantine, and vector control with occupational and\ngeographic specificity.", "comment": "13 pages, 15 figures, submitted to IEEE Transactions on Systems, Man,\n  and Cybernetics: Systems", "pdf_url": "http://arxiv.org/pdf/2502.06212v2", "cate": "eess.SY", "date": "2025-02-10", "updated": "2025-07-30"}
{"id": "2507.22387", "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs", "authors": ["Homaira Huda Shomee", "Suman Kalyan Maity", "Sourav Medya"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22387v1", "summary": "Large language models (LLMs) have emerged as transformative approaches in\nseveral important fields. This paper aims for a paradigm shift for patent\nwriting by leveraging LLMs to overcome the tedious patent-filing process. In\nthis work, we present PATENTWRITER, the first unified benchmarking framework\nfor evaluating LLMs in patent abstract generation. Given the first claim of a\npatent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a\nconsistent setup spanning zero-shot, few-shot, and chain-of-thought prompting\nstrategies to generate the abstract of the patent. Our benchmark PATENTWRITER\ngoes beyond surface-level evaluation: we systematically assess the output\nquality using a comprehensive suite of metrics -- standard NLP measures (e.g.,\nBLEU, ROUGE, BERTScore), robustness under three types of input perturbations,\nand applicability in two downstream patent classification and retrieval tasks.\nWe also conduct stylistic analysis to assess length, readability, and tone.\nExperimental results show that modern LLMs can generate high-fidelity and\nstylistically appropriate patent abstracts, often surpassing domain-specific\nbaselines. Our code and dataset are open-sourced to support reproducibility and\nfuture research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22387v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2505.17206", "title": "FB-RAG: Improving RAG with Forward and Backward Lookup", "authors": ["Kushal Chawla", "Alfy Samuel", "Anoop Kumar", "Daben Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.17206v2", "summary": "Traditional Retrieval-Augmented Generation (RAG) struggles with complex\nqueries that lack strong signals to retrieve the most relevant context, forcing\na trade-off between choosing a small context that misses key information and a\nlarge context that confuses the LLM. To address this, we propose\nForward-Backward RAG (FB-RAG), a new training-free framework based on a simple\nyet powerful forward-looking strategy. FB-RAG employs a light-weight LLM to\npeek into potential future generations, using evidence from multiple sampled\noutputs to precisely identify the most relevant context for a final, more\npowerful generator. This improves performance without complex finetuning or\nReinforcement Learning common in prior work. Across 9 datasets, FB-RAG\nconsistently delivers strong results. Further, the performance gains can be\nachieved with reduced latency due to a shorter, more focused prompt for the\npowerful generator. On EN.QA dataset, FB-RAG matches the leading baseline with\nover 48% latency reduction or achieves an 8% performance improvement with a 10%\nlatency reduction. Our analysis finds cases where even when the forward-looking\nLLM fails to generate correct answers, its attempts are sufficient to guide the\nfinal model to an accurate response, demonstrating how smaller LLMs can\nsystematically improve the performance and efficiency of larger ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.17206v2", "cate": "cs.CL", "date": "2025-05-22", "updated": "2025-07-29"}
{"id": "2506.20031", "title": "Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning", "authors": ["Prithvi Poddar", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for presentation in proceedings of IEEE CASE 2025", "url": "http://arxiv.org/abs/2506.20031v2", "summary": "Operations in disaster response, search \\& rescue, and military missions that\ninvolve multiple agents demand automated processes to support the planning of\nthe courses of action (COA). Moreover, traverse-affecting changes in the\nenvironment (rain, snow, blockades, etc.) may impact the expected performance\nof a COA, making it desirable to have a pool of COAs that are diverse in task\ndistributions across agents. Further, variations in agent capabilities, which\ncould be human crews and/or autonomous systems, present practical opportunities\nand computational challenges to the planning process. This paper presents a new\ntheoretical formulation and computational framework to generate such diverse\npools of COAs for operations with soft variations in agent-task compatibility.\nKey to the problem formulation is a graph abstraction of the task space and the\npool of COAs itself to quantify its diversity. Formulating the COAs as a\ncentralized multi-robot task allocation problem, a genetic algorithm is used\nfor (order-ignoring) allocations of tasks to each agent that jointly maximize\ndiversity within the COA pool and overall compatibility of the agent-task\nmappings. A graph neural network is trained using a policy gradient approach to\nthen perform single agent task sequencing in each COA, which maximizes\ncompletion rates adaptive to task features. Our tests of the COA generation\nprocess in a simulated environment demonstrate significant performance gain\nover a random walk baseline, small optimality gap in task sequencing, and\nexecution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task\noperations.", "comment": "Accepted for presentation in proceedings of IEEE CASE 2025", "pdf_url": "http://arxiv.org/pdf/2506.20031v2", "cate": "cs.LG", "date": "2025-06-24", "updated": "2025-07-29"}
{"id": "2503.16418", "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity", "authors": ["Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Hao Kang", "Xin Lu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight). Project page: this https URL Code and model: this https URL", "url": "http://arxiv.org/abs/2503.16418v2", "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.", "comment": "ICCV 2025 (Highlight). Project page:\n  https://bytedance.github.io/InfiniteYou/ Code and model:\n  https://github.com/bytedance/InfiniteYou", "pdf_url": "http://arxiv.org/pdf/2503.16418v2", "cate": "cs.CV", "date": "2025-03-20", "updated": "2025-07-29"}
{"id": "2507.20835", "title": "Minimum Attention Control (MAC) in a Receding Horizon Framework with Applications", "authors": ["Ganesh Teja Theertham", "Santhosh Kumar Varanasi", "Phanindra Jampana"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      21 pages, 12 figures", "url": "http://arxiv.org/abs/2507.20835v2", "summary": "Minimum Attention Control (MAC) is a control technique that provides minimal\ninput changes to meet the control objective. Mathematically, the zero norm of\nthe input changes is used as a constraint for the given control objective and\nminimized with respect to the process dynamics. In this paper, along with the\nzero norm constraint, stage costs are also considered for reference tracking in\na receding horizon framework. For this purpose, the optimal inputs of the\nprevious horizons are also considered in the optimization problem of the\ncurrent horizon. An alternating minimization algorithm is applied to solve the\noptimization problem (Minimum Attention Model Predictive Control (MAMPC)). The\nouter step of the optimization is a quadratic program, while the inner step,\nwhich solves for sparsity, has an analytical solution. The proposed algorithm\nis implemented on two case studies: a four-tank system with slow dynamics and a\nfuel cell stack with fast dynamics. A detailed comparative study of the\nproposed algorithm with standard MPC indicates sparse control actions with a\ntradeoff in the tracking error.", "comment": "21 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.20835v2", "cate": "eess.SY", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2507.22410", "title": "Question Generation for Assessing Early Literacy Reading Comprehension", "authors": ["Xiaocheng Yang", "Sumuk Shashidhar", "Dilek Hakkani-Tur"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      2 pages, 1 figure, accepted by SLaTE 2025", "url": "http://arxiv.org/abs/2507.22410v1", "summary": "Assessment of reading comprehension through content-based interactions plays\nan important role in the reading acquisition process. In this paper, we propose\na novel approach for generating comprehension questions geared to K-2 English\nlearners. Our method ensures complete coverage of the underlying material and\nadaptation to the learner's specific proficiencies, and can generate a large\ndiversity of question types at various difficulty levels to ensure a thorough\nevaluation. We evaluate the performance of various language models in this\nframework using the FairytaleQA dataset as the source material. Eventually, the\nproposed approach has the potential to become an important part of autonomous\nAI-driven English instructors.", "comment": "2 pages, 1 figure, accepted by SLaTE 2025", "pdf_url": "http://arxiv.org/pdf/2507.22410v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.01413", "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages of main body, 5 tables, 5 figures, 42 pages of appendix", "url": "http://arxiv.org/abs/2506.01413v5", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions", "comment": "15 pages of main body, 5 tables, 5 figures, 42 pages of appendix", "pdf_url": "http://arxiv.org/pdf/2506.01413v5", "cate": "cs.CV", "date": "2025-06-02", "updated": "2025-07-29"}
{"id": "2506.20380", "title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis", "authors": ["Zhengpeng Feng", "Clement Atzberger", "Sadiq Jaffer", "Jovana Knezevic", "Silja Sormunen", "Robin Young", "Madeline C Lisaius", "Markus Immitzer", "David A. Coomes", "Anil Madhavapeddy", "Andrew Blake", "Srinivasan Keshav"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.20380v3", "summary": "Satellite remote sensing from repeated observations and multiple sensors\nenables a wide range of downstream applications, including climate modeling,\ncarbon accounting, and strategies for conservation and sustainable land use.\nHowever, satellite time series are voluminous, often corrupted by sensor noise,\nclouds, and atmospheric conditions, and unevenly spaced in time, making them\nchallenging to use. We present TESSERA, an open, global, land-oriented remote\nsensing foundation model that uses self-supervised learning to generate\n`ready-to-use' embeddings at 10~m scale from pixel-level satellite time series\ndata. TESSERA uses two parallel Transformer-based encoders to combine optical\ndata from ten Sentinel-2 spectral bands at 10-60~m spatial resolution and two\nSentinel-1 synthetic aperture radar backscatter coefficients at 10~m resolution\nto create embeddings that are subsequently fused with a multilayer perceptron\nto create annual global embedding maps. We compare our work with\nstate-of-the-art task-specific models and other foundation models in five\ndiverse downstream tasks and find that TESSERA closely matches or outperforms\nthese baselines. We believe that TESSERA's ease of use, openness, computation-,\nlabel-, and data-efficiency, and high performance will prove transformative in\na wide range of vegetation-oriented ecological and agricultural applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20380v3", "cate": "cs.LG", "date": "2025-06-25", "updated": "2025-07-29"}
{"id": "2503.18513", "title": "LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene", "authors": ["Xiaoyu Zhang", "Weihong Pan", "Chong Bao", "Xiyu Zhang", "Xiaojun Xiang", "Hanqing Jiang", "Hujun Bao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to CVPR 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2503.18513v3", "summary": "Humans perceive and comprehend their surroundings through information\nspanning multiple frequencies. In immersive scenes, people naturally scan their\nenvironment to grasp its overall structure while examining fine details of\nobjects that capture their attention. However, current NeRF frameworks\nprimarily focus on modeling either high-frequency local views or the broad\nstructure of scenes with low-frequency information, which is limited to\nbalancing both. We introduce FA-NeRF, a novel frequency-aware framework for\nview synthesis that simultaneously captures the overall scene structure and\nhigh-definition details within a single NeRF model. To achieve this, we propose\na 3D frequency quantification method that analyzes the scene's frequency\ndistribution, enabling frequency-aware rendering. Our framework incorporates a\nfrequency grid for fast convergence and querying, a frequency-aware feature\nre-weighting strategy to balance features across different frequency contents.\nExtensive experiments show that our method significantly outperforms existing\napproaches in modeling entire scenes while preserving fine details. Project\npage: https://coscatter.github.io/LookCloser/", "comment": "Accepted to CVPR 2025. Project page:\n  https://coscatter.github.io/LookCloser", "pdf_url": "http://arxiv.org/pdf/2503.18513v3", "cate": "cs.CV", "date": "2025-03-24", "updated": "2025-07-29"}
{"id": "2309.01112", "title": "Swing Leg Motion Strategy for Heavy-load Legged Robot Based on Force Sensing", "authors": ["Ze Fu", "Yinghui Li", "Weizhong Guo"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      The manuscript is withdrawn due to ongoing major revisions and improvements to the methodology and experimental validation", "url": "http://arxiv.org/abs/2309.01112v2", "summary": "The heavy-load legged robot has strong load carrying capacity and can adapt\nto various unstructured terrains. But the large weight results in higher\nrequirements for motion stability and environmental perception ability. In\norder to utilize force sensing information to improve its motion performance,\nin this paper, we propose a finite state machine model for the swing leg in the\nstatic gait by imitating the movement of the elephant. Based on the presence or\nabsence of additional terrain information, different trajectory planning\nstrategies are provided for the swing leg to enhance the success rate of\nstepping and save energy. The experimental results on a novel quadruped robot\nshow that our method has strong robustness and can enable heavy-load legged\nrobots to pass through various complex terrains autonomously and smoothly.", "comment": "The manuscript is withdrawn due to ongoing major revisions and\n  improvements to the methodology and experimental validation", "pdf_url": "http://arxiv.org/pdf/2309.01112v2", "cate": "cs.RO", "date": "2023-09-03", "updated": "2025-07-30"}
{"id": "2507.22411", "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models", "authors": ["Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.22411v1", "summary": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.22411v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.02733", "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering", "authors": ["Xiaoyi Feng", "Kaifeng Zou", "Caichun Cen", "Tao Huang", "Hui Guo", "Zizhou Huang", "Yingli Zhao", "Mingqing Zhang", "Ziyuan Zheng", "Diwei Wang", "Yuntao Zou", "Dagang Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.02733v2", "summary": "Existing optical flow datasets focus primarily on real-world simulation or\nsynthetic human motion, but few are tailored to Celluloid(cel) anime character\nmotion: a domain with unique visual and motion characteristics. To bridge this\ngap and facilitate research in optical flow estimation and downstream tasks\nsuch as anime video generation and line drawing colorization, we introduce\nLinkTo-Anime, the first high-quality dataset specifically designed for cel\nanime character motion generated with 3D model rendering. LinkTo-Anime provides\nrich annotations including forward and backward optical flow, occlusion masks,\nand Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230\ntraining frames, 720 validation frames, and 4,320 test frames. Furthermore, a\ncomprehensive benchmark is constructed with various optical flow estimation\nmethods to analyze the shortcomings and limitations across multiple datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.02733v2", "cate": "cs.CV", "date": "2025-06-03", "updated": "2025-07-29"}
{"id": "2507.00090", "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "authors": ["Michael Corbeau", "Emmanuelle Claeys", "Mathieu Serrurier", "Pascale Zaraté"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      accepted at IEEE SMC 2025 Vienna", "url": "http://arxiv.org/abs/2507.00090v3", "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "comment": "accepted at IEEE SMC 2025 Vienna", "pdf_url": "http://arxiv.org/pdf/2507.00090v3", "cate": "cs.LG", "date": "2025-06-30", "updated": "2025-07-29"}
{"id": "2504.01596", "title": "DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image", "authors": ["Jijun Xiang", "Xuan Zhu", "Xianqi Wang", "Yu Wang", "Hong Zhang", "Fei Guo", "Xin Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 15 figures, 7 tables", "url": "http://arxiv.org/abs/2504.01596v2", "summary": "Depth enhancement, which uses RGB images as guidance to convert raw signals\nfrom dToF into high-precision, dense depth maps, is a critical task in computer\nvision. Although existing super-resolution-based methods show promising results\non public datasets, they often rely on idealized assumptions like accurate\nregion correspondences and reliable dToF inputs, overlooking calibration errors\nthat cause misalignment and anomaly signals inherent to dToF imaging, limiting\nreal-world applicability. To address these challenges, we propose a novel\ncompletion-based method, named DEPTHOR, featuring advances in both the training\nstrategy and model architecture. First, we propose a method to simulate\nreal-world dToF data from the accurate ground truth in synthetic datasets to\nenable noise-robust training. Second, we design a novel network that\nincorporates monocular depth estimation (MDE), leveraging global depth\nrelationships and contextual information to improve prediction in challenging\nregions. On the ZJU-L5 dataset, our training strategy significantly enhances\ndepth completion models, achieving results comparable to depth super-resolution\nmethods, while our model achieves state-of-the-art results, improving Rel and\nRMSE by 27% and 18%, respectively. On a more challenging set of dToF samples we\ncollected, our method outperforms SOTA methods on preliminary stereo-based GT,\nimproving Rel and RMSE by 23% and 22%, respectively. Our Code is available at\nhttps://github.com/ShadowBbBb/Depthor", "comment": "16 pages, 15 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2504.01596v2", "cate": "cs.CV", "date": "2025-04-02", "updated": "2025-07-29"}
{"id": "2412.06636", "title": "Free-Gate: Planning, Control And Policy Composition via Free Energy Gating", "authors": ["Francesca Rossi", "Émiland Garrabé", "Giovanni Russo"], "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      15 pages, 2 figures", "url": "http://arxiv.org/abs/2412.06636v3", "summary": "We consider the problem of optimally composing a set of primitives to tackle\nplanning and control tasks. To address this problem, we introduce a free energy\ncomputational model for planning and control via policy composition: Free-Gate.\nWithin Free-Gate, control primitives are combined via a gating mechanism that\nminimizes variational free energy. This composition problem is formulated as a\nfinite-horizon optimal control problem, which we prove remains convex even when\nthe cost is not convex in states/actions and the environment is nonlinear,\nstochastic and non-stationary. We develop an algorithm that computes the\noptimal primitives composition and demonstrate its effectiveness via in-silico\nand hardware experiments on an application involving robot navigation in an\nenvironment with obstacles. The experiments highlight that Free-Gate enables\nthe robot to navigate to the destination despite only having available simple\nmotor primitives that, individually, could not fulfill the task.", "comment": "15 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2412.06636v3", "cate": "math.OC", "date": "2024-12-09", "updated": "2025-07-30"}
{"id": "2507.22445", "title": "AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini", "authors": ["Jill Walker Rettberg", "Hermann Wigers"], "categories": ["cs.CL", "cs.AI", "H.1.2; I.2.4; I.2.0; I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement number 101142306. The project is also supported by the Center for Digital Narrative, which is funded by the Research Council of Norway through its Centres of Excellence scheme, project number 332643", "url": "http://arxiv.org/abs/2507.22445v1", "summary": "Can a language model trained largely on Anglo-American texts generate stories\nthat are culturally relevant to other nationalities? To find out, we generated\n11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a\n1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although\nthe stories do include surface-level national symbols and themes, they\noverwhelmingly conform to a single narrative plot structure across countries: a\nprotagonist lives in or returns home to a small town and resolves a minor\nconflict by reconnecting with tradition and organising community events.\nReal-world conflicts are sanitised, romance is almost absent, and narrative\ntension is downplayed in favour of nostalgia and reconciliation. The result is\na narrative homogenisation: an AI-generated synthetic imaginary that\nprioritises stability above change and tradition above growth. We argue that\nthe structural homogeneity of AI-generated narratives constitutes a distinct\nform of AI bias, a narrative standardisation that should be acknowledged\nalongside the more familiar representational bias. These findings are relevant\nto literary studies, narratology, critical AI studies, NLP research, and\nefforts to improve the cultural alignment of generative AI.", "comment": "This project has received funding from the European Union's Horizon\n  2020 research and innovation programme under grant agreement number\n  101142306. The project is also supported by the Center for Digital Narrative,\n  which is funded by the Research Council of Norway through its Centres of\n  Excellence scheme, project number 332643", "pdf_url": "http://arxiv.org/pdf/2507.22445v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.05413", "title": "SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs", "authors": ["Patrik Czakó", "Gábor Kertész", "Sándor Szénási"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, 5 tables. Accepted to IEEE SMC 2025 conference proceedings", "url": "http://arxiv.org/abs/2506.05413v2", "summary": "We present SmoothRot, a novel post-training quantization technique to enhance\nthe efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot\naddresses the critical challenge of massive activation outliers, by integrating\nchannel-wise scaling with Hadamard transformations. Our technique effectively\ntransforms extreme outliers into quantization-friendly activations,\nsignificantly improving quantization accuracy. Experiments conducted on popular\nLLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot\nconsistently reduces the performance gap between quantized and FP16 models by\napproximately 10-30\\% across language generation and zero-shot reasoning tasks,\nwithout introducing additional inference latency. Code is available at\nhttps://github.com/czakop/smoothrot.", "comment": "6 pages, 3 figures, 5 tables. Accepted to IEEE SMC 2025 conference\n  proceedings", "pdf_url": "http://arxiv.org/pdf/2506.05413v2", "cate": "cs.CL", "date": "2025-06-04", "updated": "2025-07-29"}
{"id": "2507.07848", "title": "\"So, Tell Me About Your Policy...\": Distillation of interpretable policies from Deep Reinforcement Learning agents", "authors": ["Giovanni Dispoto", "Paolo Bonetti", "Marcello Restelli"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ECAI 2025", "url": "http://arxiv.org/abs/2507.07848v2", "summary": "Recent advances in Reinforcement Learning (RL) largely benefit from the\ninclusion of Deep Neural Networks, boosting the number of novel approaches\nproposed in the field of Deep Reinforcement Learning (DRL). These techniques\ndemonstrate the ability to tackle complex games such as Atari, Go, and other\nreal-world applications, including financial trading. Nevertheless, a\nsignificant challenge emerges from the lack of interpretability, particularly\nwhen attempting to comprehend the underlying patterns learned, the relative\nimportance of the state features, and how they are integrated to generate the\npolicy's output. For this reason, in mission-critical and real-world settings,\nit is often preferred to deploy a simpler and more interpretable algorithm,\nalthough at the cost of performance. In this paper, we propose a novel\nalgorithm, supported by theoretical guarantees, that can extract an\ninterpretable policy (e.g., a linear policy) without disregarding the\npeculiarities of expert behavior. This result is obtained by considering the\nadvantage function, which includes information about why an action is superior\nto the others. In contrast to previous works, our approach enables the training\nof an interpretable policy using previously collected experience. The proposed\nalgorithm is empirically evaluated on classic control environments and on a\nfinancial trading scenario, demonstrating its ability to extract meaningful\ninformation from complex expert policies.", "comment": "Accepted at ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.07848v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-29"}
{"id": "2505.02178", "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "authors": ["Shubhendu Jena", "Amine Ouasfi", "Mae Younes", "Adnane Boukhayma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page : this https URL", "url": "http://arxiv.org/abs/2505.02178v3", "summary": "We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.", "comment": "ICCV 2025. Project page :\n  https://shubhendu-jena.github.io/Sparfels-web/", "pdf_url": "http://arxiv.org/pdf/2505.02178v3", "cate": "cs.CV", "date": "2025-05-04", "updated": "2025-07-29"}
{"id": "2412.09978", "title": "Coordinated vehicle dispatching and charging scheduling for an electric ride-hailing fleet under charging congestion and dynamic prices", "authors": ["Tai-Yu Ma", "Richard D. Connors", "Francesco Viti"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.09978v3", "summary": "Effective utilization of charging station capacity plays an important role in\nenhancing the profitability of ride-hailing systems using electric vehicles.\nExisting studies assume constant energy prices and uncapacitated charging\nstations or do not explicitly consider vehicle queueing at charging stations,\nresulting in over-optimistic charging infrastructure utilization. In this\nstudy, we develop a dynamic charging scheduling method (named CongestionAware)\nthat anticipates vehicles' energy needs and coordinates their charging\noperations with real-time energy prices to avoid long waiting time at charging\nstations and increase the total profit of the system. A sequential mixed\ninteger linear programming model is proposed to devise vehicles' day-ahead\ncharging plans based on their experienced charging waiting times and energy\nconsumption. The obtained charging plans are adapted within the day in response\nto vehicles' energy needs and charging station congestion. The developed\ncharging policy is tested using NYC yellow taxi data in a Manhattan-like study\narea with a fleet size of 100 vehicles given the scenarios of 3000 and 4000\ncustomers per day. The computational results show that our CongestionAware\npolicy outperforms different benchmark policies with up to +15.06% profit and\n+19.16% service rate for 4000 customers per day. Sensitivity analysis is\nconducted with different system parameters and managerial insights are\ndiscussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.09978v3", "cate": "math.OC", "date": "2024-12-13", "updated": "2025-07-30"}
{"id": "2507.22448", "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance", "authors": ["Jingwei Zuo", "Maksim Velikanov", "Ilyas Chahed", "Younes Belkada", "Dhia Eddine Rhayem", "Guillaume Kunsch", "Hakim Hacid", "Hamza Yous", "Brahim Farhat", "Ibrahim Khadraoui", "Mugariya Farooq", "Giulia Campesan", "Ruxandra Cojocaru", "Yasser Djilali", "Shi Hu", "Iheb Chaabane", "Puneesh Khanna", "Mohamed El Amine Seddik", "Ngoc Dung Huynh", "Phuc Le Khac", "Leen AlQadi", "Billel Mokeddem", "Mohamed Chami", "Abdalgader Abubaker", "Mikhail Lubinets", "Kacper Piskorski", "Slim Frikha"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Technical report of Falcon-H1 model series", "url": "http://arxiv.org/abs/2507.22448v1", "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.", "comment": "Technical report of Falcon-H1 model series", "pdf_url": "http://arxiv.org/pdf/2507.22448v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.09081", "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Jin-Ge Yao", "Bowen Qin", "Richeng Xuan", "Xi Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025 Demo", "url": "http://arxiv.org/abs/2506.09081v3", "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible at\nhttps://github.com/flageval-baai/FlagEvalMM.", "comment": "Accepted by ACL 2025 Demo", "pdf_url": "http://arxiv.org/pdf/2506.09081v3", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-28"}
{"id": "2507.15566", "title": "Prediction accuracy versus rescheduling flexibility in elective surgery management", "authors": ["Pieter Smet", "Martina Doneda", "Ettore Lanzarone", "Giuliana Carello"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15566v2", "summary": "The availability of downstream resources plays is critical in planning the\nadmission of elective surgery patients. The most crucial one is inpatient beds.\nTo ensure bed availability, hospitals may use machine learning (ML) models to\npredict patients' length-of-stay (LOS) in the admission planning stage.\nHowever, the real value of the LOS for each patient may differ from the\npredicted one, potentially making the schedule infeasible. To address such\ninfeasibilities, it is possible to implement rescheduling strategies that take\nadvantage of operational flexibility. For example, planners may postpone\nadmission dates, relocate patients to different wards, or even transfer\npatients who are already admitted among wards. A straightforward assumption is\nthat better LOS predictions can help reduce the impact of rescheduling.\nHowever, the training process of ML models that can make such accurate\npredictions can be very costly. Building on previous work that proposed\nsimulated ML for evaluating data-driven approaches, this paper explores the\nrelationship between LOS prediction accuracy and rescheduling flexibility\nacross various corrective policies. Specifically, we examine the most effective\npatient rescheduling strategies under LOS prediction errors to prevent bed\noverflows while optimizing resource utilization", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15566v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-29"}
{"id": "2505.03134", "title": "Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control", "authors": ["Sajjad Rezvani Boroujeni", "Hossein Abedi", "Tom Bush"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 7 figures, published in Computer and Decision Making - An International Journal (COMDEM)", "url": "http://arxiv.org/abs/2505.03134v3", "summary": "Visual defect detection in industrial glass manufacturing remains a critical\nchallenge due to the low frequency of defective products, leading to imbalanced\ndatasets that limit the performance of deep learning models and computer vision\nsystems. This paper presents a novel approach using Denoising Diffusion\nProbabilistic Models (DDPMs) to generate synthetic defective glass product\nimages for data augmentation, effectively addressing class imbalance issues in\nmanufacturing quality control and automated visual inspection. The methodology\nsignificantly enhances image classification performance of standard CNN\narchitectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting\nanomalies by increasing the minority class representation. Experimental results\ndemonstrate substantial improvements in key machine learning metrics,\nparticularly in recall for defective samples across all tested deep neural\nnetwork architectures while maintaining perfect precision on the validation\nset. The most dramatic improvement was observed in ResNet50V2's overall\nclassification accuracy, which increased from 78\\% to 93\\% when trained with\nthe augmented data. This work provides a scalable, cost-effective approach to\nenhancing automated defect detection in glass manufacturing that can\npotentially be extended to other industrial quality assurance systems and\nindustries with similar class imbalance challenges.", "comment": "12 pages, 7 figures, published in Computer and Decision Making - An\n  International Journal (COMDEM)", "pdf_url": "http://arxiv.org/pdf/2505.03134v3", "cate": "cs.CV", "date": "2025-05-06", "updated": "2025-07-29"}
{"id": "2503.05508", "title": "Design, Dynamic Modeling and Control of a 2-DOF Robotic Wrist Actuated by Twisted and Coiled Actuators", "authors": ["Yunsong Zhang", "Xinyu Zhou", "Feitian Zhang"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05508v2", "summary": "Artificial muscle-driven modular soft robots exhibit significant potential\nfor executing complex tasks. However, their broader applicability remains\nconstrained by the lack of dynamic model-based control strategies tailored for\nmulti-degree-of-freedom (DOF) configurations. This paper presents a novel\ndesign of a 2-DOF robotic wrist, envisioned as a fundamental building block for\nsuch advanced robotic systems. The wrist module is actuated by twisted and\ncoiled actuators (TCAs) and utilizes a compact 3RRRR parallel mechanism to\nachieve a lightweight structure with enhanced motion capability. A\ncomprehensive Lagrangian dynamic model is developed to capture the module's\ncomplex nonlinear behavior. Leveraging this model, a nonlinear model predictive\ncontroller (NMPC) is designed to ensure accurate trajectory tracking. A\nphysical prototype of the robotic wrist is fabricated, and extensive\nexperiments are performed to validate its motion performance and the fidelity\nof the proposed dynamic model. Subsequently, comparative evaluations between\nthe NMPC and a conventional PID controller are conducted under various\noperating conditions. Experimental results demonstrate the effectiveness and\nrobustness of the dynamic model-based control approach in managing the motion\nof TCA-driven robotic wrists. Finally, to illustrate its practical utility and\nintegrability, the wrist module is incorporated into a multi-segment soft\nrobotic arm, where it successfully executes a trajectory tracking task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05508v2", "cate": "cs.RO", "date": "2025-03-07", "updated": "2025-07-30"}
{"id": "2507.22457", "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models", "authors": ["Tian Yun", "Chen Sun", "Ellie Pavlick"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      CONLL 2025. Project webpage: this https URL", "url": "http://arxiv.org/abs/2507.22457v1", "summary": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill.", "comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.22457v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.10978", "title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Minjae Kim", "Jaewon Min", "Wooseok Jang", "Sangwu Lee", "Sayak Paul", "Susung Hong", "Seungryong Kim"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2506.10978v3", "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.", "comment": "Project page: https://cvlab-kaist.github.io/HeadHunter/", "pdf_url": "http://arxiv.org/pdf/2506.10978v3", "cate": "cs.CV", "date": "2025-06-12", "updated": "2025-07-29"}
{"id": "2507.17107", "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models", "authors": ["Andrii Balashov"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The manuscript has been withdrawn due to significant overlap in methodology and results with a prior work ( arXiv:2505.11711 ) that we were not aware of at the time of submission. To maintain academic integrity and avoid redundancy in the literature, we have chosen to withdraw this version", "url": "http://arxiv.org/abs/2507.17107v2", "summary": "Reinforcement learning (RL) is a key post-pretraining step for aligning large\nlanguage models (LLMs) with complex tasks and human preferences. While it is\noften assumed that RL fine-tuning requires updating most of a model's\nparameters, we challenge this assumption with a surprising finding: RL\nfine-tuning consistently modifies only a small subnetwork (typically 5-30% of\nweights), leaving most parameters unchanged. We call this phenomenon RL-induced\nparameter update sparsity. It arises naturally, without any sparsity\nconstraints or parameter-efficient tuning, and appears across multiple RL\nalgorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,\nMeta, and open-source LLMs). Moreover, the subnetworks updated by RL show\nsubstantial overlap across different seeds, datasets, and algorithms-far\nexceeding chance-suggesting a partially transferable structure in the\npretrained model. We show that fine-tuning only this sparse subnetwork recovers\nfull model performance and yields parameters nearly identical to the fully\nfine-tuned model. Our analysis suggests this sparsity emerges because RL\noperates near the model's original distribution, requiring only targeted\nchanges. KL penalties, gradient clipping, and on-policy dynamics have limited\neffect on the sparsity pattern. These findings shed new light on how RL adapts\nmodels: not by shifting all weights, but by focusing training on a small,\nconsistently updated subnetwork. This insight enables more efficient RL methods\nand reframes sparsity through the lens of the lottery ticket hypothesis.", "comment": "The manuscript has been withdrawn due to significant overlap in\n  methodology and results with a prior work (arXiv:2505.11711) that we were not\n  aware of at the time of submission. To maintain academic integrity and avoid\n  redundancy in the literature, we have chosen to withdraw this version", "pdf_url": "http://arxiv.org/pdf/2507.17107v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-28"}
{"id": "2505.05049", "title": "UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model", "authors": ["Timo Kaiser", "Thomas Norrenbrock", "Bodo Rosenhahn"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICML'25", "url": "http://arxiv.org/abs/2505.05049v4", "summary": "The introduction of the Segment Anything Model (SAM) has paved the way for\nnumerous semantic segmentation applications. For several tasks, quantifying the\nuncertainty of SAM is of particular interest. However, the ambiguous nature of\nthe class-agnostic foundation model SAM challenges current uncertainty\nquantification (UQ) approaches. This paper presents a theoretically motivated\nuncertainty quantification model based on a Bayesian entropy formulation\njointly respecting aleatoric, epistemic, and the newly introduced task\nuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ\nmethod. Our model traces the root of uncertainty back to under-parameterised\nmodels, insufficient prompts or image ambiguities. Our proposed deterministic\nUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,\nDAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ\nalternative that can support user-prompting, enhance semi-supervised pipelines,\nor balance the tradeoff between accuracy and cost efficiency.", "comment": "Accepted to ICML'25", "pdf_url": "http://arxiv.org/pdf/2505.05049v4", "cate": "cs.CV", "date": "2025-05-08", "updated": "2025-07-29"}
{"id": "2505.08370", "title": "Distributionally Robust LQG with Kullback-Leibler Ambiguity Sets", "authors": ["Marta Fochesato", "Lucia Falconi", "Mattia Zorzi", "Augusto Ferrante", "John Lygeros"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08370v2", "summary": "The Linear Quadratic Gaussian (LQG) controller is known to be inherently\nfragile to model misspecifications common in real-world situations. We consider\ndiscrete-time partially observable stochastic linear systems and provide a\nrobustification of the standard LQG against distributional uncertainties on the\nprocess and measurement noise. Our distributionally robust formulation\nspecifies the admissible perturbations by defining a relative entropy based\nambiguity set individually for each time step along a finite-horizon\ntrajectory, and minimizes the worst-case cost across all admissible\ndistributions. We prove that the optimal control policy is still linear, as in\nstandard LQG, and derive a computational scheme grounded on iterative best\nresponse that provably converges to the set of saddle points. Finally, we\nconsider the case of endogenous uncertainty captured via decision-dependent\nambiguity sets and we propose an approximation scheme based on dynamic\nprogramming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08370v2", "cate": "math.OC", "date": "2025-05-13", "updated": "2025-07-30"}
{"id": "2507.22462", "title": "IFEvalCode: Controlled Code Generation", "authors": ["Jian Yang", "Wei Zhang", "Shukai Liu", "Linzheng Chai", "Yingshui Tan", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou", "Guanglin Niu", "Zhoujun Li", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.22462v1", "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.22462v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.18985", "title": "GLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models", "authors": ["Guanxi Shen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Keywords: Explainable Computer Vision, Large Vision-Language Models, AI Interpretability, Explainable AI, Visual Saliency, Attribution Maps, Cross-Modal Attribution, Human Attention Alignment, AI Transparency", "url": "http://arxiv.org/abs/2506.18985v3", "summary": "Recent large vision-language models (LVLMs) have advanced capabilities in\nvisual question answering (VQA). However, interpreting where LVLMs direct their\nvisual attention remains a significant challenge, yet is essential for\nunderstanding model behavior. We introduce GLIMPSE (Gradient-Layer Importance\nMapping for Prompted Visual Saliency Explanation), a lightweight,\nmodel-agnostic framework that jointly attributes LVLM outputs to the most\nrelevant visual evidence and textual signals that support open-ended\ngeneration. GLIMPSE fuses gradient-weighted attention, adaptive layer\npropagation, and relevance-weighted token aggregation to produce holistic\nresponse-level heat maps for interpreting cross-modal reasoning, outperforming\nprior methods in faithfulness and pushing the state-of-the-art in\nhuman-attention alignment. We demonstrate an analytic approach to uncover\nfine-grained insights into LVLM cross-modal attribution, trace reasoning\ndynamics, analyze systematic misalignment, diagnose hallucination and bias, and\nensure transparency.", "comment": "Keywords: Explainable Computer Vision, Large Vision-Language Models,\n  AI Interpretability, Explainable AI, Visual Saliency, Attribution Maps,\n  Cross-Modal Attribution, Human Attention Alignment, AI Transparency", "pdf_url": "http://arxiv.org/pdf/2506.18985v3", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-07-29"}
{"id": "2507.19529", "title": "Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction", "authors": ["Obumneme Nwafor", "Mohammed Abdul Majeed Al Hooti"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Updated version", "url": "http://arxiv.org/abs/2507.19529v2", "summary": "As green hydrogen emerges as a major component of global decarbonisation,\nOman has positioned itself strategically through national auctions and\ninternational partnerships. Following two successful green hydrogen project\nrounds, the country launched its third auction (R3) in the Duqm region. While\nthis area exhibits relative geospatial homogeneity, it is still vulnerable to\nenvironmental fluctuations that pose inherent risks to productivity. Despite\ngrowing global investment in green hydrogen, operational data remains scarce,\nwith major projects like Saudi Arabia's NEOM facility not expected to commence\nproduction until 2026, and Oman's ACME Duqm project scheduled for 2028. This\nabsence of historical maintenance and performance data from large-scale\nhydrogen facilities in desert environments creates a major knowledge gap for\naccurate risk assessment for infrastructure planning and auction decisions.\nGiven this data void, environmental conditions emerge as accessible and\nreliable proxy for predicting infrastructure maintenance pressures, because\nharsh desert conditions such as dust storms, extreme temperatures, and humidity\nfluctuations are well-documented drivers of equipment degradation in renewable\nenergy systems. To address this challenge, this paper proposes an Artificial\nIntelligence decision support system that leverages publicly available\nmeteorological data to develop a predictive Maintenance Pressure Index (MPI),\nwhich predicts risk levels and future maintenance demands on hydrogen\ninfrastructure. This tool strengthens regulatory foresight and operational\ndecision-making by enabling temporal benchmarking to assess and validate\nperformance claims over time. It can be used to incorporate temporal risk\nintelligence into auction evaluation criteria despite the absence of historical\noperational benchmarks.", "comment": "Updated version", "pdf_url": "http://arxiv.org/pdf/2507.19529v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-29"}
{"id": "2506.02751", "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS", "authors": ["Chuanyu Fu", "Yuqi Zhang", "Kunbin Yao", "Guanying Chen", "Yuan Xiong", "Chuan Huang", "Shuguang Cui", "Xiaochun Cao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2506.02751v3", "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.", "comment": "ICCV 2025. Project page: https://fcyycf.github.io/RobustSplat/", "pdf_url": "http://arxiv.org/pdf/2506.02751v3", "cate": "cs.CV", "date": "2025-06-03", "updated": "2025-07-29"}
{"id": "2507.10078", "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Control Systems Letters", "url": "http://arxiv.org/abs/2507.10078v2", "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.", "comment": "Accepted to IEEE Control Systems Letters", "pdf_url": "http://arxiv.org/pdf/2507.10078v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-30"}
{"id": "2507.22478", "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 2 figures, work in progress", "url": "http://arxiv.org/abs/2507.22478v1", "summary": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql.", "comment": "16 pages, 2 figures, work in progress", "pdf_url": "http://arxiv.org/pdf/2507.22478v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.01918", "title": "End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning", "authors": ["Christian Bongiorno", "Efstratios Manolakis", "Rosario Nunzio Mantegna"], "categories": ["q-fin.PM", "cs.AI", "math.OC", "physics.data-an", "stat.ML", "91G10 (Primary) 68T07, 91G60, 62P05 (Secondary)", "I.2.6; I.5.1; G.3; J.4"], "primary_category": "Subjects:       Portfolio Management (q-fin.PM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01918v2", "summary": "We develop a rotation-invariant neural network that provides the global\nminimum-variance portfolio by jointly learning how to lag-transform historical\nreturns and how to regularise both the eigenvalues and the marginal\nvolatilities of large equity covariance matrices. This explicit mathematical\nmapping offers clear interpretability of each module's role, so the model\ncannot be regarded as a pure black-box. The architecture mirrors the analytical\nform of the global minimum-variance solution yet remains agnostic to dimension,\nso a single model can be calibrated on panels of a few hundred stocks and\napplied, without retraining, to one thousand US equities-a cross-sectional jump\nthat demonstrates robust out-of-sample generalisation. The loss function is the\nfuture realized minimum portfolio variance and is optimized end-to-end on real\ndaily returns. In out-of-sample tests from January 2000 to December 2024 the\nestimator delivers systematically lower realised volatility, smaller maximum\ndrawdowns, and higher Sharpe ratios than the best analytical competitors,\nincluding state-of-the-art non-linear shrinkage. Furthermore, although the\nmodel is trained end-to-end to produce an unconstrained (long-short)\nminimum-variance portfolio, we show that its learned covariance representation\ncan be used in general optimizers under long-only constraints with virtually no\nloss in its performance advantage over competing estimators. These gains\npersist when the strategy is executed under a highly realistic implementation\nframework that models market orders at the auctions, empirical slippage,\nexchange fees, and financing charges for leverage, and they remain stable\nduring episodes of acute market stress.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01918v2", "cate": "q-fin.PM", "date": "2025-07-02", "updated": "2025-07-29"}
{"id": "2507.19846", "title": "A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets", "authors": ["Harish Saragadam", "Chetana K Nayak", "Joy Bose"], "categories": ["cs.LG", "cs.IR", "68T50", "I.2.7; I.2.6; H.3.3; H.4.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 7 figures", "url": "http://arxiv.org/abs/2507.19846v2", "summary": "Resolution of incidents or problem tickets is a common theme in service\nindustries in any sector, including billing and charging systems in telecom\ndomain. Machine learning can help to identify patterns and suggest resolutions\nfor the problem tickets, based on patterns in the historical data of the\ntickets. However, this process may be complicated due to a variety of phenomena\nsuch as data drift and issues such as missing data, lack of data pertaining to\nresolutions of past incidents, too many similar sounding resolutions due to\nfree text and similar sounding text. This paper proposes a robust ML-driven\nsolution employing clustering, supervised learning, and advanced NLP models to\ntackle these challenges effectively. Building on previous work, we demonstrate\nclustering-based resolution identification, supervised classification with LDA,\nSiamese networks, and One-shot learning, Index embedding. Additionally, we\npresent a real-time dashboard and a highly available Kubernetes-based\nproduction deployment. Our experiments with both the open-source Bitext\ncustomer-support dataset and proprietary telecom datasets demonstrate high\nprediction accuracy.", "comment": "9 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.19846v2", "cate": "cs.LG", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2506.19651", "title": "PEVLM: Parallel Encoding for Vision-Language Models", "authors": ["Letian Kang", "Shixian Luo", "Yiqiang Li", "Yuxin Yin", "Shenxuan Zhou", "Xiaoyang Yu", "Jin Yang", "Yong Wu"], "categories": ["cs.CV", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.19651v3", "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\nmultimodal understanding and generation tasks. However, their application to\nlong video understanding remains hindered by the quadratic complexity of\nstandard attention mechanisms. In this work, we introduce \\textbf{PEVLM}, a\nfine-tuning-free parallel encoding method designed to enhance the prefilling\nefficiency of VLMs in long video scenarios. PEVLM partitions the input video\ninto context blocks with a shared sink block, while preserving sequential\nposition embeddings to align the attention weight distribution with that of\nFull-Attention. This design reduces attention complexity from $O((T \\times\nN)^2)$ to $O(T \\times N)$ where $T$ is the number of frames and $N$ the number\nof tokens per frame, without sacrificing accuracy. Extensive experiments across\nmultiple state-of-the-art models and benchmarks demonstrate that PEVLM\nconsistently outperforms existing parallel encoding approaches, achieving up to\n\\textbf{7.47x} speedup in attention computation and reducing end-to-end latency\nby \\textbf{40\\%}. Remarkably, PEVLM not only maintains high accuracy, but in\nsome settings even surpasses Full-Attention performance. Under strict latency\nconstraints, it achieves substantial gains, improving accuracy from\n\\textbf{23.26\\%} to \\textbf{61.03\\%}. These results underscore the\neffectiveness of PEVLM for low-latency, long-context video understanding,\nmaking it a promising solution for real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19651v3", "cate": "cs.CV", "date": "2025-06-24", "updated": "2025-07-29"}
{"id": "2507.19947", "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "authors": ["Supawich Sitdhipol", "Waritwong Sukprasongdee", "Ekapol Chuangsuwanich", "Rina Tse"], "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to the 2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC); Supplementary video: this https URL", "url": "http://arxiv.org/abs/2507.19947v2", "summary": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance.", "comment": "Accepted to the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC); Supplementary video: https://cu-asl.github.io/fp-lgn/", "pdf_url": "http://arxiv.org/pdf/2507.19947v2", "cate": "cs.RO", "date": "2025-07-26", "updated": "2025-07-30"}
{"id": "2507.22533", "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records", "authors": ["Dongchen Li", "Jitao Liang", "Wei Li", "Xiaoyu Wang", "Longbing Cao", "Kun Yu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22533v1", "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22533v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.04270", "title": "ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts", "authors": ["Sangbum Choi", "Kyeongryeol Go", "Taewoong Jang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures", "url": "http://arxiv.org/abs/2507.04270v3", "summary": "Foundation models have revolutionized AI, yet they struggle with zero-shot\ndeployment in real-world industrial settings due to a lack of high-quality,\ndomain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an\nindustry-ready vision foundation model that leverages multi-modal prompting\n(textual and visual) for generalization without retraining. Trained on a\ncompact yet representative 0.9 million annotated samples from a proprietary\nbillion-scale industrial dataset, ZERO demonstrates competitive performance on\nacademic benchmarks like LVIS-Val and significantly outperforms existing models\nacross 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in\nthe CVPR 2025 Object Instance Detection Challenge and 4th place in the\nFoundational Few-shot Object Detection Challenge, highlighting its practical\ndeployability and generalizability with minimal adaptation and limited data. To\nthe best of our knowledge, ZERO is the first vision foundation model explicitly\nbuilt for domain-specific, zero-shot industrial applications.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.04270v3", "cate": "cs.CV", "date": "2025-07-06", "updated": "2025-07-29"}
{"id": "1909.06064", "title": "Active learning for level set estimation under input uncertainty and its extensions", "authors": ["Yu Inatsu", "Masayuki Karasuyama", "Keiichi Inoue", "Ichiro Takeuchi"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      38 pages, 12 figures", "url": "http://arxiv.org/abs/1909.06064v2", "summary": "Testing under what conditions the product satisfies the desired properties is\na fundamental problem in manufacturing industry. If the condition and the\nproperty are respectively regarded as the input and the output of a black-box\nfunction, this task can be interpreted as the problem called Level Set\nEstimation (LSE) -- the problem of identifying input regions such that the\nfunction value is above (or below) a threshold. Although various methods for\nLSE problems have been developed so far, there are still many issues to be\nsolved for their practical usage. As one of such issues, we consider the case\nwhere the input conditions cannot be controlled precisely, i.e., LSE problems\nunder input uncertainty. We introduce a basic framework for handling input\nuncertainty in LSE problem, and then propose efficient methods with proper\ntheoretical guarantees. The proposed methods and theories can be generally\napplied to a variety of challenges related to LSE under input uncertainty such\nas cost-dependent input uncertainties and unknown input uncertainties. We apply\nthe proposed methods to artificial and real data to demonstrate the\napplicability and effectiveness.", "comment": "38 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/1909.06064v2", "cate": "stat.ML", "date": "2019-09-13", "updated": "2025-07-29"}
{"id": "2507.09242", "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM International Conference on Multimedia 2025", "url": "http://arxiv.org/abs/2507.09242v2", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "comment": "ACM International Conference on Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.09242v2", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-29"}
{"id": "2507.22542", "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support", "authors": ["Long S. T. Nguyen", "Truong P. Hua", "Thanh M. Nguyen", "Toan Q. Pham", "Nam K. Ngo", "An X. Nguyen", "Nghi D. M. Pham", "Nghia H. Nguyen", "Tho T. Quan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under review at ICCCI 2025", "url": "http://arxiv.org/abs/2507.22542v1", "summary": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.", "comment": "Under review at ICCCI 2025", "pdf_url": "http://arxiv.org/pdf/2507.22542v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.05714", "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation", "authors": ["YiHan Jiao", "ZheHao Tan", "Dan Yang", "DuoLin Sun", "Jie Feng", "Yue Shen", "Jian Wang", "Peng Wei"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05714v2", "summary": "Retrieval-augmented generation (RAG) has become a fundamental paradigm for\naddressing the challenges faced by large language models in handling real-time\ninformation and domain-specific problems. Traditional RAG systems primarily\nrely on the in-context learning (ICL) capabilities of the large language model\nitself. Still, in-depth research on the specific capabilities needed by the RAG\ngeneration model is lacking, leading to challenges with inconsistent document\nquality and retrieval system imperfections. Even the limited studies that\nfine-tune RAG generative models often \\textit{lack a granular focus on RAG\ntask} or \\textit{a deeper utilization of chain-of-thought processes}. To\naddress this, we propose that RAG models should possess three progressively\nhierarchical abilities (1) Filtering: the ability to select relevant\ninformation; (2) Combination: the ability to combine semantic information\nacross paragraphs; and (3) RAG-specific reasoning: the ability to further\nprocess external knowledge using internal knowledge. Thus, we introduce our new\nRAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning\nRetrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\"\nstrategy. This method enhances the model's open-book examination capability by\nutilizing multi-level progressive chain-of-thought. Experiments show that the\nHIRAG training strategy significantly improves the model's performance on\ndatasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05714v2", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-29"}
{"id": "2402.12335", "title": "Image Super-resolution Inspired Electron Density Prediction", "authors": ["Chenghan Li", "Or Sharir", "Shunyue Yuan", "Garnet K. Chan"], "categories": ["physics.chem-ph", "cs.LG"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.12335v2", "summary": "Drawing inspiration from the domain of image super-resolution, we view the\nelectron density as a 3D grayscale image and use a convolutional residual\nnetwork to transform a crude and trivially generated guess of the molecular\ndensity into an accurate ground-state quantum mechanical density. We find that\nthis model outperforms all prior density prediction approaches. Because the\ninput is itself a real-space density, the predictions are equivariant to\nmolecular symmetry transformations even though the model is not constructed to\nbe. Due to its simplicity, the model is directly applicable to unseen molecular\nconformations and chemical elements. We show that fine-tuning on limited new\ndata provides high accuracy even in challenging cases of exotic elements and\ncharge states. Our work suggests new routes to learning real-space physical\nquantities drawing from the established ideas of image processing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.12335v2", "cate": "physics.chem-ph", "date": "2024-02-19", "updated": "2025-07-29"}
{"id": "2507.10095", "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text", "authors": ["Bingchao Wang", "Zhiwei Ning", "Jianyu Ding", "Xuanang Gao", "Yin Li", "Dongsheng Jiang", "Jie Yang", "Wei Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.10095v2", "summary": "CLIP has shown promising performance across many short-text tasks in a\nzero-shot manner. However, limited by the input length of the text encoder,\nCLIP struggles on under-stream tasks with long-text inputs ($>77$ tokens). To\nremedy this issue, we propose FIX-CLIP, which includes three novel modules: (1)\nA dual-branch training pipeline that aligns short and long texts with masked\nand raw images, respectively, which boosts the long-text representation while\npreserving the short-text ability. (2) Multiple learnable regional prompts with\nunidirectional masks in Transformer layers for regional information extraction.\n(3) A hierarchical feature alignment module in the intermediate encoder layers\nto promote the consistency of multi-scale features. Furthermore, we collect 30M\nimages and utilize existing MLLMs to synthesize long-text captions for\ntraining. Extensive experiments show that FIX-CLIP achieves state-of-the-art\nperformance on both long-text and short-text retrieval benchmarks. For\ndownstream applications, we reveal that FIX-CLIP's text encoder delivers\npromising performance in a plug-and-play manner for diffusion models with\nlong-text input. The code is available at\nhttps://github.com/bcwang-sjtu/Fix-CLIP.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.10095v2", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-29"}
{"id": "2507.22545", "title": "ControlMed: Adding Reasoning Control to Medical Language Model", "authors": ["Sung-Min Lee", "Siyoon Lee", "Juyeon Kim", "Kyungmin Roh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.22545v1", "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.22545v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.08920", "title": "AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model", "authors": ["Changze Lv", "Jiang Zhou", "Siyu Long", "Lihao Wang", "Jiangtao Feng", "Dongyu Xue", "Yu Pei", "Hao Wang", "Zherui Zhang", "Yuchen Cai", "Zhiqiang Gao", "Ziyuan Ma", "Jiakai Hu", "Chaochen Gao", "Jingjing Gong", "Yuxuan Song", "Shuyi Zhang", "Xiaoqing Zheng", "Deyi Xiong", "Lei Bai", "Wanli Ouyang", "Ya-Qin Zhang", "Wei-Ying Ma", "Bowen Zhou", "Hao Zhou"], "categories": ["q-bio.BM", "cs.AI"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08920v2", "summary": "We introduce AMix-1, a powerful protein foundation model built on Bayesian\nFlow Networks and empowered by a systematic training methodology, encompassing\npretraining scaling laws, emergent capability analysis, in-context learning\nmechanism, and test-time scaling algorithm. To guarantee robust scalability, we\nestablish a predictive scaling law and reveal the progressive emergence of\nstructural understanding via loss perspective, culminating in a strong\n1.7-billion model. Building on this foundation, we devise a multiple sequence\nalignment (MSA)-based in-context learning strategy to unify protein design into\na general framework, where AMix-1 recognizes deep evolutionary signals among\nMSAs and consistently generates structurally and functionally coherent\nproteins. This framework enables the successful design of a dramatically\nimproved AmeR variant with an up to $50\\times$ activity increase over its wild\ntype. Pushing the boundaries of protein engineering, we further empower AMix-1\nwith an evolutionary test-time scaling algorithm for in silico directed\nevolution that delivers substantial, scalable performance gains as verification\nbudgets are intensified, laying the groundwork for next-generation\nlab-in-the-loop protein design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08920v2", "cate": "q-bio.BM", "date": "2025-07-11", "updated": "2025-07-29"}
{"id": "2406.02470", "title": "Meta-Designing Quantum Experiments with Language Models", "authors": ["Sören Arlt", "Haonan Duan", "Felix Li", "Sang Michael Xie", "Yuhuai Wu", "Mario Krenn"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      8+23 pages, 5 figures", "url": "http://arxiv.org/abs/2406.02470v2", "summary": "Artificial Intelligence (AI) can solve complex scientific problems beyond\nhuman capabilities, but the resulting solutions offer little insight into the\nunderlying physical principles. One prominent example is quantum physics, where\ncomputers can discover experiments for the generation of specific quantum\nstates, but it is unclear how finding general design concepts can be automated.\nHere, we address this challenge by training a transformer-based language model\nto create human-readable Python code, which solves an entire class of problems\nin a single pass. This strategy, which we call meta-design, enables scientists\nto gain a deeper understanding and extrapolate to larger experiments without\nadditional optimization. To demonstrate the effectiveness of our approach, we\nuncover previously unknown experimental generalizations of important quantum\nstates, e.g. from condensed matter physics. The underlying methodology of\nmeta-design can naturally be extended to fields such as materials science or\nengineering.", "comment": "8+23 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2406.02470v2", "cate": "quant-ph", "date": "2024-06-04", "updated": "2025-07-29"}
{"id": "2507.11245", "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models", "authors": ["X. Feng", "H. Yu", "M. Wu", "S. Hu", "J. Chen", "C. Zhu", "J. Wu", "X. Chu", "K. Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.11245v2", "summary": "With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.", "comment": "Project Page: https://amap-ml.github.io/NarrLV-Website/", "pdf_url": "http://arxiv.org/pdf/2507.11245v2", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-29"}
{"id": "2507.22564", "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs", "authors": ["Xikang Yang", "Biyu Zhou", "Xuehai Tang", "Jizhong Han", "Songlin Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22564v1", "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22564v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.13420", "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery", "authors": ["Alessandro Pistola", "Valentina Orru'", "Nicolo' Marchetti", "Marco Roccetti"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      25 pages, 9 Figures", "url": "http://arxiv.org/abs/2507.13420v2", "summary": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization", "comment": "25 pages, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2507.13420v2", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-29"}
{"id": "2406.03616", "title": "BEACON: A Bayesian Optimization Strategy for Novelty Search in Expensive Black-Box Systems", "authors": ["Wei-Ting Tang", "Ankush Chakrabarty", "Joel A. Paulson"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.03616v4", "summary": "Novelty search (NS) refers to a class of exploration algorithms that seek to\nuncover diverse system behaviors through simulations or experiments. Such\ndiversity is central to many AI-driven discovery and design tasks, including\nmaterial and drug development, neural architecture search, and reinforcement\nlearning. However, existing NS methods typically rely on evolutionary\nstrategies and other meta-heuristics that require dense sampling of the input\nspace, making them impractical for expensive black-box systems. In this work,\nwe introduce BEACON, a sample-efficient, Bayesian optimization-inspired\napproach to NS that is tailored for settings where the input-to-behavior\nrelationship is opaque and costly to evaluate. BEACON models this mapping using\nmulti-output Gaussian processes (MOGPs) and selects new inputs by maximizing a\nnovelty metric computed from posterior samples of the MOGP, effectively\nbalancing the exploration-exploitation trade-off. By leveraging recent advances\nin posterior sampling and high-dimensional GP modeling, our method remains\nscalable to large input spaces and datasets. We evaluate BEACON across ten\nsynthetic benchmarks and eight real-world tasks, including the design of\ndiverse materials for clean energy applications. Our results show that BEACON\nsignificantly outperforms existing NS baselines, consistently discovering a\nbroader set of behaviors under tight evaluation budgets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.03616v4", "cate": "stat.ML", "date": "2024-06-05", "updated": "2025-07-30"}
{"id": "2507.11642", "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11642v2", "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11642v2", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-29"}
{"id": "2507.22581", "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Krisna Mahardika Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Our code and dataset are made available at this https URL", "url": "http://arxiv.org/abs/2507.22581v1", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.", "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "pdf_url": "http://arxiv.org/pdf/2507.22581v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.13614", "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2412.03025", "url": "http://arxiv.org/abs/2507.13614v2", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "pdf_url": "http://arxiv.org/pdf/2507.13614v2", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-29"}
{"id": "2408.15393", "title": "Linear Stability Analysis of Physics-Informed Random Projection Neural Networks for ODEs", "authors": ["Gianluca Fabiani", "Erik Bollt", "Constantinos Siettos", "Athanasios N. Yannacopoulos"], "categories": ["math.NA", "cs.LG", "cs.NA", "math.DS", "65L20, 68T07, 65L04, 37N30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      17 pages, 3 figures", "url": "http://arxiv.org/abs/2408.15393v2", "summary": "We present a linear stability analysis of physics-informed random projection\nneural networks (PI-RPNNs), for the numerical solution of {the initial value\nproblem (IVP)} of (stiff) ODEs. We begin by proving that PI-RPNNs are uniform\napproximators of the solution to ODEs. We then provide a constructive proof\ndemonstrating that PI-RPNNs offer consistent and asymptotically stable\nnumerical schemes, thus convergent schemes. In particular, we prove that\nmulti-collocation PI-RPNNs guarantee asymptotic stability. Our theoretical\nresults are illustrated via numerical solutions of benchmark examples including\nindicative comparisons with the backward Euler method, the midpoint method, the\ntrapezoidal rule, the 2-stage Gauss scheme, and the 2- and 3-stage Radau\nschemes.", "comment": "17 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2408.15393v2", "cate": "math.NA", "date": "2024-08-27", "updated": "2025-07-29"}
{"id": "2507.12857", "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation", "authors": ["Shiqi Huang", "Shuting He", "Huaiyuan Qin", "Bihan Wen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight), code see this https URL", "url": "http://arxiv.org/abs/2507.12857v2", "summary": "Most existing remote sensing instance segmentation approaches are designed\nfor close-vocabulary prediction, limiting their ability to recognize novel\ncategories or generalize across datasets. This restricts their applicability in\ndiverse Earth observation scenarios. To address this, we introduce\nopen-vocabulary (OV) learning for remote sensing instance segmentation. While\ncurrent OV segmentation models perform well on natural image datasets, their\ndirect application to remote sensing faces challenges such as diverse\nlandscapes, seasonal variations, and the presence of small or ambiguous objects\nin aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$\n($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary\n$\\textbf{RE}$mote sensing instance segmentation), a framework that integrates\nmulti-granularity scene context, i.e., regional context and global context, to\nenhance both visual and textual representations. Specifically, we introduce\nRegion-Aware Integration, which refines class embeddings with regional context\nto improve object distinguishability. Additionally, we propose Global Context\nAdaptation, which enriches naive text embeddings with remote sensing global\ncontext, creating a more adaptable and expressive linguistic latent space for\nthe classifier. We establish new benchmarks for OV remote sensing instance\nsegmentation across diverse datasets. Experimental results demonstrate that,\nour proposed method achieves SOTA performance, which provides a robust solution\nfor large-scale, real-world geospatial analysis. Our code is available at\nhttps://github.com/HuangShiqi128/SCORE.", "comment": "ICCV 2025 (Highlight), code see\n  https://github.com/HuangShiqi128/SCORE", "pdf_url": "http://arxiv.org/pdf/2507.12857v2", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-29"}
{"id": "2507.22603", "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models", "authors": ["Rawan Al-Matham", "Kareem Darwish", "Raghad Al-Rasheed", "Waad Alshammari", "Muneera Alhoshan", "Amal Almazrua", "Asma Al Wazrah", "Mais Alheraki", "Firoj Alam", "Preslav Nakov", "Norah Alzahrani", "Eman alBilali", "Nizar Habash", "Abdelrahman El-Sheikh", "Muhammad Elmallah", "Haonan Li", "Hamdy Mubarak", "Mohamed Anwar", "Zaid Alyafeai", "Ahmed Abdelali", "Nora Altwairesh", "Maram Hasanain", "Abdulmohsen Al Thubaity", "Shady Shehata", "Bashar Alhafni", "Injy Hamed", "Go Inoue", "Khalid Elmadani", "Ossama Obeid", "Fatima Haouari", "Tamer Elsayed", "Emad Alghamdi", "Khalid Almubarak", "Saied Alshahrani", "Ola Aljarrah", "Safa Alajlan", "Areej Alshaqarawi", "Maryam Alshihri", "Sultana Alghurabi", "Atikah Alzeghayer", "Afrah Altamimi", "Abdullah Alfaifi", "Abdulrahman AlOsaimy"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22603v1", "summary": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22603v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.14811", "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models", "authors": ["Jiaji Zhang", "Ruichao Sun", "Hailiang Zhao", "Jiaju Wu", "Peng Chen", "Hao Li", "Yuying Liu", "Kingsum Chow", "Gang Xiong", "Shuiguang Deng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14811v3", "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14811v3", "cate": "cs.CV", "date": "2025-07-20", "updated": "2025-07-29"}
{"id": "2409.14089", "title": "Quantum enhanced stratification of Breast Cancer: exploring quantum expressivity for real omics data", "authors": ["Valeria Repetto", "Elia Giuseppe Ceroni", "Giuseppe Buonaiuto", "Romina D'Aurizio"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2409.14089v2", "summary": "Quantum Machine Learning (QML) is considered one of the most promising\napplications of Quantum Computing in the Noisy Intermediate Scale Quantum\n(NISQ) era for the impact it is thought to have in the near future. Although\npromising theoretical assumptions, the exploration of how QML could foster new\ndiscoveries in Medicine and Biology fields is still in its infancy with few\nexamples. In this study, we aimed to assess whether Quantum Kernels (QK) could\neffectively classify subtypes of Breast Cancer (BC) patients on the basis of\nmolecular characteristics. We performed an heuristic exploration of encoding\nconfigurations with different entanglement levels to determine a trade-off\nbetween kernel expressivity and performances. Our results show that QKs yield\ncomparable clustering results with classical methods while using fewer data\npoints, and are able to fit the data with a higher number of clusters.\nAdditionally, we conducted the experiments on the Quantum Processing Unit (QPU)\nto evaluate the effect of noise on the outcome. We found that less expressive\nencodings showed a higher resilience to noise, indicating that the\ncomputational pipeline can be reliably implemented on the NISQ devices. Our\nfindings suggest that QK methods show promises for application in Precision\nOncology, especially in scenarios where the dataset is limited in size and a\ngranular non-trivial stratification of complex molecular data cannot be\nachieved classically.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2409.14089v2", "cate": "quant-ph", "date": "2024-09-21", "updated": "2025-07-29"}
{"id": "2507.13568", "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning", "authors": ["Kaihong Wang", "Donghyun Kim", "Margrit Betke"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13568v2", "summary": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13568v2", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-29"}
{"id": "2507.22608", "title": "Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Yusser Al Ghussin", "Tanja Baeumel", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      preprint", "url": "http://arxiv.org/abs/2507.22608v1", "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2507.22608v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.16876", "title": "Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review", "authors": ["Charlotte Jennings", "Andrew Broad", "Lucy Godson", "Emily Clarke", "David Westhead", "Darren Treanor"], "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      Main article (50 pages, inc 3 tables, 4 figures). Supplementary material included with additional methodological information and data", "url": "http://arxiv.org/abs/2507.16876v2", "summary": "Multimodal machine learning integrating histopathology and molecular data\nshows promise for cancer prognostication. We systematically reviewed studies\ncombining whole slide images (WSIs) and high-throughput omics to predict\noverall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL\n(12/08/2024), plus citation screening, identified eligible studies. Data\nextraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed\nSWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).\n  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all\nused The Cancer Genome Atlas. Approaches included regularised Cox regression\n(n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged\n0.550-0.857; multimodal models typically outperformed unimodal ones. However,\nall studies showed unclear/high bias, limited external validation, and little\nfocus on clinical utility.\n  Multimodal WSI-omics survival prediction is a fast-growing field with\npromising results but needs improved methodological rigor, broader datasets,\nand clinical evaluation.\n  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687),\nsupported by UKRI Industrial Strategy Challenge Fund.", "comment": "Main article (50 pages, inc 3 tables, 4 figures). Supplementary\n  material included with additional methodological information and data", "pdf_url": "http://arxiv.org/pdf/2507.16876v2", "cate": "q-bio.QM", "date": "2025-07-22", "updated": "2025-07-29"}
{"id": "2410.10381", "title": "Collaborative filtering based on nonnegative/binary matrix factorization", "authors": ["Yukino Terui", "Yuka Inoue", "Yohei Hamakawa", "Kosuke Tatsumura", "Kazue Kudo"], "categories": ["cond-mat.stat-mech", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Statistical Mechanics (cond-mat.stat-mech)", "pdf_link": null, "comments": "Comments:      12 pages, 8 figures", "url": "http://arxiv.org/abs/2410.10381v4", "summary": "Collaborative filtering generates recommendations by exploiting user-item\nsimilarities based on rating data, which often contains numerous unrated items.\nTo predict scores for unrated items, matrix factorization techniques such as\nnonnegative matrix factorization (NMF) are often employed. Nonnegative/binary\nmatrix factorization (NBMF), which is an extension of NMF, approximates a\nnonnegative matrix as the product of nonnegative and binary matrices. While\nprevious studies have applied NBMF primarily to dense data such as images, this\npaper proposes a modified NBMF algorithm tailored for collaborative filtering\nwith sparse data. In the modified method, unrated entries in the rating matrix\nare masked, enhancing prediction accuracy. Furthermore, utilizing a low-latency\nIsing machine in NBMF is advantageous in terms of the computation time, making\nthe proposed method beneficial.", "comment": "12 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2410.10381v4", "cate": "cond-mat.stat-mech", "date": "2024-10-14", "updated": "2025-07-29"}
{"id": "2507.13985", "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation", "authors": ["Haoran Li", "Yuli Tian", "Kun Lan", "Yong Liao", "Lin Wang", "Pan Hui", "Peng Yuan Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Extended version of ECCV 2024 paper \"DreamScene\"", "url": "http://arxiv.org/abs/2507.13985v2", "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://jahnsonblack.github.io/DreamScene-Full/.", "comment": "Extended version of ECCV 2024 paper \"DreamScene\"", "pdf_url": "http://arxiv.org/pdf/2507.13985v2", "cate": "cs.CV", "date": "2025-07-18", "updated": "2025-07-29"}
{"id": "2507.22623", "title": "Multilingual Political Views of Large Language Models: Identification and Steering", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Ivan Vykopal", "Josef van Genabith", "Simon Ostermann", "Roberto Zamparelli"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      pre-print", "url": "http://arxiv.org/abs/2507.22623v1", "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.", "comment": "pre-print", "pdf_url": "http://arxiv.org/pdf/2507.22623v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.17860", "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "authors": ["Ko Watanabe", "Stanislav Frolov", "Adriano Lucieri", "Andreas Dengel"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17860v2", "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17860v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-29"}
{"id": "2412.20802", "title": "Robust Matrix Completion for Discrete Rating-Scale Data: Coping with Fake Profiles in Recommender Systems", "authors": ["Aurore Archimbaud", "Andreas Alfons", "Ines Wilms"], "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.20802v2", "summary": "Recommender systems are essential tools in the digital landscape for\nconnecting users with content that more closely aligns with their preferences.\nMatrix completion is a widely used statistical framework for such systems,\naiming to predict a user's preferences for items they have not yet rated by\nleveraging the observed ratings in a partially filled user-item rating matrix.\nRealistic applications of matrix completion in recommender systems must address\nseveral challenges that are too often neglected: (i) the discrete nature of\nrating-scale data, (ii) the presence of malicious users who manipulate the\nsystem to their advantage through the creation of fake profiles, and (iii)\nmissing-not-at-random patterns, where users are more likely to rate items they\nexpect to enjoy. Our goal in this paper is twofold. First, we propose a novel\nmatrix completion method, robust discrete matrix completion (RDMC), designed\nspecifically to handle the discrete nature of sparse rating-scale data and to\nremain reliable in the presence of adversarial manipulation. We evaluate RDMC\nthrough carefully designed experiments and realistic case studies. Our work\ntherefore, secondly, offers a statistically-sound blueprint for future studies\non how to evaluate matrix completion methods for recommender systems under\nrealistic scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.20802v2", "cate": "stat.ML", "date": "2024-12-30", "updated": "2025-07-29"}
{"id": "2507.14686", "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14686v2", "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14686v2", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-29"}
{"id": "2507.22676", "title": "Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures, ACM MM 2025. github: this https URL", "url": "http://arxiv.org/abs/2507.22676v1", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects.", "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "pdf_url": "http://arxiv.org/pdf/2507.22676v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.18177", "title": "Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios", "authors": ["Dhruv Jain", "Romain Modzelewski", "Romain Herault", "Clement Chatelain", "Eva Torfeh", "Sebastien Thureau"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18177v2", "summary": "In data-scarce scenarios, deep learning models often overfit to noise and\nirrelevant patterns, which limits their ability to generalize to unseen\nsamples. To address these challenges in medical image segmentation, we\nintroduce Diff-UMamba, a novel architecture that combines the UNet framework\nwith the mamba mechanism to model long-range dependencies. At the heart of\nDiff-UMamba is a noise reduction module, which employs a signal differencing\nstrategy to suppress noisy or irrelevant activations within the encoder. This\nencourages the model to filter out spurious features and enhance task-relevant\nrepresentations, thereby improving its focus on clinically significant regions.\nAs a result, the architecture achieves improved segmentation accuracy and\nrobustness, particularly in low-data settings. Diff-UMamba is evaluated on\nmultiple public datasets, including medical segmentation decathalon dataset\n(lung and pancreas) and AIIB23, demonstrating consistent performance gains of\n1-3% over baseline methods in various segmentation tasks. To further assess\nperformance under limited data conditions, additional experiments are conducted\non the BraTS-21 dataset by varying the proportion of available training\nsamples. The approach is also validated on a small internal non-small cell lung\ncancer dataset for the segmentation of gross tumor volume in cone beam CT,\nwhere it achieves a 4-5% improvement over baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18177v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-29"}
{"id": "2501.11673", "title": "Randomized Kaczmarz Methods with Beyond-Krylov Convergence", "authors": ["Michał Dereziński", "Deanna Needell", "Elizaveta Rebrova", "Jiaming Yang"], "categories": ["math.NA", "cs.DS", "cs.LG", "cs.NA", "math.OC", "stat.ML"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      SIMAX", "url": "http://arxiv.org/abs/2501.11673v2", "summary": "Randomized Kaczmarz methods form a family of linear system solvers which\nconverge by repeatedly projecting their iterates onto randomly sampled\nequations. While effective in some contexts, such as highly over-determined\nleast squares, Kaczmarz methods are traditionally deemed secondary to Krylov\nsubspace methods, since this latter family of solvers can exploit outliers in\nthe input's singular value distribution to attain fast convergence on\nill-conditioned systems.\n  In this paper, we introduce Kaczmarz++, an accelerated randomized block\nKaczmarz algorithm that exploits outlying singular values in the input to\nattain a fast Krylov-style convergence. Moreover, we show that Kaczmarz++\ncaptures large outlying singular values provably faster than popular Krylov\nmethods, for both over- and under-determined systems. We also develop an\noptimized variant for positive semidefinite systems, called CD++, demonstrating\nempirically that it is competitive in arithmetic operations with both CG and\nGMRES on a collection of benchmark problems. To attain these results, we\nintroduce several novel algorithmic improvements to the Kaczmarz framework,\nincluding adaptive momentum acceleration, Tikhonov-regularized projections, and\na memoization scheme for reusing information from previously sampled equation\nblocks.", "comment": "SIMAX", "pdf_url": "http://arxiv.org/pdf/2501.11673v2", "cate": "math.NA", "date": "2025-01-20", "updated": "2025-07-29"}
{"id": "2507.16518", "title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning", "authors": ["Xiuwei Chen", "Wentao Hu", "Hanhui Li", "Jun Zhou", "Zisheng Chen", "Meng Cao", "Yihan Zeng", "Kui Zhang", "Yu-Jie Yuan", "Jianhua Han", "Hang Xu", "Xiaodan Liang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16518v2", "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nimpressive reasoning capabilities. However, further enhancing existing MLLMs\nnecessitates high-quality vision-language datasets with carefully curated task\ncomplexities, which are both costly and challenging to scale. Although recent\nself-improving models that iteratively refine themselves offer a feasible\nsolution, they still suffer from two core challenges: (i) most existing methods\naugment visual or textual data separately, resulting in discrepancies in data\ncomplexity (e.g., over-simplified diagrams paired with redundant textual\ndescriptions); and (ii) the evolution of data and models is also separated,\nleading to scenarios where models are exposed to tasks with mismatched\ndifficulty levels. To address these issues, we propose C2-Evo, an automatic,\nclosed-loop self-improving framework that jointly evolves both training data\nand model capabilities. Specifically, given a base dataset and a base model,\nC2-Evo enhances them by a cross-modal data evolution loop and a data-model\nevolution loop. The former loop expands the base dataset by generating complex\nmultimodal problems that combine structured textual sub-problems with\niteratively specified geometric diagrams, while the latter loop adaptively\nselects the generated problems based on the performance of the base model, to\nconduct supervised fine-tuning and reinforcement learning alternately.\nConsequently, our method continuously refines its model and training data, and\nconsistently obtains considerable performance gains across multiple\nmathematical reasoning benchmarks. Our code, models, and datasets will be\nreleased.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16518v2", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-29"}
{"id": "2507.22716", "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs", "authors": ["Jie He", "Victor Gutierrez Basulto", "Jeff Z. Pan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22716v1", "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22716v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.20133", "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering", "authors": ["Anas Mohamed", "Azal Ahmad Khan", "Xinran Wang", "Ahmad Faraz Khan", "Shuwen Ge", "Saman Bahzad Khan", "Ayaan Ahmad", "Ali Anwar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20133v2", "summary": "Generative AI can now synthesize strikingly realistic images from text, yet\noutput quality remains highly sensitive to how prompts are phrased. Direct\nPreference Optimization (DPO) offers a lightweight, off-policy alternative to\nRL for automatic prompt engineering, but its token-level regularization leaves\nsemantic inconsistency unchecked as prompts that win higher preference scores\ncan still drift away from the user's intended meaning.\n  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency\nyet retains its simplicity and efficiency. Sem-DPO adjusts the DPO loss using a\nweight based on how different the winning prompt is from the original, reducing\nthe impact of training examples that are semantically misaligned. We provide\nthe first analytical bound on semantic drift for preference-tuned prompt\ngenerators, showing that Sem-DPO keeps learned prompts within a provably\nbounded neighborhood of the original text. On three standard text-to-image\nprompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%\nhigher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,\nPickScore) than DPO, while also outperforming state-of-the-art baselines. These\nfindings suggest that strong flat baselines augmented with semantic weighting\nshould become the new standard for prompt-optimization studies and lay the\ngroundwork for broader, semantics-aware preference optimization in language\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20133v2", "cate": "cs.CL", "date": "2025-07-27", "updated": "2025-07-29"}
{"id": "2501.15196", "title": "A Review on Self-Supervised Learning for Time Series Anomaly Detection: Recent Advances and Open Challenges", "authors": ["Aitor Sánchez-Ferrera", "Borja Calvo", "Jose A. Lozano"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.15196v2", "summary": "Time series anomaly detection presents various challenges due to the\nsequential and dynamic nature of time-dependent data. Traditional unsupervised\nmethods frequently encounter difficulties in generalization, often overfitting\nto known normal patterns observed during training and struggling to adapt to\nunseen normality. In response to this limitation, self-supervised techniques\nfor time series have garnered attention as a potential solution to undertake\nthis obstacle and enhance the performance of anomaly detectors. This paper\npresents a comprehensive review of the recent methods that make use of\nself-supervised learning for time series anomaly detection. A taxonomy is\nproposed to categorize these methods based on their primary characteristics,\nfacilitating a clear understanding of their diversity within this field. The\ninformation contained in this survey, along with additional details that will\nbe periodically updated, is available on the following GitHub repository:\nhttps://github.com/Aitorzan3/Awesome-Self-Supervised-Time-Series-Anomaly-Detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.15196v2", "cate": "stat.ML", "date": "2025-01-25", "updated": "2025-07-29"}
{"id": "2507.17121", "title": "Addressing High Class Imbalance in Multi-Class Diabetic Retinopathy Severity Grading with Augmentation and Transfer Learning", "authors": ["Faisal Ahmed"], "categories": ["cs.CV", "cs.LG", "F.2.2; I.2.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 1 Figure", "url": "http://arxiv.org/abs/2507.17121v2", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and\nearly diagnosis through automated retinal image analysis can significantly\nreduce the risk of blindness. This paper presents a robust deep learning\nframework for both binary and five-class DR classification, leveraging transfer\nlearning and extensive data augmentation to address the challenges of class\nimbalance and limited training data. We evaluate a range of pretrained\nconvolutional neural network architectures, including variants of ResNet and\nEfficientNet, on the APTOS 2019 dataset.\n  For binary classification, our proposed model achieves a state-of-the-art\naccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of\n98.9%, and an AUC of 99.4%. In the more challenging five-class severity\nclassification task, our model obtains a competitive accuracy of 84.6% and an\nAUC of 94.1%, outperforming several existing approaches. Our findings also\ndemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between\naccuracy and computational efficiency across both tasks.\n  These results underscore the effectiveness of combining class-balanced\naugmentation with transfer learning for high-performance DR diagnosis. The\nproposed framework provides a scalable and accurate solution for DR screening,\nwith potential for deployment in real-world clinical environments.", "comment": "9 pages, 1 Figure", "pdf_url": "http://arxiv.org/pdf/2507.17121v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-28"}
{"id": "2507.22720", "title": "Investigating Hallucination in Conversations for Low Resource Languages", "authors": ["Amit Das", "Md. Najib Hasan", "Souvika Sarkar", "Zheng Zhang", "Fatemeh Jamshidi", "Tathagata Bhattacharya", "Nilanjana Raychawdhury", "Dongji Feng", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22720v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22720v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.20987", "title": "JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1", "authors": ["Xinhan Di", "Kristin Qi", "Pengqian Yu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      WiCV @ ICCV 2025", "url": "http://arxiv.org/abs/2507.20987v2", "summary": "Recent advances in diffusion-based video generation have enabled\nphoto-realistic short clips, but current methods still struggle to achieve\nmulti-modal consistency when jointly generating whole-body motion and natural\nspeech. Current approaches lack comprehensive evaluation frameworks that assess\nboth visual and audio quality, and there are insufficient benchmarks for\nregion-specific performance analysis. To address these gaps, we introduce the\nJoint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1),\ncomprising a large-scale multi-modal dataset with 10,000 unique identities\nacross 2 million video samples, and an evaluation protocol for assessing joint\naudio-video generation of whole-body animatable avatars. Our evaluation of SOTA\nmodels reveals consistent performance disparities between face/hand-centric and\nwhole-body performance, which incidates essential areas for future research.\nThe dataset and evaluation tools are publicly available at\nhttps://github.com/deepreasonings/WholeBodyBenchmark.", "comment": "WiCV @ ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.20987v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-29"}
{"id": "2502.01575", "title": "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing Censored Data with Recursively Imputed Trees", "authors": ["Tomer Meir", "Uri Shalit", "Malka Gorfine"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.01575v3", "summary": "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.01575v3", "cate": "stat.ML", "date": "2025-02-03", "updated": "2025-07-28"}
{"id": "2507.19141", "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering", "authors": ["Jie Chen", "Zhangchi Hu", "Peixi Wu", "Huyue Zhu", "Hebei Li", "Xiaoyan Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.19141v2", "summary": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing\nplane-based methods in dynamic Gaussian splatting suffer from an unsuitable\nlow-rank assumption, causing feature overlap and poor rendering quality.\nAlthough 4D hash encoding provides an explicit representation without low-rank\nconstraints, directly applying it to the entire dynamic scene leads to\nsubstantial hash collisions and redundancy. To address these challenges, we\npresent DASH, a real-time dynamic scene rendering framework that employs 4D\nhash encoding coupled with self-supervised decomposition. Our approach begins\nwith a self-supervised decomposition mechanism that separates dynamic and\nstatic components without manual annotations or precomputed masks. Next, we\nintroduce a multiresolution 4D hash encoder for dynamic elements, providing an\nexplicit representation that avoids the low-rank assumption. Finally, we\npresent a spatio-temporal smoothness regularization strategy to mitigate\nunstable deformation artifacts. Experiments on real-world datasets demonstrate\nthat DASH achieves state-of-the-art dynamic rendering performance, exhibiting\nenhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU.\nCode: https://github.com/chenj02/DASH.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.19141v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-29"}
{"id": "2507.22729", "title": "Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning", "authors": ["Benedikt Roth", "Stephan Rappensperger", "Tianming Qiu", "Hamza Imamović", "Julian Wörmann", "Hao Shen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22729v1", "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22729v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.20114", "title": "Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives", "authors": ["Brian Liu", "Rahul Mazumder", "Peter Radchenko"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.20114v3", "summary": "Tree ensembles are non-parametric methods widely recognized for their\naccuracy and ability to capture complex interactions. While these models excel\nat prediction, they are difficult to interpret and may fail to uncover useful\nrelationships in the data. We propose an estimator to extract compact sets of\ndecision rules from tree ensembles. The extracted models are accurate and can\nbe manually examined to reveal relationships between the predictors and the\nresponse. A key novelty of our estimator is the flexibility to jointly control\nthe number of rules extracted and the interaction depth of each rule, which\nimproves accuracy. We develop a tailored exact algorithm to efficiently solve\noptimization problems underlying our estimator and an approximate algorithm for\ncomputing regularization paths, sequences of solutions that correspond to\nvarying model sizes. We also establish novel non-asymptotic prediction error\nbounds for our proposed approach, comparing it to an oracle that chooses the\nbest data-dependent linear combination of the rules in the ensemble subject to\nthe same complexity constraint as our estimator. The bounds illustrate that the\nlarge-sample predictive performance of our estimator is on par with that of the\noracle. Through experiments, we demonstrate that our estimator outperforms\nexisting algorithms for rule extraction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20114v3", "cate": "stat.ML", "date": "2025-06-25", "updated": "2025-07-29"}
{"id": "2507.19705", "title": "Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attributes", "authors": ["Asmae Lamsaf", "Lucia Cascone", "Hugo Proença", "João Neves"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at IJCB2025", "url": "http://arxiv.org/abs/2507.19705v2", "summary": "Bias analysis for synthetic face detection is bound to become a critical\ntopic in the coming years. Although many detection models have been developed\nand several datasets have been released to reliably identify synthetic content,\none crucial aspect has been largely overlooked: these models and training\ndatasets can be biased, leading to failures in detection for certain\ndemographic groups and raising significant social, legal, and ethical issues.\nIn this work, we introduce an evaluation framework to contribute to the\nanalysis of bias of synthetic face detectors with respect to several facial\nattributes. This framework exploits synthetic data generation, with evenly\ndistributed attribute labels, for mitigating any skew in the data that could\notherwise influence the outcomes of bias analysis. We build on the proposed\nframework to provide an extensive case study of the bias level of five\nstate-of-the-art detectors in synthetic datasets with 25 controlled facial\nattributes. While the results confirm that, in general, synthetic face\ndetectors are biased towards the presence/absence of specific facial\nattributes, our study also sheds light on the origins of the observed bias\nthrough the analysis of the correlations with the balancing of facial\nattributes in the training sets of the detectors, and the analysis of detectors\nactivation maps in image pairs with controlled attribute modifications.", "comment": "Accepted at IJCB2025", "pdf_url": "http://arxiv.org/pdf/2507.19705v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-29"}
{"id": "2507.22744", "title": "Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index", "authors": ["Praveenkumar Katwe", "Rakesh Chandra", "Balabantaray Kali", "Prasad Vittala"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8", "url": "http://arxiv.org/abs/2507.22744v1", "summary": "Reducing hallucinations in abstractive summarization remains a critical\nchallenge for deploying language models (LMs) in real-world settings. In this\nwork, we introduce a rewarddriven fine-tuning framework that explicitly\noptimizes for Entity Hallucination Index (EHI), a metric designed to quantify\nthe presence, correctness, and grounding of named entities in generated\nsummaries. Given a corpus of meeting transcripts, we first generate baseline\nsummaries using a pre-trained LM and compute EHI scores via automatic entity\nextraction and matching. We then apply reinforcement learning to fine-tune the\nmodel parameters, using EHI as a reward signal to bias generation toward\nentity-faithful outputs. Our approach does not rely on human-written factuality\nannotations, enabling scalable fine-tuning. Experiments demonstrate consistent\nimprovements in EHI across datasets, with qualitative analysis revealing a\nsignificant reduction in entity-level hallucinations without degradation in\nfluency or informativeness. We release a reproducible Colab pipeline,\nfacilitating further research on hallucination-aware model fine-tuning using\nlightweight, hallucintion metrics like EHI.", "comment": "8", "pdf_url": "http://arxiv.org/pdf/2507.22744v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.00683", "title": "Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer", "authors": ["Satadeep Bhattacharjee", "Seung-Cheol Lee"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00683v5", "summary": "The recently proposed physics-based framework by Huo and\nJohnson~\\cite{huo2024capturing} models the attention mechanism of Large\nLanguage Models (LLMs) as an interacting two-body spin system, offering a\nfirst-principles explanation for phenomena like repetition and bias. Building\non this hypothesis, we extract the complete Query-Key weight matrices from a\nproduction-grade GPT-2 model and derive the corresponding effective Hamiltonian\nfor every attention head. From these Hamiltonians, we obtain analytic phase\nboundaries and logit gap criteria that predict which token should dominate the\nnext-token distribution for a given context. A systematic evaluation on 144\nheads across 20 factual-recall prompts reveals a strong negative correlation\nbetween the theoretical logit gaps and the model's empirical token rankings\n($r\\approx-0.70$, $p<10^{-3}$).Targeted ablations further show that suppressing\nthe heads most aligned with the spin-bath predictions induces the anticipated\nshifts in output probabilities, confirming a causal link rather than a\ncoincidental association. Taken together, our findings provide the first strong\nempirical evidence for the spin-bath analogy in a production-grade model. In\nthis work, we utilize the context-field lens, which provides physics-grounded\ninterpretability and motivates the development of novel generative models\nbridging theoretical condensed matter physics and artificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00683v5", "cate": "cond-mat.mtrl-sci", "date": "2025-07-01", "updated": "2025-07-29"}
{"id": "2507.19847", "title": "Knowledge Regularized Negative Feature Tuning of Vision-Language Models for Out-of-Distribution Detection", "authors": ["Wenjie Zhu", "Yabin Zhang", "Xin Jin", "Wenjun Zeng", "Lei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ACMMM 2025", "url": "http://arxiv.org/abs/2507.19847v2", "summary": "Out-of-distribution (OOD) detection is crucial for building reliable machine\nlearning models. Although negative prompt tuning has enhanced the OOD detection\ncapabilities of vision-language models, these tuned models often suffer from\nreduced generalization performance on unseen classes and styles. To address\nthis challenge, we propose a novel method called Knowledge Regularized Negative\nFeature Tuning (KR-NFT), which integrates an innovative adaptation architecture\ntermed Negative Feature Tuning (NFT) and a corresponding\nknowledge-regularization (KR) optimization strategy. Specifically, NFT applies\ndistribution-aware transformations to pre-trained text features, effectively\nseparating positive and negative features into distinct spaces. This separation\nmaximizes the distinction between in-distribution (ID) and OOD images.\nAdditionally, we introduce image-conditional learnable factors through a\nlightweight meta-network, enabling dynamic adaptation to individual images and\nmitigating sensitivity to class and style shifts. Compared to traditional\nnegative prompt tuning, NFT demonstrates superior efficiency and scalability.\nTo optimize this adaptation architecture, the KR optimization strategy is\ndesigned to enhance the discrimination between ID and OOD sets while mitigating\npre-trained knowledge forgetting. This enhances OOD detection performance on\ntrained ID classes while simultaneously improving OOD detection on unseen ID\ndatasets. Notably, when trained with few-shot samples from ImageNet dataset,\nKR-NFT not only improves ID classification accuracy and OOD detection but also\nsignificantly reduces the FPR95 by 5.44\\% under an unexplored generalization\nsetting with unseen ID categories. Codes can be found at\n\\href{https://github.com/ZhuWenjie98/KRNFT}.", "comment": "accepted by ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.19847v2", "cate": "cs.CV", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2507.22752", "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset", "authors": ["Jindřich Libovický", "Jindřich Helcl", "Andrei Manea", "Gianluca Vico"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22752v1", "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22752v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.18114", "title": "Nonconvex Optimization Framework for Group-Sparse Feedback Linear-Quadratic Optimal Control. I: Penalty Approach", "authors": ["Lechen Feng", "Xun Li", "Yuan-Hua Ni"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18114v2", "summary": "This paper develops a unified nonconvex optimization framework for the design\nof group-sparse feedback controllers in infinite-horizon linear-quadratic (LQ)\nproblems. We address two prominent extensions of the classical LQ problem: the\ndistributed LQ problem with fixed communication topology (DFT-LQ) and the\nsparse feedback LQ problem (SF-LQ), both of which are motivated by the need for\nscalable and structure-aware control in large-scale systems. Unlike existing\napproaches that rely on convex relaxations or are limited to block-diagonal\nstructures, we directly formulate the controller synthesis as a\nfinite-dimensional nonconvex optimization problem with group $\\ell_0$-norm\nregularization, capturing general sparsity patterns. We establish a connection\nbetween DFT-LQ and SF-LQ problems, showing that both can be addressed within\nour unified framework. Furthermore, we propose a penalty-based proximal\nalternating linearized minimization (PALM) algorithm and provide a rigorous\nconvergence analysis under mild assumptions, overcoming the lack of coercivity\nin the objective function. The proposed method admits efficient solvers for all\nsubproblems and guarantees global convergence to critical points. Our results\nfill a key gap in the literature by enabling the direct design of group-sparse\nfeedback gains with theoretical guarantees, without resorting to convex\nsurrogates or restrictive structural assumptions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18114v2", "cate": "math.OC", "date": "2025-07-24", "updated": "2025-07-29"}
{"id": "2507.19946", "title": "SCALAR: Scale-wise Controllable Visual Autoregressive Learning", "authors": ["Ryan Xu", "Dongyang Jin", "Yancheng Bai", "Rui Lan", "Xu Duan", "Lei Sun", "Xiangxiang Chu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19946v2", "summary": "Controllable image synthesis, which enables fine-grained control over\ngenerated outputs, has emerged as a key focus in visual generative modeling.\nHowever, controllable generation remains challenging for Visual Autoregressive\n(VAR) models due to their hierarchical, next-scale prediction style. Existing\nVAR-based methods often suffer from inefficient control encoding and disruptive\ninjection mechanisms that compromise both fidelity and efficiency. In this\nwork, we present SCALAR, a controllable generation method based on VAR,\nincorporating a novel Scale-wise Conditional Decoding mechanism. SCALAR\nleverages a pretrained image encoder to extract semantic control signal\nencodings, which are projected into scale-specific representations and injected\ninto the corresponding layers of the VAR backbone. This design provides\npersistent and structurally aligned guidance throughout the generation process.\nBuilding on SCALAR, we develop SCALAR-Uni, a unified extension that aligns\nmultiple control modalities into a shared latent space, supporting flexible\nmulti-conditional guidance in a single model. Extensive experiments show that\nSCALAR achieves superior generation quality and control precision across\nvarious tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19946v2", "cate": "cs.CV", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2507.22753", "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective", "authors": ["Sowmya Vajjala", "Bashar Alhafni", "Stefano Bannò", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Pre-print", "url": "http://arxiv.org/abs/2507.22753v1", "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future.", "comment": "Pre-print", "pdf_url": "http://arxiv.org/pdf/2507.22753v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.18972", "title": "TiVy: Time Series Visual Summary for Scalable Visualization", "authors": ["Gromit Yeuk-Yin Chan", "Luis Gustavo Nonato", "Themis Palpanas", "Cláudio T. Silva", "Juliana Freire"], "categories": ["cs.GR", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      to be published in TVCG (IEEE VIS 2025)", "url": "http://arxiv.org/abs/2507.18972v2", "summary": "Visualizing multiple time series presents fundamental tradeoffs between\nscalability and visual clarity. Time series capture the behavior of many\nlarge-scale real-world processes, from stock market trends to urban activities.\nUsers often gain insights by visualizing them as line charts, juxtaposing or\nsuperposing multiple time series to compare them and identify trends and\npatterns. However, existing representations struggle with scalability: when\ncovering long time spans, leading to visual clutter from too many small\nmultiples or overlapping lines. We propose TiVy, a new algorithm that\nsummarizes time series using sequential patterns. It transforms the series into\na set of symbolic sequences based on subsequence visual similarity using\nDynamic Time Warping (DTW), then constructs a disjoint grouping of similar\nsubsequences based on the frequent sequential patterns. The grouping result, a\nvisual summary of time series, provides uncluttered superposition with fewer\nsmall multiples. Unlike common clustering techniques, TiVy extracts similar\nsubsequences (of varying lengths) aligned in time. We also present an\ninteractive time series visualization that renders large-scale time series in\nreal-time. Our experimental evaluation shows that our algorithm (1) extracts\nclear and accurate patterns when visualizing time series data, (2) achieves a\nsignificant speed-up (1000X) compared to a straightforward DTW clustering. We\nalso demonstrate the efficiency of our approach to explore hidden structures in\nmassive time series data in two usage scenarios.", "comment": "to be published in TVCG (IEEE VIS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18972v2", "cate": "cs.GR", "date": "2025-07-25", "updated": "2025-07-28"}
{"id": "2507.20198", "title": "When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios", "authors": ["Kele Shao", "Keda Tao", "Kejia Zhang", "Sicheng Feng", "Mu Cai", "Yuzhang Shang", "Haoxuan You", "Can Qin", "Yang Sui", "Huan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      For ongoing updates and to track the latest advances in this promising area, we maintain a public repository: this https URL", "url": "http://arxiv.org/abs/2507.20198v3", "summary": "Multimodal large language models (MLLMs) have made remarkable strides,\nlargely driven by their ability to process increasingly long and complex\ncontexts, such as high-resolution images, extended video sequences, and lengthy\naudio input. While this ability significantly enhances MLLM capabilities, it\nintroduces substantial computational challenges, primarily due to the quadratic\ncomplexity of self-attention mechanisms with numerous input tokens. To mitigate\nthese bottlenecks, token compression has emerged as an auspicious and critical\napproach, efficiently reducing the number of tokens during both training and\ninference. In this paper, we present the first systematic survey and synthesis\nof the burgeoning field of multimodal long context token compression.\nRecognizing that effective compression strategies are deeply tied to the unique\ncharacteristics and redundancies of each modality, we categorize existing\napproaches by their primary data focus, enabling researchers to quickly access\nand learn methods tailored to their specific area of interest: (1)\nimage-centric compression, which addresses spatial redundancy in visual data;\n(2) video-centric compression, which tackles spatio-temporal redundancy in\ndynamic sequences; and (3) audio-centric compression, which handles temporal\nand spectral redundancy in acoustic signals. Beyond this modality-driven\ncategorization, we further dissect methods based on their underlying\nmechanisms, including transformation-based, similarity-based, attention-based,\nand query-based approaches. By providing a comprehensive and structured\noverview, this survey aims to consolidate current progress, identify key\nchallenges, and inspire future research directions in this rapidly evolving\ndomain. We also maintain a public repository to continuously track and update\nthe latest advances in this promising area.", "comment": "For ongoing updates and to track the latest advances in this\n  promising area, we maintain a public repository:\n  https://github.com/cokeshao/Awesome-Multimodal-Token-Compression", "pdf_url": "http://arxiv.org/pdf/2507.20198v3", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-30"}
{"id": "2507.22758", "title": "MASCA: LLM based-Multi Agents System for Credit Assessment", "authors": ["Gautam Jajoo", "Pranjal A Chitale", "Saksham Agarwal"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ACL REALM Workshop. Work in Progress", "url": "http://arxiv.org/abs/2507.22758v1", "summary": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring.", "comment": "Accepted at ACL REALM Workshop. Work in Progress", "pdf_url": "http://arxiv.org/pdf/2507.22758v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.20331", "title": "From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos", "authors": ["Chenjian Gao", "Lihe Ding", "Rui Han", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.20331v2", "summary": "Inserting 3D objects into videos is a longstanding challenge in computer\ngraphics with applications in augmented reality, virtual try-on, and video\ncomposition. Achieving both temporal consistency, or realistic lighting remains\ndifficult, particularly in dynamic scenarios with complex object motion,\nperspective changes, and varying illumination. While 2D diffusion models have\nshown promise for producing photorealistic edits, they often struggle with\nmaintaining temporal coherence across frames. Conversely, traditional 3D\nrendering methods excel in spatial and temporal consistency but fall short in\nachieving photorealistic lighting. In this work, we propose a hybrid object\ninsertion pipeline that combines the strengths of both paradigms. Specifically,\nwe focus on inserting bracelets into dynamic wrist scenes, leveraging the high\ntemporal consistency of 3D Gaussian Splatting (3DGS) for initial rendering and\nrefining the results using a 2D diffusion-based enhancement model to ensure\nrealistic lighting interactions. Our method introduces a shading-driven\npipeline that separates intrinsic object properties (albedo, shading,\nreflectance) and refines both shading and sRGB images for photorealism. To\nmaintain temporal coherence, we optimize the 3DGS model with multi-frame\nweighted adjustments. This is the first approach to synergize 3D rendering and\n2D diffusion for video object insertion, offering a robust solution for\nrealistic and consistent video editing. Project Page:\nhttps://cjeen.github.io/BraceletPaper/", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.20331v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-29"}
{"id": "2507.22811", "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph", "authors": ["Debayan Banerjee", "Tilahun Abedissa Taffa", "Ricardo Usbeck"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22811v1", "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22811v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.20511", "title": "Beyond Class Tokens: LLM-guided Dominant Property Mining for Few-shot Classification", "authors": ["Wei Zhuo", "Runjie Luo", "Wufeng Xue", "Linlin Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures", "url": "http://arxiv.org/abs/2507.20511v2", "summary": "Few-shot Learning (FSL), which endeavors to develop the generalization\nability for recognizing novel classes using only a few images, faces\nsignificant challenges due to data scarcity. Recent CLIP-like methods based on\ncontrastive language-image pertaining mitigate the issue by leveraging textual\nrepresentation of the class name for unseen image discovery. Despite the\nachieved success, simply aligning visual representations to class name\nembeddings would compromise the visual diversity for novel class\ndiscrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method\n(BCT-CLIP) that explores \\textbf{dominating properties} via contrastive\nlearning beyond simply using class tokens. Through leveraging LLM-based prior\nknowledge, our method pushes forward FSL with comprehensive structural image\nrepresentations, including both global category representation and the\npatch-aware property embeddings. In particular, we presented a novel\nmulti-property generator (MPG) with patch-aware cross-attentions to generate\nmultiple visual property tokens, a Large-Language Model (LLM)-assistant\nretrieval procedure with clustering-based pruning to obtain dominating property\ndescriptions, and a new contrastive learning strategy for property-token\nlearning. The superior performances on the 11 widely used datasets demonstrate\nthat our investigation of dominating properties advances discriminative\nclass-specific representation learning and few-shot classification.", "comment": "11 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.20511v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-29"}
{"id": "2507.22829", "title": "Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization", "authors": ["Weijia Zhang", "Songgaojun Deng", "Evangelos Kanoulas"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, and 5 tables", "url": "http://arxiv.org/abs/2507.22829v1", "summary": "Query-focused table summarization requires complex reasoning, often\napproached through step-by-step natural language (NL) plans. However, NL plans\nare inherently ambiguous and lack structure, limiting their conversion into\nexecutable programs like SQL and hindering scalability, especially for\nmulti-table tasks. To address this, we propose a paradigm shift to structured\nrepresentations. We introduce a new structured plan, TaSoF, inspired by\nformalism in traditional multi-agent systems, and a framework, SPaGe, that\nformalizes the reasoning process in three phases: 1) Structured Planning to\ngenerate TaSoF from a query, 2) Graph-based Execution to convert plan steps\ninto SQL and model dependencies via a directed cyclic graph for parallel\nexecution, and 3) Summary Generation to produce query-focused summaries. Our\nmethod explicitly captures complex dependencies and improves reliability.\nExperiments on three public benchmarks show that SPaGe consistently outperforms\nprior models in both single- and multi-table settings, demonstrating the\nadvantages of structured representations for robust and scalable summarization.", "comment": "10 pages, 4 figures, and 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.22829v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2502.07327", "title": "Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos", "authors": ["Haowen Gao", "Liang Pang", "Shicheng Xu", "Leigang Qu", "Tat-Seng Chua", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.IR", "cs.CV"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      13 pages, Accepted at ACMMM2025", "url": "http://arxiv.org/abs/2502.07327v2", "summary": "With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.", "comment": "13 pages, Accepted at ACMMM2025", "pdf_url": "http://arxiv.org/pdf/2502.07327v2", "cate": "cs.IR", "date": "2025-02-11", "updated": "2025-07-29"}
{"id": "2507.22887", "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning", "authors": ["Kwesi Cobbina", "Tianyi Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22887v1", "summary": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22887v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2504.06385", "title": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching", "authors": ["Paul Roetzer", "Florian Bernard"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      8 pages main paper, 9 pages supplementary", "url": "http://arxiv.org/abs/2504.06385v3", "summary": "Geometric consistency, i.e. the preservation of neighbourhoods, is a natural\nand strong prior in 3D shape matching. Geometrically consistent matchings are\ncrucial for many downstream applications, such as texture transfer or\nstatistical shape modelling. Yet, in practice, geometric consistency is often\noverlooked, or only achieved under severely limiting assumptions (e.g. a good\ninitialisation). In this work, we propose a novel formalism for computing\nglobally optimal and geometrically consistent matchings between 3D shapes which\nis scalable in practice. Our key idea is to represent the surface of the source\nshape as a collection of cyclic paths, which are then consistently matched to\nthe target shape. Mathematically, we construct a hyper product graph (between\nsource and target shape), and then cast 3D shape matching as a minimum-cost\ncirculation flow problem in this hyper graph, which yields global geometrically\nconsistent matchings between both shapes. We empirically show that our\nformalism is efficiently solvable and that it leads to high-quality results.", "comment": "8 pages main paper, 9 pages supplementary", "pdf_url": "http://arxiv.org/pdf/2504.06385v3", "cate": "cs.GR", "date": "2025-04-08", "updated": "2025-07-29"}
{"id": "2507.22074", "title": "CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs", "authors": ["Yangshu Yuan", "Heng Chen", "Xinyi Jiang", "Christian Ng", "Kexin Qiu"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22074v1", "summary": "The rapid advancement of Large Language Models (LLMs) and Large\nVision-Language Models (LVLMs) has enhanced our ability to process and generate\nhuman language and visual information. However, these models often struggle\nwith complex, multi-step multi-modal instructions that require logical\nreasoning, dynamic feedback integration, and iterative self-correction. To\naddress this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a\nnovel framework that introduces a context-aware iterative reasoning and\nself-correction module. CIMR operates in two stages: initial reasoning and\nresponse generation, followed by iterative refinement using parsed multi-modal\nfeedback. A dynamic fusion module deeply integrates textual, visual, and\ncontextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual\nInstruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced\nMulti-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,\noutperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5\n(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the\nefficacy of its iterative reasoning and self-correction capabilities in complex\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22074v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.22080", "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.22080v1", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.22080v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.22133", "title": "Prompt Optimization and Evaluation for LLM Automated Red Teaming", "authors": ["Michael Freenor", "Lauren Alvarez", "Milton Leal", "Lily Smith", "Joel Garrett", "Yelyzaveta Husieva", "Madeline Woodruff", "Ryan Miller", "Erich Kummerfeld", "Rafael Medeiros", "Sander Schulhoff"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      9 pages, 5 Figures, and 1 Appendix item", "url": "http://arxiv.org/abs/2507.22133v1", "summary": "Applications that use Large Language Models (LLMs) are becoming widespread,\nmaking the identification of system vulnerabilities increasingly important.\nAutomated Red Teaming accelerates this effort by using an LLM to generate and\nexecute attacks against target systems. Attack generators are evaluated using\nthe Attack Success Rate (ASR) the sample mean calculated over the judgment of\nsuccess for each attack. In this paper, we introduce a method for optimizing\nattack generator prompts that applies ASR to individual attacks. By repeating\neach attack multiple times against a randomly seeded target, we measure an\nattack's discoverability the expectation of the individual attack success. This\napproach reveals exploitable patterns that inform prompt optimization,\nultimately enabling more robust evaluation and refinement of generators.", "comment": "9 pages, 5 Figures, and 1 Appendix item", "pdf_url": "http://arxiv.org/pdf/2507.22133v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22160", "title": "Strategic Deflection: Defending LLMs from Logit Manipulation", "authors": ["Yassine Rachidy", "Jihad Rbaiti", "Youssef Hmamouche", "Faissal Sehbaoui", "Amal El Fallah Seghrouchni"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.22160v1", "summary": "With the growing adoption of Large Language Models (LLMs) in critical areas,\nensuring their security against jailbreaking attacks is paramount. While\ntraditional defenses primarily rely on refusing malicious prompts, recent\nlogit-level attacks have demonstrated the ability to bypass these safeguards by\ndirectly manipulating the token-selection process during generation. We\nintroduce Strategic Deflection (SDeflection), a defense that redefines the\nLLM's response to such advanced attacks. Instead of outright refusal, the model\nproduces an answer that is semantically adjacent to the user's request yet\nstrips away the harmful intent, thereby neutralizing the attacker's harmful\nintent. Our experiments demonstrate that SDeflection significantly lowers\nAttack Success Rate (ASR) while maintaining model performance on benign\nqueries. This work presents a critical shift in defensive strategies, moving\nfrom simple refusal to strategic content redirection to neutralize advanced\nthreats.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.22160v1", "cate": "cs.CR", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22197", "title": "Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence", "authors": ["Matthieu Queloz"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      39 pages; final, published version", "url": "http://arxiv.org/abs/2507.22197v1", "summary": "This paper argues that explainability is only one facet of a broader ideal\nthat shapes our expectations towards artificial intelligence (AI).\nFundamentally, the issue is to what extent AI exhibits systematicity--not\nmerely in being sensitive to how thoughts are composed of recombinable\nconstituents, but in striving towards an integrated body of thought that is\nconsistent, coherent, comprehensive, and parsimoniously principled. This richer\nconception of systematicity has been obscured by the long shadow of the\n\"systematicity challenge\" to connectionism, according to which network\narchitectures are fundamentally at odds with what Fodor and colleagues termed\n\"the systematicity of thought.\" I offer a conceptual framework for thinking\nabout \"the systematicity of thought\" that distinguishes four senses of the\nphrase. I use these distinctions to defuse the perceived tension between\nsystematicity and connectionism and show that the conception of systematicity\nthat historically shaped our sense of what makes thought rational,\nauthoritative, and scientific is more demanding than the Fodorian notion. To\ndetermine whether we have reason to hold AI models to this ideal of\nsystematicity, I then argue, we must look to the rationales for systematization\nand explore to what extent they transfer to AI models. I identify five such\nrationales and apply them to AI. This brings into view the \"hard systematicity\nchallenge.\" However, the demand for systematization itself needs to be\nregulated by the rationales for systematization. This yields a dynamic\nunderstanding of the need to systematize thought, which tells us how systematic\nwe need AI models to be and when.", "comment": "39 pages; final, published version", "pdf_url": "http://arxiv.org/pdf/2507.22197v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22281", "title": "CoEx -- Co-evolving World-model and Exploration", "authors": ["Minsoo Kim", "Seung-won Hwang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22281v1", "summary": "Planning in modern LLM agents relies on the utilization of LLM as an internal\nworld model, acquired during pretraining. However, existing agent designs fail\nto effectively assimilate new observations into dynamic updates of the world\nmodel. This reliance on the LLM's static internal world model is progressively\nprone to misalignment with the underlying true state of the world, leading to\nthe generation of divergent and erroneous plans. We introduce a hierarchical\nagent architecture, CoEx, in which hierarchical state abstraction allows LLM\nplanning to co-evolve with a dynamically updated model of the world. CoEx plans\nand interacts with the world by using LLM reasoning to orchestrate dynamic\nplans consisting of subgoals, and its learning mechanism continuously\nincorporates these subgoal experiences into a persistent world model in the\nform of a neurosymbolic belief state, comprising textual inferences and\ncode-based symbolic memory. We evaluate our agent across a diverse set of agent\nscenarios involving rich environments and complex tasks including ALFWorld,\nPDDL, and Jericho. Our experiments show that CoEx outperforms existing agent\nparadigms in planning and exploration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22281v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22359", "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models", "authors": ["Qianhong Guo", "Wei Xie", "Xiaofang Cai", "Enze Wang", "Shuoyoucheng Ma", "Kai Chen", "Xiaofeng Wang", "Baosheng Wang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22359v1", "summary": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22359v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22543", "title": "Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law", "authors": ["Yanjin He", "Qingkai Zeng", "Meng Jiang"], "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22543v1", "summary": "Tokenization is a fundamental step in natural language processing (NLP) and\nother sequence modeling domains, where the choice of vocabulary size\nsignificantly impacts model performance. Despite its importance, selecting an\noptimal vocabulary size remains underexplored, typically relying on heuristics\nor dataset-specific choices. In this work, we propose a principled method for\ndetermining the vocabulary size by analyzing token frequency distributions\nthrough Zipf's law. We show that downstream task performance correlates with\nhow closely token distributions follow power-law behavior, and that aligning\nwith Zipfian scaling improves both model efficiency and effectiveness.\nExtensive experiments across NLP, genomics, and chemistry demonstrate that\nmodels consistently achieve peak performance when the token distribution\nclosely adheres to Zipf's law, establishing Zipfian alignment as a robust and\ngeneralizable criterion for vocabulary size selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22543v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22565", "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning", "authors": ["Afshin Khadangi", "Amir Sartipi", "Igor Tchappi", "Ramin Bahmani", "Gilbert Fridgen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22565v1", "summary": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22565v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22607", "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 5 figures, 6 tables. Work in progress", "url": "http://arxiv.org/abs/2507.22607v1", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.", "comment": "21 pages, 5 figures, 6 tables. Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.22607v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22847", "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology", "authors": ["Han Jiang", "Pengda Wang", "Xiaoyuan Yi", "Xing Xie", "Ziang Xiao"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22847v1", "summary": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22847v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2311.07052", "title": "Towards the Law of Capacity Gap in Distilling Language Models", "authors": ["Chen Zhang", "Qiuchi Li", "Dawei Song", "Zheyu Ye", "Yan Gao", "Yan Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      32 pages, 10 figures, 15 tables, accepted to ACL 2025. Code and checkpoints are available at this https URL", "url": "http://arxiv.org/abs/2311.07052v4", "summary": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors.", "comment": "32 pages, 10 figures, 15 tables, accepted to ACL 2025. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA", "pdf_url": "http://arxiv.org/pdf/2311.07052v4", "cate": "cs.CL", "date": "2023-11-13", "updated": "2025-07-30"}
{"id": "2408.16440", "title": "Instruction-tuned Large Language Models for Machine Translation in the Medical Domain", "authors": ["Miguel Rios"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models for Machine Translation in the Medical Domain. In Proceedings of Machine Translation Summit XX Volume 1, pages 162-172", "url": "http://arxiv.org/abs/2408.16440v2", "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.", "comment": "Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models\n  for Machine Translation in the Medical Domain. In Proceedings of Machine\n  Translation Summit XX Volume 1, pages 162-172", "pdf_url": "http://arxiv.org/pdf/2408.16440v2", "cate": "cs.CL", "date": "2024-08-29", "updated": "2025-07-30"}
{"id": "2409.14820", "title": "Past Meets Present: Creating Historical Analogy with Large Language Models", "authors": ["Nianqi Li", "Siyu Yuan", "Jiangjie Chen", "Jiaqing Liang", "Feng Wei", "Zujie Liang", "Deqing Yang", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025 (Outstanding Paper Award)", "url": "http://arxiv.org/abs/2409.14820v2", "summary": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method.", "comment": "Accepted to ACL 2025 (Outstanding Paper Award)", "pdf_url": "http://arxiv.org/pdf/2409.14820v2", "cate": "cs.CL", "date": "2024-09-23", "updated": "2025-07-30"}
{"id": "2410.02744", "title": "Neutral Residues: Revisiting Adapters for Model Extension", "authors": ["Franck Signe Talla", "Edouard Grave", "Hervé Jégou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025", "url": "http://arxiv.org/abs/2410.02744v2", "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2410.02744v2", "cate": "cs.CL", "date": "2024-10-03", "updated": "2025-07-30"}
{"id": "2410.21306", "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges", "authors": ["Farid Ariai", "Joel Mackenzie", "Gianluca Demartini"], "categories": ["cs.CL", "cs.AI", "A.1; I.2.7; J.1"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      35 pages", "url": "http://arxiv.org/abs/2410.21306v3", "summary": "Natural Language Processing (NLP) is revolutionising the way both\nprofessionals and laypersons operate in the legal field. The considerable\npotential for NLP in the legal sector, especially in developing computational\nassistance tools for various legal processes, has captured the interest of\nresearchers for years. This survey follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a\nfinal selection of 131 after manual filtering. It explores foundational\nconcepts related to NLP in the legal domain, illustrating the unique aspects\nand challenges of processing legal texts, such as extensive document lengths,\ncomplex language, and limited open legal datasets. We provide an overview of\nNLP tasks specific to legal text, such as Document Summarisation, Named Entity\nRecognition, Question Answering, Argument Mining, Text Classification, and\nJudgement Prediction. Furthermore, we analyse both developed legal-oriented\nlanguage models, and approaches for adapting general-purpose language models to\nthe legal domain. Additionally, we identify sixteen open research challenges,\nincluding the detection and mitigation of bias in artificial intelligence\napplications, the need for more robust and interpretable models, and improving\nexplainability to handle the complexities of legal language and reasoning.", "comment": "35 pages", "pdf_url": "http://arxiv.org/pdf/2410.21306v3", "cate": "cs.CL", "date": "2024-10-25", "updated": "2025-07-30"}
{"id": "2412.03334", "title": "Yankari: A Monolingual Yoruba Dataset", "authors": ["Maro Akpobi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2412.03334v2", "summary": "This paper presents Yankari, a large-scale monolingual dataset for the Yoruba\nlanguage, aimed at addressing the critical gap in Natural Language Processing\n(NLP) resources for this important West African language. Despite being spoken\nby over 30 million people, Yoruba has been severely underrepresented in NLP\nresearch and applications. We detail our methodology for creating this dataset,\nwhich includes careful source selection, automated quality control, and\nrigorous data cleaning processes. The Yankari dataset comprises 51,407\ndocuments from 13 diverse sources, totaling over 30 million tokens. Our\napproach focuses on ethical data collection practices, avoiding problematic\nsources and addressing issues prevalent in existing datasets. We provide\nthorough automated evaluations of the dataset, demonstrating its quality\ncompared to existing resources. The Yankari dataset represents a significant\nadvancement in Yoruba language resources, providing a foundation for developing\nmore accurate NLP models, supporting comparative linguistic studies, and\ncontributing to the digital accessibility of the Yoruba language.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2412.03334v2", "cate": "cs.CL", "date": "2024-12-04", "updated": "2025-07-30"}
{"id": "2412.08528", "title": "Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck", "authors": ["Andor Diera", "Lukas Galke", "Fabian Karl", "Ansgar Scherp"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.08528v2", "summary": "Continual learning remains a challenge across various natural language\nprocessing (NLP) tasks, as models updated with new training data often risk\ncatastrophic forgetting of previously acquired knowledge. We introduce a\ndiscrete key-value bottleneck (DKVB) for encoder-only language models, enabling\nefficient continual learning through localized updates. Inspired by a discrete\nkey-value bottleneck in vision, we consider new and NLP-specific challenges. We\ncompare different bottleneck architectures for NLP and introduce a new,\ntask-independent initialization technique for the discrete keys. We evaluate\nour DKVB for NLP in four continual learning scenarios and show that it\nalleviates catastrophic forgetting. Our experiments demonstrate that the\nproposed approach achieves competitive performance compared to popular\ncontinual learning methods while incurring lower computational costs.\nFurthermore, we show that DKVB remains effective even in challenging\nsingle-head continual learning scenarios where no task ID is provided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.08528v2", "cate": "cs.CL", "date": "2024-12-11", "updated": "2025-07-30"}
{"id": "2412.14373", "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling", "authors": ["William Han", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "categories": ["cs.CL", "eess.SP", "I.2.7; J.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      38 pages, 9 figures; Accepted to MLHC 2025", "url": "http://arxiv.org/abs/2412.14373v3", "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods.", "comment": "38 pages, 9 figures; Accepted to MLHC 2025", "pdf_url": "http://arxiv.org/pdf/2412.14373v3", "cate": "cs.CL", "date": "2024-12-18", "updated": "2025-07-29"}
{"id": "2412.15239", "title": "Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs", "authors": ["Hortense Fong", "George Gui"], "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC", "stat.ME", "68T50, 91F20", "H.3.1; I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.15239v3", "summary": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters, we demonstrate that our framework complements\nexisting feature engineering techniques by amplifying their marginal\nexplanatory power on average by 31%. The results reveal that different types of\nengagement-continuing to read, commenting, and voting-are driven by distinct\ncombinations of current and anticipated content features. Our framework\nprovides a novel way to study and explore how audience forward-looking beliefs\nshape their engagement with narrative media, with implications for marketing\nstrategy in content-focused industries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.15239v3", "cate": "cs.CL", "date": "2024-12-13", "updated": "2025-07-29"}
{"id": "2412.16936", "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Fengyuan Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.16936v2", "summary": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.16936v2", "cate": "cs.CL", "date": "2024-12-22", "updated": "2025-07-30"}
{"id": "2501.09213", "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training", "authors": ["Hongzhou Yu", "Tianhao Cheng", "Yingwen Wang", "Wen He", "Qing Wang", "Ying Cheng", "Yuejie Zhang", "Rui Feng", "Xiaobo Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.09213v3", "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.09213v3", "cate": "cs.CL", "date": "2025-01-16", "updated": "2025-07-30"}
{"id": "2502.14907", "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale", "authors": ["Hajar Emami Gohari", "Swanand Ravindra Kadhe", "Syed Yousaf Shah", "Constantin Adam", "Abdulhamid Adebayo", "Praneet Adusumilli", "Farhan Ahmed", "Nathalie Baracaldo Angel", "Santosh Subhashrao Borse", "Yuan-Chi Chang", "Xuan-Hong Dang", "Nirmit Desai", "Revital Eres", "Ran Iwamoto", "Alexei Karve", "Yan Koyfman", "Wei-Han Lee", "Changchang Liu", "Boris Lublinsky", "Takuyo Ohko", "Pablo Pesce", "Maroun Touma", "Shiqiang Wang", "Shalisha Witherspoon", "Herbert Woisetschläger", "David Wood", "Kun-Lung Wu", "Issei Yoshida", "Syed Zawad", "Petros Zerfos", "Yi Zhou", "Bishwaranjan Bhattacharjee"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.14907v2", "summary": "Data quantity and quality play a vital role in determining the performance of\nLarge Language Models (LLMs). High-quality data, in particular, can\nsignificantly boost the LLM's ability to generalize on a wide range of\ndownstream tasks. Large pre-training datasets for leading LLMs remain\ninaccessible to the public, whereas many open datasets are small in size (less\nthan 5 trillion tokens), limiting their suitability for training large models.\n  In this paper, we introduce GneissWeb, a large dataset yielding around 10\ntrillion tokens that caters to the data quality and quantity requirements of\ntraining LLMs. Our GneissWeb recipe that produced the dataset consists of\nsharded exact sub-string deduplication and a judiciously constructed ensemble\nof quality filters. GneissWeb achieves a favorable trade-off between data\nquality and quantity, producing models that outperform models trained on\nstate-of-the-art open large datasets (5+ trillion tokens).\n  We show that models trained using GneissWeb dataset outperform those trained\non FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed\non a set of 11 commonly used benchmarks (both zero-shot and few-shot) for\npre-training dataset evaluation. When the evaluation set is extended to 20\nbenchmarks (both zero-shot and few-shot), models trained using GneissWeb still\nachieve a 1.75 percentage points advantage over those trained on\nFineWeb-V1.1.0.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.14907v2", "cate": "cs.CL", "date": "2025-02-19", "updated": "2025-07-29"}
{"id": "2503.03044", "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing", "authors": ["Gabriele Sarti", "Vilém Zouhar", "Grzegorz Chrupała", "Ana Guerberof-Arenas", "Malvina Nissim", "Arianna Bisazza"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by TACL (pre-MIT Press publication version); Code: this https URL . Dataset: this https URL", "url": "http://arxiv.org/abs/2503.03044v2", "summary": "Word-level quality estimation (QE) methods aim to detect erroneous spans in\nmachine translations, which can direct and facilitate human post-editing. While\nthe accuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. In this study, we investigate the\nimpact of word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated from behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows.", "comment": "Accepted by TACL (pre-MIT Press publication version); Code:\n  https://github.com/gsarti/qe4pe. Dataset:\n  https://huggingface.co/datasets/gsarti/qe4pe", "pdf_url": "http://arxiv.org/pdf/2503.03044v2", "cate": "cs.CL", "date": "2025-03-04", "updated": "2025-07-30"}
{"id": "2504.01282", "title": "Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing", "authors": ["Jihyun Janice Ahn", "Wenpeng Yin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      accepted in COLM2025, 9 pages", "url": "http://arxiv.org/abs/2504.01282v2", "summary": "While the inconsistency of LLMs is not a novel topic, prior research has\npredominantly addressed two types of generative inconsistencies: i) Randomness\nInconsistency: running the same LLM multiple trials, yielding varying\nresponses; ii) Paraphrase Inconsistency: paraphrased prompts result in\ndifferent responses from the same LLM. Randomness Inconsistency arises from the\ninherent randomness due to stochastic sampling in generative models, while\nParaphrase Inconsistency is a consequence of the language modeling objectives,\nwhere paraphrased prompts alter the distribution of vocabulary logits. This\nresearch discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM\nself-inconsistency: given a question and a couple of LLM-generated answer\ncandidates, the LLM often has conflicting responses when prompted \"Which are\ncorrect answers?\" and \"Which are incorrect answers?\". PRIN poses a big concern\nas it undermines the credibility of LLM-as-a-judge, and suggests a challenge\nfor LLMs to adhere to basic logical rules. We conduct a series of experiments\nto investigate PRIN, examining the extent of PRIN across different LLMs,\nmethods to mitigate it, potential applications, and its relationship with\nRandomness Inconsistency and Paraphrase Inconsistency. As the first study to\nexplore PRIN, our findings offer valuable insights into the inner workings of\nLLMs and contribute to advancing trustworthy AI.", "comment": "accepted in COLM2025, 9 pages", "pdf_url": "http://arxiv.org/pdf/2504.01282v2", "cate": "cs.CL", "date": "2025-04-02", "updated": "2025-07-30"}
{"id": "2504.05008", "title": "Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergei Tilga", "Ekaterina Artemova"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05008v2", "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05008v2", "cate": "cs.CL", "date": "2025-04-07", "updated": "2025-07-30"}
{"id": "2504.11829", "title": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.11829v3", "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.11829v3", "cate": "cs.CL", "date": "2025-04-16", "updated": "2025-07-29"}
{"id": "2505.08450", "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation", "authors": ["Kazuki Hayashi", "Hidetaka Kamigaito", "Shinya Kouda", "Taro Watanabe"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08450v2", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08450v2", "cate": "cs.CL", "date": "2025-05-13", "updated": "2025-07-30"}
{"id": "2505.12474", "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages, 17 figures and 8 tables", "url": "http://arxiv.org/abs/2505.12474v2", "summary": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration.", "comment": "20 pages, 17 figures and 8 tables", "pdf_url": "http://arxiv.org/pdf/2505.12474v2", "cate": "cs.CL", "date": "2025-05-18", "updated": "2025-07-30"}
{"id": "2505.15038", "title": "Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering", "authors": ["Haiyan Zhao", "Xuansheng Wu", "Fan Yang", "Bo Shen", "Ninghao Liu", "Mengnan Du"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures, 4 tables", "url": "http://arxiv.org/abs/2505.15038v2", "summary": "Linear concept vectors effectively steer LLMs, but existing methods suffer\nfrom noisy features in diverse datasets that undermine steering robustness. We\npropose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively\nkeep the most discriminative SAE latents while reconstructing hidden\nrepresentations. Our key insight is that concept-relevant signals can be\nexplicitly separated from dataset noise by scaling up activations of top-k\nlatents that best differentiate positive and negative samples. Applied to\nlinear probing and difference-in-mean, SDCV consistently improves steering\nsuccess rates by 4-16\\% across six challenging concepts, while maintaining\ntopic relevance.", "comment": "12 pages, 4 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2505.15038v2", "cate": "cs.CL", "date": "2025-05-21", "updated": "2025-07-29"}
{"id": "2505.19959", "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACL'25 main track", "url": "http://arxiv.org/abs/2505.19959v2", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.", "comment": "Accepted by ACL'25 main track", "pdf_url": "http://arxiv.org/pdf/2505.19959v2", "cate": "cs.CL", "date": "2025-05-26", "updated": "2025-07-30"}
{"id": "2505.21354", "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "authors": ["Bidyarthi Paul", "Jalisha Jashim Era", "Mirazur Rahman Zim", "Tahmid Sattar Aothoi", "Faisal Muhammad Shah"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.21354v2", "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.21354v2", "cate": "cs.CL", "date": "2025-05-27", "updated": "2025-07-30"}
{"id": "2506.04585", "title": "MuSciClaims: Multimodal Scientific Claim Verification", "authors": ["Yash Kumar Lal", "Manikanta Bandham", "Mohammad Saqib Hasan", "Apoorva Kashi", "Mahnaz Koupaee", "Niranjan Balasubramanian"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.04585v2", "summary": "Assessing scientific claims requires identifying, extracting, and reasoning\nwith multimodal data expressed in information-rich figures in scientific\nliterature. Despite the large body of work in scientific QA, figure captioning,\nand other multimodal reasoning tasks over chart-based data, there are no\nreadily usable multimodal benchmarks that directly test claim verification\nabilities. To remedy this gap, we introduce a new benchmark MuSciClaims\naccompanied by diagnostics tasks. We automatically extract supported claims\nfrom scientific articles, which we manually perturb to produce contradicted\nclaims. The perturbations are designed to test for a specific set of claim\nverification capabilities. We also introduce a suite of diagnostic tasks that\nhelp understand model failures. Our results show most vision-language models\nare poor (~0.3-0.5 F1), with even the best model only achieving 0.72 F1. They\nare also biased towards judging claims as supported, likely misunderstanding\nnuanced perturbations within the claims. Our diagnostics show models are bad at\nlocalizing correct evidence within figures, struggle with aggregating\ninformation across modalities, and often fail to understand basic components of\nthe figure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.04585v2", "cate": "cs.CL", "date": "2025-06-05", "updated": "2025-07-30"}
{"id": "2506.09147", "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perlić", "Ekaterina Borisova", "Markarit Vartampetian"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.09147v2", "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.09147v2", "cate": "cs.CL", "date": "2025-06-10", "updated": "2025-07-29"}
{"id": "2506.19073", "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations", "authors": ["Jackson Trager", "Diego Alves", "Matteo Guida", "Mikel K. Ngueajio", "Ameeta Agrawal", "Flor Plaza-del-Arco", "Yalda Daryanai", "Farzan Karimi-Malekabadi", "Francielle Vargas"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2506.19073v2", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2506.19073v2", "cate": "cs.CL", "date": "2025-06-23", "updated": "2025-07-30"}
{"id": "2507.15347", "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "authors": ["Amedeo Buonanno", "Alessandro Rivetti", "Francesco A. N. Palmieri", "Giovanni Di Gennaro", "Gianmarco Romano"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Presented to the Italian Workshop on Neural Networks (WIRN2025) and it will appear in a Springer Chapter", "url": "http://arxiv.org/abs/2507.15347v2", "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models", "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter", "pdf_url": "http://arxiv.org/pdf/2507.15347v2", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-30"}
{"id": "2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 7 Figures, 10 Tables", "url": "http://arxiv.org/abs/2507.15586v4", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose EviOmni, which learns to extract\nrational evidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of EviOmni, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "comment": "16 pages, 7 Figures, 10 Tables", "pdf_url": "http://arxiv.org/pdf/2507.15586v4", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-30"}
{"id": "2507.15779", "title": "Reservoir Computing as a Language Model", "authors": ["Felix Köster", "Atsushi Uchida"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, 1 table Code available at: this https URL", "url": "http://arxiv.org/abs/2507.15779v2", "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.", "comment": "8 pages, 5 figures, 1 table Code available at:\n  https://github.com/fekoester/Shakespeare_Res", "pdf_url": "http://arxiv.org/pdf/2507.15779v2", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-30"}
{"id": "2507.19741", "title": "Basic Reading Distillation", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACL2025", "url": "http://arxiv.org/abs/2507.19741v2", "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation.", "comment": "Accepted by ACL2025", "pdf_url": "http://arxiv.org/pdf/2507.19741v2", "cate": "cs.CL", "date": "2025-07-26", "updated": "2025-07-29"}
{"id": "2507.20930", "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models", "authors": ["Likun Tan", "Kuan-Wei Huang", "Kevin Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20930v2", "summary": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/shield.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20930v2", "cate": "cs.CL", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2507.22050", "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router", "authors": ["Minghao Guo", "Qingcheng Zeng", "Xujiang Zhao", "Yanchi Liu", "Wenchao Yu", "Mengnan Du", "Haifeng Chen", "Wei Cheng"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 pages, work in progress", "url": "http://arxiv.org/abs/2507.22050v2", "summary": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches. Our codes are available at\nhttps://github.com/MinghoKwok/DeepSieve.", "comment": "22 pages, work in progress", "pdf_url": "http://arxiv.org/pdf/2507.22050v2", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2404.07214", "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions", "authors": ["Akash Ghosh", "Arkadeep Acharya", "Sriparna Saha", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      One of the first survey on Visual Language Models", "url": "http://arxiv.org/abs/2404.07214v3", "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.", "comment": "One of the first survey on Visual Language Models", "pdf_url": "http://arxiv.org/pdf/2404.07214v3", "cate": "cs.CV", "date": "2024-02-20", "updated": "2025-07-30"}
{"id": "2410.07336", "title": "Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training", "authors": ["Sara Sarto", "Nicholas Moratelli", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Journal of Computer Vision (2025)", "url": "http://arxiv.org/abs/2410.07336v2", "summary": "Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.", "comment": "International Journal of Computer Vision (2025)", "pdf_url": "http://arxiv.org/pdf/2410.07336v2", "cate": "cs.CV", "date": "2024-10-09", "updated": "2025-07-29"}
{"id": "2411.08003", "title": "Can adversarial attacks by large language models be attributed?", "authors": ["Manuel Cebrian", "Andres Abeliuk", "Jan Arne Telle"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.FL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      22 pages, 5 figures, 2 tables", "url": "http://arxiv.org/abs/2411.08003v3", "summary": "Attributing outputs from Large Language Models (LLMs) in adversarial\nsettings-such as cyberattacks and disinformation campaigns-presents significant\nchallenges that are likely to grow in importance. We approach this attribution\nproblem from both a theoretical and an empirical perspective, drawing on formal\nlanguage theory (identification in the limit) and data-driven analysis of the\nexpanding LLM ecosystem. By modeling an LLM's set of possible outputs as a\nformal language, we analyze whether finite samples of text can uniquely\npinpoint the originating model. Our results show that, under mild assumptions\nof overlapping capabilities among models, certain classes of LLMs are\nfundamentally non-identifiable from their outputs alone. We delineate four\nregimes of theoretical identifiability: (1) an infinite class of deterministic\n(discrete) LLM languages is not identifiable (Gold's classical result from\n1967); (2) an infinite class of probabilistic LLMs is also not identifiable (by\nextension of the deterministic case); (3) a finite class of deterministic LLMs\nis identifiable (consistent with Angluin's tell-tale criterion); and (4) even a\nfinite class of probabilistic LLMs can be non-identifiable (we provide a new\ncounterexample establishing this negative result). Complementing these\ntheoretical insights, we quantify the explosion in the number of plausible\nmodel origins (hypothesis space) for a given output in recent years. Even under\nconservative assumptions-each open-source model fine-tuned on at most one new\ndataset-the count of distinct candidate models doubles approximately every 0.5\nyears, and allowing multi-dataset fine-tuning combinations yields doubling\ntimes as short as 0.28 years. This combinatorial growth, alongside the\nextraordinary computational cost of brute-force likelihood attribution across\nall models and potential users, renders exhaustive attribution infeasible in\npractice.", "comment": "22 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2411.08003v3", "cate": "cs.AI", "date": "2024-11-12", "updated": "2025-07-29"}
{"id": "2502.13820", "title": "Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning", "authors": ["Aleksander Ficek", "Somshubra Majumdar", "Vahid Noroozi", "Boris Ginsburg"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      COLM 2025", "url": "http://arxiv.org/abs/2502.13820v3", "summary": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy.", "comment": "COLM 2025", "pdf_url": "http://arxiv.org/pdf/2502.13820v3", "cate": "cs.AI", "date": "2025-02-19", "updated": "2025-07-30"}
{"id": "2503.07631", "title": "OWLViz: An Open-World Benchmark for Visual Question Answering", "authors": ["Thuy Nguyen", "Dang Nguyen", "Hoang Nguyen", "Thuan Luong", "Long Hoang Dang", "Viet Dac Lai"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages + appendix", "url": "http://arxiv.org/abs/2503.07631v3", "summary": "We present a challenging benchmark for the Open WorLd VISual question\nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that\nrequire integrating multiple capabilities, including visual understanding, web\nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on\nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best\nmodel, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which\nrely on limited vision and vision-language models as tools, perform even worse.\nThis performance gap reveals significant limitations in multimodal systems'\nability to select appropriate tools and execute complex reasoning sequences,\nestablishing new directions for advancing practical AI research.", "comment": "8 pages + appendix", "pdf_url": "http://arxiv.org/pdf/2503.07631v3", "cate": "cs.LG", "date": "2025-03-04", "updated": "2025-07-30"}
{"id": "2504.11257", "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.11257v4", "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://microsoft.github.io/FIVE-UI-Evol/ .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.11257v4", "cate": "cs.HC", "date": "2025-04-15", "updated": "2025-07-30"}
{"id": "2504.13932", "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining", "authors": ["Deyu Cao", "Samin Aref"], "categories": ["cs.LG", "cs.CL", "68T50, 68T07, 68T09, 68U15", "I.2.7; I.2.6; I.2.4"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This is a post-peer-review accepted manuscript from the proceedings of the 22nd International Conference on Modeling Decisions for Artificial Intelligence (MDAI'25). The publisher authenticated version and full citation details are available on Springer's website (LNAI 15957). this https URL", "url": "http://arxiv.org/abs/2504.13932v3", "summary": "The growing use of large language models has raised environmental and\neconomic concerns about their intensity of resource usage during inference.\nServing these models to each user requires substantial energy and water for\ncooling. Model compression techniques like quantization can shrink large\nlanguage models and make them more resource efficient at the cost of potential\nperformance degradation. Quantization methods compress model size through\nreplacing their high-precision parameters by quantized values of lower\nprecision. Among existing methods, the ApiQ method achieves superior accuracy\npreservation at minimal memory and time overhead. We investigate two ideas to\nextend performance in ultra-low-bit quantization beyond ApiQ's level. First, we\nlook into combining existing quantization-aware training techniques with ApiQ's\npartial training. We show that this does not outperform the baseline ApiQ\nmethod with limited training data and frozen weights. This leads to two key\ninsights: (1) The substantial representational capacity that is gained through\nfull retraining is unlikely to be feasible through partial training. (2) This\ngain may depend on using a large and diverse dataset in quantization-aware\ntraining. Second, through a novel approach informed by the two insights, we\npropose an ultra-low-bit quantization method that builds upon ApiQ and extends\nits performance without the need for full retraining. This publicly available\nmethod relies on a saliency-aware regularization term that prioritizes\npreserving the most impactful parameters during quantization. Our experiments\non LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's\naccuracy degradation by 10.85% and 7.54% respectively. A Python implementation\nof the proposed quantization method is publicly available on GitHub\nhttps://github.com/TokuyuSou/ULB-SAPR.", "comment": "This is a post-peer-review accepted manuscript from the proceedings\n  of the 22nd International Conference on Modeling Decisions for Artificial\n  Intelligence (MDAI'25). The publisher authenticated version and full citation\n  details are available on Springer's website (LNAI 15957).\n  https://doi.org/10.1007/978-3-032-00891-6_28", "pdf_url": "http://arxiv.org/pdf/2504.13932v3", "cate": "cs.LG", "date": "2025-04-14", "updated": "2025-07-30"}
{"id": "2505.19010", "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19010v2", "summary": "Multi-modal learning has emerged as a crucial research direction, as\nintegrating textual and visual information can substantially enhance\nperformance in tasks such as classification, retrieval, and scene\nunderstanding. Despite advances with large pre-trained models, existing\napproaches often suffer from insufficient cross-modal interactions and rigid\nfusion strategies, failing to fully harness the complementary strengths of\ndifferent modalities. To address these limitations, we propose Co-AttenDWG,\nco-attention with dimension-wise gating, and expert fusion. Our approach first\nprojects textual and visual features into a shared embedding space, where a\ndedicated co-attention mechanism enables simultaneous, fine-grained\ninteractions between modalities. This is further strengthened by a\ndimension-wise gating network, which adaptively modulates feature contributions\nat the channel level to emphasize salient information. In parallel, dual-path\nencoders independently refine modality-specific representations, while an\nadditional cross-attention layer aligns the modalities further. The resulting\nfeatures are aggregated via an expert fusion module that integrates learned\ngating and self-attention, yielding a robust unified representation.\nExperimental results on the MIMIC and SemEval Memotion 1.0 datasets show that\nCo-AttenDWG achieves state-of-the-art performance and superior cross-modal\nalignment, highlighting its effectiveness for diverse multi-modal applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19010v2", "cate": "cs.CV", "date": "2025-05-25", "updated": "2025-07-30"}
{"id": "2506.06157", "title": "Masked Language Models are Good Heterogeneous Graph Generalizers", "authors": ["Jinyu Yang", "Cheng Yang", "Shanyuan Cui", "Zeyuan Guo", "Liangwei Yang", "Muhan Zhang", "Zhiqiang Zhang", "Chuan Shi"], "categories": ["cs.SI", "cs.CL"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.06157v2", "summary": "Heterogeneous graph neural networks (HGNNs) excel at capturing structural and\nsemantic information in heterogeneous graphs (HGs), while struggling to\ngeneralize across domains and tasks. With the rapid advancement of large\nlanguage models (LLMs), a recent study explored the integration of HGNNs with\nLLMs for generalizable heterogeneous graph learning. However, this approach\ntypically encodes structural information as HG tokens using HGNNs, and\ndisparities in embedding spaces between HGNNs and LLMs have been shown to bias\nthe LLM's comprehension of HGs. Moreover, since these HG tokens are often\nderived from node-level tasks, the model's ability to generalize across tasks\nremains limited. To this end, we propose a simple yet effective Masked Language\nModeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual\nsequences instead of HG tokens to extract structural and semantic information\ninherent in HGs, and designs customized textual templates to unify different\ngraph tasks into a coherent cloze-style 'mask' token prediction paradigm.\nSpecifically,MLM4HG first converts HGs from various domains to texts based on\nmetapaths, and subsequently combines them with the unified task texts to form a\nHG-based corpus. Moreover, the corpus is fed into a pretrained LM for\nfine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to\ngeneralize to unseen target HGs. Extensive cross-domain and multi-task\nexperiments on four real-world datasets demonstrate the superior generalization\nperformance of MLM4HG over state-of-the-art methods in both few-shot and\nzero-shot scenarios. Our code is available at\nhttps://github.com/BUPT-GAMMA/MLM4HG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.06157v2", "cate": "cs.SI", "date": "2025-06-06", "updated": "2025-07-30"}
{"id": "2506.15677", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15677v3", "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15677v3", "cate": "cs.AI", "date": "2025-06-18", "updated": "2025-07-29"}
{"id": "2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07610v3", "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models show difficulty perception\nmisaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs,\ndefault to formulaic derivation over visualization, and paradoxically suffer\nperformance degradation from Chain-of-Thought prompting in open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench\ndemonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in\nspatial visualization tasks, thereby addressing a significant lacuna in the\nfield. The benchmark data and evaluation code are publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07610v3", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-30"}
{"id": "2507.07966", "title": "Scaling RL to Long Videos", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code at this https URL and model at this https URL", "url": "http://arxiv.org/abs/2507.07966v3", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).", "comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B", "pdf_url": "http://arxiv.org/pdf/2507.07966v3", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-30"}
{"id": "2507.08771", "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity", "authors": ["Chenyang Song", "Weilin Zhao", "Xu Han", "Chaojun Xiao", "Yingfa Chen", "Yuxuan Li", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      21 pages, 7 figures, 15 tables", "url": "http://arxiv.org/abs/2507.08771v2", "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).", "comment": "21 pages, 7 figures, 15 tables", "pdf_url": "http://arxiv.org/pdf/2507.08771v2", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-30"}
{"id": "2507.20884", "title": "The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?", "authors": ["Dinh Nam Pham", "Eleftherios Avramidis"], "categories": ["cs.CV", "cs.CL", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at 9th International Workshop on Sign Language Translation and Avatar Technologies @ ACM IVA'25", "url": "http://arxiv.org/abs/2507.20884v2", "summary": "Non-manual facial features play a crucial role in sign language\ncommunication, yet their importance in automatic sign language recognition\n(ASLR) remains underexplored. While prior studies have shown that incorporating\nfacial features can improve recognition, related work often relies on\nhand-crafted feature extraction and fails to go beyond the comparison of manual\nfeatures versus the combination of manual and facial features. In this work, we\nsystematically investigate the contribution of distinct facial regionseyes,\nmouth, and full faceusing two different deep learning models (a CNN-based model\nand a transformer-based model) trained on an SLR dataset of isolated signs with\nrandomly selected classes. Through quantitative performance and qualitative\nsaliency map evaluation, we reveal that the mouth is the most important\nnon-manual facial feature, significantly improving accuracy. Our findings\nhighlight the necessity of incorporating facial features in ASLR.", "comment": "Accepted at 9th International Workshop on Sign Language Translation\n  and Avatar Technologies @ ACM IVA'25", "pdf_url": "http://arxiv.org/pdf/2507.20884v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-29"}
