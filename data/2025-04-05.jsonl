{"id": "2504.02064", "pdf": "https://arxiv.org/pdf/2504.02064", "abs": "https://arxiv.org/abs/2504.02064", "authors": ["Fabio Yáñez-Romero", "Andrés Montoyo", "Armando Suárez", "Yoan Gutiérrez", "Ruslan Mitkov"], "title": "From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Researchers have relegated natural language processing tasks to\nTransformer-type models, particularly generative models, because these models\nexhibit high versatility when performing generation and classification tasks.\nAs the size of these models increases, they achieve outstanding results. Given\ntheir widespread use, many explainability techniques are developed based on\nthese models. However, this process becomes computationally expensive due to\nthe large size of the models. Additionally, transformers interpret input\ninformation through tokens that fragment input words into sequences lacking\ninherent semantic meaning, complicating the explanation of the model from the\nvery beginning. This study proposes a novel methodology to achieve\nexplainability in natural language processing tasks by automatically converting\nsentences into graphs and maintaining semantics through nodes and relations\nthat express fundamental linguistic concepts. It also allows the subsequent\nexploitation of this knowledge in subsequent tasks, making it possible to\nobtain trends and understand how the model associates the different elements\ninside the text with the explained task. The experiments delivered promising\nresults in determining the most critical components within the text structure\nfor a given classification."}
{"id": "2504.02091", "pdf": "https://arxiv.org/pdf/2504.02091", "abs": "https://arxiv.org/abs/2504.02091", "authors": ["Joseph Heffner", "Chongyu Qin", "Martin Chadwick", "Chris Knutsen", "Christopher Summerfield", "Zeb Kurth-Nelson", "Robb B. Rutledge"], "title": "Increasing happiness through conversations with artificial intelligence", "categories": ["cs.CL"], "comment": "26 pages, 4 figures", "summary": "Chatbots powered by artificial intelligence (AI) have rapidly become a\nsignificant part of everyday life, with over a quarter of American adults using\nthem multiple times per week. While these tools offer potential benefits and\nrisks, a fundamental question remains largely unexplored: How do conversations\nwith AI influence subjective well-being? To investigate this, we conducted a\nstudy where participants either engaged in conversations with an AI chatbot (N\n= 334) or wrote journal entires (N = 193) on the same randomly assigned topics\nand reported their momentary happiness afterward. We found that happiness after\nAI chatbot conversations was higher than after journaling, particularly when\ndiscussing negative topics such as depression or guilt. Leveraging large\nlanguage models for sentiment analysis, we found that the AI chatbot mirrored\nparticipants' sentiment while maintaining a consistent positivity bias. When\ndiscussing negative topics, participants gradually aligned their sentiment with\nthe AI's positivity, leading to an overall increase in happiness. We\nhypothesized that the history of participants' sentiment prediction errors, the\ndifference between expected and actual emotional tone when responding to the AI\nchatbot, might explain this happiness effect. Using computational modeling, we\nfind the history of these sentiment prediction errors over the course of a\nconversation predicts greater post-conversation happiness, demonstrating a\ncentral role of emotional expectations during dialogue. Our findings underscore\nthe effect that AI interactions can have on human well-being."}
{"id": "2504.02106", "pdf": "https://arxiv.org/pdf/2504.02106", "abs": "https://arxiv.org/abs/2504.02106", "authors": ["Xiao Wang", "Daniil Larionov", "Siwei Wu", "Yiqi Liu", "Steffen Eger", "Nafise Sadat Moosavi", "Chenghua Lin"], "title": "ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating the quality of generated text automatically remains a significant\nchallenge. Conventional reference-based metrics have been shown to exhibit\nrelatively weak correlation with human evaluations. Recent research advocates\nthe use of large language models (LLMs) as source-based metrics for natural\nlanguage generation (NLG) assessment. While promising, LLM-based metrics,\nparticularly those using smaller models, still fall short in aligning with\nhuman judgments. In this work, we introduce ContrastScore, a contrastive\nevaluation metric designed to enable higher-quality, less biased, and more\nefficient assessment of generated text. We evaluate ContrastScore on two NLG\ntasks: machine translation and summarization. Experimental results show that\nContrastScore consistently achieves stronger correlation with human judgments\nthan both single-model and ensemble-based baselines. Notably, ContrastScore\nbased on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as\nmany parameters, demonstrating its efficiency. Furthermore, it effectively\nmitigates common evaluation biases such as length and likelihood preferences,\nresulting in more robust automatic evaluation."}
{"id": "2504.02116", "pdf": "https://arxiv.org/pdf/2504.02116", "abs": "https://arxiv.org/abs/2504.02116", "authors": ["Xiulin Yang"], "title": "Language Models at the Syntax-Semantics Interface: A Case Study of the Long-Distance Binding of Chinese Reflexive ziji", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores whether language models can effectively resolve the\ncomplex binding patterns of the Mandarin Chinese reflexive ziji, which are\nconstrained by both syntactic and semantic factors. We construct a dataset of\n240 synthetic sentences using templates and examples from syntactic literature,\nalong with 320 natural sentences from the BCC corpus. Evaluating 21 language\nmodels against this dataset and comparing their performance to judgments from\nnative Mandarin speakers, we find that none of the models consistently\nreplicates human-like judgments. The results indicate that existing language\nmodels tend to rely heavily on sequential cues, though not always favoring the\nclosest strings, and often overlooking subtle semantic and syntactic\nconstraints. They tend to be more sensitive to noun-related than verb-related\nsemantics."}
{"id": "2504.02122", "pdf": "https://arxiv.org/pdf/2504.02122", "abs": "https://arxiv.org/abs/2504.02122", "authors": ["Jonas F. Lotz", "Hendra Setiawan", "Stephan Peitz", "Yova Kementchedjhieva"], "title": "Overcoming Vocabulary Constraints with Pixel-level Fallback", "categories": ["cs.CL"], "comment": null, "summary": "Subword tokenization requires balancing computational efficiency and\nvocabulary coverage, which often leads to suboptimal performance on languages\nand scripts not prioritized during training. We propose to augment pretrained\nlanguage models with a vocabulary-free encoder that generates input embeddings\nfrom text rendered as pixels. Through experiments on English-centric language\nmodels, we demonstrate that our approach substantially improves machine\ntranslation performance and facilitates effective cross-lingual transfer,\noutperforming tokenizer-based methods. Furthermore, we find that pixel-based\nrepresentations outperform byte-level approaches and standard vocabulary\nexpansion. Our approach enhances the multilingual capabilities of monolingual\nlanguage models without extensive retraining and reduces decoding latency via\ninput compression."}
{"id": "2504.02132", "pdf": "https://arxiv.org/pdf/2504.02132", "abs": "https://arxiv.org/abs/2504.02132", "authors": ["Ezzeldin Shereen", "Dan Ristea", "Burak Hasircioglu", "Shae McFadden", "Vasilios Mavroudis", "Chris Hicks"], "title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.IR"], "comment": "8 pages, 6 figures", "summary": "Multimodal retrieval augmented generation (M-RAG) has recently emerged as a\nmethod to inhibit hallucinations of large multimodal models (LMMs) through a\nfactual knowledge base (KB). However, M-RAG also introduces new attack vectors\nfor adversaries that aim to disrupt the system by injecting malicious entries\ninto the KB. In this work, we present a poisoning attack against M-RAG\ntargeting visual document retrieval applications, where the KB contains images\nof document pages. Our objective is to craft a single image that is retrieved\nfor a variety of different user queries, and consistently influences the output\nproduced by the generative model, thus creating a universal denial-of-service\n(DoS) attack against the M-RAG system. We demonstrate that while our attack is\neffective against a diverse range of widely-used, state-of-the-art retrievers\n(embedding models) and generators (LMMs), it can also be ineffective against\nrobust embedding models. Our attack not only highlights the vulnerability of\nM-RAG pipelines to poisoning attacks, but also sheds light on a fundamental\nweakness that potentially hinders their performance even in benign settings."}
{"id": "2504.02146", "pdf": "https://arxiv.org/pdf/2504.02146", "abs": "https://arxiv.org/abs/2504.02146", "authors": ["Lingzhi Shen", "Yunfei Long", "Xiaohao Cai", "Guanming Chen", "Yuhan Wang", "Imran Razzak", "Shoaib Jameel"], "title": "LL4G: Self-Supervised Dynamic Optimization for Graph-Based Personality Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Graph-based personality detection constructs graph structures from textual\ndata, particularly social media posts. Current methods often struggle with\nsparse or noisy data and rely on static graphs, limiting their ability to\ncapture dynamic changes between nodes and relationships. This paper introduces\nLL4G, a self-supervised framework leveraging large language models (LLMs) to\noptimize graph neural networks (GNNs). LLMs extract rich semantic features to\ngenerate node representations and to infer explicit and implicit relationships.\nThe graph structure adaptively adds nodes and edges based on input data,\ncontinuously optimizing itself. The GNN then uses these optimized\nrepresentations for joint training on node reconstruction, edge prediction, and\ncontrastive learning tasks. This integration of semantic and structural\ninformation generates robust personality profiles. Experimental results on\nKaggle and Pandora datasets show LL4G outperforms state-of-the-art models."}
{"id": "2504.02178", "pdf": "https://arxiv.org/pdf/2504.02178", "abs": "https://arxiv.org/abs/2504.02178", "authors": ["Shanilka Haturusinghe", "Tharindu Cyril Weerasooriya", "Marcos Zampieri", "Christopher M. Homan", "S. R. Liyanage"], "title": "Subasa -- Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala", "categories": ["cs.CL"], "comment": "Accepted to appear at NAACL SRW 2025", "summary": "Accurate detection of offensive language is essential for a number of\napplications related to social media safety. There is a sharp contrast in\nperformance in this task between low and high-resource languages. In this\npaper, we adapt fine-tuning strategies that have not been previously explored\nfor Sinhala in the downstream task of offensive language detection. Using this\napproach, we introduce four models: \"Subasa-XLM-R\", which incorporates an\nintermediate Pre-Finetuning step using Masked Rationale Prediction. Two\nvariants of \"Subasa-Llama\" and \"Subasa-Mistral\", are fine-tuned versions of\nLlama (3.2) and Mistral (v0.3), respectively, with a task-specific strategy. We\nevaluate our models on the SOLD benchmark dataset for Sinhala offensive\nlanguage detection. All our models outperform existing baselines. Subasa-XLM-R\nachieves the highest Macro F1 score (0.84) surpassing state-of-the-art large\nlanguage models like GPT-4o when evaluated on the same SOLD benchmark dataset\nunder zero-shot settings. The models and code are publicly available."}
{"id": "2504.02254", "pdf": "https://arxiv.org/pdf/2504.02254", "abs": "https://arxiv.org/abs/2504.02254", "authors": ["Seunghyun Yoo"], "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks", "categories": ["cs.CL", "cs.AI", "68T50, 68T05, 68U35"], "comment": "9 pages, 5 figures, 1 table", "summary": "Recent advancements in Large Language Models (LLMs) have not only showcased\nimpressive creative capabilities but also revealed emerging agentic behaviors\nthat exploit linguistic ambiguity in adversarial settings. In this study, we\ninvestigate how an LLM, acting as an autonomous agent, leverages semantic\nambiguity to generate deceptive puzzles that mislead and challenge human users.\nInspired by the popular puzzle game \"Connections\", we systematically compare\npuzzles produced through zero-shot prompting, role-injected adversarial\nprompts, and human-crafted examples, with an emphasis on understanding the\nunderlying agent decision-making processes. Employing computational analyses\nwith HateBERT to quantify semantic ambiguity, alongside subjective human\nevaluations, we demonstrate that explicit adversarial agent behaviors\nsignificantly heighten semantic ambiguity -- thereby increasing cognitive load\nand reducing fairness in puzzle solving. These findings provide critical\ninsights into the emergent agentic qualities of LLMs and underscore important\nethical considerations for evaluating and safely deploying autonomous language\nsystems in both educational technologies and entertainment."}
{"id": "2504.02293", "pdf": "https://arxiv.org/pdf/2504.02293", "abs": "https://arxiv.org/abs/2504.02293", "authors": ["Sharif Md. Abdullah", "Abhijit Paul", "Shebuti Rayana", "Ahmedul Kabir", "Zarif Masud"], "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla", "categories": ["cs.CL", "cs.AI"], "comment": "Initial Version", "summary": "Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language\n(BdSL) remains a understudied domain. Specifically, there are no works on\nBangla text-to-gloss translation task. To address this gap, we begin by\naddressing the dataset problem. We take inspiration from grammatical rule based\ngloss generation used in Germany and American sign langauage (ASL) and adapt it\nfor BdSL. We also leverage LLM to generate synthetic data and use\nback-translation, text generation for data augmentation. With dataset prepared,\nwe started experimentation. We fine-tuned pretrained mBART-50 and\nmBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a\nnovel seq-to-seq model with multi-head attention. We observe significant high\nperformance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual\nmodel from Facebook. We then explored why we observe such high performance with\nmBART. We soon notice an interesting property of mBART -- it was trained on\nshuffled and masked text data. And as we know, gloss form has shuffling\nproperty. So we hypothesize that mBART is inherently good at text-to-gloss\ntasks. To find support against this hypothesis, we trained mBART-50 on\nPHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50\nfinetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark,\nfar outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 =\n55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on\nthe results, this study proposes a new paradigm for text-to-gloss task using\nmBART models. Additionally, our results show that BdSL text-to-gloss task can\ngreatly benefit from rule-based synthetic dataset."}
{"id": "2504.02304", "pdf": "https://arxiv.org/pdf/2504.02304", "abs": "https://arxiv.org/abs/2504.02304", "authors": ["Minheng Ni", "Ennan Wu", "Zidong Gong", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Lijuan Wang", "Wangmeng Zuo"], "title": "Measurement of LLM's Philosophies of Human Nature", "categories": ["cs.CL"], "comment": null, "summary": "The widespread application of artificial intelligence (AI) in various tasks,\nalong with frequent reports of conflicts or violations involving AI, has\nsparked societal concerns about interactions with AI systems. Based on\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\nvalidated over decades to effectively assess individuals' attitudes toward\nhuman nature, we design the standardized psychological scale specifically\ntargeting large language models (LLM), named the Machine-based Philosophies of\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\ntrust in humans, and there is a significant negative correlation between the\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\nmental loop learning framework, which enables LLM to continuously optimize its\nvalue system during virtual interactions by constructing moral scenarios,\nthereby improving its attitude toward human nature. Experiments demonstrate\nthat mental loop learning significantly enhances their trust in humans compared\nto persona or instruction prompts. This finding highlights the potential of\nhuman-based psychological assessments for LLM, which can not only diagnose\ncognitive biases but also provide a potential solution for ethical learning in\nartificial intelligence. We release the M-PHNS evaluation code and data at\nhttps://github.com/kodenii/M-PHNS."}
{"id": "2504.02310", "pdf": "https://arxiv.org/pdf/2504.02310", "abs": "https://arxiv.org/abs/2504.02310", "authors": ["Zidong Yu", "Shuo Wang", "Nan Jiang", "Weiqiang Huang", "Xu Han", "Junliang Du"], "title": "Improving Harmful Text Detection with Joint Retrieval and External Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Harmful text detection has become a crucial task in the development and\ndeployment of large language models, especially as AI-generated content\ncontinues to expand across digital platforms. This study proposes a joint\nretrieval framework that integrates pre-trained language models with knowledge\ngraphs to improve the accuracy and robustness of harmful text detection.\nExperimental results demonstrate that the joint retrieval approach\nsignificantly outperforms single-model baselines, particularly in low-resource\ntraining scenarios and multilingual environments. The proposed method\neffectively captures nuanced harmful content by leveraging external contextual\ninformation, addressing the limitations of traditional detection models. Future\nresearch should focus on optimizing computational efficiency, enhancing model\ninterpretability, and expanding multimodal detection capabilities to better\ntackle evolving harmful content patterns. This work contributes to the\nadvancement of AI safety, ensuring more trustworthy and reliable content\nmoderation systems."}
{"id": "2504.02323", "pdf": "https://arxiv.org/pdf/2504.02323", "abs": "https://arxiv.org/abs/2504.02323", "authors": ["Clayton Cohn", "Nicole Hutchins", "Ashwin T S", "Gautam Biswas"], "title": "CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning, and Active Learning for Generalizable Formative Assessment Scoring", "categories": ["cs.CL"], "comment": "Submitted to IEEE Transactions on Learning Technologies. Currently\n  under review", "summary": "Large language models (LLMs) have created new opportunities to assist\nteachers and support student learning. Methods such as chain-of-thought (CoT)\nprompting enable LLMs to grade formative assessments in science, providing\nscores and relevant feedback to students. However, the extent to which these\nmethods generalize across curricula in multiple domains (such as science,\ncomputing, and engineering) remains largely untested. In this paper, we\nintroduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based\napproach to formative assessment scoring that (1) leverages Evidence-Centered\nDesign (ECD) principles to develop curriculum-aligned formative assessments and\nrubrics, (2) applies human-in-the-loop prompt engineering to automate response\nscoring, and (3) incorporates teacher and student feedback to iteratively\nrefine assessment questions, grading rubrics, and LLM prompts for automated\ngrading. Our findings demonstrate that CoTAL improves GPT-4's scoring\nperformance, achieving gains of up to 24.5% over a non-prompt-engineered\nbaseline. Both teachers and students view CoTAL as effective in scoring and\nexplaining student responses, each providing valuable refinements to enhance\ngrading accuracy and explanation quality."}
{"id": "2504.02327", "pdf": "https://arxiv.org/pdf/2504.02327", "abs": "https://arxiv.org/abs/2504.02327", "authors": ["Weibin Liao", "Xin Gao", "Tianyu Jia", "Rihong Qiu", "Yifan Zhu", "Yang Lin", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling\nseamless interaction with databases. Recent advancements in Large Language\nModels (LLMs) have demonstrated remarkable performance in this domain. However,\nexisting NL2SQL methods predominantly rely on closed-source LLMs leveraging\nprompt engineering, while open-source models typically require fine-tuning to\nacquire domain-specific knowledge. Despite these efforts, open-source LLMs\nstruggle with complex NL2SQL tasks due to the indirect expression of user query\nobjectives and the semantic gap between user queries and database schemas.\nInspired by the application of reinforcement learning in mathematical\nproblem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT\n(Learning NL2SQL with AST-guided Task Decomposition), a novel framework that\nimproves the performance of open-source LLMs on complex NL2SQL tasks through\ntask decomposition and reinforcement learning. LearNAT introduces three key\ncomponents: (1) a Decomposition Synthesis Procedure that leverages Abstract\nSyntax Trees (ASTs) to guide efficient search and pruning strategies for task\ndecomposition, (2) Margin-aware Reinforcement Learning, which employs\nfine-grained step-level optimization via DPO with AST margins, and (3) Adaptive\nDemonstration Reasoning, a mechanism for dynamically selecting relevant\nexamples to enhance decomposition capabilities. Extensive experiments on two\nbenchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a\n7B-parameter open-source LLM to achieve performance comparable to GPT-4, while\noffering improved efficiency and accessibility."}
{"id": "2504.02395", "pdf": "https://arxiv.org/pdf/2504.02395", "abs": "https://arxiv.org/abs/2504.02395", "authors": ["Mattia Proietti", "Alessandro Lenci"], "title": "The quasi-semantic competence of LLMs: a case study on the part-whole relation", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the extent and depth of the semantic competence of \\emph{Large\nLanguage Models} (LLMs) is at the center of the current scientific agenda in\nArtificial Intelligence (AI) and Computational Linguistics (CL). We contribute\nto this endeavor by investigating their knowledge of the \\emph{part-whole}\nrelation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical\norganization, but it is significantly understudied. We used data from\nConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic\nfeature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with\n\\textit{part-whole} relations. We employed several methods based on three\nlevels of analysis: i.) \\textbf{behavioral} testing via prompting, where we\ndirectly queried the models on their knowledge of meronymy, ii.) sentence\n\\textbf{probability} scoring, where we tested models' abilities to discriminate\ncorrect (real) and incorrect (asymmetric counterfactual) \\textit{part-whole}\nrelations, and iii.) \\textbf{concept representation} analysis in vector space,\nwhere we proved the linear organization of the \\textit{part-whole} concept in\nthe embedding and unembedding spaces. These analyses present a complex picture\nthat reveals that the LLMs' knowledge of this relation is only partial. They\nhave just a ``\\emph{quasi}-semantic'' competence and still fall short of\ncapturing deep inferential properties."}
{"id": "2504.02398", "pdf": "https://arxiv.org/pdf/2504.02398", "abs": "https://arxiv.org/abs/2504.02398", "authors": ["Gallil Maimon", "Michael Hassid", "Amit Roth", "Yossi Adi"], "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims."}
{"id": "2504.02403", "pdf": "https://arxiv.org/pdf/2504.02403", "abs": "https://arxiv.org/abs/2504.02403", "authors": ["Max Müller-Eberstein", "Mike Zhang", "Elisa Bassignana", "Peter Brunsgaard Trolle", "Rob van der Goot"], "title": "DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted at C3NLP at NAACL", "summary": "Large Language Models (LLMs) have seen widespread societal adoption. However,\nwhile they are able to interact with users in languages beyond English, they\nhave been shown to lack cultural awareness, providing anglocentric or\ninappropriate responses for underrepresented language communities. To\ninvestigate this gap and disentangle linguistic versus cultural proficiency, we\nconduct the first cultural evaluation study for the mid-resource language of\nDanish, in which native speakers prompt different models to solve tasks\nrequiring cultural awareness. Our analysis of the resulting 1,038 interactions\nfrom 63 demographically diverse participants highlights open challenges to\ncultural adaptation: Particularly, how currently employed automatically\ntranslated data are insufficient to train or measure cultural adaptation, and\nhow training on native-speaker data can more than double response acceptance\nrates. We release our study data as DaKultur - the first native Danish cultural\nawareness dataset."}
{"id": "2504.02060", "pdf": "https://arxiv.org/pdf/2504.02060", "abs": "https://arxiv.org/abs/2504.02060", "authors": ["Minh-Quan Ho-Le", "Duy-Khang Ho", "Van-Tu Ninh", "Cathal Gurrin", "Minh-Triet Tran"], "title": "LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated via Semi-Automatic Clustering", "categories": ["cs.CV", "cs.IR"], "comment": "11 pages, 4 figures", "summary": "Lifelogging involves continuously capturing personal data through wearable\ncameras, providing an egocentric view of daily activities. Lifelog retrieval\naims to search and retrieve relevant moments from this data, yet existing\nmethods largely overlook activity-level annotations, which capture temporal\nrelationships and enrich semantic understanding. In this work, we introduce\nLSC-ADL, an ADL-annotated lifelog dataset derived from the LSC dataset,\nincorporating Activities of Daily Living (ADLs) as a structured semantic layer.\nUsing a semi-automatic approach featuring the HDBSCAN algorithm for intra-class\nclustering and human-in-the-loop verification, we generate accurate ADL\nannotations to enhance retrieval explainability. By integrating action\nrecognition into lifelog retrieval, LSC-ADL bridges a critical gap in existing\nresearch, offering a more context-aware representation of daily life. We\nbelieve this dataset will advance research in lifelog retrieval, activity\nrecognition, and egocentric vision, ultimately improving the accuracy and\ninterpretability of retrieved content. The ADL annotations can be downloaded at\nhttps://bit.ly/lsc-adl-annotations."}
{"id": "2504.02404", "pdf": "https://arxiv.org/pdf/2504.02404", "abs": "https://arxiv.org/abs/2504.02404", "authors": ["Xiang Feng", "Wentao Jiang", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Baosheng Yu", "Hua Jin", "Bo Du", "Jing Zhang"], "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology", "categories": ["cs.CL"], "comment": "23 pages, 9 figures", "summary": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench."}
{"id": "2504.02061", "pdf": "https://arxiv.org/pdf/2504.02061", "abs": "https://arxiv.org/abs/2504.02061", "authors": ["Yuxin Guo", "Shuailei Ma", "Shijie Ma", "Xiaoyi Bao", "Chen-Wei Xie", "Kecheng Zheng", "Tingyu Weng", "Siyang Sun", "Yun Zheng", "Wei Zou"], "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ICLR 2025", "summary": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations."}
{"id": "2504.02411", "pdf": "https://arxiv.org/pdf/2504.02411", "abs": "https://arxiv.org/abs/2504.02411", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation", "categories": ["cs.CL"], "comment": "25 pages, 8 figures, 21 tables", "summary": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\nmulti-domain applications face challenges like lack of diverse benchmarks and\npoor out-of-domain generalization. The first contribution of this work is to\nintroduce a diverse benchmark comprising a variety of question-answering tasks\nfrom 8 sources and covering 13 domains. Our second contribution consists in\nsystematically testing out-of-domain generalization for typical RAG tuning\nstrategies. While our findings reveal that standard fine-tuning fails to\ngeneralize effectively, we show that sequence-level distillation with\nteacher-generated labels improves out-of-domain performance by providing more\ncoherent supervision. Our findings highlight key strategies for improving\nmulti-domain RAG robustness."}
{"id": "2504.02154", "pdf": "https://arxiv.org/pdf/2504.02154", "abs": "https://arxiv.org/abs/2504.02154", "authors": ["Chao Huang", "Susan Liang", "Yunlong Tang", "Li Ma", "Yapeng Tian", "Chenliang Xu"], "title": "FreSca: Unveiling the Scaling Space in Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://wikichao.github.io/FreSca/", "summary": "Diffusion models offer impressive controllability for image tasks, primarily\nthrough noise predictions that encode task-specific information and\nclassifier-free guidance enabling adjustable scaling. This scaling mechanism\nimplicitly defines a ``scaling space'' whose potential for fine-grained\nsemantic manipulation remains underexplored. We investigate this space,\nstarting with inversion-based editing where the difference between\nconditional/unconditional noise predictions carries key semantic information.\nOur core contribution stems from a Fourier analysis of noise predictions,\nrevealing that its low- and high-frequency components evolve differently\nthroughout diffusion. Based on this insight, we introduce FreSca, a\nstraightforward method that applies guidance scaling independently to different\nfrequency bands in the Fourier domain. FreSca demonstrably enhances existing\nimage editing methods without retraining. Excitingly, its effectiveness extends\nto image understanding tasks such as depth estimation, yielding quantitative\ngains across multiple datasets."}
{"id": "2504.02438", "pdf": "https://arxiv.org/pdf/2504.02438", "abs": "https://arxiv.org/abs/2504.02438", "authors": ["Chuanqi Cheng", "Jian Guan", "Wei Wu", "Rui Yan"], "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLaMP, a hierarchical video-language model that\nprocesses hour-long videos at ``mixed precision'' through two key mechanisms:\n(1) differential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLaMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLaMP's superior performance\nacross four video understanding benchmarks, particularly on long-form content.\nNotably, ViLaMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance."}
{"id": "2504.02158", "pdf": "https://arxiv.org/pdf/2504.02158", "abs": "https://arxiv.org/abs/2504.02158", "authors": ["Jaehoon Choi", "Dongki Jung", "Yonghan Lee", "Sungmin Eum", "Dinesh Manocha", "Heesung Kwon"], "title": "UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We present UAVTwin, a method for creating digital twins from real-world\nenvironments and facilitating data augmentation for training downstream models\nembedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses\non synthesizing foreground components, such as various human instances in\nmotion within complex scene backgrounds, from UAV perspectives. This is\nachieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing\nbackgrounds along with controllable synthetic human models that display diverse\nappearances and actions in multiple poses. To the best of our knowledge,\nUAVTwin is the first approach for UAV-based perception that is capable of\ngenerating high-fidelity digital twins based on 3DGS. The proposed work\nsignificantly enhances downstream models through data augmentation for\nreal-world environments with multiple dynamic objects and significant\nappearance variations-both of which typically introduce artifacts in 3DGS-based\nmodeling. To tackle these challenges, we propose a novel appearance modeling\nstrategy and a mask refinement module to enhance the training of 3D Gaussian\nSplatting. We demonstrate the high quality of neural rendering by achieving a\n1.23 dB improvement in PSNR compared to recent methods. Furthermore, we\nvalidate the effectiveness of data augmentation by showing a 2.5% to 13.7%\nimprovement in mAP for the human detection task."}
{"id": "2504.02441", "pdf": "https://arxiv.org/pdf/2504.02441", "abs": "https://arxiv.org/abs/2504.02441", "authors": ["Lianlei Shan", "Shixian Luo", "Zezhou Zhu", "Yu Yuan", "Yong Wu"], "title": "Cognitive Memory in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 9 figures", "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."}
{"id": "2504.02160", "pdf": "https://arxiv.org/pdf/2504.02160", "abs": "https://arxiv.org/abs/2504.02160", "authors": ["Shaojin Wu", "Mengqi Huang", "Wenxu Wu", "Yufeng Cheng", "Fei Ding", "Qian He"], "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://bytedance.github.io/UNO Code and model:\n  https://github.com/bytedance/UNO", "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration."}
{"id": "2504.02495", "pdf": "https://arxiv.org/pdf/2504.02495", "abs": "https://arxiv.org/abs/2504.02495", "authors": ["Zijun Liu", "Peiyi Wang", "Runxin Xu", "Shirong Ma", "Chong Ruan", "Peng Li", "Yang Liu", "Yu Wu"], "title": "Inference-Time Scaling for Generalist Reward Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review. 42 pages", "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced."}
{"id": "2504.02168", "pdf": "https://arxiv.org/pdf/2504.02168", "abs": "https://arxiv.org/abs/2504.02168", "authors": ["Xinglong Sun", "Barath Lakshmanan", "Maying Shen", "Shiyi Lan", "Jingde Chen", "Jose M. Alvarez"], "title": "MDP: Multidimensional Vision Model Pruning with Latency Constraint", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at CVPR 2025", "summary": "Current structural pruning methods face two significant limitations: (i) they\noften limit pruning to finer-grained levels like channels, making aggressive\nparameter reduction challenging, and (ii) they focus heavily on parameter and\nFLOP reduction, with existing latency-aware methods frequently relying on\nsimplistic, suboptimal linear models that fail to generalize well to\ntransformers, where multiple interacting dimensions impact latency. In this\npaper, we address both limitations by introducing Multi-Dimensional Pruning\n(MDP), a novel paradigm that jointly optimizes across a variety of pruning\ngranularities-including channels, query, key, heads, embeddings, and blocks.\nMDP employs an advanced latency modeling technique to accurately capture\nlatency variations across all prunable dimensions, achieving an optimal balance\nbetween latency and accuracy. By reformulating pruning as a Mixed-Integer\nNonlinear Program (MINLP), MDP efficiently identifies the optimal pruned\nstructure across all prunable dimensions while respecting latency constraints.\nThis versatile framework supports both CNNs and transformers. Extensive\nexperiments demonstrate that MDP significantly outperforms previous methods,\nespecially at high pruning ratios. On ImageNet, MDP achieves a 28% speed\nincrease with a +1.4 Top-1 accuracy improvement over prior work like HALP for\nResNet50 pruning. Against the latest transformer pruning method, Isomorphic,\nMDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy\nimprovement."}
{"id": "2504.02521", "pdf": "https://arxiv.org/pdf/2504.02521", "abs": "https://arxiv.org/abs/2504.02521", "authors": ["Kushal Jain", "Piyushi Goyal", "Kumar Shridhar"], "title": "UNDO: Understanding Distillation as Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation has emerged as an effective strategy for compressing\nlarge language models' (LLMs) knowledge into smaller, more efficient student\nmodels. However, standard one-shot distillation methods often produce\nsuboptimal results due to a mismatch between teacher-generated rationales and\nthe student's specific learning requirements. In this paper, we introduce the\nUNDO: UNderstanding Distillation as Optimization framework, designed to bridge\nthis gap by iteratively identifying the student's errors and prompting the\nteacher to refine its explanations accordingly. Each iteration directly targets\nthe student's learning deficiencies, motivating the teacher to provide tailored\nand enhanced rationales that specifically address these weaknesses. Empirical\nevaluations on various challenging mathematical and commonsense reasoning tasks\ndemonstrate that our iterative distillation method, UNDO, significantly\noutperforms standard one-step distillation methods, achieving performance gains\nof up to 20%. Additionally, we show that teacher-generated data refined through\nour iterative process remains effective even when applied to different student\nmodels, underscoring the broad applicability of our approach. Our work\nfundamentally reframes knowledge distillation as an iterative teacher-student\ninteraction, effectively leveraging dynamic refinement by the teacher for\nbetter knowledge distillation."}
{"id": "2504.02180", "pdf": "https://arxiv.org/pdf/2504.02180", "abs": "https://arxiv.org/abs/2504.02180", "authors": ["Pei-Chi Chen", "Yi Yao", "Chan-Feng Hsu", "HongXia Xie", "Hung-Jen Chen", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "Foreground Focus: Enhancing Coherence and Fidelity in Camouflaged Image Generation", "categories": ["cs.CV", "I.4.0; I.4.8; I.2.10"], "comment": null, "summary": "Camouflaged image generation is emerging as a solution to data scarcity in\ncamouflaged vision perception, offering a cost-effective alternative to data\ncollection and labeling. Recently, the state-of-the-art approach successfully\ngenerates camouflaged images using only foreground objects. However, it faces\ntwo critical weaknesses: 1) the background knowledge does not integrate\neffectively with foreground features, resulting in a lack of\nforeground-background coherence (e.g., color discrepancy); 2) the generation\nprocess does not prioritize the fidelity of foreground objects, which leads to\ndistortion, particularly for small objects. To address these issues, we propose\na Foreground-Aware Camouflaged Image Generation (FACIG) model. Specifically, we\nintroduce a Foreground-Aware Feature Integration Module (FAFIM) to strengthen\nthe integration between foreground features and background knowledge. In\naddition, a Foreground-Aware Denoising Loss is designed to enhance foreground\nreconstruction supervision. Experiments on various datasets show our method\noutperforms previous methods in overall camouflaged image quality and\nforeground fidelity."}
{"id": "2504.02559", "pdf": "https://arxiv.org/pdf/2504.02559", "abs": "https://arxiv.org/abs/2504.02559", "authors": ["Siddharth Khincha", "Tushar Kataria", "Ankita Anand", "Dan Roth", "Vivek Gupta"], "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables", "categories": ["cs.CL"], "comment": "17 Pages, 11 Tables, 2 Figures", "summary": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures"}
{"id": "2504.02199", "pdf": "https://arxiv.org/pdf/2504.02199", "abs": "https://arxiv.org/abs/2504.02199", "authors": ["Tae-Young Lee", "Sundong Park", "Minwoo Jeon", "Hyoseok Hwang", "Gyeong-Moon Park"], "title": "ESC: Erasing Space Concept for Knowledge Deletion", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 14 figures, 18 tables, CVPR 2025", "summary": "As concerns regarding privacy in deep learning continue to grow, individuals\nare increasingly apprehensive about the potential exploitation of their\npersonal knowledge in trained models. Despite several research efforts to\naddress this, they often fail to consider the real-world demand from users for\ncomplete knowledge erasure. Furthermore, our investigation reveals that\nexisting methods have a risk of leaking personal knowledge through embedding\nfeatures. To address these issues, we introduce a novel concept of Knowledge\nDeletion (KD), an advanced task that considers both concerns, and provides an\nappropriate metric, named Knowledge Retention score (KR), for assessing\nknowledge retention in feature space. To achieve this, we propose a novel\ntraining-free erasing approach named Erasing Space Concept (ESC), which\nrestricts the important subspace for the forgetting knowledge by eliminating\nthe relevant activations in the feature. In addition, we suggest ESC with\nTraining (ESC-T), which uses a learnable mask to better balance the trade-off\nbetween forgetting and preserving knowledge in KD. Our extensive experiments on\nvarious datasets and models demonstrate that our proposed methods achieve the\nfastest and state-of-the-art performance. Notably, our methods are applicable\nto diverse forgetting scenarios, such as facial domain setting, demonstrating\nthe generalizability of our methods. The code is available at\nhttp://github.com/KU-VGI/ESC ."}
{"id": "2504.02572", "pdf": "https://arxiv.org/pdf/2504.02572", "abs": "https://arxiv.org/abs/2504.02572", "authors": ["Fabio Celli", "Georgios Spathulas"], "title": "Language Models reach higher Agreement than Humans in Historical Interpretation", "categories": ["cs.CL"], "comment": null, "summary": "This paper compares historical annotations by humans and Large Language\nModels. The findings reveal that both exhibit some cultural bias, but Large\nLanguage Models achieve a higher consensus on the interpretation of historical\nfacts from short texts. While humans tend to disagree on the basis of their\npersonal biases, Large Models disagree when they skip information or produce\nhallucinations. These findings have significant implications for digital\nhumanities, enabling large-scale annotation and quantitative analysis of\nhistorical data. This offers new educational and research opportunities to\nexplore historical interpretations from different Language Models, fostering\ncritical thinking about bias."}
{"id": "2504.02214", "pdf": "https://arxiv.org/pdf/2504.02214", "abs": "https://arxiv.org/abs/2504.02214", "authors": ["Hyunho Lee", "Wenwen Li"], "title": "Geospatial Artificial Intelligence for Satellite-based Flood Extent Mapping: Concepts, Advances, and Future Perspectives", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 5 figures", "summary": "Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent\nmapping systematically integrates artificial intelligence techniques with\nsatellite data to identify flood events and assess their impacts, for disaster\nmanagement and spatial decision-making. The primary output often includes flood\nextent maps, which delineate the affected areas, along with additional\nanalytical outputs such as uncertainty estimation and change detection."}
{"id": "2504.02590", "pdf": "https://arxiv.org/pdf/2504.02590", "abs": "https://arxiv.org/abs/2504.02590", "authors": ["Kepu Zhang", "Guofu Xie", "Weijie Yu", "Mingyue Xu", "Xu Tang", "Yaxin Li", "Jun Xu"], "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks."}
{"id": "2504.02231", "pdf": "https://arxiv.org/pdf/2504.02231", "abs": "https://arxiv.org/abs/2504.02231", "authors": ["Zhipu Cui", "Andong Tian", "Zhi Ying", "Jialiang Lu"], "title": "AC-LoRA: Auto Component LoRA for Personalized Artistic Style Image Generation", "categories": ["cs.CV", "cs.AI", "68T05, 68U10", "I.2.6; I.4.0"], "comment": "11 pages, 4 figures, ICCGV 2025, SPIE", "summary": "Personalized image generation allows users to preserve styles or subjects of\na provided small set of images for further image generation. With the\nadvancement in large text-to-image models, many techniques have been developed\nto efficiently fine-tune those models for personalization, such as Low Rank\nAdaptation (LoRA). However, LoRA-based methods often face the challenge of\nadjusting the rank parameter to achieve satisfactory results. To address this\nchallenge, AutoComponent-LoRA (AC-LoRA) is proposed, which is able to\nautomatically separate the signal component and noise component of the LoRA\nmatrices for fast and efficient personalized artistic style image generation.\nThis method is based on Singular Value Decomposition (SVD) and dynamic\nheuristics to update the hyperparameters during training. Superior performance\nover existing methods in overcoming model underfitting or overfitting problems\nis demonstrated. The results were validated using FID, CLIP, DINO, and\nImageReward, achieving an average of 9% improvement."}
{"id": "2504.02604", "pdf": "https://arxiv.org/pdf/2504.02604", "abs": "https://arxiv.org/abs/2504.02604", "authors": ["Hedi Naouara", "Jean-Pierre Lorré", "Jérôme Louradour"], "title": "LinTO Audio and Textual Datasets to Train and Evaluate Automatic Speech Recognition in Tunisian Arabic Dialect", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing Automatic Speech Recognition (ASR) systems for Tunisian Arabic\nDialect is challenging due to the dialect's linguistic complexity and the\nscarcity of annotated speech datasets. To address these challenges, we propose\nthe LinTO audio and textual datasets -- comprehensive resources that capture\nphonological and lexical features of Tunisian Arabic Dialect. These datasets\ninclude a variety of texts from numerous sources and real-world audio samples\nfeaturing diverse speakers and code-switching between Tunisian Arabic Dialect\nand English or French. By providing high-quality audio paired with precise\ntranscriptions, the LinTO audio and textual datasets aim to provide qualitative\nmaterial to build and benchmark ASR systems for the Tunisian Arabic Dialect.\n  Keywords -- Tunisian Arabic Dialect, Speech-to-Text, Low-Resource Languages,\nAudio Data Augmentation"}
{"id": "2504.02244", "pdf": "https://arxiv.org/pdf/2504.02244", "abs": "https://arxiv.org/abs/2504.02244", "authors": ["Xu Cao", "Pranav Virupaksha", "Wenqi Jia", "Bolin Lai", "Fiona Ryan", "Sangmin Lee", "James M. Rehg"], "title": "SocialGesture: Delving into Multi-person Gesture Understanding", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Previous research in human gesture recognition has largely overlooked\nmulti-person interactions, which are crucial for understanding the social\ncontext of naturally occurring gestures. This limitation in existing datasets\npresents a significant challenge in aligning human gestures with other\nmodalities like language and speech. To address this issue, we introduce\nSocialGesture, the first large-scale dataset specifically designed for\nmulti-person gesture analysis. SocialGesture features a diverse range of\nnatural scenarios and supports multiple gesture analysis tasks, including\nvideo-based recognition and temporal localization, providing a valuable\nresource for advancing the study of gesture during complex social interactions.\nFurthermore, we propose a novel visual question answering (VQA) task to\nbenchmark vision language models'(VLMs) performance on social gesture\nunderstanding. Our findings highlight several limitations of current gesture\nrecognition models, offering insights into future directions for improvement in\nthis field. SocialGesture is available at\nhuggingface.co/datasets/IrohXu/SocialGesture."}
{"id": "2504.02671", "pdf": "https://arxiv.org/pdf/2504.02671", "abs": "https://arxiv.org/abs/2504.02671", "authors": ["Zishuo Liu", "Carlos Rabat Villarreal", "Mostafa Rahgouy", "Amit Das", "Zheng Zhang", "Chang Ren", "Dongji Feng"], "title": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems", "categories": ["cs.CL"], "comment": "7 pages,7 tables, 5 figures", "summary": "Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\ninvolve real-world impracticalities or ambiguous concepts, making them\nchallenging even for humans to solve. Despite advancements in AI, particularly\nwith large language models (LLMs) in various reasoning tasks, FPs remain\nrelatively under-explored. This work conducted an exploratory study to examine\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\noverall performance of three advanced LLMs using a publicly available FP\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\ninto standard and specific questions, hypothesizing that LLMs would perform\nbetter on standard questions, which are characterized by clarity and\nconciseness, than on specific ones. Comparative experiments confirmed this\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\nof both accuracy and efficiency."}
{"id": "2504.02259", "pdf": "https://arxiv.org/pdf/2504.02259", "abs": "https://arxiv.org/abs/2504.02259", "authors": ["Jinhui Ye", "Zihan Wang", "Haosen Sun", "Keshigeyan Chandrasegaran", "Zane Durante", "Cristobal Eyzaguirre", "Yonatan Bisk", "Juan Carlos Niebles", "Ehsan Adeli", "Li Fei-Fei", "Jiajun Wu", "Manling Li"], "title": "Re-thinking Temporal Search for Long-Form Video Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; A real-world long video needle-in-haystack\n  benchmark; long-video QA with human ref frames", "summary": "Efficient understanding of long-form videos remains a significant challenge\nin computer vision. In this work, we revisit temporal search paradigms for\nlong-form video understanding, studying a fundamental issue pertaining to all\nstate-of-the-art (SOTA) long-context vision-language models (VLMs). In\nparticular, our contributions are two-fold: First, we formulate temporal search\nas a Long Video Haystack problem, i.e., finding a minimal set of relevant\nframes (typically one to five) among tens of thousands of frames from\nreal-world long videos given specific queries. To validate our formulation, we\ncreate LV-Haystack, the first benchmark containing 3,874 human-annotated\ninstances with fine-grained evaluation metrics for assessing keyframe search\nquality and computational efficiency. Experimental results on LV-Haystack\nhighlight a significant research gap in temporal search capabilities, with SOTA\nkeyframe selection methods achieving only 2.1% temporal F1 score on the LVBench\nsubset.\n  Next, inspired by visual search in images, we re-think temporal searching and\npropose a lightweight keyframe searching framework, T*, which casts the\nexpensive temporal search as a spatial search problem. T* leverages superior\nvisual localization capabilities typically used in images and introduces an\nadaptive zooming-in mechanism that operates across both temporal and spatial\ndimensions. Our extensive experiments show that when integrated with existing\nmethods, T* significantly improves SOTA long-form video understanding\nperformance. Specifically, under an inference budget of 32 frames, T* improves\nGPT-4o's performance from 50.5% to 53.1% and LLaVA-OneVision-72B's performance\nfrom 56.5% to 62.4% on LongVideoBench XL subset. Our PyTorch code, benchmark\ndataset and models are included in the Supplementary material."}
{"id": "2504.02674", "pdf": "https://arxiv.org/pdf/2504.02674", "abs": "https://arxiv.org/abs/2504.02674", "authors": ["Jacqueline Rowe", "Edward Gow-Smith", "Mark Hepple"], "title": "Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole", "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 7 tables. To be published in Proceedings of the\n  8th Workshop on Technologies for Machine Translation of Low-Resource\n  Languages (NAACL 2025)", "summary": "We introduce a new dataset for machine translation of Guinea-Bissau Creole\n(Kiriol), comprising around 40 thousand parallel sentences to English and\nPortuguese. This dataset is made up of predominantly religious data (from the\nBible and texts from the Jehovah's Witnesses), but also a small amount of\ngeneral domain data (from a dictionary). This mirrors the typical resource\navailability of many low resource languages. We train a number of\ntransformer-based models to investigate how to improve domain transfer from\nreligious data to a more general domain. We find that adding even 300 sentences\nfrom the target domain when training substantially improves the translation\nperformance, highlighting the importance and need for data collection for\nlow-resource languages, even on a small-scale. We additionally find that\nPortuguese-to-Kiriol translation models perform better on average than other\nsource and target language pairs, and investigate how this relates to the\nmorphological complexity of the languages involved and the degree of lexical\noverlap between creoles and lexifiers. Overall, we hope our work will stimulate\nresearch into Kiriol and into how machine translation might better support\ncreole languages in general."}
{"id": "2504.02261", "pdf": "https://arxiv.org/pdf/2504.02261", "abs": "https://arxiv.org/abs/2504.02261", "authors": ["Chaojun Ni", "Xiaofeng Wang", "Zheng Zhu", "Weijie Wang", "Haoyun Li", "Guosheng Zhao", "Jie Li", "Wenkang Qin", "Guan Huang", "Wenjun Mei"], "title": "WonderTurbo: Generating Interactive 3D World in 0.72 Seconds", "categories": ["cs.CV"], "comment": "Project Page: https://wonderturbo.github.io", "summary": "Interactive 3D generation is gaining momentum and capturing extensive\nattention for its potential to create immersive virtual experiences. However, a\ncritical challenge in current 3D generation technologies lies in achieving\nreal-time interactivity. To address this issue, we introduce WonderTurbo, the\nfirst real-time interactive 3D scene generation framework capable of generating\nnovel perspectives of 3D scenes within 0.72 seconds. Specifically, WonderTurbo\naccelerates both geometric and appearance modeling in 3D scene generation. In\nterms of geometry, we propose StepSplat, an innovative method that constructs\nefficient 3D geometric representations through dynamic updates, each taking\nonly 0.26 seconds. Additionally, we design QuickDepth, a lightweight depth\ncompletion module that provides consistent depth input for StepSplat, further\nenhancing geometric accuracy. For appearance modeling, we develop FastPaint, a\n2-steps diffusion model tailored for instant inpainting, which focuses on\nmaintaining spatial appearance consistency. Experimental results demonstrate\nthat WonderTurbo achieves a remarkable 15X speedup compared to baseline\nmethods, while preserving excellent spatial consistency and delivering\nhigh-quality output."}
{"id": "2504.02708", "pdf": "https://arxiv.org/pdf/2504.02708", "abs": "https://arxiv.org/abs/2504.02708", "authors": ["Nikhil Verma", "Manasa Bharadwaj"], "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context", "categories": ["cs.CL"], "comment": "14 pages, 11 Figures, 2 Tables, currently under review at ACL 2025", "summary": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages."}
{"id": "2504.02264", "pdf": "https://arxiv.org/pdf/2504.02264", "abs": "https://arxiv.org/abs/2504.02264", "authors": ["Wenzhuo Liu", "Wenshuo Wang", "Yicheng Qiao", "Qiannan Guo", "Jiayin Zhu", "Pengfei Li", "Zilong Chen", "Huiming Yang", "Zhiwei Li", "Lening Wang", "Tiao Tan", "Huaping Liu"], "title": "MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception", "categories": ["cs.CV"], "comment": null, "summary": "Advanced driver assistance systems require a comprehensive understanding of\nthe driver's mental/physical state and traffic context but existing works often\nneglect the potential benefits of joint learning between these tasks. This\npaper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework\nthat simultaneously recognizes driver behavior (e.g., looking around, talking),\ndriver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking,\nturning), and traffic context (e.g., traffic jam, traffic smooth). A key\nchallenge is avoiding negative transfer between tasks, which can impair\nlearning performance. To address this, we introduce two key components into the\nframework: one is the multi-axis region attention network to extract global\ncontext-sensitive features, and the other is the dual-branch multimodal\nembedding to learn multimodal embeddings from both task-shared and\ntask-specific features. The former uses a multi-attention mechanism to extract\ntask-relevant features, mitigating negative transfer caused by task-unrelated\nfeatures. The latter employs a dual-branch structure to adaptively adjust\ntask-shared and task-specific parameters, enhancing cross-task knowledge\ntransfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE\ndataset, using a series of ablation studies, and show that it outperforms\nstate-of-the-art methods across all four tasks. The code is available on\nhttps://github.com/Wenzhuo-Liu/MMTL-UniAD."}
{"id": "2504.02725", "pdf": "https://arxiv.org/pdf/2504.02725", "abs": "https://arxiv.org/abs/2504.02725", "authors": ["Kehua Feng", "Keyan Ding", "Jing Yu", "Menghan Li", "Yuhao Wang", "Tong Xu", "Xinda Wang", "Qiang Zhang", "Huajun Chen"], "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization", "categories": ["cs.CL"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency."}
{"id": "2504.02270", "pdf": "https://arxiv.org/pdf/2504.02270", "abs": "https://arxiv.org/abs/2504.02270", "authors": ["Samuel Sze", "Daniele De Martini", "Lars Kunze"], "title": "MinkOcc: Towards real-time label-efficient semantic occupancy prediction", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages", "summary": "Developing 3D semantic occupancy prediction models often relies on dense 3D\nannotations for supervised learning, a process that is both labor and\nresource-intensive, underscoring the need for label-efficient or even\nlabel-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D\nsemantic occupancy prediction framework for cameras and LiDARs that proposes a\ntwo-step semi-supervised training procedure. Here, a small dataset of\nexplicitly 3D annotations warm-starts the training process; then, the\nsupervision is continued by simpler-to-annotate accumulated LiDAR sweeps and\nimages -- semantically labelled through vision foundational models. MinkOcc\neffectively utilizes these sensor-rich supervisory cues and reduces reliance on\nmanual labeling by 90\\% while maintaining competitive accuracy. In addition,\nthe proposed model incorporates information from LiDAR and camera data through\nearly fusion and leverages sparse convolution networks for real-time\nprediction. With its efficiency in both supervision and computation, we aim to\nextend MinkOcc beyond curated datasets, enabling broader real-world deployment\nof 3D semantic occupancy prediction in autonomous driving."}
{"id": "2504.02732", "pdf": "https://arxiv.org/pdf/2504.02732", "abs": "https://arxiv.org/abs/2504.02732", "authors": ["Federico Barbero", "Álvaro Arroyo", "Xiangming Gu", "Christos Perivolaropoulos", "Michael Bronstein", "Petar Veličkovi ć", "Razvan Pascanu"], "title": "Why do LLMs attend to the first token?", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."}
{"id": "2504.02272", "pdf": "https://arxiv.org/pdf/2504.02272", "abs": "https://arxiv.org/abs/2504.02272", "authors": ["Shaocong Long", "Qianyu Zhou", "Xiangtai Li", "Chenhao Ying", "Yunhai Tong", "Lizhuang Ma", "Yuan Luo", "Dacheng Tao"], "title": "Generative Classifier for Domain Generalization", "categories": ["cs.CV"], "comment": "Code will be available at https://github.com/longshaocong/GCDG", "summary": "Domain generalization (DG) aims to improve the generalizability of computer\nvision models toward distribution shifts. The mainstream DG methods focus on\nlearning domain invariance, however, such methods overlook the potential\ninherent in domain-specific information. While the prevailing practice of\ndiscriminative linear classifier has been tailored to domain-invariant\nfeatures, it struggles when confronted with diverse domain-specific\ninformation, e.g., intra-class shifts, that exhibits multi-modality. To address\nthese issues, we explore the theoretical implications of relying on domain\ninvariance, revealing the crucial role of domain-specific information in\nmitigating the target risk for DG. Drawing from these insights, we propose\nGenerative Classifier-driven Domain Generalization (GCDG), introducing a\ngenerative paradigm for the DG classifier based on Gaussian Mixture Models\n(GMMs) for each class across domains. GCDG consists of three key modules:\nHeterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB),\nand Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the\nfeature distributions and thereby capture valuable domain-specific information\nvia GMMs. SCB identifies the neural units containing spurious correlations and\nperturbs them, mitigating the risk of HLC learning spurious patterns.\nMeanwhile, DCB ensures a balanced contribution of components in HLC, preventing\nthe underestimation or neglect of critical components. In this way, GCDG excels\nin capturing the nuances of domain-specific information characterized by\ndiverse distributions. GCDG demonstrates the potential to reduce the target\nrisk and encourage flat minima, improving the generalizability. Extensive\nexperiments show GCDG's comparable performance on five DG benchmarks and one\nface anti-spoofing dataset, seamlessly integrating into existing DG methods\nwith consistent improvements."}
{"id": "2504.02733", "pdf": "https://arxiv.org/pdf/2504.02733", "abs": "https://arxiv.org/abs/2504.02733", "authors": ["Aryan Agrawal", "Lisa Alazraki", "Shahin Honarvar", "Marek Rei"], "title": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study", "categories": ["cs.CL"], "comment": "Building Trust Workshop, ICLR 2025", "summary": "Large Language Models (LLMs) are highly vulnerable to input perturbations, as\neven a small prompt change may result in a substantially different output.\nExisting methods to enhance LLM robustness are primarily focused on perturbed\ndata samples, whereas improving resiliency to perturbations of task-level\ninstructions has remained relatively underexplored. In this work, we focus on\ncharacter- and word-level edits of task-specific instructions, which\nsubstantially degrade downstream performance. We experiment with a variety of\ntechniques to enhance the robustness of LLMs, including self-denoising and\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\nrole-oriented). We find that, on average, self-denoising -- whether performed\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\nperformance gains than alternative strategies, including more complex baselines\nsuch as ensembling and supervised methods."}
{"id": "2504.02277", "pdf": "https://arxiv.org/pdf/2504.02277", "abs": "https://arxiv.org/abs/2504.02277", "authors": ["Amit Rand", "Hadi Ibrahim"], "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 4 figures, 5 tables. For supplementary material and code,\n  see https://github.com/Hadi-M-Ibrahim/Beyond-Conventional-Transformers/", "summary": "Medical imaging, particularly X-ray analysis, often involves detecting\nmultiple conditions simultaneously within a single scan, making multi-label\nclassification crucial for real-world clinical applications. We present the\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\nspecifically to address the unique challenges of X-ray abnormality detection.\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\nintegrating a specialized module that efficiently captures both detailed local\ninformation and broader global context. To the best of our knowledge, this is\nthe first work to propose a task-specific attention mechanism for diagnosing\nchest X-rays, as well as to attempt multi-label classification using an\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\nthe EfficientViT architecture and employing knowledge distillation, our\nproposed model significantly improves performance on the CheXpert dataset, a\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\napproach achieves an area under the curve (AUC) of 0.85, an absolute\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\nto a substantial approximate 233% relative improvement over random guessing\n(AUC = 0.5)."}
{"id": "2504.02768", "pdf": "https://arxiv.org/pdf/2504.02768", "abs": "https://arxiv.org/abs/2504.02768", "authors": ["Jaap Jumelet", "Leonie Weissweiler", "Arianna Bisazza"], "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages."}
{"id": "2504.02279", "pdf": "https://arxiv.org/pdf/2504.02279", "abs": "https://arxiv.org/abs/2504.02279", "authors": ["Trung Thanh Nguyen", "Yasutomo Kawanishi", "Vijay John", "Takahiro Komamizu", "Ichiro Ide"], "title": "MultiTSF: Transformer-based Sensor Fusion for Human-Centric Multi-view and Multi-modal Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Action recognition from multi-modal and multi-view observations holds\nsignificant potential for applications in surveillance, robotics, and smart\nenvironments. However, existing methods often fall short of addressing\nreal-world challenges such as diverse environmental conditions, strict sensor\nsynchronization, and the need for fine-grained annotations. In this study, we\npropose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF).\nThe proposed method leverages a Transformer-based to dynamically model\ninter-view relationships and capture temporal dependencies across multiple\nviews. Additionally, we introduce a Human Detection Module to generate\npseudo-ground-truth labels, enabling the model to prioritize frames containing\nhuman activity and enhance spatial feature learning. Comprehensive experiments\nconducted on our in-house MultiSensor-Home dataset and the existing MM-Office\ndataset demonstrate that MultiTSF outperforms state-of-the-art methods in both\nvideo sequence-level and frame-level action recognition settings."}
{"id": "2504.02789", "pdf": "https://arxiv.org/pdf/2504.02789", "abs": "https://arxiv.org/abs/2504.02789", "authors": ["Karin de Langis", "Jong Inn Park", "Bin Hu", "Khanh Chi Le", "Andreas Schramm", "Michael C. Mensink", "Andrew Elfenbein", "Dongyeop Kang"], "title": "A Framework for Robust Cognitive Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Emergent cognitive abilities in large language models (LLMs) have been widely\nobserved, but their nature and underlying mechanisms remain poorly understood.\nA growing body of research draws on cognitive science to investigate LLM\ncognition, but standard methodologies and experimen-tal pipelines have not yet\nbeen established. To address this gap we develop CognitivEval, a framework for\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\nparticular emphasis on robustness in response collection. The key features of\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\ngathers both generations and model probability estimates. Our experiments\ndemonstrate that these features lead to more robust experimental outcomes.\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\nillustrating the framework's generalizability across various experimental tasks\nand obtaining a cognitive profile of several state of the art LLMs.\nCognitivEval will be released publicly to foster broader collaboration within\nthe cognitive science community."}
{"id": "2504.02286", "pdf": "https://arxiv.org/pdf/2504.02286", "abs": "https://arxiv.org/abs/2504.02286", "authors": ["Xiaolong Sun", "Le Wang", "Sanping Zhou", "Liushuai Shi", "Kun Xia", "Mengnan Liu", "Yabing Wang", "Gang Hua"], "title": "Moment Quantization for Video Temporal Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Video temporal grounding is a critical video understanding task, which aims\nto localize moments relevant to a language description. The challenge of this\ntask lies in distinguishing relevant and irrelevant moments. Previous methods\nfocused on learning continuous features exhibit weak differentiation between\nforeground and background features. In this paper, we propose a novel\nMoment-Quantization based Video Temporal Grounding method (MQVTG), which\nquantizes the input video into various discrete vectors to enhance the\ndiscrimination between relevant and irrelevant moments. Specifically, MQVTG\nmaintains a learnable moment codebook, where each video moment matches a\ncodeword. Considering the visual diversity, i.e., various visual expressions\nfor the same moment, MQVTG treats moment-codeword matching as a clustering\nprocess without using discrete vectors, avoiding the loss of useful information\nfrom direct hard quantization. Additionally, we employ effective\nprior-initialization and joint-projection strategies to enhance the maintained\nmoment codebook. With its simple implementation, the proposed method can be\nintegrated into existing temporal grounding models as a plug-and-play\ncomponent. Extensive experiments on six popular benchmarks demonstrate the\neffectiveness and generalizability of MQVTG, significantly outperforming\nstate-of-the-art methods. Further qualitative analysis shows that our method\neffectively groups relevant features and separates irrelevant ones, aligning\nwith our goal of enhancing discrimination."}
{"id": "2504.02800", "pdf": "https://arxiv.org/pdf/2504.02800", "abs": "https://arxiv.org/abs/2504.02800", "authors": ["Zhuohan Ge", "Nicole Hu", "Darian Li", "Yubo Wang", "Shihao Qi", "Yuming Xu", "Han Shi", "Jason Zhang"], "title": "A Survey of Large Language Models in Mental Health Disorder Detection on Social Media", "categories": ["cs.CL", "I.2.7; J.3; J.4"], "comment": "13 pages, 4 figures", "summary": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions."}
{"id": "2504.02287", "pdf": "https://arxiv.org/pdf/2504.02287", "abs": "https://arxiv.org/abs/2504.02287", "authors": ["Trung Thanh Nguyen", "Yasutomo Kawanishi", "Vijay John", "Takahiro Komamizu", "Ichiro Ide"], "title": "MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal multi-view action recognition is a rapidly growing field in\ncomputer vision, offering significant potential for applications in\nsurveillance. However, current datasets often fail to address real-world\nchallenges such as wide-area environmental conditions, asynchronous data\nstreams, and the lack of frame-level annotations. Furthermore, existing methods\nface difficulties in effectively modeling inter-view relationships and\nenhancing spatial feature learning. In this study, we propose the Multi-modal\nMulti-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the\nMultiSensor-Home dataset, a novel benchmark designed for comprehensive action\nrecognition in home environments. The MultiSensor-Home dataset features\nuntrimmed videos captured by distributed sensors, providing high-resolution RGB\nand audio data along with detailed multi-view frame-level action labels. The\nproposed MultiTSF method leverages a Transformer-based fusion mechanism to\ndynamically model inter-view relationships. Furthermore, the method also\nintegrates a external human detection module to enhance spatial feature\nlearning. Experiments on MultiSensor-Home and MM-Office datasets demonstrate\nthe superiority of MultiTSF over the state-of-the-art methods. The quantitative\nand qualitative results highlight the effectiveness of the proposed method in\nadvancing real-world multi-modal multi-view action recognition."}
{"id": "2504.02807", "pdf": "https://arxiv.org/pdf/2504.02807", "abs": "https://arxiv.org/abs/2504.02807", "authors": ["Fan Zhou", "Zengzhi Wang", "Nikhil Ranjan", "Zhoujun Cheng", "Liping Tang", "Guowei He", "Zhengzhong Liu", "Eric P. Xing"], "title": "MegaMath: Pushing the Limits of Open Math Corpora", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 15 figures, 22 tables", "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets."}
{"id": "2504.02312", "pdf": "https://arxiv.org/pdf/2504.02312", "abs": "https://arxiv.org/abs/2504.02312", "authors": ["Xiaoda Yang", "Jiayang Xu", "Kaixuan Luan", "Xinyu Zhan", "Hongshun Qiu", "Shijun Shi", "Hao Li", "Shuai Yang", "Li Zhang", "Checheng Yu", "Cewu Lu", "Lixin Yang"], "title": "OmniCam: Unified Multimodal Video Generation via Camera Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Camera control, which achieves diverse visual effects by changing camera\nposition and pose, has attracted widespread attention. However, existing\nmethods face challenges such as complex interaction and limited control\ncapabilities. To address these issues, we present OmniCam, a unified multimodal\ncamera control framework. Leveraging large language models and video diffusion\nmodels, OmniCam generates spatio-temporally consistent videos. It supports\nvarious combinations of input modalities: the user can provide text or video\nwith expected trajectory as camera path guidance, and image or video as content\nreference, enabling precise control over camera motion. To facilitate the\ntraining of OmniCam, we introduce the OmniTr dataset, which contains a large\ncollection of high-quality long-sequence trajectories, videos, and\ncorresponding descriptions. Experimental results demonstrate that our model\nachieves state-of-the-art performance in high-quality camera-controlled video\ngeneration across various metrics."}
{"id": "2504.02810", "pdf": "https://arxiv.org/pdf/2504.02810", "abs": "https://arxiv.org/abs/2504.02810", "authors": ["Haowei Lin", "Xiangyu Wang", "Ruilin Yan", "Baizhou Huang", "Haotian Ye", "Jianhua Zhu", "Zihao Wang", "James Zou", "Jianzhu Ma", "Yitao Liang"], "title": "Generative Evaluation of Complex Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."}
{"id": "2504.02316", "pdf": "https://arxiv.org/pdf/2504.02316", "abs": "https://arxiv.org/abs/2504.02316", "authors": ["Yuan Zhou", "Shilong Jin", "Litao Hua", "Wanjun Lv", "Haoran Duan", "Jungong Han"], "title": "ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 11 figures, 3 tables", "summary": "Recent advances in zero-shot text-to-3D generation have revolutionized 3D\ncontent creation by enabling direct synthesis from textual descriptions. While\nstate-of-the-art methods leverage 3D Gaussian Splatting with score distillation\nto enhance multi-view rendering through pre-trained text-to-image (T2I) models,\nthey suffer from inherent view biases in T2I priors. These biases lead to\ninconsistent 3D generation, particularly manifesting as the multi-face Janus\nproblem, where objects exhibit conflicting features across views. To address\nthis fundamental challenge, we propose ConsDreamer, a novel framework that\nmitigates view bias by refining both the conditional and unconditional terms in\nthe score distillation process: (1) a View Disentanglement Module (VDM) that\neliminates viewpoint biases in conditional prompts by decoupling irrelevant\nview components and injecting precise camera parameters; and (2) a\nsimilarity-based partial order loss that enforces geometric consistency in the\nunconditional term by aligning cosine similarities with azimuth relationships.\nExtensive experiments demonstrate that ConsDreamer effectively mitigates the\nmulti-face Janus problem in text-to-3D generation, outperforming existing\nmethods in both visual quality and consistency."}
{"id": "2504.01963", "pdf": "https://arxiv.org/pdf/2504.01963", "abs": "https://arxiv.org/abs/2504.01963", "authors": ["R. M. Aratchige", "W. M. K. S. Ilmini"], "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "This survey investigates foundational technologies essential for developing\neffective Large Language Model (LLM)-based multi-agent systems. Aiming to\nanswer how best to optimize these systems for collaborative, dynamic\nenvironments, we focus on four critical areas: Architecture, Memory, Planning,\nand Technologies/Frameworks. By analyzing recent advancements and their\nlimitations - such as scalability, real-time response challenges, and agent\ncoordination constraints, we provide a detailed view of the technological\nlandscape. Frameworks like the Mixture of Agents architecture and the ReAct\nplanning model exemplify current innovations, showcasing improvements in role\nassignment and decision-making. This review synthesizes key strengths and\npersistent challenges, offering practical recommendations to enhance system\nscalability, agent collaboration, and adaptability. Our findings provide a\nroadmap for future research, supporting the creation of robust, efficient\nmulti-agent systems that advance both individual agent performance and\ncollective system resilience."}
{"id": "2504.02318", "pdf": "https://arxiv.org/pdf/2504.02318", "abs": "https://arxiv.org/abs/2504.02318", "authors": ["Samuel Clarke", "Suzannah Wistreich", "Yanjie Ze", "Jiajun Wu"], "title": "X-Capture: An Open-Source Portable Device for Multi-Sensory Learning", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://xcapture.github.io/", "summary": "Understanding objects through multiple sensory modalities is fundamental to\nhuman perception, enabling cross-sensory integration and richer comprehension.\nFor AI and robotic systems to replicate this ability, access to diverse,\nhigh-quality multi-sensory data is critical. Existing datasets are often\nlimited by their focus on controlled environments, simulated objects, or\nrestricted modality pairings. We introduce X-Capture, an open-source, portable,\nand cost-effective device for real-world multi-sensory data collection, capable\nof capturing correlated RGBD images, tactile readings, and impact audio. With a\nbuild cost under $1,000, X-Capture democratizes the creation of multi-sensory\ndatasets, requiring only consumer-grade tools for assembly. Using X-Capture, we\ncurate a sample dataset of 3,000 total points on 500 everyday objects from\ndiverse, real-world environments, offering both richness and variety. Our\nexperiments demonstrate the value of both the quantity and the sensory breadth\nof our data for both pretraining and fine-tuning multi-modal representations\nfor object-centric tasks such as cross-sensory retrieval and reconstruction.\nX-Capture lays the groundwork for advancing human-like sensory representations\nin AI, emphasizing scalability, accessibility, and real-world applicability."}
{"id": "2504.02009", "pdf": "https://arxiv.org/pdf/2504.02009", "abs": "https://arxiv.org/abs/2504.02009", "authors": ["Zhonghang Li", "Lianghao Xia", "Xubin Ren", "Jiabin Tang", "Tianyi Chen", "Yong Xu", "Chao Huang"], "title": "Urban Computing in the Era of Large Language Models", "categories": ["cs.CY", "cs.CL"], "comment": "36 pages", "summary": "Urban computing has emerged as a multidisciplinary field that harnesses\ndata-driven technologies to address challenges and improve urban living.\nTraditional approaches, while beneficial, often face challenges with\ngeneralization, scalability, and contextual understanding. The advent of Large\nLanguage Models (LLMs) offers transformative potential in this domain. This\nsurvey explores the intersection of LLMs and urban computing, emphasizing the\nimpact of LLMs in processing and analyzing urban data, enhancing\ndecision-making, and fostering citizen engagement. We provide a concise\noverview of the evolution and core technologies of LLMs. Additionally, we\nsurvey their applications across key urban domains, such as transportation,\npublic safety, and environmental monitoring, summarizing essential tasks and\nprior works in various urban contexts, while highlighting LLMs' functional\nroles and implementation patterns. Building on this, we propose potential\nLLM-based solutions to address unresolved challenges. To facilitate in-depth\nresearch, we compile a list of available datasets and tools applicable to\ndiverse urban scenarios. Finally, we discuss the limitations of current\napproaches and outline future directions for advancing LLMs in urban computing."}
{"id": "2504.02328", "pdf": "https://arxiv.org/pdf/2504.02328", "abs": "https://arxiv.org/abs/2504.02328", "authors": ["Congpei Qiu", "Yanhao Wu", "Wei Ke", "Xiuxiu Bai", "Tong Zhang"], "title": "Refining CLIP's Spatial Awareness: A Visual-Centric Perspective", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) excels in global alignment\nwith language but exhibits limited sensitivity to spatial information, leading\nto strong performance in zero-shot classification tasks but underperformance in\ntasks requiring precise spatial understanding. Recent approaches have\nintroduced Region-Language Alignment (RLA) to enhance CLIP's performance in\ndense multimodal tasks by aligning regional visual representations with\ncorresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA\nsuffer from notable loss in spatial awareness, which is crucial for dense\nprediction tasks. To address this, we propose the Spatial Correlation\nDistillation (SCD) framework, which preserves CLIP's inherent spatial structure\nand mitigates the above degradation. To further enhance spatial correlations,\nwe introduce a lightweight Refiner that extracts refined correlations directly\nfrom CLIP before feeding them into SCD, based on an intriguing finding that\nCLIP naturally captures high-quality dense features. Together, these components\nform a robust distillation framework that enables CLIP ViTs to integrate both\nvisual-language and visual-centric improvements, achieving state-of-the-art\nresults across various open-vocabulary dense prediction benchmarks."}
{"id": "2504.02051", "pdf": "https://arxiv.org/pdf/2504.02051", "abs": "https://arxiv.org/abs/2504.02051", "authors": ["Alfonso Amayuelas", "Jingbo Yang", "Saaket Agashe", "Ashwin Nagarajan", "Antonis Antoniades", "Xin Eric Wang", "William Wang"], "title": "Self-Resource Allocation in Multi-Agent LLM Systems", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of LLMs as agents, there is a growing interest in\nconnecting multiple agents into multi-agent systems to solve tasks\nconcurrently, focusing on their role in task assignment and coordination. This\npaper explores how LLMs can effectively allocate computational tasks among\nmultiple agents, considering factors such as cost, efficiency, and performance.\nIn this work, we address key questions, including the effectiveness of LLMs as\norchestrators and planners, comparing their effectiveness in task assignment\nand coordination. Our experiments demonstrate that LLMs can achieve high\nvalidity and accuracy in resource allocation tasks. We find that the planner\nmethod outperforms the orchestrator method in handling concurrent actions,\nresulting in improved efficiency and better utilization of agents.\nAdditionally, we show that providing explicit information about worker\ncapabilities enhances the allocation strategies of planners, particularly when\ndealing with suboptimal workers."}
{"id": "2504.02335", "pdf": "https://arxiv.org/pdf/2504.02335", "abs": "https://arxiv.org/abs/2504.02335", "authors": ["Seif Mzoughi", "Mohamed Elshafeia", "Foutse Khomh"], "title": "Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing", "categories": ["cs.CV", "cs.SE"], "comment": null, "summary": "Image segmentation is critical for applications such as medical imaging,\naugmented reality, and video surveillance. However, segmentation models often\nlack robustness, making them vulnerable to adversarial perturbations from\nsubtle image distortions. In this work, we propose SegRMT, a metamorphic\ntesting approach that leverages genetic algorithms (GA) to optimize sequences\nof spatial and spectral transformations while preserving image fidelity via a\npredefined PSNR threshold. Using the Cityscapes dataset, our method generates\nadversarial examples that effectively challenge the DeepLabV3 segmentation\nmodel. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection\nover Union (mIoU) to 6.4%, outperforming other adversarial baselines that\ndecrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial\ntraining, SegRMT boosts model performance, achieving mIoU improvements up to\n73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to\n53.8%, compared to only 2%-10% for other methods. These findings demonstrate\nthat SegRMT not only simulates realistic image distortions but also enhances\nthe robustness of segmentation models, making it a valuable tool for ensuring\nreliable performance in safety-critical applications."}
{"id": "2504.02107", "pdf": "https://arxiv.org/pdf/2504.02107", "abs": "https://arxiv.org/abs/2504.02107", "authors": ["Jeffrey Li", "Mohammadreza Armandpour", "Iman Mirzadeh", "Sachin Mehta", "Vaishaal Shankar", "Raviteja Vemulapalli", "Samy Bengio", "Oncel Tuzel", "Mehrdad Farajtabar", "Hadi Pouransari", "Fartash Faghri"], "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": "Code available at: https://github.com/apple/ml-tic-lm", "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains."}
{"id": "2504.02337", "pdf": "https://arxiv.org/pdf/2504.02337", "abs": "https://arxiv.org/abs/2504.02337", "authors": ["Ming-Jia Yang", "Yu-Xiao Guo", "Yang Liu", "Bin Zhou", "Xin Tong"], "title": "LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic, room-level indoor scenes with semantically plausible\nand detailed appearances from in-the-wild images is crucial for various\napplications in VR, AR, and robotics. The success of NeRF-based generative\nmethods indicates a promising direction to address this challenge. However,\nunlike their success at the object level, existing scene-level generative\nmethods require additional information, such as multiple views, depth images,\nor semantic guidance, rather than relying solely on RGB images. This is because\nNeRF-based methods necessitate prior knowledge of camera poses, which is\nchallenging to approximate for indoor scenes due to the complexity of defining\nalignment and the difficulty of globally estimating poses from a single image,\ngiven the unseen parts behind the camera. To address this challenge, we\nredefine global poses within the framework of Local-Pose-Alignment (LPA) -- an\nanchor-based multi-local-coordinate system that uses a selected number of\nanchors as the roots of these coordinates. Building on this foundation, we\nintroduce LPA-GAN, a novel NeRF-based generative approach that incorporates\nspecific modifications to estimate the priors of camera poses under LPA. It\nalso co-optimizes the pose predictor and scene generation processes. Our\nablation study and comparisons with straightforward extensions of NeRF-based\nobject generative methods demonstrate the effectiveness of our approach.\nFurthermore, visual comparisons with other techniques reveal that our method\nachieves superior view-to-view consistency and semantic normality."}
{"id": "2504.02111", "pdf": "https://arxiv.org/pdf/2504.02111", "abs": "https://arxiv.org/abs/2504.02111", "authors": ["Giannis Chatziveroglou", "Richard Yun", "Maura Kelleher"], "title": "Exploring LLM Reasoning Through Controlled Prompt Variations", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study investigates the reasoning robustness of large language models\n(LLMs) on mathematical problem-solving tasks under systematically introduced\ninput perturbations. Using the GSM8K dataset as a controlled testbed, we\nevaluate how well state-of-the-art models maintain logical consistency and\ncorrectness when confronted with four categories of prompt perturbations:\nirrelevant context, pathological instructions, factually relevant but\nnon-essential context, and a combination of the latter two. Our experiments,\nconducted on thirteen open-source and closed-source LLMs, reveal that\nintroducing irrelevant context within the model's context window significantly\ndegrades performance, suggesting that distinguishing essential from extraneous\ndetails remains a pressing challenge. Surprisingly, performance regressions are\nrelatively insensitive to the complexity of the reasoning task, as measured by\nthe number of steps required, and are not strictly correlated with model size.\nMoreover, we observe that certain perturbations inadvertently trigger\nchain-of-thought-like reasoning behaviors, even without explicit prompting. Our\nfindings highlight critical vulnerabilities in current LLMs and underscore the\nneed for improved robustness against noisy, misleading, and contextually dense\ninputs, paving the way for more resilient and reliable reasoning in real-world\napplications."}
{"id": "2504.02345", "pdf": "https://arxiv.org/pdf/2504.02345", "abs": "https://arxiv.org/abs/2504.02345", "authors": ["Masakazu Yoshimura", "Junji Otsuka", "Radu Berdan", "Takeshi Ohashi"], "title": "SemiISP/SemiIE: Semi-Supervised Image Signal Processor and Image Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW", "categories": ["cs.CV"], "comment": null, "summary": "DNN-based methods have been successful in Image Signal Processor (ISP) and\nimage enhancement (IE) tasks. However, the cost of creating training data for\nthese tasks is considerably higher than for other tasks, making it difficult to\nprepare large-scale datasets. Also, creating personalized ISP and IE with\nminimal training data can lead to new value streams since preferred image\nquality varies depending on the person and use case. While semi-supervised\nlearning could be a potential solution in such cases, it has rarely been\nutilized for these tasks. In this paper, we realize semi-supervised learning\nfor ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method.\nAlthough existing sRGB-to-RAW methods can generate pseudo-RAW image datasets\nthat improve the accuracy of RAW-based high-level computer vision tasks such as\nobject detection, their quality is not sufficient for ISP and IE tasks that\nrequire precise image quality definition. Therefore, we also propose a\nsRGB-to-RAW method that can improve the image quality of these tasks. The\nproposed semi-supervised learning with the proposed sRGB-to-RAW method\nsuccessfully improves the image quality of various models on various datasets."}
{"id": "2504.02128", "pdf": "https://arxiv.org/pdf/2504.02128", "abs": "https://arxiv.org/abs/2504.02128", "authors": ["Apurba Pokharel", "Ram Dantu", "Shakila Zaman", "Sirisha Talapuru", "Vinh Quach"], "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figure, 3 tables", "summary": "Blockchain consensus mechanisms have relied on algorithms such as\nProof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality\nand integrity. However, these approaches struggle with adaptability for\ndecision-making where the opinions of each matter rather than reaching an\nagreement based on honest majority or weighted consensus. This paper introduces\na novel deliberation-based consensus mechanism where Large Language Models\n(LLMs) act as rational agents engaging in structured discussions to reach a\nunanimous consensus. By leveraging graded consensus and a multi-round\ndeliberation process, our approach ensures both unanimous consensus for\ndefinitive problems and graded confidence for prioritized decisions and\npolicies. We provide a formalization of our system and use it to show that the\nproperties of blockchains: consistency, agreement, liveness, and determinism\nare maintained. Moreover, experimental results demonstrate our system's\nfeasibility, showcasing how our deliberation method's convergence, block\nproperties, and accuracy enable decision-making on blockchain networks. We also\naddress key challenges with this novel approach such as degeneration of\nthoughts, hallucinations, malicious models and nodes, resource consumption, and\nscalability."}
{"id": "2504.02351", "pdf": "https://arxiv.org/pdf/2504.02351", "abs": "https://arxiv.org/abs/2504.02351", "authors": ["Chengxi Zeng", "Yuxuan Jiang", "Fan Zhang", "Alberto Gambaruto", "Tilo Burghardt"], "title": "Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation."}
{"id": "2504.02144", "pdf": "https://arxiv.org/pdf/2504.02144", "abs": "https://arxiv.org/abs/2504.02144", "authors": ["Oam Patel", "Jason Wang", "Nikhil Shivakumar Nayak", "Suraj Srinivas", "Himabindu Lakkaraju"], "title": "Towards Interpretable Soft Prompts", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML", "68T50", "I.2.0; G.3"], "comment": "9 pages, 8 figures", "summary": "Soft prompts have been popularized as a cheap and easy way to improve\ntask-specific LLM performance beyond few-shot prompts. Despite their origin as\nan automated prompting method, however, soft prompts and other trainable\nprompts remain a black-box method with no immediately interpretable connections\nto prompting. We create a novel theoretical framework for evaluating the\ninterpretability of trainable prompts based on two desiderata: faithfulness and\nscrutability. We find that existing methods do not naturally satisfy our\nproposed interpretability criterion. Instead, our framework inspires a new\ndirection of trainable prompting methods that explicitly optimizes for\ninterpretability. To this end, we formulate and test new\ninterpretability-oriented objective functions for two state-of-the-art prompt\ntuners: Hard Prompts Made Easy (PEZ) and RLPrompt. Our experiments with GPT-2\ndemonstrate a fundamental trade-off between interpretability and the\ntask-performance of the trainable prompt, explicating the hardness of the soft\nprompt interpretability problem and revealing odd behavior that arises when one\noptimizes for an interpretability proxy."}
{"id": "2504.02356", "pdf": "https://arxiv.org/pdf/2504.02356", "abs": "https://arxiv.org/abs/2504.02356", "authors": ["Janghyun Kim", "Minseong Kweon", "Jinsun Park", "Ukcheol Shin"], "title": "All-day Depth Completion via Thermal-LiDAR Fusion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Depth completion, which estimates dense depth from sparse LiDAR and RGB\nimages, has demonstrated outstanding performance in well-lit conditions.\nHowever, due to the limitations of RGB sensors, existing methods often struggle\nto achieve reliable performance in harsh environments, such as heavy rain and\nlow-light conditions. Furthermore, we observe that ground truth depth maps\noften suffer from large missing measurements in adverse weather conditions such\nas heavy rain, leading to insufficient supervision. In contrast, thermal\ncameras are known for providing clear and reliable visibility in such\nconditions, yet research on thermal-LiDAR depth completion remains\nunderexplored. Moreover, the characteristics of thermal images, such as\nblurriness, low contrast, and noise, bring unclear depth boundary problems. To\naddress these challenges, we first evaluate the feasibility and robustness of\nthermal-LiDAR depth completion across diverse lighting (eg., well-lit,\nlow-light), weather (eg., clear-sky, rainy), and environment (eg., indoor,\noutdoor) conditions, by conducting extensive benchmarks on the MS$^2$ and ViViD\ndatasets. In addition, we propose a framework that utilizes COntrastive\nlearning and Pseudo-Supervision (COPS) to enhance depth boundary clarity and\nimprove completion accuracy by leveraging a depth foundation model in two key\nways. First, COPS enforces a depth-aware contrastive loss between different\ndepth points by mining positive and negative samples using a monocular depth\nfoundation model to sharpen depth boundaries. Second, it mitigates the issue of\nincomplete supervision from ground truth depth maps by leveraging foundation\nmodel predictions as dense depth priors. We also provide in-depth analyses of\nthe key challenges in thermal-LiDAR depth completion to aid in understanding\nthe task and encourage future research."}
{"id": "2504.02163", "pdf": "https://arxiv.org/pdf/2504.02163", "abs": "https://arxiv.org/abs/2504.02163", "authors": ["Lewis Matheson Creed"], "title": "Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "50 Pages, 10 figures, Honours Thesis", "summary": "The limited availability of training data for low-resource languages makes\napplying machine learning techniques challenging. Ancient Egyptian is one such\nlanguage with few resources. However, innovative applications of data\naugmentation methods, such as Neural Style Transfer, could overcome these\nbarriers. This paper presents a novel method for generating datasets of ancient\nEgyptian hieroglyphs by applying NST to a digital typeface. Experimental\nresults found that image classification models trained on NST-generated\nexamples and photographs demonstrate equal performance and transferability to\nreal unseen images of hieroglyphs."}
{"id": "2504.02362", "pdf": "https://arxiv.org/pdf/2504.02362", "abs": "https://arxiv.org/abs/2504.02362", "authors": ["Haodian Wang", "Long Peng", "Yuejin Sun", "Zengyu Wan", "Yang Wang", "Yang Cao"], "title": "Brightness Perceiving for Recursive Low-Light Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the wide dynamic range in real low-light scenes, there will be large\ndifferences in the degree of contrast degradation and detail blurring of\ncaptured images, making it difficult for existing end-to-end methods to enhance\nlow-light images to normal exposure. To address the above issue, we decompose\nlow-light image enhancement into a recursive enhancement task and propose a\nbrightness-perceiving-based recursive enhancement framework for high dynamic\nrange low-light image enhancement. Specifically, our recursive enhancement\nframework consists of two parallel sub-networks: Adaptive Contrast and Texture\nenhancement network (ACT-Net) and Brightness Perception network (BP-Net). The\nACT-Net is proposed to adaptively enhance image contrast and details under the\nguidance of the brightness adjustment branch and gradient adjustment branch,\nwhich are proposed to perceive the degradation degree of contrast and details\nin low-light images. To adaptively enhance images captured under different\nbrightness levels, BP-Net is proposed to control the recursive enhancement\ntimes of ACT-Net by exploring the image brightness distribution properties.\nFinally, in order to coordinate ACT-Net and BP-Net, we design a novel\nunsupervised training strategy to facilitate the training procedure. To further\nvalidate the effectiveness of the proposed method, we construct a new dataset\nwith a broader brightness distribution by mixing three low-light datasets.\nCompared with eleven existing representative methods, the proposed method\nachieves new SOTA performance on six reference and no reference metrics.\nSpecifically, the proposed method improves the PSNR by 0.9 dB compared to the\nexisting SOTA method."}
{"id": "2504.02234", "pdf": "https://arxiv.org/pdf/2504.02234", "abs": "https://arxiv.org/abs/2504.02234", "authors": ["Jacy Reese Anthis", "Ryan Liu", "Sean M. Richardson", "Austin C. Kozlowski", "Bernard Koch", "James Evans", "Erik Brynjolfsson", "Michael Bernstein"], "title": "LLM Social Simulations Are a Promising Research Method", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted these methods. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a literature\nsurvey of empirical comparisons between LLMs and human research subjects,\ncommentaries on the topic, and related work. We identify promising directions\nwith prompting, fine-tuning, and complementary methods. We believe that LLM\nsocial simulations can already be used for exploratory research, such as pilot\nexperiments for psychology, economics, sociology, and marketing. More\nwidespread use may soon be possible with rapidly advancing LLM capabilities,\nand researchers should prioritize developing conceptual models and evaluations\nthat can be iteratively deployed and refined at pace with ongoing AI advances."}
{"id": "2504.02386", "pdf": "https://arxiv.org/pdf/2504.02386", "abs": "https://arxiv.org/abs/2504.02386", "authors": ["Kim Sung-Bin", "Jeongsoo Choi", "Puyuan Peng", "Joon Son Chung", "Tae-Hyun Oh", "David Harwath"], "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models", "categories": ["cs.CV", "eess.AS"], "comment": "https://voicecraft-dub.github.io/", "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that\nsynthesizes high-quality speech from text and facial cues. This task has broad\napplications in filmmaking, multimedia creation, and assisting voice-impaired\nindividuals. Building on the success of Neural Codec Language Models (NCLMs)\nfor speech synthesis, our method extends their capabilities by incorporating\nvideo features, ensuring that synthesized speech is time-synchronized and\nexpressively aligned with facial movements while preserving natural prosody. To\ninject visual cues, we design adapters to align facial features with the NCLM\ntoken space and introduce audio-visual fusion layers to merge audio-visual\ninformation within the NCLM framework. Additionally, we curate CelebV-Dub, a\nnew dataset of expressive, real-world videos specifically designed for\nautomated video dubbing. Extensive experiments show that our model achieves\nhigh-quality, intelligible, and natural speech synthesis with accurate lip\nsynchronization, outperforming existing methods in human perception and\nperforming favorably in objective evaluations. We also adapt VoiceCraft-Dub for\nthe video-to-speech task, demonstrating its versatility for various\napplications."}
{"id": "2504.02268", "pdf": "https://arxiv.org/pdf/2504.02268", "abs": "https://arxiv.org/abs/2504.02268", "authors": ["Waris Gill", "Justin Cechmanek", "Tyler Hutcherson", "Srijith Rajamohan", "Jen Agarwal", "Muhammad Ali Gulzar", "Manvinder Singh", "Benoit Dion"], "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data", "categories": ["cs.LG", "cs.CL"], "comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences", "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."}
{"id": "2504.02391", "pdf": "https://arxiv.org/pdf/2504.02391", "abs": "https://arxiv.org/abs/2504.02391", "authors": ["Laibin Chang", "Yunke Wang", "JiaXing Huang", "Longxiang Deng", "Bo Du", "Chang Xu"], "title": "Marine Saliency Segmenter: Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Marine Saliency Segmentation (MSS) plays a pivotal role in various\nvision-based marine exploration tasks. However, existing marine segmentation\ntechniques face the dilemma of object mislocalization and imprecise boundaries\ndue to the complex underwater environment. Meanwhile, despite the impressive\nperformance of diffusion models in visual segmentation, there remains potential\nto further leverage contextual semantics to enhance feature learning of\nregion-level salient objects, thereby improving segmentation outcomes. Building\non this insight, we propose DiffMSS, a novel marine saliency segmenter based on\nthe diffusion model, which utilizes semantic knowledge distillation to guide\nthe segmentation of marine salient objects. Specifically, we design a\nregion-word similarity matching mechanism to identify salient terms at the word\nlevel from the text descriptions. These high-level semantic features guide the\nconditional feature learning network in generating salient and accurate\ndiffusion conditions with semantic knowledge distillation. To further refine\nthe segmentation of fine-grained structures in unique marine organisms, we\ndevelop the dedicated consensus deterministic sampling to suppress\noverconfident missegmentations. Comprehensive experiments demonstrate the\nsuperior performance of DiffMSS over state-of-the-art methods in both\nquantitative and qualitative evaluations."}
{"id": "2504.02507", "pdf": "https://arxiv.org/pdf/2504.02507", "abs": "https://arxiv.org/abs/2504.02507", "authors": ["Abhay Kumar", "Louis Owen", "Nilabhra Roy Chowdhury", "Fabian Güra"], "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip."}
{"id": "2504.02397", "pdf": "https://arxiv.org/pdf/2504.02397", "abs": "https://arxiv.org/abs/2504.02397", "authors": ["Boseung Jeong", "Jicheol Park", "Sungyeon Kim", "Suha Kwak"], "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Video-text retrieval, the task of retrieving videos based on a textual query\nor vice versa, is of paramount importance for video understanding and\nmultimodal information retrieval. Recent methods in this area rely primarily on\nvisual and textual features and often ignore audio, although it helps enhance\noverall comprehension of video content. Moreover, traditional models that\nincorporate audio blindly utilize the audio input regardless of whether it is\nuseful or not, resulting in suboptimal video representation. To address these\nlimitations, we propose a novel video-text retrieval framework, Audio-guided\nVIdeo representation learning with GATEd attention (AVIGATE), that effectively\nleverages audio cues through a gated attention mechanism that selectively\nfilters out uninformative audio signals. In addition, we propose an adaptive\nmargin-based contrastive loss to deal with the inherently unclear\npositive-negative relationship between video and text, which facilitates\nlearning better video-text alignment. Our extensive experiments demonstrate\nthat AVIGATE achieves state-of-the-art performance on all the public\nbenchmarks."}
{"id": "2504.02577", "pdf": "https://arxiv.org/pdf/2504.02577", "abs": "https://arxiv.org/abs/2504.02577", "authors": ["Erik Arakelyan"], "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": "PhD thesis", "summary": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities."}
{"id": "2504.02416", "pdf": "https://arxiv.org/pdf/2504.02416", "abs": "https://arxiv.org/abs/2504.02416", "authors": ["Peifu Liu", "Huiyan Bai", "Tingfa Xu", "Jihui Wang", "Huan Chen", "Jianan Li"], "title": "Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline", "categories": ["cs.CV"], "comment": "Accepted by TGRS 2025", "summary": "The objective of hyperspectral remote sensing image salient object detection\n(HRSI-SOD) is to identify objects or regions that exhibit distinct spectrum\ncontrasts with the background. This area holds significant promise for\npractical applications; however, progress has been limited by a notable\nscarcity of dedicated datasets and methodologies. To bridge this gap and\nstimulate further research, we introduce the first HRSI-SOD dataset, termed\nHRSSD, which includes 704 hyperspectral images and 5327 pixel-level annotated\nsalient objects. The HRSSD dataset poses substantial challenges for salient\nobject detection algorithms due to large scale variation, diverse\nforeground-background relations, and multi-salient objects. Additionally, we\npropose an innovative and efficient baseline model for HRSI-SOD, termed the\nDeep Spectral Saliency Network (DSSN). The core of DSSN is the Cross-level\nSaliency Assessment Block, which performs pixel-wise attention and evaluates\nthe contributions of multi-scale similarity maps at each spatial location,\neffectively reducing erroneous responses in cluttered regions and emphasizes\nsalient regions across scales. Additionally, the High-resolution Fusion Module\ncombines bottom-up fusion strategy and learned spatial upsampling to leverage\nthe strengths of multi-scale saliency maps, ensuring accurate localization of\nsmall objects. Experiments on the HRSSD dataset robustly validate the\nsuperiority of DSSN, underscoring the critical need for specialized datasets\nand methodologies in this domain. Further evaluations on the HSOD-BIT and\nHS-SOD datasets demonstrate the generalizability of the proposed method. The\ndataset and source code are publicly available at\nhttps://github.com/laprf/HRSSD."}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research."}
{"id": "2504.02417", "pdf": "https://arxiv.org/pdf/2504.02417", "abs": "https://arxiv.org/abs/2504.02417", "authors": ["Lili Liang", "Guanglu Sun"], "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method."}
{"id": "2504.02605", "pdf": "https://arxiv.org/pdf/2504.02605", "abs": "https://arxiv.org/abs/2504.02605", "authors": ["Daoguang Zan", "Zhirong Huang", "Wei Liu", "Hanwu Chen", "Linhao Zhang", "Shulin Xin", "Lu Chen", "Qi Liu", "Xiaojian Zhong", "Aoyan Li", "Siyao Liu", "Yongsheng Xiao", "Liangqiang Chen", "Yuyu Zhang", "Jing Su", "Tianyu Liu", "Rui Long", "Kai Shen", "Liang Xiang"], "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI."}
{"id": "2504.02433", "pdf": "https://arxiv.org/pdf/2504.02433", "abs": "https://arxiv.org/abs/2504.02433", "authors": ["Zhongjian Wang", "Peng Zhang", "Jinwei Qi", "Guangyuan Wang Sheng Xu", "Bang Zhang", "Liefeng Bo"], "title": "OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication", "categories": ["cs.CV"], "comment": "Project Page https://humanaigc.github.io/omnitalker", "summary": "Recent years have witnessed remarkable advances in talking head generation,\nowing to its potential to revolutionize the human-AI interaction from text\ninterfaces into realistic video chats. However, research on text-driven talking\nheads remains underexplored, with existing methods predominantly adopting a\ncascaded pipeline that combines TTS systems with audio-driven talking head\nmodels. This conventional pipeline not only introduces system complexity and\nlatency overhead but also fundamentally suffers from asynchronous audiovisual\noutput and stylistic discrepancies between generated speech and visual\nexpressions. To address these limitations, we introduce OmniTalker, an\nend-to-end unified framework that simultaneously generates synchronized speech\nand talking head videos from text and reference video in real-time zero-shot\nscenarios, while preserving both speech style and facial styles. The framework\nemploys a dual-branch diffusion transformer architecture: the audio branch\nsynthesizes mel-spectrograms from text, while the visual branch predicts\nfine-grained head poses and facial dynamics. To bridge modalities, we introduce\na novel audio-visual fusion module that integrates cross-modal information to\nensure temporal synchronization and stylistic coherence between audio and\nvisual outputs. Furthermore, our in-context reference learning module\neffectively captures both speech and facial style characteristics from a single\nreference video without introducing an extra style extracting module. To the\nbest of our knowledge, OmniTalker presents the first unified framework that\njointly models speech style and facial style in a zero-shot setting, achieving\nreal-time inference speed of 25 FPS. Extensive experiments demonstrate that our\nmethod surpasses existing approaches in generation quality, particularly\nexcelling in style preservation and audio-video synchronization."}
{"id": "2504.02620", "pdf": "https://arxiv.org/pdf/2504.02620", "abs": "https://arxiv.org/abs/2504.02620", "authors": ["Leonardo Iurada", "Marco Ciccone", "Tatiana Tommasi"], "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted ICLR 2025 - https://github.com/iurada/talos-task-arithmetic", "summary": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications."}
{"id": "2504.02436", "pdf": "https://arxiv.org/pdf/2504.02436", "abs": "https://arxiv.org/abs/2504.02436", "authors": ["Zhengcong Fei", "Debang Li", "Di Qiu", "Jiahua Wang", "Yikun Dou", "Rui Wang", "Jingtao Xu", "Mingyuan Fan", "Guibin Chen", "Yang Li", "Yahui Zhou"], "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation."}
{"id": "2504.02670", "pdf": "https://arxiv.org/pdf/2504.02670", "abs": "https://arxiv.org/abs/2504.02670", "authors": ["Maciej Besta", "Lorenzo Paleari", "Jia Hao Andrea Jiang", "Robert Gerstenberger", "You Wu", "Patrick Iff", "Ales Kubicek", "Piotr Nyczyk", "Diana Khimey", "Jón Gunnar Hannesson", "Grzegorz Kwaśniewski", "Marcin Copik", "Hubert Niewiadomski", "Torsten Hoefler"], "title": "Affordable AI Assistants with Knowledge Graph of Thoughts", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants."}
{"id": "2504.02437", "pdf": "https://arxiv.org/pdf/2504.02437", "abs": "https://arxiv.org/abs/2504.02437", "authors": ["Renwu Li", "Wenjing Ke", "Dong Li", "Lu Tian", "Emad Barsoum"], "title": "MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM", "categories": ["cs.CV"], "comment": null, "summary": "We present MonoGS++, a novel fast and accurate Simultaneous Localization and\nMapping (SLAM) method that leverages 3D Gaussian representations and operates\nsolely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods\nlargely depended on depth sensors, our approach reduces the hardware dependency\nand only requires RGB input, leveraging online visual odometry (VO) to generate\nsparse point clouds in real-time. To reduce redundancy and enhance the quality\nof 3D scene reconstruction, we implemented a series of methodological\nenhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian\ninsertion to avoid adding redundant Gaussians in previously well-reconstructed\nareas. Secondly, we introduced clarity-enhancing Gaussian densification module\nand planar regularization to handle texture-less areas and flat surfaces\nbetter. We achieved precise camera tracking results both on the synthetic\nReplica and real-world TUM-RGBD datasets, comparable to those of the\nstate-of-the-art. Additionally, our method realized a significant 5.57x\nimprovement in frames per second (fps) over the previous state-of-the-art,\nMonoGS."}
{"id": "2504.02793", "pdf": "https://arxiv.org/pdf/2504.02793", "abs": "https://arxiv.org/abs/2504.02793", "authors": ["Gaurav Verma", "Jiawei Zhou", "Mohit Chandra", "Srijan Kumar", "Munmun De Choudhury"], "title": "A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "comment": "pre-print; 7 pages of main content, 1 figure, 1 table", "summary": "Large artificial intelligence (AI) models have garnered significant attention\nfor their remarkable, often \"superhuman\", performance on standardized\nbenchmarks. However, when these models are deployed in high-stakes verticals\nsuch as healthcare, education, and law, they often reveal notable limitations.\nFor instance, they exhibit brittleness to minor variations in input data,\npresent contextually uninformed decisions in critical settings, and undermine\nuser trust by confidently producing or reproducing inaccuracies. These\nchallenges in applying large models necessitate cross-disciplinary innovations\nto align the models' capabilities with the needs of real-world applications. We\nintroduce a framework that addresses this gap through a layer-wise abstraction\nof innovations aimed at meeting users' requirements with large models. Through\nmultiple case studies, we illustrate how researchers and practitioners across\nvarious fields can operationalize this framework. Beyond modularizing the\npipeline of transforming large models into useful \"vertical systems\", we also\nhighlight the dynamism that exists within different layers of the framework.\nFinally, we discuss how our framework can guide researchers and practitioners\nto (i) optimally situate their innovations (e.g., when vertical-specific\ninsights can empower broadly impactful vertical-agnostic innovations), (ii)\nuncover overlooked opportunities (e.g., spotting recurring problems across\nverticals to develop practically useful foundation models instead of chasing\nbenchmarks), and (iii) facilitate cross-disciplinary communication of critical\nchallenges (e.g., enabling a shared vocabulary for AI developers, domain\nexperts, and human-computer interaction scholars)."}
{"id": "2504.02440", "pdf": "https://arxiv.org/pdf/2504.02440", "abs": "https://arxiv.org/abs/2504.02440", "authors": ["Hao Wang", "Shuo Zhang", "Biao Leng"], "title": "HGFormer: Topology-Aware Vision Transformer with HyperGraph Learning", "categories": ["cs.CV"], "comment": null, "summary": "The computer vision community has witnessed an extensive exploration of\nvision transformers in the past two years. Drawing inspiration from traditional\nschemes, numerous works focus on introducing vision-specific inductive biases.\nHowever, the implicit modeling of permutation invariance and fully-connected\ninteraction with individual tokens disrupts the regional context and spatial\ntopology, further hindering higher-order modeling. This deviates from the\nprinciple of perceptual organization that emphasizes the local groups and\noverall topology of visual elements. Thus, we introduce the concept of\nhypergraph for perceptual exploration. Specifically, we propose a\ntopology-aware vision transformer called HyperGraph Transformer (HGFormer).\nFirstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm\nfor semantic guidance during hypergraph construction. Secondly, we present a\ntopology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph\ntopology as perceptual indications to guide the aggregation of global and\nunbiased information during hypergraph messaging. Using HGFormer as visual\nbackbone, we develop an effective and unitive representation, achieving\ndistinct and detailed scene depictions. Empirical experiments show that the\nproposed HGFormer achieves competitive performance compared to the recent SoTA\ncounterparts on various visual benchmarks. Extensive ablation and visualization\nstudies provide comprehensive explanations of our ideas and contributions."}
{"id": "2504.02828", "pdf": "https://arxiv.org/pdf/2504.02828", "abs": "https://arxiv.org/abs/2504.02828", "authors": ["Jinqi Luo", "Tianjiao Ding", "Kwan Ho Ryan Chan", "Hancheng Min", "Chris Callison-Burch", "René Vidal"], "title": "Concept Lancet: Image Editing with Compositional Representation Transplant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan", "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation."}
{"id": "2504.02451", "pdf": "https://arxiv.org/pdf/2504.02451", "abs": "https://arxiv.org/abs/2504.02451", "authors": ["Jiayi Gao", "Zijin Yin", "Changcheng Hua", "Yuxin Peng", "Kongming Liang", "Zhanyu Ma", "Jun Guo", "Yang Liu"], "title": "ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer", "categories": ["cs.CV"], "comment": null, "summary": "The development of Text-to-Video (T2V) generation has made motion transfer\npossible, enabling the control of video motion based on existing footage.\nHowever, current methods have two limitations: 1) struggle to handle\nmulti-subjects videos, failing to transfer specific subject motion; 2) struggle\nto preserve the diversity and accuracy of motion as transferring to subjects\nwith varying shapes. To overcome these, we introduce \\textbf{ConMo}, a\nzero-shot framework that disentangle and recompose the motions of subjects and\ncamera movements. ConMo isolates individual subject and background motion cues\nfrom complex trajectories in source videos using only subject masks, and\nreassembles them for target video generation. This approach enables more\naccurate motion control across diverse subjects and improves performance in\nmulti-subject scenarios. Additionally, we propose soft guidance in the\nrecomposition stage which controls the retention of original motion to adjust\nshape constraints, aiding subject shape adaptation and semantic transformation.\nUnlike previous methods, ConMo unlocks a wide range of applications, including\nsubject size and position editing, subject removal, semantic modifications, and\ncamera motion simulation. Extensive experiments demonstrate that ConMo\nsignificantly outperforms state-of-the-art methods in motion fidelity and\nsemantic consistency. The code is available at\nhttps://github.com/Andyplus1/ConMo."}
{"id": "2504.02454", "pdf": "https://arxiv.org/pdf/2504.02454", "abs": "https://arxiv.org/abs/2504.02454", "authors": ["Changshuo Wang", "Shuting He", "Xiang Fang", "Meiqing Wu", "Siew-Kei Lam", "Prayag Tiwari"], "title": "Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot point cloud semantic segmentation aims to accurately segment\n\"unseen\" new categories in point cloud scenes using limited labeled data.\nHowever, pretraining-based methods not only introduce excessive time overhead\nbut also overlook the local structure representation among irregular point\nclouds. To address these issues, we propose a pretraining-free local structure\nfitting network for few-shot point cloud semantic segmentation, named\nTaylorSeg. Specifically, inspired by Taylor series, we treat the local\nstructure representation of irregular point clouds as a polynomial fitting\nproblem and propose a novel local structure fitting convolution, called\nTaylorConv. This convolution learns the low-order basic information and\nhigh-order refined information of point clouds from explicit encoding of local\ngeometric structures. Then, using TaylorConv as the basic component, we\nconstruct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a\nparametric TaylorSeg-PN. The former can achieve performance comparable to\nexisting parametric models without pretraining. For the latter, we equip it\nwith an Adaptive Push-Pull (APP) module to mitigate the feature distribution\ndifferences between the query set and the support set. Extensive experiments\nvalidate the effectiveness of the proposed method. Notably, under the 2-way\n1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on\nthe S3DIS and ScanNet datasets respectively, compared to the previous\nstate-of-the-art methods. Our code is available at\nhttps://github.com/changshuowang/TaylorSeg."}
{"id": "2504.02464", "pdf": "https://arxiv.org/pdf/2504.02464", "abs": "https://arxiv.org/abs/2504.02464", "authors": ["Ruixiao Zhang", "Runwei Guan", "Xiangyu Chen", "Adam Prugel-Bennett", "Xiaohao Cai"], "title": "CornerPoint3D: Look at the Nearest Corner Instead of the Center", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.04061", "summary": "3D object detection aims to predict object centers, dimensions, and rotations\nfrom LiDAR point clouds. Despite its simplicity, LiDAR captures only the near\nside of objects, making center-based detectors prone to poor localization\naccuracy in cross-domain tasks with varying point distributions. Meanwhile,\nexisting evaluation metrics designed for single-domain assessment also suffer\nfrom overfitting due to dataset-specific size variations. A key question\narises: Do we really need models to maintain excellent performance in the\nentire 3D bounding boxes after being applied across domains? Actually, one of\nour main focuses is on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsizes is much more difficult. To address these issues, we rethink cross-domain\n3D object detection from a practical perspective. We propose two new metrics\nthat evaluate a model's ability to detect objects' closer-surfaces to the LiDAR\nsensor. Additionally, we introduce EdgeHead, a refinement head that guides\nmodels to focus more on learnable closer surfaces, significantly improving\ncross-domain performance under both our new and traditional BEV/3D metrics.\nFurthermore, we argue that predicting the nearest corner rather than the object\ncenter enhances robustness. We propose a novel 3D object detector, coined as\nCornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise\nthe learning and detection of the nearest corner of each object. Our proposed\nmethods realize a balanced trade-off between the detection quality of entire\nbounding boxes and the locating accuracy of closer surfaces to the LiDAR\nsensor, outperforming the traditional center-based detector CenterPoint in\nmultiple cross-domain tasks and providing a more practically reasonable and\nrobust cross-domain 3D object detection solution."}
{"id": "2504.02471", "pdf": "https://arxiv.org/pdf/2504.02471", "abs": "https://arxiv.org/abs/2504.02471", "authors": ["Håkon Næss Sandum", "Hans Ole Ørka", "Oliver Tomic", "Erik Næsset", "Terje Gobakken"], "title": "Semantic segmentation of forest stands using deep learning", "categories": ["cs.CV"], "comment": "31 pages, 7 figures, 4 tables", "summary": "Forest stands are the fundamental units in forest management inventories,\nsilviculture, and financial analysis within operational forestry. Over the past\ntwo decades, a common method for mapping stand borders has involved delineation\nthrough manual interpretation of stereographic aerial images. This is a\ntime-consuming and subjective process, limiting operational efficiency and\nintroducing inconsistencies. Substantial effort has been devoted to automating\nthe process, using various algorithms together with aerial images and canopy\nheight models constructed from airborne laser scanning (ALS) data, but manual\ninterpretation remains the preferred method. Deep learning (DL) methods have\ndemonstrated great potential in computer vision, yet their application to\nforest stand delineation remains unexplored in published research. This study\npresents a novel approach, framing stand delineation as a multiclass\nsegmentation problem and applying a U-Net based DL framework. The model was\ntrained and evaluated using multispectral images, ALS data, and an existing\nstand map created by an expert interpreter. Performance was assessed on\nindependent data using overall accuracy, a standard metric for classification\ntasks that measures the proportions of correctly classified pixels. The model\nachieved an overall accuracy of 0.73. These results demonstrate strong\npotential for DL in automated stand delineation. However, a few key challenges\nwere noted, especially for complex forest environments."}
{"id": "2504.02478", "pdf": "https://arxiv.org/pdf/2504.02478", "abs": "https://arxiv.org/abs/2504.02478", "authors": ["Bizhu Wu", "Jinheng Xie", "Keming Shen", "Zhe Kong", "Jianfeng Ren", "Ruibin Bai", "Rong Qu", "Linlin Shen"], "title": "MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities", "categories": ["cs.CV"], "comment": null, "summary": "Recent motion-aware large language models have demonstrated promising\npotential in unifying motion comprehension and generation. However, existing\napproaches primarily focus on coarse-grained motion-text modeling, where text\ndescribes the overall semantics of an entire motion sequence in just a few\nwords. This limits their ability to handle fine-grained motion-relevant tasks,\nsuch as understanding and controlling the movements of specific body parts. To\novercome this limitation, we pioneer MG-MotionLLM, a unified motion-language\nmodel for multi-granular motion comprehension and generation. We further\nintroduce a comprehensive multi-granularity training scheme by incorporating a\nset of novel auxiliary tasks, such as localizing temporal boundaries of motion\nsegments via detailed text as well as motion detailed captioning, to facilitate\nmutual reinforcement for motion-text modeling across various levels of\ngranularity. Extensive experiments show that our MG-MotionLLM achieves superior\nperformance on classical text-to-motion and motion-to-text tasks, and exhibits\npotential in novel fine-grained motion comprehension and editing tasks. Project\npage: CVI-SZU/MG-MotionLLM"}
{"id": "2504.02480", "pdf": "https://arxiv.org/pdf/2504.02480", "abs": "https://arxiv.org/abs/2504.02480", "authors": ["Kyungmin Choi", "JaKeoung Koo", "Stephen McLaughlin", "Abderrahim Halimi"], "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information."}
{"id": "2504.02494", "pdf": "https://arxiv.org/pdf/2504.02494", "abs": "https://arxiv.org/abs/2504.02494", "authors": ["Faisal Mohammad", "Duksan Ryu"], "title": "Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Semiconductor wafer defect classification is critical for ensuring high\nprecision and yield in manufacturing. Traditional CNN-based models often\nstruggle with class imbalances and recognition of the multiple overlapping\ndefect types in wafer maps. To address these challenges, we propose ViT-Tiny, a\nlightweight Vision Transformer (ViT) framework optimized for wafer defect\nclassification. Trained on the WM-38k dataset. ViT-Tiny outperforms its\nViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and\nCNN-based architectures. Through extensive ablation studies, we determine that\na patch size of 16 provides optimal performance. ViT-Tiny achieves an F1-score\nof 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification,\nimproving recall by 2.86% in two-defect classification, and increasing\nprecision by 3.13% in three-defect classification. Additionally, it\ndemonstrates enhanced robustness under limited labeled data conditions, making\nit a computationally efficient and reliable solution for real-world\nsemiconductor defect detection."}
{"id": "2504.02496", "pdf": "https://arxiv.org/pdf/2504.02496", "abs": "https://arxiv.org/abs/2504.02496", "authors": ["Jiuniu Wang", "Wenjia Xu", "Qingzhong Wang", "Antoni B. Chan"], "title": "Group-based Distinctive Image Captioning with Memory Difference Encoding and Attention", "categories": ["cs.CV", "cs.MM"], "comment": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:2108.09151", "summary": "Recent advances in image captioning have focused on enhancing accuracy by\nsubstantially increasing the dataset and model size. While conventional\ncaptioning models exhibit high performance on established metrics such as BLEU,\nCIDEr, and SPICE, the capability of captions to distinguish the target image\nfrom other similar images is under-explored. To generate distinctive captions,\na few pioneers employed contrastive learning or re-weighted the ground-truth\ncaptions. However, these approaches often overlook the relationships among\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events). In this paper, we introduce a novel approach to\nenhance the distinctiveness of image captions, namely Group-based Differential\nDistinctive Captioning Method, which visually compares each image with other\nimages in one similar group and highlights the uniqueness of each image. In\nparticular, we introduce a Group-based Differential Memory Attention (GDMA)\nmodule, designed to identify and emphasize object features in an image that are\nuniquely distinguishable within its image group, i.e., those exhibiting low\nsimilarity with objects in other images. This mechanism ensures that such\nunique object features are prioritized during caption generation for the image,\nthereby enhancing the distinctiveness of the resulting captions. To further\nrefine this process, we select distinctive words from the ground-truth captions\nto guide both the language decoder and the GDMA module. Additionally, we\npropose a new evaluation metric, the Distinctive Word Rate (DisWordRate), to\nquantitatively assess caption distinctiveness. Quantitative results indicate\nthat the proposed method significantly improves the distinctiveness of several\nbaseline models, and achieves state-of-the-art performance on distinctiveness\nwhile not excessively sacrificing accuracy..."}
{"id": "2504.02508", "pdf": "https://arxiv.org/pdf/2504.02508", "abs": "https://arxiv.org/abs/2504.02508", "authors": ["Zhuguanyu Wu", "Jiayi Zhang", "Jiaxin Chen", "Jinyang Guo", "Di Huang", "Yunhong Wang"], "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision Transformers (ViTs) have become one of the most commonly used\nbackbones for vision tasks. Despite their remarkable performance, they often\nsuffer significant accuracy drops when quantized for practical deployment,\nparticularly by post-training quantization (PTQ) under ultra-low bits.\nRecently, reconstruction-based PTQ methods have shown promising performance in\nquantizing Convolutional Neural Networks (CNNs). However, they fail when\napplied to ViTs, primarily due to the inaccurate estimation of output\nimportance and the substantial accuracy degradation in quantizing post-GELU\nactivations. To address these issues, we propose \\textbf{APHQ-ViT}, a novel PTQ\napproach based on importance estimation with Average Perturbation Hessian\n(APH). Specifically, we first thoroughly analyze the current approximation\napproaches with Hessian loss, and propose an improved average perturbation\nHessian loss. To deal with the quantization of the post-GELU activations, we\ndesign an MLP Reconstruction (MR) method by replacing the GELU function in MLP\nwith ReLU and reconstructing it by the APH loss on a small unlabeled\ncalibration set. Extensive experiments demonstrate that APHQ-ViT using linear\nquantizers outperforms existing PTQ methods by substantial margins in 3-bit and\n4-bit across different vision tasks. The source code is available at\nhttps://github.com/GoatWu/APHQ-ViT."}
{"id": "2504.02512", "pdf": "https://arxiv.org/pdf/2504.02512", "abs": "https://arxiv.org/abs/2504.02512", "authors": ["Emad Bahrami", "Olga Zatsarynna", "Gianpiero Francesca", "Juergen Gall"], "title": "Towards Generalizing Temporal Action Segmentation to Unseen Views", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While there has been substantial progress in temporal action segmentation,\nthe challenge to generalize to unseen views remains unaddressed. Hence, we\ndefine a protocol for unseen view action segmentation where camera views for\nevaluating the model are unavailable during training. This includes changing\nfrom top-frontal views to a side view or even more challenging from exocentric\nto egocentric views. Furthermore, we present an approach for temporal action\nsegmentation that tackles this challenge. Our approach leverages a shared\nrepresentation at both the sequence and segment levels to reduce the impact of\nview differences during training. We achieve this by introducing a sequence\nloss and an action loss, which together facilitate consistent video and action\nrepresentations across different views. The evaluation on the Assembly101,\nIkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a\n12.8% increase in F1@50 for unseen exocentric views and a substantial 54%\nimprovement for unseen egocentric views."}
{"id": "2504.02515", "pdf": "https://arxiv.org/pdf/2504.02515", "abs": "https://arxiv.org/abs/2504.02515", "authors": ["Nedko Savov", "Naser Kazemi", "Mohammad Mahdi", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "Exploration-Driven Generative Interactive Environments", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Modern world models require costly and time-consuming collection of large\nvideo datasets with action demonstrations by people or by environment-specific\nagents. To simplify training, we focus on using many virtual environments for\ninexpensive, automatically collected interaction data. Genie, a recent\nmulti-environment world model, demonstrates simulation abilities of many\nenvironments with shared behavior. Unfortunately, training their model requires\nexpensive demonstrations. Therefore, we propose a training framework merely\nusing a random agent in virtual environments. While the model trained in this\nmanner exhibits good controls, it is limited by the random exploration\npossibilities. To address this limitation, we propose AutoExplore Agent - an\nexploration agent that entirely relies on the uncertainty of the world model,\ndelivering diverse data from which it can learn the best. Our agent is fully\nindependent of environment-specific rewards and thus adapts easily to new\nenvironments. With this approach, the pretrained multi-environment model can\nquickly adapt to new environments achieving video fidelity and controllability\nimprovement. In order to obtain automatically large-scale interaction datasets\nfor pretraining, we group environments with similar behavior and controls. To\nthis end, we annotate the behavior and controls of 974 virtual environments - a\ndataset that we name RetroAct. For building our model, we first create an open\nimplementation of Genie - GenieRedux and apply enhancements and adaptations in\nour version GenieRedux-G. Our code and data are available at\nhttps://github.com/insait-institute/GenieRedux."}
{"id": "2504.02517", "pdf": "https://arxiv.org/pdf/2504.02517", "abs": "https://arxiv.org/abs/2504.02517", "authors": ["Yash Kulthe", "Andrew Gilbert", "John Collomosse"], "title": "MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely\nkeyed watermarks within images rendered by a single Neural Radiance Field\n(NeRF) model, whilst maintaining high visual quality. Our approach extends the\nTensoRF NeRF model by incorporating a dedicated watermark grid alongside the\nexisting geometry and appearance grids. This extension ensures higher watermark\ncapacity without entangling watermark signals with scene content. We propose a\nFiLM-based conditional modulation mechanism that dynamically activates\nwatermarks based on input identifiers, allowing multiple independent watermarks\nto be embedded and extracted without requiring model retraining. MultiNeRF is\nvalidated on the NeRF-Synthetic and LLFF datasets, with statistically\nsignificant improvements in robust capacity without compromising rendering\nquality. By generalizing single-watermark NeRF methods into a flexible\nmulti-watermarking framework, MultiNeRF provides a scalable solution for 3D\ncontent. attribution."}
{"id": "2504.02519", "pdf": "https://arxiv.org/pdf/2504.02519", "abs": "https://arxiv.org/abs/2504.02519", "authors": ["Christian Alexander Holz", "Christian Bader", "Markus Enzweiler", "Matthias Drüppel"], "title": "Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents novel Machine Learning (ML) methodologies for\nMulti-Object Tracking (MOT), specifically designed to meet the increasing\ncomplexity and precision demands of Advanced Driver Assistance Systems (ADAS).\nWe introduce three Neural Network (NN) models that address key challenges in\nMOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii)\nthe Single-Association Network (SANT) for mapping individual Sensor Object (SO)\nto existing tracks, and (iii) the Multi-Association Network (MANTa) for\nassociating multiple SOs to multiple tracks. These models are seamlessly\nintegrated into a traditional Kalman Filter (KF) framework, maintaining the\nsystem's modularity by replacing relevant components without disrupting the\noverall architecture. Importantly, all three networks are designed to be run in\na realtime, embedded environment. Each network contains less than 50k trainable\nparameters. Our evaluation, conducted on the public KITTI tracking dataset,\ndemonstrates significant improvements in tracking performance. SPENT reduces\nthe Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT\nand MANTa achieve up to 95% accuracy in sensor object-to-track assignments.\nThese results underscore the effectiveness of incorporating task-specific NNs\ninto traditional tracking systems, boosting performance and robustness while\npreserving modularity, maintainability, and interpretability."}
{"id": "2504.02522", "pdf": "https://arxiv.org/pdf/2504.02522", "abs": "https://arxiv.org/abs/2504.02522", "authors": ["Fatemeh Behrad", "Tinne Tuytelaars", "Johan Wagemans"], "title": "Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The capacity of Vision transformers (ViTs) to handle variable-sized inputs is\noften constrained by computational complexity and batch processing limitations.\nConsequently, ViTs are typically trained on small, fixed-size images obtained\nthrough downscaling or cropping. While reducing computational burden, these\nmethods result in significant information loss, negatively affecting tasks like\nimage aesthetic assessment. We introduce Charm, a novel tokenization approach\nthat preserves Composition, High-resolution, Aspect Ratio, and Multi-scale\ninformation simultaneously. Charm prioritizes high-resolution details in\nspecific regions while downscaling others, enabling shorter fixed-size input\nsequences for ViTs while incorporating essential information. Charm is designed\nto be compatible with pre-trained ViTs and their learned positional embeddings.\nBy providing multiscale input and introducing variety to input tokens, Charm\nimproves ViT performance and generalizability for image aesthetic assessment.\nWe avoid cropping or changing the aspect ratio to further preserve information.\nExtensive experiments demonstrate significant performance improvements on\nvarious image aesthetic and quality assessment datasets (up to 8.1 %) using a\nlightweight ViT backbone. Code and pre-trained models are available at\nhttps://github.com/FBehrad/Charm."}
{"id": "2504.02524", "pdf": "https://arxiv.org/pdf/2504.02524", "abs": "https://arxiv.org/abs/2504.02524", "authors": ["Yunhao Lv", "Lingyu Chen", "Jian Wang", "Yangxi Li", "Fang Chen"], "title": "SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2304.05919 by other authors", "summary": "In recent years, deep learning methods such as convolutional neural network\n(CNN) and transformers have made significant progress in CT multi-organ\nsegmentation. However, CT multi-organ segmentation methods based on masked\nimage modeling (MIM) are very limited. There are already methods using MAE for\nCT multi-organ segmentation task, we believe that the existing methods do not\nidentify the most difficult areas to reconstruct. To this end, we propose a MIM\nself-training framework with hard patches mining masked autoencoders for CT\nmulti-organ segmentation tasks (selfMedHPM). The method performs ViT\nself-pretraining on the training set of the target data and introduces an\nauxiliary loss predictor, which first predicts the patch loss and determines\nthe location of the next mask. SelfMedHPM implementation is better than various\ncompetitive methods in abdominal CT multi-organ segmentation and body CT\nmulti-organ segmentation. We have validated the performance of our method on\nthe Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen\nmult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body\nmulti-organ segmentation tasks."}
{"id": "2504.02534", "pdf": "https://arxiv.org/pdf/2504.02534", "abs": "https://arxiv.org/abs/2504.02534", "authors": ["Mykola Lavreniuk", "Nataliia Kussul", "Andrii Shelestov", "Bohdan Yailymov", "Yevhenii Salii", "Volodymyr Kuzin", "Zoltan Szantoi"], "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery", "categories": ["cs.CV"], "comment": null, "summary": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything."}
{"id": "2504.02536", "pdf": "https://arxiv.org/pdf/2504.02536", "abs": "https://arxiv.org/abs/2504.02536", "authors": ["Konrad Gadzicki", "Kerstin Schill", "Christoph Zetzsche"], "title": "A Sensorimotor Vision Transformer", "categories": ["cs.CV", "I.4.8; I.5.1"], "comment": "14 pages, 5 figures", "summary": "This paper presents the Sensorimotor Transformer (SMT), a vision model\ninspired by human saccadic eye movements that prioritize high-saliency regions\nin visual input to enhance computational efficiency and reduce memory\nconsumption. Unlike traditional models that process all image patches\nuniformly, SMT identifies and selects the most salient patches based on\nintrinsic two-dimensional (i2D) features, such as corners and occlusions, which\nare known to convey high-information content and align with human fixation\npatterns. The SMT architecture uses this biological principle to leverage\nvision transformers to process only the most informative patches, allowing for\na substantial reduction in memory usage that scales with the sequence length of\nselected patches. This approach aligns with visual neuroscience findings,\nsuggesting that the human visual system optimizes information gathering through\nselective, spatially dynamic focus. Experimental evaluations on Imagenet-1k\ndemonstrate that SMT achieves competitive top-1 accuracy while significantly\nreducing memory consumption and computational complexity, particularly when a\nlimited number of patches is used. This work introduces a saccade-like\nselection mechanism into transformer-based vision models, offering an efficient\nalternative for image analysis and providing new insights into biologically\nmotivated architectures for resource-constrained applications."}
{"id": "2504.02542", "pdf": "https://arxiv.org/pdf/2504.02542", "abs": "https://arxiv.org/abs/2504.02542", "authors": ["Fa-Ting Hong", "Zunnan Xu", "Zixiang Zhou", "Jun Zhou", "Xiu Li", "Qin Lin", "Qinglin Lu", "Dan Xu"], "title": "Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation", "categories": ["cs.CV"], "comment": null, "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce \\textbf{ACTalker}, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict."}
{"id": "2504.02545", "pdf": "https://arxiv.org/pdf/2504.02545", "abs": "https://arxiv.org/abs/2504.02545", "authors": ["Bo-Kai Ruan", "Hong-Han Shuai"], "title": "MAD: Makeup All-in-One with Cross-Domain Diffusion Model", "categories": ["cs.CV"], "comment": "Project page: https://basiclab.github.io/MAD", "summary": "Existing makeup techniques often require designing multiple models to handle\ndifferent inputs and align features across domains for different makeup tasks,\ne.g., beauty filter, makeup transfer, and makeup removal, leading to increased\ncomplexity. Another limitation is the absence of text-guided makeup try-on,\nwhich is more user-friendly without needing reference images. In this study, we\nmake the first attempt to use a single model for various makeup tasks.\nSpecifically, we formulate different makeup tasks as cross-domain translations\nand leverage a cross-domain diffusion model to accomplish all tasks. Unlike\nexisting methods that rely on separate encoder-decoder configurations or\ncycle-based mechanisms, we propose using different domain embeddings to\nfacilitate domain control. This allows for seamless domain switching by merely\nchanging embeddings with a single model, thereby reducing the reliance on\nadditional modules for different tasks. Moreover, to support precise\ntext-to-makeup applications, we introduce the MT-Text dataset by extending the\nMT dataset with textual annotations, advancing the practicality of makeup\ntechnologies."}
{"id": "2504.02555", "pdf": "https://arxiv.org/pdf/2504.02555", "abs": "https://arxiv.org/abs/2504.02555", "authors": ["Hesong Li", "Ziqi Wu", "Ruiwen Shao", "Tao Zhang", "Ying Fu"], "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement", "categories": ["cs.CV"], "comment": "Acceped by CVPR2025", "summary": "Scanning Transmission Electron Microscopy (STEM) enables the observation of\natomic arrangements at sub-angstrom resolution, allowing for atomically\nresolved analysis of the physical and chemical properties of materials.\nHowever, due to the effects of noise, electron beam damage, sample thickness,\netc, obtaining satisfactory atomic-level images is often challenging. Enhancing\nSTEM images can reveal clearer structural details of materials. Nonetheless,\nexisting STEM image enhancement methods usually overlook unique features in the\nfrequency domain, and existing datasets lack realism and generality. To resolve\nthese issues, in this paper, we develop noise calibration, data synthesis, and\nenhancement methods for STEM images. We first present a STEM noise calibration\nmethod, which is used to synthesize more realistic STEM images. The parameters\nof background noise, scan noise, and pointwise noise are obtained by\nstatistical analysis and fitting of real STEM images containing atoms. Then we\nuse these parameters to develop a more general dataset that considers both\nregular and random atomic arrangements and includes both HAADF and BF mode\nimages. Finally, we design a spatial-frequency interactive network for STEM\nimage enhancement, which can explore the information in the frequency domain\nformed by the periodicity of atomic arrangement. Experimental results show that\nour data is closer to real STEM images and achieves better enhancement\nperformances together with our network. Code will be available at\nhttps://github.com/HeasonLee/SFIN}{https://github.com/HeasonLee/SFIN."}
{"id": "2504.02558", "pdf": "https://arxiv.org/pdf/2504.02558", "abs": "https://arxiv.org/abs/2504.02558", "authors": ["Andrei Dumitriu", "Florin Tatui", "Florin Miron", "Radu Tudor Ionescu", "Radu Timofte"], "title": "Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results", "categories": ["cs.CV", "cs.AI", "I.4.0; I.4.9"], "comment": "Accepted at CVPR 2023 NTIRE Workshop", "summary": "Rip currents are the leading cause of fatal accidents and injuries on many\nbeaches worldwide, emphasizing the importance of automatically detecting these\nhazardous surface water currents. In this paper, we address a novel task: rip\ncurrent instance segmentation. We introduce a comprehensive dataset containing\n$2,466$ images with newly created polygonal annotations for instance\nsegmentation, used for training and validation. Additionally, we present a\nnovel dataset comprising $17$ drone videos (comprising about $24K$ frames)\ncaptured at $30 FPS$, annotated with both polygons for instance segmentation\nand bounding boxes for object detection, employed for testing purposes. We\ntrain various versions of YOLOv8 for instance segmentation on static images and\nassess their performance on the test dataset (videos). The best results were\nachieved by the YOLOv8-nano model (runnable on a portable device), with an\nmAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the\ntest dataset. The results provide a baseline for future research in rip current\nsegmentation. Our work contributes to the existing literature by introducing a\ndetailed, annotated dataset, and training a deep learning model for instance\nsegmentation of rip currents. The code, training details and the annotated\ndataset are made publicly available at https://github.com/Irikos/rip_currents."}
{"id": "2504.02560", "pdf": "https://arxiv.org/pdf/2504.02560", "abs": "https://arxiv.org/abs/2504.02560", "authors": ["Yongqi Zhai", "Luyang Tang", "Wei Jiang", "Jiayu Yang", "Ronggang Wang"], "title": "L-LBVC: Long-Term Motion Estimation and Prediction for Learned Bi-Directional Video Compression", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted to 2025 Data Compression Conference (DCC)", "summary": "Recently, learned video compression (LVC) has shown superior performance\nunder low-delay configuration. However, the performance of learned\nbi-directional video compression (LBVC) still lags behind traditional\nbi-directional coding. The performance gap mainly arises from inaccurate\nlong-term motion estimation and prediction of distant frames, especially in\nlarge motion scenes. To solve these two critical problems, this paper proposes\na novel LBVC framework, namely L-LBVC. Firstly, we propose an adaptive motion\nestimation module that can handle both short-term and long-term motions.\nSpecifically, we directly estimate the optical flows for adjacent frames and\nnon-adjacent frames with small motions. For non-adjacent frames with large\nmotions, we recursively accumulate local flows between adjacent frames to\nestimate long-term flows. Secondly, we propose an adaptive motion prediction\nmodule that can largely reduce the bit cost for motion coding. To improve the\naccuracy of long-term motion prediction, we adaptively downsample reference\nframes during testing to match the motion ranges observed during training.\nExperiments show that our L-LBVC significantly outperforms previous\nstate-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets\nunder random access configuration."}
{"id": "2504.02602", "pdf": "https://arxiv.org/pdf/2504.02602", "abs": "https://arxiv.org/abs/2504.02602", "authors": ["Abdul Rehman", "Talha Meraj", "Aiman Mahmood Minhas", "Ayisha Imran", "Mohsen Ali", "Waqas Sultani", "Mubarak Shah"], "title": "Leveraging Sparse Annotations for Leukemia Diagnosis on the Large Leukemia Dataset", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Leukemia is 10th most frequently diagnosed cancer and one of the leading\ncauses of cancer related deaths worldwide. Realistic analysis of Leukemia\nrequires White Blook Cells (WBC) localization, classification, and\nmorphological assessment. Despite deep learning advances in medical imaging,\nleukemia analysis lacks a large, diverse multi-task dataset, while existing\nsmall datasets lack domain diversity, limiting real world applicability. To\novercome dataset challenges, we present a large scale WBC dataset named Large\nLeukemia Dataset (LLD) and novel methods for detecting WBC with their\nattributes. Our contribution here is threefold. First, we present a large-scale\nLeukemia dataset collected through Peripheral Blood Films (PBF) from several\npatients, through multiple microscopes, multi cameras, and multi magnification.\nTo enhance diagnosis explainability and medical expert acceptance, each\nleukemia cell is annotated at 100x with 7 morphological attributes, ranging\nfrom Cell Size to Nuclear Shape. Secondly, we propose a multi task model that\nnot only detects WBCs but also predicts their attributes, providing an\ninterpretable and clinically meaningful solution. Third, we propose a method\nfor WBC detection with attribute analysis using sparse annotations. This\napproach reduces the annotation burden on hematologists, requiring them to mark\nonly a small area within the field of view. Our method enables the model to\nleverage the entire field of view rather than just the annotated regions,\nenhancing learning efficiency and diagnostic accuracy. From diagnosis\nexplainability to overcoming domain shift challenges, presented datasets could\nbe used for many challenging aspects of microscopic image analysis. The\ndatasets, code, and demo are available at:\nhttps://im.itu.edu.pk/sparse-leukemiaattri/"}
{"id": "2504.02612", "pdf": "https://arxiv.org/pdf/2504.02612", "abs": "https://arxiv.org/abs/2504.02612", "authors": ["Jiwoo Chung", "Sangeek Hyun", "Hyunjun Kim", "Eunseo Koh", "MinKyu Lee", "Jae-Pil Heo"], "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive~(VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize local\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage."}
{"id": "2504.02617", "pdf": "https://arxiv.org/pdf/2504.02617", "abs": "https://arxiv.org/abs/2504.02617", "authors": ["Lihua Liu", "Jiehong Lin", "Zhenxin Liu", "Kui Jia"], "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Novel object pose estimation from RGB images presents a significant challenge\nfor zero-shot generalization, as it involves estimating the relative 6D\ntransformation between an RGB observation and a CAD model of an object that was\nnot seen during training. In this paper, we introduce PicoPose, a novel\nframework designed to tackle this task using a three-stage pixel-to-pixel\ncorrespondence learning process. Firstly, PicoPose matches features from the\nRGB observation with those from rendered object templates, identifying the\nbest-matched template and establishing coarse correspondences. Secondly,\nPicoPose smooths the correspondences by globally regressing a 2D affine\ntransformation, including in-plane rotation, scale, and 2D translation, from\nthe coarse correspondence map. Thirdly, PicoPose applies the affine\ntransformation to the feature map of the best-matched template and learns\ncorrespondence offsets within local regions to achieve fine-grained\ncorrespondences. By progressively refining the correspondences, PicoPose\nsignificantly improves the accuracy of object poses computed via PnP/RANSAC.\nPicoPose achieves state-of-the-art performance on the seven core datasets of\nthe BOP benchmark, demonstrating exceptional generalization to novel objects\nrepresented by CAD models or object reference images. Code and models are\navailable at https://github.com/foollh/PicoPose."}
{"id": "2504.02697", "pdf": "https://arxiv.org/pdf/2504.02697", "abs": "https://arxiv.org/abs/2504.02697", "authors": ["Xingguang Zhang", "Nicholas Chimitt", "Xijun Wang", "Yu Yuan", "Stanley H. Chan"], "title": "Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR 2025, project page: https://xg416.github.io/MambaTM/", "summary": "Atmospheric turbulence is a major source of image degradation in long-range\nimaging systems. Although numerous deep learning-based turbulence mitigation\n(TM) methods have been proposed, many are slow, memory-hungry, and do not\ngeneralize well. In the spatial domain, methods based on convolutional\noperators have a limited receptive field, so they cannot handle a large spatial\ndependency required by turbulence. In the temporal domain, methods relying on\nself-attention can, in theory, leverage the lucky effects of turbulence, but\ntheir quadratic complexity makes it difficult to scale to many frames.\nTraditional recurrent aggregation methods face parallelization challenges.\n  In this paper, we present a new TM method based on two concepts: (1) A\nturbulence mitigation network based on the Selective State Space Model\n(MambaTM). MambaTM provides a global receptive field in each layer across\nspatial and temporal dimensions while maintaining linear computational\ncomplexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state\nspace model. Unlike classical Zernike-based representations of phase\ndistortion, the new LPD map uniquely captures the actual effects of turbulence,\nsignificantly improving the model's capability to estimate degradation by\nreducing the ill-posedness. Our proposed method exceeds current\nstate-of-the-art networks on various synthetic and real-world TM benchmarks\nwith significantly faster inference speed. The code is available at\nhttp://github.com/xg416/MambaTM."}
{"id": "2504.02730", "pdf": "https://arxiv.org/pdf/2504.02730", "abs": "https://arxiv.org/abs/2504.02730", "authors": ["Hui Zhang", "Qinglin Zhao", "Mengchu Zhou", "Li Feng"], "title": "HQViT: Hybrid Quantum Vision Transformer for Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 8 figures", "summary": "Transformer-based architectures have revolutionized the landscape of deep\nlearning. In computer vision domain, Vision Transformer demonstrates remarkable\nperformance on par with or even surpassing that of convolutional neural\nnetworks. However, the quadratic computational complexity of its self-attention\nmechanism poses challenges for classical computing, making model training with\nhigh-dimensional input data, e.g., images, particularly expensive. To address\nsuch limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that\nleverages the principles of quantum computing to accelerate model training\nwhile enhancing model performance. HQViT introduces whole-image processing with\namplitude encoding to better preserve global image information without\nadditional positional encoding. By leveraging quantum computation on the most\ncritical steps and selectively handling other components in a classical way, we\nlower the cost of quantum resources for HQViT. The qubit requirement is\nminimized to $O(log_2N)$ and the number of parameterized quantum gates is only\n$O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum\ndevices. By offloading the computationally intensive attention coefficient\nmatrix calculation to the quantum framework, HQViT reduces the classical\ncomputational load by $O(T^2d)$. Extensive experiments across various computer\nvision datasets demonstrate that HQViT outperforms existing models, achieving a\nmaximum improvement of up to $10.9\\%$ (on the MNIST 10-classification task)\nover the state of the art. This work highlights the great potential to combine\nquantum and classical computing to cope with complex image classification\ntasks."}
{"id": "2504.02762", "pdf": "https://arxiv.org/pdf/2504.02762", "abs": "https://arxiv.org/abs/2504.02762", "authors": ["Ahmet Burak Yildirim", "Mustafa Utku Aydogdu", "Duygu Ceylan", "Aysegul Dundar"], "title": "MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MD-ProjTex, a method for fast and consistent text-guided texture\ngeneration for 3D shapes using pretrained text-to-image diffusion models. At\nthe core of our approach is a multi-view consistency mechanism in UV space,\nwhich ensures coherent textures across different viewpoints. Specifically,\nMD-ProjTex fuses noise predictions from multiple views at each diffusion step\nand jointly updates the per-view denoising directions to maintain 3D\nconsistency. In contrast to existing state-of-the-art methods that rely on\noptimization or sequential view synthesis, MD-ProjTex is computationally more\nefficient and achieves better quantitative and qualitative results."}
{"id": "2504.02763", "pdf": "https://arxiv.org/pdf/2504.02763", "abs": "https://arxiv.org/abs/2504.02763", "authors": ["Benjy Friedmann", "Michael Werman"], "title": "CanonNet: Canonical Ordering and Curvature Learning for Point Cloud Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud processing poses two fundamental challenges: establishing\nconsistent point ordering and effectively learning fine-grained geometric\nfeatures. Current architectures rely on complex operations that limit\nexpressivity while struggling to capture detailed surface geometry. We present\nCanonNet, a lightweight neural network composed of two complementary\ncomponents: (1) a preprocessing pipeline that creates a canonical point\nordering and orientation, and (2) a geometric learning framework where networks\nlearn from synthetic surfaces with precise curvature values. This modular\napproach eliminates the need for complex transformation-invariant architectures\nwhile effectively capturing local geometric properties. Our experiments\ndemonstrate state-of-the-art performance in curvature estimation and\ncompetitive results in geometric descriptor tasks with significantly fewer\nparameters (\\textbf{100X}) than comparable methods. CanonNet's efficiency makes\nit particularly suitable for real-world applications where computational\nresources are limited, demonstrating that mathematical preprocessing can\neffectively complement neural architectures for point cloud analysis. The code\nfor the project is publicly available\n\\hyperlink{https://benjyfri.github.io/CanonNet/}{https://benjyfri.github.io/CanonNet/}."}
{"id": "2504.02764", "pdf": "https://arxiv.org/pdf/2504.02764", "abs": "https://arxiv.org/abs/2504.02764", "authors": ["Shengjun Zhang", "Jinzhao Li", "Xin Fei", "Hao Liu", "Yueqi Duan"], "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "In this paper, we propose Scene Splatter, a momentum-based paradigm for video\ndiffusion to generate generic scenes from single image. Existing methods, which\nemploy video generation models to synthesize novel views, suffer from limited\nvideo length and scene inconsistency, leading to artifacts and distortions\nduring further reconstruction. To address this issue, we construct noisy\nsamples from original features as momentum to enhance video details and\nmaintain scene consistency. However, for latent features with the perception\nfield that spans both known and unknown regions, such latent-level momentum\nrestricts the generative ability of video diffusion in unknown regions.\nTherefore, we further introduce the aforementioned consistent video as a\npixel-level momentum to a directly generated video without momentum for better\nrecovery of unseen regions. Our cascaded momentum enables video diffusion\nmodels to generate both high-fidelity and consistent novel views. We further\nfinetune the global Gaussian representations with enhanced frames and render\nnew frames for momentum update in the next step. In this manner, we can\niteratively recover a 3D scene, avoiding the limitation of video length.\nExtensive experiments demonstrate the generalization capability and superior\nperformance of our method in high-fidelity and consistent scene generation."}
{"id": "2504.02775", "pdf": "https://arxiv.org/pdf/2504.02775", "abs": "https://arxiv.org/abs/2504.02775", "authors": ["Yoon Gyo Jung", "Jaewoo Park", "Jaeho Yoon", "Kuan-Chuan Peng", "Wonchul Kim", "Andrew Beng Jin Teoh", "Octavia Camps"], "title": "TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR2025", "summary": "We aim to solve unsupervised anomaly detection in a practical challenging\nenvironment where the normal dataset is both contaminated with defective\nregions and its product class distribution is tailed but unknown. We observe\nthat existing models suffer from tail-versus-noise trade-off where if a model\nis robust against pixel noise, then its performance deteriorates on tail class\nsamples, and vice versa. To mitigate the issue, we handle the tail class and\nnoise samples independently. To this end, we propose TailSampler, a novel class\nsize predictor that estimates the class cardinality of samples based on a\nsymmetric assumption on the class-wise distribution of embedding similarities.\nTailSampler can be utilized to sample the tail class samples exclusively,\nallowing to handle them separately. Based on these facets, we build a\nmemory-based anomaly detection model TailedCore, whose memory both well\ncaptures tail class information and is noise-robust. We extensively validate\nthe effectiveness of TailedCore on the unsupervised long-tail noisy anomaly\ndetection setting, and show that TailedCore outperforms the state-of-the-art in\nmost settings."}
{"id": "2504.02778", "pdf": "https://arxiv.org/pdf/2504.02778", "abs": "https://arxiv.org/abs/2504.02778", "authors": ["Vincent Gbouna Zakka", "Luis J. Manso", "Zhuangzhuang Dai"], "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human activity recognition is increasingly vital for supporting independent\nliving, particularly for the elderly and those in need of assistance. Domestic\nservice robots with monitoring capabilities can enhance safety and provide\nessential support. Although image-based methods have advanced considerably in\nthe past decade, their adoption remains limited by concerns over privacy and\nsensitivity to low-light or dark conditions. As an alternative, millimetre-wave\n(mmWave) radar can produce point cloud data which is privacy-preserving.\nHowever, processing the sparse and noisy point clouds remains a long-standing\nchallenge. While graph-based methods and attention mechanisms show promise,\nthey predominantly rely on \"fixed\" kernels; kernels that are applied uniformly\nacross all neighbourhoods, highlighting the need for adaptive approaches that\ncan dynamically adjust their kernels to the specific geometry of each local\nneighbourhood in point cloud data. To overcome this limitation, we introduce an\nadaptive approach within the graph convolutional framework. Instead of a single\nshared weight function, our Multi-Head Adaptive Kernel (MAK) module generates\nmultiple dynamic kernels, each capturing different aspects of the local feature\nspace. By progressively refining local features while maintaining global\nspatial context, our method enables convolution kernels to adapt to varying\nlocal features. Experimental results on benchmark datasets confirm the\neffectiveness of our approach, achieving state-of-the-art performance in human\nactivity recognition. Our source code is made publicly available at:\nhttps://github.com/Gbouna/MAK-GCN"}
{"id": "2504.02782", "pdf": "https://arxiv.org/pdf/2504.02782", "abs": "https://arxiv.org/abs/2504.02782", "authors": ["Zhiyuan Yan", "Junyan Ye", "Weijia Li", "Zilong Huang", "Shenghai Yuan", "Xiangyang He", "Kaiqing Lin", "Jun He", "Conghui He", "Li Yuan"], "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval."}
{"id": "2504.02799", "pdf": "https://arxiv.org/pdf/2504.02799", "abs": "https://arxiv.org/abs/2504.02799", "authors": ["Anita Rau", "Mark Endo", "Josiah Aklilu", "Jaewoo Heo", "Khaled Saab", "Alberto Paderno", "Jeffrey Jopling", "F. Christopher Holsinger", "Serena Yeung-Levy"], "title": "Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models offer a new paradigm for AI-driven image\nunderstanding, enabling models to perform tasks without task-specific training.\nThis flexibility holds particular promise across medicine, where\nexpert-annotated data is scarce. Yet, VLMs' practical utility in\nintervention-focused domains--especially surgery, where decision-making is\nsubjective and clinical scenarios are variable--remains uncertain. Here, we\npresent a comprehensive analysis of 11 state-of-the-art VLMs across 17 key\nvisual understanding tasks in surgical AI--from anatomy recognition to skill\nassessment--using 13 datasets spanning laparoscopic, robotic, and open\nprocedures. In our experiments, VLMs demonstrate promising generalizability, at\ntimes outperforming supervised models when deployed outside their training\nsetting. In-context learning, incorporating examples during testing, boosted\nperformance up to three-fold, suggesting adaptability as a key strength. Still,\ntasks requiring spatial or temporal reasoning remained difficult. Beyond\nsurgery, our findings offer insights into VLMs' potential for tackling complex\nand dynamic scenarios in clinical and broader real-world applications."}
{"id": "2504.02801", "pdf": "https://arxiv.org/pdf/2504.02801", "abs": "https://arxiv.org/abs/2504.02801", "authors": ["Jay N. Paranjape", "Celso de Melo", "Vishal M. Patel"], "title": "F-ViTA: Foundation Model Guided Visible to Thermal Translation", "categories": ["cs.CV"], "comment": null, "summary": "Thermal imaging is crucial for scene understanding, particularly in low-light\nand nighttime conditions. However, collecting large thermal datasets is costly\nand labor-intensive due to the specialized equipment required for infrared\nimage capture. To address this challenge, researchers have explored\nvisible-to-thermal image translation. Most existing methods rely on Generative\nAdversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a\nstyle transfer problem. As a result, these approaches attempt to learn both the\nmodality distribution shift and underlying physical principles from limited\ntraining data. In this paper, we propose F-ViTA, a novel approach that\nleverages the general world knowledge embedded in foundation models to guide\nthe diffusion process for improved translation. Specifically, we condition an\nInstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation\nmodels such as SAM and Grounded DINO. This allows the model to learn meaningful\ncorrelations between scene objects and their thermal signatures in infrared\nimagery. Extensive experiments on five public datasets demonstrate that F-ViTA\noutperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes\nwell to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared\n(LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the\nsame visible image. Code: https://github.com/JayParanjape/F-ViTA/tree/master."}
{"id": "2504.02812", "pdf": "https://arxiv.org/pdf/2504.02812", "abs": "https://arxiv.org/abs/2504.02812", "authors": ["Van Nguyen Nguyen", "Stephen Tyree", "Andrew Guo", "Mederic Fourmy", "Anas Gouda", "Taeyeop Lee", "Sungphill Moon", "Hyeontae Son", "Lukas Ranftl", "Jonathan Tremblay", "Eric Brachmann", "Bertram Drost", "Vincent Lepetit", "Carsten Rother", "Stan Birchfield", "Jiri Matas", "Yann Labbe", "Martin Sundermeyer", "Tomas Hodan"], "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2403.09799", "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/"}
{"id": "2504.02817", "pdf": "https://arxiv.org/pdf/2504.02817", "abs": "https://arxiv.org/abs/2504.02817", "authors": ["Kangle Deng", "Hsueh-Ti Derek Liu", "Yiheng Zhu", "Xiaoxia Sun", "Chong Shang", "Kiran Bhat", "Deva Ramanan", "Jun-Yan Zhu", "Maneesh Agrawala", "Tinghui Zhou"], "title": "Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization", "categories": ["cs.CV"], "comment": "Project Page: https://oat-3d.github.io/", "summary": "Many 3D generative models rely on variational autoencoders (VAEs) to learn\ncompact shape representations. However, existing methods encode all shapes into\na fixed-size token, disregarding the inherent variations in scale and\ncomplexity across 3D data. This leads to inefficient latent representations\nthat can compromise downstream generation. We address this challenge by\nintroducing Octree-based Adaptive Tokenization, a novel framework that adjusts\nthe dimension of latent representations according to shape complexity. Our\napproach constructs an adaptive octree structure guided by a\nquadric-error-based subdivision criterion and allocates a shape latent vector\nto each octree cell using a query-based transformer. Building upon this\ntokenization, we develop an octree-based autoregressive generative model that\neffectively leverages these variable-sized representations in shape generation.\nExtensive experiments demonstrate that our approach reduces token counts by 50%\ncompared to fixed-size methods while maintaining comparable visual quality.\nWhen using a similar token length, our method produces significantly\nhigher-quality shapes. When incorporated with our downstream generative model,\nour method creates more detailed and diverse 3D content than existing\napproaches."}
{"id": "2504.02819", "pdf": "https://arxiv.org/pdf/2504.02819", "abs": "https://arxiv.org/abs/2504.02819", "authors": ["Yuexi Du", "Jiazhen Zhang", "Nicha C. Dvornek", "John A. Onofrey"], "title": "GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings", "categories": ["cs.CV", "cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "Symmetry, where certain features remain invariant under geometric\ntransformations, can often serve as a powerful prior in designing convolutional\nneural networks (CNNs). While conventional CNNs inherently support\ntranslational equivariance, extending this property to rotation and reflection\nhas proven challenging, often forcing a compromise between equivariance,\nefficiency, and information loss. In this work, we introduce Gaussian Mixture\nRing Convolution (GMR-Conv), an efficient convolution kernel that smooths\nradial symmetry using a mixture of Gaussian-weighted rings. This design\nmitigates discretization errors of circular kernels, thereby preserving robust\nrotation and reflection equivariance without incurring computational overhead.\nWe further optimize both the space and speed efficiency of GMR-Conv via a novel\nparameterization and computation strategy, allowing larger kernels at an\nacceptable cost. Extensive experiments on eight classification and one\nsegmentation datasets demonstrate that GMR-Conv not only matches conventional\nCNNs' performance but can also surpass it in applications with orientation-less\ndata. GMR-Conv is also proven to be more robust and efficient than the\nstate-of-the-art equivariant learning methods. Our work provides inspiring\nempirical evidence that carefully applied radial symmetry can alleviate the\nchallenges of information loss, marking a promising advance in equivariant\nnetwork architectures. The code is available at\nhttps://github.com/XYPB/GMR-Conv."}
{"id": "2504.02821", "pdf": "https://arxiv.org/pdf/2504.02821", "abs": "https://arxiv.org/abs/2504.02821", "authors": ["Mateusz Pach", "Shyamgopal Karthik", "Quentin Bouniot", "Serge Belongie", "Zeynep Akata"], "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint. The code is available at\n  https://github.com/ExplainableML/sae-for-vlm", "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs."}
{"id": "2504.02823", "pdf": "https://arxiv.org/pdf/2504.02823", "abs": "https://arxiv.org/abs/2504.02823", "authors": ["Divya Velayudhan", "Abdelfatah Ahmed", "Mohamad Alansari", "Neha Gour", "Abderaouf Behouch", "Taimur Hassan", "Syed Talal Wasim", "Nabil Maalej", "Muzammal Naseer", "Juergen Gall", "Mohammed Bennamoun", "Ernesto Damiani", "Naoufel Werghi"], "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted at CVPR 2025", "summary": "Advancements in Computer-Aided Screening (CAS) systems are essential for\nimproving the detection of security threats in X-ray baggage scans. However,\ncurrent datasets are limited in representing real-world, sophisticated threats\nand concealment tactics, and existing approaches are constrained by a\nclosed-set paradigm with predefined labels. To address these challenges, we\nintroduce STCray, the first multimodal X-ray baggage security dataset,\ncomprising 46,642 image-caption paired scans across 21 threat categories,\ngenerated using an X-ray scanner for airport security. STCray is meticulously\ndeveloped with our specialized protocol that ensures domain-aware, coherent\ncaptions, that lead to the multi-modal instruction following data in X-ray\nbaggage security. This allows us to train a domain-aware visual AI assistant\nnamed STING-BEE that supports a range of vision-language tasks, including scene\ncomprehension, referring threat localization, visual grounding, and visual\nquestion answering (VQA), establishing novel baselines for multi-modal learning\nin X-ray baggage security. Further, STING-BEE shows state-of-the-art\ngeneralization in cross-domain settings. Code, data, and models are available\nat https://divs1159.github.io/STING-BEE/."}
{"id": "2504.02826", "pdf": "https://arxiv.org/pdf/2504.02826", "abs": "https://arxiv.org/abs/2504.02826", "authors": ["Xiangyu Zhao", "Peiyuan Zhang", "Kexian Tang", "Hao Li", "Zicheng Zhang", "Guangtao Zhai", "Junchi Yan", "Hua Yang", "Xue Yang", "Haodong Duan"], "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing", "categories": ["cs.CV"], "comment": "27 pages, 23 figures, 1 table. Technical Report", "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench."}
{"id": "2504.02828", "pdf": "https://arxiv.org/pdf/2504.02828", "abs": "https://arxiv.org/abs/2504.02828", "authors": ["Jinqi Luo", "Tianjiao Ding", "Kwan Ho Ryan Chan", "Hancheng Min", "Chris Callison-Burch", "René Vidal"], "title": "Concept Lancet: Image Editing with Compositional Representation Transplant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan", "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation."}
{"id": "2504.01987", "pdf": "https://arxiv.org/pdf/2504.01987", "abs": "https://arxiv.org/abs/2504.01987", "authors": ["Ilir Tahiraj", "Markus Edinger", "Dominik Kulmer", "Markus Lienkamp"], "title": "CaLiV: LiDAR-to-Vehicle Calibration of Arbitrary Sensor Setups via Object Reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In autonomous systems, sensor calibration is essential for a safe and\nefficient navigation in dynamic environments. Accurate calibration is a\nprerequisite for reliable perception and planning tasks such as object\ndetection and obstacle avoidance. Many existing LiDAR calibration methods\nrequire overlapping fields of view, while others use external sensing devices\nor postulate a feature-rich environment. In addition, Sensor-to-Vehicle\ncalibration is not supported by the vast majority of calibration algorithms. In\nthis work, we propose a novel target-based technique for extrinsic\nSensor-to-Sensor and Sensor-to-Vehicle calibration of multi-LiDAR systems\ncalled CaLiV. This algorithm works for non-overlapping FoVs, as well as\narbitrary calibration targets, and does not require any external sensing\ndevices. First, we apply motion to produce FoV overlaps and utilize a simple\nunscented Kalman filter to obtain vehicle poses. Then, we use the Gaussian\nmixture model-based registration framework GMMCalib to align the point clouds\nin a common calibration frame. Finally, we reduce the task of recovering the\nsensor extrinsics to a minimization problem. We show that both translational\nand rotational Sensor-to-Sensor errors can be solved accurately by our method.\nIn addition, all Sensor-to-Vehicle rotation angles can also be calibrated with\nhigh accuracy. We validate the simulation results in real-world experiments.\nThe code is open source and available on https://github.com/TUMFTM/CaLiV."}
{"id": "2504.01988", "pdf": "https://arxiv.org/pdf/2504.01988", "abs": "https://arxiv.org/abs/2504.01988", "authors": ["Suman Raj", "Bhavani A Madhabhavi", "Madhav Kumar", "Prabhav Gupta", "Yogesh Simmhan"], "title": "Distance Estimation to Support Assistive Drones for the Visually Impaired using Robust Calibration", "categories": ["cs.RO", "cs.CV"], "comment": "39 pages", "summary": "Autonomous navigation by drones using onboard sensors, combined with deep\nlearning and computer vision algorithms, is impacting a number of domains. We\nexamine the use of drones to autonomously assist Visually Impaired People\n(VIPs) in navigating outdoor environments while avoiding obstacles. Here, we\npresent NOVA, a robust calibration technique using depth maps to estimate\nabsolute distances to obstacles in a campus environment. NOVA uses a\ndynamic-update method that can adapt to adversarial scenarios. We compare NOVA\nwith SOTA depth map approaches, and with geometric and regression-based\nbaseline models, for distance estimation to VIPs and other obstacles in diverse\nand dynamic conditions. We also provide exhaustive evaluations to validate the\nrobustness and generalizability of our methods. NOVA predicts distances to VIP\nwith an error <30cm and to different obstacles like cars and bicycles with a\nmaximum of 60cm error, which are better than the baselines. NOVA also clearly\nout-performs SOTA depth map methods, by upto 5.3-14.6x."}
{"id": "2504.01989", "pdf": "https://arxiv.org/pdf/2504.01989", "abs": "https://arxiv.org/abs/2504.01989", "authors": ["Yi Yao", "Miao Fan", "Shengtong Xu", "Haoyi Xiong", "Xiangzeng Liu", "Wenbo Hu", "Wenbing Huang"], "title": "A Concise Survey on Lane Topology Reasoning for HD Mapping", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by IEEE IV'25", "summary": "Lane topology reasoning techniques play a crucial role in high-definition\n(HD) mapping and autonomous driving applications. While recent years have\nwitnessed significant advances in this field, there has been limited effort to\nconsolidate these works into a comprehensive overview. This survey\nsystematically reviews the evolution and current state of lane topology\nreasoning methods, categorizing them into three major paradigms: procedural\nmodeling-based methods, aerial imagery-based methods, and onboard sensors-based\nmethods. We analyze the progression from early rule-based approaches to modern\nlearning-based solutions utilizing transformers, graph neural networks (GNNs),\nand other deep learning architectures. The paper examines standardized\nevaluation metrics, including road-level measures (APLS and TLTS score), and\nlane-level metrics (DET and TOP score), along with performance comparisons on\nbenchmark datasets such as OpenLane-V2. We identify key technical challenges,\nincluding dataset availability and model efficiency, and outline promising\ndirections for future research. This comprehensive review provides researchers\nand practitioners with insights into the theoretical frameworks, practical\nimplementations, and emerging trends in lane topology reasoning for HD mapping\napplications."}
{"id": "2504.01996", "pdf": "https://arxiv.org/pdf/2504.01996", "abs": "https://arxiv.org/abs/2504.01996", "authors": ["Khizar Anjum", "Parul Pandey", "Vidyasagar Sadhu", "Roberto Tron", "Dario Pompili"], "title": "Real-Time Navigation for Autonomous Aerial Vehicles Using Video", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Submitted to Journal of Real-Time Image Processing", "summary": "Most applications in autonomous navigation using mounted cameras rely on the\nconstruction and processing of geometric 3D point clouds, which is an expensive\nprocess. However, there is another simpler way to make a space navigable\nquickly: to use semantic information (e.g., traffic signs) to guide the agent.\nHowever, detecting and acting on semantic information involves Computer\nVision~(CV) algorithms such as object detection, which themselves are demanding\nfor agents such as aerial drones with limited onboard resources. To solve this\nproblem, we introduce a novel Markov Decision Process~(MDP) framework to reduce\nthe workload of these CV approaches. We apply our proposed framework to both\nfeature-based and neural-network-based object-detection tasks, using open-loop\nand closed-loop simulations as well as hardware-in-the-loop emulations. These\nholistic tests show significant benefits in energy consumption and speed with\nonly a limited loss in accuracy compared to models based on static features and\nneural networks."}
{"id": "2504.02045", "pdf": "https://arxiv.org/pdf/2504.02045", "abs": "https://arxiv.org/abs/2504.02045", "authors": ["Zhaoyang Zhang", "Yannick Hold-Geoffroy", "Miloš Hašan", "Chen Ziwen", "Fujun Luan", "Julie Dorsey", "Yiwei Hu"], "title": "WorldPrompter: Traversable Text-to-Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Scene-level 3D generation is a challenging research topic, with most existing\nmethods generating only partial scenes and offering limited navigational\nfreedom. We introduce WorldPrompter, a novel generative pipeline for\nsynthesizing traversable 3D scenes from text prompts. We leverage panoramic\nvideos as an intermediate representation to model the 360{\\deg} details of a\nscene. WorldPrompter incorporates a conditional 360{\\deg} panoramic video\ngenerator, capable of producing a 128-frame video that simulates a person\nwalking through and capturing a virtual environment. The resulting video is\nthen reconstructed as Gaussian splats by a fast feedforward 3D reconstructor,\nenabling a true walkable experience within the 3D scene. Experiments\ndemonstrate that our panoramic video generation model achieves convincing view\nconsistency across frames, enabling high-quality panoramic Gaussian splat\nreconstruction and facilitating traversal over an area of the scene.\nQualitative and quantitative results also show it outperforms the\nstate-of-the-art 360{\\deg} video generators and 3D scene generation models."}
{"id": "2504.02084", "pdf": "https://arxiv.org/pdf/2504.02084", "abs": "https://arxiv.org/abs/2504.02084", "authors": ["Nick Chodura", "Melissa Greeff", "Joshua Woods"], "title": "Evaluation of Flight Parameters in UAV-based 3D Reconstruction for Rooftop Infrastructure Assessment", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "8 pages, 6 figures, 2 tables", "summary": "Rooftop 3D reconstruction using UAV-based photogrammetry offers a promising\nsolution for infrastructure assessment, but existing methods often require high\npercentages of image overlap and extended flight times to ensure model accuracy\nwhen using autonomous flight paths. This study systematically evaluates key\nflight parameters-ground sampling distance (GSD) and image overlap-to optimize\nthe 3D reconstruction of complex rooftop infrastructure. Controlled UAV flights\nwere conducted over a multi-segment rooftop at Queen's University using a DJI\nPhantom 4 Pro V2, with varied GSD and overlap settings. The collected data were\nprocessed using Reality Capture software and evaluated against ground truth\nmodels generated from UAV-based LiDAR and terrestrial laser scanning (TLS).\nExperimental results indicate that a GSD range of 0.75-1.26 cm combined with\n85% image overlap achieves a high degree of model accuracy, while minimizing\nimages collected and flight time. These findings provide guidance for planning\nautonomous UAV flight paths for efficient rooftop assessments."}
{"id": "2504.02132", "pdf": "https://arxiv.org/pdf/2504.02132", "abs": "https://arxiv.org/abs/2504.02132", "authors": ["Ezzeldin Shereen", "Dan Ristea", "Burak Hasircioglu", "Shae McFadden", "Vasilios Mavroudis", "Chris Hicks"], "title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.IR"], "comment": "8 pages, 6 figures", "summary": "Multimodal retrieval augmented generation (M-RAG) has recently emerged as a\nmethod to inhibit hallucinations of large multimodal models (LMMs) through a\nfactual knowledge base (KB). However, M-RAG also introduces new attack vectors\nfor adversaries that aim to disrupt the system by injecting malicious entries\ninto the KB. In this work, we present a poisoning attack against M-RAG\ntargeting visual document retrieval applications, where the KB contains images\nof document pages. Our objective is to craft a single image that is retrieved\nfor a variety of different user queries, and consistently influences the output\nproduced by the generative model, thus creating a universal denial-of-service\n(DoS) attack against the M-RAG system. We demonstrate that while our attack is\neffective against a diverse range of widely-used, state-of-the-art retrievers\n(embedding models) and generators (LMMs), it can also be ineffective against\nrobust embedding models. Our attack not only highlights the vulnerability of\nM-RAG pipelines to poisoning attacks, but also sheds light on a fundamental\nweakness that potentially hinders their performance even in benign settings."}
{"id": "2504.02151", "pdf": "https://arxiv.org/pdf/2504.02151", "abs": "https://arxiv.org/abs/2504.02151", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "title": "Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "7 pages", "summary": "The rapid use of artificial intelligence (AI) in processes such as coding,\nimage processing, and data prediction means it is crucial to understand and\nvalidate the data we are working with fully. This paper dives into the hurdles\nof analyzing high-dimensional data, especially when it gets too complex.\nTraditional methods in data analysis often look at direct connections between\ninput variables, which can miss out on the more complicated relationships\nwithin the data.\n  To address these issues, we explore several tested techniques, such as\nremoving specific variables to see their impact and using statistical analysis\nto find connections between multiple variables. We also consider the role of\nsynthetic data and how information can sometimes be redundant across different\nsensors. These analyses are typically very computationally demanding and often\nrequire much human effort to make sense of the results.\n  A common approach is to treat the entire dataset as one unit and apply\nadvanced models to handle it. However, this can become problematic with larger,\nnoisier datasets and more complex models. So, we suggest methods to identify\noverall patterns that can help with tasks like classification or regression\nbased on the idea that more straightforward approaches might be more\nunderstandable.\n  Our research looks at two datasets: a real-world dataset and a synthetic one.\nThe goal is to create a methodology that highlights key features on a global\nscale that lead to predictions, making it easier to validate or quantify the\ndata set. By reducing the dimensionality with this method, we can simplify the\nmodels used and thus clarify the insights we gain. Furthermore, our method can\nreveal unexplored relationships between specific inputs and outcomes, providing\na way to validate these new connections further."}
{"id": "2504.02161", "pdf": "https://arxiv.org/pdf/2504.02161", "abs": "https://arxiv.org/abs/2504.02161", "authors": ["Zhen Meng", "Kan Chen", "Xiangmin Xu", "Erwin Jose Lopez Pulgarin", "Emma Li", "Philip G. Zhao", "David Flynn"], "title": "Preference-Driven Active 3D Scene Representation for Robotic Inspection in Nuclear Decommissioning", "categories": ["cs.RO", "cs.CV"], "comment": "This work has been submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025", "summary": "Active 3D scene representation is pivotal in modern robotics applications,\nincluding remote inspection, manipulation, and telepresence. Traditional\nmethods primarily optimize geometric fidelity or rendering accuracy, but often\noverlook operator-specific objectives, such as safety-critical coverage or\ntask-driven viewpoints. This limitation leads to suboptimal viewpoint\nselection, particularly in constrained environments such as nuclear\ndecommissioning. To bridge this gap, we introduce a novel framework that\nintegrates expert operator preferences into the active 3D scene representation\npipeline. Specifically, we employ Reinforcement Learning from Human Feedback\n(RLHF) to guide robotic path planning, reshaping the reward function based on\nexpert input. To capture operator-specific priorities, we conduct interactive\nchoice experiments that evaluate user preferences in 3D scene representation.\nWe validate our framework using a UR3e robotic arm for reactor tile inspection\nin a nuclear decommissioning scenario. Compared to baseline methods, our\napproach enhances scene representation while optimizing trajectory efficiency.\nThe RLHF-based policy consistently outperforms random selection, prioritizing\ntask-critical details. By unifying explicit 3D geometric modeling with implicit\nhuman-in-the-loop optimization, this work establishes a foundation for\nadaptive, safety-critical robotic perception systems, paving the way for\nenhanced automation in nuclear decommissioning, remote maintenance, and other\nhigh-risk environments."}
{"id": "2504.02163", "pdf": "https://arxiv.org/pdf/2504.02163", "abs": "https://arxiv.org/abs/2504.02163", "authors": ["Lewis Matheson Creed"], "title": "Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "50 Pages, 10 figures, Honours Thesis", "summary": "The limited availability of training data for low-resource languages makes\napplying machine learning techniques challenging. Ancient Egyptian is one such\nlanguage with few resources. However, innovative applications of data\naugmentation methods, such as Neural Style Transfer, could overcome these\nbarriers. This paper presents a novel method for generating datasets of ancient\nEgyptian hieroglyphs by applying NST to a digital typeface. Experimental\nresults found that image classification models trained on NST-generated\nexamples and photographs demonstrate equal performance and transferability to\nreal unseen images of hieroglyphs."}
{"id": "2504.02216", "pdf": "https://arxiv.org/pdf/2504.02216", "abs": "https://arxiv.org/abs/2504.02216", "authors": ["Samuel Fernández-Menduiña", "Eduardo Pavez", "Antonio Ortega"], "title": "Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Many images and videos are primarily processed by computer vision algorithms,\ninvolving only occasional human inspection. When this content requires\ncompression before processing, e.g., in distributed applications, coding\nmethods must optimize for both visual quality and downstream task performance.\nWe first show that, given the features obtained from the original and the\ndecoded images, an approach to reduce the effect of compression on a task loss\nis to perform rate-distortion optimization (RDO) using the distance between\nfeatures as a distortion metric. However, optimizing directly such a\nrate-distortion trade-off requires an iterative workflow of encoding, decoding,\nand feature evaluation for each coding parameter, which is computationally\nimpractical. We address this problem by simplifying the RDO formulation to make\nthe distortion term computable using block-based encoders. We first apply\nTaylor's expansion to the feature extractor, recasting the feature distance as\na quadratic metric with the Jacobian matrix of the neural network. Then, we\nreplace the linearized metric with a block-wise approximation, which we call\ninput-dependent squared error (IDSE). To reduce computational complexity, we\napproximate IDSE using Jacobian sketches. The resulting loss can be evaluated\nblock-wise in the transform domain and combined with the sum of squared errors\n(SSE) to address both visual quality and computer vision performance.\nSimulations with AVC across multiple feature extractors and downstream neural\nnetworks show up to 10% bit-rate savings for the same computer vision accuracy\ncompared to RDO based on SSE, with no decoder complexity overhead and just a 7%\nencoder complexity increase."}
{"id": "2504.02222", "pdf": "https://arxiv.org/pdf/2504.02222", "abs": "https://arxiv.org/abs/2504.02222", "authors": ["Liying Xu", "Hongliang He", "Wei Han", "Hanbin Huang", "Siwei Feng", "Guohong Fu"], "title": "APSeg: Auto-Prompt Model with Acquired and Injected Knowledge for Nuclear Instance Segmentation and Classification", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "Nuclear instance segmentation and classification provide critical\nquantitative foundations for digital pathology diagnosis. With the advent of\nthe foundational Segment Anything Model (SAM), the accuracy and efficiency of\nnuclear segmentation have improved significantly. However, SAM imposes a strong\nreliance on precise prompts, and its class-agnostic design renders its\nclassification results entirely dependent on the provided prompts. Therefore,\nwe focus on generating prompts with more accurate localization and\nclassification and propose \\textbf{APSeg}, \\textbf{A}uto-\\textbf{P}rompt model\nwith acquired and injected knowledge for nuclear instance \\textbf{Seg}mentation\nand classification. APSeg incorporates two knowledge-aware modules: (1)\nDistribution-Guided Proposal Offset Module (\\textbf{DG-POM}), which learns\ndistribution knowledge through density map guided, and (2) Category Knowledge\nSemantic Injection Module (\\textbf{CK-SIM}), which injects morphological\nknowledge derived from category descriptions. We conducted extensive\nexperiments on the PanNuke and CoNSeP datasets, demonstrating the effectiveness\nof our approach. The code will be released upon acceptance."}
{"id": "2504.02280", "pdf": "https://arxiv.org/pdf/2504.02280", "abs": "https://arxiv.org/abs/2504.02280", "authors": ["YiMing Yu", "Jason Zutty"], "title": "LLM-Guided Evolution: An Autonomous Model Optimization for Object Detection", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "In machine learning, Neural Architecture Search (NAS) requires domain\nknowledge of model design and a large amount of trial-and-error to achieve\npromising performance. Meanwhile, evolutionary algorithms have traditionally\nrelied on fixed rules and pre-defined building blocks. The Large Language Model\n(LLM)-Guided Evolution (GE) framework transformed this approach by\nincorporating LLMs to directly modify model source code for image\nclassification algorithms on CIFAR data and intelligently guide mutations and\ncrossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT)\ntechnique, which establishes feedback loops, allowing LLMs to refine their\ndecisions iteratively based on how previous operations performed. In this\nstudy, we perform NAS for object detection by improving LLM-GE to modify the\narchitecture of You Only Look Once (YOLO) models to enhance performance on the\nKITTI dataset. Our approach intelligently adjusts the design and settings of\nYOLO to find the optimal algorithms against objective such as detection\naccuracy and speed. We show that LLM-GE produced variants with significant\nperformance improvements, such as an increase in Mean Average Precision from\n92.5% to 94.5%. This result highlights the flexibility and effectiveness of\nLLM-GE on real-world challenges, offering a novel paradigm for automated\nmachine learning that combines LLM-driven reasoning with evolutionary\nstrategies."}
{"id": "2504.02329", "pdf": "https://arxiv.org/pdf/2504.02329", "abs": "https://arxiv.org/abs/2504.02329", "authors": ["Seif Mzoughi", "Ahmed Hajyahmed", "Mohamed Elshafei", "Foutse Khomh anb Diego Elias Costa"], "title": "Towards Assessing Deep Learning Test Input Generators", "categories": ["cs.LG", "cs.CV", "cs.SE"], "comment": "Accepted to EASE 2025", "summary": "Deep Learning (DL) systems are increasingly deployed in safety-critical\napplications, yet they remain vulnerable to robustness issues that can lead to\nsignificant failures. While numerous Test Input Generators (TIGs) have been\ndeveloped to evaluate DL robustness, a comprehensive assessment of their\neffectiveness across different dimensions is still lacking. This paper presents\na comprehensive assessment of four state-of-the-art TIGs--DeepHunter,\nDeepFault, AdvGAN, and SinVAD--across multiple critical aspects:\nfault-revealing capability, naturalness, diversity, and efficiency. Our\nempirical study leverages three pre-trained models (LeNet-5, VGG16, and\nEfficientNetB3) on datasets of varying complexity (MNIST, CIFAR-10, and\nImageNet-1K) to evaluate TIG performance. Our findings reveal important\ntrade-offs in robustness revealing capability, variation in test case\ngeneration, and computational efficiency across TIGs. The results also show\nthat TIG performance varies significantly with dataset complexity, as tools\nthat perform well on simpler datasets may struggle with more complex ones. In\ncontrast, others maintain steadier performance or better scalability. This\npaper offers practical guidance for selecting appropriate TIGs aligned with\nspecific objectives and dataset characteristics. Nonetheless, more work is\nneeded to address TIG limitations and advance TIGs for real-world,\nsafety-critical systems."}
{"id": "2504.02334", "pdf": "https://arxiv.org/pdf/2504.02334", "abs": "https://arxiv.org/abs/2504.02334", "authors": ["Boris Sukhovilov"], "title": "Determining Sphere Radius through Pairwise Distances", "categories": ["cs.CG", "cs.CV", "68U05 (Primary) 65D18 (Secondary)", "I.3.5; I.4.5"], "comment": "10 pages, we share the implementation of our method as open source\n  code at https://github.com/boris-sukhovilov/Sphere_Radius", "summary": "We propose a novel method for determining the radius of a spherical surface\nbased on the distances measured between points on this surface. We consider the\nmost general case of determining the radius when the distances are measured\nwith errors and the sphere has random deviations from its ideal shape. For the\nsolution, we used the minimally necessary four points and an arbitrary N number\nof points. We provide a new closed form solution for the radius of the sphere\nthrough the matrix of pairwise distances. We also determine the standard\ndeviation of the radius estimate caused by measurement errors and deviations of\nthe sphere from its ideal shape. We found optimal configurations of points on\nthe sphere that provide the minimum standard deviation of the radius estimate.\nThis paper describes our solution and provides all the mathematical\nderivations. We share the implementation of our method as open source code at\nhttps://github.com/boris-sukhovilov/Sphere_Radius."}
{"id": "2504.02361", "pdf": "https://arxiv.org/pdf/2504.02361", "abs": "https://arxiv.org/abs/2504.02361", "authors": ["Takahiro Shirakawa", "Tomoyuki Suzuki", "Daichi Haraguchi"], "title": "MG-Gen: Single Image to Motion Graphics Generation with Layer Decomposition", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "General image-to-video generation methods often produce suboptimal animations\nthat do not meet the requirements of animated graphics, as they lack active\ntext motion and exhibit object distortion. Also, code-based animation\ngeneration methods typically require layer-structured vector data which are\noften not readily available for motion graphic generation. To address these\nchallenges, we propose a novel framework named MG-Gen that reconstructs data in\nvector format from a single raster image to extend the capabilities of\ncode-based methods to enable motion graphics generation from a raster image in\nthe framework of general image-to-video generation. MG-Gen first decomposes the\ninput image into layer-wise elements, reconstructs them as HTML format data and\nthen generates executable JavaScript code for the reconstructed HTML data. We\nexperimentally confirm that \\ours{} generates motion graphics while preserving\ntext readability and input consistency. These successful results indicate that\ncombining layer decomposition and animation code generation is an effective\nstrategy for motion graphics generation."}
{"id": "2504.02373", "pdf": "https://arxiv.org/pdf/2504.02373", "abs": "https://arxiv.org/abs/2504.02373", "authors": ["Hantang Li", "Jinhua Hao", "Lei Xiong", "Shuyuan Zhu"], "title": "HPGN: Hybrid Priors-Guided Network for Compressed Low-Light Image Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures", "summary": "In practical applications, conventional methods generate large volumes of\nlow-light images that require compression for efficient storage and\ntransmission. However, most existing methods either disregard the removal of\npotential compression artifacts during the enhancement process or fail to\nestablish a unified framework for joint task enhancement of images with varying\ncompression qualities. To solve this problem, we propose the hybrid\npriors-guided network (HPGN), which enhances compressed low-light images by\nintegrating both compression and illumination priors. Our approach fully\nutilizes the JPEG quality factor (QF) and DCT quantization matrix (QM) to guide\nthe design of efficient joint task plug-and-play modules. Additionally, we\nemploy a random QF generation strategy to guide model training, enabling a\nsingle model to enhance images across different compression levels.\nExperimental results confirm the superiority of our proposed method."}
{"id": "2504.02382", "pdf": "https://arxiv.org/pdf/2504.02382", "abs": "https://arxiv.org/abs/2504.02382", "authors": ["Yudi Sang", "Yanzhen Liu", "Sutuke Yibulayimu", "Yunning Wang", "Benjamin D. Killeen", "Mingxu Liu", "Ping-Cheng Ku", "Ole Johannsen", "Karol Gotkowski", "Maximilian Zenk", "Klaus Maier-Hein", "Fabian Isensee", "Peiyan Yue", "Yi Wang", "Haidong Yu", "Zhaohong Pan", "Yutong He", "Xiaokun Liang", "Daiqi Liu", "Fuxin Fan", "Artur Jurgas", "Andrzej Skalski", "Yuxi Ma", "Jing Yang", "Szymon Płotka", "Rafał Litka", "Gang Zhu", "Yingchun Song", "Mathias Unberath", "Mehran Armand", "Dan Ruan", "S. Kevin Zhou", "Qiyong Cao", "Chunpeng Zhao", "Xinbao Wu", "Yu Wang"], "title": "Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "PENGWIN 2024 Challenge Report", "summary": "The segmentation of pelvic fracture fragments in CT and X-ray images is\ncrucial for trauma diagnosis, surgical planning, and intraoperative guidance.\nHowever, accurately and efficiently delineating the bone fragments remains a\nsignificant challenge due to complex anatomy and imaging limitations. The\nPENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance\nautomated fracture segmentation by benchmarking state-of-the-art algorithms on\nthese complex tasks. A diverse dataset of 150 CT scans was collected from\nmultiple clinical centers, and a large set of simulated X-ray images was\ngenerated using the DeepDRR method. Final submissions from 16 teams worldwide\nwere evaluated under a rigorous multi-metric testing scheme. The top-performing\nCT algorithm achieved an average fragment-wise intersection over union (IoU) of\n0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the\nbest algorithm attained an IoU of 0.774, highlighting the greater challenges\nposed by overlapping anatomical structures. Beyond the quantitative evaluation,\nthe challenge revealed methodological diversity in algorithm design. Variations\nin instance representation, such as primary-secondary classification versus\nboundary-core separation, led to differing segmentation strategies. Despite\npromising results, the challenge also exposed inherent uncertainties in\nfragment definition, particularly in cases of incomplete fractures. These\nfindings suggest that interactive segmentation approaches, integrating human\ndecision-making with task-relevant information, may be essential for improving\nmodel reliability and clinical applicability."}
{"id": "2504.02408", "pdf": "https://arxiv.org/pdf/2504.02408", "abs": "https://arxiv.org/abs/2504.02408", "authors": ["Naomi Silverstein", "Efrat Leibowitz", "Ron Beloosesky", "Haim Azhari"], "title": "Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 7 figures", "summary": "Ultrasound is a widely accessible and cost-effective medical imaging tool\ncommonly used for prenatal evaluation of the fetal brain. However, it has\nlimitations, particularly in the third trimester, where the complexity of the\nfetal brain requires high image quality for extracting quantitative data. In\ncontrast, magnetic resonance imaging (MRI) offers superior image quality and\ntissue differentiation but is less available, expensive, and requires\ntime-consuming acquisition. Thus, transforming ultrasonic images into an\nMRI-mimicking display may be advantageous and allow better tissue anatomy\npresentation. To address this goal, we have examined the use of artificial\nintelligence, implementing a diffusion model renowned for generating\nhigh-quality images. The proposed method, termed \"Dual Diffusion Imposed\nCorrelation\" (DDIC), leverages a diffusion-based translation methodology,\nassuming a shared latent space between ultrasound and MRI domains. Model\ntraining was obtained utilizing the \"HC18\" dataset for ultrasound and the \"CRL\nfetal brain atlas\" along with the \"FeTA \" datasets for MRI. The generated\npseudo-MRI images provide notable improvements in visual discrimination of\nbrain tissue, especially in the lateral ventricles and the Sylvian fissure,\ncharacterized by enhanced contrast clarity. Improvement was demonstrated in\nMutual information, Peak signal-to-noise ratio, Fr\\'echet Inception Distance,\nand Contrast-to-noise ratio. Findings from these evaluations indicate\nstatistically significant superior performance of the DDIC compared to other\ntranslation methodologies. In addition, a Medical Opinion Test was obtained\nfrom 5 gynecologists. The results demonstrated display improvement in 81% of\nthe tested images. In conclusion, the presented pseudo-MRI images hold the\npotential for streamlining diagnosis and enhancing clinical outcomes through\nimproved representation."}
{"id": "2504.02439", "pdf": "https://arxiv.org/pdf/2504.02439", "abs": "https://arxiv.org/abs/2504.02439", "authors": ["Jack Sander", "Giammarco Caroleo", "Alessandro Albini", "Perla Maiolino"], "title": "Estimating Scene Flow in Robot Surroundings with Distributed Miniaturized Time-of-Flight Sensors", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 5 figures, 2 tables, 1 algorithm", "summary": "Tracking motions of humans or objects in the surroundings of the robot is\nessential to improve safe robot motions and reactions. In this work, we present\nan approach for scene flow estimation from low-density and noisy point clouds\nacquired from miniaturized Time of Flight (ToF) sensors distributed on the\nrobot body. The proposed method clusters points from consecutive frames and\napplies Iterative Closest Point (ICP) to estimate a dense motion flow, with\nadditional steps introduced to mitigate the impact of sensor noise and\nlow-density data points. Specifically, we employ a fitness-based classification\nto distinguish between stationary and moving points and an inlier removal\nstrategy to refine geometric correspondences. The proposed approach is\nvalidated in an experimental setup where 24 ToF are used to estimate the\nvelocity of an object moving at different controlled speeds. Experimental\nresults show that the method consistently approximates the direction of the\nmotion and its magnitude with an error which is in line with sensor noise."}
{"id": "2504.02465", "pdf": "https://arxiv.org/pdf/2504.02465", "abs": "https://arxiv.org/abs/2504.02465", "authors": ["Soumyaratna Debnath", "Ashish Tiwari", "Kaustubh Sadekar", "Shanmuganathan Raman"], "title": "RASP: Revisiting 3D Anamorphic Art for Shadow-Guided Packing of Irregular Objects", "categories": ["cs.GR", "cs.CV"], "comment": "Conference on Computer Vision and Pattern Recognition (CVPR) 2025", "summary": "Recent advancements in learning-based methods have opened new avenues for\nexploring and interpreting art forms, such as shadow art, origami, and sketch\nart, through computational models. One notable visual art form is 3D Anamorphic\nArt in which an ensemble of arbitrarily shaped 3D objects creates a realistic\nand meaningful expression when observed from a particular viewpoint and loses\nits coherence over the other viewpoints. In this work, we build on insights\nfrom 3D Anamorphic Art to perform 3D object arrangement. We introduce RASP, a\ndifferentiable-rendering-based framework to arrange arbitrarily shaped 3D\nobjects within a bounded volume via shadow (or silhouette)-guided optimization\nwith an aim of minimal inter-object spacing and near-maximal occupancy.\nFurthermore, we propose a novel SDF-based formulation to handle inter-object\nintersection and container extrusion. We demonstrate that RASP can be extended\nto part assembly alongside object packing considering 3D objects to be \"parts\"\nof another 3D object. Finally, we present artistic illustrations of multi-view\nanamorphic art, achieving meaningful expressions from multiple viewpoints\nwithin a single ensemble."}
{"id": "2504.02473", "pdf": "https://arxiv.org/pdf/2504.02473", "abs": "https://arxiv.org/abs/2504.02473", "authors": ["Rick van Essen", "Eldert van Henten", "Lammert Kooistra", "Gert Kootstra"], "title": "Adaptive path planning for efficient object search by UAVs in agricultural fields", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents an adaptive path planner for object search in\nagricultural fields using UAVs. The path planner uses a high-altitude coverage\nflight path and plans additional low-altitude inspections when the detection\nnetwork is uncertain. The path planner was evaluated in an offline simulation\nenvironment containing real-world images. We trained a YOLOv8 detection network\nto detect artificial plants placed in grass fields to showcase the potential of\nour path planner. We evaluated the effect of different detection certainty\nmeasures, optimized the path planning parameters, investigated the effects of\nlocalization errors and different numbers of objects in the field. The YOLOv8\ndetection confidence worked best to differentiate between true and false\npositive detections and was therefore used in the adaptive planner. The optimal\nparameters of the path planner depended on the distribution of objects in the\nfield, when the objects were uniformly distributed, more low-altitude\ninspections were needed compared to a non-uniform distribution of objects,\nresulting in a longer path length. The adaptive planner proved to be robust\nagainst localization uncertainty. When increasing the number of objects, the\nflight path length increased, especially when the objects were uniformly\ndistributed. When the objects were non-uniformly distributed, the adaptive path\nplanner yielded a shorter path than a low-altitude coverage path, even with\nhigh number of objects. Overall, the presented adaptive path planner allowed to\nfind non-uniformly distributed objects in a field faster than a coverage path\nplanner and resulted in a compatible detection accuracy. The path planner is\nmade available at https://github.com/wur-abe/uav_adaptive_planner."}
{"id": "2504.02477", "pdf": "https://arxiv.org/pdf/2504.02477", "abs": "https://arxiv.org/abs/2504.02477", "authors": ["Xiaofeng Han", "Shunpeng Chen", "Zenghuang Fu", "Zhe Feng", "Lue Fan", "Dong An", "Changwei Wang", "Li Guo", "Weiliang Meng", "Xiaopeng Zhang", "Rongtao Xu", "Shibiao Xu"], "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision", "categories": ["cs.RO", "cs.CV"], "comment": "27 pages, 11 figures, survey paper submitted to Information Fusion", "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV."}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research."}
{"id": "2504.02620", "pdf": "https://arxiv.org/pdf/2504.02620", "abs": "https://arxiv.org/abs/2504.02620", "authors": ["Leonardo Iurada", "Marco Ciccone", "Tatiana Tommasi"], "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted ICLR 2025 - https://github.com/iurada/talos-task-arithmetic", "summary": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications."}
{"id": "2504.02628", "pdf": "https://arxiv.org/pdf/2504.02628", "abs": "https://arxiv.org/abs/2504.02628", "authors": ["Chu Han", "Bingchao Zhao", "Jiatai Lin", "Shanshan Lyu", "Longfei Wang", "Tianpeng Deng", "Cheng Lu", "Changhong Liang", "Hannah Y. Wen", "Xiaojing Guo", "Zhenwei Shi", "Zaiyi Liu"], "title": "Towards Computation- and Communication-efficient Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Despite the impressive performance across a wide range of applications,\ncurrent computational pathology models face significant diagnostic efficiency\nchallenges due to their reliance on high-magnification whole-slide image\nanalysis. This limitation severely compromises their clinical utility,\nespecially in time-sensitive diagnostic scenarios and situations requiring\nefficient data transfer. To address these issues, we present a novel\ncomputation- and communication-efficient framework called Magnification-Aligned\nGlobal-Local Transformer (MAGA-GLTrans). Our approach significantly reduces\ncomputational time, file transfer requirements, and storage overhead by\nenabling effective analysis using low-magnification inputs rather than\nhigh-magnification ones. The key innovation lies in our proposed magnification\nalignment (MAGA) mechanism, which employs self-supervised learning to bridge\nthe information gap between low and high magnification levels by effectively\naligning their feature representations. Through extensive evaluation across\nvarious fundamental CPath tasks, MAGA-GLTrans demonstrates state-of-the-art\nclassification performance while achieving remarkable efficiency gains: up to\n10.7 times reduction in computational time and over 20 times reduction in file\ntransfer and storage requirements. Furthermore, we highlight the versatility of\nour MAGA framework through two significant extensions: (1) its applicability as\na feature extractor to enhance the efficiency of any CPath architecture, and\n(2) its compatibility with existing foundation models and\nhistopathology-specific encoders, enabling them to process low-magnification\ninputs with minimal information loss. These advancements position MAGA-GLTrans\nas a particularly promising solution for time-sensitive applications,\nespecially in the context of intraoperative frozen section diagnosis where both\naccuracy and efficiency are paramount."}
{"id": "2504.02647", "pdf": "https://arxiv.org/pdf/2504.02647", "abs": "https://arxiv.org/abs/2504.02647", "authors": ["Feng Gao", "Miao Fu", "Jingchao Cao", "Junyu Dong", "Qian Du"], "title": "Adaptive Frequency Enhancement Network for Remote Sensing Image Semantic Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IEEE TGRS 2025", "summary": "Semantic segmentation of high-resolution remote sensing images plays a\ncrucial role in land-use monitoring and urban planning. Recent remarkable\nprogress in deep learning-based methods makes it possible to generate\nsatisfactory segmentation results. However, existing methods still face\nchallenges in adapting network parameters to various land cover distributions\nand enhancing the interaction between spatial and frequency domain features. To\naddress these challenges, we propose the Adaptive Frequency Enhancement Network\n(AFENet), which integrates two key components: the Adaptive Frequency and\nSpatial feature Interaction Module (AFSIM) and the Selective feature Fusion\nModule (SFM). AFSIM dynamically separates and modulates high- and low-frequency\nfeatures according to the content of the input image. It adaptively generates\ntwo masks to separate high- and low-frequency components, therefore providing\noptimal details and contextual supplementary information for ground object\nfeature representation. SFM selectively fuses global context and local detailed\nfeatures to enhance the network's representation capability. Hence, the\ninteractions between frequency and spatial features are further enhanced.\nExtensive experiments on three publicly available datasets demonstrate that the\nproposed AFENet outperforms state-of-the-art methods. In addition, we also\nvalidate the effectiveness of AFSIM and SFM in managing diverse land cover\ntypes and complex scenarios. Our codes are available at\nhttps://github.com/oucailab/AFENet."}
{"id": "2504.02666", "pdf": "https://arxiv.org/pdf/2504.02666", "abs": "https://arxiv.org/abs/2504.02666", "authors": ["Mei Li", "Yuxiang Lu", "Qinyan Dai", "Suizhi Huang", "Yue Ding", "Hongtao Lu"], "title": "BECAME: BayEsian Continual Learning with Adaptive Model MErging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Learning (CL) strives to learn incrementally across tasks while\nmitigating catastrophic forgetting. A key challenge in CL is balancing\nstability (retaining prior knowledge) and plasticity (learning new tasks).\nWhile representative gradient projection methods ensure stability, they often\nlimit plasticity. Model merging techniques offer promising solutions, but prior\nmethods typically rely on empirical assumptions and carefully selected\nhyperparameters. In this paper, we explore the potential of model merging to\nenhance the stability-plasticity trade-off, providing theoretical insights that\nunderscore its benefits. Specifically, we reformulate the merging mechanism\nusing Bayesian continual learning principles and derive a closed-form solution\nfor the optimal merging coefficient that adapts to the diverse characteristics\nof tasks. To validate our approach, we introduce a two-stage framework named\nBECAME, which synergizes the expertise of gradient projection and adaptive\nmerging. Extensive experiments show that our approach outperforms\nstate-of-the-art CL methods and existing merging strategies."}
{"id": "2504.02797", "pdf": "https://arxiv.org/pdf/2504.02797", "abs": "https://arxiv.org/abs/2504.02797", "authors": ["Prashanth Chandran", "Agon Serifi", "Markus Gross", "Moritz Bächer"], "title": "Spline-based Transformers", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce Spline-based Transformers, a novel class of Transformer models\nthat eliminate the need for positional encoding. Inspired by workflows using\nsplines in computer animation, our Spline-based Transformers embed an input\nsequence of elements as a smooth trajectory in latent space. Overcoming\ndrawbacks of positional encoding such as sequence length extrapolation,\nSpline-based Transformers also provide a novel way for users to interact with\ntransformer latent spaces by directly manipulating the latent control points to\ncreate new latent trajectories and sequences. We demonstrate the superior\nperformance of our approach in comparison to conventional positional encoding\non a variety of datasets, ranging from synthetic 2D to large-scale real-world\ndatasets of images, 3D shapes, and animations."}
